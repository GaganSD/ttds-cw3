	title	description	owner	date	subtitle	keywords	#downloads	#views	#votes	dataset_slug
0	MNIST	"The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.
Source: http://yann.lecun.com/exdb/mnist/
Image Source: https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png"	https://paperswithcode.com/dataset/mnist							
1	CelebA	"CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.
Source: Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention
Image Source: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"	https://paperswithcode.com/dataset/celeba	01/01/2015	CelebFaces Attributes Dataset					
2	JFT-300M	JFT-300M is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images.	https://paperswithcode.com/dataset/jft-300m	10/07/2017	JFT-300M					
3	GLUE	"General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.
Source: Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models
Image Source: https://gluebenchmark.com/"	https://paperswithcode.com/dataset/glue	01/01/2019	General Language Understanding Evaluation benchmark					
4	MultiNLI	"The Multi-Genre Natural Language Inference (MultiNLI) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.
Source: Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information"	https://paperswithcode.com/dataset/multinli	01/01/2018	Multi-Genre Natural Language Inference					
5	ImageNet	"The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.
The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.
ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.
The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.

Total number of non-empty WordNet synsets: 21841
Total number of images: 14197122
Number of images with bounding box annotations: 1,034,908
Number of synsets with SIFT features: 1000
Number of images with SIFT features: 1.2 million

Source: ImageNet Large Scale Visual Recognition Challenge
Image Source: https://cs.stanford.edu/people/karpathy/cnnembed/"	https://paperswithcode.com/dataset/imagenet	01/01/2009						
6	Penn Treebank	"The English Penn Treebank (PTB) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).
The corpus is also commonly used for character-level and word-level Language Modelling.
Source: Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling
Image Source: https://dl.acm.org/doi/10.5555/972470.972475"	https://paperswithcode.com/dataset/penn-treebank	01/01/1993						
7	WikiText-103	"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.
Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.
Source: The WikiText Long Term Dependency Language Modeling Dataset
Image Source: https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"	https://paperswithcode.com/dataset/wikitext-103	26/09/2016	WikiText-103					
8	LFW	"The LFW dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.
Source: A Performance Evaluation of Loss Functions for Deep Face Recognition
Image Source: http://vis-www.cs.umass.edu/lfw"	https://paperswithcode.com/dataset/lfw		Labeled Faces in the Wild					
9	WikiSQL	"WikiSQL consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.
Source: SQL-to-Text Generation with Graph-to-Sequence Model
Image Source: https://blog.einstein.ai/how-to-talk-to-your-database/"	https://paperswithcode.com/dataset/wikisql	01/01/2017	WikiSQL					
10	OpenAI Gym	"OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text.
Source: https://github.com/openai/gym
Image Source: https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947"	https://paperswithcode.com/dataset/openai-gym	01/01/2016	OpenAI Gym					
11	WikiText-2	"The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.
Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.
Source: The WikiText Long Term Dependency Language Modeling Dataset
Image Source: https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"	https://paperswithcode.com/dataset/wikitext-2	26/09/2016	WikiText-2					
12	WikiLarge	"WikiLarge comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references
Source: Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders
Image Source: https://arxiv.org/pdf/1904.02767.pdf"	https://paperswithcode.com/dataset/wikilarge	01/01/2017						
13	Food-101	"The  Food-101 dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.
Source: Combining Weakly and Webly Supervised Learning for Classifying Food Images
Image Source: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/"	https://paperswithcode.com/dataset/food-101	01/01/2014						
14	Fashion-MNIST	"Fashion-MNIST is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.
Source: Generative Probabilistic Novelty Detection with Adversarial Autoencoders
Image Source: https://github.com/zalandoresearch/fashion-mnist"	https://paperswithcode.com/dataset/fashion-mnist	01/01/2017						
15	ShapeNet	"ShapeNet is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).
Source: A review on deep learning techniques for 3D sensed data classification
Image Source: ShapeNet: An Information-Rich 3D Model Repository"	https://paperswithcode.com/dataset/shapenet	01/01/2015						
16	CINIC-10	"CINIC-10 is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.
Source: Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge
Image Source: https://arxiv.org/abs/1810.03505"	https://paperswithcode.com/dataset/cinic-10	01/01/2018	CINIC-10					
17	Flickr30k	"The Flickr30k dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.
Source: Guiding Long-Short Term Memory for Image Caption Generation
Image Source: Dual-Path Convolutional Image-Text Embedding with Instance Loss"	https://paperswithcode.com/dataset/flickr30k	01/01/2014	Flickr30k					
18	COCO	"The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.
Splits:
The first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.
Based on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.
Annotations:
The dataset has annotations for

object detection: bounding boxes and per-instance segmentation masks with 80 object categories,
captioning: natural language descriptions of the images (see MS COCO Captions),
keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),
stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),
panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),
dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.
The annotations are publicly available only for training and validation images.

Source: https://cocodataset.org/
Image Source: https://cocodataset.org/"	https://paperswithcode.com/dataset/coco	01/01/2014	Microsoft Common Objects in Context					
19	BSDS500	"Berkeley Segmentation Data Set 500 (BSDS500) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.
Source: Object Contour Detection with a Fully Convolutional Encoder-Decoder Network
Image Source: https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html"	https://paperswithcode.com/dataset/bsds500	01/01/2011	Berkeley Segmentation Dataset 500					
20	MHP	"The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting.
Source: Multiple-Human Parsing in the Wild
Image Source: Li et al"	https://paperswithcode.com/dataset/mhp	19/05/2017	Multiple-Human Parsing					
21	LRW	"The Lip Reading in the Wild (LRW) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.
Source: Towards Pose-invariant Lip-Reading
Image Source: https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html"	https://paperswithcode.com/dataset/lrw	01/01/2016	Lip Reading in the Wild					
22	FDDB	"The Face Detection Dataset and Benchmark (FDDB) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.
Source: A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications"	https://paperswithcode.com/dataset/fddb		Face Detection Dataset and Benchmark					
23	GOT-10k	"The GOT-10k dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.
Source: http://got-10k.aitestunion.com/
Image Source: https://arxiv.org/pdf/1810.11981.pdf"	https://paperswithcode.com/dataset/got-10k	01/01/2018	Generic Object Tracking Benchmark					
24	CIFAR10		https://paperswithcode.com/dataset/cifar10							
25	DukeMTMC-reID	"The DukeMTMC-reID (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.
NOTE: This dataset has been retracted.
Source: Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification"	https://paperswithcode.com/dataset/dukemtmc-reid	06/09/2016						
26	KITTI	"KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. Álvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.
Source: A Review on Deep Learning Techniques Applied to Semantic Segmentation
Image Source: http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d"	https://paperswithcode.com/dataset/kitti	01/01/2012						
27	UCF101	"UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.
Source: Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification
Image Source: https://www.crcv.ucf.edu/data/UCF101.php"	https://paperswithcode.com/dataset/ucf101	03/12/2012	UCF101 Human Actions dataset					
28	HMDB51	"The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,849 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.
Source: Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors
Image Source: https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database"	https://paperswithcode.com/dataset/hmdb51	01/01/2011						
29	MIMIC-III	"The Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average.
Source: Interaction Matching for Long-Tail Multi-Label Classification"	https://paperswithcode.com/dataset/mimic-iii							
30	TID2013	"TID2013 is a dataset for image quality assessment that contains 25 reference images and 3000 distorted images (25 reference images x 24 types of distortions x 5 levels of distortions). 
Source: Lukin et al"	https://paperswithcode.com/dataset/tid2013		TID2013					
31	LIVE	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/live		Laboratory for Image & Video Engineering					
32	Visual Genome	"Visual Genome contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.
Source: RaAM: A Relation-aware Attention Model for Visual Question Answering
Image Source: Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"	https://paperswithcode.com/dataset/visual-genome	01/01/2017						
33	COCO-Stuff	"The Common Objects in COntext-stuff (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.
Source: Image Colorization: A Survey and Dataset
Image Source: https://github.com/nightrome/cocostuff"	https://paperswithcode.com/dataset/coco-stuff	01/01/2018	Common Objects in COntext-stuff					
34	MARS	"MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).
Source: Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets"	https://paperswithcode.com/dataset/mars	01/01/2016	Motion Analysis and Re-identification Set					
35	iLIDS-VID	"The iLIDS-VID dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.
Source: http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html
Image Source: http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html"	https://paperswithcode.com/dataset/ilids-vid	01/01/2009	iLIDS-VID					
36	Market-1501	"Market-1501 is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.
Source: A Survey of Pruning Methods for Efficient Person Re-identification Across Domains
Image Source: https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification"	https://paperswithcode.com/dataset/market-1501	01/01/2015						
37	VIPeR	"The Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front), 45°, 90° (right), 135°, and 180° (back).
Source: PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification
Image Source: Qin et al"	https://paperswithcode.com/dataset/viper		Viewpoint Invariant Pedestrian Recognition					
38	CUHK01	"This dataset contains 971 identities from two disjoint camera views. Each identity has two samples per camera view. It is used for Person Re-identification.
Paper: Li W., Zhao R., Wang X. (2013) Human Reidentification with Transferred Metric Learning. In: Lee K.M., Matsushita Y., Rehg J.M., Hu Z. (eds) Computer Vision – ACCV 2012. ACCV 2012. Lecture Notes in Computer Science, vol 7724. Springer, Berlin, Heidelberg"	https://paperswithcode.com/dataset/cuhk01		CUHK Person Re-identification					
39	PRID2011	"PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances
Source: PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification"	https://paperswithcode.com/dataset/prid2011	01/01/2011	Person RE-ID 2011					
40	CUHK03	"The CUHK03 consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training
Source: Attention Driven Person Re-identification
Image Source: Person Re-Identification Techniques for Intelligent Video Surveillance Systems"	https://paperswithcode.com/dataset/cuhk03	01/01/2014	Chinese University of Hong Kong Re-identification					
41	VehicleID	"The “VehicleID” dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.“MINI-cooper”, “Audi A6L” and “BWM 1 Series”).
Source: https://www.pkuml.org/resources/pku-vehicleid.html
Image Source: https://www.pkuml.org/resources/pku-vehicleid.html"	https://paperswithcode.com/dataset/vehicleid	01/01/2016	PKU VehicleID					
42	BP4D	"The BP4D-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.
The database includes forty-one participants (23 women, 18 men). They were 18 – 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.
The database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).
Source: http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html
Image Source: http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html"	https://paperswithcode.com/dataset/bp4d	01/01/2014						
43	DISFA	"The Denver Intensity of Spontaneous Facial Action (DISFA) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.
Source: Deep Learning For Smile Recognition
Image Source: https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237"	https://paperswithcode.com/dataset/disfa	01/01/2013	Denver Intensity of Spontaneous Facial Action					
44	CUB-200-2011	"The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.
Source: Fine-grained Visual-textual Representation Learning
Image Source: http://www.vision.caltech.edu/visipedia/CUB-200-2011.html"	https://paperswithcode.com/dataset/cub-200-2011		Caltech-UCSD Birds-200-2011					
45	SUN397	"The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.
Image Source: The Selection of Useful Visual Words in Class-Imbalanced Image Classification"	https://paperswithcode.com/dataset/sun397		SUN397					
46	FewRel	"The FewRel (Few-Shot Relation Classification Dataset) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).
Source: Neural Snowball for Few-Shot Relation Learning
Image Source: https://www.aclweb.org/anthology/D18-1514.pdf"	https://paperswithcode.com/dataset/fewrel	01/01/2018	Few-Shot Relation Classification Dataset					
47	DuReader	"DuReader is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations – each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.
Source: https://arxiv.org/pdf/1711.05073v4.pdf
Image Source: https://arxiv.org/pdf/1711.05073v4.pdf"	https://paperswithcode.com/dataset/dureader	01/01/2018						
48	SearchQA	"SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. 
Source: SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine"	https://paperswithcode.com/dataset/searchqa	18/04/2017						
49	CoQA	"CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.
CoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.
Source: https://stanfordnlp.github.io/coqa/
Image Source: https://stanfordnlp.github.io/coqa/"	https://paperswithcode.com/dataset/coqa	01/01/2018	Conversational Question Answering Challenge					
50	MovieQA	"The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.
Source: Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents
Image Source: https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716"	https://paperswithcode.com/dataset/movieqa	01/01/2016	MovieQA					
51	Hutter Prize	"The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.
Source: NLP Progress"	https://paperswithcode.com/dataset/hutter-prize							
52	ICDAR 2013	"The ICDAR 2013 dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.
Source: Single Shot Text Detector with Regional Attention
Image Source: https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856"	https://paperswithcode.com/dataset/icdar-2013	01/01/2013	ICDAR 2013					
53	Visual Madlibs	"Visual Madlibs is a dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.
Source: Visual Madlibs: Fill in the blank Image Generation and Question Answering
Image Source: Yu et al"	https://paperswithcode.com/dataset/visual-madlibs							
54	DAQUAR	"DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images.
Source: A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input
Image Source: https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/"	https://paperswithcode.com/dataset/daquar							
55	Visual7W	"Visual7W is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.
Source: https://github.com/yukezhu/visual7w-toolkit
Image Source: http://ai.stanford.edu/~yukez/visual7w/"	https://paperswithcode.com/dataset/visual7w	01/01/2016						
56	FM-IQA	"FM-IQA is a question-answering dataset containing over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations.
Source: Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering"	https://paperswithcode.com/dataset/fm-iqa		Freestyle Multilingual Image Question Answering					
57	NewsQA	"The NewsQA dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.

Documents are CNN news articles.
Questions are written by human users in natural language.
Answers may be multiword passages of the source text.
Questions may be unanswerable.
NewsQA is collected using a 3-stage, siloed process.
Questioners see only an article’s headline and highlights.
Answerers see the question and the full article, then select an answer passage.
Validators see the article, the question, and a set of answers that they rank.
NewsQA is more natural and more challenging than previous datasets.

Source: https://www.microsoft.com/en-us/research/project/newsqa-dataset/
Image Source: Trischler et al"	https://paperswithcode.com/dataset/newsqa	01/01/2017						
58	TriviaQA	"TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.
Source: Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data
Image Source: Joshi et al"	https://paperswithcode.com/dataset/triviaqa	01/01/2017						
59	RecipeQA	"RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.
Source: RecipeQA"	https://paperswithcode.com/dataset/recipeqa	04/09/2018						
60	SQuAD	"The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.
Source: Deep Learning Based Text Classification: A Comprehensive Review
Image Source: https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html"	https://paperswithcode.com/dataset/squad	01/01/2016	Stanford Question Answering Dataset					
61	NarrativeQA	"The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.
Source: DeepMind
Image Source: Kočiský et al"	https://paperswithcode.com/dataset/narrativeqa	19/12/2017	NarrativeQA					
62	CliCR	"CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports.
Source: CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension"	https://paperswithcode.com/dataset/clicr	26/03/2018	CliCR					
63	MS MARCO	"The MS MARCO (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.
The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.
Source: https://microsoft.github.io/msmarco/
Image Source: https://arxiv.org/pdf/1809.08267.pdf"	https://paperswithcode.com/dataset/ms-marco	01/01/2016	Microsoft Machine Reading Comprehension Dataset					
64	MultiRC	"MultiRC (Multi-Sentence Reading Comprehension) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.
The dataset was designed with three key challenges in mind:
* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.
* The correct answer(s) is not required to be a span in the text.
* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.
The entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.
Source: https://cogcomp.seas.upenn.edu/multirc/
Image Source: https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/"	https://paperswithcode.com/dataset/multirc	01/01/2018	Multi-Sentence Reading Comprehension					
65	HotpotQA	"HotpotQA is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. 
A diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.
Source: Answering Complex Open-domain Questions Through Iterative Query Generation
Image Source: Yang et al"	https://paperswithcode.com/dataset/hotpotqa	01/01/2018						
66	RACE	"The ReAding Comprehension dataset from Examinations (RACE) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.
Source: Dynamic Fusion Networks for Machine Reading Comprehension
Image Source: Lai et al"	https://paperswithcode.com/dataset/race	01/01/2017	ReAding Comprehension dataset from Examinations					
67	QuAC	"Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.
Source: https://paperswithcode.com/paper/quac-question-answering-in-context-1/
Image Source: https://paperswithcode.com/paper/quac-question-answering-in-context-1/"	https://paperswithcode.com/dataset/quac	01/01/2018	Question Answering in Context					
68	Wizard-of-Oz	"The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.
Description from NLP Progress
Image source: Mrkšić et al."	https://paperswithcode.com/dataset/wizard-of-oz	12/06/2016						
69	VCR	"Visual Commonsense Reasoning (VCR) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.
Source: Visual Commonsense R-CNN
Image Source: From Recognition to Cognition: Visual Commonsense Reasoning"	https://paperswithcode.com/dataset/vcr	01/01/2019	Visual Commonsense Reasoning					
70	SWAG	"Given a partial description like ""she opened the hood of the car,"" humans can reason about the situation and anticipate what might come next (""then, she examined the engine""). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.
The dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.
Source: SWAG
Image Source: Zellers et al"	https://paperswithcode.com/dataset/swag	16/08/2018	Situations With Adversarial Generations					
71	Event2Mind	"Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations.
Source: Event2Mind: Commonsense Inference on Events, Intents, and Reactions"	https://paperswithcode.com/dataset/event2mind							
72	XNLI	"The Cross-lingual Natural Language Inference (XNLI) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.
Source: CamemBERT: a Tasty French Language Model
Image Source: https://github.com/facebookresearch/XNLI"	https://paperswithcode.com/dataset/xnli	01/01/2018	Cross-lingual Natural Language Inference					
73	SNLI	"The SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.
Source: Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences"	https://paperswithcode.com/dataset/snli	01/01/2015	Stanford Natural Language Inference					
74	SciTail	"The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label.
Source: Allen Institute for AI
Image source: Allen Institute for AI"	https://paperswithcode.com/dataset/scitail							
75	CHB-MIT	"The CHB-MIT dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings.
Source: Learning Robust Features using Deep Learning for Automatic Seizure Detection
Image Source: https://archive.physionet.org/pn6/chbmit/"	https://paperswithcode.com/dataset/chb-mit		CHB-MIT Scalp EEG					
76	2010 i2b2/VA	2010 i2b2/VA is a biomedical dataset.	https://paperswithcode.com/dataset/2010-i2b2-va		2010 i2b2/VA					
77	AFW	"AFW (Annotated Faces in the Wild) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.
Source: Face detection, pose estimation, and landmark localization in the wild"	https://paperswithcode.com/dataset/afw	01/01/2012	Annotated Faces in the Wild					
78	TempEval-3	"Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems – in each task and in general.
We present TempEval-3 Silver data, with 666K words, and TempEval-3 Platinum, an evaluation set with 6K words. Documents are annotation with EVENT and TIMEX3 spans and also TLINKs, following the TimeML standard."	https://paperswithcode.com/dataset/tempeval-3	22/06/2012	TempEval-3: events, times, and temporal relations					
79	TimeBank	"Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. 
Source: Enriching TimeBank: Towards a more precise annotation of temporal relations in a text"	https://paperswithcode.com/dataset/timebank							
80	SemEval-2010 Task 8	The dataset for the SemEval-2010 Task 8 is a dataset for multi-way classification of mutually exclusive semantic relations between pairs of nominals.	https://paperswithcode.com/dataset/semeval-2010-task-8	23/11/2019						
81	IEMOCAP	"Multimodal Emotion Recognition IEMOCAP The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.
Source: Multi-attention Recurrent Network for Human Communication Comprehension
Image Source: https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf"	https://paperswithcode.com/dataset/iemocap	01/01/2008	The Interactive Emotional Dyadic Motion Capture (IEMOCAP) Database					
82	Charades-STA	"Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.
Source: TALL: Temporal Activity Localization via Language Query"	https://paperswithcode.com/dataset/charades-sta							
83	SentEval	"SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.
Source: SentEval: An Evaluation Toolkit for Universal Sentence Representations"	https://paperswithcode.com/dataset/senteval							
84	JFLEG	"JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. 
Source: JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction"	https://paperswithcode.com/dataset/jfleg		JHU FLuency-Extended GUG corpus					
85	ESC-50	"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.
Source: The NIGENS General Sound Events Database
Image Source: https://github.com/karolpiczak/ESC-50"	https://paperswithcode.com/dataset/esc-50	01/01/2015	ESC-50					
86	Quora Question Pairs	"Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.
Source: Bilateral Multi-Perspective Matching for Natural Language Sentences"	https://paperswithcode.com/dataset/quora-question-pairs							
87	MMI	"The MMI Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters.
Source: https://mmifacedb.eu/
Image Source: https://mmifacedb.eu/"	https://paperswithcode.com/dataset/mmi	01/01/2005	MMI Facial Expression Database					
88	JAFFE	"The JAFFE dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.
Source: Balanced k-Means and Min-Cut Clustering
Image Source: https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190"	https://paperswithcode.com/dataset/jaffe	01/01/1998	Japanese Female Facial Expression					
89	Oulu-CASIA	"The Oulu-CASIA NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 × 240 pixels.
Source: Facial expression recognition from near-infrared videos
Image Source: https://arxiv.org/abs/1712.03474"	https://paperswithcode.com/dataset/oulu-casia	01/01/2011	Oulu-CASIA NIR&VIS facial expression database					
90	SFEW	"The Static Facial Expressions in the Wild (SFEW) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer.
Source: Deep Facial Expression Recognition: A Survey
Image Source: https://computervisiononline.com/dataset/1105138659"	https://paperswithcode.com/dataset/sfew	01/01/2011	Static Facial Expression in the Wild					
91	ATIS	"The ATIS (Airline Travel Information Systems) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.
Source: Spoken Language Intent Detection using Confusion2Vec"	https://paperswithcode.com/dataset/atis	01/01/1990	Airline Travel Information Systems					
92	ActivityNet	"The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.
Source: Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection"	https://paperswithcode.com/dataset/activitynet	01/01/2015						
93	MSRA-TD500	"The MSRA-TD500 dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text.
Source: Detecting Text in the Wild with Deep Character Embedding Network
Image Source: http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg"	https://paperswithcode.com/dataset/msra-td500	01/01/2012	MSRA Text Detection 500 Database					
94	ICDAR 2015	ICDAR 2015 was a scene text detection used for the ICDAR 2015 conference.	https://paperswithcode.com/dataset/icdar-2015							
95	Total-Text	"Total-Text is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively.
Source: Convolutional Character Networks
Image Source: https://github.com/cs-chan/Total-Text-Dataset"	https://paperswithcode.com/dataset/total-text	01/01/2017						
96	DOTA	"DOTA is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. The images are collected from different sensors and platforms. Each image is of the size in the range from 800 × 800 to 20,000 × 20,000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. The instances in DOTA images are annotated by experts in aerial image interpretation by arbitrary (8 d.o.f.) quadrilateral. We will continue to update DOTA, to grow in size and scope to reflect evolving real-world conditions. Now it has three versions:
DOTA-v1.0 contains 15 common categories, 2,806 images and 188, 282 instances. The proportions of the training set, validation set, and testing set in DOTA-v1.0 are 1/2, 1/6, and 1/3, respectively.
DOTA-v1.5 uses the same images as DOTA-v1.0, but the extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, ”container crane” is added. It contains 403,318 instances in total. The number of images and dataset splits are the same as DOTA-v1.0. This version was released for the DOAI Challenge 2019 on Object Detection in Aerial Images in conjunction with IEEE CVPR 2019.
DOTA-v2.0 collects more Google Earth, GF-2 Satellite, and aerial images. There are 18 common categories, 11,268 images and 1,793,658 instances in DOTA-v2.0. Compared to DOTA-v1.5, it further adds the new categories of ”airport” and ”helipad”. The 11,268 images of DOTA are split into training, validation, test-dev, and test-challenge sets. To avoid the problem of overfitting, the proportion of training and validation set is smaller than the test set. Furthermore, we have two test sets, namely test-dev and test-challenge. Training contains 1,830 images and 268,627 instances. Validation contains 593 images and 81,048 instances. We released the images and ground truths for training and validation sets. Test-dev contains 2,792 images and 353,346 instances. We released the images but not the ground truths. Test-challenge contains 6,053 images and 1,090,637 instances.
Source: https://captain-whu.github.io/DOTA/index.html
Image Source: https://captain-whu.github.io/DOTA/"	https://paperswithcode.com/dataset/dota	01/01/2018	Dataset for Object deTection in Aerial Images					
97	HRSC2016	High-resolution ship collections 2016 (HRSC2016) is a data set used for scientific research. Currently, all of the images in HRSC2016 were collected from Google Earth.	https://paperswithcode.com/dataset/hrsc2016		High resolution ship collections 2016					
98	ShanghaiTech	"The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.
Source: Iterative Crowd Counting
Image Source: Li et al"	https://paperswithcode.com/dataset/shanghaitech	01/01/2016						
99	UCSD	"The UCSD Anomaly Detection Dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either:
the circulation of non pedestrian entities in the walkways
anomalous pedestrian motion patterns
Commonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. A few instances of people in wheelchair were also recorded. All abnormalities are naturally occurring, i.e. they were not staged for the purposes of assembling the dataset. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames.
Source: The UCSD Anomaly Detection Dataset
Image Source: http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf"	https://paperswithcode.com/dataset/ucsd	01/01/2010	UCSD Anomaly Detection Dataset					
100	DCASE 2017	"The DCASE 2017 rare sound events dataset contains isolated sound events for three classes: 148 crying babies (mean duration 2.25s), 139 glasses breaking (mean duration 1.16s), and 187 gun shots (mean duration 1.32s). As with the DCASE 2016 data, silences are not excluded from active event markings in the annotations. While this data set contains many samples per class, there are only three classes
Source: The NIGENS General Sound Events Database
Image Source: https://arxiv.org/pdf/1911.06878.pdf"	https://paperswithcode.com/dataset/dcase-2017		DCASE 2017					
101	PANDORA	"PANDORA is the first large-scale dataset of Reddit comments labeled with three personality models (including the well-established Big 5 model) and demographics (age, gender, and location) for more than 10k users.
Source: PANDORA Talks: Personality and Demographics on Reddit"	https://paperswithcode.com/dataset/pandora	09/04/2020						
102	AVA	"AVA is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:

Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. 
Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. 
Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.
Image Source: https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png"	https://paperswithcode.com/dataset/ava	01/01/2018	Atomic Visual Actions					
103	EPIC-KITCHENS-55	"The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. ‘open fridge‘). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits.
Source: Egocentric Hand Track and Object-based Human Action Recognition
Image Source: https://epic-kitchens.github.io/2020-100"	https://paperswithcode.com/dataset/epic-kitchens	01/01/2018						
104	Charades	"The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.
Source: Temporal Reasoning Graph for Activity Recognition"	https://paperswithcode.com/dataset/charades	01/01/2016						
105	OTB-2015	"OTB-2015, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.
Image Source: http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html"	https://paperswithcode.com/dataset/otb-2015	01/01/2015						
106	OTB-2013	"OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark.
Source: Marvasti-Zadeh"	https://paperswithcode.com/dataset/otb-2013	01/01/2013						
107	LaSOT	"LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated
tracking benchmark. The average video length of LaSOT
is more than 2,500 frames, and each sequence comprises
various challenges deriving from the wild where target objects may disappear and re-appear again in the view."	https://paperswithcode.com/dataset/lasot		Large-scale Single Object Tracking					
108	TrackingNet	"TrackingNet is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.
Source: Learning the Model Update for Siamese Trackers
Image Source: https://arxiv.org/abs/1803.10794"	https://paperswithcode.com/dataset/trackingnet	01/01/2018						
109	VOT2018	"VOT2018 is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.
Source: Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How
Image Source: https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770"	https://paperswithcode.com/dataset/vot2018	01/01/2018	VOT2018					
110	VOT2017	"VOT2017 is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.
Source: GradNet: Gradient-Guided Network for Visual Object Tracking
Image Source: https://ieeexplore.ieee.org/document/8265440"	https://paperswithcode.com/dataset/vot2017	01/01/2017	Visual Object Tracking Challenge					
111	AG News	"AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.
Source: https://arxiv.org/pdf/1509.01626.pdf"	https://paperswithcode.com/dataset/ag-news	01/01/2015	AG’s News Corpus					
112	DBpedia	"DBpedia (from ""DB"" for ""database"") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.
Source: https://en.wikipedia.org/wiki/DBpedia"	https://paperswithcode.com/dataset/dbpedia	01/01/2007	DBpedia					
113	SST	"The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews. It was parsed with the Stanford
parser and includes a total of 215,154 unique phrases
from those parse trees, each annotated by 3 human judges.
Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive.
The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary."	https://paperswithcode.com/dataset/sst	01/10/2013	Stanford Sentiment Treebank					
114	SUBJ	"Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., ""two and a half stars"") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.
Source: SUBJ"	https://paperswithcode.com/dataset/subj		Subjectivity dataset					
115	BDD100K	Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page.	https://paperswithcode.com/dataset/bdd100k	08/04/2020						
116	GTA5	"The GTA5 dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.
Source: Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation
Image Source: Richter et al"	https://paperswithcode.com/dataset/gta5	01/01/2016	Grand Theft Auto 5					
117	MovieLens	"The MovieLens datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 — a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.
Source: The MovieLens Datasets: History and Context
Image Source: http://files.grouplens.org/papers/harper-tiis2015.pdf"	https://paperswithcode.com/dataset/movielens	01/01/2016	MovieLens					
118	Middlebury	"The Middlebury Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.
Source: https://vision.middlebury.edu/stereo/data/
Image Source: https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625"	https://paperswithcode.com/dataset/middlebury	01/01/2002	Middlebury Stereo					
119	V-COCO	"Verbs in COCO (V-COCO) is a dataset that builds off COCO for human-object interaction detection. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.
Source: Visual Compositional Learning for Human-Object Interaction Detection
Image Source: https://www.researchgate.net/figure/Pose-estimation-and-action-recognition-results-on-the-V-COCO-Dataset-16-which-has_fig9_339477856"	https://paperswithcode.com/dataset/v-coco	01/01/2015	Verbs in COCO					
120	HICO-DET	"HICO-DET is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.
Source: Visual Compositional Learning for Human-Object Interaction Detection
Image Source: http://www-personal.umich.edu/~ywchao/hico/"	https://paperswithcode.com/dataset/hico-det	01/01/2018	HICO-DET					
121	UTD-MHAD	"The UTD-MHAD dataset consists of 27 different actions performed by 8 subjects. Each subject repeated the action for 4 times, resulting in 861 action sequences in total. The RGB, depth, skeleton and the inertial sensor signals were recorded.
Source: Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition
Image Source: https://www.researchgate.net/figure/Sample-shots-of-the-27-actions-in-the-UTD-MHAD-database_fig12_283090976"	https://paperswithcode.com/dataset/utd-mhad	01/01/2015						
122	MPII	"The MPII Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.
Source: 2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning
Image Source: http://human-pose.mpi-inf.mpg.de/"	https://paperswithcode.com/dataset/mpii	01/01/2014	MPII Human Pose					
123	Kinetics	"The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.
Source: Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey"	https://paperswithcode.com/dataset/kinetics	19/05/2017	Kinetics Human Action Video Dataset					
124	MSRC-12	"The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. The data set includes 594 sequences and 719,359 frames—approximately six hours and 40 minutes—collected from 30 people performing 12 gestures. In total, there are 6,244 gesture instances. The motion files contain tracks of 20 joints estimated using the Kinect Pose Estimation pipeline. The body poses are captured at a sample rate of 30Hz with an accuracy of about two centimeters in joint positions.
Source: https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/"	https://paperswithcode.com/dataset/msrc-12	01/01/2012	MSRC-12 Kinect Gesture Dataset					
125	TIMIT	"The TIMIT Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech.
Source: Improving neural networks by preventing co-adaptation of feature detectors
Image Source: https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/"	https://paperswithcode.com/dataset/timit		TIMIT Acoustic-Phonetic Continuous Speech Corpus					
126	Volleyball	"Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.
Source: https://github.com/mostafa-saad/deep-activity-rec#dataset
Image Source: https://github.com/mostafa-saad/deep-activity-rec#dataset"	https://paperswithcode.com/dataset/volleyball	01/01/2016						
127	Collective Activity	"The Collective Activity Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.
Source: http://vhosts.eecs.umich.edu/vision//activity-dataset.html
Image Source: http://vhosts.eecs.umich.edu/vision//activity-dataset.html"	https://paperswithcode.com/dataset/collective-activity	01/01/2009						
128	MOT16	"The MOT16 dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.
Source: SOT for MOT
Image Source: https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746"	https://paperswithcode.com/dataset/mot16	02/03/2016	Multiple Object Tracking 2016					
129	NUS-WIDE	"The NUS-WIDE dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes.
Source: Parallel Grid Pooling for Data Augmentation
Image Source: Li et al"	https://paperswithcode.com/dataset/nus-wide	01/01/2009						
130	PASCAL VOC 2007	"PASCAL VOC 2007 is a dataset for image recognition. The twenty object classes that have been selected are:
Person: person
Animal: bird, cat, cow, dog, horse, sheep
Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train
Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor
The dataset can be used for image classification and object detection tasks.
Image Source: Object Detection and Recognition in Images"	https://paperswithcode.com/dataset/pascal-voc-2007		PASCAL VOC 2007					
131	CelebA-HQ	"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.
Source: IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis
Image Source: https://github.com/tkarras/progressive_growing_of_gans"	https://paperswithcode.com/dataset/celeba-hq	27/10/2017	CelebA-HQ					
132	GTEA	"The Georgia Tech Egocentric Activities (GTEA) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.
Source: TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation
Image Source: http://cbs.ic.gatech.edu/fpv/"	https://paperswithcode.com/dataset/gtea	01/01/2011	Georgia Tech Egocentric Activity					
133	50 Salads	"Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures – characterized by interactions between hands, tools, and manipulable objects – frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.
The dataset includes
RGB video data 640×480 pixels at 30 Hz
Depth maps 640×480 pixels at 30 Hz
3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.
Synchronization parameters for temporal alignment of video and accelerometer data
Annotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe"	https://paperswithcode.com/dataset/50-salads	08/09/2013						
134	DIV2K	"DIV2K is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild ×4 and realistic wild ×4 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.
Source: Unsupervised Image Super-Resolution with an Indirect Supervised Path"	https://paperswithcode.com/dataset/div2k	01/01/2017						
135	MAESTRO	"The MAESTRO dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).
Source: https://magenta.tensorflow.org/datasets/maestro
Image Source: https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458"	https://paperswithcode.com/dataset/maestro	01/01/2019	MAESTRO					
136	CASIA-B	"CASIA-B is a large multiview gait database, which is created in January 2005. There are 124 subjects, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered. Besides the video files, we still provide human silhouettes extracted from video files. The detailed information about Dataset B and an evaluation framework can be found in this paper .
The format of the video filename in Dataset B is 'xxx-mm-nn-ttt.avi', where
xxx: subject id, from 001 to 124.
mm: walking status, can be 'nm' (normal), 'cl' (in a coat) or 'bg' (with a bag).
nn: sequence number.
ttt: view angle, can be '000', '018', ..., '180'."	https://paperswithcode.com/dataset/casia-b		CASIA-B					
137	AFLW	"The Annotated Facial Landmarks in the Wild (AFLW) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.
Source: Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints"	https://paperswithcode.com/dataset/aflw	01/01/2011	Annotated Facial Landmarks in the Wild					
138	BIWI	"The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation.
Source: https://www.kaggle.com/kmader/biwi-kinect-head-pose-database
Image Source: https://icu.ee.ethz.ch/research/datsets.html"	https://paperswithcode.com/dataset/biwi	01/01/2011						
139	STB	"3D hand pose data set created using stereo camera

contains 18,000 RGB images and paired depth images
3D positions of hand joints (21 joints)"	https://paperswithcode.com/dataset/stb	08/10/2016	Stereo Hand Pose Benchmark					
140	YCB-Video	"The YCB-Video dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.
Source: https://rse-lab.cs.washington.edu/projects/posecnn/
Image Source: https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565"	https://paperswithcode.com/dataset/ycb-video	01/01/2018						
141	ApolloCar3D	"ApolloCar3DT is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. 
Source: ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving
Image Source: http://apolloscape.auto/car_instance.html"	https://paperswithcode.com/dataset/apollocar3d							
142	Multi-Ego	"A new multi-view egocentric dataset, Multi-Ego. The dataset is recorded simultaneously by three cameras, covering a wide variety of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth.
Source: Multi-Stream Dynamic Video Summarization"	https://paperswithcode.com/dataset/multi-ego							
143	SumMe	"The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).
Source: https://gyglim.github.io/me/vsum/index.html
Image Source: https://gyglim.github.io/me/vsum/index.html"	https://paperswithcode.com/dataset/summe	01/01/2014	SumMe					
144	Reddit TIFU	"Reddit TIFU dataset is a newly collected Reddit dataset, where TIFU denotes the name of /r/tifu subbreddit.
There are 122,933 text-summary pairs in total.
Source: https://github.com/ctr4si/MMN"	https://paperswithcode.com/dataset/reddit-tifu	02/11/2018						
145	DAVIS 2016	"DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.
Source: Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation
Image Source: https://davischallenge.org/"	https://paperswithcode.com/dataset/davis-2016	01/01/2016	DAVIS 2016					
146	FBMS-59	"The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets.
Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html
Image Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html"	https://paperswithcode.com/dataset/fbms-59	01/01/2010	Freiburg-Berkeley Motion Segmentation					
147	CamVid	"CamVid (Cambridge-driving Labeled Video Database) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object
Source: A Review on Deep Learning TechniquesApplied to Semantic Segmentation
Image Source: http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/"	https://paperswithcode.com/dataset/camvid	01/01/2009	Cambridge-driving Labeled Video Database					
148	DAVIS 2017	"DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing
Source: Siam R-CNN: Visual Tracking by Re-Detection
Image Source: https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902"	https://paperswithcode.com/dataset/davis-2017	01/01/2017	DAVIS 2017					
149	CCGbank	"CCGbank is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar derivations. It pairs syntactic derivations with sets of word-word dependencies which approximate the underlying predicate-argument structure.
The dataset contains 99.44% of the sentences in the Penn Treebank, for which it corrects a number of inconsistencies and errors in the original annotation.
Source: CCGbank"	https://paperswithcode.com/dataset/ccgbank	15/05/2005						
150	SVHN	"Street View House Numbers (SVHN) is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.
Source: Competitive Multi-scale Convolution
Image Source: http://ufldl.stanford.edu/housenumbers/"	https://paperswithcode.com/dataset/svhn		Street View House Numbers					
151	STL-10	"The STL-10 is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.
Source: Unsupervised Feature Learning with C-SVDDNet
Image Source: https://cs.stanford.edu/~acoates/stl10/"	https://paperswithcode.com/dataset/stl-10	01/01/2011	Self-Taught Learning 10					
152	CIFAR-10	"The CIFAR-10 dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class.
The criteria for deciding whether an image belongs to a class were as follows:

The class name should be high on the list of likely answers to the question “What is in this picture?”
The image should be photo-realistic. Labelers were instructed to reject line drawings.
The image should contain only one prominent instance of the object to which the class refers.
The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.

Source: https://www.cs.toronto.edu/~kriz/cifar.html
Image Source: https://www.cs.toronto.edu/~kriz/cifar.html"	https://paperswithcode.com/dataset/cifar-10							
153	Clothing1M	"Clothing1M contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively.
Source: Label-Noise Robust Generative Adversarial Networks
Image Source: https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf"	https://paperswithcode.com/dataset/clothing1m	01/01/2015						
154	CIFAR-100	"The CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.
The criteria for deciding whether an image belongs to a class were as follows:

The class name should be high on the list of likely answers to the question “What is in this picture?”
The image should be photo-realistic. Labelers were instructed to reject line drawings.
The image should contain only one prominent instance of the object to which the class refers.
The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.

Source: https://www.cs.toronto.edu/~kriz/cifar.html
Image Source: https://www.cs.toronto.edu/~kriz/cifar.html"	https://paperswithcode.com/dataset/cifar-100							
155	ADE20K	"The ADE20K semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.
Source: Cooperative Image Segmentation and Restoration in Adverse Environmental Conditions
Image Source: https://groups.csail.mit.edu/vision/datasets/ADE20K/"	https://paperswithcode.com/dataset/ade20k	01/01/2017						
156	MPII Human Pose	"MPII Human Pose Dataset is a dataset for human pose estimation. It consists of around 25k images extracted from online videos. Each image contains one or more people, with over 40k people annotated in total. Among the 40k samples, ∼28k samples are for training and the remainder are for testing. Overall the dataset covers 410 human activities and each image is provided with an activity label. Images were extracted from a YouTube video and provided with preceding and following un-annotated frames.
Source: Accelerating Large-Kernel Convolution Using Summed-Area Tables
Image Source: http://human-pose.mpi-inf.mpg.de/"	https://paperswithcode.com/dataset/mpii-human-pose	01/01/2014	MPII Human Pose					
157	Human3.6M	"The Human3.6M dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.
Source: Space-Time Representation of People Based on 3D Skeletal Data: A Review
Image Source: Yu et al"	https://paperswithcode.com/dataset/human3-6m	01/01/2014						
158	CIHP	"The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. The dataset can be used for the human part segmentation task.
Source: Parsing R-CNN for Instance-Level Human Analysis
Image Source: https://arxiv.org/abs/1808.00157"	https://paperswithcode.com/dataset/cihp	01/01/2018	Crowd Instance-level Human Parsing					
159	MultiMNIST	"The MultiMNIST dataset is generated from MNIST. The training and tests are generated by overlaying a digit on top of another digit from the same set (training or test) but different class. Each digit is shifted up to 4 pixels in each direction resulting in a 36×36 image. Considering a digit in a 28×28 image is bounded in a 20×20 box, two digits bounding boxes on average have 80% overlap. For each digit in the MNIST dataset 1,000 MultiMNIST examples are generated, so the training set size is 60M and the test set size is 10M.
Source: https://arxiv.org/pdf/1710.09829.pdf
Image Source: Sabour et al"	https://paperswithcode.com/dataset/multimnist	01/01/2017						
160	iNaturalist	"The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.
Source: Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning
Image Source: https://github.com/visipedia/inat_comp/tree/master/2017"	https://paperswithcode.com/dataset/inaturalist	01/01/2018						
161	ScanNet	"ScanNet is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.
Source: A Review of Point Cloud Semantic Segmentation
Image Source: http://www.scan-net.org/"	https://paperswithcode.com/dataset/scannet	01/01/2017						
162	SBD	"The Semantic Boundaries Dataset (SBD) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.
Source: Weakly Supervised Object Boundaries
Image Source: http://home.bharathh.info/pubs/codes/SBD/download.html"	https://paperswithcode.com/dataset/sbd	01/01/2011	Semantic Boundaries Dataset					
163	SK-LARGE	"SK-LARGE is a benchmark dataset for object skeleton detection, built on the MS COCO dataset. It contains 1491 images, 746 for training and 745 for testing.
Source: DeepFlux for Skeletons in the Wild
Image Source: http://kaizhao.net/sk-large"	https://paperswithcode.com/dataset/sk-large	01/01/2017	SK-LARGE					
164	Indian Pines	"Indian Pines is a Hyperspectral image segmentation dataset. The input data consists of hyperspectral bands over a single landscape in Indiana, US, (Indian Pines data set) with 145×145 pixels. For each pixel, the data set contains 220 spectral reflectance bands which represent different portions of the electromagnetic spectrum in the wavelength range 0.4−2.5⋅10−6.
Source: Layer-Parallel Training of Deep Residual Neural Networks
Image Source: http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines"	https://paperswithcode.com/dataset/indian-pines							
165	Pavia University	"The Pavia University dataset is a hyperspectral image dataset which gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy. The image consists of 610×340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42,776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow.
Source: Diversity in Machine Learning
Image Source: http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University"	https://paperswithcode.com/dataset/pavia-university		Pavia University					
166	RVL-CDIP	"The RVL-CDIP dataset consists of scanned document images belonging to 16 classes such as letter, form, email, resume, memo, etc. The dataset has 320,000 training, 40,000 validation and 40,000 test images. The images are characterized by low quality, noise, and low resolution, typically 100 dpi.
Source: Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning
Image Source: https://www.cs.cmu.edu/~aharley/rvl-cdip/"	https://paperswithcode.com/dataset/rvl-cdip	01/01/2015	RVL-CDIP					
167	COCO Captions	"COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.
Source: Microsoft COCO Captions: Data Collection and Evaluation Server"	https://paperswithcode.com/dataset/coco-captions							
168	RotoWire	"This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the ""rotowire"" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.
Source: Challenges in Data-to-Document Generation
Image Source: https://arxiv.org/pdf/1707.08052v1.pdf"	https://paperswithcode.com/dataset/rotowire	01/01/2017	RotoWire					
169	WikiBio	"WikiBio is a dataset introduced for generating biography notes based on information found in an infobox – a fact table describing a person. Each sample in the dataset contains the infobox and the first sentence of each biography article as reference. On average, each reference sentence has 26.1 words. The corpus contains 728,321 instances, which has been divided into three sub-parts to provide 582,659 for training, 72,831 for validation and 72,831 for testing.
Source: Table-to-Text: Describing Table Region with Natural Language
Image Source: https://arxiv.org/pdf/1603.07771.pdf"	https://paperswithcode.com/dataset/wikibio	01/01/2016						
170	DailyDialog	"DailyDialog is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.
Source: http://yanran.li/dailydialog
Image Source: https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/"	https://paperswithcode.com/dataset/dailydialog	01/01/2017						
171	WebNLG	"The WebNLG corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.
Initially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.
The corpus is also used for a reverse task of triplets extraction.
Versioning history of the dataset can be found here.
Source: Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation
Image Source: https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/"	https://paperswithcode.com/dataset/webnlg	01/01/2017						
172	MegaFace	"MegaFace was a publicly available dataset which is used for evaluating the performance of face recognition algorithms with up to a million distractors (i.e., up to a million people who are not in the test set). MegaFace contains 1M images from 690K individuals with unconstrained pose, expression, lighting, and exposure. MegaFace captures many different subjects rather than many images of a small number of subjects. The gallery set of MegaFace is collected from a subset of Flickr. The probe set of MegaFace used in the challenge consists of two databases; Facescrub and FGNet. FGNet contains 975 images of 82 individuals, each with several images spanning ages from 0 to 69. Facescrub dataset contains more than 100K face images of 530 people. The MegaFace challenge evaluates performance of face recognition algorithms by increasing the numbers of “distractors” (going from 10 to 1M) in the gallery set. In order to evaluate the face recognition algorithms fairly, MegaFace challenge has two protocols including large or small training sets. If a training set has more than 0.5M images and 20K subjects, it is considered as large. Otherwise, it is considered as small.
NOTE: This dataset has been retired. 
Source: A Deep Face Identification Network Enhanced by Facial Attributes Prediction"	https://paperswithcode.com/dataset/megaface	01/01/2016						
173	IJB-B	"The IJB-B dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.
Source: An Automatic System for Unconstrained Video-Based Face Recognition
Image Source: https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf"	https://paperswithcode.com/dataset/ijb-b	01/01/2017	IARPA Janus Benchmark-B					
174	IJB-A	"The IARPA Janus Benchmark A (IJB-A) database is developed with the aim to augment more challenges to the face recognition task by collecting facial images with a wide variations in pose, illumination, expression, resolution and occlusion. IJB-A is constructed by collecting 5,712 images and 2,085 videos from 500 identities, with an average of 11.4 images and 4.2 videos per identity.
Source: von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification
Image Source: Ruan et al"	https://paperswithcode.com/dataset/ijb-a	01/01/2015	IARPA Janus Benchmark A					
175	300W	"The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as “party”, “conference”, “protests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common “neutral” or “smile”, such as “surprise” or “scream”.
Images were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.
Many images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.
Source: https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf
Image Source: https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg"	https://paperswithcode.com/dataset/300w	01/01/2013	300 Faces-In-The-Wild					
176	FG-NET	"FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years
Source: Large age-gap face verification by feature injection in deep networks
Image Source: https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621"	https://paperswithcode.com/dataset/fg-net	01/01/2002						
177	IJB-C	"The IJB-C dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images.
Source: Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results
Image Source: https://noblis.org/wp-content/uploads/2018/03/icb2018.pdf"	https://paperswithcode.com/dataset/ijb-c	01/01/2018	IARPA Janus Benchmark-C					
178	PASCAL Face	"The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.
Source: Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results
Image Source: https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926"	https://paperswithcode.com/dataset/pascal-face	01/01/2014	PASCAL Face					
179	AFLW2000-3D	"AFLW2000-3D is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.
Source: https://www.tensorflow.org/datasets/catalog/aflw2k3d
Image Source: http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm"	https://paperswithcode.com/dataset/aflw2000-3d	01/01/2016						
180	Florence	"The Florence 3D faces dataset consists of:

High-resolution 3D scans of human faces from many subjects.
Several video sequences of varying resolution, conditions and zoom level for each subject.
Each subject is recorded in the following situations:
In a controlled setting in HD video.
In a less-constrained (but still indoor) setting using a standard, PTZ surveillance camera.
In an unconstrained, outdoor environment under challenging recording conditions.

Source: https://www.micc.unifi.it/resources/datasets/florence-3d-faces/
Image Source: https://www.micc.unifi.it/resources/datasets/florence-3d-faces/"	https://paperswithcode.com/dataset/florence	01/01/2011	Florence 3D Faces					
181	MORPH	"MORPH is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.
Source: Deep Ordinal Regression Forests
Image Source: https://uncw.edu/oic/tech/morph.html"	https://paperswithcode.com/dataset/morph	01/01/2006						
182	CUFS	"CUHK Face Sketch database (CUFS) is for research on face sketch synthesis and face sketch recognition. It includes 188 faces from the Chinese University of Hong Kong (CUHK) student database, 123 faces from the AR database [1], and 295 faces from the XM2VTS database [2]. There are 606 faces in total. For each face, there is a sketch drawn by an artist based on a photo taken in a frontal pose, under normal lighting condition, and with a neutral expression.
[1] A. M. Martinez, and R. Benavente, “The AR Face Database,” CVC Technical Report #24, June 1998.
[2] K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre, “XM2VTSDB: the Extended of M2VTS Database,” in Proceedings of International Conference on Audio- and Video-Based Person Authentication, pp. 72-77, 1999.
Source Paper: X. Wang and X. Tang, “Face Photo-Sketch Synthesis and Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Vol. 31, 2009
Image Source: CUHK Face Sketch Database (CUFS)
Source: CUHK Face Sketch Database (CUFS)"	https://paperswithcode.com/dataset/cufs	01/11/2009	CUHK Face Sketch Database					
183	CUFSF	"The CUHK Face Sketch FERET (CUFSF) is a dataset for research on face sketch synthesis and face sketch recognition. It contains two types of face images: photo and sketch. Total 1,194 images (one image per subject) were collected with lighting variations from the FERET dataset. For each subject, a sketch is drawn with shape exaggeration.
Source: Deeply Coupled Auto-encoder Networks forCross-view Classification
Image Source: http://mmlab.ie.cuhk.edu.hk/archive/cufsf/"	https://paperswithcode.com/dataset/cufsf	01/01/2011	CUHK Face Sketch FERET Database					
184	Caltech-101	"The Caltech101 dataset contains images from 101 object categories (e.g., “helicopter”, “elephant” and “chair” etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300×200 pixels.
Source: Simple and Efficient Learning using Privileged Information"	https://paperswithcode.com/dataset/caltech-101	01/01/2004						
185	Oxford-IIIT Pets	"The Oxford-IIIT Pet Dataset has 37 categories with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation.
Source: https://www.robots.ox.ac.uk/~vgg/data/pets/
Image Source: https://www.robots.ox.ac.uk/~vgg/data/pets/"	https://paperswithcode.com/dataset/oxford-iiit-pets	01/01/2012						
186	Stanford Cars	"The Stanford Cars dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360×240.
Source: View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network
Image Source: https://ai.stanford.edu/~jkrause/cars/car_dataset.html"	https://paperswithcode.com/dataset/stanford-cars	01/01/2013						
187	NABirds	"NABirds V1 is a collection of 48,000 annotated photographs of the 400 species of birds that are commonly observed in North America. More than 100 photographs are available for each species, including separate annotations for males, females and juveniles that comprise 700 visual categories. This dataset is to be used for fine-grained visual categorization experiments.
Source: https://dl.allaboutbirds.org/nabirds
Image Source: https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf"	https://paperswithcode.com/dataset/nabirds	01/01/2015	North America Birds					
188	Stanford Dogs	"The Stanford Dogs dataset contains 20,580 images of 120 classes of dogs from around the world, which are divided into 12,000 images for training and 8,580 images for testing.
Source: Universal-to-Specific Framework for Complex Action Recognition
Image Source: https://www.tensorflow.org/datasets/catalog/stanford_dogs"	https://paperswithcode.com/dataset/stanford-dogs		Stanford Dogs					
189	FFHQ	"Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.
Source: Flickr-Faces-HQ Dataset (FFHQ)
Image Source: https://github.com/NVlabs/ffhq-dataset"	https://paperswithcode.com/dataset/ffhq	01/01/2019	Flickr-Faces-HQ					
190	RaFD	"The Radboud Faces Database (RaFD) is a set of pictures of 67 models (both adult and children, males and females) displaying 8 emotional expressions.
Source: http://www.socsci.ru.nl:8180/RaFD2/RaFD
Image Source: http://www.socsci.ru.nl:8180/RaFD2/RaFD"	https://paperswithcode.com/dataset/rafd		Radboud Faces Database					
191	WikiQA	"The WikiQA corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions.
Source: http://aka.ms/WikiQA
Image Source: Yang et al"	https://paperswithcode.com/dataset/wikiqa	01/01/2015	Wikipedia open-domain Question Answering					
192	WebQuestions	"The WebQuestions dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.
Example questions (answers) in the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).
Source: Question Answering with Subgraph Embeddings
Image Source: Berant et al"	https://paperswithcode.com/dataset/webquestions	01/01/2013						
193	SimpleQuestions	"SimpleQuestions is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.
Source: Hierarchical Memory Networks
Image Source: https://paperswithcode.com/paper/large-scale-simple-question-answering-with/"	https://paperswithcode.com/dataset/simplequestions	01/01/2015						
194	TrecQA	"Text Retrieval Conference Question Answering (TrecQA) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split.
Source: A Gated Self-attention Memory Network for Answer Selection"	https://paperswithcode.com/dataset/trecqa	01/01/2007	Text Retrieval Conference Question Answering					
195	WikiHop	"WikiHop is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents.
The dataset includes a masked variant, where all candidates and their mentions in the supporting documents are replaced by random but consistent placeholder tokens.
Source: Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs
Image Source: http://qangaroo.cs.ucl.ac.uk/"	https://paperswithcode.com/dataset/wikihop	01/01/2017						
196	TuSimple	"The TuSimple dataset consists of 6,408 road images on US highways. The resolution of image is 1280×720. The dataset is composed of 3,626 for training, 358 for validation, and 2,782 for testing called the TuSimple test set of which the images are under different weather conditions.
Source: End-to-End Lane Marker Detection via Row-wise Classification
Image Source: https://www.researchgate.net/figure/a-Example-from-TuSimple-dataset-b-Derived-dataset-for-training-coordinate-network_fig4_330589970"	https://paperswithcode.com/dataset/tusimple							
197	GTSRB	"The German Traffic Sign Recognition Benchmark (GTSRB) contains 43 classes of traffic signs, split into 39,209 training images and 12,630 test images. The images have varying light conditions and rich backgrounds.
Source: Invisible Backdoor Attacks Against Deep Neural Networks
Image Source: https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388"	https://paperswithcode.com/dataset/gtsrb		German Traffic Sign Recognition Benchmark					
198	Tsinghua-Tencent 100K	Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. We call this benchmark Tsinghua-Tencent 100K. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available.	https://paperswithcode.com/dataset/tsinghua-tencent-100k		Traffic-Sign Detection and Classification in the Wild					
199	PA-100K	"PA-100K is a recent-proposed large pedestrian attribute dataset, with 100,000 images in total collected from outdoor surveillance cameras. It is split into 80,000 images for the training set, and 10,000 for the validation set and 10,000 for the test set. This dataset is labeled by 26 binary attributes. The common features existing in both selected dataset is that the images are blurry due to the relatively low resolution and the positive ratio of each binary attribute is low.
Source: Localization Guided Learning for Pedestrian Attribute Recognition
Image Source: https://github.com/xh-liu/HydraPlus-Net"	https://paperswithcode.com/dataset/pa-100k	01/01/2017	PA-100K Dataset					
200	PETA	"The PEdesTrian Attribute dataset (PETA) is a dataset fore recognizing pedestrian attributes, such as gender and clothing style, at a far distance. It is of interest in video surveillance scenarios where face and body close-shots and hardly available. It consists of 19,000 pedestrian images with 65 attributes (61 binary and 4 multi-class). Those images contain 8705 persons.
Source: Attribute Aware Pooling for Pedestrian Attribute Recognition
Image Source: http://mmlab.ie.cuhk.edu.hk/projects/PETA.html"	https://paperswithcode.com/dataset/peta	01/01/2014	Pedestrian Attribute					
201	RAP	"The Richly Annotated Pedestrian (RAP) dataset is a dataset for pedestrian attribute recognition. It contains 41,585 images collected from indoor surveillance cameras. Each image is annotated with 72 attributes, while only 51 binary attributes with the positive ratio above 1% are selected for evaluation. There are 33,268 images for the training set and 8,317 for testing.
Source: Localization Guided Learning for Pedestrian Attribute Recognition
Image Source: http://www.rapdataset.com/rapv1.html"	https://paperswithcode.com/dataset/rap	01/01/2016	Richly Annotated Pedestrian					
202	PhC-U373	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/phc-u373							
203	DRIVE	"The Digital Retinal Images for Vessel Extraction (DRIVE) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). 
The set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.
Source: Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation
Image Source: https://drive.grand-challenge.org/"	https://paperswithcode.com/dataset/drive	01/01/2004	Digital Retinal Images for Vessel Extraction					
204	STARE	"The STARE (Structured Analysis of the Retina) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700×605) color fundus images. For each image, two groups of annotations are provided..
Source: DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels
Image Source: https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756"	https://paperswithcode.com/dataset/stare	01/01/1998	Structured Analysis of the Retina					
205	CHASE_DB1	"CHASE_DB1 is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999×960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.
Source: MixModule: Mixed CNN Kernel Module for Medical Image Segmentation
Image Source: https://www.mdpi.com/2073-8994/9/11/276"	https://paperswithcode.com/dataset/chase-db1	01/01/2012	CHASE_DB1					
206	LUNA	"The LUNA challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified.
Source: Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge"	https://paperswithcode.com/dataset/luna							
207	Tox21	"The Tox21 data set comprises 12,060 training samples and 647 test samples that represent chemical compounds. There are 801 ""dense features"" that represent chemical descriptors, such as molecular weight, solubility or surface area, and 272,776 ""sparse features"" that represent chemical substructures (ECFP10, DFS6, DFS8; stored in Matrix Market Format ). Machine learning methods can either use sparse or dense data or combine them. For each sample there are 12 binary labels that represent the outcome (active/inactive) of 12 different toxicological experiments. Note that the label matrix contains many missing values (NAs). The original data source and Tox21 challenge site is https://tripod.nih.gov/tox21/challenge/.
Source: Tox21 Machine Learning Data Set
Image Source: https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full"	https://paperswithcode.com/dataset/tox21-1		Tox21 Machine Learning Data Set					
208	QM9	"QM9 provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.
Source: QM9 Dataset
Image Source: https://pubs.acs.org/doi/pdf/10.1021/ci300415d"	https://paperswithcode.com/dataset/qm9		Quantum Machines 9					
209	Douban	"We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. 
|      |Train|Val| Test         | 
| ------------- |:-------------:|:-------------:|:-------------:|
| session-response pairs  | 1m|50k| 10k |
| Avg. positive response per session     | 1|1| 1.18    | 
| Fless Kappa | N\A|N\A|0.41      | 
| Min turn per session | 3|3| 3      | 
| Max ture per session | 98|91|45    | 
| Average turn per session | 6.69|6.75|5.95    | 
| Average Word per utterance | 18.56|18.50|20.74   | 
The test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.
<br>
As far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0"	https://paperswithcode.com/dataset/douban	06/12/2016	Douban Conversation Corpus					
210	Criteo	"Criteo contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset.
Source: AMER: Automatic Behavior Modeling and Interaction Exploration in Recommender System
Image Source: https://www.kaggle.com/c/criteo-display-ad-challenge"	https://paperswithcode.com/dataset/criteo		Display Advertising Challenge					
211	iPinYou	"The iPinYou Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs.
Source: iPinYou Global RTB Bidding Algorithm Competition Dataset
Image Source: http://contest.ipinyou.com/ipinyou-dataset.pdf"	https://paperswithcode.com/dataset/ipinyou	01/01/2014	iPinYou Global RTB Bidding Algorithm Competition Dataset					
212	PASCAL-Part	"PASCAL-Part is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object. For categories that do not have a consistent set of parts (e.g., boat), it provides the silhouette annotation. 
It can also serve as a set for human semantic part segmentation: It contains multiple humans per image in unconstrained poses and occlusions (1,716 for training and 1,817 for testing). It provides careful pixel-wise annotations for six body parts (i.e., head, torso, upper/lower-arms, and upper-/lower-legs).
Source: The Ultimate Theory of Human Parsing
Image Source: https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png"	https://paperswithcode.com/dataset/pascal-person-part	01/01/2014	PASCAL-Part					
213	Citeseer	"The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.
Source: https://linqs.soe.ucsc.edu/data"	https://paperswithcode.com/dataset/citeseer	01/01/1998						
214	Cora	"The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.
Source: https://relational.fit.cvut.cz/dataset/CORA
Image Source: https://arxiv.org/abs/1611.08402"	https://paperswithcode.com/dataset/cora	01/01/2000						
215	Pubmed	"The Pubmed dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.
Source: https://linqs.soe.ucsc.edu/data"	https://paperswithcode.com/dataset/pubmed	01/01/2008						
216	NELL	"NELL is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs.
Source: A Survey on Knowledge Graphs: Representation, Acquisition and Applications
Image Source: http://rtw.ml.cmu.edu/rtw/"	https://paperswithcode.com/dataset/nell	01/01/2010	Never Ending Language Learning					
217	BlogCatalog	"BlogCatalog is a graph dataset for a network of social relationships of bloggers listed in the BlogCatalog website. The network has 88,800 nodes and 2.1M edges.
Source: Graph Representation Learning: A Survey
Image Source: https://www.blogcatalog.com/"	https://paperswithcode.com/dataset/blogcatalog	01/01/2015	BlogCatalog					
218	WN18	"The WN18 dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue.
Source: http://nlpprogress.com/english/relation_prediction.html"	https://paperswithcode.com/dataset/wn18	01/01/2013	WordNet18					
219	Scan2CAD	"Scan2CAD is an alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans. The top 3 annotated model classes are chairs, tables and cabinets which arises due to the nature of indoor scenes in ScanNet. The number of objects aligned per scene ranges from 1 to 40 with an average of 9.3.
Additionally, all ShapeNet CAD models used in the Scan2CAD dataset are annotated with their rotational symmetries: either none, 2-fold, 4-fold or infinite rotational symmetries around a canonical axis of the object.
Source: Scan2CAD: Learning CAD Model Alignment in RGB-D Scans
Image Source: Scan2CAD: Learning CAD Model Alignment in RGB-D Scans"	https://paperswithcode.com/dataset/scan2cad	01/01/2019						
220	UTKFace	"The UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.
Source: https://susanqq.github.io/UTKFace/
Image Source: https://susanqq.github.io/UTKFace/"	https://paperswithcode.com/dataset/utkface	01/01/2017						
221	AFAD	The Asian Face Age Dataset (AFAD) is a new dataset proposed for evaluating the performance of age estimation, which contains more than 160K facial images and the corresponding age and gender labels. This dataset is oriented to age estimation on Asian faces, so all the facial images are for Asian faces. It is noted that the AFAD is the biggest dataset for age estimation to date. It is well suited to evaluate how deep learning methods can be adopted for age estimation.	https://paperswithcode.com/dataset/afad	01/06/2016	Asian Face Age Dataset					
222	CACD	"The Cross-Age Celebrity Dataset (CACD) contains 163,446 images from 2,000 celebrities collected from the Internet. The images are collected from search engines using celebrity name and year (2004-2013) as keywords. Therefore, it is possible to estimate the ages of the celebrities on the images by simply subtract the birth year from the year of which the photo was taken.
Source: https://bcsiriuschen.github.io/CARC/
Image Source: https://www.pkuml.org/resources/pku-vehicleid.html"	https://paperswithcode.com/dataset/cacd	01/01/2014	Cross-Age Celebrity Dataset					
223	JIGSAWS	"The JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:

kinematic data: Cartesian positions, orientations, velocities, angular velocities and gripper angle describing the motion of the manipulators.
video data: stereo video captured from the endoscopic camera. Sample videos of the JIGSAWS tasks can be downloaded from the official webpage.
manual annotations including:
gesture (atomic surgical activity segment labels).
skill (global rating score using modified objective structured assessments of technical skills).
experimental setup: a standardized cross-validation experimental setup that can be used to evaluate automatic surgical gesture recognition and skill assessment methods.

Source: https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release
Image Source: https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release"	https://paperswithcode.com/dataset/jigsaws		JHU-ISI Gesture and Skill Assessment Working Set					
224	CompCars	"The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. 
The dataset can be used for the tasks of:

Fine-grained classification
Attribute prediction
Car model verification

The dataset can be also used for other tasks such as image ranking, multi-task learning, and 3D reconstruction."	https://paperswithcode.com/dataset/compcars		Comprehensive Cars					
225	RT-GENE	"Presents a diverse eye-gaze dataset.
Source: RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments"	https://paperswithcode.com/dataset/rt-gene							
226	WN18RR	"WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.
Source: End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion"	https://paperswithcode.com/dataset/wn18rr	01/12/2013						
227	FB15k-237	FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,079 triples with 14,505 entities and 237 relation types.	https://paperswithcode.com/dataset/fb15k-237	01/07/2015						
228	T-LESS	"T-LESS is a dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects.
Source: http://cmp.felk.cvut.cz/t-less/
Image Source: http://cmp.felk.cvut.cz/t-less/"	https://paperswithcode.com/dataset/t-less	01/01/2017						
229	ACE 2004	"ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.
The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.
Source: https://catalog.ldc.upenn.edu/LDC2005T09"	https://paperswithcode.com/dataset/ace-2004		ACE 2004 Multilingual Training Corpus					
230	ACE 2005	"ACE 2005 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC.
Source: https://catalog.ldc.upenn.edu/LDC2006T06
Image Source: https://arxiv.org/pdf/1811.06031.pdf"	https://paperswithcode.com/dataset/ace-2005		ACE 2005 Multilingual Training Corpus					
231	GENIA	"The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.
The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.
The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:

Part-of-Speech annotation
Constituency (phrase structure) syntactic annotation
Term annotation
Event annotation
Relation annotation
Coreference annotation

Source: http://www.geniaproject.org/genia-corpus
Image Source: http://www.geniaproject.org/genia-corpus"	https://paperswithcode.com/dataset/genia	01/01/2003						
232	SemEval 2014 Task 4 Sub Task 2	"Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.
Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).
For example:
“I loved their fajitas” → {fajitas: positive}
“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}
“The fajitas are their first plate” → {fajitas: neutral}
“The fajitas were great to taste, but not to see” → {fajitas: conflict}"	https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2	01/08/2014						
233	Ohsumed	"Ohsumed includes medical abstracts from the MeSH categories of the year 1991. In [Joachims, 1997] were used the first 20,000 documents divided in 10,000 for training and 10,000 for testing. The specific task was to categorize the 23 cardiovascular diseases categories. After selecting the such category subset, the unique abstract number becomes 13,929 (6,286 for training and 7,643 for testing). As current computers can easily manage larger number of documents we make available all 34,389 cardiovascular diseases abstracts out of 50,216 medical abstracts contained in the year 1991.
Source: http://disi.unitn.it/moschitti/corpora.htm"	https://paperswithcode.com/dataset/ohsumed							
234	MR	"MR Movie Reviews is a dataset for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., ""two and a half stars"") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.
Source: http://www.cs.cornell.edu/people/pabo/movie-review-data/
Image Source: https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png"	https://paperswithcode.com/dataset/mr	01/01/2004	MR Movie Reviews					
235	STS Benchmark	"STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums.
Source: STS Benchmark"	https://paperswithcode.com/dataset/sts-benchmark							
236	Weibo NER	"The Weibo NER dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo.
Source: Chinese NER Using Lattice LSTM
Image Source: https://en.wikipedia.org/wiki/Sina_Weibo"	https://paperswithcode.com/dataset/weibo-ner	01/01/2015	Weibo NER					
237	Resume NER	"Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%.
Source: Query-Based Named Entity Recognition
Image Source: https://arxiv.org/pdf/1805.02023.pdf"	https://paperswithcode.com/dataset/resume-ner	01/01/2018	Resume NER					
238	Reuters-21578	"The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.
Source: Topic Model Based Multi-Label Classification from the Crowd"	https://paperswithcode.com/dataset/reuters-21578							
239	FCE	"The Cambridge Learner Corpus First Certificate in English (CLC FCE) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.
Source: Compositional Sequence Labeling Models for Error Detection in Learner Writing"	https://paperswithcode.com/dataset/fce	01/01/2011	First Certificate in English					
240	TACRED	"TACRED is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. These examples are created by combining available human annotations from the TAC KBP challenges and crowdsourcing.
Source: https://nlp.stanford.edu/projects/tacred/"	https://paperswithcode.com/dataset/tacred	01/09/2017	The TAC Relation Extraction Dataset					
241	Natural Questions	"The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.
Source: A BERT Baseline for the Natural Questions
Image Source: https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question/"	https://paperswithcode.com/dataset/natural-questions	01/01/2019						
242	MUTAG	"In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.
Source: Fast and Deep Graph Neural Networks"	https://paperswithcode.com/dataset/mutag							
243	NCI1	"The NCI1 dataset comes from the cheminformatics domain, where each input graph is used as representation of a chemical compound: each vertex stands for an atom of the molecule, and edges between vertices represent bonds between atoms. This dataset is relative to anti-cancer screens where the chemicals are assessed as positive or negative to cell lung cancer. Each vertex has an input label representing the corresponding atom type, encoded by a one-hot-encoding scheme into a vector of 0/1 elements.
Source: Ring Reservoir Neural Networks for Graphs"	https://paperswithcode.com/dataset/nci1	01/01/2006						
244	PROTEINS	"PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.
Source: Fast and Deep Graph Neural Networks"	https://paperswithcode.com/dataset/proteins	01/01/2005						
245	ENZYMES	"ENZYMES is a dataset of 600 protein tertiary structures obtained from the BRENDA enzyme database. The ENZYMES dataset contains 6 enzymes.
Source: When Work Matters: Transforming Classical Network Structures to Graph CNN"	https://paperswithcode.com/dataset/enzymes	01/01/2005						
246	COLLAB	"COLLAB is a scientific collaboration dataset. A graph corresponds to a researcher’s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher’s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2.
Source: 1 Introduction"	https://paperswithcode.com/dataset/collab	01/01/2015						
247	BC5CDR	"BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.
Source: https://www.ncbi.nlm.nih.gov/research/bionlp/Data/
Image Source: https://arxiv.org/pdf/1805.10586.pdf"	https://paperswithcode.com/dataset/bc5cdr	01/01/2016	BioCreative V CDR corpus					
248	JNLPBA	JNLPBA is a biomedical dataset that comes from the GENIA version 3.02 corpus (Kim et al., 2003). It was created with a controlled search on MEDLINE. From this search 2,000 abstracts were selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification. 36 terminal classes were used to annotate the GENIA corpus.	https://paperswithcode.com/dataset/jnlpba		JNLPBA					
249	ChemProt	"ChemProt consists of 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts and was used in the BioCreative VI text mining chemical-protein interactions shared task.
Source: Peng et al."	https://paperswithcode.com/dataset/chemprot							
250	SciERC	"SciERC dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.
Source: http://nlp.cs.washington.edu/sciIE/
Image Source: http://nlp.cs.washington.edu/sciIE/"	https://paperswithcode.com/dataset/scierc	01/01/2018						
251	Paper Field	Paper Field is built from the Microsoft Academic Graph and maps paper titles to one of 7 fields of study. Each field of study - geography, politics, economics, business, sociology, medicine, and psychology - has approximately 12K training examples.	https://paperswithcode.com/dataset/paper-field		Paper Field					
252	PASCAL Context	"The PASCAL Context dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.
Source: Image Segmentation Using Deep Learning:A Survey
Image Source: https://cs.stanford.edu/~roozbeh/pascal-context/"	https://paperswithcode.com/dataset/pascal-context	01/01/2014						
253	SCUT-CTW1500	"The SCUT-CTW1500 dataset contains 1,500 images: 1,000 for training and 500 for testing. In particular, it provides 10,751 cropped text instance images, including 3,530 with curved text. The images are manually harvested from the Internet, image libraries such as Google Open-Image, or phone cameras. The dataset contains a lot of horizontal and multi-oriented text.
Source: Text Recognition in the Wild: A Survey
Image Source: https://github.com/Yuliang-Liu/Curve-Text-Detector"	https://paperswithcode.com/dataset/scut-ctw1500	01/01/2017						
254	OCHuman	"This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human.
Source: https://github.com/liruilong940607/OCHumanApi
Image Source: https://github.com/liruilong940607/OCHumanApi"	https://paperswithcode.com/dataset/ochuman							
255	YAGO3-10	YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations, and most of the triples describe attributes of persons such as citizenship, gender, and profession.	https://paperswithcode.com/dataset/yago3-10		Yet Another Great Ontology 3-10					
256	MSU-MFSD	"The MSU-MFSD dataset contains 280 video recordings of genuine and attack faces. 35 individuals have participated in the development of this database with a total of 280 videos. Two kinds of cameras with different resolutions (720×480 and 640×480) were used to record the videos from the 35 individuals. For the real accesses, each individual has two video recordings captured with the Laptop cameras and Android, respectively. For the video attacks, two types of cameras, the iPhone and Canon cameras were used to capture high definition videos on each of the subject. The videos taken with Canon camera were then replayed on iPad Air screen to generate the HD replay attacks while the videos recorded by the iPhone mobile were replayed itself to generate the mobile replay attacks. Photo attacks were produced by printing the 35 subjects’ photos on A3 papers using HP colour printer. The recording videos with respect to the 35 individuals were divided into training (15 subjects with 120 videos) and testing (40 subjects with 160 videos) datasets, respectively.
Source: Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM Architecture
Image Source: face anti-spoofing based on color texture analysis"	https://paperswithcode.com/dataset/msu-mfsd	01/01/2015						
257	SciCite	"SciCite is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.
Source: Structural Scaffolds for Citation Intent Classification in Scientific Publications
Image Source: https://arxiv.org/pdf/1904.01608v2.pdf"	https://paperswithcode.com/dataset/scicite	01/01/2019	SciCite					
258	PanoContext	"The PanoContext dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.
Source: LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image
Image Source: https://panocontext.cs.princeton.edu/paper.pdf"	https://paperswithcode.com/dataset/panocontext	01/01/2014	PanoContext					
259	Office-31	"The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288×2848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640×480) exhibit significant noise and color as well as white balance artifacts.
Source: Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors
Image Source: https://www.researchgate.net/publication/310953258"	https://paperswithcode.com/dataset/office-31	01/01/2010	Office Dataset					
260	ImageCLEF-DA	"The ImageCLEF-DA dataset is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which contains three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P). For each domain, there are 12 categories and 50 images in each category.
Source: Domain-Symmetric Networks for Adversarial Domain Adaptation
Image Source: https://www.imageclef.org/2014/adaptation"	https://paperswithcode.com/dataset/imageclef-da	01/01/2017	ImageCLEF-DA					
261	Office-Home	"Office-Home is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.
Source: Multi-component Image Translation for Deep Domain Generalization
Image Source: Wen et al"	https://paperswithcode.com/dataset/office-home	01/01/2017						
262	HPatches	"The HPatches is a recent dataset for local patch descriptor evaluation that consists of 116 sequences of 6 images with known homography. The dataset is split into two parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences with significant illumination change, both natural and artificial.
Source: RF-Net: An End-to-End Image Matching Network based on Receptive Field
Image Source: https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf"	https://paperswithcode.com/dataset/hpatches	01/01/2017	Homography-patches dataset					
263	CityPersons	"The CityPersons dataset is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average of the number of pedestrians in an image is 7. The visible-region and full-body annotations are provided.
Source: NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing
Image Source: https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons"	https://paperswithcode.com/dataset/citypersons	01/01/2017						
264	CREMI	"MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images.
About
The goal of this challenge is to evaluate algorithms for automatic reconstruction of neurons and neuronal connectivity from serial section electron microscopy data. The comparison is performed not only by evaluating the quality of neuron segmentations, but also by assessing the accuracy of detecting synapses and identifying synaptic partners. The challenge is carried out on three large and diverse datasets from adult Drosophila melanogaster brain tissue, comprising neuron segmentation ground truth and annotations for synaptic connections. A successful solution would demonstrate its efficiency and generalizability, and carry great potential to reduce the time spent on manual reconstruction of neural circuits in electron microscopy volumes.
Description
We provide three datasets, each consisting of two (5 μm)3 volumes (training and testing, each 1250 px × 1250 px × 125 px) of serial section EM of the adult fly brain. Each volume has neuron and synapse labelings and annotations for pre- and post-synaptic partners."	https://paperswithcode.com/dataset/cremi							
265	ContactDB	"ContactDB is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images.
Source: https://arxiv.org/abs/1904.06830
Image Source: https://github.com/samarth-robo/contactdb_utils"	https://paperswithcode.com/dataset/contactdb							
266	Polyvore	"This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.
Source: GitHub
Image Source: https://arxiv.org/pdf/1707.05691.pdf"	https://paperswithcode.com/dataset/polyvore	01/01/2017	Polyvore Outfits					
267	Comic2k	"Comic2k is a dataset used for cross-domain object detection which contains 2k comic images with image and instance-level annotations.
Image Source: https://naoto0804.github.io/cross_domain_detection/"	https://paperswithcode.com/dataset/comic2k	01/01/2018						
268	PeopleArt	"People-Art is an object detection dataset which consists of people in 43 different styles. People contained in this dataset are quite different from those in common photographs. There are 42 categories of art styles and movements including Naturalism, Cubism, Socialist Realism, Impressionism, and Suprematism
Source: Point Linking Network for Object Detection
Image Source: https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597"	https://paperswithcode.com/dataset/peopleart	01/01/2016	PeopleArt					
269	IconArt	"This dataset contains 5955 painting images (from WikiCommons) : a train set of 2978 images and a test set of 2977 images (for classification task). 1480 of the 2977 images are annotated with bounding boxes for 7 iconographic classes : ‘angel’,‘Child_Jesus’,‘crucifixion_of_Jesus’,‘Mary’,‘nudity’, ‘ruins’,‘Saint_Sebastien’.
The dataset IconArt dataset was introduced in the following paper : ""Weakly Supervised Object Detection in Artworks"" Gonthier et al. ECCV 2018 Workshop Computer Vision for Art Analysis - VISART 2018.
https://wsoda.telecom-paristech.fr/ 
https://zenodo.org/record/4737435"	https://paperswithcode.com/dataset/iconart	05/10/2018						
270	COFW	"The Caltech Occluded Faces in the Wild (COFW) dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones, etc.). All images were hand annotated using the same 29 landmarks as in LFPW. Both the landmark positions as well as their occluded/unoccluded state were annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over 23.
Source: http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset
Image Source: http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset"	https://paperswithcode.com/dataset/cofw	01/01/2013	Caltech Occluded Faces in the Wild					
271	RWTH-PHOENIX-Weather 2014	The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only.	https://paperswithcode.com/dataset/rwth-phoenix-weather-2014							
272	VRD	"The Visual Relationship Dataset (VRD) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.
Source: Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation
Image Source: https://cs.stanford.edu/people/ranjaykrishna/vrd/"	https://paperswithcode.com/dataset/vrd	01/01/2016	Visual Relationship Detection dataset					
273	PPI	"protein roles—in terms of their cellular functions from
gene ontology—in various protein-protein interaction (PPI) graphs, with each graph corresponding
to a different human tissue [41]. positional gene sets are used, motif gene sets and immunological
signatures as features and gene ontology sets as labels (121 in total), collected from the Molecular
Signatures Database [34]. The average graph contains 2373 nodes, with an average degree of 28.8."	https://paperswithcode.com/dataset/ppi	07/06/2017	Protein-Protein Interactions (PPI)					
274	Kuzushiji-MNIST	"Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images). Since MNIST restricts us to 10 classes, the authors chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST. Kuzushiji is a Japanese cursive writing style.
Source: Deep Learning for Classical Japanese Literature
Image Source: https://github.com/rois-codh/kmnist"	https://paperswithcode.com/dataset/kuzushiji-mnist							
275	Slashdot	"The Slashdot dataset is a relational dataset obtained from Slashdot. Slashdot is a technology-related news website know for its specific user community. The website features user-submitted and editor-evaluated current primarily technology oriented news. In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. The network cotains friend/foe links between the users of Slashdot. The network was obtained in February 2009.
Source: http://snap.stanford.edu/data/soc-sign-Slashdot090221.html"	https://paperswithcode.com/dataset/slashdot	01/01/2010						
276	Epinions	"The Epinions dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.
It contains 75,879 nodes and 50,8837 edges.
Source: https://snap.stanford.edu/data/soc-Epinions1.html"	https://paperswithcode.com/dataset/epinions	01/01/2003						
277	VOT2016	"VOT2016 is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.
Source: Video Saliency Detection by 3D Convolutional Neural Networks
Image Source: https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png"	https://paperswithcode.com/dataset/vot2016	01/01/2016	VOT2016					
278	CULane	"CULane is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. The dataset is divided into 88880 images for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories.
Source: https://xingangpan.github.io/projects/CULane.html
Image Source: https://xingangpan.github.io/projects/CULane.html"	https://paperswithcode.com/dataset/culane	01/01/2018						
279	Udacity	"The Udacity dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic.
Source: Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks
Image Source: https://www.researchgate.net/figure/Sample-from-the-Udacity-dataset-with-the-original-ground-truth-bounding-boxes-Note-that_fig3_345652980"	https://paperswithcode.com/dataset/udacity							
280	SUN360	"The goal of the SUN360 panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene.
Source: Scene UNderstanding 360° panorama
Image Source: http://3dvision.princeton.edu/projects/2012/SUN360/"	https://paperswithcode.com/dataset/sun360	01/01/2012	Scene UNderstanding 360° panorama					
281	ACL Title and Abstract Dataset	"This dataset gathers 10,874 title and abstract pairs from the ACL Anthology Network (until 2016).
The structure of the data is as follows:
-   title
-   abstract
-   \newline
This dataset is used in our published paper:
Paper Abstract Writing through Editing Mechanism
Citation
@inproceedings{wang-etal-2018-paper,
    title = ""Paper Abstract Writing through Editing Mechanism"",
    author = ""Wang, Qingyun  and
      Zhou, Zhihao  and
      Huang, Lifu  and
      Whitehead, Spencer  and
      Zhang, Boliang  and
      Ji, Heng  and
      Knight, Kevin"",
    booktitle = ""Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"",
    month = jul,
    year = ""2018"",
    address = ""Melbourne, Australia"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/P18-2042"",
    doi = ""10.18653/v1/P18-2042"",
    pages = ""260--265"",
    abstract = ""We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30{\%} and by non-expert at a rate up to 80{\%}."",
}"	https://paperswithcode.com/dataset/acl-title-and-abstract-dataset	15/05/2018						
282	Wikipedia Person and Animal Dataset	This dataset gathers 428,748 person and 12,236 animal infobox with descriptions based on Wikipedia dump (2018/04/01) and Wikidata (2018/04/12).	https://paperswithcode.com/dataset/wikipedia-person-and-animal-dataset	06/09/2018						
283	VizWiz	"The VizWiz-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.
Source: https://vizwiz.org/tasks-and-datasets/vqa/
Image Source: https://vizwiz.org/tasks-and-datasets/vqa/"	https://paperswithcode.com/dataset/vizwiz	01/01/2018	VizWiz-VQA					
284	KT3DMoSeg	"Please find more details of this dataset at https://alex-xun-xu.github.io/ProjectPage/CVPR_18/index.html
3D motion segmentation has been the key problem in computer vision research due to the application in structure from motion and robotics. Traditional motion segmentation approaches are often evaluated on artificial dataset like Hopkins 155 [1] and its variants. Because the vanishing camera translation effect is often overlooked, these approaches would fail in real world scenes where camera is carrying out significant translation and scene has complex structure. We proposed the KT3DMoSeg to address the 3D motion segmentation problem in real world scenes. The KT3DMoSeg dataset was created upon the KITTI benchmark [2] by manually selecting 22 sequences and labelling each individual foreground object. We select sequence with more significant camera translation so camera mounted on moving cars are preferred. We are interested in the interplay of multiple motions, so clips with more than 3 motions are also chosen, as long as these moving objects contain enough features for forming motion hypotheses. 22 short clips, each with 10-20 frames, are chosen for evaluation. We extract dense trajectories from each sequence using [3] and prune out trajectories shorter than 5 frames.
Reference
[1] R. Tron and R. Vidal. A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms. CVPR, 2007.
[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research, 2013.
[3] N. Sundaram, T. Brox, and K. Keutzer. Dense point trajectories by GPU-accelerated large displacement optical flow. In ECCV, 2010."	https://paperswithcode.com/dataset/kt3dmoseg	15/06/2018						
285	Hopkins155	"The Hopkins 155 dataset consists of 156 video sequences of two or three motions. Each video sequence motion corresponds to a low-dimensional subspace. There are 39−550 data vectors drawn from two or three motions for each video sequence.
Source: Symmetric low-rank representation for subspace clustering
Image Source: http://www.vision.jhu.edu/data/hopkins155/"	https://paperswithcode.com/dataset/hopkins155	01/01/2007						
286	S3DIS	"The Stanford 3D Indoor Scene Dataset (S3DIS) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories.
Source: Grid-GCN for Fast and Scalable Point Cloud Learning
Image Source: https://www.researchgate.net/figure/Examples-of-classified-scenes-in-S3DIS-dataset-left-with-groundtruth-right_fig2_328307943"	https://paperswithcode.com/dataset/s3dis	01/06/2016	Stanford 3D Indoor Scene Dataset (S3DIS)					
287	VoxCeleb1	VoxCeleb1 is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.	https://paperswithcode.com/dataset/voxceleb1	26/06/2017	VoxCeleb1					
288	OQMD v1.2	"The OQMD is a database of DFT calculated thermodynamic and structural properties of one million materials, created in Chris Wolverton's group at Northwestern University.
The OQMD v1.2 dataset for CGNN is downloadable from this link, which contains 561,888 materials. Its format is described in here. The original data is available at the OQMD website."	https://paperswithcode.com/dataset/oqmd-v1-2	27/05/2019	The Open Quantum Materials Database					
289	Moments in Time	Moments in Time is a large-scale dataset for recognizing and understanding action in videos. The dataset includes a collection of one million labeled 3 second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene.	https://paperswithcode.com/dataset/moments-in-time							
290	VQA-CP	"The VQA-CP dataset was constructed by reorganizing VQA v2 such that the correlation between the question type and correct answer differs in the training and test splits. For example, the most common answer to questions starting with What sport… is tennis in the training set, but skiing in the test set. A model that guesses an answer primarily from the question will perform poorly.
Source: Unshuffling Data for Improved Generalization
Image Source: https://arxiv.org/pdf/1712.00377.pdf"	https://paperswithcode.com/dataset/vqa-cp	01/01/2018						
291	LJSpeech	"This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.
Source: The LJ Speech Dataset
Image Source: https://keithito.com/LJ-Speech-Dataset/
Audio Source: https://keithito.com/LJ-Speech-Dataset/"	https://paperswithcode.com/dataset/ljspeech		The LJ Speech Dataset					
292	QNLI	"The QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark.
Source: https://arxiv.org/pdf/1804.07461.pdf"	https://paperswithcode.com/dataset/qnli	01/01/2019	Question-answering NLI					
293	RTE	The Recognizing Textual Entailment (RTE) datasets come from a series of textual entailment challenges. Data from RTE1, RTE2, RTE3 and RTE5 is combined. Examples are constructed based on news and Wikipedia text.	https://paperswithcode.com/dataset/rte		Recognizing Textual Entailment					
294	MRPC	"Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).
Source: Exploiting Semantic Annotations and Q-Learning for Constructing an Efficient Hierarchy/Graph Texts Organization
Image Source: https://www.aclweb.org/anthology/I05-5002.pdf"	https://paperswithcode.com/dataset/mrpc	01/01/2005	Microsoft Research Paraphrase Corpus					
295	CODAH	"The COmmonsense Dataset Adversarially-authored by Humans (CODAH) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. It contains 2801 questions in total, and uses 5-fold cross validation for evaluation.
Source: CODAH Dataset
Image Source: https://www.aclweb.org/anthology/W19-2008.pdf"	https://paperswithcode.com/dataset/codah		COmmonsense Dataset Adversarially-authored by Humans					
296	CrowdPose	"The CrowdPose dataset contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. The test set includes 8,000 images. The crowded images containing homes are extracted from MSCOCO, MPII and AI Challenger.
Source: Human Pose Estimation for Real-World Crowded Scenarios
Image Source: https://github.com/Jeff-sjtu/CrowdPose"	https://paperswithcode.com/dataset/crowdpose	01/01/2019	CrowdPose					
297	MemexQA	"A large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers.
Source: MemexQA: Visual Memex Question Answering"	https://paperswithcode.com/dataset/memexqa							
298	ECSSD	"The Extended Complex Scene Saliency Dataset (ECSSD) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.
Source: SAD: Saliency-based Defenses Against Adversarial Examples"	https://paperswithcode.com/dataset/ecssd	01/01/2013	Extended Complex Scene Saliency Dataset					
299	HKU-IS	"HKU-IS is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.
Source: Deep Contrast Learning for Salient Object Detection
Image Source: https://sites.google.com/site/ligb86/mdfsaliency/"	https://paperswithcode.com/dataset/hku-is	01/01/2015						
300	PASCAL-S	"PASCAL-S is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.
Source: Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection"	https://paperswithcode.com/dataset/pascal-s	01/01/2014						
301	DUT-OMRON	"The DUT-OMRON dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.
Source: Global Context-Aware Progressive Aggregation Network for Salient Object Detection"	https://paperswithcode.com/dataset/dut-omron	01/01/2013						
302	MSMT17	"MSMT17 is a multi-scene multi-time person re-identification dataset. The dataset consists of 180 hours of videos, captured by 12 outdoor cameras, 3 indoor cameras, and during 12 time slots. The videos cover a long period of time and present complex lighting variations, and it contains a large number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes.
The dataset is not available anymore."	https://paperswithcode.com/dataset/msmt17	23/11/2017						
303	USPS	"USPS is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.
Source: Hallucinating Agnostic Images to Generalize Across Domains
Image Source: https://ieeexplore.ieee.org/document/291440"	https://paperswithcode.com/dataset/usps	01/01/1994	USPS					
304	SIXray	"The SIXray dataset is constructed by the Pattern Recognition and Intelligent System Development Laboratory, University of Chinese Academy of Sciences. It contains 1,059,231 X-ray images which are collected from some several subway stations. There are six common categories of prohibited items, namely, gun, knife, wrench, pliers, scissors and hammer. It has three subsets called SIXray10, SIXray100 and SIXray1000, There are image-level annotations provided by human security inspectors for the whole dataset. In addition the images in the test set are annotated with a bounding-box for each prohibited item to evaluate the performance of object localization.
Source: https://github.com/MeioJane/SIXray
Image Source: https://github.com/MeioJane/SIXray"	https://paperswithcode.com/dataset/sixray							
305	Django	"The Django dataset is a dataset for code generation comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description.
Source: Latent Predictor Networks for Code Generation
Image Source: https://github.com/microsoft/vscode-docs/issues/2696"	https://paperswithcode.com/dataset/django	01/01/2015	Django					
306	PACS	"PACS is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.
Source: Deep Domain-Adversarial Image Generation for Domain Generalisation
Image Source: https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033"	https://paperswithcode.com/dataset/pacs	01/01/2017	Photo-Art-Cartoon-Sketch					
307	BioGRID	"BioGRID is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species.
Source: https://thebiogrid.org/"	https://paperswithcode.com/dataset/biogrid	01/01/2006	Biological General Repository for Interaction Datasets					
308	Freiburg Forest	"The Freiburg Forest dataset was collected using a Viona autonomous mobile robot platform equipped with cameras for capturing multi-spectral and multi-modal images. The dataset may be used for evaluation of different perception algorithms for segmentation, detection, classification, etc. All scenes were recorded at 20 Hz with a camera resolution of 1024x768 pixels. The data was collected on three different days to have enough variability in lighting conditions as shadows and sun angles play a crucial role in the quality of acquired images. The robot traversed about 4.7 km each day. The dataset creators provide manually annotated pixel-wise ground truth segmentation masks for 6 classes: Obstacle, Trail, Sky, Grass, Vegetation, and Void.
Source: http://deepscene.cs.uni-freiburg.de/
Image Source: http://deepscene.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-forest	15/09/2021	Freiburg Forest					
309	SNIPS	"The SNIPS Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:

SearchCreativeWork (e.g. Find me the I, Robot television show),
GetWeather (e.g. Is it windy in Boston, MA right now?),
BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),
PlayMusic (e.g. Play the last track from Beyoncé off Spotify),
AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),
RateBook (e.g. Give 6 stars to Of Mice and Men),
SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).
The training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.

Source: https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/"	https://paperswithcode.com/dataset/snips	01/01/2018	SNIPS Natural Language Understanding benchmark					
310	Nottingham	"The Nottingham Dataset is a collection of 1200 American and British folk songs.
Source: Rethinking Recurrent Latent Variable Model for Music Composition
Image Source: https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/"	https://paperswithcode.com/dataset/nottingham		Nottingham					
311	Cluttered Omniglot	"Dataset for one-shot segmentation.
Source: One-Shot Segmentation in Clutter"	https://paperswithcode.com/dataset/cluttered-omniglot							
312	PKU-MMD	"The PKU-MMD dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view.
Source: Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation
Image Source: https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html"	https://paperswithcode.com/dataset/pku-mmd	01/01/2017	PKU-MMD					
313	NTU RGB+D	"NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training.
Source: Action Recognition for Depth Video using Multi-view Dynamic Images"	https://paperswithcode.com/dataset/ntu-rgb-d	01/01/2016						
314	Birdsnap	"Birdsnap is a large bird dataset consisting of 49,829 images from 500 bird species with 47,386 images used for training and 2,443 images used for testing.
Source: Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks
Image Source: http://thomasberg.org/"	https://paperswithcode.com/dataset/birdsnap	01/01/2014						
315	CoLA	"The Corpus of Linguistic Acceptability (CoLA) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.
Source: https://nyu-mll.github.io/CoLA/
Image Source: https://arxiv.org/pdf/1805.12471.pdf"	https://paperswithcode.com/dataset/cola	01/01/2018	Corpus of Linguistic Acceptability					
316	ASTD	"Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.
Source: ASTD: Arabic Sentiment Tweets Dataset"	https://paperswithcode.com/dataset/astd		Arabic Sentiment Tweets Dataset					
317	LSMDC	"This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets.
Source: Use What You Have: Video Retrieval Using Representations From Collaborative Experts
Image Source: https://sites.google.com/site/describingmovies/"	https://paperswithcode.com/dataset/lsmdc	01/01/2015	Large Scale Movie Description Challenge					
318	MSR-VTT	"MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.
Source: Learning to Discretely Compose Reasoning Module Networksfor Video Captioning"	https://paperswithcode.com/dataset/msr-vtt	01/01/2016						
319	MSVD	"The Microsoft Research Video Description Corpus (MSVD) dataset consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data.
Source: https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F
Image Source: https://arxiv.org/pdf/1609.06782.pdf"	https://paperswithcode.com/dataset/msvd	01/01/2011	Microsoft Research Video Description Corpus					
320	DiDeMo	"The Distinct Describable Moments (DiDeMo) dataset is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions. The videos are collected from Flickr and each video is trimmed to a maximum of 30 seconds. The videos in the dataset are divided into 5-second segments to reduce the complexity of annotation. The dataset is split into training, validation and test sets containing 8,395, 1,065 and 1,004 videos respectively. The dataset contains a total of 26,892 moments and one moment could be associated with descriptions from multiple annotators. The descriptions in DiDeMo dataset are detailed and contain camera movement, temporal transition indicators, and activities. Moreover, the descriptions in DiDeMo are verified so that each description refers to a single moment.
Source: Weakly Supervised Video Moment Retrieval From Text Queries
Image Source: https://www.di.ens.fr/~miech/datasetviz/"	https://paperswithcode.com/dataset/didemo	01/01/2017	Distinct Describable Moments					
321	MuPoTS-3D	"MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.
Source: DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild
Image Source: http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/"	https://paperswithcode.com/dataset/mupots-3d	01/01/2018	Multiperson Pose Test Set in 3DMulti-person Pose estimation Test Set in 3D					
322	Helsinki Prosody Corpus	"The Helsinki Prosody Corpus is a dataset for predicting prosodic prominence from written text. The prosodic annotations are automatically generated, high quality prosodic for the 'clean' subsets of LibriTTS corpus (Zen et al., 2019), comprising of 262.5 hours of read speech from 1230 speakers. The transcribed sentences were aligned and then prosodically annotated with word-level acoustic prominence labels.
Source: Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations"	https://paperswithcode.com/dataset/helsinki-prosody-corpus	06/08/2019						
323	WMCA	"The Wide Multi Channel Presentation Attack (WMCA) database consists of 1941 short video recordings of both bonafide and presentation attacks from 72 different identities. The data is recorded from several channels including color, depth, infra-red, and thermal.
Additionally, the pulse reading data for bonafide recordings is also provided.
Preprocessed images for some of the channels are also provided for part of the data used in the reference publication.
The WMCA database is produced at Idiap within the framework of “IARPA BATL” and “H2020 TESLA” projects and it is intended for investigation of presentation attack detection (PAD) methods for face recognition systems."	https://paperswithcode.com/dataset/wmca	19/09/2019	Wide Multi Channel Presentation Attack					
324	AQUAINT	"The AQUAINT Corpus consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).
Source: Linguistic Data Consortium
Image Source: https://catalog.ldc.upenn.edu/LDC2002T31"	https://paperswithcode.com/dataset/aquaint							
325	MAFL	"The MAFL dataset contains manually annotated facial landmark locations for 19,000 training and 1,000 test images.
Source: Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance
Image Source: http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html"	https://paperswithcode.com/dataset/mafl	01/01/2014	Multi-Attribute Facial Landmark					
326	Species-800	Species-800 is a corpus for species entities, which is based on manually annotated abstracts. It comprises 800 PubMed abstracts that contain identified organism mentions. To increase the corpus taxonomic mention diversity the 800 abstracts were collected by selecting 100 abstracts from the following 8 categories: bacteriology, botany, entomology, medicine, mycology, protistology, virology and zoology. 800 has been annotated with a focus at the species level; however, higher taxa mentions (such as genera, families and orders) have also been considered.	https://paperswithcode.com/dataset/species-800		Species-800					
327	LINNAEUS	"LINNAEUS is a general-purpose dictionary matching software, capable of processing multiple types of document formats in the biomedical domain (MEDLINE, PMC, BMC, OTMI, text, etc.). It can produce multiple types of output (XML, HTML, tab-separated-value file, or save to a database). It also contains methods for acting as a server (including load balancing across several servers), allowing clients to request matching over a network. A package with files for recognizing and identifying species names is available for LINNAEUS, showing 94% recall and 97% precision compared to LINNAEUS-species-corpus.
Source: LINNAEUS"	https://paperswithcode.com/dataset/linnaeus							
328	NLVR	"NLVR contains 92,244 pairs of human-written English sentences grounded in synthetic images. Because the images are synthetically generated, this dataset can be used for semantic parsing.
Source: http://lil.nlp.cornell.edu/nlvr/
Image Source: http://lil.nlp.cornell.edu/nlvr/"	https://paperswithcode.com/dataset/nlvr	01/01/2017	Natural Language Visual Reasoningnatural language for visual reasoning					
329	ChestX-ray14	"ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.
Source: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610
Image Source: https://nihcc.app.box.com/v/ChestXray-NIHCC"	https://paperswithcode.com/dataset/chestx-ray14	01/01/2017	ChestX-ray14					
330	HICO	"HICO is a benchmark for recognizing human-object interactions (HOI). 
Key features:

A diverse set of interactions with common object categories
A list of well-defined, sense-based HOI categories
An exhaustive labeling of co-occurring interactions with an object category in each image
The annotation of each HOI instance (i.e. a human and an object bounding box with an interaction class label) in all images

Source: HICO: A Benchmark for Recognizing Human-Object Interactions in Images"	https://paperswithcode.com/dataset/hico		Humans Interacting with Common Objects					
331	Adverse Drug Events (ADE) Corpus	"Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports.
A significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F₁ score of 0.70 indicating a potential useful application of the corpus."	https://paperswithcode.com/dataset/ade-corpus	01/01/2012						
332	Sports-1M	"The Sports-1M dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.
Source: Review of Action Recognition and Detection Methods
Image Source: Computer Vision for Sports"	https://paperswithcode.com/dataset/sports-1m	01/01/2014						
333	YouTube-8M	"The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc
Source: Audio-Visual Embedding for Cross-Modal Music Video Retrieval through Supervised Deep CCA"	https://paperswithcode.com/dataset/youtube-8m	01/01/2016						
334	Something-Something V2	"The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.
Source
Image Source"	https://paperswithcode.com/dataset/something-something-v2	01/01/2017	20BN-Something-Something Dataset V2					
335	Jester	"6.5 million anonymous ratings of jokes by users of the Jester Joke Recommender System.
Source: Jester Datasets for Recommender Systems and Collaborative Filtering Research
Image Source: http://eigentaste.berkeley.edu/"	https://paperswithcode.com/dataset/jester	01/01/2001	Jester dataset					
336	Something-Something V1	"The 20BN-SOMETHING-SOMETHING dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 108,499 videos, with 86,017 in the training set, 11,522 in the validation set and 10,960 in the test set. There are 174 labels.
⚠️ Attention: This is the outdated V1 of the dataset. V2 is available here.
Source: https://20bn.com/datasets/something-something/v1
Image Source: https://20bn.com/datasets/something-something/v1"	https://paperswithcode.com/dataset/something-something-v1	01/01/2017	20BN-Something-Something Dataset V1					
337	HVU	"HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.
Source: Large Scale Holistic Video Understanding"	https://paperswithcode.com/dataset/hvu		Holistic Video Understanding					
338	PTC	"PTC is a collection of 344 chemical compounds represented as graphs which report the carcinogenicity for rats. There are 19 node labels for each node.
Source: Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity"	https://paperswithcode.com/dataset/ptc		Predictive Toxicology Challenge					
339	UT-Kinect	"The UT-Kinect dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s.
Source: https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html
Image Source: https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html"	https://paperswithcode.com/dataset/ut-kinect	01/01/2012	UTKinect-Action3D Dataset					
340	IPC-grounded		https://paperswithcode.com/dataset/ipc-grounded							
341	CAD-120	"The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor. Being able to detect human activities is important for making personal assistant robots useful in performing assistive tasks. The CAD dataset comprises twelve different activities (composed of several sub-activities) performed by four people in different environments, such as a kitchen, a living room, and office, etc.
Source: https://www.re3data.org/repository/r3d100012216
Image Source: https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041"	https://paperswithcode.com/dataset/cad-120	01/01/2013						
342	NTU RGB+D 120	"NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. 
Source: NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding"	https://paperswithcode.com/dataset/ntu-rgb-d-120							
343	CK+	"The Extended Cohn-Kanade (CK+) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.
Source: EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition"	https://paperswithcode.com/dataset/ck	01/01/2010	Extended Cohn-Kanade dataset					
344	YouTube-VOS 2018 val	"Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.
Source: CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing
Image Source: https://youtube-vos.org/"	https://paperswithcode.com/dataset/youtube-vos	01/01/2018	Youtube Video Object Segmentation					
345	TabFact	"TabFact is a large-scale dataset which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. TabFact is the first dataset to evaluate language inference on structured data, which involves mixed reasoning skills in both symbolic and linguistic aspects. 
Source: GitHub"	https://paperswithcode.com/dataset/tabfact	05/09/2019	TabFact					
346	MPI-INF-3DHP	"MPI-INF-3DHP is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras.
Source: Anatomy-aware 3D Human Pose Estimation in Videos
Image Source: https://arxiv.org/abs/1611.09813"	https://paperswithcode.com/dataset/mpi-inf-3dhp	01/01/2017						
347	Beijing Multi-Site Air-Quality Dataset	This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.	https://paperswithcode.com/dataset/beijing-air-quality	11/08/2017						
348	PhysioNet Challenge 2012	"The PhysioNet Challenge 2012 dataset is publicly available and contains the de-identified records of 8000 patients in Intensive Care Units (ICU). Each record consists of roughly 48 hours of multivariate time series data with up to 37 features recorded at various times from the patients during their stay such as respiratory rate, glucose etc.
Source: Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT)
Image Source: https://physionet.org/content/challenge-2016/1.0.0/"	https://paperswithcode.com/dataset/physionet-challenge-2012		PhysioNet Challenge 2012					
349	MuJoCo	MuJoCo (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods.	https://paperswithcode.com/dataset/mujoco	07/10/2012						
350	SunYs		https://paperswithcode.com/dataset/sunys		Lungvesselct					
351	WFLW	"The Wider Facial Landmarks in the Wild or WFLW database contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions.
Source: Deep Entwined Learning Head Pose and Face Alignment Inside an Attentional Cascade with Doubly-Conditional fusion
Image Source: https://wywu.github.io/projects/LAB/WFLW.html"	https://paperswithcode.com/dataset/wflw	01/01/2018	Wider Facial Landmarks in the Wild					
352	REDS	"The realistic and dynamic scenes (REDS) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively
Source: Video Super Resolution Based on Deep Learning: A comprehensive survey
Image Source: https://seungjunnah.github.io/Datasets/reds.html"	https://paperswithcode.com/dataset/reds		"REalistic and Diverse Scenes dataset
realistic and dynamic scenes"					
353	nuScenes	"The nuScenes dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.
Source: PointPainting: Sequential Fusion for 3D Object Detection"	https://paperswithcode.com/dataset/nuscenes	26/03/2019						
354	Sleep-EDF	"The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers. Some records also contain respiration and body temperature. Corresponding hypnograms (sleep patterns) were manually scored by well-trained technicians according to the Rechtschaffen and Kales manual, and are also available.
Source: https://www.physionet.org/content/sleep-edfx/1.0.0/"	https://paperswithcode.com/dataset/sleep-edf		Sleep-EDF Expanded					
355	CommonsenseQA	"The CommonsenseQA is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each.
The dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):

a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”),
the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”)
for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually.

Source: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge
Image Source: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"	https://paperswithcode.com/dataset/commonsenseqa	01/01/2019						
356	3DPW	"The 3D Poses in the Wild dataset is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.
The dataset includes:

60 video sequences.
2D pose annotations.
3D poses obtained with the method introduced in the paper.
Camera poses for every frame in the sequences.
3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.
18 3D models in different clothing variations.

Source: https://virtualhumans.mpi
Image Source: https://virtualhumans.mpi"	https://paperswithcode.com/dataset/3dpw	01/01/2018						
357	VOT2019	"VOT2019 is a Visual Object Tracking benchmark for short-term tracking in RGB.
Source: https://www.votchallenge.net/vot2019/dataset.html
Image Source: https://www.votchallenge.net/vot2019/dataset.html"	https://paperswithcode.com/dataset/vot2019							
358	MUSDB18	"The MUSDB18 is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems.
The dataset is split into training and test sets with 100 and 50 songs, respectively. All signals are stereophonic and encoded at 44.1kHz.
Source: https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems
Image Source: https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems"	https://paperswithcode.com/dataset/musdb18							
359	BoolQ	"BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings.
Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context.
Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified and questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Only questions that were marked as having a yes/no answer are used, and each question is paired with the selected passage instead of the entire document.
Source: https://github.com/google-research-datasets/boolean-questions
Image Source: BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"	https://paperswithcode.com/dataset/boolq	01/01/2019	Boolean Questions					
360	COPA	"The Choice Of Plausible Alternatives (COPA) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%.
Source: Choice of Plausible Alternatives (COPA)
Image Source: https://people.ict.usc.edu/~gordon/copa.html"	https://paperswithcode.com/dataset/copa	01/01/2011	Choice of Plausible Alternatives					
361	ReCoRD	"Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd].
Image Source: Zhang et al"	https://paperswithcode.com/dataset/record	30/10/2018	ReCoRD					
362	LIDC-IDRI	"The LIDC-IDRI dataset contains lesion annotations from four experienced thoracic radiologists. LIDC-IDRI contains 1,018 low-dose lung CTs from 1010 lung patients.
Source: A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans
Image Source: https://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=IJACSA&SerialNo=15"	https://paperswithcode.com/dataset/lidc-idri		LIDC-IDRI					
363	ORL	"The ORL Database of Faces contains 400 images from 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel.
Source: https://cam-orl.co.uk/facedatabase.html
Image Source: https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition"	https://paperswithcode.com/dataset/orl	01/01/1994	Our Database of Faces					
364	EgoGesture	"The EgoGesture dataset contains 2,081 RGB-D videos, 24,161 gesture samples and 2,953,224 frames from 50 distinct subjects.
Source: http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html
Image Source: http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html"	https://paperswithcode.com/dataset/egogesture	01/01/2018						
365	Street Scene	"Street Scene is a dataset for video anomaly detection. Street Scene consists of 46 training and 35 testing high resolution 1280×720 video sequences taken from a USB camera overlooking a scene of a two-lane street with bike lanes and pedestrian sidewalks during daytime. The dataset is challenging because of the variety of activity taking place such as cars driving, turning, stopping and parking; pedestrians walking, jogging and pushing strollers; and bikers riding in bike lanes. In addition the videos contain changing shadows, moving background such as a flag and trees blowing in the wind, and occlusions caused by trees and large vehicles. There are a total of 56,847 frames for training and 146,410 frames for testing, extracted from the original videos at 15 frames per second. The dataset contains a total of 205 naturally occurring anomalous events ranging from illegal activities such as jaywalking and illegal U-turns to simply those that do not occur in the training set such as pets being walked and a metermaid ticketing a car.
Source: A Survey of Single-SceneVideo Anomaly Detection
Image Source: https://www.merl.com/demos/video-anomaly-detection"	https://paperswithcode.com/dataset/street-scene	01/01/2019						
366	Materials Project	"The Materials Project is a collection of chemical compounds labelled with different attributes.
The dataset links:

MP 2018.6.1 (69,239 materials)
MP 2019.4.1 (133,420 materials)"	https://paperswithcode.com/dataset/materials-project							
367	Semantic3D	"Semantic3D is a point cloud dataset of scanned outdoor scenes with over 3 billion points. It contains 15 training and 15 test scenes annotated with 8 class labels. This large labelled 3D point cloud data set of natural covers a range of diverse urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles to name just a few. The point clouds provided are scanned statically with state-of-the-art equipment and contain very fine details.
Source: Tangent Convolutions for Dense Prediction in 3D
Image Source: http://www.semantic3d.net/"	https://paperswithcode.com/dataset/semantic3d	01/01/2017						
368	SemanticKITTI	"SemanticKITTI is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing.
Source: Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation
Image Source: https://github.com/PRBonn/semantic-kitti-api"	https://paperswithcode.com/dataset/semantickitti	01/01/2019						
369	Wine	"These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.
Source: UCI Machine Learning Repository Wine Dataset
Image Source: https://archive.ics.uci.edu/ml/datasets/Wine"	https://paperswithcode.com/dataset/wine		Wine Data Set					
370	JSB Chorales	"The JSB chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the
18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining
three voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively."	https://paperswithcode.com/dataset/jsb-chorales	27/06/2012						
371	Tiny ImageNet	"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images.
Source: Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI
Image Source: https://arxiv.org/pdf/1707.08819.pdf"	https://paperswithcode.com/dataset/tiny-imagenet		Tiny ImageNet					
372	AFHQ	"Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various
breeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. 
All images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.
Source: StarGAN v2: Diverse Image Synthesis for Multiple Domains"	https://paperswithcode.com/dataset/afhq	04/12/2019	Animal Faces-HQ					
373	FSS-1000	"FSS-1000 is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.
Source: https://github.com/HKUSTCV/FSS-1000
Image Source: https://github.com/HKUSTCV/FSS-1000"	https://paperswithcode.com/dataset/fss-1000							
374	Reddit	"The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.
Source: https://arxiv.org/pdf/1706.02216.pdf
Image Source: https://minimaxir.com/2016/05/reddit-graph/"	https://paperswithcode.com/dataset/reddit	01/01/2017						
375	DeepFashion	"DeepFashion is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.
Source: A Benchmark for Inpainting of Clothing Images with Irregular Holes"	https://paperswithcode.com/dataset/deepfashion	01/01/2016						
376	FER2013	"Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48×48, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images – 600, while other labels have nearly 5,000 samples each.
Source: Eavesdrop the Composition Proportion of Training Labels in Federated Learning
Image Source: https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280"	https://paperswithcode.com/dataset/fer2013	01/01/2013	Facial Expression Recognition 2013 Dataset					
377	Pinterest	"The Pinterest dataset contains more than 1 million images associated to Pinterest users’ who have “pinned” them.
Source: https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf"	https://paperswithcode.com/dataset/pinterest	01/01/2015						
378	LOL	"The LOL dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400×600.
Source: Unsupervised Real-world Low-light Image Enhancement with Decoupled Networks
Image Source: https://daooshee.github.io/BMVC2018website/"	https://paperswithcode.com/dataset/lol	01/01/2018	LOw-Light dataset					
379	HRF	"The HRF dataset is a dataset for retinal vessel segmentation which comprises 45 images and is organized as 15 subsets. Each subset contains one healthy fundus image, one image of patient with diabetic retinopathy and one glaucoma image. The image sizes are 3,304 x 2,336, with a training/testing image split of 22/23.
Source: Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation
Image Source: https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531"	https://paperswithcode.com/dataset/hrf	01/01/2013	High-Resolution Fundus					
380	DBRD	"The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels. It is greatly influenced by the Large Movie Review Dataset and intended as a benchmark for sentiment classification in Dutch. 
Source: DBRD"	https://paperswithcode.com/dataset/dbrd		Dutch Book Reviews Dataset					
381	Kaggle-Credit Card Fraud Dataset	"The dataset contains transactions made by credit cards in September 2013 by European cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.  
It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification."	https://paperswithcode.com/dataset/kaggle-credit-card-fraud-dataset	01/04/2021						
382	Thyroid	"Thyroid is a dataset for detection of thyroid diseases, in which patients diagnosed with hypothyroid or subnormal are anomalies against normal patients. It contains 2800 training data instance and 972 test instances, with 29 or so attributes.
Source: Deep Reinforcement Learning for Unknown Anomaly Detection
Image Source: https://www.researchgate.net/figure/Features-of-Thyroid-dataset_tbl1_285711967"	https://paperswithcode.com/dataset/thyroid		Thyroid Disease					
383	Argoverse	"Argoverse is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called “agent”, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.
Source: Learning Lane Graph Representations for Motion Forecasting
Image Source: https://arxiv.org/pdf/1911.02620.pdf"	https://paperswithcode.com/dataset/argoverse	01/01/2019						
384	CLEVR	"CLEVR (Compositional Language and Elementary Visual Reasoning) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, A test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.
Source: On transfer learning using a MAC model variant
Image Source: Johnson et al"	https://paperswithcode.com/dataset/clevr	20/12/2016	Compositional Language and Elementary Visual Reasoning					
385	Tai-Chi-HD	"Thai-Chi-HD is a high resolution dataset which can be used as reference benchmark for evaluating frameworks for image animation and video generation. It consists of cropped videos of full human bodies performing Tai Chi actions.
Image source: https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf"	https://paperswithcode.com/dataset/tai-chi-hd	29/02/2020						
386	CMU-MOSEI	"CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics.
Source: https://www.amir-zadeh.com/datasets
Image Source: https://www.amir-zadeh.com/datasets"	https://paperswithcode.com/dataset/cmu-mosei	01/01/2018						
387	AffectNet	"AffectNet is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.
Source: Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition
Image Source: http://mohammadmahoor.com/affectnet/"	https://paperswithcode.com/dataset/affectnet	01/01/2019						
388	FER+	"The FER+ dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.
Source: https://github.com/Microsoft/FERPlus
Image Source: https://github.com/Microsoft/FERPlus"	https://paperswithcode.com/dataset/fer	01/01/2016	Face Expression Recognition Plus dataset					
389	CommonGen	"CommonGen is constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. 
Source: CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"	https://paperswithcode.com/dataset/commongen							
390	The China Physiological Signal Challenge 2018	"The China Physiological Signal Challenge 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include one normal ECG type and eight abnormal types.
Source: An Open Access Database for Evaluating the Algorithms of Electrocardiogram Rhythm and Morphology Abnormality Detection"	https://paperswithcode.com/dataset/the-china-physiological-signal-challenge-2018	01/09/2018						
391	University-1652	"Contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. University-1652 is a drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. 
Source: University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization"	https://paperswithcode.com/dataset/university-1652							
392	FQuAD	"A French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version.
Source: FQuAD: French Question Answering Dataset"	https://paperswithcode.com/dataset/fquad		French Question Answering Dataset					
393	HARD	"The Hotel Arabic-Reviews Dataset (HARD) contains 93700 hotel reviews in Arabic language. The hotel reviews were collected from Booking.com website during June/July 2016. The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.
Source: HARD"	https://paperswithcode.com/dataset/hard		Hotel Arabic-Reviews Dataset					
394	3DFAW	"3DFAW contains 23k images with 66 3D face keypoint annotations.
Source: Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild
Image Source: http://mhug.disi.unitn.it/workshop/3dfaw/"	https://paperswithcode.com/dataset/3dfaw	01/01/2016						
395	Breakfast	"The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded “in the wild” as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.
Source: https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/
Image Source: https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/"	https://paperswithcode.com/dataset/breakfast	01/01/2014	The Breakfast Actions Dataset					
396	NELL-995	NELL-995 KG Completion Dataset	https://paperswithcode.com/dataset/nell-995	20/07/2017						
397	Composition-1K	"Composition-1K is a large-scale image matting dataset including 49300 training images and 1000 testing images.
Image source: https://arxiv.org/pdf/1703.03872v3.pdf"	https://paperswithcode.com/dataset/composition-1k	10/03/2017						
398	KolektorSDD	"The dataset is constructed from images of defective production items that were provided and annotated by Kolektor Group d.o.o.. The images were captured in a controlled industrial environment in a real-world case.
The dataset consists of 399 images at 500 x ~1250 px in size.
Please cite our paper published in the Journal of Intelligent Manufacturing when using this dataset:
@article{Tabernik2019JIM,
  author = {Tabernik, Domen and {\v{S}}ela, Samo and Skvar{\v{c}}, Jure and 
  Sko{\v{c}}aj, Danijel},
  journal = {Journal of Intelligent Manufacturing},
  title = {{Segmentation-Based Deep-Learning Approach for Surface-Defect Detection}},
  year = {2019},
  month = {May},
  day = {15},
  issn={1572-8145},
  doi={10.1007/s10845-019-01476-x}
}"	https://paperswithcode.com/dataset/kolektorsdd	20/03/2019	Kolektor Surface-Defect Dataset					
399	ASLG-PC12	"An artificial corpus built using grammatical dependencies rules due to the lack of resources for Sign Language.
Source: ASLG-PC12"	https://paperswithcode.com/dataset/aslg-pc12	01/03/2013	English-ASL Gloss Parallel Corpus 2012					
400	CIFAR10-DVS	"CIFAR10-DVS is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128×128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera.
Source: Structure-Aware Network for Lane Marker Extraction with Dynamic Vision Sensor
Image Source: https://www.frontiersin.org/articles/10.3389/fnins.2017.00309/full"	https://paperswithcode.com/dataset/cifar10-dvs		CIFAR10-DVS					
401	Stanford Online Products	"Stanford Online Products (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing
Source: Deep Metric Learning with Alternating Projections onto Feasible Sets
Image Source: https://cvgl.stanford.edu/projects/lifted_struct/"	https://paperswithcode.com/dataset/stanford-online-products	01/01/2016	Stanford Online Products					
402	Ecoli	The Ecoli dataset is a dataset for protein localization. It contains 336 E.coli proteins split into 8 different classes.	https://paperswithcode.com/dataset/ecoli	01/01/1996						
403	Yeast	"Yeast dataset consists of a protein-protein interaction network. Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology.
Source: http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm"	https://paperswithcode.com/dataset/yeast							
404	MOT17	"The Multiple Object Tracking 17 (MOT17) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.
Source: Deep Affinity Network for Multiple Object Tracking
Image Source: https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502"	https://paperswithcode.com/dataset/mot17	02/03/2016	Multiple Object Tracking 17					
405	MOT20	"MOT20 is a dataset for multiple object tracking. The dataset contains 8 challenging video sequences (4 train, 4 test) in unconstrained environments, from crowded places such as train stations, town squares and a sports stadium.
Image Source: https://motchallenge.net/vis/MOT20-04"	https://paperswithcode.com/dataset/mot20		MOT20					
406	SEMAINE	"The SEMAINE videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.
Source: ROBUST MODELING OF EPISTEMIC MENTAL STATES"	https://paperswithcode.com/dataset/semaine	01/01/2012						
407	R2R	"R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI.
Source: Natural language interaction with robots"	https://paperswithcode.com/dataset/room-to-room	01/01/2018	Room-to-Room					
408	SceneNN	"SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.
All scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.
Source: SceneNN: A Scene Meshes Dataset with aNNotations
Image Source: http://103.24.77.34/scenenn/home/"	https://paperswithcode.com/dataset/scenenn	01/01/2016						
409	EGTEA	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/egtea							
410	GAP	GAP is a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.	https://paperswithcode.com/dataset/gap	14/08/2015	GAP Benchmark Suite					
411	Rotated MNIST	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/rotated-mnist							
412	BIPED	"Details
It contains 250 outdoor images of 1280$\times$720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked several times in order to correct possible mistakes or wrong edges by just one subject. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images can be appreciated looking at the GT, see Figs above.
BIPED dataset has 250 images in high definition. Thoses images are already split up for training and testing. 200 for training and 50 for testing.
Version
The current version is the second one."	https://paperswithcode.com/dataset/biped	04/09/2019	Barcelona Images for Perceptual Edge Detection					
413	StereoSet	"A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion.
Source: StereoSet: Measuring stereotypical bias in pretrained language models"	https://paperswithcode.com/dataset/stereoset							
414	MIT-States	"The MIT-States dataset has 245 object classes, 115 attribute classes and ∼53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords.
Source: Attributes as Operators: Factorizing Unseen Attribute-Object Compositions
Image Source: http://web.mit.edu/phillipi/Public/states_and_transformations/index.html"	https://paperswithcode.com/dataset/mit-states	01/01/2015						
415	Caltech-256	"Caltech-256 is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.
Source: Exploiting Non-Linear Redundancy for Neural Model Compression
Image Source: ML4A"	https://paperswithcode.com/dataset/caltech-256							
416	SCDE	"SCDE is a human-created sentence cloze dataset, collected from public school English examinations in China. The task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers.
Source: SCDE"	https://paperswithcode.com/dataset/scde	27/04/2020	SCDE					
417	VATEX	"VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.
Source: https://arxiv.org/pdf/1904.03493.pdf
Image Source: https://arxiv.org/pdf/1904.03493.pdf"	https://paperswithcode.com/dataset/vatex	01/01/2019	Video And TEXt					
418	ViGGO	"The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts.
Source: VIGGO"	https://paperswithcode.com/dataset/viggo	26/10/2019						
419	ISTD	"The Image Shadow Triplets dataset (ISTD) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.
Source: ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal
Image Source: Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal"	https://paperswithcode.com/dataset/istd	01/01/2018						
420	LCQMC	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/lcqmc	20/08/2018	Large-scale Chinese Question Matching Corpus					
421	Ciao	"The Ciao dataset contains rating information of users given to items, and also contain item category information. The data comes from the Epinions dataset.
Source: Collaborative Translational Metric Learning"	https://paperswithcode.com/dataset/ciao	01/01/2012						
422	THUCNews	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/thucnews							
423	SICK	"The Sentences Involving Compositional Knowledge (SICK) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.
Source: Multi-Label Transfer Learning for Multi-Relational Semantic Similarity
Image Source: https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619"	https://paperswithcode.com/dataset/sick	01/01/2014	Sentences Involving Compositional Knowledge					
424	FB15k	"The FB15k dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.
Source: https://www.microsoft.com/en-us/download/details.aspx?id=52312
Image Source: http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf"	https://paperswithcode.com/dataset/fb15k	01/01/2013	Freebase 15K					
425	CJRC	"The Chinese judicial reading comprehension (CJRC) dataset contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. 
Source: CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension"	https://paperswithcode.com/dataset/cjrc		Chinese judicial reading comprehension					
426	HyperLex	"A dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. 
Source: HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment"	https://paperswithcode.com/dataset/hyperlex							
427	DBLP	"The DBLP is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.
The data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc.
Source: https://www.aminer.org/citation"	https://paperswithcode.com/dataset/dblp	01/01/2008	Citation Network Dataset					
428	ACM	"The ACM dataset contains papers published in KDD, SIGMOD, SIGCOMM, MobiCOMM, and VLDB and are divided into three classes (Database, Wireless Communication, Data Mining). An heterogeneous graph is constructed, which comprises 3025 papers, 5835 authors, and 56 subjects. Paper features correspond to elements of a bag-of-words represented of keywords.
Source: https://arxiv.org/pdf/1903.07293.pdf"	https://paperswithcode.com/dataset/acm		"Association for Computing Machinery
Active Contour Model
algebraic collective model
and-Compare Module
Active Contour Models"					
429	FNC-1	"FNC-1 was designed as a stance detection dataset and it contains 75,385 labeled headline and article pairs. The pairs are labelled as either agree, disagree, discuss, and unrelated. Each headline in the dataset is phrased as a statement
Source: Investigating Rumor News Using Agreement-Aware Search
Image Source: http://www.fakenewschallenge.org/"	https://paperswithcode.com/dataset/fnc-1		Fake News Challenge Stage 1					
430	GYAFC	"Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.
Yahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. 
The Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly
across different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus
which consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.
Source: Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"	https://paperswithcode.com/dataset/gyafc	17/03/2018	Grammarly’s Yahoo Answers Formality Corpus					
431	AIDS	"AIDS is a graph dataset. It consists of 2000 graphs representing molecular compounds which are constructed from the AIDS Antiviral Screen Database of Active Compounds. It contains 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI.
Source: DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model
Image Source: https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222"	https://paperswithcode.com/dataset/aids	01/01/2008	AIDS					
432	Sydney Urban Objects	"This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees.
It was collected in order to test matching and classification algorithms. It aims to provide non-ideal sensing conditions that are representative of practical urban sensing systems, with a large variability in viewpoint and occlusion.
Source: http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml
Image Source: http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml"	https://paperswithcode.com/dataset/sydney-urban-objects							
433	Digits	"The DIGITS dataset consists of 1797 8×8 grayscale images (1439 for training and 360 for testing) of handwritten digits.
Source: Differentially Private Variational Dropout
Image Source: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html"	https://paperswithcode.com/dataset/digits		Optical Recognition of Handwritten Digits					
434	Mutagenicity	"Mutagenicity is a chemical compound dataset of drugs, which can be categorized into two classes: mutagen and non-mutagen.
Source: Hierarchical Graph Pooling with Structure Learning"	https://paperswithcode.com/dataset/mutagenicity							
435	SIDER	"SIDER contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug–target relations.
Source: Side Effect Resource
Image Source: http://sideeffects.embl.de/drugs/2756/"	https://paperswithcode.com/dataset/sider		SIDER					
436	RCV1	"The RCV1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.
Source: Random Projections for Linear Support Vector Machines
Image Source: https://www.nasdaq.com/publishers/reuters"	https://paperswithcode.com/dataset/rcv1	01/01/2004	Reuters Corpus Volume 1					
437	CrossTask	"CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.
Source: CrossTask
Image Source: https://arxiv.org/pdf/1903.08225v2.pdf"	https://paperswithcode.com/dataset/crosstask	01/01/2019	CrossTask					
438	YouCook2	"YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.
Source: http://youcook2.eecs.umich.edu/
Image Source: https://competitions.codalab.org/competitions/20594"	https://paperswithcode.com/dataset/youcook2	01/01/2018						
439	FaceForensics	"FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:


Source-to-Target: where the authors reenact over 1000 videos with new facial expressions extracted from other videos, which e.g. can be used to train a classifier to detect fake images or videos.


Selfreenactment: where the authors use Face2Face to reenact the facial expressions of videos with their own facial expressions as input to get pairs of videos, which e.g. can be used to train supervised generative refinement models."	https://paperswithcode.com/dataset/faceforensics							
440	Stacked MNIST	"The Stacked MNIST dataset is derived from the standard MNIST dataset with an increased number of discrete modes. 240,000 RGB images in the size of 32×32 are synthesized by stacking three random digit images from MNIST along the color channel, resulting in 1,000 explicit modes in a uniform distribution corresponding to the number of possible triples of digits.
Source: Inclusive GAN: Improving Data and Minority Coverage in Generative Models
Image Source: https://arxiv.org/abs/1705.07761"	https://paperswithcode.com/dataset/stacked-mnist	01/01/2017	Stacked MNIST					
441	CARPK	"The Car Parking Lot Dataset (CARPK) contains nearly 90,000 cars from 4 different parking lots collected by means of drone (PHANTOM 3 PROFESSIONAL). The images are collected with the drone-view at approximate 40 meters height. The image set is annotated by bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottom-right points. It is supporting object counting, object localizing, and further investigations with the annotation format in bounding boxes.
Source: https://lafi.github.io/LPN/
Image Source: https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610"	https://paperswithcode.com/dataset/carpk	01/01/2017	car parking lot dataset					
442	Pix3D	"The Pix3D dataset is a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc.
Source: http://pix3d.csail.mit.edu/
Image Source: http://pix3d.csail.mit.edu/"	https://paperswithcode.com/dataset/pix3d	01/01/2018						
443	Cell	"The CELL benchmark is made of fluorescence microscopy images of cell. 
Source: Multi-Domain Adversarial Learning
Image Source: https://arxiv.org/pdf/1903.09239v1.pdf"	https://paperswithcode.com/dataset/cell							
444	FBMS	"The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.
Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/
Image Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/"	https://paperswithcode.com/dataset/fbms	01/01/2014	Freiburg-Berkeley Motion Segmentation					
445	NVGesture	"The NVGesture dataset focuses on touchless driver controlling. It contains 1532 dynamic gestures fallen into 25 classes. It includes 1050 samples for training and 482 for testing. The videos are recorded with three modalities (RGB, depth, and infrared).
Source: Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for Gesture Recognition
Image Source: Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network"	https://paperswithcode.com/dataset/nvgesture-1							
446	SUN09	"The SUN09 dataset consists of 12,000 annotated images with more than 200 object categories. It consists of natural, indoor and outdoor images. Each image contains an average of 7 different annotated objects and the average occupancy of each object is 5% of image size. The frequencies of object categories follow a power law distribution.
Source: A Pooling Approach to Modelling Spatial Relations forImage Retrieval and Annotation
Image Source: http://people.csail.mit.edu/myungjin/HContext.html"	https://paperswithcode.com/dataset/sun09	01/01/2010	SUN09					
447	COIN	"The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.
Source: COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis"	https://paperswithcode.com/dataset/coin							
448	Kinetics-600	"The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.
Source: Learning to Localize Actions from Moments
Image Source: https://towardsdatascience.com/downloading-the-kinetics-dataset-for-human-action-recognition-in-deep-learning-500c3d50f776"	https://paperswithcode.com/dataset/kinetics-600	01/01/2018						
449	AudioSet	"Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.
Source: Curriculum Audiovisual Learning"	https://paperswithcode.com/dataset/audioset	01/01/2017						
450	DIVA-HisDB	The database consists of 150 annotated pages of three different medieval manuscripts with challenging layouts. Furthermore, we provide a layout analysis ground-truth which has been iterated on, reviewed, and refined by an expert in medieval studies.	https://paperswithcode.com/dataset/diva-hisdb	23/10/2016						
451	TDIUC	"Task Directed Image Understanding Challenge (TDIUC) dataset is a Visual Question Answering dataset which consists of 1.6M questions and 170K images sourced from MS COCO and the Visual Genome Dataset. The image-question pairs are split into 12 categories and 4 additional evaluation matrices which help evaluate models’ robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. The TDIUC dataset divides the VQA paradigm into 12 different task directed question types. These include questions that require a simpler task (e.g., object presence, color attribute) and more complex tasks (e.g., counting, positional reasoning). The dataset includes also an “Absurd” question category in which questions are irrelevant to the image contents to help balance the dataset.
Source: Question-Agnostic Attention for Visual Question Answering
Image Source: https://kushalkafle.com/projects/tdiuc.html"	https://paperswithcode.com/dataset/tdiuc	01/01/2017	Task Directed Image Understanding Challenge					
452	Mall	"The Mall is a dataset for crowd counting and profiling research. Its images are collected from publicly accessible webcam. It mainly includes 2,000 video frames, and the head position of every pedestrian in all frames is annotated. A total of more than 60,000 pedestrians are annotated in this dataset.
Source: Drone Based RGBT Vehicle Detection and Counting: A Challenge
Image Source: http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf"	https://paperswithcode.com/dataset/mall	01/01/2012	Mall Dataset					
453	A3D	"A new dataset of diverse traffic accidents.
Source: Unsupervised Traffic Accident Detection in First-Person Videos"	https://paperswithcode.com/dataset/a3d		AnAn Accident Detection					
454	FRGC	"The data for FRGC consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor.
Source: https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc
Image Source: https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105"	https://paperswithcode.com/dataset/frgc	01/01/2005	Face Recognition Grand Challenge					
455	HAR	"The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects.
Source: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
Image Source: https://www.youtube.com/watch?v=XOEN9W05_4A"	https://paperswithcode.com/dataset/har	01/01/2013	Human Activity Recognition Using Smartphones					
456	MOT15	"MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.
Source: FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking
Image Source: https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377"	https://paperswithcode.com/dataset/mot15	01/01/2015	Multiple Object Tracking 15					
457	CASIA-MFSD	"CASIA-MFSD is a dataset for face anti-spoofing. It contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing.
Source: Improving Face Anti-Spoofing by 3D Virtual Synthesis
Image Source: https://link.springer.com/referenceworkentry/10.1007%2F978-1-4899-7488-4_9067"	https://paperswithcode.com/dataset/casia-mfsd	01/01/2012	CASIA-MFSD					
458	Replay-Attack	"The Replay-Attack Database for face spoofing consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds.
Source: https://www.idiap.ch/dataset/replayattack
Image Source: https://www.researchgate.net/figure/Sample-images-from-the-PRINT-ATTACK-1-and-REPLAY-ATTACK-12-databases-Top-and-bottom_fig1_330400888"	https://paperswithcode.com/dataset/replay-attack	01/01/2012						
459	Delicious	"Delicious : This data set contains tagged web pages retrieved from the website delicious.com.
Source: Text segmentation on multilabel documents: A distant-supervised approach
Image Source: http://mlkd.csd.auth.gr/multilabel.html"	https://paperswithcode.com/dataset/delicious		Delicious					
460	WeChat	The WeChat dataset for fake news detection contains more than 20k news labelled as fake news or not.	https://paperswithcode.com/dataset/wechat							
461	RAF-DB	"The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.
Source: Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition
Image Source: http://www.whdeng.cn/raf/model1.html"	https://paperswithcode.com/dataset/raf-db	01/01/2017	Real-world Affective Faces					
462	FERG	"FERG is a database of cartoon characters with annotated facial expressions containing 55,769 annotated face images of six characters. The images for each character are grouped into 7 types of cardinal expressions, viz. anger, disgust, fear, joy, neutral, sadness and surprise.
Source: VGAN-Based Image Representation Learningfor Privacy-Preserving Facial Expression Recognition
Image Source: http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html"	https://paperswithcode.com/dataset/ferg	01/01/2016	Facial Expression Research Group Database					
463	COCO-Text	"The COCO-Text dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text.
Source: Improving Text Proposals for Scene Images with Fully Convolutional Networks
Image Source: https://vision.cornell.edu/se3/coco-text-2/"	https://paperswithcode.com/dataset/coco-text	01/01/2016	COCO-Text					
464	DiscoFuse	"DiscoFuse was created by applying a rule-based splitting method on two corpora -
sports articles crawled from the Web, and Wikipedia. See the paper for a detailed
description of the dataset generation process and evaluation.
DiscoFuse has two parts with 44,177,443 and 16,642,323 examples sourced from Sports articles and Wikipedia, respectively.
For each part, a random split is provided to train (98% of the examples), development (1%) and test (1%) sets. In addition, as the original data distribution is highly skewed (see details in the paper), a balanced version for each part is also provided.
Source: Google Research"	https://paperswithcode.com/dataset/discofuse	27/02/2019	DiscoFuse					
465	FIGER	"The FIGER dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as person/doctor, art/written_work and building/hotel. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually.
Source: http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152"	https://paperswithcode.com/dataset/figer	01/01/2012	Fine-Grained Entity Recognition					
466	CUHK-SYSU	"The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery.
Source: http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html
Image Source: http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html"	https://paperswithcode.com/dataset/cuhk-sysu	01/01/2017	CUHK-SYSU Person Search Dataset					
467	Chairs	"The Chairs dataset contains rendered images of around 1000 different three-dimensional chair models.
Source: Adversarial Disentanglement with Grouped Observations
Image Source: https://www.di.ens.fr/willow/research/seeing3Dchairs/"	https://paperswithcode.com/dataset/chairs	01/01/2014						
468	ZINC	"ZINC is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs.
Source: ZINC15
Image Source: https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559"	https://paperswithcode.com/dataset/zinc	01/01/2012	ZINC					
469	QED	"QED is a linguistically principled framework for explanations in question answering. Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps:
sentence selection := identification of a sentence implying an answer to the question
referential equality := identification of noun phrases in the question and the answer sentence that refer to the same thing
predicate entailment := confirmation that the predicate in the sentence entails the predicate in the question once referential equalities are abstracted away.
The QED dataset is an expert-annotated dataset of QED explanations build upon a subset of the Google Natural Questions dataset.
Source: https://github.com/google-research-datasets/QED
Image Source: https://github.com/google-research-datasets/QED"	https://paperswithcode.com/dataset/qed							
470	MEF	"Multi-exposure image fusion (MEF) is considered
an effective quality enhancement technique widely adopted in
consumer electronics, but little work has been dedicated to the
perceptual quality assessment of multi-exposure fused images.
In this paper, we first build an MEF database and carry
out a subjective user study to evaluate the quality of images
generated by different MEF algorithms. There are several useful
findings. First, considerable agreement has been observed among
human subjects on the quality of MEF images. Second, no single
state-of-the-art MEF algorithm produces the best quality for
all test images. Third, the existing objective quality models for
general image fusion are very limited in predicting perceived
quality of MEF images. Motivated by the lack of appropriate
objective models, we propose a novel objective image quality
assessment (IQA) algorithm for MEF images based on the
principle of the structural similarity approach and a novel
measure of patch structural consistency. Our experimental results
on the subjective database show that the proposed model well
correlates with subjective judgments and significantly outperforms the existing IQA models for general image fusion. Finally,
we demonstrate the potential application of the proposed model
by automatically tuning the parameters of MEF algorithms"	https://paperswithcode.com/dataset/mef		Multi-exposure image fusion					
471	DICM	"DICM is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.
Source: Deep Retinex Decomposition for Low-Light Enhancement
Image Source: GLADNet: Low-Light Enhancement Network with Global Awareness"	https://paperswithcode.com/dataset/dicm	01/01/2012	DICM					
472	GuessWhat?!	"GuessWhat?! is a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images.
GuessWhat?! is a cooperative two-player game in which
both players see the picture of a rich visual scene with several objects. One player – the oracle – is randomly assigned
an object (which could be a person) in the scene. This object is not known by the other player – the questioner –
whose goal it is to locate the hidden object. To do so, the
questioner can ask a series of yes-no questions which are
answered by the oracle.
Source: https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through
Image Source: Vries et al"	https://paperswithcode.com/dataset/guesswhat							
473	ObjectNet	"ObjectNet is a test set of images collected directly using crowd-sourcing. ObjectNet is unique as the objects are captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance. There are 50,000 images in the test set which controls for rotation, background and viewpoint. There are 313 object classes with 113 overlapping ImageNet.
Source: On Robustness and Transferability of Convolutional Neural Networks
Image Source: https://objectnet.dev/"	https://paperswithcode.com/dataset/objectnet	01/01/2019						
474	ActivityNet Captions	"The ActivityNet Captions dataset is built on ActivityNet v1.3 which includes 20k YouTube untrimmed videos with 100k caption annotations. The videos are 120 seconds long on average. Most of the videos contain over 3 annotated events with corresponding start/end time and human-written sentences, which contain 13.5 words on average. The number of videos in train/validation/test split is 10024/4926/5044, respectively.
Source: Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning
Image Source: https://cs.stanford.edu/people/ranjaykrishna/densevid/"	https://paperswithcode.com/dataset/activitynet-captions	01/01/2017						
475	smallNORB	"The smallNORB dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).
The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).
Source: https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/
Image Source: https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10"	https://paperswithcode.com/dataset/smallnorb	01/01/2004						
476	DocRED	"DocRED (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data.
DocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. In addition to the human-annotated data, the dataset provides large-scale distantly supervised data over 101,873 documents.
Source: DocRED: A Large-Scale Document-Level Relation Extraction Dataset
Image Source: DocRED: A Large-Scale Document-Level Relation Extraction Dataset"	https://paperswithcode.com/dataset/docred	01/01/2019						
477	iMaterialist	"Constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes.
Source: The iMaterialist Fashion Attribute Dataset"	https://paperswithcode.com/dataset/imaterialist							
478	ImageNet-C	"ImageNet-C is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set.
Source: Selective Brain Damage: Measuring the Disparate Impact of Model Pruning
Image Source: https://arxiv.org/pdf/1807.01697.pdf"	https://paperswithcode.com/dataset/imagenet-c	01/01/2019	ImageNet-C					
479	ImageNet-A	"The ImageNet-A dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models.
Source: On Robustness and Transferability of Convolutional Neural Networks
Image Source: https://github.com/hendrycks/natural-adv-examples"	https://paperswithcode.com/dataset/imagenet-a	01/01/2019						
480	BIOSSES	"The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the ""TAC2 Biomedical Summarization Track Training Data Set"" .
The sentence pairs were evaluated by five different human experts that judged their similarity and gave scores in a range [0-4]. Our guideline was prepared based on SemEval 2012 Task 6 Guideline.
Image source: BIOSSES"	https://paperswithcode.com/dataset/biosses	15/07/2017	Biomedical Semantic Similarity Estimation System					
481	MedNLI	"The MedNLI dataset consists of the sentence pairs developed by Physicians from the Past Medical History section of MIMIC-III clinical notes annotated for Definitely True, Maybe True and Definitely False. The dataset contains 11,232 training, 1,395 development and 1,422 test instances. This provides a natural language inference task (NLI) grounded in the medical history of patients.
Source: MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning
Image Source: https://arxiv.org/abs/1904.02181"	https://paperswithcode.com/dataset/mednli		Medical Natural Language Inference					
482	UCF-QNRF	"The UCF-QNRF dataset is a crowd counting dataset and it contains large diversity both in scenes, as well as in background types. It consists of 1535 images high-resolution images from Flickr, Web Search and Hajj footage. The number of people (i.e., the count) varies from 50 to 12,000 across images.
Source: Understanding the impact of mistakes on background regions in crowd counting
Image Source: https://www.crcv.ucf.edu/data/ucf-qnrf/"	https://paperswithcode.com/dataset/ucf-qnrf	01/01/2018						
483	WiderPerson	"WiderPerson contains a total of 13,382 images with 399,786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild.
Source: WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild
Image Source: http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/"	https://paperswithcode.com/dataset/widerperson							
484	CID	"The CID (Campus Image Dataset) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst.
Source: https://github.com/505030475/ExtremeLowLight"	https://paperswithcode.com/dataset/cid		Campus Image Dataset					
485	LeNER-Br	LeNER-Br is a dataset for named entity recognition (NER) in Brazilian Legal Text.	https://paperswithcode.com/dataset/lener-br							
486	DAVIS	"The Densely Annotation Video Segmentation dataset (DAVIS) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.
Source: TENet: Triple Excitation Network for Video Salient Object Detection"	https://paperswithcode.com/dataset/davis	01/01/2016	Densely Annotated VIdeo Segmentation					
487	VIST	"The Visual Storytelling Dataset (VIST) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person.
Source: Stories for Images-in-Sequence by using Visual and Narrative Components This research was partially funded by Pendulibrium and the Faculty of computer science and engineering, Ss. Cyril and Methodius University in Skopje.
Image Source: https://arxiv.org/pdf/1604.03968.pdf"	https://paperswithcode.com/dataset/vist	01/01/2013	Visual Storytelling					
488	DTD	"The Describable Textures Dataset (DTD) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.
Source: Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting
Image Source: https://www.robots.ox.ac.uk/~vgg/data/dtd/"	https://paperswithcode.com/dataset/dtd	01/01/2014	Describable Textures Dataset					
489	Adience	"The Adience dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.
Source: Understanding and Comparing Deep Neural Networksfor Age and Gender Classification
Image Source: https://talhassner.github.io/home/projects/Adience/Adience-data.html"	https://paperswithcode.com/dataset/adience	01/01/2014						
490	Matterport3D	"The Matterport3D dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.
Source: Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention"	https://paperswithcode.com/dataset/matterport3d	01/01/2017						
491	ToTTo	"ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.
During the dataset creation process, tables from English Wikipedia are matched with (noisy) descriptions. Each table cell mentioned in the description is highlighted and the descriptions are iteratively cleaned and corrected to faithfully reflect the content of the highlighted cells.
Source: Google Research Datasets"	https://paperswithcode.com/dataset/totto	29/04/2020	ToTTo					
492	PCam	PatchCamelyon is an image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than ImageNet, trainable on a single GPU.	https://paperswithcode.com/dataset/pcam		PatchCamelyon					
493	Kumar	"The Kumar dataset contains 30 1,000×1,000 image tiles from seven organs (6 breast, 6 liver, 6 kidney, 6 prostate, 2 bladder, 2 colon and 2 stomach) of The Cancer Genome Atlas (TCGA) database acquired at 40× magnification. Within each image, the boundary of each nucleus is fully annotated.
Source: Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in Histology Images"	https://paperswithcode.com/dataset/kumar							
494	HellaSwag	HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).	https://paperswithcode.com/dataset/hellaswag							
495	LAMBADA	"The LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.
Source: Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches
Image Source: https://arxiv.org/pdf/1606.06031.pdf"	https://paperswithcode.com/dataset/lambada	01/01/2016						
496	PIQA	"PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP. 
Source: PIQA"	https://paperswithcode.com/dataset/piqa	26/11/2019	Physical Interaction: Question Answering					
497	OpenBookQA	"OpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small “book” of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.
Additionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID.
Source: https://allenai.org/data/open-book-qa
Image Source: https://arxiv.org/pdf/1809.02789.pdf"	https://paperswithcode.com/dataset/openbookqa	01/01/2018						
498	WSC	"The Winograd Schema Challenge was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.
The original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as WSC285 and WSC273, respectively.
Source: https://arxiv.org/pdf/2004.13831.pdf
Image Source: https://arxiv.org/pdf/1907.11983.pdf"	https://paperswithcode.com/dataset/wsc	01/01/2012	Winograd Schema Challenge					
499	arXiv	"Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.
The data covers papers in the period from January 1993 to April 2003 (124 months).
Source: https://snap.stanford.edu/data/cit-HepTh.html"	https://paperswithcode.com/dataset/arxiv		Arxiv HEP-TH (high energy physics theory) citation graph					
500	LEVIR-CD	"LEVIR-CD is a new large-scale remote sensing building Change Detection dataset. The introduced dataset would be a new benchmark for evaluating change detection (CD) algorithms, especially those based on deep learning.
LEVIR-CD consists of 637 very high-resolution (VHR, 0.5m/pixel) Google Earth (GE) image patch pairs with a size of 1024 × 1024 pixels. These bitemporal images with time span of 5 to 14 years have significant land-use changes, especially the construction growth. LEVIR-CD covers various types of buildings, such as villa residences, tall apartments, small garages and large warehouses. Here, we focus on building-related changes, including the building growth (the change from soil/grass/hardened ground or building under construction to new build-up regions) and the building decline. These bitemporal images are annotated by remote sensing image interpretation experts using binary labels (1 for change and 0 for unchanged). Each sample in our dataset is annotated by one annotator and then double-checked by another to produce high-quality annotations. The fully annotated LEVIR-CD contains a total of 31,333 individual change-building instances."	https://paperswithcode.com/dataset/levir-cd	22/05/2020						
501	FEVER	"FEVER is a publicly available dataset for fact extraction and verification against textual sources.
It consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.
The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was
extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.
Source: FEVER: a large-scale dataset for Fact Extraction and VERification"	https://paperswithcode.com/dataset/fever	14/03/2018	Fact Extraction and VERification					
502	MELD	"Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.
Source: https://affective-meld.github.io/
Image Source: https://affective-meld.github.io/"	https://paperswithcode.com/dataset/meld	01/01/2019	Multimodal EmotionLines Dataset					
503	EmoryNLP	EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.	https://paperswithcode.com/dataset/emorynlp	14/08/2017						
504	4D Light Field Dataset	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/4d-light-field-dataset							
505	Virtual KITTI 2	Virtual KITTI 2 is an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15◦). For each sequence we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset’s capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.	https://paperswithcode.com/dataset/virtual-kitti-2	29/01/2020						
506	WHAMR!	"WHAMR! is a dataset for noisy and reverberant speech separation. It extends WHAM! by introducing synthetic reverberation to the
speech sources in addition to the existing noise. Room impulse responses were generated and convolved using pyroomacoustics. Reverberation times were chosen to approximate domestic and classroom environments (expected to be similar to the restaurants and coffee shops where the WHAM! noise was collected), and
further classified as high, medium, and low reverberation based on a
qualitative assessment of the mixture’s noise recording."	https://paperswithcode.com/dataset/whamr	22/10/2019	WHAM! with synthetic reverberated sources					
507	DEMAND	"The DEMAND (Diverse Environments Multichannel Acoustic Noise Database) provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.
Source: DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments
Image Source: https://asa.scitation.org/doi/pdf/10.1121/1.4799597"	https://paperswithcode.com/dataset/demand		Diverse Environments Multi-channel Acoustic Noise Database					
508	BUFF	"BUFF consists of 5 subjects, 3 male and 2 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit.
They perform 3 different motions i) hips ii) tilt_twist_left iii) shoulders_mill.
Source: http://buff.is.tue.mpg.de/
Image Source: http://buff.is.tue.mpg.de/"	https://paperswithcode.com/dataset/buff	01/01/2017	Bodies Under Flowing Fashion					
509	Taskonomy	"Taskonomy provides a large and high-quality dataset of varied indoor scenes.

Complete pixel-level geometric information via aligned meshes.
Semantic information via knowledge distillation from ImageNet, MS COCO, and MIT Places.
Globally consistent camera poses. Complete camera intrinsics.
High-definition images.
3x times big as ImageNet.

Source: Taskonomy
Image Source: http://taskonomy.stanford.edu/"	https://paperswithcode.com/dataset/taskonomy							
510	Abalone	"Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.
Source: UCL Machine Learning Repository
Image Source: http://archive.ics.uci.edu/ml/datasets/Abalone"	https://paperswithcode.com/dataset/abalone		Abalone					
511	Letter	"Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.
Source: UCL Machine Learning Repository Letter Recognition
Image Source: http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf"	https://paperswithcode.com/dataset/letter	01/01/1991	Letter Recognition Data Set					
512	Electricity	"Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.
Data Set Information:
This archive contains 2075259 measurements gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).
Notes:
1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.
2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.
Attribute Information:
1.date: Date in format dd/mm/yyyy
2.time: time in format hh:mm:ss
3.global_active_power: household global minute-averaged active power (in kilowatt)
4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)
5.voltage: minute-averaged voltage (in volt)
6.global_intensity: household global minute-averaged current intensity (in ampere)
7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).
8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.
9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.
We suggest the following pseudo-APA reference format for referring to this repository:
Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
Here is a BiBTeX citation as well:
@misc{Dua:2019 ,
author = ""Dua, Dheeru and Graff, Casey"",
year = ""2017"",
title = ""{UCI} Machine Learning Repository"",
url = ""http://archive.ics.uci.edu/ml"",
institution = ""University of California, Irvine, School of Information and Computer Sciences"" }"	https://paperswithcode.com/dataset/electricity	13/12/2021	Individual household electric power consumption Data Set					
513	NetHack Learning Environment	"The NetHack Learning Environment (NLE) is a Reinforcement Learning environment based on NetHack 3.6.6. It is designed to provide a standard reinforcement learning interface to the game, and comes with tasks that function as a first step to evaluate agents on this new environment.
NetHack is one of the oldest and arguably most impactful videogames in history, as well as being one of the hardest roguelikes currently being played by humans. It is procedurally generated, rich in entities and dynamics, and overall an extremely challenging environment for current state-of-the-art RL agents, while being much cheaper to run compared to other challenging testbeds. Through NLE, the authors wish to establish NetHack as one of the next challenges for research in decision making and machine learning.
Source: https://github.com/facebookresearch/nle
Image Source: https://github.com/facebookresearch/nle"	https://paperswithcode.com/dataset/nethack-learning-environment		NetHack Learning Environment					
514	Kvasir-SEG	"Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. 
Source: Kvasir-SEG: A Segmented Polyp Dataset
Image Source: https://datasets.simula.no/kvasir-seg/"	https://paperswithcode.com/dataset/kvasir-seg							
515	2018 Data Science Bowl	"This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.
Each image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders:
images contains the image file.
masks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).
The second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details.
As with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set."	https://paperswithcode.com/dataset/2018-data-science-bowl	18/07/2018	2018 Data Science Bowl Find the nuclei in divergent images to advance medical discovery					
516	CVC-ClinicDB	"CVC-ClinicDB is an open-access dataset of 612 images with a resolution of 384×288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos.
Source: ResUNet++: An Advanced Architecture for Medical Image Segmentation
Image Source: https://polyp.grand-challenge.org/CVCClinicDB/"	https://paperswithcode.com/dataset/cvc-clinicdb							
517	CAT2000	"Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.
Source: CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research"	https://paperswithcode.com/dataset/cat2000							
518	FixaTons	"FixaTons is a large collection of datasets human scanpaths (temporally ordered sequences of fixations) and saliency maps. 
Source: FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity"	https://paperswithcode.com/dataset/fixatons							
519	ImageNet-R	"ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.
ImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images.
Source: The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"	https://paperswithcode.com/dataset/imagenet-r		ImageNet-Rendition					
520	20 Newsgroups	The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.	https://paperswithcode.com/dataset/20-newsgroups							
521	HACS	"HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.
Authors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled
from 492K, 6K and 6K videos, respectively."	https://paperswithcode.com/dataset/hacs		Human Action Clips and Segments					
522	Kinetics-700	Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.	https://paperswithcode.com/dataset/kinetics-700	15/07/2019	Kinetics-700					
523	Completion3D	"The Completion3D benchmark is a dataset for evaluating state-of-the-art 3D Object Point Cloud Completion methods. Ggiven a partial 3D object point cloud the goal is to infer a complete 3D point cloud for the object.
Source: TopNet: Structural Point Cloud Decoder"	https://paperswithcode.com/dataset/completion3d							
524	QMNIST	"The exact pre-processing steps used to construct the MNIST dataset have long been lost. This leaves us with no reliable way to associate its characters with the ID of the writer and little hope to recover the full MNIST testing set that had 60K images but was never released. The official MNIST testing set only contains 10K randomly sampled images and is often considered too small to provide meaningful confidence intervals.
The QMNIST dataset was generated from the original data found in the NIST Special Database 19 with the goal to match the MNIST preprocessing as closely as possible.
QMNIST is licensed under the BSD-style license.
Source: https://github.com/facebookresearch/qmnist
Image Source: https://github.com/facebookresearch/qmnist"	https://paperswithcode.com/dataset/qmnist							
525	ROCStories	"ROCStories is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871.
Source: Incorporating Structured Commonsense Knowledge in Story Completion
Image Source: https://cs.rochester.edu/nlp/rocstories/"	https://paperswithcode.com/dataset/rocstories	01/01/2016						
526	ePillID	"ePillID is a benchmark for developing and evaluating computer vision models for pill identification. The ePillID benchmark is designed as a low-shot fine-grained benchmark, reflecting real-world challenges for developing image-based pill identification systems.
The characteristics of the ePillID benchmark include:
* Reference and consumer images: The reference images are taken with controlled lighting and backgrounds, and with professional equipment. The consumer images are taken with real-world settings including different lighting, backgrounds, and equipment. For most of the pills, one image per side (two images per pill type) is available from the NIH Pillbox dataset.
* Low-shot and fine-grained setting: 13k images representing 9804 appearance classes (two sides for 4902 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting.
Source: https://github.com/usuyama/ePillID-benchmark
Image Source: https://github.com/usuyama/ePillID-benchmark"	https://paperswithcode.com/dataset/epillid							
527	CodeSearchNet	"The CodeSearchNet Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes:
* Six million methods overall
* Two million of which have associated documentation (docstrings, JavaDoc, and more)
* Metadata that indicates the original location (repository or line number, for example) where the data was found
Source: https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/"	https://paperswithcode.com/dataset/codesearchnet							
528	WikiTableQuestions	"WikiTableQuestions is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions.
Source: Explaining Queries over Web Tables to Non-Experts
Image Source: https://ppasupat.github.io/WikiTableQuestions/"	https://paperswithcode.com/dataset/wikitablequestions	01/01/2015						
529	AViD	"Is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries.
Source: AViD Dataset: Anonymized Videos from Diverse Countries"	https://paperswithcode.com/dataset/avid							
530	MTL-AQA	"A new multitask action quality assessment (AQA) dataset, the largest to date, comprising of more than 1600 diving samples; contains detailed annotations for  fine-grained action recognition, commentary generation, and estimating the AQA score. Videos from multiple angles provided wherever available.
Source: What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment"	https://paperswithcode.com/dataset/mtl-aqa							
531	AQA-7	"Consists of 1106 action samples from seven actions with quality scores as measured by expert human judges.
Source: Action Quality Assessment Across Multiple Actions"	https://paperswithcode.com/dataset/aqa-7							
532	AGENDA	"Abstract GENeration DAtaset (AGENDA) is a dataset of knowledge graphs paired with scientific abstracts. The dataset consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences.
Source: Text Generation from Knowledge Graphs with Graph Transformers"	https://paperswithcode.com/dataset/agenda	04/04/2019	Abstract GENeration DAtaset					
533	GoPro	"The GoPro dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth shapr image that are obtained by a high-speed camera.
Source: Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networksfor Non-Uniform Single Image Deblurring
Image Source: Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring"	https://paperswithcode.com/dataset/gopro	01/01/2017						
534	AMZ Computers	AMZ Computers is a co-purchase graph extracted from Amazon, where nodes represent products, edges represent the co-purchased relations of products, and features are bag-of-words vectors extracted from product reviews.	https://paperswithcode.com/dataset/amz-computers		amazon_electronics_computers					
535	SVG-Icons8	"A new large-scale dataset along with an open-source library for SVG manipulation.
Source: DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation"	https://paperswithcode.com/dataset/svg-icons8							
536	K2HPD	"Includes 100K depth images under challenging scenarios.
Source: Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning"	https://paperswithcode.com/dataset/k2hpd							
537	Binarized MNIST	"A binarized version of MNIST.
Source: Binarized MNIST"	https://paperswithcode.com/dataset/binarized-mnist							
538	CAMO	Camouflaged Object (CAMO) dataset specifically designed for the task of camouflaged object segmentation. We focus on two categories, i.e., naturally camouflaged objects and artificially camouflaged objects, which usually correspond to animals and humans in the real world, respectively. Camouflaged object images consists of 1250 images (1000 images for the training set and 250 images for the testing set). Non-camouflaged object images are collected from the MS-COCO dataset (1000 images for the training set and 250 images for the testing set). CAMO has objectness mask ground-truth.	https://paperswithcode.com/dataset/camo	20/05/2021	Camouflaged Object					
539	CAS-VSR-W1k (LRW-1000)	"LRW-1000 has been renamed as CAS-VSR-W1k.* It is a naturally-distributed large-scale benchmark for word-level lipreading in the wild, including 1000 classes with about 718,018 video samples from more than 2000 individual speakers. There are more than 1,000,000 Chinese character instances in total. Each class corresponds to the syllables of a Mandarin word which is composed by one or several Chinese characters. This dataset aims to cover a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications.
Source: VIPL
Image Source: https://arxiv.org/pdf/1810.06990v6.pdf"	https://paperswithcode.com/dataset/lrw-1000	01/01/2019	CAS-VSR-W1k (LRW-1000)					
540	LRS2	"The Oxford-BBC Lip Reading Sentences 2 (LRS2) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.
Source: Audio-visual Recognition of Overlapped speech for the LRS2 dataset
Image Source: https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html"	https://paperswithcode.com/dataset/lrs2	01/01/2017	Lip Reading Sentences 2					
541	Moving MNIST	"The Moving MNIST dataset contains 10,000 video sequences, each consisting of 20 frames. In each video sequence, two digits move independently around the frame, which has a spatial resolution of 64×64 pixels. The digits frequently intersect with each other and bounce off the edges of the frame
Source: Mutual Suppression Network for Video Prediction using Disentangled Features
Image Source: http://www.cs.toronto.edu/~nitish/unsupervised_video/"	https://paperswithcode.com/dataset/moving-mnist	01/01/2015						
542	Sprites	"The Sprites dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.
Source: Challenges in Disentangling Independent Factors of Variation"	https://paperswithcode.com/dataset/sprites	01/01/2015						
543	BigPatent	"Consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.
Source: BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization"	https://paperswithcode.com/dataset/bigpatent							
544	NoW Benchmark	"The goal of this benchmark is to introduce a standard evaluation metric to measure the accuracy and robustness of 3D face reconstruction methods under variations in viewing angle, lighting, and common occlusions. 
The dataset contains 2054 2D images of 100 subjects, captured with an iPhone X, and a separate 3D head scan for each subject. This head scan serves as ground truth for the evaluation. The subjects are selected to contain variations in age, BMI, and sex (55 female, 45 male)."	https://paperswithcode.com/dataset/now-benchmark	16/05/2019						
545	WikiHow	"WikiHow is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles.
Source: WikiHow: A Large Scale Text Summarization Dataset
Image Source: WikiHow: A Large Scale Text Summarization Dataset"	https://paperswithcode.com/dataset/wikihow	01/01/2018						
546	Horse-10	Horse-10 is an animal pose estimation dataset. It comprises 30 diverse Thoroughbred horses, for which 22 body parts were labeled by an expert in 8,114 frames (animal pose estimation). Horses have various coat colors and the “in-the-wild” aspect of the collected data at various Thoroughbred yearling sales and farms added additional complexity.  The authors introduce Horse-C to contrast the domain shift inherent in the Horse-10 dataset with domain shift induced by common image corruptions.	https://paperswithcode.com/dataset/horse-10	24/09/2019						
547	FreiHAND	"FreiHAND is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples.
Source: Knowledge as Priors: Cross-Modal Knowledge Generalizationfor Datasets without Superior Knowledge
Image Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html"	https://paperswithcode.com/dataset/freihand	01/01/2019	FreiHAND					
548	DomainNet	"DomainNet is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game “Quick Draw!”.
Source: What is being transferred in transfer learning?
Image Source: http://ai.bu.edu/M3SDA/"	https://paperswithcode.com/dataset/domainnet	01/01/2019	DomainNet					
549	Skeleton-Mimetics	"A dataset derived from the recently introduced Mimetics dataset.
Source: Quo Vadis, Skeleton Action Recognition ?"	https://paperswithcode.com/dataset/skeleton-mimetics							
550	Universal Dependencies	"The Universal Dependencies (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels.
Source: Evaluating Contextualized Embeddings on 54 Languagesin POS Tagging, Lemmatization and Dependency Parsing
Image Source: https://universaldependencies.org/introduction.html"	https://paperswithcode.com/dataset/universal-dependencies	01/01/2016						
551	TallyQA	"TallyQA is a large-scale dataset for open-ended counting.
Source: TallyQA: Answering Complex Counting Questions"	https://paperswithcode.com/dataset/tallyqa							
552	CrisisMMD	CrisisMMD is a large multi-modal dataset collected from Twitter during different natural disasters. It consists of several thousands of manually annotated tweets and images collected during seven major natural disasters including earthquakes, hurricanes, wildfires, and floods that happened in the year 2017 across different parts of the World. The provided datasets include three types of annotations.	https://paperswithcode.com/dataset/crisismmd	02/05/2018						
553	UAVA	"The UAVA,<i>UAV-Assistant</i>, dataset is specifically designed for fostering applications which consider UAVs and humans as cooperative agents.
We employ a real-world 3D scanned dataset (<a href=""https://niessner.github.io/Matterport/"">Matterport3D</a>), physically-based rendering, a gamiﬁed simulator for realistic drone navigation trajectory collection, to generate realistic multimodal data both from the user’s exocentric view of the drone, as well as the drone’s egocentric view."	https://paperswithcode.com/dataset/uava	20/08/2020	UAV Assistant					
554	CMU Panoptic	"CMU Panoptic is a large scale dataset providing 3D pose annotations for multiple people engaging social activities. It contains 65 videos with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters.
Source: Single-Stage Multi-Person Pose Machines
Image Source: http://domedb.perception.cs.cmu.edu/"	https://paperswithcode.com/dataset/cmu-panoptic	01/01/2015	CMU Panoptic Dataset					
555	Set5	"The Set5 dataset is a dataset consisting of 5 images (“baby”, “bird”, “butterfly”, “head”, “woman”) commonly used for testing performance of Image Super-Resolution models.
Image Source: http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html"	https://paperswithcode.com/dataset/set5	01/01/2012						
556	ContactPose	"ContactPose is a dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. 
Source: ContactPose: A Dataset of Grasps with Object Contact and Hand Pose"	https://paperswithcode.com/dataset/contactpose							
557	DHF1K	"DHF1K is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server.
Source: ViP: Video Platform for PyTorch
Image Source: https://arxiv.org/pdf/1801.07424.pdf"	https://paperswithcode.com/dataset/dhf1k	01/01/2018						
558	How2	"The How2 dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations.
Source: exploring multiview correlations in open-domain videos"	https://paperswithcode.com/dataset/how2	01/01/2018						
559	ASSET	"ASSET is a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.
Source: ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations"	https://paperswithcode.com/dataset/asset	01/05/2020	ASSET					
560	TurkCorpus	"TurkCorpus, a dataset with 2,359 original sentences from English Wikipedia, each with 8 manual reference simplifications.
The dataset is divided into two subsets: 2,000 sentences for validation and 359 for testing of sentence simplification models."	https://paperswithcode.com/dataset/turkcorpus	01/01/2016						
561	MLFP	"The MLFP dataset consists of face presentation attacks captured with seven 3D latex masks and three 2D print attacks. The dataset contains videos captured from color, thermal and infrared channels.
Source: Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks
Image Source: http://iab-rubric.org/papers/2017_cvprw_18.pdf"	https://paperswithcode.com/dataset/mlfp	01/01/2017	Multispectral Latex Mask based Video Face Presentation Attack					
562	CoNLL++	"CoNLL++ is a corrected version of the CoNLL03 NER dataset where 5.38% of the test sentences have been fixed.
Source: CrossWeigh: Training Named Entity Tagger from Imperfect Annotations"	https://paperswithcode.com/dataset/conll							
563	SOC	SOC (Salient Objects in Clutter) is a dataset for Salient Object Detection (SOD). It includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes.	https://paperswithcode.com/dataset/soc	16/03/2018	Salient Objects in Clutter					
564	CoSal2015	"Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection.
Source: Adaptive Graph Convolutional Network with Attention Graph Clustering for Co-saliency Detection
Image Source: https://arxiv.org/pdf/1604.07090.pdf"	https://paperswithcode.com/dataset/cosal2015	01/01/2016	CoSal2015					
565	SIP	"The Salient Person dataset (SIP) contains 929 salient person samples with different poses and illumination conditions.
Source: Accurate RGB-D Salient Object Detection via Collaborative Learning
Image Source: https://arxiv.org/pdf/1907.06781.pdf"	https://paperswithcode.com/dataset/sip	01/01/2019	Salient Person					
566	NJU2K	"NJU2K is a large RGB-D dataset containing 1,985 image pairs. The stereo images were collected from the Internet and 3D movies, while photographs were taken by a Fuji W3 camera.
Source: Bifurcated Backbone Strategy for RGB-D Salient Object Detection
Image Source: Depth saliency based on anisotropic center-surround difference"	https://paperswithcode.com/dataset/nju2k	01/01/2014						
567	NLPR	"The NLPR dataset for salient object detection consists of 1,000 image pairs captured by a standard Microsoft Kinect with a resolution of 640×480. The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets).
Source: Bifurcated Backbone Strategy for RGB-D Salient Object Detection
Image Source: https://sites.google.com/site/rgbdsaliency/dataset"	https://paperswithcode.com/dataset/nlpr	01/01/2014						
568	DES	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/des							
569	LFSD	"The Light Field Saliency Database (LFSD) contains 100 light fields with 360×360 spatial resolution. A rough focal stack and an all-focus image are provided for each light field. The images in this dataset usually have one salient foreground object and a background with good color contrast.
Source: Light Field Saliency Detection with Deep Convolutional Networks
Image Source: https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/"	https://paperswithcode.com/dataset/lfsd	01/01/2014	Light Field Saliency Database					
570	Cam2BEV	"The dataset contains two subsets of synthetic, semantically segmented road-scene images, which have been created for developing and applying the methodology described in the paper ""A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View"" (IEEE Xplore, arXiv, YouTube)
The dataset can be used through the official code implementation of the Cam2BEV methodology described on Github.
| Dataset | # Training Samples | # Validation Samples | # Vehicle Cameras | # Semantic Classes | Contained Images (examples) |
| --- | --- | --- | --- | --- | --- |
| Dataset 1: 360° Surround | 33199 | 3731 | 4 (front, rear, left, right) | 30 (CityScapes) | front camera, rear camera, left camera, right camera, bird's eye view, bird's eye view incl. occlusion, homography view |
| Dataset 2: Front Camera only | 32246 | 3172 | 1 (front) | 30 (CityScapes) | front camera, bird's eye view, bird's eye view incl. occlusion, homography view |"	https://paperswithcode.com/dataset/cam2bev	08/05/2020						
571	ssTEM	"We provide two image stacks where each contains 20 sections from serial section Transmission Electron Microscopy (ssTEM) of the Drosophila melanogaster third instar larva ventral nerve cord. Both stacks measure approx. 4.7 x 4.7 x 1 microns with a resolution of 4.6 x 4.6 nm/pixel and section thickness of 45-50 nm.
In addition to the raw image data, we provide for the first stack a dense labeling of neuron membranes (including orientation and junction), mitochondria, synapses and glia/extracellular space. The first stack serves as a training dataset, and a second stack of the same dimension can be used as a test dataset.
Source: https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713
Image source:  https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713"	https://paperswithcode.com/dataset/sstem							
572	VeRi-776	"VeRi-776 is a vehicle re-identification dataset which contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow. The dataset contains bounding boxes, types, colors and brands.
Source: VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification
Image Source: https://vehiclereid.github.io/VeRi/"	https://paperswithcode.com/dataset/veri-776	01/01/2016	VeRi-776					
573	UNSW-NB15	"UNSW-NB15 is a network intrusion dataset. It contains nine different attacks, includes DoS, worms, Backdoors, and Fuzzers. The dataset contains raw network packets. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.
Source: Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs
Image Source: https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/
Paper: UNSW-NB15: a comprehensive data set for network intrusion detection systems"	https://paperswithcode.com/dataset/unsw-nb15	01/11/2015	UNSQ-NB15					
574	FarsTail	"Natural Language Inference (NLI), also called Textual Entailment, is an important task in NLP with the goal of determining the inference relationship between a premise p and a hypothesis h. It is a three-class problem, where each pair (p, h) is assigned to one of these classes: ""ENTAILMENT"" if the hypothesis can be inferred from the premise, ""CONTRADICTION"" if the hypothesis contradicts the premise, and ""NEUTRAL"" if none of the above holds. There are large datasets such as SNLI, MNLI, and SciTail for NLI in English, but there are few datasets for poor-data languages like Persian. Persian (Farsi) language is a pluricentric language spoken by around 110 million people in countries like Iran, Afghanistan, and Tajikistan. FarsTail is the first relatively large-scale Persian dataset for NLI task. A total of 10,367 samples are generated from a collection of 3,539 multiple-choice questions. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances, respectively.
Source: https://github.com/dml-qom/FarsTail
Image Source: https://github.com/dml-qom/FarsTail"	https://paperswithcode.com/dataset/farstail	18/09/2020						
575	CUHK-PEDES	"The CUHK-PEDES dataset is a caption-annotated pedestrian dataset. It contains 40,206 images over 13,003 persons. Images are collected from five existing person re-identification datasets, CUHK03, Market-1501, SSM, VIPER, and CUHK01 while each image is annotated with 2 text descriptions by crowd-sourcing workers. Sentences incorporate rich details about person appearances, actions, poses.
Source: MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination
Image Source: https://www.researchgate.net/figure/Image-samples-in-three-datasets-For-MSCOCO-and-Flickr30k-dataset-we-view-every-image_fig2_321095980"	https://paperswithcode.com/dataset/cuhk-pedes	01/01/2017	CUHK-PEDES					
576	AND Dataset	"The AND Dataset contains 13700 handwritten samples and 15 corresponding expert examined features for each sample. The dataset is released for public use and the methods can be extended to provide explanations on other verification tasks like face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification.
Source: Explanation based Handwriting Verification"	https://paperswithcode.com/dataset/and-dataset	14/08/2019						
577	Multi-dSprites		https://paperswithcode.com/dataset/multi-dsprites	22/01/2019						
578	Object Discovery	"The Object Discovery dataset was collected by downloading images from Internet for airplane, car and horse. It is significantly larger and thus, diverse in terms of viewpoints, texture, color etc
Source: One shot Joint Colocalization & Cosegmentation
Image Source: http://people.csail.mit.edu/mrub/ObjectDiscovery/"	https://paperswithcode.com/dataset/object-discovery	01/01/2013						
579	Open Entity	"The Open Entity dataset is a collection of about 6,000 sentences with fine-grained entity types annotations. The entity types are free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Sentences were sampled from Gigaword, OntoNotes and web articles. On average each sentence has 5 labels.
Source: Ultra-Fine Entity Typing
Image Source: Ultra-Fine Entity Typing"	https://paperswithcode.com/dataset/open-entity-1	01/01/2018						
580	RITE	"The RITE (Retinal Images vessel Tree Extraction) is a database that enables comparative studies on segmentation or classification of arteries and veins on retinal fundus images, which is established based on the public available DRIVE database (Digital Retinal Images for Vessel Extraction).
RITE contains 40 sets of images, equally separated into a training subset and a test subset, the same as DRIVE. The two subsets are built from the corresponding two subsets in DRIVE. For each set, there is a fundus photograph, a vessel reference standard, and a Arteries/Veins (A/V) reference standard. 

The fundus photograph is inherited from DRIVE. 
For the training set, the vessel reference standard is a modified version of 1st_manual from DRIVE. 
For the test set, the vessel reference standard is 2nd_manual from DRIVE. 
For the A/V reference standard, four types of vessels are labelled using four colors based on the vessel reference standard. 
Arteries are labelled in red; veins are labelled in blue; the overlapping of arteries and veins are labelled in green; the vessels which are uncertain are labelled in white. 
The fundus photograph is in tif format. And the vessel reference standard and the A/V reference standard are in png format.  

The dataset is described in more detail in our paper, which you will cite if you use the dataset in any way: 
Hu Q, Abràmoff MD, Garvin MK. Automated separation of binary overlapping trees in low-contrast color retinal images. Med Image Comput Comput Assist Interv. 2013;16(Pt 2):436-43. PubMed PMID: 24579170 https://doi.org/10.1007/978-3-642-40763-5_54"	https://paperswithcode.com/dataset/rite	01/01/2013	Retinal Images vessel Tree Extraction					
581	Contract Discovery	"A new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts.
Source: Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines"	https://paperswithcode.com/dataset/contract-discovery							
582	UI-PRMD	UI-PRMD is a data set of movements related to common exercises performed by patients in physical therapy and rehabilitation programs. The data set consists of 10 rehabilitation exercises. A sample of 10 healthy individuals repeated each exercise 10 times in front of two sensory systems for motion capturing: a Vicon optical tracker, and a Kinect camera. The data is presented as positions and angles of the body joints in the skeletal models provided by the Vicon and Kinect mocap systems.	https://paperswithcode.com/dataset/ui-prmd	29/01/2019	University of Idaho – Physical Rehabilitation Movement Dataset					
583	TVR	"A new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. 
Source: TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval"	https://paperswithcode.com/dataset/tvr		TV show Retrieval					
584	TVQA	"The TVQA dataset is a large-scale vido dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.
Source: Two-stream Spatiotemporal Feature for Video QA Task
Image Source: https://arxiv.org/abs/1809.01696"	https://paperswithcode.com/dataset/tvqa	01/01/2018	TVQA					
585	DialogRE	"DialogRE is the first human-annotated dialogue-based relation extraction dataset, containing 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy Friends. The are annotations for all occurrences of 36 possible relation types that exist between an argument pair in a dialogue. DialogRE is available in English and Chinese.
Source: DialogRE"	https://paperswithcode.com/dataset/dialogre	17/04/2020						
586	Tweebank	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/tweebank	23/04/2018						
587	ScanObjectNN	"ScanObjectNN is a newly published real-world dataset comprising of 2902 3D objects in 15 categories. It is a challenging point cloud classification datasets due to the background, missing parts and deformations.
Source: A Self Contour-based Rotation and Translation-Invariant Transformation for Point Clouds Recognition
Image Source: https://hkust-vgd.github.io/scanobjectnn/"	https://paperswithcode.com/dataset/scanobjectnn	01/01/2019						
588	COMA	"CoMA contains 17,794 meshes of the human face in various expressions
Source: DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects
Image Source: https://coma.is.tue.mpg.de/"	https://paperswithcode.com/dataset/coma	01/01/2018	COMA					
589	ToLD-Br	"The Toxic Language Detection for Brazilian Portuguese (ToLD-Br) is a dataset with tweets in Brazilian Portuguese annotated according to different toxic aspects.
Source: https://github.com/JAugusto97/ToLD-Br"	https://paperswithcode.com/dataset/told-br		Toxic Language Detection for Brazilian Portuguese					
590	MIMIC-CXR	"MIMIC-CXR from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA.
Source: Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification
Image Source: https://arxiv.org/abs/1901.07042"	https://paperswithcode.com/dataset/mimic-cxr	01/01/2019	MIMIC-CXR					
591	CheXpert	"The CheXpert dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.
Source: Deep Mining External Imperfect Data for Chest X-ray Disease Screening
Image Source: https://stanfordmlgroup.github.io/competitions/chexpert/"	https://paperswithcode.com/dataset/chexpert	01/01/2019	CheXpert					
592	DeepFix	DeepFix consists of a program repair dataset (fix compiler errors in C programs). It enables research around automatically fixing programming errors using deep learning.	https://paperswithcode.com/dataset/deepfix	04/02/2017						
593	DUC 2004	"The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.
Source: Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction
Image Source: https://duc.nist.gov/duc2004/"	https://paperswithcode.com/dataset/duc-2004		DUC 2004					
594	CL-SciSumm		https://paperswithcode.com/dataset/cl-scisumm							
595	UT-Interaction	"The UT-Interaction dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, resulting in 8 executions of human activities per video on average. Several participants with more than 15 different clothing conditions appear in the videos. The videos are taken with the resolution of 720*480, 30fps, and the height of a person in the video is about 200 pixels.
Source: https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html
Image Source: https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html"	https://paperswithcode.com/dataset/ut-interaction							
596	AVSD	"The Audio Visual Scene-Aware Dialog (AVSD) dataset, or DSTC7 Track 3, is a audio-visual dataset for dialogue understanding. The goal with the dataset and track was to design systems to generate responses in a dialog about a video, given the dialog history and audio-visual content of the video.
Source: The Eighth Dialog System Technology Challenge
Image Source: http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf"	https://paperswithcode.com/dataset/avsd	01/01/2018	Audio-Visual Scene-Aware Dialog					
597	eQASC	"This dataset contains 98k 2-hop explanations for questions in the QASC dataset, with annotations indicating if they are valid (~25k) or invalid (~73k) explanations.
This repository addresses the current lack of training data for distinguish valid multihop explanations from invalid, by providing three new datasets. The main one, eQASC, contains 98k explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer.
The second dataset, eQASC-perturbed, is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/eqasc	07/10/2020	eQASC					
598	ImageNet-LT	"ImageNet Long-Tailed is a subset of /dataset/imagenet dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set.
Source: Large-Scale Long-Tailed Recognition in an Open World"	https://paperswithcode.com/dataset/imagenet-lt		ImageNet Long-Tailed					
599	Places-LT	"Places-LT has an imbalanced training set with 62,500 images for 365 classes from Places-2. The class frequencies follow a natural power law distribution with a maximum number of 4,980 images per class and a minimum number of 5 images per class. The validation and testing sets are balanced and contain 20 and 100 images per class respectively.
Source: Long-Tailed Recognition Using Class-Balanced Experts"	https://paperswithcode.com/dataset/places-lt	01/01/2019						
600	Salinas	Salinas Scene is a hyperspectral dataset collected by the 224-band AVIRIS sensor over Salinas Valley, California, and is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. 20 water absorption bands were discarder: [108-112], [154-167], 224. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. Salinas groundtruth contains 16 classes.	https://paperswithcode.com/dataset/salinas		Salinas Scene					
601	RST-DT	"The Rhetorical Structure Theory (RST) Discourse Treebank consists of 385 Wall Street Journal articles
from the Penn Treebank annotated with discourse structure in the RST framework along with
human-generated extracts and abstracts associated with the source documents.
In the RST framework (Mann and Thompson, 1988), a text's discourse structure can be
represented as a tree in four aspects:
(1) the leaves correspond to text fragments called elementary discourse units (the mininal discourse units);
(2) the internal nodes of the tree correspond to contiguous text spans;
(3) each node is characterized by its nuclearity, or essential unit of information; and
(4) each node is also characterized by a rhetorical relation between two or more non-overlapping, adjacent text spans. 
Data
The data in this release is divided into a training set (347 documents) and a test set (38 documents).
All annotations were produced using a discourse annotation tool that can be downloaded from http://www.isi.edu/~marcu/discourse."	https://paperswithcode.com/dataset/rst-dt	21/02/2002	RST Discourse Treebank					
602	SQA	"The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.
Source: SQA"	https://paperswithcode.com/dataset/sqa	01/07/2017	SequentialQA					
603	BC4CHEMD	"Introduced by Krallinger et al. in The CHEMDNER corpus of chemicals and drugs and its annotation principles
BC4CHEMD is a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators."	https://paperswithcode.com/dataset/bc4chemd		BioCreative IV Chemical compound and drug name recognition					
604	SherLIiC	"SherLIiC is a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types.
Source: SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference"	https://paperswithcode.com/dataset/sherliic	04/06/2019	SherLIiC					
605	XCOPA	"The Cross-lingual Choice of Plausible Alternatives (XCOPA) dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across languages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages.
Source: https://github.com/cambridgeltl/xcopa"	https://paperswithcode.com/dataset/xcopa							
606	DebateSum	"DebateSum consists of 187328 debate documents, arguments (also can be thought of as abstractive summaries, or queries), word-level extractive summaries, citations, and associated metadata organized by topic-year. This data is ready for analysis by NLP systems.
Source: https://github.com/Hellisotherpeople/DebateSum"	https://paperswithcode.com/dataset/debatesum							
607	iSAID	"iSAID contains 655,451 object instances for 15 categories across 2,806 high-resolution images. The images of iSAID is the same as the DOTA-v1.0 dataset, which are manily collected from the Google Earth, some are taken by satellite JL-1, the others are taken by satellite GF-2 of the China Centre for Resources Satellite Data and Application.
Source: iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images
Image Source: iSAID"	https://paperswithcode.com/dataset/isaid							
608	RuDaS	Logical rules are a popular knowledge representation language in many domains. Recently, neural networks have been proposed to support the complex rule induction process. However, we argue that existing datasets and evaluation approaches are lacking in various dimensions; for example, different kinds of rules or dependencies between rules are neglected. Moreover, for the development of neural approaches, we need large amounts of data to learn from and adequate, approximate evaluation measures. In this paper, we provide a tool for generating diverse datasets and for evaluating neural rule learning systems, including novel performance metrics.	https://paperswithcode.com/dataset/rudas	16/09/2019	Synthetic Datasets for Rule Learning					
609	ReClor	"Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. ReClor is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.
Source: ReClor"	https://paperswithcode.com/dataset/reclor	11/02/2020						
610	SEN12MS-CR	"Curates a large novel data set for training new cloud removal approaches and evaluate on two recently proposed performance metrics of image quality and diversity.
Source: Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery"	https://paperswithcode.com/dataset/sen12ms-cr							
611	ConvAI2	"The ConvAI2 NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers.
To avoid modeling that takes advantage of trivial word overlap, additional rewritten sets of the same train and test personas were crowdsourced, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging. For example “I just got my nails done” is revised as “I love to pamper myself on a regular basis” and “I am on a diet now” is revised as “I need to lose weight.”
The training, validation and hidden test sets consists of 17,878, 1,000 and 1,015 dialogues, respectively.
Source: The Second Conversational Intelligence Challenge (ConvAI2)
Image Source: The Second Conversational Intelligence Challenge (ConvAI2)"	https://paperswithcode.com/dataset/convai2	01/01/2019	Conversational Intelligence Challenge 2					
612	EmpatheticDialogues	"The EmpatheticDialogues dataset is a large-scale multi-turn empathetic dialogue dataset collected on the Amazon Mechanical Turk, containing 24,850 one-to-one open-domain conversations. Each conversation was obtained by pairing two crowd-workers: a speaker and a listener. The speaker is asked to talk about the personal emotional feelings. The listener infers the underlying emotion through what the speaker says and responds empathetically. The dataset provides 32 evenly distributed emotion labels.
Source: Empathetic Dialogue Generation viaKnowledge Enhancing and Emotion Dependency Modeling
Image Source: Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"	https://paperswithcode.com/dataset/empatheticdialogues	01/01/2019						
613	Wizard of Wikipedia	Wizard of Wikipedia is a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. It is used to train and evaluate dialogue systems for knowledgeable open dialogue with clear grounding	https://paperswithcode.com/dataset/wizard-of-wikipedia	03/11/2018						
614	PPM-100	"PPM is a portrait matting benchmark with the following characteristics:

Fine Annotation - All images are labeled and checked carefully.
Natural Background - All images use the original background without replacement.
Rich Diversity - The images cover full/half body and various postures.
High Resolution - The resolution of images is between 1080p and 4k.

Dataset is created by authors of real-time matting model MODNet to measure performance for matting task."	https://paperswithcode.com/dataset/phm-100	24/11/2020						
615	GoEmotions	"GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.

Number of examples: 58,009.
Number of labels: 27 + Neutral.
Maximum sequence length in training and evaluation datasets: 30.

On top of the raw data, the dataset also includes a version filtered based on reter-agreement, which contains a train/test/validation split:

Size of training dataset: 43,410.
Size of test dataset: 5,427.
Size of validation dataset: 5,426.

The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.
Source: Google Research"	https://paperswithcode.com/dataset/goemotions	01/05/2020	GoEmotions					
616	RecipeNLG		https://paperswithcode.com/dataset/recipenlg	15/12/2020						
617	DroneDeploy	"From DroneDeploy:
We’ve collected a dataset of aerial orthomosaics and elevation images. These have been annotated into 6 different classes: Ground, Water, Vegetation, Cars, Clutter, and Buildings. The resolution of the images is approximately 10cm per pixel which gives them a great level of detail. We’re looking forward to making more data available and encourage more research into the impact this imagery can have in furthering safety, conservation, and efficiency.
Image source: https://arxiv.org/pdf/2012.02024v1.pdf
Source: DroneDeploy Segmentation Benchmark Challenge
Image Source: title"	https://paperswithcode.com/dataset/dronedeploy							
618	fastMRI	"The fastMRI dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets.
The deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.
Knee MRI: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression.
Brain MRI: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent.
Source: https://fastmri.med.nyu.edu/
Image Source: https://fastmri.med.nyu.edu/"	https://paperswithcode.com/dataset/fastmri							
619	WHO-COVID19 Dataset	COVID19 Data from the World Health Organization	https://paperswithcode.com/dataset/who							
620	MAMS	"MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).
Source: MAMS"	https://paperswithcode.com/dataset/mams	01/11/2019	Multi Aspect Multi-Sentiment					
621	DDRel	"DDRel is a dataset for interpersonal relation classification in dyadic dialogues. It consists of 6,300 dyadic dialogue sessions between 694 pairs of speakers with 53,126 utterances in total. It is constructed by crawling movie scripts from IMSDb and annotating the relation labels for each session according to 13 pre-defines relationships.
Source: https://github.com/JiaQiSJTU/DialogueRelationClassification"	https://paperswithcode.com/dataset/ddrel							
622	SYSU-MM01	"The SYSU-MM01 is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery.
Source: An Efficient Framework for Visible-Infrared Cross Modality Person Re-Identification
Image Source: https://github.com/wuancong/SYSU-MM01"	https://paperswithcode.com/dataset/sysu-mm01	01/01/2017	SYSU-MM01					
623	MusicNet	"MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.
Source: MusicNet"	https://paperswithcode.com/dataset/musicnet	29/11/2016						
624	HateXplain	"Covers multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.
Source: HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection"	https://paperswithcode.com/dataset/hatexplain							
625	RECCON	"RECCON is a dataset for the task of recognizing emotion cause in conversations.
Source: Recognizing Emotion Cause in Conversations"	https://paperswithcode.com/dataset/reccon							
626	RegDB	RegDB is used for Visible-Infrared Re-ID which handles the cross-modality matching between the daytime visible and night-time infrared images. The dataset contains images of 412 people. It includes 10 color and 10 thermal images for each person.	https://paperswithcode.com/dataset/regdb	13/01/2020	Dongguk Body-based Person Recognition Database (DBPerson-Recog-DB1)					
627	Kennedy Space Center	Kennedy Space Center is a dataset for the classification of wetland vegetation at the Kennedy Space Center, Florida using hyperspectral imagery. Hyperspectral data were acquired over KSC on March 23, 1996 using JPL's Airborne Visible/Infrared Imaging Spectrometer.	https://paperswithcode.com/dataset/kennedy-space-center		Kennedy Space Center					
628	VLCS	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/vlcs							
629	Cholec80	"Cholec80 is an endoscopic video dataset containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in Strasbourg hospital, France. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, a tool is defined as present in an image if at least half of the tool tip is visible.
Source: EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos[https://arxiv.org/pdf/1602.03012.pdf]"	https://paperswithcode.com/dataset/cholec80	09/02/2016	Surgical Workflow Dataset					
630	Multi-PIE	"The Multi-PIE (Multi Pose, Illumination, Expressions) dataset consists of face images of 337 subjects taken under different pose, illumination and expressions. The pose range contains 15 discrete views, capturing a face profile-to-profile. Illumination changes were modeled using 19 flashlights located in different places of the room.
Source: Hybrid VAE: Improving Deep Generative Models using Partial Observations"	https://paperswithcode.com/dataset/multi-pie	01/01/2008						
631	The Pile	"The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.
Datasheet: Datasheet for the Pile"	https://paperswithcode.com/dataset/the-pile	31/12/2020	The Pile					
632	ECB+	"The ECB+ corpus is an extension to the EventCorefBank (ECB, Bejan and Harabagiu, 2010). A newly added corpus component consists of 502 documents that belong to the 43 topics of the ECB but that describe different seminal events than those already captured in the ECB. All corpus texts were found through Google Search and were annotated with mentions of events and their times, locations, human and non-human participants as well as with within- and cross-document event and entity coreference information. The 2012 version of annotation of the ECB corpus (Lee et al., 2012) was used as a starting point for re-annotation of the ECB according to the ECB+ annotation guideline.
The major differences with respect to the 2012 version of annotation of the ECB are:
(a) five event components are annotated in text:
actions (annotation tags starting with ACTION and NEG)
times (annotation tags starting with TIME)
locations (annotation tags starting with LOC)
human participants (annotation tags starting with HUMAN)
non-human participants (annotation tags starting with NON_HUMAN)

(b) specific action classes and entity subtypes are distinguished for each of the five main event components resulting in a total tagset of 30 annotation tags based on ACE annotation guidelines (LDC 2008), TimeML (Pustejovsky et al., 2003 and Sauri et al., 2005)
(c) intra- and cross-document coreference relations between mentions of the five event components were established:
INTRA_DOC_COREF tag captures within document coreference chains that do not participate in cross-document relations; within document coreference was annotated by means of the CAT tool (Bartalesi et al., 2012)
CROSS_DOC_COREF tag indicates cross-document coreference relations created in the CROMER tool (Girardi et al., 2014); all coreference branches refer by means of relation target IDs to the so called TAG_DESCRIPTORS, pointing to human friendly instance names (assigned by coders) and also to instance_id-s

(d) events are annotated from an “event-centric” perspective, i.e. annotation tags are assigned depending on the role a mention plays in an event (for more information see ECB+ references)."	https://paperswithcode.com/dataset/ecb	01/05/2014	extension to the EventCorefBank					
633	S2ORC	"A large corpus of 81.1M English-language academic papers spanning many academic disciplines. Rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/s2orc	07/11/2019	S2ORC					
634	WebEdit	Fact-based Text Editing dataset based on WebNLG dataset.	https://paperswithcode.com/dataset/webedit							
635	RotoEdit	Fact-based Text Editing dataset based on RotoWire dataset	https://paperswithcode.com/dataset/rotoedit							
636	WebVision	"The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. 
The same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development."	https://paperswithcode.com/dataset/webvision-database							
637	NASA Worldview	"In this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names: Fish, Flower, Gravel, Sugar. For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four.
The images were downloaded from NASA Worldview. Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.
The labels were created in a crowd-sourcing activity at the Max-Planck-Institite for Meteorology in Hamburg, Germany, and the Laboratoire de météorologie dynamique in Paris, France. A team of 68 scientists identified areas of cloud patterns in each image, and each images was labeled by approximately 3 different scientists. Ground truth was determined by the union of the areas marked by all labelers for that image, after removing any black band area from the areas.
The segment for each cloud formation label for an image is encoded into a single row, even if there are several non-contiguous areas of the same formation in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank. You can read more about the encoding standard on the Evaluation page.
Files
train.csv - the run length encoded segmentations for each image-label pair in the train_images
train_images.zip - folder of training images
test_images.zip - folder of test images; your task is to predict the segmentations masks of each of the 4 cloud types (labels) for each image. IMPORTANT: Your prediction masks should be scaled down to 350 x 525 px.
sample_submission.csv - a sample submission file in the correct format"	https://paperswithcode.com/dataset/nasa-worldview		Understanding Clouds from Satellite Images					
638	CDD Dataset (season-varying)	Source: CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS	https://paperswithcode.com/dataset/cdd-dataset-season-varying	01/06/2018						
639	LabelMe	"LabelMe database is a large collection of images with ground truth labels for object detection and recognition. The annotations come from two different sources, including the LabelMe online annotation tool.
Source: LabelMe: A Database and Web-Based Tool for Image Annotation
Image Source: Russell et al"	https://paperswithcode.com/dataset/labelme	01/01/2008						
640	ICT-3DHP	"ICT-3DHP is collected using the Microsoft Kinect sensor and contains RGB images and depth maps of about 14k frames, divided in 10 sequences. The image resolution is 640 × 480 pixels. An hardware sensor (Polhemus Fastrack) is exploited to generate the ground truth annotation. The device is placed on a white cap worn by each subject, visible in both RGB and depth frames.
Source: POSEidon: Face-from-Depth for Driver Pose Estimation"	https://paperswithcode.com/dataset/ict-3dhp							
641	ETH	"ETH is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS.
Source: Scale-aware Fast R-CNN for Pedestrian Detection
Image Source: https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3"	https://paperswithcode.com/dataset/eth	01/01/2007	ETH Pedestrian					
642	UCY	"The UCY dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (Δt=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view.
Source: Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data
Image Source: http://trajnet.stanford.edu/data.php?n=1"	https://paperswithcode.com/dataset/ucy	01/01/2007						
643	IAM	"The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.
Source: Measuring Human Perception to Improve Handwritten Document Transcription
Image Source: https://fki.tic.heia-fr.ch/databases/iam-handwriting-database"	https://paperswithcode.com/dataset/iam	01/01/2002	IAM Handwriting					
644	WebKB	"WebKB is a dataset that includes web pages from computer science departments of various universities. 4,518 web pages are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project). Additionally there is Other miscellanea category that is not comparable to the rest.
Source: Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation"	https://paperswithcode.com/dataset/webkb	01/01/1998	WebKB					
645	MemeTracker	"The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites.
Source: Marked Temporal Dynamics Modeling based on Recurrent Neural Network
Image Source: http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html"	https://paperswithcode.com/dataset/memetracker	01/01/2009	MemeTracker					
646	English Web Treebank	"English Web Treebank is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated.
Source: https://catalog.ldc.upenn.edu/LDC2012T13"	https://paperswithcode.com/dataset/english-web-treebank		English Web Treebank					
647	Silhouettes	"The Caltech 101 Silhouettes dataset consists of 4,100 training samples, 2,264 validation samples and 2,307 test samples. The datast is based on CalTech 101 image annotations. Each image in the CalTech 101 data set includes a high-quality polygon outline of the primary object in the scene. To create the CalTech 101 Silhouettes data set, the authors center and scale each outline and render it on a DxD pixel image-plane. The outline is rendered as a filled, black polygon on a white background. Many object classes exhibit silhouettes that have distinctive class-specific features. A relatively small number of classes like soccer ball, pizza, stop sign, and yin-yang are indistinguishable based on shape, but have been left-in in the data.
Source: 1 Introduction
Image Source: https://people.cs.umass.edu/~marlin/data.shtml"	https://paperswithcode.com/dataset/silhouettes	01/01/2010	CalTech 101 Silhouettes					
648	ETH SfM	"The ETH SfM (structure-from-motion) dataset is a dataset for 3D Reconstruction. The benchmark investigates how different methods perform in terms of building a 3D model from a set of available 2D images.
Source: SOSNet: Second Order Similarity Regularization forLocal Descriptor Learning
Image Source: https://cvg.ethz.ch/research/symmetries-in-sfm/"	https://paperswithcode.com/dataset/eth-sfm	01/01/2017	ETH Structure-from-Motion					
649	INRIA Person	"The INRIA Person dataset is a dataset of images of persons used for pedestrian detection. It consists of 614 person detections for training and 288 for testing.
Source: http://pascal.inrialpes.fr/data/human/
Image Source: https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181"	https://paperswithcode.com/dataset/inria-person	01/01/2005						
650	INRIA-Horse	"The INRIA-Horse dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction.
Source: Dynamical And-Or Graph Learning for Object Shape Modeling and Detection
Image Source: http://calvin-vision.net/datasets/inria-horses/"	https://paperswithcode.com/dataset/inria-horse	01/01/2004	INRIA-Horse					
651	INRIA Aerial Image Labeling	"The INRIA Aerial Image Labeling dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints.
Source: Distance transform regression for spatially-aware deep semantic segmentation
Image Source: https://project.inria.fr/aerialimagelabeling/"	https://paperswithcode.com/dataset/inria-aerial-image-labeling	01/01/2017	INRIA Aerial Image Labeling					
652	Office-Caltech-10	"Office-Caltech-10 a standard benchmark for domain adaptation, which consists of Office 10 and Caltech 10 datasets. It contains the 10 overlapping categories between the Office dataset and Caltech256 dataset. SURF BoW historgram features, vector quantized to 800 dimensions are also available for this dataset.
Source: Impact of ImageNet Model Selection on Domain Adaptation
Image Source: https://arxiv.org/abs/1409.5241"	https://paperswithcode.com/dataset/office-caltech-10	01/01/2010	Office-Caltech-10					
653	Poser	"The Poser dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package.
Source: Overlapping Cover Local Regression Machines
Image Source: https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487"	https://paperswithcode.com/dataset/poser	01/01/2006	Poser					
654	UKP	"The UKP Argument Annotated Essays corpus consists of argument annotated persuasive essays including annotations of argument components and argumentative relations.
Source: https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp
Image Source: https://www.aclweb.org/anthology/C14-1142.pdf"	https://paperswithcode.com/dataset/ukp	01/01/2014	UKP Argument Annotated Essays					
655	ETH BIWI Walking Pedestrians	"The BIWI Walking Pedestrians dataset consists of walking pedestrians in busy scenarios from a birds eye view.
Source: https://icu.ee.ethz.ch/research/datsets.html
Image Source: https://icu.ee.ethz.ch/research/datsets.html"	https://paperswithcode.com/dataset/eth-biwi-walking-pedestrians	01/01/2009	ETH BIWI Walking Pedestrians					
656	WASABI	"The WASABI Song Corpus is a large corpus of songs enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis.
More specifically, given that lyrics encode an important part of the semantics of a song, the authors focus on the description of the methods they proposed to extract relevant information from the lyrics, such as their structure segmentation, their topics, the explicitness of the lyrics content, the salient passages of a song and the emotions conveyed.
The corpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at different levels with the output of the above mentioned methods. Such corpus labels and the provided methods can be exploited by music search engines and music professionals (e.g. journalists, radio presenters) to better handle large collections of lyrics, allowing an intelligent browsing, categorization and segmentation recommendation of songs.
Source: https://github.com/micbuffa/WasabiDataset
Image Source: https://arxiv.org/abs/1912.02477"	https://paperswithcode.com/dataset/wasabi		WASABI					
657	Multilingual Reuters	"The Multilingual Reuters Collection dataset comprises over 11,000 articles from six classes in five languages, i.e., English (E), French (F), German (G), Italian (I), and Spanish (S).
Source: Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network
Image Source: https://papers.nips.cc/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf"	https://paperswithcode.com/dataset/multilingual-reuters	01/01/2009	Multilingual Reuters Collection					
658	Pan+ChiPhoto	"Pan+ChiPhoto dataset is a Chinese character dataset. It is built by the combination of two datasets: ChiPhoto and Pan_Chinese_Character dataset. The images in this dataset are mainly captured at outdoors in Beijing and Shanghai, China, which involve various scenes like signs, boards, advertisements, banners, objects with texts printed on their surfaces.
Source: Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs
Image Source: https://www.researchgate.net/publication/318679069_Multi-order_Co-occurrence_Activations_Encoded_with_Fisher_Vector_for_Scene_Character_Recognition"	https://paperswithcode.com/dataset/pan-chiphoto	01/01/2016	Pan+ChiPhoto					
659	ISI_Bengali_Character	"The ISI_Bengali_Character dataset contains 158 classes of Bengali numerals, characters or their parts. 19,530 Bengali character samples are available. Most of the images in the dataset are synthesized.
Source: Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs
Image Source: https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html"	https://paperswithcode.com/dataset/isi-bengali-character	01/01/2016	ISI_Bengali_Character					
660	Florentine	"The Florentine dataset is a dataset of facial gestures which contains facial clips from 160 subjects (both male and female), where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. 1032 clips were captured for posed expressions and 1745 clips for induced facial expressions amounting to a total of 2777 video clips. Genuine facial expressions were induced in subjects using visual stimuli, i.e. videos selected randomly from a bank of Youtube videos to generate a specific emotion.
Source: Deep video gesture recognition using illumination invariants
Image Source: https://www.micc.unifi.it/resources/datasets/florence-3d-faces/"	https://paperswithcode.com/dataset/florentine	01/01/2014	Florentine					
661	INRIA DLFD	"The INRIA Dense Light Field Dataset (DLFD) is a dataset for testing depth estimation methods in a light field. DLFD contains 39 scenes with disparity range [-4,4] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.
Source: http://clim.inria.fr/Datasets/InriaSynLF/index.html
Image Source: http://clim.inria.fr/Datasets/InriaSynLF/index.html"	https://paperswithcode.com/dataset/inria-dlfd	01/01/2019	INRIA Dense Light Field					
662	INRIA SLFD	"The INRIA Sprse Light Field Dataset (SLFD) is a dataset for testing depth estimation methods in a light field. SLFD contains 53 scenes with disparity range [-20,20] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.
Source: http://clim.inria.fr/Datasets/InriaSynLF/index.html
Image Source: http://clim.inria.fr/Datasets/InriaSynLF/index.html"	https://paperswithcode.com/dataset/inria-slfd	01/01/2019	INRIA Sparse Light Field					
663	AIDS Antiviral Screen	"The AIDS Antiviral Screen dataset is a dataset of screens checking tens of thousands of compounds for evidence of anti-HIV activity. The available screen results are chemical graph-structured data of these various compounds.
Source: Graph Neural Processes Towards Bayesian Graph Neural Networks"	https://paperswithcode.com/dataset/aids-antiviral-screen	01/01/2008	AIDS Antiviral Screen					
664	Retinal Microsurgery	"The Retinal Microsurgery dataset is a dataset for surgical instrument tracking. It consists of 18 in-vivo sequences, each with 200 frames of resolution 1920 × 1080 pixels. The dataset is further classified into four instrument-dependent subsets. The annotated tool joints are n=3 and semantic classes c=2 (tool and background).
Source: Concurrent Segmentation and Localization for Tracking of Surgical Instruments
Image Source: https://sites.google.com/site/sznitr/research/retinalmicrosurgery"	https://paperswithcode.com/dataset/retinal-microsurgery	01/01/2016	Retinal Microsurgery					
665	Daimler Monocular Pedestrian Detection	"The Daimler Monocular Pedestrian Detection dataset is a dataset for pedestrian detection in urban environments. The training set contains 15560 pedestrian samples (image cut-outs at 48×96 resolution) and 6744 additional full images without pedestrians for extracting negative samples. The test set contains an independent sequence with more than 21790 images and 56492 pedestrian labels (fully visible or partially occluded), captured from a vehicle during a 27 min driving through the urban traffic.
Source: A Large Scale Urban Surveillance Video Dataset for Multiple-Object Tracking and Behavior Analysis
Image Source: http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html"	https://paperswithcode.com/dataset/daimler-monocular-pedestrian-detection	01/01/2009	Daimler Monocular Pedestrian Detection					
666	ETHZ-Shape	"The ETHZ Shape dataset contains images of five diverse shape-based classes, collected from Flickr and Google Images. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The authors deliberately selected several images where the object comprises only a rather small portion of the image, and made an effort to include objects appearing at a wide range of scales. The objects are mostly unoccluded and are all taken from approximately the same viewpoint (the side).
Source: http://calvin-vision.net/datasets/ethz-shape-classes/
Image Source: http://calvin-vision.net/datasets/ethz-shape-classes/"	https://paperswithcode.com/dataset/ethz-shape	01/01/2008	ETHZ-Shape					
667	L-Bird	"The L-Bird (Large-Bird) dataset contains nearly 4.8 million images which are obtained by searching images of a total of 10,982 bird species from the Internet.
Source: Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data
Image Source: https://arxiv.org/pdf/1511.06789.pdf"	https://paperswithcode.com/dataset/l-bird	01/01/2016	Large-Bird					
668	Extended BBC Pose	"Extended BBC Pose is a pose estimation dataset which extends the BBC Pose dataset with 72 additional training videos. Combined with the original BBC TV dataset, the dataset contains 92 videos (82 training, 5 validation and 5 testing), i.e. around 7 million frames. The frames of the new 72 videos are automatically assigned joint locations (used as ground truth for training) with the tracker of Charles et al. IJCV'13.
Source: https://www.robots.ox.ac.uk/~vgg/data/pose/
Image Source: https://www.robots.ox.ac.uk/~vgg/data/pose/"	https://paperswithcode.com/dataset/extended-bbc-pose	01/01/2014	Extended BBC Pose					
669	Short BBC Pose	"Short BBC Pose contains five one-hour-long videos with sign language signers each with different sleeve length (in contrast to the BBC pose and Extended BBC Pose, which only contain signers with moderately long sleeves). Each of the five videos has 200 test frames (which have been manually annotated with joint locations), amounting to 1,000 test frames in total. Test frames were selected by the authors to contain a diverse range of poses.
Source: https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation
Image Source: https://www.robots.ox.ac.uk/~vgg/publications/2013/Charles13/charles13.pdf"	https://paperswithcode.com/dataset/short-bbc-pose	01/01/2013	Short BBC Pose					
670	ChaLearn Pose	"ChaLearn Pose is a subset of the ChaLearn 2013 Multi-modal gesture dataset from Escalera et al. ICMI'13, which contains 23 hours of Kinect data of 27 persons performing 20 Italian gestures. The data includes RGB, depth, foreground segmentations and full body skeletons. In this dataset, both the training and testing labels are noisy (from Kinect).
Source: https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation
Image Source: http://sunai.uoc.edu/chalearnLAP/"	https://paperswithcode.com/dataset/chalearn-pose	01/01/2013	ChaLearn Pose					
671	VoxCeleb2	"VoxCeleb2 is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected ‘in the wild’, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example – visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets.
Source: VoxCeleb2: Deep Speaker Recognition
Image Source: https://www.robots.ox.ac.uk/~vgg/data/voxceleb/"	https://paperswithcode.com/dataset/voxceleb2	01/01/2018	VoxCeleb2					
672	VCTK	"This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, ""The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,"" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.
Source: CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)"	https://paperswithcode.com/dataset/vctk		CSTR VCTK Corpus					
673	DIRHA	"DIRHA-English is a multi-microphone database composed of real and simulated sequences of 1-minute. The overall corpus is composed of different types of sequences including: 1) Phonetically-rich sentences; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands).
The sequences are available for both UK and US English at 48 kHz. The DIRHA-English dataset offers the possibility to work with a very large number of microphone channels, to use of microphone arrays having different characteristics and to work considering different speech recognition tasks (e.g., phone-loop, keyword spotting, ASR with small and very large language models).
Source: The DIRHA-English Corpus
Image Source: https://arxiv.org/pdf/1710.02560v1.pdf"	https://paperswithcode.com/dataset/dirha	01/01/2015	Distant-speech Interaction for Robust Home Applications					
674	VoxForge	"VoxForge is an open speech dataset that was set up to collect transcribed speech for use with Free and Open Source Speech Recognition Engines (on Linux, Windows and Mac).
Image Source: http://www.voxforge.org/home"	https://paperswithcode.com/dataset/voxforge		VoxForge					
675	Penn Action	"The Penn Action Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence.
Source: http://dreamdragon.github.io/PennAction/
Image Source: http://dreamdragon.github.io/PennAction/"	https://paperswithcode.com/dataset/penn-action	01/01/2013						
676	TVSum	"TVSum Dataset
Title-based Video Summarization (TVSum) dataset used in our CVPR 2015 paper ""TVSum: Summarizing web videos using titles.""
Overview
Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video). The video and annotation data permits an automatic evaluation of various video summarization techniques, without having to conduct (expensive) user study.
The videos, collected from YouTube, comes with the Creative Commons CC-BY (v3.0) license. We release both the video files and their URLs. The shot-level importance scores are annotated via Amazon Mechanical Turk -- each video was annotated by 20 crowd-workers. The dataset has been reviewed to conform to Yahoo's data protection standards, including strict controls on privacy.
Task
The primary task of the dataset is video summarization, where the goal is to create a short, meaningful summary of a given video. The summary may contain a few shots that capture the highlights of a video and are non-redundant. Although the task is inherently subjective, we carefully curated the dataset and annotated it so that the evaluation is done in an objective way. (We have a reasonably high degree of inter-rater reliability, with the Cronbach’s alpha of 0.81.)
Evaluations
Let’s say we’ve generated a 15 second-long summary of video “Will a cat eat dog food?”, shown below:"	https://paperswithcode.com/dataset/tvsum-1		TVSum: Summarizing web videos using titles					
677	FLIC	"The FLIC dataset contains 5003 images from popular Hollywood movies. The images were obtained by running a state-of-the-art person detector on every tenth frame of 30 movies. People detected with high confidence (roughly 20K candidates) were then sent to the crowdsourcing marketplace Amazon Mechanical Turk to obtain ground truth labelling. Each image was annotated by five Turkers to label 10 upper body joints. The median-of-five labelling was taken in each image to be robust to outlier annotation. Finally, images were rejected manually by if the person was occluded or severely non-frontal.
Source: https://bensapp.github.io/flic-dataset.html
Image Source: https://www.tensorflow.org/datasets/catalog/flic"	https://paperswithcode.com/dataset/flic	01/01/2013	Frames Labelled in Cinema					
678	WikiArt	"WikiArt contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.
Source: Adding New Tasks to a Single Network with Weight Transformations using Binary Masks
Image Source: https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e"	https://paperswithcode.com/dataset/wikiart	01/01/2015						
679	Sim10k	"SIM10k is a synthetic dataset containing 10,000 images, which is rendered from the video game Grand Theft Auto V (GTA5).
Source: Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation
Image Source: https://arxiv.org/pdf/1610.01983.pdf"	https://paperswithcode.com/dataset/sim10k	01/01/2017						
680	EYEDIAP	"The EYEDIAP dataset is a dataset for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras. The recording methodology was designed by systematically including, and isolating, most of the variables which affect the remote gaze estimation algorithms:

Head pose variations.
Person variation.
Changes in ambient and sensing condition.
Types of target: screen or 3D object.

Source: https://www.idiap.ch/dataset/eyediap
Image Source: https://www.idiap.ch/dataset/eyediap"	https://paperswithcode.com/dataset/eyediap	01/01/2014						
681	G3D	"The Gaming 3D Dataset (G3D) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: “punch right”, “punch left”, “kick right”, “kick left”, “defend”, “golf swing”, “tennis swing forehand”, “tennis swing backhand”, “tennis serve”, “throw bowling ball”, “aim and fire gun”, “walk”, “run”, “jump”, “climb”, “crouch”, “steer a car”, “wave”, “flap” and “clap”.
Source: Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN
Image Source: G3D: A gaming action dataset and real time action recognition evaluation framework"	https://paperswithcode.com/dataset/g3d	01/01/2012	Gaming 3D Dataset					
682	O-HAZE	"The O-Haze dataset contains 35 hazy images (size 2833×4657 pixels) for training. It has 5 hazy images for validation along with their corresponding ground truth images.
Source: Single image dehazing for a variety of haze scenarios using back projected pyramid network
Image Source: https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/"	https://paperswithcode.com/dataset/o-haze-1	01/01/2018						
683	UMIST	"The Sheffield (previously UMIST) Face Database consists of 564 images of 20 individuals (mixed race/gender/appearance). Each individual is shown in a range of poses from profile to frontal views – each in a separate directory labelled 1a, 1b, … 1t and images are numbered consecutively as they were taken. The files are all in PGM format, approximately 220 x 220 pixels with 256-bit grey-scale.
Source: https://www.visioneng.org.uk/datasets/
Image Source: https://www.visioneng.org.uk/datasets/"	https://paperswithcode.com/dataset/umist-1							
684	CVUSA	"A large dataset containing millions of pairs of ground-level and aerial/satellite images from across the United States.
Source: http://mvrl.cs.uky.edu/datasets/cvusa/
Image Source: https://arxiv.org/pdf/1612.02709.pdf"	https://paperswithcode.com/dataset/cvusa-1	01/01/2015	Cross-View USA					
685	FC100	"The FC100 dataset (Fewshot-CIFAR100) is a newly split dataset based on CIFAR-100 for few-shot learning. It contains 20 high-level categories which are divided into 12, 4, 4 categories for training, validation and test. There are 60, 20, 20 low-level classes in the corresponding split containing 600 images of size 32 × 32 per class. Smaller image size makes it more challenging for few-shot learning.
Source: Prototype Rectification for Few-Shot Learning"	https://paperswithcode.com/dataset/fc100	01/01/2018	Fewshot-CIFAR100					
686	PASCAL-5i	"PASCAL-5i is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.
Source: AMP: Adaptive Masked Proxies for Few-Shot Segmentation
Image Source: https://arxiv.org/pdf/1709.03410.pdf"	https://paperswithcode.com/dataset/pascal-5i	01/01/2017						
687	TrajNet	"The TrajNet Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t−7,t−6,…,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,…,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird’s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird’s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird’s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants).
Source: Transformer Networks for Trajectory Forecasting
Image Source: http://trajnet.stanford.edu/"	https://paperswithcode.com/dataset/trajnet-1							
688	Set12	"Set12 is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256×256.
Source: Designing and Training of A Dual CNN for Image Denoising
Image Source: https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598"	https://paperswithcode.com/dataset/set12	01/01/2017						
689	TotalCapture	"The TotalCapture dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.
Source: Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras
Image Source: https://cvssp.org/data/totalcapture/"	https://paperswithcode.com/dataset/totalcapture	01/01/2017						
690	I-HAZE	"The I-Haze dataset contains 25 indoor hazy images (size 2833×4657 pixels) training. It has 5 hazy images for validation along with their corresponding ground truth images.
Source: Single image dehazing for a variety of haze scenarios using back projected pyramid network
Image Source: https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/"	https://paperswithcode.com/dataset/i-haze-1	01/01/2018						
691	SEED	"The SEED dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.
Source: http://bcmi.sjtu.edu.cn/home/seed/index.html
Image Source: http://bcmi.sjtu.edu.cn/home/seed/index.html"	https://paperswithcode.com/dataset/seed-1	01/01/2015	SJTU Emotion EEG Dataset					
692	SHREC	"The SHREC dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.
Source: Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition
Image Source: http://tosca.cs.technion.ac.il/book/shrec.html"	https://paperswithcode.com/dataset/shrec	01/01/2015	SHape REtrieval Contest					
693	Florence3D	"The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.
Source: https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/
Image Source: https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/"	https://paperswithcode.com/dataset/florence3d	01/01/2013						
694	SNAP	"SNAP is a collection of large network datasets. It includes graphs representing social networks, citation networks, web graphs, online communities, online reviews and more.
Social networks : online social networks, edges represent interactions between people
Networks with ground-truth communities : ground-truth network communities in social and information networks
Communication networks : email communication networks with edges representing communication
Citation networks : nodes represent papers, edges represent citations
Collaboration networks : nodes represent scientists, edges represent collaborations (co-authoring a paper)
Web graphs : nodes represent webpages and edges are hyperlinks
Amazon networks : nodes represent products and edges link commonly co-purchased products
Internet networks : nodes represent computers and edges communication
Road networks : nodes represent intersections and edges roads connecting the intersections
Autonomous systems : graphs of the internet
Signed networks : networks with positive and negative edges (friend/foe, trust/distrust)
Location-based online social networks : social networks with geographic check-ins
Wikipedia networks, articles, and metadata : talk, editing, voting, and article data from Wikipedia
Temporal networks : networks where edges have timestamps
Twitter and Memetracker : memetracker phrases, links and 467 million Tweets
Online communities : data from online communities such as Reddit and Flickr
Online reviews : data from online review systems such as BeerAdvocate and Amazon
User actions : actions of users on social platforms.
Face-to-face communication networks : networks of face-to-face (non-online) interactions
Graph classification datasets : disjoint graphs from different classes
Image Source: https://snap.stanford.edu/data/"	https://paperswithcode.com/dataset/snap	28/06/2009	Stanford Large Network Dataset Collection					
695	BioASQ	"BioASQ is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).
Source: Transferability of Natural Language Inference to Biomedical Question Answering
Image Source: http://participants-area.bioasq.org/datasets/"	https://paperswithcode.com/dataset/bioasq	01/01/2015	Biomedical Semantic Indexing and Question Answering					
696	STRING	STRING is a collection of protein-protein interaction (PPI) networks.	https://paperswithcode.com/dataset/string	01/01/2015	STRING					
697	OpenWebText	"OpenWebText is an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).
Source: RoBERTa: A Robustly Optimized BERT Pretraining Approach"	https://paperswithcode.com/dataset/openwebtext							
698	Foursquare	"The Foursquare dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories).
Another subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins.
Source: https://sites.google.com/site/yangdingqi/home/foursquare-dataset"	https://paperswithcode.com/dataset/foursquare	01/01/2016						
699	PeerRead	"PearRead is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.
Source: https://github.com/allenai/PeerRead"	https://paperswithcode.com/dataset/peerread	01/01/2018						
700	Kinship	"This relational database consists of 24 unique names in two families (they have equivalent structures).
Source: https://archive.ics.uci.edu/ml/datasets/kinship"	https://paperswithcode.com/dataset/kinship							
701	Mindboggle	"Mindboggle is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 32 manually labeled parcels.
Source: Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data
Image Source: https://mindboggle.info/data.html"	https://paperswithcode.com/dataset/mindboggle	01/01/2017						
702	Learning to Rank Challenge	"The Yahoo! Learning to Rank Challenge dataset consists of 709,877 documents encoded in 700 features and sampled from query logs of the Yahoo! search engine, spanning 29,921 queries.
Source: Ranking for Relevance and Display Preferencesin Complex Presentation Layouts"	https://paperswithcode.com/dataset/learning-to-rank-challenge	01/01/2011	Yahoo! Learning to Rank Challenge					
703	Linux	"The LINUX dataset consists of 48,747 Program Dependence Graphs (PDG) generated from the Linux kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements
Source: Convolutional Set Matching for Graph Similarity"	https://paperswithcode.com/dataset/linux	01/01/2012	Linux Program Dependence Graphs					
704	AMiner	The AMiner Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others.	https://paperswithcode.com/dataset/aminer	01/01/2008						
705	Email-EU	"EmailEU is a directed temporal network constructed from email exchanges in a large European research institution for a 803-day period. It contains 986 email addresses as nodes and 332,334 emails as edges with timestamps. There are 42 ground truth departments in the dataset.
Source: gl2vec: Learning Feature Representation Using Graphlets for Directed Networks"	https://paperswithcode.com/dataset/email-eu	01/01/2017						
706	IMDB-BINARY	"IMDB-BINARY is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.
Source: A simple yet effective baseline for non-attributed graph classification"	https://paperswithcode.com/dataset/imdb-binary	01/01/2015						
707	NCBI Disease	"The NCBI Disease corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.
Source: A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization"	https://paperswithcode.com/dataset/ncbi-disease-1	01/01/2014						
708	arXiv Astro-Ph	"Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.
Source: https://snap.stanford.edu/data/ca-AstroPh.html"	https://paperswithcode.com/dataset/arxiv-astro-ph		arXiv Astro Physics					
709	MSLR-WEB10K	"The MSLR-WEB10K dataset consists of 10,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair. It is a subset of the MSLR-WEB30K dataset.
Source: Dueling Bandits with Qualitative Feedback"	https://paperswithcode.com/dataset/mslr-web10k	01/01/2013						
710	BeerAdvocate	"BeerAdvocate is a dataset that consists of beer reviews from beeradvocate. The data span a period of more than 10 years, including all ~1.5 million reviews up to November 2011. Each review includes ratings in terms of five ""aspects"": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review.
Source: https://snap.stanford.edu/data/web-BeerAdvocate.html"	https://paperswithcode.com/dataset/beeradvocate	01/01/2013						
711	Epinion	"The Epinions dataset is trust network dataset. For each user, it contains his profile, his ratings and his trust relations. For each rating, it has the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating.
Source: https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm"	https://paperswithcode.com/dataset/epinion	01/01/2012						
712	Stanford Light Field	"The Stanford Light Field Archive is a collection of several light fields for research in computer graphics and vision.
Source: http://lightfield.stanford.edu/
Image Source: http://lightfield.stanford.edu/"	https://paperswithcode.com/dataset/stanford-light-field							
713	Arxiv GR-QC	"Arxiv GR-QC (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.
Source: https://snap.stanford.edu/data/ca-GrQc.html"	https://paperswithcode.com/dataset/arxiv-gr-qc	01/01/2007	General Relativity and Quantum Cosmology collaboration network					
714	Orkut	"Orkut is a social network dataset consisting of friendship social network and ground-truth communities from Orkut.com on-line social network where users form friendship each other.
Each connected component in a group is regarded as a separate ground-truth community. The ground-truth communities which have less than 3 nodes are removed. The dataset also provides the top 5,000 communities with highest quality and the largest connected component of the network.
Source: https://snap.stanford.edu/data/com-Orkut.html"	https://paperswithcode.com/dataset/orkut	01/01/2012						
715	Friendster	"Friendster is an on-line gaming network. Before re-launching as a game website, Friendster was a social networking site where users can form friendship edge each other. Friendster social network also allows users form a group which other members can then join. The Friendster dataset consist of ground-truth communities (based on user-defined groups) and the social network from induced subgraph of the nodes that either belong to at least one community or are connected to other nodes that belong to at least one community.
Source: https://snap.stanford.edu/data/com-Friendster.html"	https://paperswithcode.com/dataset/friendster	01/01/2012						
716	MQ2008	The MQ2008 dataset is a dataset for Learning to Rank. It contains 800 queries with labelled documents.	https://paperswithcode.com/dataset/mq2008	01/01/2013						
717	IMDB-MULTI	"IMDB-MULTI is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.
Source: Learning metrics for persistence-based summaries and applications for graph classification"	https://paperswithcode.com/dataset/imdb-multi	01/01/2015						
718	REDDIT-12K	"Reddit12k contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community.
Source: Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity"	https://paperswithcode.com/dataset/reddit-12k	01/01/2015						
719	REDDIT-BINARY	"REDDIT-BINARY consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.
Source: A simple yet effective baseline for non-attributed graph classification"	https://paperswithcode.com/dataset/reddit-binary	01/01/2015						
720	MQ2007	"The MQ2007 dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are “relevant”, “partially relevant”, and “not relevant”.
Source: ARSM GRADIENT ESTIMATOR FOR SUPERVISED LEARNING TO RANK"	https://paperswithcode.com/dataset/mq2007							
721	Amazon Fine Foods	"Amazon Fine Foods is a dataset that consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review.
Source: https://snap.stanford.edu/data/web-FineFoods.html"	https://paperswithcode.com/dataset/amazon-fine-foods	01/01/2013						
722	REDDIT-5K	Reddit-5K is a relational dataset extracted from Reddit.	https://paperswithcode.com/dataset/reddit-5k	01/01/2015	REDDIT-MULTI-5K					
723	LastFM Asia	A social network of LastFM users which was collected from the public API in March 2020. Nodes are LastFM users from Asian countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is multinomial node classification - one has to predict the location of users. This target feature was derived from the country field for each user.	https://paperswithcode.com/dataset/lastfm-asia	16/05/2020						
724	EMNIST	"EMNIST (extended MNIST) has 4 times more data than MNIST. It is a set of handwritten digits with a 28 x 28 format.
Source: Domain Discrepancy Measure for Complex Models in Unsupervised Domain Adaptation
Image Source: https://arxiv.org/pdf/1803.01900.pdf"	https://paperswithcode.com/dataset/emnist	17/02/2017	Extended MNIST					
725	Arcade Learning Environment	"The Arcade Learning Environment (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design.
Source: https://github.com/mgbellemare/Arcade-Learning-Environment
Image Source: https://github.com/muupan/async-rl/blob/master/README.md"	https://paperswithcode.com/dataset/arcade-learning-environment	01/01/2012	Arcade Learning Environment					
726	MedleyDB	"MedleyDB, is a dataset of annotated, royalty-free multitrack recordings. It was curated primarily to support research on melody extraction. For each song melody f₀ annotations are provided as well as instrument activations for evaluating automatic instrument recognition. The original dataset consists of 122 multitrack songs out of which 108 include melody annotations.
The songs in MedleyDB were obtained from the following sources:

Independent Artists (30 songs)
NYU's Dolan Recording Studio (32 songs)
Weathervane Music (25 songs)
Music Delta (35 songs)

MedleyDB contains songs of a variety of musical genres: Singer/Songwriter, Classical, Rock, World/Folk, Fusion, Jazz, Pop, Musical Theatre, Rap. For each song three types of audio content are given: a mix, stems, and raw audio. All types of audio files are .wav files with a sample rate of 44.1 kHz and a bit depth of 16.
Source: https://medleydb.weebly.com/
Image Source: https://medleydb.weebly.com/
Audio Source: https://zenodo.org/record/1438309"	https://paperswithcode.com/dataset/medleydb	01/01/2014						
727	MedleyDB 2.0	"MedleyDB 2.0 is a superset of the MedleyDB – a dataset of annotated, royalty-free multitrack recordings. The second iteration of the dataset includes 74 new multitrack recordings resulting in 194 songs in total.
Source: https://medleydb.weebly.com/
Image Source: https://medleydb.weebly.com/
Audio Source: https://zenodo.org/record/1438309"	https://paperswithcode.com/dataset/medleydb-2-0							
728	MIR-1K	"MIR-1K (Multimedia Information Retrieval lab, 1000 song clips) is a dataset designed for singing voice separation. It contains:

1000 song clips with the music accompaniment and the singing voice recorded as left and right channels, respectively,
Manual annotations of pitch contours in semitone, indices and types for unvoiced frames, lyrics, and vocal/non-vocal segments,
The speech recordings of the lyrics by the same person who sang the songs.

The duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 minutes. These clips are extracted from 110 karaoke songs which contain a mixture track and a music accompaniment track. These songs are freely selected from 5000 Chinese pop songs and sung by researchers from MIR lab (8 females and 11 males). Most of the singers are amateur and do not have professional music training.
Source: https://sites.google.com/site/unvoicedsoundseparation/mir-1k
Audio Source: https://sites.google.com/site/unvoicedsoundseparation/sounddemosforjournal"	https://paperswithcode.com/dataset/mir-1k	01/01/2010						
729	MagnaTagATune	"MagnaTagATune dataset contains 25,863 music clips. Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists. The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. Each audio clip is supplied with a vector of binary annotations of 188 tags. These annotations are obtained by humans playing the two-player online TagATune game. In this game, the two players are either presented with the same or a different audio clip. Subsequently, they are asked to come up with tags for their specific audio clip. Afterward, players view each other’s tags and are asked to decide whether they were presented the same audio clip. Tags are only assigned when more than two players agreed. The annotations include tags like ’singer’, ’no singer’, ’violin’, ’drums’, ’classical’, ’jazz’. The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag. There are 16 parts, and researchers comonnly use parts 1-12 for training, part 13 for validation and parts 14-16 for testing.
Source: Brains on Beats
Audio Source: http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset"	https://paperswithcode.com/dataset/magnatagatune	01/01/2009						
730	Lakh MIDI Dataset	"The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.
LMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.
A lakh is a unit of measure used in the Indian number system which signifies 100,000.
Source: https://colinraffel.com/projects/lmd/
Audio Source: https://colinraffel.com/projects/lmd/"	https://paperswithcode.com/dataset/lakh-midi-dataset							
731	iKala	"The iKala dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.
This dataset is not available anymore.
Source: http://mac.citi.sinica.edu.tw/ikala/"	https://paperswithcode.com/dataset/ikala	01/01/2015						
732	CAL500	"CAL500 (Computer Audition Lab 500) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:

29 instruments were annotated as present in the song or not,
22 vocal characteristics were annotated as relevant to the singer or not,
36 genres,
18 emotions were rated on a scale from one to three (e.g., not happy"",neutral"", ``happy""),
15 song concepts describing the acoustic qualities of the song, artist and recording (e.g., tempo, energy, sound quality),
15 usage terms (e.g., ""I would listen to this song while driving, sleeping, etc."").

Source: http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt
Audio Source: http://calab1.ucsd.edu/~datasets/cal500/cal500data/"	https://paperswithcode.com/dataset/cal500	01/01/2008	Computer Audition Lab 500					
733	URMP	"URMP (University of Rochester Multi-Modal Musical Performance) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.
Source: http://www2.ece.rochester.edu/projects/air/projects/URMP.html
Image Source: http://www2.ece.rochester.edu/projects/air/projects/URMP.html
Audio Source: http://www2.ece.rochester.edu/projects/air/projects/URMP.html"	https://paperswithcode.com/dataset/urmp	01/01/2019	University of Rochester Multi-Modal Musical Performance					
734	FMA	"The Free Music Archive (FMA) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.
There are four subsets defined by the authors:

Full: the complete dataset,
Large: the full dataset with audio limited to 30 seconds clips extracted from the middle of the tracks (or entire track if shorter than 30 seconds),
Medium: a selection of 25,000 30s clips having a single root genre,
Small: a balanced subset containing 8,000 30s clips with 1,000 clips per one of 8 root genres.

The official split into training, validation and test sets (80/10/10) uses stratified sampling to preserve the percentage of tracks per genre. Songs of the same artists are part of one set only.
Source: FMA: A Dataset For Music Analysis
Audio Source: https://github.com/mdeff/fma"	https://paperswithcode.com/dataset/fma	01/01/2017	Free Music Archive					
735	CCMixter	"CCMixter is a singing voice separation dataset consisting of 50 full-length stereo tracks from ccMixter featuring many different musical genres. For each song there are three WAV files available: the background music, the voice signal, and their sum.
Source: Kernel Additive Models for Source Separation
Audio Source: https://members.loria.fr/ALiutkus/kam/"	https://paperswithcode.com/dataset/ccmixter	01/01/2014						
736	GoodSounds	"GoodSounds dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (“good-sound”, “bad”, “scale-good”, “scale-bad”).
Source: A real-time system for measuring sound goodness in instrumental sounds
Image Source: A real-time system for measuring sound goodness in instrumental sounds
Audio Source: https://zenodo.org/record/820937"	https://paperswithcode.com/dataset/goodsounds							
737	Jamendo Corpus	"The Jamendo Corpus is a voice detection dataset consisting of 93 songs with Creative Commons license from the Jamendo free music sharing website. Segments of each song are annotated as “voice” (sung or spoken) or “no-voice”. The songs constitute a total of about 6 hours of music. The files are all from different artists and represent various genres from mainstream commercial music. The Jamendo audio files are coded in stereo Vorbis OGG 44.1kHz with 112KB/s bitrate. The original split contains 61, 16 and 16 songs in training, validation and testing set, respectively.
Source: Vocal detection in music with support vector machines
Audio Source: https://zenodo.org/record/2585988"	https://paperswithcode.com/dataset/jamendo-corpus	01/01/2008						
738	ForeDeCk	"ForeDeCk is a time series database compiled at the National Technical University of Athens that contains 900,000 continuous time series, built from multiple, diverse and publicly accessible sources. ForeDeCk emphasizes business forecasting applications, including series from relevant domains such as industries, services, tourism, imports & exports, demographics, education, labor & wage, government, households, bonds, stocks, insurances, loans, real estate, transportation, and natural resources & environment.
Source: Are forecasting competitions data representative of the reality?"	https://paperswithcode.com/dataset/foredeck							
739	M4	"The M4 dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.
The M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.
Source: The M4 competition: Results, findings, conclusion and way forward
Image Source: Are forecasting competitions data representative of the reality?"	https://paperswithcode.com/dataset/m4							
740	MUSDB18-HQ	"MUSDB18-HQ is a high-quality version of the MUSDB18 music tracks dataset. The high-quality dataset consists of the same 150 songs, but instead of MP4 files (compressed with Advanced Audio Coding encoder at 256kbps, with bandwidth limited to 16kHz), the songs are provided as raw WAV files.
Image Source: https://sigsep.github.io/datasets/musdb.html"	https://paperswithcode.com/dataset/musdb18-hq							
741	Slakh2100	"The Synthesized Lakh (Slakh) Dataset is a dataset for audio source separation that is synthesized from the Lakh MIDI Dataset v0.1 using professional-grade sample-based virtual instruments. This first release of Slakh, called Slakh2100, contains 2100 automatically mixed tracks and accompanying MIDI files synthesized using a professional-grade sampling engine. The tracks in Slakh2100 are split into training (1500 tracks), validation (375 tracks), and test (225 tracks) subsets, totaling 145 hours of mixtures.
Source: http://www.slakh.com/
Image Source: http://www.slakh.com/
Audio Source: http://www.slakh.com/"	https://paperswithcode.com/dataset/slakh2100	01/01/2019	Synthesized Lakh Dataset					
742	GuitarSet	"GuitarSet is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:

6 players,
2 versions: comping and soloing,
5 styles: Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk,
3 progressions: 12 Bar Blues, Autumn Leaves, and Pachelbel Canon,
2 tempi: slow and fast.

Each excerpt is annotated with 6 pitch contour and midi note annotations (one per string), 2 chord annotations (instructed and performed), beat and tempo annotations.
Source: https://guitarset.weebly.com/
Audio Source: https://zenodo.org/record/3371780"	https://paperswithcode.com/dataset/guitarset	01/01/2018						
743	Mixing Secrets	"Mixing Secrets is an instrument recognition dataset containing 258 multi-track recordings sourced from the Mixing Secrets for The Small Studio website. The dataset was labelled to be consistent with MedleyDB format.
Source: Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music
Image Source: Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music
Audio Source: https://multitracksearch.cambridge-mt.com/ms-mtk-search.htm"	https://paperswithcode.com/dataset/mixing-secrets							
744	OpenMIC-2018	"OpenMIC-2018 is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform.
Source: OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition
Image Source: OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition
Audio Source: https://zenodo.org/record/1432913"	https://paperswithcode.com/dataset/openmic-2018	01/01/2018						
745	CAL500exp	"The CAL500 Expansion (CAL500exp) dataset is an enriched version of the CAL500 music information retrieval dataset. CAL500exp is designed to facilitate music auto-tagging in a smaller temporal scale. The dataset consists of the same songs split into 3,223 acoustically homogenous segments of 3 to 16 seconds. The tag labels are annotated in the segment level instead of track level. The annotations were obtained from annotators with strong music background.
Source: Towards time-varying music auto-tagging based on CAL500 expansion
Image Source: Towards time-varying music auto-tagging based on CAL500 expansion
Audio Source: http://calab1.ucsd.edu/~datasets/cal500/cal500data/"	https://paperswithcode.com/dataset/cal500exp	01/01/2014	CAL500 Expansion					
746	CAL10K	"The CAL10K dataset (introduced as Swat10k) contains 10,870 songs that are weakly-labelled using a tag vocabulary of 475 acoustic tags and 153 genre tags. The tags have all been harvested from Pandora’s website and result from song annotations performed by expert musicologists involved with the Music Genome Project.
Source: Exploring automatic music annotation with “acoustically-objectiv” tags"	https://paperswithcode.com/dataset/cal10k		Computer Audition Lab 10000					
747	MuseScore	"The MuseScore dataset is a collection of 344,166 audio and MIDI pairs downloaded from MuseScore website. The audio is usually synthesized by the MuseScore synthesizer. The audio clips have diverse musical genres and are about two mins long on average.
Due to copyright issues the dataset is not publicly available, but can be collected and processed with the provided source code.
Source: Multitask learning for frame-level instrument recognition
Image Source: https://biboamy.github.io/instrument-demo/demo.html
Audio Source: Somewhere over the rainbow"	https://paperswithcode.com/dataset/musescore	01/01/2019						
748	MTG-Jamendo	"The MTG-Jamendo dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format.
A subset of the dataset is used in the Emotion and Theme Recognition in Music Task within MediaEval 2019.
Source: https://mtg.github.io/mtg-jamendo-dataset/
Audio Source: https://essentia.upf.edu/datasets/mtg-jamendo/raw_30s/audio/"	https://paperswithcode.com/dataset/mtg-jamendo							
749	LibriCount	"LibriCount is a synthetic dataset for speaker count estimation. The dataset contains a simulated cocktail party environment of 0 to 10 speakers, mixed with 0dB SNR from random utterances of different speakers from the LibriSpeech CleanTest dataset. All recordings are of 5s durations, and all speakers are active for the most part of the recording.
Source: https://faroit.com/#libricount
Image Source: https://faroit.com/#libricount
Audio Source: https://zenodo.org/record/1216072"	https://paperswithcode.com/dataset/libricount							
750	MultiWOZ	"The Multi-domain Wizard-of-Oz (MultiWOZ) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.
Source: Contents
Image Source: Zhang et al"	https://paperswithcode.com/dataset/multiwoz	01/01/2018	Multi-domain Wizard-of-Oz					
751	ReVerb Challenge	"The REVERB (REverberant Voice Enhancement and Recognition Benchmark) challenge is a benchmark for evaluation of automatic speech recognition techniques. The challenge assumes the scenario of capturing utterances spoken by a single stationary distant-talking speaker with 1-channe, 2-channel or 8-channel microphone-arrays in reverberant meeting rooms. It features both real recordings and simulated data.
The challenge constis of speech enhancement and automatic speech recognition tasks in reverberant environments. The speech enhancement challenge task consists of enhancing noisy reverberant speech with single-/multi-channel speech enhancement techniques, and evaluating the enhanced data in terms of objective and subjective evaluation metrics. The automatic speech recognition challenge task consists of improving the recognition accuracy of the same reverberant speech. The background noise is mostly stationary and the signal-to-noise ratio is modest.
Source: https://reverb2014.dereverberation.com/index.html"	https://paperswithcode.com/dataset/reverb-challenge	01/01/2013	REverberant Voice Enhancement and Recognition Benchmark					
752	PERSONA-CHAT	"The PERSONA-CHAT dataset contains multi-turn dialogues conditioned on personas. The dataset consists of 8939 complete dialogues for training, 1000 for validation, and 968 for testing. Each dialogue was performed between two crowd-source workers assuming artificial personas (described by 3 to 5 profile sentences, such as “I like to ski”, “I am an artist”, “I eat sardines for breakfast daily”). There are 955 possible personas for training, 100 for validation, and 100 for testing. Additionally, a version of revised persona descriptions are also provided by rephrasing, generalizing, or specializing the original ones.
Source: Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots
Image Source: https://arxiv.org/pdf/1801.07243.pdf"	https://paperswithcode.com/dataset/persona-chat-1	01/01/2018						
753	MPQA Opinion Corpus	"The MPQA Opinion Corpus contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).
Source: http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf
Image Source: https://mpqa.cs.pitt.edu/"	https://paperswithcode.com/dataset/mpqa-opinion-corpus	01/01/2005	Multi-Perspective Question Answering					
754	DROP	"Discrete Reasoning Over Paragraphs DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set.
Source: https://allennlp.org/drop
Image Source: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"	https://paperswithcode.com/dataset/drop	01/01/2019	Discrete Reasoning Over Paragraphs					
755	New York Times Annotated Corpus	"The New York Times Annotated Corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:

Over 1.8 million articles (excluding wire services articles that appeared during the covered period).
Over 650,000 article summaries written by library scientists.
Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.
Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.
As part of the New York Times' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions ""Bill Clinton"" and another refers to ""President William Jefferson Clinton"", both articles will be tagged with ""CLINTON, BILL"".

Source: https://catalog.ldc.upenn.edu/LDC2008T19"	https://paperswithcode.com/dataset/new-york-times-annotated-corpus	17/10/2008						
756	VisDial	"Visual Dialog (VisDial) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the other person acted as an ‘answerer’. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to “imagine the scene better”. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.
VisDial v1.0 contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.
Source: Granular Multimodal Attention Networks for Visual Dialog
Image Source: https://arxiv.org/pdf/1611.08669.pdf"	https://paperswithcode.com/dataset/visdial	01/01/2017	Visual Dialog					
757	AMR Bank	"The AMR Bank is a set of English sentences paired with simple, readable semantic representations. Version 3.0 released in 2020 consists of 59,255 sentences.
Each AMR is a single rooted, directed graph. AMRs include PropBank semantic roles, within-sentence coreference, named entities and types, modality, negation, questions, quantities, and so on.
The image presents an AMR of a sample sentence “The boy wants to go”.
Source: https://amr.isi.edu/
Image Source: https://amr.isi.edu/language.html"	https://paperswithcode.com/dataset/amr-bank	01/01/2013	Abstract Meaning Representation					
758	WMT 2016	"WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.
The conference featured ten shared tasks:

a news translation task,
an IT domain translation task,
a biomedical translation task,
an automatic post-editing task,
a metrics task (assess MT quality given reference translation).
a quality estimation task (assess MT quality without access to any reference),
a tuning task (optimize a given MT system),
a pronoun translation task,
a bilingual document alignment task,
a multimodal translation task.

Source: http://www.statmt.org/wmt16/index.html"	https://paperswithcode.com/dataset/wmt-2016	01/01/2016						
759	WMT 2016 News	"News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.
The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.
Some training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10⁹ corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.
Source: https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/
Image Source: https://www.aclweb.org/anthology/W16-2301.pdf"	https://paperswithcode.com/dataset/wmt-2016-news	01/01/2016	WMT 2016 News Translation Task					
760	WMT 2016 IT	"The IT Translation Task is a shared task introduced in the First Conference on Machine Translation. Compared to WMT 2016 News, this task brought several novelties to WMT:

4 out of the 7 langauges of the IT task are new in WMT,
adaptation to the IT domain with its specifics such as frequent named entities (mostly menu items, names of products and companies) and technical jargon,
adaptation to translation of answers in helpdesk service setting (many of the sentences are instructions with imperative verbs, which is very rare in the News translation task).

The test set consisted of 1000 answers from the Batch 3 of the QTLeap Corpus. The in-domain training data contained 2000 answers from the Batches 1 and 2 and also localization files from several open-source projects (LibreOffice, KDE, VLC) and bilingual dictionaries of IT-related terms extracted from Wikipedia. The out-of-domain training data contained all the corpora from the WMT 2016 News, plus PaCo2-EuEn Basque-English corpus and SETimes with Bulgarian-English parallel sentences. “Constrained” systems were restricted to use only these training data provided by the organizers.
The task was evaluated on the following language pairs:

English → Bulgarian
English → Czech
English → German
English → Spanish
English → Basque
English → Dutch
English → Portuguese

Source: http://www.statmt.org/wmt16/index.html
Image Source: Bojar et al"	https://paperswithcode.com/dataset/wmt-2016-it	01/01/2016	WMT 2016 IT Translation Task					
761	WMT 2016 Biomedical	"The Biomedical Translation Shared Task was first introduced at the First Conference of Machine Translation. The task aims to evaluate systems for the translation of biomedical titles and abstracts from scientific publications. The data includes three language pairs (English ↔ Portuguese, English  ↔ Spanish, English  ↔ French) and two sub-domains of biological sciences and health sciences.
The training data consists mainly of the Scielo corpus, a parallel collection of scientific publications composed of either titles, abstracts or title and abstracts which were retrieved from the Scielo database. For the Scielo corpus, a parallel documents are provided for all language pairs in the two sub-domains, except for the English  ↔ French, where only health was considered, as there were inadequate parallel documents available for biology in that pair. The training data was aligned using the GMA alignment tool. Additionally, a corpus of parallel titles from MEDLINEⓇ for all three language pairs were provided as well as monolingual documents for the four languages, retrieved from the Scielo database. These consist of documents in the Scielo database which have no corresponding document in another language.
The test set consisted of 500 documents (title and abstract) for each of the two directions of each language pair. None of the test documents was included in the training data and there is no overlap of documents between the test sets for any language pair, translation direction and sub-domain.
Source: http://www.statmt.org/wmt16/index.html
Image Source: https://www.aclweb.org/anthology/W16-2301.pdf"	https://paperswithcode.com/dataset/wmt-2016-biomedical	01/01/2016	WMT 2016 Biomedical Translation Task					
762	XSum	"The Extreme Summarization (XSum) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question “What is the article about?”. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.
Source: https://arxiv.org/pdf/1808.08745.pdf
Image Source: https://arxiv.org/pdf/1808.08745.pdf"	https://paperswithcode.com/dataset/xsum	01/01/2018						
763	WMT 2014	"WMT 2014 is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:

a news translation task,
a quality estimation task,
a metrics task,
a medical text translation task.

Source: https://www.aclweb.org/anthology/W14-3302.pdf"	https://paperswithcode.com/dataset/wmt-2014	01/01/2014						
764	WMT 2014 Medical	"The Medical Translation Task of WMT 2014 addresses the problem of domain-specific and genre-specific machine translation. The task is split into two subtasks: summary translation, focused on translation of sentences from summaries of medical articles, and query translation, focused on translation of queries entered by users into medical information search engines. Both subtasks included translation between English and Czech, German, and French, in both directions.
Source: https://www.aclweb.org/anthology/W14-3302.pdf"	https://paperswithcode.com/dataset/wmt-2014-medical	01/01/2014	WMT 2014 Medical Translation Task					
765	WMT 2015	"WMT 2015 is a collection of datasets used in shared tasks of the Tenth Workshop on Statistical Machine Translation. The workshop featured five tasks:

a news translation task,
a metrics task,
a tuning task,
a quality estimation task,
an automatic post-editing task.

Source: https://www.aclweb.org/anthology/W15-3001.pdf"	https://paperswithcode.com/dataset/wmt-2015	01/01/2015						
766	WMT 2015 News	"News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, French, Russian) and additional 1500 sentences from each of the 5 languages translated to English. The sentences are taken from newspaper articles for each language pair, except for French, where the test set was drawn from user-generated comments on the news articles (from Guardian and Le Monde). The translation was done by professional translators.
The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.
Some training corpora were identical from WMT 2014 (Europarl, United Nations, French-English 10⁹ corpus, CzEng, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (News Commentary, monolingual news data). Additionally, the Finnish Europarl and Finnish-English Wikipedia Headline corpus were added.
Source: https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/
Image Source: httpshttps://www.aclweb.org/anthology/W15-3001.pdf"	https://paperswithcode.com/dataset/wmt-2015-news	01/01/2015	WMT 2015 News Translation Task					
767	SHAPES	"SHAPES is a dataset of synthetic images designed to benchmark systems for understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, e.g. Is there a red shape above a circle? or Is a red shape blue?. Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases.
Each image is a 30×30 RGB image depicting a 3×3 grid of objects. Each object is characterized by shape (circle, square, triangle), colour (red, green, blue) and size (small, big).
Source: Visual Question Answering: A Survey of Methods and Datasets
Image Source: https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset"	https://paperswithcode.com/dataset/shapes-1	01/01/2016	Swarm Heuristics based Adaptive and Penalized Estimation of Splines					
768	AG’s Corpus	"Antonio Gulli’s corpus of news articles is a collection of more than 1 million news articles. The articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004.
The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non - commercial activity.
A subset of this corpus, AG News, consisting of the 4 largest classes is a popular topic classification dataset.
Source: AG's corpus of news articles"	https://paperswithcode.com/dataset/ags-corpus		AG's corpus of news articlesNews					
769	QUASAR-S	"QUASAR-S is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 37,362 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The answer to each question is restricted to be another software entity, from an output vocabulary of 4874 entities.
Source: Quasar: Datasets for Question Answering by Search and Reading
Image Source: Quasar: Datasets for Question Answering by Search and Reading"	https://paperswithcode.com/dataset/quasar-s	01/01/2017	QUestion Answering by Search And Reading – Stack Overflow					
770	QUASAR-T	"QUASAR-T is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 43,013 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. The answers to these questions are free-form spans of text, though most are noun phrases.
Source: Quasar: Datasets for Question Answering by Search and Reading
Image Source: Quasar: Datasets for Question Answering by Search and Reading"	https://paperswithcode.com/dataset/quasar-t	01/01/2017	QUestion Answering by Search And Reading – Trivia					
771	MLDoc	"Multilingual Document Classification Corpus (MLDoc) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:

uniform class coverage: same number of examples for each class and language,
official train / development / test split: for each language a training data of different sizes (1K, 2K, 5K and 10K stories), a development (1K) and a test corpus (4K) are provided (with exception of Spanish and Russian with 9458 and 5216 training documents respectively.

Source: A Corpus for Multilingual Document Classification in Eight Languages"	https://paperswithcode.com/dataset/mldoc	01/01/2018	Multilingual Document Classification Corpus					
772	WMT 2018	"WMT 2018 is a collection of datasets used in shared tasks of the Third Conference on Machine Translation. The conference builds on a series of twelve previous annual workshops and conferences on Statistical Machine Translation.
The conference featured ten shared tasks:

a news translation task,
a biomedical translation task,
a multimodal machine translation task,
a metrics task,
a quality estimation task,
an automatic post-editing task,
a parallel corpus filtering task.

Source: http://www.statmt.org/wmt18/"	https://paperswithcode.com/dataset/wmt-2018	01/01/2018						
773	WMT 2018 News	"News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Chinese, Czech, Estonian, German, Finnish, Russian, Turkish) and additional 1500 sentences from each of the 7 languages translated to English. The sentences were selected from dozens of news websites and translated by professional translators.
The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.
Some training corpora were identical from WMT 2017 (Europarl, Common Crawl, SETIMES2, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (United Nations, CzEng v1.7, News Commentary v13, monolingual news data).
Additionally, the EU Press Release parallel corpus for German, Finnish and Estonian was added.
Source: https://www.statmt.org/wmt18/translation-task.html"	https://paperswithcode.com/dataset/wmt-2018-news	01/01/2018	WMT 2018 News Translation Task					
774	ArxivPapers	"The ArxivPapers dataset is an unlabelled collection of over 104K papers related to machine learning and published on arXiv.org between 2007–2020. The dataset includes around 94K papers (for which LaTeX source code is available) in a structured form in which paper is split into a title, abstract, sections, paragraphs and references. Additionally, the dataset contains over 277K tables extracted from the LaTeX papers.
Due to the papers license the dataset is published as a metadata and open-source pipeline that can be used to obtain and convert the papers. 
Source: AxCell: Automatic Extraction of Results from Machine Learning Papers
Image Source: AxCell: Automatic Extraction of Results from Machine Learning Papers"	https://paperswithcode.com/dataset/arxivpapers							
775	SegmentedTables	"The SegmentedTables dataset is a collection of almost 2,000 tables extracted from 352 machine learning papers. Each table consists of rich text content, layout and caption. Tables are annotated with types (leaderboard, ablation, irrelevant) and cells of relevant tables are annotated with semantic roles (such as “paper model”, “competing model”, “dataset”, “metric”).
Due to the license of source papers the dataset is published as a metadata, all annotations and open-source pipeline that can be used to extract the tables.
Source: AxCell: Automatic Extraction of Results from Machine Learning Papers
Image Source: AxCell: Automatic Extraction of Results from Machine Learning Papers"	https://paperswithcode.com/dataset/segmentedtables							
776	LinkedResults	"The LinkedResults dataset contains around 1,600 results capturing performance of machine learning models from tables of 239 papers. All tables come from a subset of SegmentedTables dataset. Each result is a tuple of form (task, dataset, metric name, metric value) and is linked to a particular table, row and cell it originates from.
Source: AxCell: Automatic Extraction of Results from Machine Learning Papers"	https://paperswithcode.com/dataset/linkedresults	29/04/2020						
777	PWC Leaderboards	"The Papers with Code Leaderboards dataset is a collection of over 5,000 results capturing performance of machine learning models. Each result is a tuple of form (task, dataset, metric name, metric value). The data was collected using the Papers with Code review interface.
Source: AxCell: Automatic Extraction of Results from Machine Learning Papers
Image Source: AxCell: Automatic Extraction of Results from Machine Learning Papers"	https://paperswithcode.com/dataset/pwc-leaderboards		Papers with Code Leaderboards					
778	SKU110K	"The Sku110k dataset provides 11,762 images with more than 1.7 million annotated bounding boxes captured in densely packed scenarios, including 8,233 images for training, 588 images for validation, and 2,941 images for testing. There are around 1,733,678 instances in total. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [−15∘, 15∘].
Source: Rethinking Object Detection in Retail Stores
Image Source: https://github.com/eg4000/SKU110K_CVPR19"	https://paperswithcode.com/dataset/sku110k	01/01/2019	SKU110K					
779	UBIRIS.v2	"The UBIRIS.v2 iris dataset contains 11,102 iris images from 261 subjects with 10 images each subject. The images were captured under unconstrained conditions (at-a-distance, on-the-move and on the visible wavelength), with realistic noise factors.
Source: Constrained Design of Deep Iris Networks
Image Source: https://arxiv.org/pdf/1905.09481.pdf"	https://paperswithcode.com/dataset/ubiris-v2	01/01/2010	UBIRIS.v2					
780	VIVA	"The VIVA challenge’s dataset is a multimodal dynamic hand gesture dataset specifically designed with difficult settings of cluttered background, volatile illumination, and frequent occlusion for studying natural human activities in real-world driving settings. This dataset was captured using a Microsoft Kinect device, and contains 885 intensity and depth video sequences of 19 different dynamic hand gestures performed by 8 subjects inside a vehicle.
Source: Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition
Image Source: http://www.site.uottawa.ca/research/viva/projects/hand_detection/index.html"	https://paperswithcode.com/dataset/viva		Vision for Intelligent Vehicles and Applications					
781	ITOP	"The ITOP dataset consists of 40K training and 10K testing depth images for each of the front-view and top-view tracks. This dataset contains depth images with 20 actors who perform 15 sequences each and is recorded by two Asus Xtion Pro cameras. The ground-truth of this dataset is the 3D coordinates of 15 body joints.
Source: V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map
Image Source: https://www.youtube.com/watch?v=4gPI-GOf9wg"	https://paperswithcode.com/dataset/itop	01/01/2016	Invariant-Top View Dataset					
782	Dayton	"The Dayton dataset is a dataset for ground-to-aerial (or aerial-to-ground) image translation, or cross-view image synthesis. It contains images of road views and aerial views of roads. There are 76,048 images in total and the train/test split is 55,000/21,048. The images in the original dataset have 354×354 resolution.
Source: Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation
Image Source: https://arxiv.org/abs/1912.06112"	https://paperswithcode.com/dataset/dayton	01/01/2016	Dayton					
783	AOLP	"The application-oriented license plate (AOLP) benchmark database has 2049 images of Taiwan license plates. This database is categorized into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples. AC refers to the cases that a vehicle passes a fixed passage with a lower speed or full stop. This is the easiest situation. The images are captured under different illuminations and different weather conditions. LE refers to the cases that a vehicle violates traffic laws and is captured by roadside camera. The background are really cluttered, with road sign and multiple plates in one image. RP refers to the cases that the camera is held on a patrolling vehicle, and the images are taken with arbitrary viewpoints and distances.
Source: Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs
Image Source: http://aolpr.ntust.edu.tw/lab/index.html"	https://paperswithcode.com/dataset/aolp	01/01/2013	Application-oriented License Plate					
784	Set11	"Set11 is a dataset of 11 grayscale images. It is a dataset used for image reconstruction and image compression.
Source: ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing
Image Source: https://arxiv.org/pdf/1706.07929.pdf"	https://paperswithcode.com/dataset/set11	01/01/2016	Set11					
785	SALICON	"The SALIency in CONtext (SALICON) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.
The ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.
The training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.
The testing data contains only the image and resolution fields.
Source: DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations
Image Source: http://salicon.net/explore/"	https://paperswithcode.com/dataset/salicon	01/01/2015	Salicency in Context					
786	GRID Dataset	"The QMUL underGround Re-IDentification (GRID) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution.
Source: https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html"	https://paperswithcode.com/dataset/grid							
787	Flickr30K Entities	"The Flickr30K Entities dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image.
Source: http://bryanplummer.com/Flickr30kEntities/
Image Source: http://bryanplummer.com/Flickr30kEntities/"	https://paperswithcode.com/dataset/flickr30k-entities	01/01/2015						
788	FGVC-Aircraft	"FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.
Aircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:

Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.
Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.
Family, e.g. Boeing 737. The dataset comprises 70 different families.
Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.
The data is divided into three equally-sized training, validation and test subsets.

Source: https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/
Image Source: Fine-Grained Visual Classification of Aircraft"	https://paperswithcode.com/dataset/fgvc-aircraft-1	01/01/2013						
789	DUTS	"DUTS is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.
Source: http://saliencydetection.net/duts/
Image Source: https://ieeexplore.ieee.org/document/8099887"	https://paperswithcode.com/dataset/duts	01/01/2017						
790	LIP	"The LIP (Look into Person) dataset is a large-scale dataset focusing on semantic understanding of a person. It contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels and 2D human poses with 16 key points. The images are collected from real-world scenarios and the subjects appear with challenging poses and view, heavy occlusions, various appearances and low resolution.
Source: http://sysu-hcp.net/lip/
Image Source: http://sysu-hcp.net/lip/"	https://paperswithcode.com/dataset/lip	01/01/2017	Look into Person					
791	ApolloScape	"ApolloScape is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.
Source: A2D2: Audi Autonomous Driving Dataset
Image Source: https://arxiv.org/pdf/1803.06184.pdf"	https://paperswithcode.com/dataset/apolloscape-1	01/01/2018						
792	PoseTrack	"The PoseTrack dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance.
Source: Simple Baselines for Human Pose Estimation and Tracking
Image Source: https://posetrack.net/"	https://paperswithcode.com/dataset/posetrack	01/01/2018						
793	ICVL Hand Posture	"The ICVL dataset is a hand pose estimation dataset that consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame.
Source: AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation
Image Source: Tang et al.; Latent Regression Forest: Structured Estimation of 3D Hand Poses"	https://paperswithcode.com/dataset/icvl-hand-posture	01/01/2014	ICVL Hand Posture Dataset					
794	SegTrack-v2	"SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video.
Source: https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html
Image Source: https://www.researchgate.net/publication/325842926_Semantic_Video_Segmentation_A_Review_on_Recent_Approaches"	https://paperswithcode.com/dataset/segtrack-v2-1	01/01/2013						
795	Foggy Cityscapes	"Foggy Cityscapes is a synthetic foggy dataset which simulates fog on real scenes. Each foggy image is rendered with a clear image and depth map from Cityscapes. Thus the annotations and data split in Foggy Cityscapes are inherited from Cityscapes.
Source: Exploring Object Relation in Mean Teacher for Cross-Domain Detection
Image Source: http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/"	https://paperswithcode.com/dataset/foggy-cityscapes	01/01/2018						
796	Vimeo90K	"The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.
Source: https://arxiv.org/pdf/1711.09078.pdf
Image Source: http://toflow.csail.mit.edu/"	https://paperswithcode.com/dataset/vimeo90k-1	01/01/2019						
797	MPIIGaze	"MPIIGaze is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.
Source: https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild
Image Source: https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild"	https://paperswithcode.com/dataset/mpiigaze	01/01/2015						
798	ReferItGame	"The ReferIt dataset contains 130,525 expressions for referring to 96,654 objects in 19,894 images of natural scenes.
Source: BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up Visual Processing with Referring Expressions
Image Source: http://tamaraberg.com/referitgame/"	https://paperswithcode.com/dataset/referitgame	01/01/2014	ReferItGame					
799	MultiTHUMOS	"The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.
Source: http://ai.stanford.edu/~syyeung/everymoment.html
Image Source: http://ai.stanford.edu/~syyeung/everymoment.html"	https://paperswithcode.com/dataset/multithumos	01/01/2018						
800	CrowdHuman	"CrowdHuman is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10× boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with ∼340k person and ∼99k ignore region annotations in the CrowdHuman training subset.
Source: SADet: Learning An Efficient and Accurate Pedestrian Detector
Image Source: http://www.crowdhuman.org/"	https://paperswithcode.com/dataset/crowdhuman	01/01/2018						
801	MSRDailyActivity3D	"DailyActivity3D dataset is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, sit down. If possible, each subject performs an activity in two different poses: “sitting on sofa” and “standing”. The total number of the activity samples is 320.
This dataset is designed to cover human’s daily activities in the living room. When the performer stands close to the sofa or sits on the sofa, the 3D joint positions extracted by the skeleton tracker are very noisy. Moreover, most of the activities involve the humans-object interactions. Thus this dataset is more challenging.
Source: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf
Image Source: https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition"	https://paperswithcode.com/dataset/msrdailyactivity3d	01/01/2012						
802	McMaster	"The McMaster dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500×500.
Source: FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising
Image Source: https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf"	https://paperswithcode.com/dataset/mcmaster	01/01/2011						
803	Sketch	"The Sketch dataset contains over 20,000 sketches evenly distributed over 250 object categories.
Source: http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/
Image Source: http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/"	https://paperswithcode.com/dataset/sketch	01/01/2012						
804	Wireframe	"The Wireframe dataset consists of 5,462 images (5,000 for training, 462 for test) of indoor and outdoor man-made scenes.
Source: MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line Segment Detection
Image Source: https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf"	https://paperswithcode.com/dataset/wireframe	01/01/2018						
805	MNIST-M	"MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.
Source: A Review of Single-Source Deep Unsupervised Visual Domain Adaptation
Image Source: https://arxiv.org/pdf/1505.07818v4.pdf"	https://paperswithcode.com/dataset/mnist-m	01/01/2017						
806	SPIDER	"Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. In Spider 1.0, different complex SQL queries and databases appear in train and test sets. To do well on it, systems must generalize well to not only new SQL queries but also new database schemas.
Source: Spider
Image Source: https://yale-lily.github.io/spider"	https://paperswithcode.com/dataset/spider-1	01/01/2018	SPIDER					
807	tieredImageNet	"The tieredImageNet dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes.
Source: tieredImageNet
Image Source: https://arxiv.org/pdf/1803.00676.pdf"	https://paperswithcode.com/dataset/tieredimagenet	01/01/2018	tieredImageNet					
808	aPY	"aPY is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, …, zebra).
Source: From Classical to Generalized Zero-Shot Learning: a Simple Adaptation Process
Image Source: https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf"	https://paperswithcode.com/dataset/apy	01/01/2009	Attribute Pascal and Yahoo					
809	VisDA-2017	"VisDA-2017 is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..
Source: Gradually Vanishing Bridge for Adversarial Domain Adaptation
Image Source: http://ai.bu.edu/visda-2017/"	https://paperswithcode.com/dataset/visda-2017	01/01/2017	VisDA-2017					
810	PDBBind	"The PDBBind database provides a comprehensive collection of structures of protein-ligand complexes and their binding affinity data. The original experimental data in Protein Data Bank (PDB) are selected to PDBBind database based on certain quality requirements and curated for applications.
Source: Representability of algebraic topology for biomolecules in machine learning based scoring and virtual screening
Image Source: http://www.pdbbind.org.cn/browse.php"	https://paperswithcode.com/dataset/pdbbind-1	01/01/2015	PDBBind					
811	ImageNet-32	"Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels.
Source: Self-supervised Knowledge Distillation Using Singular Value Decomposition
Image Source: https://arxiv.org/pdf/1707.08819v3.pdf"	https://paperswithcode.com/dataset/imagenet-32	01/01/2017						
812	MVTecAD	"MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects.
There are two common metrics: Detection AUROC and Segmentation (or pixelwise) AUROC
Detection (or, classification) methods output single float (anomaly score) per input test image. 
Segmentation methods output anomaly probability for each pixel. 
""To assess segmentation performance, we evaluate the relative per-region overlap of the segmentation with the ground truth. To get an additional performance measure that is independent of the determined threshold, we compute the area under the receiver operating characteristic curve (ROC AUC). We define the true positive rate as the percentage of pixels that were correctly classified as anomalous"" [1]
Later segmentation metric was improved to balance regions with small and large area, see PRO-AUC and other in [2]
Source: MVTEC ANOMALY DETECTION DATASET
Image Source: https://www.mvtec.com/company/research/datasets/mvtec-ad/
[1] Paul Bergmann et al, ""MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection""
[2] Bergmann, P., Batzner, K., Fauser, M. et al. The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Int J Comput Vis (2021). https://doi.org/10.1007/s11263-020-01400-4"	https://paperswithcode.com/dataset/mvtecad	01/01/2019	MVTEC ANOMALY DETECTION DATASET					
813	Kvasir	"The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.
Source: Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis
Image Source: https://datasets.simula.no/kvasir/"	https://paperswithcode.com/dataset/kvasir	01/01/2017	The Kvasir Dataset					
814	Syn2Real	"Syn2Real, a synthetic-to-real visual domain adaptation benchmark meant to encourage further development of robust domain transfer methods. The goal is to train a model on a synthetic ""source"" domain and then update it so that its performance improves on a real ""target"" domain, without using any target annotations. It includes three tasks, illustrated in figures above: the more traditional closed-set classification task with a known set of categories; the less studied open-set classification task with unknown object categories in the target domain; and the object detection task, which involves localizing instances of objects by predicting their bounding boxes and corresponding class labels.
Source: Syn2Real
Image Source: https://ai.bu.edu/syn2real/"	https://paperswithcode.com/dataset/syn2real	01/01/2018	Syn2Real					
815	ANLI	"The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.
Source: The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding
Image Source: https://arxiv.org/pdf/1910.14599.pdf"	https://paperswithcode.com/dataset/anli	01/01/2019	Adversarial NLI					
816	Cityscapes	"Cityscapes is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.
Source: A Review on Deep Learning Techniques Applied to Semantic Segmentation
Image Source: https://www.cityscapes-dataset.com/dataset-overview/"	https://paperswithcode.com/dataset/cityscapes	01/01/2016						
817	PASCAL VOC	"The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.
Source: Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey
Image Source: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg"	https://paperswithcode.com/dataset/pascal-voc	01/01/2010	PASCAL Visual Object Classes Challenge					
818	VGG Face	"The VGG Face dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images.
Source: https://www.robots.ox.ac.uk/~vgg/data/vgg_face/
Image Source: http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf"	https://paperswithcode.com/dataset/vgg-face-1	01/01/2015						
819	LibriSpeech	"The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.
Source: State-of-the-art Speech Recognition using Multi-stream Self-attention with Dilated 1D Convolutions"	https://paperswithcode.com/dataset/librispeech	01/01/2015						
820	CASIA-WebFace	"The CASIA-WebFace dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.
Source: On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs"	https://paperswithcode.com/dataset/casia-webface	01/01/2014						
821	Set14	"The Set14 dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.
Image Source: https://www.ece.rice.edu/~wakin/images/"	https://paperswithcode.com/dataset/set14	01/01/2010						
822	MS-Celeb-1M	"The MS-Celeb-1M dataset is a large-scale face recognition dataset consists of 100K identities, and each identity has about 100 facial images. The original identity labels are obtained automatically from webpages.
NOTE: This dataset is currently inactive. 
Source: Learning to Cluster Faces on an Affinity Graph
Image Source: MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition"	https://paperswithcode.com/dataset/ms-celeb-1m	01/01/2016						
823	UCI Machine Learning Repository	UCI Machine Learning Repository is a collection of over 550 datasets.	https://paperswithcode.com/dataset/uci-machine-learning-repository							
824	SYNTHIA	"The SYNTHIA dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.
Source: Orientation-aware Semantic Segmentation on Icosahedron Spheres
Image Source: https://synthia-dataset.net/"	https://paperswithcode.com/dataset/synthia	01/01/2016	SYNTHetic Collection of Imagery and Annotations					
825	NYUv2	"The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:

1449 densely labeled pairs of aligned RGB and depth images
464 new scenes taken from 3 cities
407,024 new unlabeled frames
Each object is labeled with a class and an instance number.
The dataset has several components:
Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.
Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.
Toolbox: Useful functions for manipulating the data and labels.

Source: https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html
Image Source: https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html"	https://paperswithcode.com/dataset/nyuv2	01/01/2012	NYU-Depth V2					
826	Urban100	"The Urban100 dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.
Image Source: http://vllab.ucmerced.edu/wlai24/LapSRN/"	https://paperswithcode.com/dataset/urban100	01/01/2015						
827	VGGFace2	"The VGGFace2 dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity. The dataset is divided into two splits, one for the training and one for test. The latter contains around 170000 images divided into 500 identities while all the other images belong to the remaining 8631 classes available for training. While constructing the datasets, the authors focused their efforts on reaching a very low label noise and a high pose and age diversity thus, making the VGGFace2 dataset a suitable choice to train state-of-the-art deep learning models on face-related tasks. The images of the training set have an average resolution of 137x180 pixels, with less than 1% at a resolution below 32 pixels (considering the shortest side).
CAUTION: Authors note that the distribution of identities in the VGG-Face dataset may not be representative of the global human population. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.
Source: Cross-Resolution Learning for Face Recognition"	https://paperswithcode.com/dataset/vggface2-1	01/01/2018						
828	PASCAL3D+	"The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.
Source: Convolutional Models for Joint Object Categorization and Pose Estimation
Image Source: Beyond PASCAL: A benchmark for 3D object detection in the wild"	https://paperswithcode.com/dataset/pascal3d-2	01/01/2014						
829	SUN RGB-D	"The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.
Source: Mix and match networks: multi-domain alignment for unpaired image-to-image translation
Image Source: https://rgbd.cs.princeton.edu/"	https://paperswithcode.com/dataset/sun-rgb-d	01/01/2015	SUN RGB-D					
830	SUNCG	"SUNCG is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.
The dataset is currently not available.
Source: https://sscnet.cs.princeton.edu/
Image Source: https://sscnet.cs.princeton.edu/"	https://paperswithcode.com/dataset/suncg	01/01/2017	SUNCG					
831	Places205	"The Places205 dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).
Source: Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs
Image Source: http://places.csail.mit.edu/browser.html"	https://paperswithcode.com/dataset/places205	01/01/2014						
832	ModelNet	"The ModelNet40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.
Source: Geometric Feedback Network for Point Cloud Classification"	https://paperswithcode.com/dataset/modelnet	01/01/2015						
833	YAGO	"Yet Another Great Ontology (YAGO) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.
Source: Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches
Image Source: https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/"	https://paperswithcode.com/dataset/yago	01/01/2007	Yet Another Great Ontology					
834	WIDER FACE	"The WIDER FACE dataset contains 32,203 images and labels 393,703 faces with a high degree of variability in scale, pose and occlusion. The database is split into training (40%), validation (10%) and testing (50%) set. Besides, the images are divided into three levels (Easy ⊆ Medium ⊆ Hard) according to the difficulties of the detection. The images and annotations of training and validation set are available online, while the annotations of testing set are not released and the results are sent to the database server for receiving the precision-recall curves.
Source: S{}^{3}FD: Single Shot Scale-invariant Face Detector"	https://paperswithcode.com/dataset/wider-face-1	01/01/2016						
835	MPI Sintel	"MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.
Source: Fast Disparity Estimation using Dense Networks*"	https://paperswithcode.com/dataset/mpi-sintel	01/01/2012						
836	Helen	"The HELEN dataset is composed of 2330 face images of 400×400 pixels with labeled facial components generated through manually-annotated contours along eyes, eyebrows, nose, lips and jawline.
Source: Face Parsing via a Fully-Convolutional Continuous CRF Neural Network
Image Source: http://www.ifp.illinois.edu/~vuongle2/helen/"	https://paperswithcode.com/dataset/helen	01/01/2012						
837	FrameNet	"FrameNet is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word.
Source: Retrofitting Distributional Embeddings to Knowledge Graphswith Functional Relations"	https://paperswithcode.com/dataset/framenet	01/01/1998						
838	LSUN	"The Large-scale Scene Understanding (LSUN) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category.
Source: Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs
Image Source: https://www.yf.io/p/lsun"	https://paperswithcode.com/dataset/lsun	01/01/2015	Large-scale Scene UNderstanding Challenge					
839	LFPW	"The Labeled Face Parts in-the-Wild (LFPW) consists of 1,432 faces from images downloaded from the web using simple text queries on sites such as google.com, flickr.com, and yahoo.com.   Each image was labeled by three MTurk workers, and 29 fiducial points, shown below, are included in dataset.
Source: https://neerajkumar.org/databases/lfpw/
Image Source: https://neerajkumar.org/databases/lfpw/"	https://paperswithcode.com/dataset/lfpw	01/01/2011	Labeled Face Parts in the Wild					
840	CARLA	"CARLA (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).
Source: Synthetic Data for Deep Learning"	https://paperswithcode.com/dataset/carla	10/11/2017	Car Learning to Act					
841	OTB	"Object Tracking Benchmark (OTB) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. OTB-2013 dataset contains 51 sequences and the OTB-2015 dataset contains all 100 sequences of the OTB dataset.
Source: Deep Meta Learning for Real-Time Target-Aware Visual Tracking"	https://paperswithcode.com/dataset/otb	01/01/2015						
842	Places365	"The Places365 dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).
Source: Semantic-Aware Scene Recognition
Image Source: Places"	https://paperswithcode.com/dataset/places365	05/09/2019						
843	Extended Yale B	"The Extended Yale B database contains 2414 frontal-face images with size 192×168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions.
Source: Learning Locality-Constrained Collaborative Representation for Robust Face Recognition
Image Source: http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html"	https://paperswithcode.com/dataset/extended-yale-b-1	01/01/2001						
844	IMDb Movie Reviews	"The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.
Source: http://nlpprogress.com/english/sentiment_analysis.html
Image Source: Maas et al"	https://paperswithcode.com/dataset/imdb-movie-reviews	01/01/2011						
845	BookCorpus	"BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).
Source: Temporal Event Knowledge Acquisition via Identifying Narratives"	https://paperswithcode.com/dataset/bookcorpus	01/01/2015						
846	FaceWarehouse	"FaceWarehouse is a 3D facial expression database that provides the facial geometry of 150 subjects, covering a wide range of ages and ethnic backgrounds.
Source: 3D Face Reconstruction with Geometry Details from a Single Image"	https://paperswithcode.com/dataset/facewarehouse	01/01/2014						
847	LSP	"The Leeds Sports Pose (LSP) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.
Source: Deep Deformation Network for Object Landmark Localization
Image: Sumer et al"	https://paperswithcode.com/dataset/lsp	01/01/2010	Leeds Sports Pose					
848	LINEMOD	"LINEMOD is an RGB+D dataset, which has become a de facto standard benchmark for 6D pose estimation. The dataset contains poorly textured objects in a cluttered scene. The dataset contains 15 object sequences. The images in each object sequence contain multiple objects, however, only one object is annotated with the ground-truth class label, bounding box, and 6D pose. The camera intrinsic matrix is also provided with the dataset.
Source: Deep-6DPose: Recovering 6D Object Pose from a Single RGB Image"	https://paperswithcode.com/dataset/linemod-1	01/01/2012						
849	KTH	"The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the KTH Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.
Source: Review of Action Recognition and Detection Methods"	https://paperswithcode.com/dataset/kth	01/01/2004	KTH Action dataset					
850	Places	"The Places dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.
Source: Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey
Image Source: http://places.csail.mit.edu/browser.html"	https://paperswithcode.com/dataset/places	01/01/2018	Places					
851	MoCap	"Collection of various motion capture recordings (walking, dancing, sports, and others) performed by over 140 subjects. The database contains free motions which you can download and use. There is a zip file of all asf/amc's on the FAQs page.
Source: https://www.re3data.org/repository/r3d100012183"	https://paperswithcode.com/dataset/mocap		CMU Graphics Lab Motion Capture Database					
852	KIT Whole-Body Human Motion	"The KIT Whole-Body Human Motion Database is a large-scale dataset of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. Captured subject-specific motion is normalized regarding the subject’s height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in this database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, See the paper for procedures and techniques used for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject–object relations.
Source: https://motion-database.humanoids.kit.edu/
Image Source: https://motion-database.humanoids.kit.edu/"	https://paperswithcode.com/dataset/kit-whole-body-human-motion	01/01/2015						
853	Meta-Dataset	"The Meta-Dataset benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: 

ILSVRC-2012 (the ImageNet dataset, consisting of natural images with 1000 categories)
Omniglot (hand-written characters, 1623 classes)
Aircraft (dataset of aircraft images, 100 classes)
CUB-200-2011 (dataset of Birds, 200 classes)
Describable Textures (different kinds of texture images with 43 categories)
Quick Draw (black and white sketches of 345 different categories)
Fungi (a large dataset of mushrooms with 1500 categories)
VGG Flower (dataset of flower images with 102 categories), 
Traffic Signs (German traffic sign images with 43 classes)
MSCOCO (images collected from Flickr, 80 classes). 

All datasets except Traffic signs and MSCOCO have a training, validation and test split (proportioned roughly into 70%, 15%, 15%). The datasets Traffic Signs and MSCOCO are reserved for testing only.
Source: Optimized Generic Feature Learning for Few-shot Classification across Domains
Image Source: Triantafillou et al"	https://paperswithcode.com/dataset/meta-dataset	01/01/2019						
854	USF	"The USF Human ID Gait Challenge Dataset is a dataset of videos for gait recognition. It has videos from 122 subjects in up to 32 possible combinations of variations in factors.
Source: http://www.eng.usf.edu/cvprg/Gait_Data.html"	https://paperswithcode.com/dataset/usf	01/01/2005	Human ID Gait Challenge Dataset					
855	BirdSong	The BirdSong dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings.	https://paperswithcode.com/dataset/birdsong	01/01/2012						
856	Oxford5k	"Oxford5K is the Oxford Buildings Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.
Source: Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN 1 Corresponding Author
Image Source: https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/"	https://paperswithcode.com/dataset/oxford5k	01/01/2007	Oxford Buildings					
857	CBSD68	"Color BSD68 dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.
Source: https://github.com/clausmichele/CBSD68-dataset"	https://paperswithcode.com/dataset/cbsd68	01/01/2001	Color BSD68					
858	ScribbleSup	"The PASCAL-Scribble Dataset is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.
In the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.
Source: https://jifengdai.org/downloads/scribble_sup/
Image Source: https://jifengdai.org/downloads/scribble_sup/"	https://paperswithcode.com/dataset/scribblesup	01/01/2016	PASCAL-Scribble Dataset					
859	Stanford Background	"The Stanford Background dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories
Source: Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation
Image Source: http://dags.stanford.edu/projects/scenedataset.html"	https://paperswithcode.com/dataset/stanford-background	01/01/2009	Standford Background Dataset					
860	New College	"The New College Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore.
Source: https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html"	https://paperswithcode.com/dataset/new-college	01/01/2009						
861	MALF	"The MALF dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.
Source: Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results
Image Source: http://www.cbsr.ia.ac.cn/faceevaluation/"	https://paperswithcode.com/dataset/malf	01/01/2015	Multi-Attribute Labelled Faces					
862	Oxford-Affine	"The Oxford-Affine dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies.
Source: A Large Dataset for Improving Patch Matching
Image Source: https://www.robots.ox.ac.uk/~vgg/data/affine/"	https://paperswithcode.com/dataset/oxford-affine	01/01/2005						
863	Oxford105k	"Oxford105k is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.
Source: Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors Extended Version"	https://paperswithcode.com/dataset/oxford105k	01/01/2007						
864	DispScenes	"The DispScenes dataset was created to address the specific problem of disparate image matching. The image pairs in all the datasets exhibit high levels of variation in illumination and viewpoint and also contain instances of occlusion. The DispScenes dataset provides manual ground truth keypoint correspondences for all images.
Source: Matching Disparate Image Pairs Using Shape-Aware ConvNets"	https://paperswithcode.com/dataset/dispscenes	01/01/2018						
865	Retrieval-SfM	"The Retrieval-SFM dataset is used for instance image retrieval. The dataset contains 28559 images from 713 locations in the world. Each image has a label indicating the location it belongs to. Most locations are famous man-made architectures such as palaces and towers, which are relatively static and positively contribute to visual place recognition. The training dataset contains various perceptual changes including variations in viewing angles, occlusions and illumination conditions, etc.
Source: Localizing Discriminative Visual Landmarks for Place Recognition"	https://paperswithcode.com/dataset/retrieval-sfm	01/01/2016						
866	VGG Cell	"The VGG Cell dataset (made up entirely of synthetic images) is the main public benchmark used to compare cell counting techniques.
Source: People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting
Image Source: https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html"	https://paperswithcode.com/dataset/vgg-cell	01/01/2018						
867	Tiny Images	"The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. 
The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.
Source: Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments"	https://paperswithcode.com/dataset/tiny-images	01/01/2008						
868	Permuted MNIST	"Permuted MNIST is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels.
Source: Lifelong Learning with Dynamically Expandable Networks
Image Source: https://arxiv.org/pdf/1810.12488.pdf"	https://paperswithcode.com/dataset/permuted-mnist	01/01/2014						
869	MNIST-8M	"MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.
Source: Scalable and Sustainable Deep Learningvia Randomized Hashing"	https://paperswithcode.com/dataset/mnist-8m		Infinite MNIST					
870	SUN3D	"SUN3D contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day.
Source: A Review on Deep Learning TechniquesApplied to Semantic Segmentation
Image Source: http://sun3d.cs.princeton.edu/"	https://paperswithcode.com/dataset/sun3d	01/01/2013	SUN3D					
871	TUM RGB-D	"TUM RGB-D is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz).
Source: https://vision.in.tum.de/data/datasets/rgbd-dataset
Image Source: https://vision.in.tum.de/research/rgb-d_sensors_kinect"	https://paperswithcode.com/dataset/tum-rgb-d	01/01/2012	TUM RGB-D					
872	SceneNet	"SceneNet is a dataset of labelled synthetic indoor scenes. There are several labeled indoor scenes, including:

11 Bedroom scenes with 428 objects
15 Office scenes with 1,203 objects
11 Kitchen scenes with 797 objects
10 Living Room scenes with 715 objects
10 Bathrooms with 556 objects

Source: https://robotvault.bitbucket.io/
Image Source: https://robotvault.bitbucket.io/big_scene.html"	https://paperswithcode.com/dataset/scenenet	01/01/2015	SceneNet					
873	SceneNet RGB-D	"SceneNet-RGBD is a synthetic dataset containing large-scale photorealistic renderings of indoor scene trajectories with pixel-level annotations. Random sampling permits virtually unlimited scene configurations, and the dataset creators provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system.
Source: ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation
Image Source: https://robotvault.bitbucket.io/scenenet-rgbd.html"	https://paperswithcode.com/dataset/scenenet-rgb-d	01/01/2017	SceneNet RGB-D					
874	SUN Attribute	"The SUN Attribute dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition.
Source: Zero-Shot Learning with Multi-Battery Factor Analysis
Image Source: https://cs.brown.edu/~gmpatter/sunattributes.html"	https://paperswithcode.com/dataset/sun-attribute	01/01/2014	SUN Attribute					
875	iSUN	"iSUN is a ground truth of gaze traces on images from the SUN dataset. The collection is partitioned into 6,000 images for training, 926 for validation and 2,000 for test.
Source: End-to-end Convolutional Network for Saliency Prediction
Image Source: http://turkergaze.cs.princeton.edu/"	https://paperswithcode.com/dataset/isun	01/01/2015	iSUN					
876	BMS-26	"The  Berkeley Motion Segmentation Dataset (BMS-26) is a dataset for motion segmentation, which consists of 26 video sequences with pixel-accurate segmentation annotation of moving objects. A total of 189 frames is annotated. 12 of the sequences are taken from the Hopkins 155 dataset and new annotation is added.
Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html
Image Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html"	https://paperswithcode.com/dataset/bms-26	01/01/2010	Berkeley Motion Segmentation					
877	Freiburg Groceries	"Freiburg Groceries is a groceries classification dataset consisting of 5000 images of size 256x256, divided into 25 categories. It has imbalanced class sizes ranging from 97 to 370 images per class. Images were taken in various aspect ratios and padded to squares.
Source: XNAS: Neural Architecture Search with Expert Advice
Image Source: http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/"	https://paperswithcode.com/dataset/freiburg-groceries	01/01/2016	Freiburg Groceries					
878	Freiburg Spatial Relations	"The Freiburg Spatial Relations dataset features 546 scenes each containing two out of 25 household objects. The depicted spatial relations can roughly be described as on top, on top on the corner, inside, inside and inclined, next to, and inclined. The dataset contains the 25 object models as textured .obj and .dae files, a low resolution .dae version for visualization in rviz, a scene description file containing the translation and rotation of the objects for each scene, a file with labels for each scene, the 15 splits used for cross validation, and a bash script to convert the models to pointclouds.
Source: http://spatialrelations.cs.uni-freiburg.de/
Image Source: http://spatialrelations.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-spatial-relations	01/01/2017	Freiburg Spatial Relations					
879	Freiburg Street Crossing	"The Freiburg Street Crossing dataset consists of data collected from three different street crossings in Freiburg, Germany; ; two of which were traffic light regulated intersections and one a zebra crossing without traffic lights. The data can be used to train agents to cross roads autonomously.
Source: http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/
Image Source: http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/"	https://paperswithcode.com/dataset/freiburg-street-crossing	01/01/2018	Freiburg Street Crossing					
880	Freiburg Campus 3D Scan	"The Freiburg Campus 3D Scan dataset consists of 3D area maps from the Freiburg campus that were scanned with 3D lasers. Areas include corridors, the outdoor campus, and some of the colleges and buildings.
Source: http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/
Image Source: http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/"	https://paperswithcode.com/dataset/freiburg-campus-3d-scan	01/01/2013	Freiburg Campus 3D Scan					
881	Plant Centroids	"Plant Centroids is a dataset for stem emerging points (SEP) detection in RGB and NIR image data. The dataset is meant to aid the construction of agricultural robots, where detecting SEPs is an important perception task (to position weeding or fertilizing tools at the plant’s center and finding natural landmarks in the field environment). The dataset contains annotations for ~2000 image sets with a broad variance of plant species and growth stages.
Source: http://plantcentroids.cs.uni-freiburg.de/
Image Source: http://plantcentroids.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/plant-centroids	01/01/2017	Plant Centroids					
882	Freiburg Across Seasons	"Freiburg Across Seasons captures long-term perceptual changes across a span of 3 years. Image sequences were recorded with a forward facing bumblebee stereo camera mounted on a car. During summer, the camera was mounted outside the car where as during winters the camera was inside the car. The image sequences are recorded at relatively low frame rates of 1Hz and 4Hz. All the images have a resolution of 1024 × 768 (width×height) and are JPEG compressed. In total, there are ground truth matchings for 8,133 images for localization based on GPS position.
Source: http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/
Image Source: http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/"	https://paperswithcode.com/dataset/freiburg-across-seasons		Freiburg Across Seasons					
883	Freiburg Terrains	"Freiburg Terrains consist of three parts: 3.7 hours of audio recordings of the microphone pointed at the robot wheels. It also contains 24K RGB images from the camera mounted on top of the robot. The dataset creators also provide the SLAM poses for each data collection run. The dataset can be used for terrain classification which is useful for agent navigation tasks.
Source: http://deepterrain.cs.uni-freiburg.de/
Image Source: http://deepterrain.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-terrains	01/01/2019	Freiburg Terrains					
884	Freiburg Block Tasks	"Freiburg Block Tasks is a dataset for robot skill learning. It consists of two datasets.
The first data set consisted of three simulated robot tasks: stacking (A), color pushing (B) and color stacking (C). The data set contains 300 multi-view demonstration videos per task. The tasks are simulated with PyBullet. Of these 300 demonstrations, 150 represent unsuccessful executions of the different tasks. The authors found it helpful to add unsuccessful demonstrations in the training of the embedding to enable training RL agents on it. Without fake examples, the distances in the embedding space for states not seen during training might be noisy. The test set contains the manipulation of blocks. Within the validation set, the blocks are replaced by cylinders of different colors.
The second data set includes real-world human executions of the simulated robot tasks (A, B and C), as well as demonstrations for a task where one has to first separate blocks in order to stack them (D). For each task, there are 60 multi-view demonstration videos, corresponding to 24 minutes of interaction. In contrast to the simulated data set, the real demonstrations contain no unsuccessful executions and are of varying length. The test set contains blocks of unseen sizes and textures, as well as unknown backgrounds.
Source: http://robotskills.cs.uni-freiburg.de/
Image Source: http://robotskills.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-block-tasks		Freiburg Block Tasks					
885	Cityscapes-Motion	"The Cityscapes-Motion dataset is a supplement to the semantic annotations provided by the Cityscapes dataset, containing 2975 training images and 500 validation images. The dataset creators provide manually annotated motion labels for the category of cars. The images are of resolution 2048×1024 pixels. The task to learn is not just semantic segmentation but also the motion status of the objects.
Source: http://deepmotion.cs.uni-freiburg.de/
Image Source: http://deepmotion.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/cityscapes-motion	01/01/2017	Cityscapes-Motion					
886	KITTI-Motion	"The KITTI-Motion dataset contains pixel-wise semantic class labels and moving object annotations for 255 images taken from the KITTI Raw dataset. The images are of resolution 1280×384 pixels and contain scenes of freeways, residential areas and inner-cities. The task is not just to semantically segment objects but also to identify their motion status.
Source: http://deepmotion.cs.uni-freiburg.de/
Image Source: http://deepmotion.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/kitti-motion	01/01/2017	KITTI-Motion					
887	MobilityAids	"MobilityAids is a dataset for perception of people and their mobility aids. The annotated dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. In total the hospital dataset has over 17, 000 annotated RGB-D images, containing people categorized according to the mobility aids they use. The images were collected in the facilities of the Faculty of Engineering of the University of Freiburg and in a hospital in Frankfurt.
Source: http://mobility-aids.informatik.uni-freiburg.de/
Image Source: http://mobility-aids.informatik.uni-freiburg.de/"	https://paperswithcode.com/dataset/mobilityaids	01/01/2017	MobilityAids					
888	RobotPush	"RobotPush is a dataset for object singulation – the task of separating cluttered objects through physical interaction. The dataset contains 3456 training images with labels and 1024 validation images with labels. It consists of simulated and real-world data collected from a PR2 robot that equipped with a Kinect 2 camera. The dataset also contains ground truth instance segmentation masks for 110 images in the test set.
Source: http://robotpush.cs.uni-freiburg.de/
Image Source: http://robotpush.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/robotpush	01/01/2017	RobotPush					
889	DeepLocCross	"DeepLocCross is a localization dataset that contains RGB-D stereo images captured at 1280 x 720 pixels at a rate of 20 Hz. The ground-truth pose labels are generated using a LiDAR-based SLAM system. In addition to the 6-DoF localization poses of the robot, the dataset additionally contains tracked detections of the observable dynamic objects. Each tracked object is identified using a unique track ID, spatial coordinates, velocity and orientation angle. Furthermore, as the dataset contains multiple pedestrian crossings, labels at each intersection indicating its safety for crossing are provided.
This dataset consists of seven training sequences with a total of 2264 images, and three testing sequences with a total of 930 images. The dynamic nature of the surrounding environment at which the dataset was captured renders the tasks of localization and visual odometry estimation extremely challenging due to the varying weather conditions, presence of shadows and motion blur caused by the movement of the robot platform. Furthermore, the presence of multiple dynamic objects often results in partial and full occlusions to the informative regions of the image. Moreover, the presence of repeated structures render the pose estimation task more challenging. Overall this dataset covers a wide range of perception related tasks such as loop closure detection, semantic segmentation, visual odometry estimation, global localization, scene flow estimation and behavior prediction.
Source: http://deeploc.cs.uni-freiburg.de/
Image Source: http://deeploc.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/deeploccross	01/01/2018	DeepLocCross					
890	DeepLoc	"DeepLoc is a large-scale urban outdoor localization dataset. The dataset is currently comprised of one scene spanning an area of 110 x 130 m, that a robot traverses multiple times with different driving patterns. The dataset creators use a LiDAR-based SLAM system with sub-centimeter and sub-degree accuracy to compute the pose labels that provided as groundtruth. Poses in the dataset are approximately spaced by 0.5 m which is twice as dense as other relocalization datasets.
Furthermore, for each image the dataset creators provide pixel-wise semantic segmentation annotations for ten categories: Background, Sky, Road, Sidewalk, Grass, Vegetation, Building, Poles & Fences, Dynamic and Void. The dataset is divided into a train and test splits such that the train set comprises seven loops with alternating driving styles amounting to 2737 images, while the test set comprises three loops with a total of 1173 images. The dataset also contains global GPS/INS data and LiDAR measurements.
This dataset can be very challenging for vision based applications such as global localization, camera relocalization, semantic segmentation, visual odometry and loop closure detection, as it contains substantial lighting, weather changes, repeating structures, reflective and transparent glass buildings.
Source: http://deeploc.cs.uni-freiburg.de/
Image Source: http://deeploc.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/deeploc	01/01/2018	DeepLoc					
891	Freiburg Lighting Adaptable Map Tracking	"Freiburg Lighting Adaptable Map Tracking is a dataset for camera trajectory estimation. The dataset consists of two subdatasets, each consisting of a Lighting Adaptable Map and three camera trajectories recorded under varying lighting conditions. The map meshes are stored in PLY format with custom properties and elements. The trajectories contain synchronized RGB-D images, exposure times and gains, ground-truth light settings and camera poses, as well as the camera tracking results presented in the paper.
Source: http://tracklam.informatik.uni-freiburg.de/
Image Source: http://tracklam.informatik.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-lighting-adaptable-map-tracking		Freiburg Lighting Adaptable Map Tracking					
892	Freiburg Poking	"The Freiburg Poking dataset is a dataset for learning intuitive physics from physical interaction. It consists of 40K of interaction data with a KUKA LBR iiwa manipulator and a fixed Azure Kinect RGB-D camera. The dataset creators built an arena of styrofoam with walls for preventing objects from falling down. At any given time there were 3-7 objects randomly chosen from a set of 34 distinct objects present on the arena. The objects differed from each other in shape, appearance, material, mass and friction.
Source: http://hind4sight.cs.uni-freiburg.de/
Image Source: http://hind4sight.cs.uni-freiburg.de/"	https://paperswithcode.com/dataset/freiburg-poking		Freiburg Poking					
893	7-Scenes	"The 7-Scenes dataset is a collection of tracked RGB-D camera frames. The dataset may be used for evaluation of methods for different applications such as dense tracking and mapping and relocalization techniques.
All scenes were recorded from a handheld Kinect RGB-D camera at 640×480 resolution. The dataset creators use an implementation of the KinectFusion system to obtain the ‘ground truth’ camera tracks, and a dense 3D model. Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets.
Source: https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/
Image Source: https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/"	https://paperswithcode.com/dataset/7-scenes	01/01/2013	7-Scenes					
894	Cross-Dataset Testbed	"The Cross-dataset Testbed is a Decaf7 based cross-dataset image classification dataset, which contains 40 categories of images from 3 domains: 3,847 images in Caltech256, 4,000 images in ImageNet, and 2,626 images for SUN. In total there are 10,473 images of 40 categories from these three domains.
Source: Probability Weighted Compact Feature for Domain Adaptive Retrieval
Image Source: https://sites.google.com/site/crossdataset/"	https://paperswithcode.com/dataset/cross-dataset-testbed	01/01/2014	Cross-Dataset Testbed					
895	Washington RGB-D	"Washington RGB-D is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.
Source: Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work
Image Source: https://rgbd-dataset.cs.washington.edu/"	https://paperswithcode.com/dataset/washington-rgb-d	01/01/2011	Washington RGB-D					
896	TUM Kitchen	"The TUM Kitchen dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384×288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately.
Source: Temporal Human Action Segmentation via Dynamic Clustering
Image Source: https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data"	https://paperswithcode.com/dataset/tum-kitchen	01/01/2009	TUM Kitchen					
897	HIC	"The Hands in action dataset (HIC) dataset has RGB-D sequences of hands interacting with objects.
Source: Learning joint reconstruction of hands and manipulated objects
Image Source: http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/"	https://paperswithcode.com/dataset/hic	01/01/2016	Hands in Action					
898	George Washington	"The George Washington dataset contains 20 pages of letters written by George Washington and his associates in 1755 and thereby categorized into historical collection. The images are annotated at word level and contain approximately 5,000 words.
Source: HWNet v2: An Efficient Word Image Representation for Handwritten Documents.
Image Source: https://www.loc.gov/resource/mgw1a.002/?sp=2"	https://paperswithcode.com/dataset/george-washington	01/01/2012	George Washington					
899	Watch-n-Patch	"The Watch-n-Patch dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations.
Source: Head Detection with Depth Images in the Wild
Image Source: https://openaccess.thecvf.com/content_cvpr_2015/papers/Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper.pdf"	https://paperswithcode.com/dataset/watch-n-patch	01/01/2015	Watch-n-Patch					
900	Parzival	"The Parzival dataset consists of 47 pages by three writers. These pages were taken from a medieval German manuscript from the 13th century that contains the epic poem Parzival by Wolfram von Eschenbach. The image size is 2000 x 3000 pixels. 24 pages are selected as training set; 14 pages are selected as test set; 2 pages are selected as validation set.
Source: https://diuf.unifr.ch/main/hisdoc/divadia
Image Source: https://diuf.unifr.ch/main/hisdoc/divadia"	https://paperswithcode.com/dataset/parzival	01/01/2009	Parzival					
901	CDTB	"Source: https://www.vicos.si/Projects/CDTB 4.2 State-of-the-art Comparison A TH CTB (color-and-depth visual object tracking) dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.
Image Source: https://www.vicos.si/Projects/CDTB"	https://paperswithcode.com/dataset/cdtb	01/01/2019	Color-and-Depth Tracking					
902	EgoDexter	"The EgoDexter dataset provides both 2D and 3D pose annotations for 4 testing video sequences with 3190 frames. The videos are recorded with body-mounted camera from egocentric viewpoints and contain cluttered backgrounds, fast camera motion, and complex interactions with various objects. Fingertip positions were manually annotated for 1485 out of 3190 frames.
Source: Hand Pose Estimation via Latent 2.5D Heatmap Regression
Image Source: https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm"	https://paperswithcode.com/dataset/egodexter	01/01/2017	EgoDexter					
903	SynthHands	"The SynthHands dataset is a dataset for hand pose estimation which consists of real captured hand motion retargeted to a virtual hand with natural backgrounds and interactions with different objects. The dataset contains data for male and female hands, both with and without interaction with objects. While the hand and foreground object are synthtically generated using Unity, the motion was obtained from real performances as described in the accompanying paper. In addition, real object textures and background images (depth and color) were used. Ground truth 3D positions are provided for 21 keypoints of the hand.
Source: Egocentric 6-DoF Tracking of Small Handheld Objects
Image Source: https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm"	https://paperswithcode.com/dataset/synthhands	01/01/2017	SynthHands					
904	Washington RGB-D Scenes v2	"The RGB-D Scenes Dataset v2 consists of 14 scenes containing furniture (chair, coffee table, sofa, table) and a subset of the objects in the RGB-D Object Dataset (bowls, caps, cereal boxes, coffee mugs, and soda cans). Each scene is a point cloud created by aligning a set of video frames using Patch Volumes Mapping.
Source: https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/
Image Source: https://arxiv.org/abs/1904.02530"	https://paperswithcode.com/dataset/washington-rgb-d-scenes-v2	01/01/2013	Washington RGB-D Scenes v2					
905	Washington RGB-D Scenes	"The RGB-D Scenes Dataset contains 8 scenes annotated with objects that belong to the Washington RGB-D Object Dataset. Each scene is a single video sequence consisting of multiple RGB-D frames.
Source: https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/
Image Source: https://arxiv.org/abs/1904.02530"	https://paperswithcode.com/dataset/washington-rgb-d-scenes	01/01/2012	Washington RGB-D Scenes					
906	MannequinChallenge	"The MannequinChallenge Dataset (MQC) provides in-the-wild videos of people in static poses while a hand-held camera pans around the scene. The dataset consists of three splits for training, validation and testing.
Source: Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild
Image Source: https://google.github.io/mannequinchallenge/www/index.html"	https://paperswithcode.com/dataset/mannequinchallenge	01/01/2019	MannequinChallenge					
907	Freiburg RGB-D People	"The Freiburg RGB-D People dataset contains 3000+ RGB-D frames acquired in a university hall from three vertically mounted Kinect sensors. The data contains mostly upright walking and standing persons seen from different orientations and with different levels of occlusions.
Source: http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html
Image Source: http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html"	https://paperswithcode.com/dataset/freiburg-rgb-d-people	01/01/2011	Freiburg RGB-D People					
908	Fraunhofer IPA Bin-Picking	"The Fraunhofer IPA Bin-Picking dataset is a large-scale dataset comprising both simulated and real-world scenes for various objects (potentially having symmetries) and is fully annotated with 6D poses. A pyhsics simulation is used to create scenes of many parts in bulk by dropping objects in a random position and orientation above a bin. Additionally, this dataset extends the Siléane dataset by providing more samples. This allows to e.g. train deep neural networks and benchmark the performance on the public Siléane dataset
Source: https://www.bin-picking.ai/en/dataset.html
Image Source: https://arxiv.org/abs/1912.12125"	https://paperswithcode.com/dataset/fraunhofer-ipa-bin-picking	01/01/2019	Fraunhofer IPA Bin-Picking					
909	PAVIS RGB-D	"PAVIS RGB-D is a dataset for person re-identification using depth information. The main motivation is that techniques such as  SDALF fail when the individuals change their clothing, therefore they cannot be used for long-term video surveillance. Depth information is the solution to deal with this problem because it stays constant for a longer period of time. The dataset is composed by four different groups of data collected using the Kinect. The first group of data has been obtained by recording 79 people with a frontal view, walking slowly, avoiding occlusions and with stretched arms (""Collaborative""). This happened in an indoor scenario, where the people were at least 2 meters away from the camera. The second (""Walking1"") and third (""Walking2"") groups of data are composed by frontal recordings of the same 79 people walking normally while entering the lab where they normally work. The fourth group (""Backwards"") is a back view recording of the people walking away from the lab.
The dataset creators provide 5 synchronized information for each person: 1) a set of 5 RGB images, 2) the foreground masks, 3) the skeletons, 4) the 3d mesh (ply), 5) the estimated floor.
Source: https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset
Image Source: https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset"	https://paperswithcode.com/dataset/pavis-rgb-d	01/01/2012	PAVIS RGB-D					
910	Couples Therapy	"The Couples Therapy corpus contains audio, video recordings and manual transcriptions of conversations between 134 real-life couples attending marital therapy. In each session, one person selected a topic that was discussed over 10 minutes with the spouse. At the end of the session, both speakers were rated separately on 33 “behavior codes” by multiple annotators based on the Couples Interaction and Social Support Rating Systems. Each behavior was rated on a Likert scale from 1, indicating absence, to 9, indicating strong presence. A session-level rating was obtained for each speaker by averaging the annotator ratings. This process was repeated for the spouse, resulting in 2 sessions per couple at a time. The total number of sessions per couple varied between 2 and 6.
Source: Modeling Interpersonal Influence of Verbal Behaviorin Couples Therapy Dyadic Interactions"	https://paperswithcode.com/dataset/couples-therapy		Couples Therapy Corpus					
911	Raider	"The Raider dataset collects fMRI recordings of 1000 voxels from the ventral temporal cortex, for 10 healthy adult participants passively watching the full-length movie “Raiders of the Lost Ark”.
Source: Time-Resolved fMRI Shared Response Model using Gaussian Process Factor Analysis
Image Source: https://arxiv.org/abs/1909.12537"	https://paperswithcode.com/dataset/raider		Raider					
912	VizDoom	"ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players’ actions are collected or runs in a constant frame rate without waiting.
Source: Arena: a toolkit for Multi-Agent Reinforcement Learning
Image Source: https://github.com/mwydmuch/ViZDoom"	https://paperswithcode.com/dataset/vizdoom	01/01/2016	VizDoom					
913	StarCraft II Learning Environment	"The StarCraft II Learning Environment (S2LE) is a reinforcement learning environment based on the game StarCraft II. The environment consists of three sub-components: a Linux StarCraft II binary, the StarCraft II API and PySC2. The StarCraft II API allows programmatic control of StarCraft II. It can be used to start a game, get observations, take actions, and review replays. PyC2 is a Python environment that wraps the StarCraft II API to ease the interaction between Python reinforcement learning agents and StarCraft II. It defines an action and observation specification, and includes a random agent and a handful of rule-based agents as examples. It also includes some mini-games as challenges and visualization tools to understand what the agent can see and do.
Source: https://github.com/deepmind/pysc2
Image Source: https://github.com/deepmind/pysc2"	https://paperswithcode.com/dataset/starcraft-ii-learning-environment	01/01/2017	StarCraft II Learning Environment					
914	AI2-THOR	"AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with.
Source: Learning Object Relation Graph andTentative Policy for Visual Navigation
Image Source: https://ai2thor.allenai.org/"	https://paperswithcode.com/dataset/ai2-thor	01/01/2017	AI2-THOR					
915	TORCS	"TORCS (The Open Racing Car Simulator) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception.
Source: MADRaS : Multi Agent Driving Simulator
Image Source: https://sourceforge.net/projects/torcs/"	https://paperswithcode.com/dataset/torcs		The Open Racing Car Simulator					
916	DeepMind Control Suite	"The DeepMind Control Suite (DMCS) is a set of simulated continuous control environments with a standardized structure and interpretable rewards. The tasks are written and powered by the MuJoCo physics engine, making them easy to identify. Control Suite tasks include Pendulum, Acrobot, Cart-pole, Cart-k-pole, Ball in cup, Point-mass, Reacher, Finger, Hooper, Fish, Cheetah, Walker, Manipulator, Manipulator extra, Stacker, Swimmer, Humanoid, Humanoid_CMU and LQR.
Source: Unsupervised Learning of Object Structure and Dynamics from Videos
Image Source: https://arxiv.org/abs/1801.00690"	https://paperswithcode.com/dataset/deepmind-control-suite	01/01/2018	DeepMind Control Suite					
917	GVGAI	"The General Video Game AI (GVGAI) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed.
Source: Rolling Horizon Evolutionary Algorithms for General Video Game Playing
Image Source: http://www.gvgai.net/"	https://paperswithcode.com/dataset/gvgai	01/01/2016	General Video Game AI					
918	StarData	"StarData is a StarCraft: Brood War replay dataset, with 65,646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second.
Source: https://github.com/TorchCraft/StarData
Image Source: https://www.youtube.com/watch?app=desktop&v=vBjgww8jDgw"	https://paperswithcode.com/dataset/stardata	01/01/2017	StarData					
919	Atari-HEAD	"Atari-HEAD is a dataset of human actions and eye movements recorded while playing Atari videos games. For every game frame, its corresponding image frame, the human keystroke action, the reaction time to make that action, the gaze positions, and immediate reward returned by the environment were recorded. The gaze data was recorded using an EyeLink 1000 eye tracker at 1000Hz. The human subjects are amateur players who are familiar with the games. The human subjects were only allowed to play for 15 minutes and were required to rest for at least 15 minutes before the next trial. Data was collected from 4 subjects, 16 games, 175 15-minute trials, and a total of 2.97 million frames/demonstrations.
Source: https://zenodo.org/record/2587121
Image Source: https://arxiv.org/abs/1903.06754"	https://paperswithcode.com/dataset/atari-head	01/01/2019						
920	Mario AI	"Mario AI was a benchmark environment for reinforcement learning. The gameplay in Mario AI, as in the original Nintendo’s version, consists in moving the controlled character, namely Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over cliffs to get past them. Mario can be in one of three states: Small, Big (can kill enemies by jumping onto them), and Fire (can shoot fireballs).
Source: https://github.com/zerg000000/mario-ai
Image Source: http://julian.togelius.com/Karakovskiy2012The.pdf"	https://paperswithcode.com/dataset/mario-ai	01/01/2010	Mario AI					
921	D4RL	"D4RL is a collection of environments for offline reinforcement learning. These environments include Maze2D, AntMaze, Adroit, Gym, Flow, FrankKitchen and CARLA.
Source: https://sites.google.com/view/d4rl/home
Image Source: https://sites.google.com/view/d4rl/home"	https://paperswithcode.com/dataset/d4rl		D4RL					
922	AtariARI	"The AtariARI (Atari Annotated RAM Interface) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces.
Source: https://arxiv.org/pdf/1906.08226.pdf
Image Source: https://github.com/mila-iqia/atari-representation-learning"	https://paperswithcode.com/dataset/atariari	01/01/2019	Atari Annotated RAM Interface					
923	Lani	"LANI is a 3D navigation environment and corpus, where an agent navigates between landmarks. Lani contains 27,965 crowd-sourced instructions for navigation in an open environment. Each datapoint includes an instruction, a human-annotated ground-truth demonstration trajectory, and an environment with various landmarks and lakes. The dataset train/dev/test split is 19,758/4,135/4,072. Each environment specification defines placement of 6–13 landmarks within a square grass field of size 50m×50m.
Source: Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction
Image Source: https://arxiv.org/pdf/1809.00786.pdf"	https://paperswithcode.com/dataset/lani	01/01/2018						
924	CHALET	"CHALET is a 3D house simulator with support for navigation and manipulation. Unlike existing systems, CHALET supports both a wide range of object manipulation, as well as supporting complex environemnt layouts consisting of multiple rooms. The range of object manipulations includes the ability to pick up and place objects, toggle the state of objects like taps or televesions, open or close containers, and insert or remove objects from these containers. In addition, the simulator comes with 58 rooms that can be combined to create houses, including 10 default house layouts. CHALET is therefore suitable for setting up challenging environments for various AI tasks that require complex language understanding and planning, such as navigation, manipulation, instruction following, and interactive question answering.
Source: https://github.com/lil-lab/chalet
Image Source: https://arxiv.org/pdf/1809.00786.pdf"	https://paperswithcode.com/dataset/chalet	01/01/2018	Cornell House Agent Learning Environment					
925	Griddly	"Griddly is an environment for grid-world based research.  Griddly provides a highly optimized game state and rendering engine with a flexible high-level interface for configuring environments. Not only does Griddly offer simple interfaces for single, multi-player and RTS games, but also multiple methods of rendering, configurable partial observability and interfaces for procedural content generation.
Source: https://griddly.readthedocs.io/en/latest/about/introduction.html
Image Source: https://griddly.readthedocs.io/en/latest/"	https://paperswithcode.com/dataset/griddly		Griddly					
926	NomBank	"NomBank is an annotation project at New York University that is related to the PropBank project at the University of Colorado.  The goal is to mark the sets of arguments that cooccur with nouns in the PropBank Corpus (the Wall Street Journal Corpus of the Penn Treebank), just as PropBank records such information for verbs.  As a side effect of the annotation process, the authors are producing a number of other resources including various dictionaries, as well as PropBank style lexical entries called frame files. These resources help the user label the various arguments and adjuncts of the head nouns with roles (sets of argument labels for each sense of each noun).  NYU and U of Colorado are making a coordinated effort to insure that, when possible, role definitions are consistent across parts of speech. For example, PropBank's frame file for the verb ""decide"" was used in the annotation of the noun ""decision"".
Source: Nombank
Image Source: https://nlp.cs.nyu.edu/meyers/NomBank.html"	https://paperswithcode.com/dataset/nombank	01/01/2004	NomBank					
927	QA-SRL	"QA-SRL was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences.
Source: Zero-Shot Relation Extraction via Reading Comprehension
Image Source: http://browse.qasrl.org/"	https://paperswithcode.com/dataset/qa-srl	01/01/2015	QA-SRL					
928	SParC	"SParC is a large-scale dataset for complex, cross-domain, and context-dependent (multi-turn) semantic parsing and text-to-SQL task (interactive natural language interfaces for relational databases).
Source: https://github.com/taoyds/sparc
Image Source: https://arxiv.org/pdf/1906.02285.pdf"	https://paperswithcode.com/dataset/sparc	01/01/2019	Semantic Parsing in Context					
929	CoNLL 2002	"The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.
Source: CoNLL 2002
Image Source: https://www.aclweb.org/anthology/W02-2024.pdf"	https://paperswithcode.com/dataset/conll-2002	01/01/2002						
930	Panlex	"PanLex translates words in thousands of languages. Its database is panlingual (emphasizes coverage of every language) and lexical (focuses on words, not sentences).
Source: PANLEX
Image Source: http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf"	https://paperswithcode.com/dataset/panlex	01/01/2014						
931	MCScript	"MCScript is used as the official dataset of SemEval2018 Task11. This dataset constructs a collection of text passages about daily life activities and a series of questions referring to each passage, and each question is equipped with two answer choices. The MCScript comprises 9731, 1411, and 2797 questions in training, development, and test set respectively.
Source: Multi-Perspective Fusion Network for Commonsense Reading Comprehension
Image Source: https://arxiv.org/pdf/1803.05223.pdf"	https://paperswithcode.com/dataset/mcscript	01/01/2018	MCScript					
932	KP20k	"KP20k is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing.
Source: Keyphrase Prediction With Pre-trained Language Model
Image Source: https://arxiv.org/pdf/1704.06879.pdf"	https://paperswithcode.com/dataset/kp20k	01/01/2017	KP20k					
933	Semantic Scholar	"The Semantic Scholar corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps).
Source: Learning Dynamic Author Representations with Temporal Language Models
Image Source: http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/"	https://paperswithcode.com/dataset/semantic-scholar	01/01/2018	Semantic Scholar					
934	EVALution	"EVALution dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives.
Source: Network Features Based Co-hyponymy Detection
Image Source: https://www.aclweb.org/anthology/W15-4208.pdf"	https://paperswithcode.com/dataset/evalution	01/01/2015	EVALution					
935	Senseval-2	"There are now many computer programs for automatically determining the sense of a word in context (Word Sense Disambiguation or WSD).  The purpose of SENSEVAL is to evaluate the strengths and weaknesses of such programs with respect to different words, different varieties of language, and different languages.
Source: SENSEVAL
Image Source: http://www.itri.brighton.ac.uk/events/senseval/"	https://paperswithcode.com/dataset/senseval-2-1	01/01/2001	Senseval-2					
936	RoboCup	"RoboCup is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.
Source: Robots as Actors in a Film: No War, A Robot Story
Image Source: https://www.robocup.org/"	https://paperswithcode.com/dataset/robocup	01/01/2008	RoboCup					
937	ShARC	"ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules.
Source: ShARC
Image Source: https://arxiv.org/abs/1809.01494"	https://paperswithcode.com/dataset/sharc	01/01/2018	Shaping Answers with Rules through Conversation					
938	Social IQA	"Social Interaction QA, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. For example, given an action like ""Jesse saw a concert"" and a question like ""Why did Jesse do this?"", humans can easily infer that Jesse wanted ""to see their favorite performer"" or ""to enjoy the music"", and not ""to see what's happening inside"" or ""to see if it works"". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations.
Source: Social IQA
Image Source: https://arxiv.org/pdf/1904.09728.pdf"	https://paperswithcode.com/dataset/social-iqa	01/01/2019	Social Interaction QA					
939	OLID	"The OLID is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019.
Source: Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection
Image Source: https://arxiv.org/pdf/1902.09666.pdf"	https://paperswithcode.com/dataset/olid	01/01/2019	Offensive Language Identification Dataset					
940	Multi-News	"Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.
Source: Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model
Image Source: https://arxiv.org/pdf/1906.01749.pdf"	https://paperswithcode.com/dataset/multi-news	01/01/2019	Multi-News					
941	CLOTH	"The Cloze Test by Teachers (CLOTH) benchmark is a collection of nearly 100,000 4-way multiple-choice cloze-style questions from middle- and high school-level English language exams, where the answer fills a blank in a given text. Each question is labeled with a type of deep reasoning it involves, where the four possible types are grammar, short-term reasoning, matching/paraphrasing, and long-term reasoning, i.e., reasoning over multiple sentences
Source: Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches
Image Source: https://arxiv.org/pdf/1711.03225.pdf"	https://paperswithcode.com/dataset/cloth	01/01/2018	CLOze test by TeacHers					
942	CosmosQA	"CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.
Source: Teaching Pretrained Models with Commonsense Reasoning: A Preliminary KB-Based Approach
Image Source: https://arxiv.org/pdf/1909.00277.pdf"	https://paperswithcode.com/dataset/cosmosqa	01/01/2019						
943	WinoBias	"WinoBias contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.
Source: WinoBias
Image Source: https://uclanlp.github.io/corefBias/overview"	https://paperswithcode.com/dataset/winobias	01/01/2018	WinoBias					
944	Spades	"Datasets Spades contains 93,319 questions derived from clueweb09 sentences. Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs.
Source: Learning an Executable Neural Semantic Parser
Image Source: https://github.com/sivareddyg/graph-parser/blob/master/data/spades/results/graphparser-ccg-supervised-dev.txt"	https://paperswithcode.com/dataset/spades	01/01/2016	Semantic PArsing of DEclarative Sentences					
945	WikiSum	"WikiSum is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.
Source: Generating Wikipedia by Summarizing Long Sequences
Image Source: https://arxiv.org/pdf/1801.10198.pdf"	https://paperswithcode.com/dataset/wikisum	01/01/2018	WikiSum					
946	DRCD	"Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.
Source: https://github.com/DRCKnowledgeTeam/DRCD
Image Source: https://arxiv.org/pdf/1806.00920.pdf"	https://paperswithcode.com/dataset/drcd	01/01/2018	Delta Reading Comprehension Dataset					
947	EmotionLines	"EmotionLines contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.
Source: Bridging Dialogue Generation and Facial Expression Synthesis
Image Source: https://arxiv.org/pdf/1802.08379.pdf"	https://paperswithcode.com/dataset/emotionlines	01/01/2018	EmotionLines					
948	Chinese Gigaword	"Chinese Gigaword corpus consists of 2.2M of headline-document pairs of news stories covering over 284 months from two Chinese newspapers, namely the Xinhua News Agency of China (XIN) and the Central News Agency of Taiwan (CNA).
Source: Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification
Image Source: https://catalog.ldc.upenn.edu/desc/addenda/LDC2011T13.jpg"	https://paperswithcode.com/dataset/chinese-gigaword		Chinese Gigaword					
949	CELEX	"CELEX database comprises three different searchable lexical databases, Dutch, English and German. The lexical data contained in each database is divided into five categories: orthography, phonology, morphology, syntax (word class) and word frequency.
Source: Polysemy and Brevity versus Frequency in Language
Image Source: https://www.aclweb.org/anthology/W17-7619.pdf"	https://paperswithcode.com/dataset/celex		CELEX					
950	MuST-C	"MuST-C currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.
Source: One-to-Many Multilingual End-to-End Speech Translation
Image Source: https://ict.fbk.eu/must-c/"	https://paperswithcode.com/dataset/must-c	01/01/2019						
951	Who-did-What	"Who-did-What collects its corpus from news and provides options for questions similar to CBT. Each question is formed from two independent articles: an article is treated as context to be read and a separate article on the same event is used to form the query.
Source: ChID: A Large-scale Chinese IDiom Dataset for Cloze Test
Image Source: https://tticnlp.github.io/who_did_what/sample.html"	https://paperswithcode.com/dataset/who-did-what	01/01/2016	Who did What					
952	MetaQA	"The MetaQA dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.
Source: https://arxiv.org/abs/1907.08176
Image Source: https://github.com/yuyuz/MetaQA"	https://paperswithcode.com/dataset/metaqa	01/01/2018	MoviE Text Audio QA					
953	FakeNewsNet	"FakeNewsNet is collected from two fact-checking websites: GossipCop and PolitiFact containing news contents with labels annotated by professional journalists and experts, along with social context information.
Source: Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News
Image Source: https://arxiv.org/pdf/1809.01286.pdf"	https://paperswithcode.com/dataset/fakenewsnet	05/09/2018	FakeNewsNet					
954	STS 2014	"STS-2014 is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes.
Source: Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering
Image Source: https://www.aclweb.org/anthology/S14-2010.pdf"	https://paperswithcode.com/dataset/sts-2014	01/01/2014	STS 2014					
955	MEDIA	"The MEDIA French corpus is dedicated to semantic extraction from speech in a context of human/machine dialogues. The corpus has manual transcription and conceptual annotation of dialogues from 250 speakers. It is split into the following three parts : (1) the training set (720 dialogues, 12K sentences), (2) the development set (79 dialogues, 1.3K sentences, and (3) the test set (200 dialogues, 3K sentences).
Source: Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems
Image Source: http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf"	https://paperswithcode.com/dataset/media	01/01/2004	MEDIA					
956	ASPEC	"ASPEC, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.
Source: ASPEC
Image Source: https://www.aclweb.org/anthology/L16-1350.pdf"	https://paperswithcode.com/dataset/aspec	01/01/2016	Asian Scientific Paper Excerpt Corpus					
957	OMICS	"OMICS is an extensive collection of knowledge for indoor service robots gathered from internet users. Currently, it contains 48 tables capturing different sorts of knowledge. Each tuple of the Help table maps a user desire to a task that may meet the desire (e.g., ⟨ “feel thirsty”, “by offering drink” ⟩). Each tuple of the Tasks/Steps table decomposes a task into several steps (e.g., ⟨ “serve a drink”, 0. “get a glass”, 1. “get a bottle”, 2. “fill class from bottle”, 3. “give class to person” ⟩). Given this, OMICS offers useful knowledge about hierarchism of naturalistic instructions, where a high-level user request (e.g., “serve a drink”) can be reduced to lower-level tasks (e.g., “get a glass”, ⋯). Another feature of OMICS is that elements of any tuple in an OMICS table are semantically related according to a predefined template. This facilitates the semantic interpretation of the OMICS tuples.
Source: Understanding User Instructions by Utilizing Open Knowledge for Service Robots
Image Source: https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf"	https://paperswithcode.com/dataset/omics	01/01/2004	Open Mind Indoor Common Sense					
958	QUASAR	"The Question Answering by Search And Reading (QUASAR) is a large-scale dataset consisting of QUASAR-S and QUASAR-T. Each of these datasets is built to focus on evaluating systems devised to understand a natural language query, a large corpus of texts and to extract an answer to the question from the corpus. Specifically, QUASAR-S comprises 37,012 fill-in-the-gaps questions that are collected from the popular website Stack Overflow using entity tags. The QUASAR-T dataset contains 43,012 open-domain questions collected from various internet sources. The candidate documents for each question in this dataset are retrieved from an Apache Lucene based search engine built on top of the ClueWeb09 dataset.
Source: MRNN: A Multi-Resolution Neural Network with Duplex Attention for Document Retrieval in the Context of Question Answering
Image Source: https://arxiv.org/pdf/1707.03904.pdf"	https://paperswithcode.com/dataset/quasar-1	01/01/2017	QUestion Answering by Search And Reading					
959	Dialogue State Tracking Challenge	"The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.
In these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.
The corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:
DSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.
DSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.
Dialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)
Source: https://github.com/matthen/dstc
Image Source: https://www.aclweb.org/anthology/W13-4065.pdf"	https://paperswithcode.com/dataset/dialogue-state-tracking-challenge	01/01/2013	Dialogue State Tracking Challenge					
960	ISEAR	"Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the ISEAR project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.
Source: https://www.unige.ch/cisa/research/materials-and-online-research/research-material/
Image Source: https://www.unige.ch/cisa/research/materials-and-online-research/research-material/"	https://paperswithcode.com/dataset/isear		International Survey on Emotion Antecedents and Reactions					
961	CMRC	"CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.
Source: A Span-Extraction Dataset for Chinese Machine Reading Comprehension
Image Source: https://www.aclweb.org/anthology/D19-1600.pdf"	https://paperswithcode.com/dataset/cmrc	01/01/2019	Chinese Machine Reading Comprehension 2018					
962	PubMed RCT	"PubMed 200k RCT is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.
Source: GitHub
Image Source: https://arxiv.org/pdf/1710.06071.pdf"	https://paperswithcode.com/dataset/pubmed-rct	01/01/2017	PubMed 200k RCT					
963	NSIDES	"Drug side effects and drug-drug interactions were mined from publicly available data. Offsides is a database of drug side-effects that were found, but are not listed on the official FDA label. Twosides is the only comprehensive database drug-drug-effect relationships. Over 3,300 drugs and 63,000 combinations connected to millions of potential adverse reactions.
Source: http://tatonettilab.org/offsides/
Image Source: http://doi.org/10.1126/scitranslmed.3003377"	https://paperswithcode.com/dataset/nsides		Offsides and Twosides (NSIDES v0.1)					
964	DDI	"The DDIExtraction 2013 task relies on the DDI corpus which contains MedLine abstracts on drug-drug interactions as well as documents describing drug-drug interactions from the DrugBank database.
Source: DDIExtraction 2013
Image Source: https://www.aclweb.org/anthology/S13-2056.pdf"	https://paperswithcode.com/dataset/ddi	01/01/2013						
965	Stylized ImageNet	"The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures.
Source: Adversarial Examples Improve Image Recognition
Image Source: https://github.com/rgeirhos/Stylized-ImageNet"	https://paperswithcode.com/dataset/stylized-imagenet	01/01/2019	Stylized ImageNet					
966	MuTual	"MuTual is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.
Source: https://github.com/Nealcly/MuTual
Image Source: https://github.com/Nealcly/MuTual"	https://paperswithcode.com/dataset/mutual		MuTual					
967	CRIM13	"The Caltech Resident-Intruder Mouse dataset (CRIM13) consists of 237x2 videos (recorded with synchronized top and side view) of pairs of mice engaging in social behavior, catalogued into thirteen different actions. Each video lasts ~10min, for a total of 88 hours of video and 8 million frames. A team of behavior experts annotated each video on a frame-by-frame basis for a state-of-the-art study of the neurophysiological mechanisms involved in aggression and courtship in mice.
Source: https://pdollar.github.io/research.html
Image Source: https://authors.library.caltech.edu/104600/1/2020.07.26.222299v1.full.pdf"	https://paperswithcode.com/dataset/crim13		Caltech Resident-Intruder Mouse 13					
968	Imagewoof	"Imagewoof is a subset of 10 dog breed classes from Imagenet. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog.
Source: https://github.com/fastai/imagenette
Image Source: https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c"	https://paperswithcode.com/dataset/imagewoof	01/01/2020	Imagewoof					
969	Imagenette	"Imagenette is a subset of 10 easily classified classes from Imagenet (bench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).
Source: https://github.com/fastai/imagenette
Image Source: https://docs.fast.ai/tutorial.imagenette.html"	https://paperswithcode.com/dataset/imagenette	01/01/2020	Imagenette					
970	Stanford-ECM	"Stanford-ECM is an egocentric multimodal dataset which comprises about 27 hours of egocentric video augmented with heart rate and acceleration data. The lengths of the individual videos cover a diverse range from 3 minutes to about 51 minutes in length. A mobile phone was used to collect egocentric video at 720x1280 resolution and 30 fps, as well as triaxial acceleration at 30Hz. The mobile phone was equipped with a wide-angle lens, so that the horizontal field of view was enlarged from 45 degrees to about 64 degrees. A wrist-worn heart rate sensor was used to capture the heart rate every 5 seconds. The phone and heart rate monitor was time-synchronized through Bluetooth, and all data was stored in the phone’s storage. Piecewise cubic polynomial interpolation was used to fill in any gaps in heart rate data. Finally, data was aligned to the millisecond level at 30 Hz.
Source: http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html
Image Source: http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html"	https://paperswithcode.com/dataset/stanford-ecm	01/01/2017	Stanford-ECM					
971	BSD	"BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.
Source: A Deep Journey into Super-resolution: A Survey
Image Source: https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015"	https://paperswithcode.com/dataset/bsd	01/01/2001	Berkeley Segmentation Dataset					
972	THUMOS14	"The THUMOS14 dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.
Source: Learning to Localize Actions from Moments
Image Source: http://crcv.ucf.edu/THUMOS14/"	https://paperswithcode.com/dataset/thumos14-1		THUMOS 2014					
973	MSRA Hand	"MSRA Hands is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively.
The camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each .bin file stores the depth pixel values in row scanning order, which are 320240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag.
joint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip.
The corresponding *.jpg file is just for visualization of depth and ground truth joints.
Source: https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt
Image Source: https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf"	https://paperswithcode.com/dataset/msra-hand	01/01/2014	MSRA Hand					
974	MSRA10K	"MSRA10K is a dataset for salient object detection that contains 10,000 images with pixel-level saliency labeling for 10K images from the MSRA salient object detection dataset. The original MRSA database provides salient object annotation in terms of bounding boxes provided by 3-9 users.
Source: https://mmcheng.net/msra10k/
Image Source: https://mmcheng.net/msra10k/"	https://paperswithcode.com/dataset/msra10k	01/01/2017	MSRA10K Salient Object Database					
975	bAbI	"The bAbI dataset is a textual QA benchmark composed of 20 different tasks. Each task is designed to test a different reasoning skill, such as deduction, induction, and coreference resolution. Some of the tasks need relational reasoning, for instance, to compare the size of different entities. Each sample is composed of a question, an answer, and a set of facts. There are two versions of the dataset, referring to different dataset sizes: bAbI-1k and bAbI-10k. The bAbI-10k version of the dataset consists of 10,000 training samples per task.
Source: Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module
Image Source: https://research.fb.com/downloads/babi/"	https://paperswithcode.com/dataset/babi-1	01/01/2016	bAbI					
976	JHMDB	"JHMDB is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).
Source: Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss
Image Source: https://arxiv.org/pdf/1712.06316.pdf"	https://paperswithcode.com/dataset/jhmdb	01/01/2013	Joint-annotated Human Motion Data Base					
977	UCF-CC-50	"UCF-CC-50 is a dataset for crowd counting and consists of images of extremely dense crowds. It has 50 images with 63,974 head center annotations in total. The head counts range between 94 and 4,543 per image. The small dataset size and large variance make this a very challenging counting dataset.
Source: Active Crowd Counting with Limited Supervision"	https://paperswithcode.com/dataset/ucf-cc-50-1	01/01/2013	UCF-CC-50					
978	SURREAL	"SURREAL (Synthetic hUmans foR REAL tasks) is a large-scale person dataset that generates photorealistic synthetic images with labeling for human part segmentation and depth estimation, producing 6.5M frames in 67.5K short clips (about 100 frames each) of 2.6K action sequences with 145 different synthetic subjects. To ensure realism, the synthetic bodies are created using the SMPL body model, whose parameters are fit by the MoSh method given raw 3D MoCap marker data.
Source: Synthetic Data for Deep Learning
Image Source: https://www.di.ens.fr/willow/research/surreal/data/"	https://paperswithcode.com/dataset/surreal-1	01/01/2017	Synthetic Humans for REAL Tasks					
979	AwA2	"Animals with Attributes 2 (AwA2) is a dataset for benchmarking transfer-learning algorithms, such as attribute base classification and zero-shot learning. AwA2 is a drop-in replacement of original Animals with Attributes (AwA) dataset, with more images released for each category. Specifically, AwA2 consists of in total 37322 images distributed in 50 animal categories. The AwA2 also provides a category-attribute matrix, which contains an 85-dim attribute vector (e.g., color, stripe, furry, size, and habitat) for each category.
Source: Learning from Noisy Web Data with Category-level Supervision
Image Source: https://arxiv.org/pdf/1604.00326.pdf"	https://paperswithcode.com/dataset/awa2-1	01/01/2019	Animals with Attributes 2					
980	AwA	"Animals with Attributes (AwA) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.
The Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.
Source: Transductive Multi-view Zero-Shot Learning
Image Source: https://cvml.ist.ac.at/AwA/"	https://paperswithcode.com/dataset/awa-1	01/01/2009	Animals with Attributes					
981	ARC	"The AI2’s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.
Source: Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering
Image Source: https://arxiv.org/abs/1803.05457"	https://paperswithcode.com/dataset/arc	01/01/2018	AI2 Reasoning Challenge					
982	PASCAL VOC 2011	"PASCAL VOC 2011 is an image segmentation dataset. It contains around 2,223 images for training, consisting of 5,034 objects. Testing consists of 1,111 images with 2,028 objects. In total there are over 5,000 precisely segmented objects for training.
Source: Scene Parsing with Integration of Parametric and Non-parametric Models
Image Source: http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html"	https://paperswithcode.com/dataset/pascal-voc-2011		PASCAL VOC 2011					
983	2D-3D-S	"The 2D-3D-S dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.
Source: https://github.com/alexsax/2D-3D-Semantics
Image Source: https://github.com/alexsax/2D-3D-Semantics"	https://paperswithcode.com/dataset/2d-3d-s	01/01/2017	2D-3D-Semantic					
984	Color FERET	"The color FERET database is a dataset for face recognition. It contains 11,338 color images of size 512×768 pixels captured in a semi-controlled environment with 13 different poses from 994 subjects.
Source: A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition
Image Source: https://www.researchgate.net/figure/Sample-results-taken-from-Color-FERET-data-set-testing-using-LBP-algorithm_fig4_308836179"	https://paperswithcode.com/dataset/color-feret	01/01/1997	Color FERET					
985	ICDAR 2017	"ICDAR2017 is a dataset for scene text detection.
Source: Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images
Image Source: https://rrc.cvc.uab.es/?ch=7"	https://paperswithcode.com/dataset/icdar-2017	01/01/2017	ICDAR 2017					
986	BUCC	"The BUCC mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure.
Source: Language-agnostic BERT Sentence Embedding
Image Source: https://comparable.limsi.fr/bucc2017/"	https://paperswithcode.com/dataset/bucc	01/01/2017	Building and Using Comparable Corpora					
987	Make3D	"The Make3D dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution.
Source: Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation
Image Source: http://make3d.cs.cornell.edu/data.html#make3d"	https://paperswithcode.com/dataset/make3d	01/01/2009						
988	Virtual KITTI	"Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.
Virtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels (cf. below for download links).
Source: https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/
Image Source: https://arxiv.org/pdf/1605.06457.pdf"	https://paperswithcode.com/dataset/virtual-kitti	01/01/2016						
989	NCLT	"The NCLT dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.
Source: http://robots.engin.umich.edu/nclt/nclt.pdf
Image Source: http://robots.engin.umich.edu/nclt/"	https://paperswithcode.com/dataset/nclt	01/01/2016	North Campus Long-Term Vision and LiDAR					
990	KITTI-Depth	"The KITTI-Depth dataset includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras. The depth images are highly sparse with only 5% of the pixels available and the rest is missing. The dataset has 86k training images, 7k validation images, and 1k test set images on the benchmark server with no access to the ground truth.
Source: Confidence Propagation through CNNs for Guided Sparse Depth Regression
Image Source: http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction"	https://paperswithcode.com/dataset/kitti-depth	01/01/2017						
991	SoF	"The Specs on Faces (SoF) dataset, a collection of 42,592 (2,662×16) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more.
Source: https://sites.google.com/view/sof-dataset
Image Source: https://sites.google.com/view/sof-dataset"	https://paperswithcode.com/dataset/sof	01/01/2017	Specs on Faces					
992	KITTI Road	"KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes:
* uu - urban unmarked (98/100)
* um - urban marked (95/96)
* umm - urban multiple marked lanes (96/94)
* urban - combination of the three above
Ground truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category ""um""). Ground truth is provided for training images only.
Source: http://www.cvlibs.net/datasets/kitti/eval_road.php
Image Source: http://www.cvlibs.net/datasets/kitti/eval_road.php"	https://paperswithcode.com/dataset/kitti-road	01/01/2013						
993	KAIST Urban	"This data set provides Light Detection and Ranging (LiDAR) data and stereo image with various position sensors targeting a highly complex urban environment. The presented data set captures features in urban environments (e.g. metropolis areas, complex buildings and residential areas). The data of 2D and 3D LiDAR are provided, which are typical types of LiDAR sensors. Raw sensor data for vehicle navigation is presented in a file format. For convenience, development tools are provided in the Robot Operating System (ROS) environment.
Source: https://sites.google.com/view/complex-urban-dataset
Image Source: https://irap.kaist.ac.kr/dataset/"	https://paperswithcode.com/dataset/kaist-urban	01/01/2019						
994	Manga109	"Manga109 has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library “Manga Library Z” (formerly the “Zeppan Manga Toshokan” library of out-of-print manga).
Source: Manga109
Image Source: https://arxiv.org/pdf/1510.04389v1.pdf"	https://paperswithcode.com/dataset/manga109	01/01/2015	Manga109					
995	GQA	"The GQA dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.
Source: Language-Conditioned Graph Networks for Relational Reasoning
Image Source: https://arxiv.org/pdf/1902.09506.pdf"	https://paperswithcode.com/dataset/gqa	01/01/2019	GQA					
996	MUSE	"The MUSE dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs.
Source: Filtered Inner Product Projection for Multilingual Embedding Alignment
Image Source: https://github.com/facebookresearch/MUSE"	https://paperswithcode.com/dataset/muse	01/01/2018	MUSE					
997	Replay-Mobile	"The Replay-Mobile Database for face spoofing consists of 1190 video clips of photo and video attack attempts to 40 clients, under different lighting conditions. These videos were recorded with current devices from the market -- an iPad Mini2 (running iOS) and a LG-G4 smartphone (running Android). This Database was produced at the Idiap Research Institute (Switzerland) within the framework of collaboration with Galician Research and Development Center in Advanced Telecommunications - Gradiant (Spain).
Source: Replay-Mobile
Image Source: https://core.ac.uk/download/pdf/148024307.pdf"	https://paperswithcode.com/dataset/replay-mobile-1	01/01/2016	Replay-Mobile					
998	Netflix Prize	"Netflix Prize consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.
Source: Indian Regional Movie Dataset for Recommender Systems
Image Source: https://www.netflixprize.com/"	https://paperswithcode.com/dataset/netflix-prize		Netflix Prize					
999	Recipe1M+	"Recipe1M+ is a dataset which contains one million structured cooking recipes with 13M associated images.
Source: Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images
Image Source: http://im2recipe.csail.mit.edu/"	https://paperswithcode.com/dataset/recipe1m-1	14/10/2018	Recipe1M+					
1000	DARPA	"Darpa is a dataset consisting of communications between source IPs and destination IPs. This dataset contains different attacks between IPs.
Source: dynnode2vec: Scalable Dynamic Network Embedding
Image Source: https://archive.ll.mit.edu/ideval/files/1999_DARPA_EvaulationSumPlans.pdf"	https://paperswithcode.com/dataset/darpa-1	01/01/1999	DARPA					
1001	HOList	"The official HOList benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof.
Source: HoList
Image Source: https://sites.google.com/view/holist/home"	https://paperswithcode.com/dataset/holist		HOList					
1002	ICDAR 2003	"The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images.
Source: Robust Scene Text Recognition Using Sparse Coding based Features
Image Source: https://www.researchgate.net/figure/The-results-of-text-localization-and-extraction-on-ICDAR-2003-dataset_fig3_290070044"	https://paperswithcode.com/dataset/icdar-2003	01/01/2003	ICDAR 2003					
1003	CASIA-FASD	"CASIA-FASD is a small face anti-spoofing dataset  containing 50 subjects.
Source: Learning Generalizable and Identity-Discriminative Representations for Face Anti-Spoofing
Image Source: https://arxiv.org/abs/1511.06316"	https://paperswithcode.com/dataset/casia-fasd	01/01/2012	CASIA-FASD					
1004	CASIA-HWDB	"CASIA-HWDB is a dataset for handwritten Chinese character recognition. It contains 300 files (240 in HWDB1.1 training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images written by one writer, as well as their corresponding labels.
Source: Generating Handwritten Chinese Characters using CycleGAN
Image Source: http://www.nlpr.ia.ac.cn/databases/handwriting/Touching_Characters_Databases.html"	https://paperswithcode.com/dataset/casia-hwdb	01/01/2011	CASIA-HWDB					
1005	TAC 2010	"TAC 2010 is a dataset for summarization that consists of 44 topics, each of which is associated with a set of 10 documents. The test dataset is composed of approximately 44 topics, divided into five categories: Accidents and Natural Disasters, Attacks, Health and Safety, Endangered Resources, Investigations and Trials.
Source: Better Summarization Evaluation with Word Embeddings for ROUGE
Image Source: https://tac.nist.gov//2010/Summarization/Guided-Summ.2010.guidelines.html"	https://paperswithcode.com/dataset/tac-2010		TAC 2010					
1006	TUM-GAID	"TUM-GAID (TUM Gait from Audio, Image and Depth) collects 305 subjects performing two walking trajectories in an indoor environment. The first trajectory is traversed from left to right and the second one from right to left. Two recording sessions were performed, one in January, where subjects wore heavy jackets and mostly winter boots, and another one in April, where subjects wore lighter clothes. The action is captured by a Microsoft Kinect sensor which provides a video stream with a resolution of 640×480 pixels and a frame rate around 30 FPS.
Source: Energy-based Tuning of Convolutional Neural Networks on Multi-GPUs
Image Source: https://www.ei.tum.de/mmk/verschiedenes/tum-gaid-database/"	https://paperswithcode.com/dataset/tum-gaid	01/01/2014	TUM-GAID					
1007	DUC 2005	"The DUC 2005 data set is a dataset for summarization which consists of 50 document collections of 25 documents each; each document collection includes a human-written query. Each document collection additionally has five human-written “reference” summaries (250 words long, each) that serve as the gold standard
Source: Search-based Structured Prediction
Image Source: https://duc.nist.gov/duc2005/tasks.html"	https://paperswithcode.com/dataset/duc-2005		DUC 2005					
1008	NIST SD 19	"NIST Special Database 19 contains NIST's entire corpus of training materials for handprinted document and character recognition. It publishes Handprinted Sample Forms from 3600 writers, 810,000 character images isolated from their forms, ground truth classifications for those images, reference forms for further data collection, and software utilities for image management and handling.
Source: https://www.nist.gov/srd/nist-special-database-19
Image Source: https://www.nist.gov/srd/nist-special-database-19"	https://paperswithcode.com/dataset/nist-sd-19		NIST Special Dataset 19					
1009	PRImA	"The Prima head pose dataset consists of 2790 images of 15 persons recorded twice. Pitch values lie in the interval [−60∘,60∘], and yaw values lie in the interval [−90∘,90∘] with a 15∘ step. Thus, there are 93 poses available for each person. All the recordings were achieved with the same background. One interesting feature of this dataset is the pose space is uniformly sampled. The dataset is annotated such that a face bounding box (manually annotated) and the corresponding yaw and pitch angle values are provided for each sample.
Source: Robust Head-Pose Estimation Based on Partially-Latent Mixture of Linear Regressions
Image Source: http://www-prima.inrialpes.fr/perso/Gourier/Faces/HPDatabase.html"	https://paperswithcode.com/dataset/prima		PRImA					
1010	TAU Urban Acoustic Scenes 2019	"TAU Urban Acoustic Scenes 2019 development dataset consists of 10-seconds audio segments from 10 acoustic scenes: airport, indoor shopping mall, metro station, pedestrian street, public square, street with medium level of traffic, travelling by a tram, travelling by a bus, travelling by an underground metro and urban park. Each acoustic scene has 1440 segments (240 minutes of audio). The dataset contains in total 40 hours of audio.
Source: https://zenodo.org/record/2589280
Image Source: http://dcase.community/challenge2019/task-acoustic-scene-classification#citation"	https://paperswithcode.com/dataset/tau-urban-acoustic-scenes-2019	01/01/2018	TAU Urban Acoustic Scenes 2019					
1011	TAU Spatial Sound Events 2019 - Ambisonic	"The TAU Spatial Sound Events 2019 - Ambisonic dataset contains recordings from a scene (along with the Microphone Array sister dataset). It provides four-channel First-Order Ambisonic (FOA) recordings. The recordings consist of stationary point sources from multiple sound classes each associated with a temporal onset and offset time, and DOA coordinate represented using azimuth and elevation angle.
The development set consists of 400, one minute long recordings sampled at 48000 Hz, and divided into four cross-validation splits of 100 recordings each. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings, the collected IRs were convolved with isolated sound events dataset from DCASE 2016 task 2. Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB.
Source: https://zenodo.org/record/2580091
Image Source: http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset"	https://paperswithcode.com/dataset/tau-spatial-sound-events-2019-ambisonic		TAU Spatial Sound Events 2019 - Ambisonic					
1012	TAU Spatial Sound Events 2019 – Microphone Array	"The TAU Spatial Sound Events 2019 – Microphone Array dataset contains recordings from a scene (along with the Ambisonic sister dataset). It provides four-channel directional microphone recordings from a tetrahedral array configuration. The recordings consist of stationary point sources from multiple sound classes each associated with a temporal onset and offset time, and DOA coordinate represented using azimuth and elevation angle.
The development set consists of 400, one minute long recordings sampled at 48000 Hz, and divided into four cross-validation splits of 100 recordings each. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor locations, at 504 unique combinations of azimuth-elevation-distance. Furthermore, in order to synthesize the recordings, the collected IRs were convolved with isolated sound events dataset from DCASE 2016 task 2. Finally, to create a realistic sound scene recording, natural ambient noise collected in the IR recording locations was added to the synthesized recordings such that the average SNR of the sound events was 30 dB.
Source: https://zenodo.org/record/2580091
Image Source: http://dcase.community/challenge2019/task-sound-event-localization-and-detection#audio-dataset"	https://paperswithcode.com/dataset/tau-spatial-sound-events-2019-microphone		TAU Spatial Sound Events 2019 – Microphone Array					
1013	CASIA V2	"CASIA V2 is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged.
Source: Copy-Move Forgery Classification via Unsupervised Domain Adaptation
Image Source: https://www.mdpi.com/1099-4300/21/4/371"	https://paperswithcode.com/dataset/casia-v2	01/01/2013	CASIA V2					
1014	KTH Multiview Football II	"KTI Multiview Football II consists of images of professional footballers during a match of the Allsvenskan league. It consists of two parts: one with ground truth pose in 2D and one with ground truth pose in both 2D and 3D. The 3D dataset has 800 time frames, captured from 3 views (2400 images). Views are calibrated and synchronized. 3D ground truth pose and orthographic camera matrices are provided for each frame. There are 14 annotated joints. Lastly, there are 2 different players and two sequences per player.
Source: http://www.csc.kth.se/~vahidk/football_data.html
Image Source: http://www.csc.kth.se/cvap/cvg/?page=footballdataset2"	https://paperswithcode.com/dataset/kth-multiview-football-ii	01/01/2013	KTH Multiview Football II					
1015	KTH Multiview Football I	"KTI Multiview Football I is a dataset of football players with annotated joints that can be used for multi-view reconstruction. The dataset includes 771 images of football players, images taken from 3 views at 257 time instances, and 14 annotated body joints.
Source: http://www.csc.kth.se/~vahidk/football_data.html
Image Source: http://www.csc.kth.se/~vahidk/football_data.html"	https://paperswithcode.com/dataset/kth-multiview-football-i	01/01/2012	KTH Multiview Football I					
1016	TUM monoVO	"TUM monoVO is a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments – ranging from narrow indoor corridors to wide outdoor scenes.
All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence.
In contrast to existing datasets, all sequences are photometrically calibrated: the dataset creators provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting).
Source: https://vision.in.tum.de/data/datasets/mono-dataset
Image Source: https://vision.in.tum.de/data/datasets/mono-dataset"	https://paperswithcode.com/dataset/tum-monovo	01/01/2016	TUM monoVO					
1017	ICL-NUIM	"The ICL-NUIM dataset aims at benchmarking RGB-D, Visual Odometry and SLAM algorithms. Two different scenes (the living room and the office room scene) are provided with ground truth. Living room has 3D surface ground truth together with the depth-maps as well as camera poses and as a result perfectly suits not just for benchmarking camera trajectory but also reconstruction. Office room scene comes with only trajectory data and does not have any explicit 3D model with it.
All data is compatible with the evaluation tools available for the TUM RGB-D dataset, and if your system can take TUM RGB-D format PNGs as input, the authors’ TUM RGB-D Compatible data will also work (given the correct camera parameters).
Source: https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html
Image Source: https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html"	https://paperswithcode.com/dataset/icl-nuim-1	01/01/2014	ICL-NUIM					
1018	EuRoC MAV	"EuRoC MAV is a visual-inertial datasets collected on-board a Micro Aerial Vehicle (MAV). The dataset contains stereo images, synchronized IMU measurements, and accurate motion and structure ground-truth. The datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data
Source: https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets
Image Source: https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets"	https://paperswithcode.com/dataset/euroc-mav	01/01/2016	EuRoC MAV					
1019	Sugar Beets 2016	"Sugar Beets 2016 is a robot dataset for plant classification as well as localization and mapping that covers the relevant stages for robotic intervention and weed control. It contains around 5TB of data recorded from a robot with a 4-channel multi-spectral camera and a RGB-D sensor to capture detailed information about the plantation.
Source: https://www.ipb.uni-bonn.de/data/sugarbeets2016/
Image Source: https://www.ipb.uni-bonn.de/data/sugarbeets2016/"	https://paperswithcode.com/dataset/sugar-beets-2016	01/01/2017	Sugar Beets 2016					
1020	HDM05	"HDM05 is a MoCap (motion capture) dataset. It contains more than three hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. HDM05 contains almost 2337 sequences with 130 motion classes performed by 5 different actors.
Source: http://resources.mpi-inf.mpg.de/HDM05/
Image Source: https://arxiv.org/pdf/1908.05750.pdf"	https://paperswithcode.com/dataset/hdm05		HDM05					
1021	USYD CAMPUS	"USYD CAMPUS is a driving dataset collected by Zhou et al at the University of Sydney (USyd) campus and surroundings. This USYD Campus Dataset contains more than 60 weeks of drives and is continuously updated. It includes multiple sensor modalities (camera, lidar, GPS, IMU, wheel encoder, steering angle, etc.) and covers various environmental conditions as well as diverse changes to illumination, scene structure, and pedestrian/vehicle traffic volumes.
Source: http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/
Image Source: http://its.acfr.usyd.edu.au/datasets/usyd-campus-dataset/"	https://paperswithcode.com/dataset/usyd-campus		USYD CAMPUS					
1022	TRECVID	"TRECVID is a yearly set of competitions centered on video retrieval and indexing, hosting a variety of video data sets.
Source: YouTube-BoundingBoxes: A Large High-PrecisionHuman-Annotated Data Set for Object Detection in Video
Image Source: https://www-nlpir.nist.gov/projects/tv2016/tv2016.html"	https://paperswithcode.com/dataset/trecvid		TRECVID					
1023	Partial-REID	"Partial REID is a specially designed partial person reidentification dataset that includes 600 images from 60 people, with 5 full-body images and 5 occluded images per person. These images were collected on a university campus by 6 cameras from different viewpoints, backgrounds and different types of occlusion. The examples of partial persons in the Partial REID dataset are shown in the Figure.
Source: Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification
Image Source: https://arxiv.org/abs/1810.07399"	https://paperswithcode.com/dataset/partial-reid	01/01/2015	Partial-REID					
1024	D-HAZY	"The D-HAZY dataset is generated from NYU depth indoor image collection. D-HAZY contains depth map for each indoor hazy image. It contains 1400+ real images and corresponding depth maps used to synthesize hazy scenes based on Koschmieder’s light propagation mode
Source: C2MSNet: A Novel approach for single image haze removal
Image Source: https://www.semanticscholar.org/paper/D-HAZY%3A-A-dataset-to-evaluate-quantitatively-Ancuti-Ancuti/9451d0b1bfbba5f3e19c083866f1394aabf7d06c"	https://paperswithcode.com/dataset/d-hazy	01/01/2016	D-HAZY					
1025	Middlebury 2014	"The Middlebury 2014 dataset contains a set of 23 high resolution stereo pairs for which known camera calibration parameters and ground truth disparity maps obtained with a structured light scanner are available. The images in the Middlebury dataset all show static indoor scenes with varying difficulties including repetitive structures, occlusions, wiry objects as well as untextured areas.
Source: Using Self-Contradiction to Learn Confidence Measures in Stereo Vision
Image Source: https://vision.middlebury.edu/stereo/data/scenes2014/"	https://paperswithcode.com/dataset/middlebury-2014	01/01/2014	Middlebury 2014					
1026	Partial-iLIDS	"Partial iLIDS is a dataset for occluded person person re-identification. It contains a total of 476 images of 119 people captured by 4 non-overlapping cameras. Some images contain people occluded by other individuals or luggage.
Source: Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification
Image Source: https://arxiv.org/pdf/1904.04975.pdf"	https://paperswithcode.com/dataset/partial-ilids	01/01/2011	Partial-iLIDS					
1027	Oxford Town Center	"The Oxford Town Center dataset is a 5-minute video with 7500 frames annotated, which is divided into 6500 for training and 1000 for testing data for pedestrian detection. The data was recorded from a CCTV camera in Oxford for research and development into activity and face recognition.
Source: LCrowdV: Generating Labeled Videos for Simulation-based Crowd Behavior Learning
Image Source: https://megapixels.cc/oxford_town_centre/"	https://paperswithcode.com/dataset/oxford-town-center	01/01/2011	Oxford Town Center					
1028	VeRi-Wild	"Veri-Wild is the largest vehicle re-identification dataset (as of CVPR 2019). The dataset is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30× 24h) under unconstrained scenarios. This dataset comprises 416,314 vehicle images of 40,671 identities. Evaluation on this dataset is split across three subsets: small, medium and large; comprising 3000, 5000 and 10,000 identities respectively (in probe and gallery sets).
Source: Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding
Image Source: https://github.com/PKU-IMRE/VERI-Wild"	https://paperswithcode.com/dataset/veri-wild	01/01/2019	VeRi-Wild					
1029	DIODE	"Diode Dense Indoor/Outdoor DEpth (DIODE) is the first standard dataset for monocular depth estimation comprising diverse indoor and outdoor scenes acquired with the same hardware setup. The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each. The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans. The ground truth density for the indoor training and validation splits are approximately 99.54% and 99%, respectively. The density of the outdoor sets are naturally lower with 67.19% for training and 78.33% for validation subsets. The indoor and outdoor ranges for the dataset are 50m and 300m, respectively.
Source: Bidirectional Attention Network for Monocular Depth Estimation
Image Source: https://diode-dataset.org/"	https://paperswithcode.com/dataset/diode	01/01/2019	Dense Indoor and Outdoor Depth					
1030	Airport	"The Airport dataset is a dataset for person re-identification which consists of 39,902 images and 9,651 identities across six cameras.
Source: An Evaluation of Deep CNN Baselines for Scene-Independent Person Re-Identification
Image Source: http://www.northeastern.edu/alert/transitioning-technology/alert-datasets/alert-airport-re-identification-dataset/"	https://paperswithcode.com/dataset/airport	01/01/2019	Airport					
1031	Musk v1	"The Musk dataset describes a set of molecules, and the objective is to detect musks from non-musks. This dataset describes a set of 92 molecules of which 47 are judged by human experts to be musks and the remaining 45 molecules are judged to be non-musks. There are 166 features available that describe the molecules based on the shape of the molecule.
Source: Estimation of Dimensions Contributing to Detected Anomalies with Variational Autoencoders"	https://paperswithcode.com/dataset/musk-v1	01/01/1997	Musk v1					
1032	Musk v2	"The Musk2 dataset is a set of 102 molecules of which 39 are judged by human experts to be musks and the remaining 63 molecules are judged to be non-musks. Each instance corresponds to a possible configuration of a molecule. The 166 features that describe these molecules depend upon the exact shape, or conformation, of the molecule.
Source: Confidence-Constrained Maximum Entropy Framework for Learning from Multi-Instance Data"	https://paperswithcode.com/dataset/musk-v2	01/01/1997	Musk v2					
1033	RMRC 2014	"The RMRC 2014 indoor dataset is a dataset for indoor semantic segmentation. It employs the NYU Depth V2 and Sun3D datasets to define the training set. The test data consists of newly acquired images.
Source: https://cs.nyu.edu/~silberman/rmrc2014/indoor.php
Image Source: https://cs.nyu.edu/~silberman/rmrc2014/indoor.php"	https://paperswithcode.com/dataset/rmrc-2014	01/01/2012	RMRC 2014					
1034	Middlebury 2001	"The Middlebury 2001 is a stereo dataset of indoor scenes with multiple handcrafted layouts.
Source: https://vision.middlebury.edu/stereo/data/scenes2001/
Image Source: https://vision.middlebury.edu/stereo/data/scenes2001/"	https://paperswithcode.com/dataset/middlebury-2001	01/01/2002	Middlebury 2001					
1035	Middlebury 2006	"The Middlebury 2006 is a stereo dataset of indoor scenes with multiple handcrafted layouts.
Source: https://vision.middlebury.edu/stereo/data/scenes2006/
Image Source: https://vision.middlebury.edu/stereo/data/scenes2006/"	https://paperswithcode.com/dataset/middlebury-2006	01/01/2007	Middlebury 2006					
1036	DukeMTMC-attribute	"The images in DukeMTMC-attribute dataset comes from Duke University. There are 1812 identities and 34183 annotated bounding boxes in the DukeMTMC-attribute dataset. This dataset contains 702 identities for training and 1110 identities for testing, corresponding to 16522 and 17661 images respectively. The attributes are annotated in the identity level, every image in this dataset is annotated with 23 attributes.
NOTE: This dataset has been retracted.
Source: Pedestrian Attribute Recognition: A Survey
Image Source: http://irip.buaa.edu.cn/mars_duke_attributes/index.html"	https://paperswithcode.com/dataset/dukemtmc-attribute	01/01/2017	DukeMTMC-attribute					
1037	Occluded REID	"Occluded REID is an occluded person dataset captured by mobile cameras, consisting of 2,000 images of 200 occluded persons (see Fig. (c)). Each identity has 5 full-body person images and 5 occluded person images with different types of occlusion.
Source: Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification
Image Source: https://github.com/tinajia2012/ICME2018_Occluded-Person-Reidentification_datasets"	https://paperswithcode.com/dataset/occluded-reid	01/01/2018	Occluded REID					
1038	NYU Hand	"The NYU Hand pose dataset contains 8252 test-set and 72757 training-set frames of captured RGBD data with ground-truth hand-pose information. For each frame, the RGBD data from 3 Kinects is provided: a frontal view and 2 side views. The training set contains samples from a single user only (Jonathan Tompson), while the test set contains samples from two users (Murphy Stein and Jonathan Tompson). A synthetic re-creation (rendering) of the hand pose is also provided for each view.
Source: https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm
Image Source: https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm"	https://paperswithcode.com/dataset/nyu-hand		NYU Hand					
1039	Middlebury 2005	"Middlebury 2005 is a stereo dataset of indoor scenes.
Source: https://vision.middlebury.edu/stereo/data/scenes2005/
Image Source: https://vision.middlebury.edu/stereo/data/scenes2005/"	https://paperswithcode.com/dataset/middlebury-2005	01/01/2007	Middlebury 2005					
1040	Middlebury MVS	"Middlebury MVS is the earliest MVS dataset for multi-view stereo network evaluation. It contains two indoor objects with low-resolution (640 × 480) images and calibrated cameras.
Source: BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks
Image Source: https://vision.middlebury.edu/mview/data/"	https://paperswithcode.com/dataset/middlebury-mvs	01/01/2006	Middlebury MVS					
1041	Middlebury 2003	"Middlebury 2003 is a stereo dataset for indoor scenes.
Source: https://vision.middlebury.edu/stereo/data/scenes2003/
Image Source: https://vision.middlebury.edu/stereo/data/scenes2003/"	https://paperswithcode.com/dataset/middlebury-2003	01/01/2003	Middlebury 2003					
1042	OpeReid	"The OpeReid dataset is a person re-identification dataset that consists of 7,413 images of 200 persons.
Source: Scalable Metric Learning via Weighted Approximate Rank Component Analysis"	https://paperswithcode.com/dataset/opereid	01/01/2014	OpeReid					
1043	Market1501-Attributes	"The Market1501-Attributes dataset is built from the Market1501 dataset. Market1501 Attribute is an augmentation of this dataset with 28 hand annotated attributes, such as gender, age, sleeve length, flags for items carried as well as upper clothes colors and lower clothes colors.
Source: Color inference from semantic labeling for person search in videos
Image Source: https://github.com/vana77/Market-1501_Attribute"	https://paperswithcode.com/dataset/market1501-attributes	01/01/2017	Market1501-Attributes					
1044	Friedman1	"The friedman1 data set is commonly used to test semi-supervised regression methods.
Source: http://search.r-project.org/library/ssr/html/friedman1.html"	https://paperswithcode.com/dataset/friedman1		Friedman1					
1045	NSynth	"NSynth is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments.
Source: Data Augmentation for Instrument Classification Robust to Audio Effects
Image Source: https://magenta.tensorflow.org/nsynth"	https://paperswithcode.com/dataset/nsynth	01/01/2017	NSynth					
1046	DCASE 2016	"DCASE 2016 is a dataset for sound event detection. It consists of 20 short mono sound files for each of 11 sound classes (from office environments, like clearthroat, drawer, or keyboard), each file containing one sound event instance. Sound files are annotated with event on- and offset times, however silences between actual physical sounds (like with a phone ringing) are not marked and hence “included” in the event.
Source: The NIGENS General Sound Events Database
Image Source: https://arxiv.org/pdf/1911.06878.pdf"	https://paperswithcode.com/dataset/dcase-2016	01/01/2016	DCASE 2016					
1047	DSTC7 Task 1	"The DSTC7 Task 1 dataset is a dataset and task for goal-oriented dialogue. The data originates from human-human conversations, which is built from online resources, specifically the Ubuntu Internet Relay Chat (IRC) channel and an Advising dataset from the University of Michigan.
Source: Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems
Image Source: https://www.aclweb.org/anthology/W19-4107.pdf"	https://paperswithcode.com/dataset/dstc7-task-1		Dialog System Technology Challenges Task 1					
1048	DSTC7 Task 2	"DSTC Task 2 is a dataset and task for end-to-end conversation modeling. The goal is to generate conversational responses that go beyond trivial chitchat by injecting informative responses that are grounded in external knowledge. The data consists of conversational data from Reddit, and contextually-relevant “facts” taken from the website that started the Reddit conversation. That is the setup is grounded, as each conversation in the data is about a specific web page that was linked at the start of the conversation.
Source: http://workshop.colips.org/dstc7/
Image Source: http://workshop.colips.org/dstc7/"	https://paperswithcode.com/dataset/dstc7-task-2		Dialog System Technology Challenges Task 2					
1049	Music21	"Music21 is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models.
Source: Music Gesture for Visual Sound Separation
Image Source: https://towardsdatascience.com/midi-music-data-extraction-using-music21-and-word2vec-on-kaggle-cb383261cd4e"	https://paperswithcode.com/dataset/music21	01/01/2010	Music21					
1050	RWC	"The RWC (Real World Computing) Music Database is a copyright-cleared music database (DB) that is available to researchers as a common foundation for research. It contains around 100 complete songs with manually labeled section boundaries. For the 50 instruments, individual sounds at half-tone intervals were captured with several variations of playing styles, dynamics, instrument manufacturers and musicians.
Source: https://staff.aist.go.jp/m.goto/RWC-MDB/
Image Source: http://www.cs.tut.fi/sgn/arg/matti/demos/basstrans/"	https://paperswithcode.com/dataset/rwc	01/01/2002	Real World Computing Music Database					
1051	DCASE 2013	"DCASE 2013 is a dataset for sound event detection. It consists of audio-only recordings where individual sound events are prominent in an acoustic scene.
Source: http://dcase.community/challenge2013/index
Image Source: https://link.springer.com/article/10.1186/s13636-018-0138-4"	https://paperswithcode.com/dataset/dcase-2013	01/01/2015	DCASE 2013					
1052	LITIS Rouen	"The LITIS-Rouen dataset  is a dataset for audio scenes. It consists of 3026 examples of 19 scene categories. Each class is specific to a location such as a train station or an open market. The audio recordings have a duration of 30 seconds and a sampling rate of 22050 Hz. The dataset has a total duration of 1500 minutes.
Source: Spatio-Temporal Attention Pooling for Audio Scene Classification
Image Source: https://www.researchgate.net/figure/Summary-of-Litis-Rouen-audio-scene-dataset_tbl1_329608235"	https://paperswithcode.com/dataset/litis-rouen	01/01/2015	LITIS Rouen					
1053	YouTube-100M	"The YouTube-100M data set consists of 100 million YouTube videos: 70M training videos, 10M evaluation videos, and 20M validation videos. Videos average 4.6 minutes each for a total of 5.4M training hours. Each of these videos is labeled with 1 or more topic identifiers from a set of 30,871 labels. There are an average of around 5 labels per video. The labels are assigned automatically based on a combination of metadata (title, description, comments, etc.), context, and image content for each video. The labels apply to the entire video and range from very generic (e.g. “Song”) to very specific (e.g. “Cormorant”).
Being machine generated, the labels are not 100% accurate and of the 30K labels, some are clearly acoustically relevant (“Trumpet”) and others are less so (“Web Page”). Videos often bear annotations with multiple degrees of specificity. For example, videos labeled with “Trumpet” are often labeled “Entertainment” as well, although no hierarchy is enforced.
Source: https://arxiv.org/pdf/1609.09430.pdf
Image Source: https://arxiv.org/pdf/1609.09430.pdf"	https://paperswithcode.com/dataset/youtube-100m	01/01/2017	YouTube-100m					
1054	TUT Acoustic Scenes 2017	"The TUT Acoustic Scenes 2017 dataset is a collection of recordings from various acoustic scenes all from distinct locations. For each recording location 3-5 minute long audio recordings are captured and are split into 10 seconds which act as unit of sample for this task. All the audio clips are recorded with 44.1 kHz sampling rate and 24 bit resolution.
Source: Ensemble of deep neural networks for acoustic scene classification
Image Source: https://www.mathworks.com/help/audio/ug/acoustic-scene-recognition-using-late-fusion.html;jsessionid=95c969bc690c06fe42a7ed17f57e"	https://paperswithcode.com/dataset/tut-acoustic-scenes-2017	01/01/2016	TUT Acoustic Scenes 2017					
1055	FSDnoisy18k	"The FSDnoisy18k dataset is an open dataset containing 42.5 hours of audio across 20 sound event classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. The audio content is taken from Freesound, and the dataset was curated using the Freesound Annotator. The noisy set of FSDnoisy18k consists of 15,813 audio clips (38.8h), and the test set consists of 947 audio clips (1.4h) with correct labels. The dataset features two main types of label noise: in-vocabulary (IV) and out-of-vocabulary (OOV). IV applies when, given an observed label that is incorrect or incomplete, the true or missing label is part of the target class set. Analogously, OOV means that the true or missing label is not covered by those 20 classes.
Source: Model-agnostic Approaches to Handling Noisy Labels When Training Sound Event Classifiers
Image Source: http://www.eduardofonseca.net/FSDnoisy18k/"	https://paperswithcode.com/dataset/fsdnoisy18k	01/01/2019	FSDnoisy18k					
1056	CHiME-Home	"CHiME-Home is a dataset for sound source recognition in a domestic environment. It uses around 6.8 hours of domestic environment audio recordings. The recordings were obtained from the CHiME projects – computational hearing in multisource environments – where recording equipment was positioned inside an English Victorian semi-detached house. The recordings were selected from 22 sessions totalling 19.5 hours, with each session made between 7:30 in the morning and 20:00 in the evening. In the considered recordings, the equipment was placed in the lounge (sitting room) near the door opening onto a hallway, with the hallway opening onto a kitchen with no door. With the lounge door typically open, prominent sounds thus may originate from sources both in the lounge and kitchen.
The choice of permitted labels was motivated by the sources present in the considered acoustic environment: Human speakers (c,m,f); human activity (p); television (v); household appliances (b). Further labels o,S,U respectively relate to any other identifiable sounds, silence, unidentifiable sounds. Labels S,U may respectively only be assigned in isolation. Annotators were acquired to assign at least one label to a chunk, thus annotators may either assign one or more labels from the set {c,m,f,v,p,b,o}, or may alternatively ‘flag’ the chunk using a single label from the set {S,U}.
Source: https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment
Image Source: https://www.researchgate.net/publication/308732345_CHiME-Home_A_dataset_for_sound_source_recognition_in_a_domestic_environment"	https://paperswithcode.com/dataset/chime-home	01/01/2015	CHiME-Home					
1057	Bach Chorales	"Bach chorales is a univariate time series based on chorales, where the task is to learn generative grammar. The dataset consists of single-line melodies of 100 Bach chorales (originally 4 voices). The melody line can be studied independently of other voices. The grand challenge is to learn a generative grammar for stylistically valid chorales.
Source: https://archive.ics.uci.edu/ml/datasets/Bach+Chorales
Image Source: https://arxiv.org/pdf/1612.01010.pdf"	https://paperswithcode.com/dataset/bach-chorales		Bach Chorales					
1058	FAIR-Play	"FAIR-Play is a video-audio dataset consisting of 1,871 video clips and their corresponding binaural audio clips recording in a music room. The video clip and binaural clip of the same index are roughly aligned.
Source: https://github.com/facebookresearch/FAIR-Play
Image Source: https://github.com/facebookresearch/FAIR-Play"	https://paperswithcode.com/dataset/fair-play	01/01/2019	FAIR-Play					
1059	BirdVox-full-night	"The BirdVox-full-night dataset contains 6 audio recordings, each about ten hours in duration. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10.
Andrew Farnsworth used the Raven software to pinpoint every avian flight call in time and frequency. He found 35402 flight calls in total. He estimates that about 25 different species of passerines (thrushes, warblers, and sparrows) are present in this recording. Species are not labeled in BirdVox-full-night, but it is possible to tell apart thrushes from warblers and sparrrows by looking at the center frequencies of their calls. The annotation process took 102 hours.
Source: https://wp.nyu.edu/birdvox/birdvox-full-night/
Image Source: https://wp.nyu.edu/birdvox/birdvox-full-night/"	https://paperswithcode.com/dataset/birdvox-full-night	01/01/2018	BirdVox-full-night					
1060	POP909	"POP909 is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms.
Source: https://arxiv.org/pdf/2008.07142.pdf
Image Source: https://arxiv.org/pdf/2008.07142.pdf"	https://paperswithcode.com/dataset/pop909		POP909					
1061	SINS	"SINS is a database of continuous real-life audio recordings in a home environment. The home is a vacation home and one person lived there during the recording period of over on week. It was collected using a network of 13 microphone arrays distributed over the multiple rooms. Each microphone array consisted of 4 linearly arranged microphones. Recordings were annotated based on the level of daily activities performed in the environment.
Source: https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf
Image Source: https://www.cs.tut.fi/sgn/arg/dcase2017/documents/workshop_papers/DCASE2017Workshop_Dekkers_141.pdf"	https://paperswithcode.com/dataset/sins		SINS					
1062	Robbie Williams	"Robbie Williams is a dataset of 65 songs by Robbie Williams. It consists of chords, keys and beats. The dataset does not include audio.
Source: A BI-DIRECTIONAL TRANSFORMER FOR MUSICAL CHORD RECOGNITION
Image Source: https://www.rwdb.info/"	https://paperswithcode.com/dataset/robbie-williams		Robbie Williams					
1063	MuseData	"MuseData is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files.
Source: https://arxiv.org/pdf/1206.6392v1.pdf
Image Source: https://arxiv.org/pdf/1206.6392v1.pdf"	https://paperswithcode.com/dataset/musedata	01/01/2012	MuseData					
1064	URBAN-SED	"URBAN-SED is a dataset of 10,000 soundscapes with sound event annotations generated using the scraper library. The dataset includes 10,000 soundscapes, totals almost 30 hours and includes close to 50,000 annotated sound events. Every soundscape is 10 seconds long and has a background of Brownian noise resembling the typical “hum” often heard in urban environments. Every soundscape contains between 1-9 sound evnts from the following classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren and street_music.
The source material for the sound events are the clips from the UrbanSound8K dataset. URBAN-SED comes pre-sorted into three sets: train, validate and test. There are 6000 soundscapes in the training set, generated using clips from folds 1-6 in UrbanSound8K, 2000 soundscapes in the validation set, generated using clips from fold 7-8 in UrbanSound8K, and 2000 soundscapes in the test set, generated using clips from folds 9-10 in UrbanSound8K.
Source: http://urbansed.weebly.com/
Image Source: http://urbansed.weebly.com/"	https://paperswithcode.com/dataset/urban-sed	01/01/2017	URBAN-SED					
1065	TUT-SED Synthetic 2016	"TUT-SED Synthetic 2016 contains of mixture signals artificially generated from isolated sound events samples. This approach is used to get more accurate onset and offset annotations than in dataset using recordings from real acoustic environments where the annotations are always subjective.
Mixture signals in the dataset are created by randomly selecting and mixing isolated sound events from 16 sound event classes together. The resulting mixtures contains sound events with varying polyphony. All together 994 sound event samples were purchased from Sound Ideas. From the 100 mixtures created, 60% were assigned for training, 20% for testing and 20% for validation. The total amount of audio material in the dataset is 566 minutes.
Different instances of the sound events are used to synthesize the training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Between events, random length silent region was introduced. Such tracks were created for four to nine event classes, and were then mixed together to form the mixture signal. As sound events are not consistently active during the samples (e.g. footsteps), automatic signal energy based annotation was applied to obtain accurate event activity within the sample. Annotation of the mixture signal was created by pooling together event activity annotation of used samples.
Source: https://webpages.tuni.fi/arg/paper/taslp2017-crnn-sed/tut-sed-synthetic-2016
Image Source: https://arxiv.org/abs/1702.06286"	https://paperswithcode.com/dataset/tut-sed-synthetic-2016	01/01/2017	TUT-SED Synthetic 2016					
1066	ISMIR Genre	"ISMIR2004 is an audio dataset consisting of 6 genres with 729 excerpts of 30 seconds. It is a dataset used for musical genre classification. The training set consists of 320 classical music samples, 115 electronic music samples, 26 jazz blues samples, 45 metal/punk samples, 101 rock/pop samples and 122 world samples.
Source: http://ismir2004.ismir.net/genre_contest/index.html
Image Source: http://ismir2004.ismir.net/genre_contest/index.html"	https://paperswithcode.com/dataset/ismir-genre		ISMIR2004 Genre					
1067	NES-MDB	"The Nintendo Entertainment System Music Database (NES-MDB) is a dataset intended for building automatic music composition systems for the NES audio synthesizer. It consists of  5278 songs from the soundtracks of 397 NES games. The dataset represents 296 unique composers, and the songs contain more than two million notes combined. It has file format options for MIDI, score and NLM (NES Language Modeling).
Source: https://github.com/chrisdonahue/nesmdb
Image Source: https://github.com/chrisdonahue/nesmdb
Audio Source: https://github.com/chrisdonahue/nesmdb"	https://paperswithcode.com/dataset/nes-mdb	01/01/2018	Nintendo Entertainment System Music Database					
1068	CLO-43SD	"CLO-43SD is a dataset for multi-class species identification in avian flight calls. It consists of 5,428 labeled audio clips of flight calls from 43 different species of North American woodwarblers (in the family Parulidae). The clips came from a variety of recording conditions, including clean recordings obtained using highly-directional shotgun microphones, recordings obtained from noisier field recordings using omnidirectional microphones, and recordings obtained from birds in captivity.
Source: https://wp.nyu.edu/birdvox/codedata/
Image Source: https://www.allaboutbirds.org/a-rosetta-stone-for-identifying-warblers-migration-calls/"	https://paperswithcode.com/dataset/clo-43sd		CLO-43SD					
1069	CLO-WTSP	"CLO-WTSP is a dataset for species-specific flight call identification for the White-Throated Sparrow. 16,703 labeled audio clips captured by remote acoustic sensors deployed in Ithaca, NY and NYC over the fall 2014 and spring 2015 migration seasons. Each clip is labeled to indicate whether it contains a flight call from the target species White-Throated Sparrow (WTSP), a flight call from a non-target species, or no flight call at all.​
Source: https://wp.nyu.edu/birdvox/codedata/
Image Source: https://en.wikipedia.org/wiki/White-throated_sparrow#/media/File:Sparrow,_White_throated.jpg"	https://paperswithcode.com/dataset/clo-wtsp		CLO-WTSP					
1070	CLO-SWTH	"CLO-SWTH is a dataset for species-specific flight call identification for the Swainson’s Thrush. 179,111 labeled audio clips captured by remote acoustic sensors deployed in Ithaca, NY and NYC over the fall 2014 and spring 2015 migration seasons. Each clip is labeled to indicate whether it contains a flight call from the target species Swainson’s Thrush (SWTH), a flight call from a non-target species, or no flight call at all.
Source: https://wp.nyu.edu/birdvox/codedata/
Image Source: https://commons.wikimedia.org/wiki/Category:Catharus_ustulatus#/media/File:A_Swainson's_thrush_perched_in_a_tree_(7d58595b-c495-4744-9f20-2b301fa1cc63).jpg)"	https://paperswithcode.com/dataset/clo-swth		CLO-SWTH					
1071	Bach Doodle	"The Bach Doodle Dataset is composed of 21.6 million harmonizations submitted from the Bach Doodle. The dataset contains both metadata about the composition (such as the country of origin and feedback), as well as a MIDI of the user-entered melody and a MIDI of the generated harmonization. The dataset contains about 6 years of user entered music.
Source: https://magenta.tensorflow.org/datasets/bach-doodle
Image Source: https://magenta.tensorflow.org/datasets/bach-doodle"	https://paperswithcode.com/dataset/bach-doodle	01/01/2019	Bach Doodle					
1072	DCASE 2018 Task 4	"DCASE2018 Task 4 is a dataset for large-scale weakly labeled semi-supervised sound event detection in domestic environments. The data are YouTube video excerpts focusing on domestic context which could be used for example in ambient assisted living applications. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events...) and potential industrial applications.
Specifically, the task employs a subset of “Audioset: An Ontology And Human-Labeled Dataset For Audio Events” by Google. Audioset consists of an expanding ontology of 632 sound event classes and a collection of 2 million human-labeled 10-second sound clips (less than 21% are shorter than 10-seconds) drawn from 2 million Youtube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds.
Task 4 focuses on a subset of Audioset that consists of 10 classes of sound events: speech, dog, cat, alarm bell ringing, dishes, frying, blender, running water, vacuum cleaner, electric shaver toothbrush.
Source: http://dcase.community/challenge2018/index
Image Source: http://dcase.community/challenge2018/task-large-scale-weakly-labeled-semi-supervised-sound-event-detection"	https://paperswithcode.com/dataset/dcase-2018-task-4	27/07/2018	DCASE 2018 Task 4					
1073	freefield1010	"Freefield1010 is a collection of 7,690 excerpts from field recordings around the world, gathered by the FreeSound project, and then standardised for research.
Source: http://dcase.community/challenge2018/task-bird-audio-detection
Image Source: https://arxiv.org/pdf/1309.5275.pdf"	https://paperswithcode.com/dataset/freefield1010	01/01/2018	freefield1010					
1074	warblrb10k	"warblrb10k is a collection of 10,000 smartphone audio recordings from around the UK, crowdsourced by users of Warblr the bird recognition app. The audio covers a wide distribution of UK locations and environments, and includes weather noise, traffic noise, human speech and even human bird imitations.
Source: http://dcase.community/challenge2018/task-bird-audio-detection
Image Source: https://www.warblr.co.uk/"	https://paperswithcode.com/dataset/warblrb10k	01/01/2018	warblrb10k					
1075	Chernobyl	"Chernobyl is a collection of 620 audio clips collected from unattended remote monitoring equipment in the Chernobyl Exclusion Zone (CEZ). This data was collected as part of the TREE (Transfer-Exposure-Effects) research project into the long-term effects of the Chernobyl accident on local ecology. The audio covers a range of birds and includes weather, large mammal and insect noise sampled across various CEZ environments, including abandoned village, grassland and forest areas.
Source: http://dcase.community/challenge2018/task-bird-audio-detection
Image Source: https://en.wikipedia.org/wiki/Effects_of_the_Chernobyl_disaster#/media/File:Chernobyl,_Ukraine.jpg"	https://paperswithcode.com/dataset/chernobyl	01/01/2018	Chernobyl					
1076	PolandNFC	"PolandNFC is a collection of 4,000 recordings from Hanna Pamuła's PhD project of monitoring autumn nocturnal bird migration. The recordings were collected every night, from September to November 2016 on the Baltic Sea coast, Poland, using Song Meter SM2 units with microphones mounted on 3–5 m poles. A subset derived from 15 nights with different weather conditions and background noise including wind, rain, sea noise, insect calls, human voice and deer calls was used in DCASE 2018 Challenge.
Source: http://dcase.community/challenge2018/task-bird-audio-detection
Image Source: http://dcase.community/challenge2018/task-bird-audio-detection"	https://paperswithcode.com/dataset/polandnfc	01/01/2018	PolandNFC					
1077	NIPS4Bplus	"NIPS4Bplus is a richly annotated birdsong audio dataset, that is comprised of recordings containing bird vocalisations along with their active species tags plus the temporal annotations acquired for them. It consists of around 687 recordings, 87 classes, species tags, annotations. The total duration of audio is around 1 hour.
Source: https://peerj.com/articles/cs-223.pdf
Image Source: https://peerj.com/articles/cs-223.pdf"	https://paperswithcode.com/dataset/nips4bplus	01/01/2018	NIPS4Bplus					
1078	BirdVox-DCASE-20k	"The BirdVox-DCASE-20k dataset contains 20,000 ten-second audio recordings. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10.
Out of these 20,000 recording, 10,017 (50.09%) contain at least one bird vocalization (either song, call, or chatter).
The dataset is a derivative work of the BirdVox-full-night dataset, containing almost as much data but formatted into ten-second excerpts rather than ten-hour full night recordings.
Source: https://zenodo.org/record/1208080
Image Source: http://dcase.community/challenge2018/task-bird-audio-detection"	https://paperswithcode.com/dataset/birdvox-dcase-20k	01/01/2018	BirdVox-DCASE-20k					
1079	BirdCLEF 2019	"BirdClef 2019 is a bird soundscape dataset. It contains around 350 hours of manually annotated soundscapes using 30 field recorders between January and June of 2017 in Ithaca, NY, USA. There are around 50,000 recordings in the dataset in total, with 659 classes. The dataset also contains species tags.
Source: https://www.imageclef.org/BirdCLEF2019
Image Source: http://dcase.community/challenge2018/task-bird-audio-detection"	https://paperswithcode.com/dataset/birdclef-2019	01/01/2019	BirdCLEF 2019					
1080	BirdCLEF 2018	"BirdClef 2018 is a bird soundscape dataset based on the contributions of the Xeno-canto network. The training set contains 36,496 recordings covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). There are about 68 hours of recordings in total, with 1,500 classes and species tags.
Source: https://www.imageclef.org/BirdCLEF2019
Image Source: http://dcase.community/challenge2018/task-bird-audio-detection"	https://paperswithcode.com/dataset/birdclef-2018		BirdCLEF 2018					
1081	FSDKaggle2018	"FSDKaggle2018 is an audio dataset containing 11,073 audio files annotated with 41 labels of the AudioSet Ontology. FSDKaggle2018 has been used for the DCASE Challenge 2018 Task 2. All audio samples are gathered from Freesound and are provided as uncompressed PCM 16 bit, 44.1 kHz mono audio files. The 41 categories of the AudioSet Ontology are:
""Acoustic_guitar"", ""Applause"", ""Bark"", ""Bass_drum"", ""Burping_or_eructation"", ""Bus"", ""Cello"", ""Chime"", ""Clarinet"", ""Computer_keyboard"", ""Cough"", ""Cowbell"", ""Double_bass"", ""Drawer_open_or_close"", ""Electric_piano"", ""Fart"", ""Finger_snapping"", ""Fireworks"", ""Flute"", ""Glockenspiel"", ""Gong"", ""Gunshot_or_gunfire"", ""Harmonica"", ""Hi-hat"", ""Keys_jangling"", ""Knock"", ""Laughter"", ""Meow"", ""Microwave_oven"", ""Oboe"", ""Saxophone"", ""Scissors"", ""Shatter"", ""Snare_drum"", ""Squeak"", ""Tambourine"", ""Tearing"", ""Telephone"", ""Trumpet"", ""Violin_or_fiddle"", ""Writing"".
Source: https://zenodo.org/record/2552860
Image Source: https://labs.freesound.org/datasets/"	https://paperswithcode.com/dataset/fsdkaggle2018	01/01/2018	FSDKaggle2018					
1082	FSDKaggle2019	"FSDKaggle2019 is an audio dataset containing 29,266 audio files annotated with 80 labels of the AudioSet Ontology. FSDKaggle2019 has been used for the DCASE Challenge 2019 Task 2, which was run as a Kaggle competition titled Freesound Audio Tagging 2019. The dataset allows development and evaluation of machine listening methods in conditions of label noise, minimal supervision, and real-world acoustic mismatch. FSDKaggle2019 consists of two train sets and one test set. One train set and the test set consists of manually-labeled data from Freesound, while the other train set consists of noisily labeled web audio data from Flickr videos taken from the YFCC dataset.
The curated train set consists of manually labeled data from FSD: 4970 total clips with a total duration of 10.5 hours.  The noisy train set has 19,815 clips with a total duration of 80 hours. The test set has 4481 clips with a total duration of 12.9 hours.
Source: https://labs.freesound.org/datasets/
Image Source: https://labs.freesound.org/datasets/"	https://paperswithcode.com/dataset/fsdkaggle2019	01/01/2019	FSDKaggle2019					
1083	Clotho	"Clotho is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long.
Source: https://zenodo.org/record/3490684
Image Source: https://arxiv.org/abs/1910.09387"	https://paperswithcode.com/dataset/clotho	01/01/2019	Clotho					
1084	DBR	"DBR dataset is an environmental audio dataset created for the Bachelor's Seminar in Signal Processing in Tampere University of Technology. The samples in the dataset were collected from the online audio database Freesound. The dataset consists of three classes, each containing 50 samples, and the classes are 'dog', 'bird', and 'rain' (hence the name DBR).
Source: https://zenodo.org/record/1069747
Image Source: https://medium.com/@anonyomous.ut.grad.student/building-an-audio-classifier-f7c4603aa989"	https://paperswithcode.com/dataset/dbr		DBR					
1085	DESED	"The DESED dataset is a dataset designed to recognize sound event classes in domestic environments. The dataset is designed to be used for sound event detection (SED, recognize events with their time boundaries) but it can also be used for sound event tagging (SET, indicate presence of an event in an audio file).
The dataset is composed of 10 event classes to recognize in 10 second audio files. The classes are: Alarm/bell/ringing, Blender, Cat, Dog, Dishes,
Electric shaver/toothbrush, Frying, Running water, Speech, Vacuum cleaner.
Source: https://project.inria.fr/desed/
Image Source: https://project.inria.fr/desed/"	https://paperswithcode.com/dataset/desed	26/10/2019	Domestic environment sound event detection					
1086	FSL4	"The FSL4 dataset contains ~4000 user-contributed loops uploaded to Freesound. Loops were selected by searching Freesound for sounds with the query terms loop and bpm, and then automatically parsing the returned sound filenames, tags and textual descriptions to identify tempo annotations made by users. For example, a sound containing the tag 120bpm is considered to have a ground truth of 120 BPM.
Source: https://zenodo.org/record/3685832
Image Source: https://archives.ismir.net/ismir2016/paper/000195.pdf"	https://paperswithcode.com/dataset/fsl4	01/01/2016	Freesound Loops 4k					
1087	Freesound One-Shot Percussive Sounds	"The Freesound One-Shot Percussive Sounds dataset contains 10254 one-shot (single event) percussive sounds from Freesound.org and the corresponding timbral analysis. These were used to train the generative model for ""Neural Percussive Synthesis Parameterised by High-Level Timbral Features"".
Source: https://zenodo.org/record/3665275
Image Source: https://freesound.org/people/Robinhood76/sounds/63616/"	https://paperswithcode.com/dataset/freesound-one-shot-percussive-sounds	01/01/2019	Freesound One-Shot Percussive Sounds					
1088	FSD50K	"Freesound Dataset 50k (or FSD50K for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more.
Source: https://zenodo.org/record/4060432
Image Source: https://labs.freesound.org/datasets/"	https://paperswithcode.com/dataset/fsd50k		Freesound Database 50K					
1089	SimSceneTVB Learning	"SimSceneTVB is a dataset of 600 simulated sound scenes of 45s each representing urban sound environments, simulated using the simScene Matlab library. The dataset is divided in two parts with a train subset (400 scenes) and a test subset (200 scenes) for the development of learning-based models.
Each scene is composed of three main sources (traffic, human voices and birds) according to an original scenario, which is composed semi-randomly conditionally to five ambiances: park, quiet street, noisy street, very noisy street and square. Separate channels for the contribution of each source are available. The base audio files used for simulation are obtained from Freesound (https://freesound.org) and LibriSpeech (http://www.openslr.org/12). The sound scenes are scaled according to a playback sound level in dB, which is drawn randomly but remains plausible according to the ambiance.
Source: https://zenodo.org/record/3248703
Image Source: https://hal.archives-ouvertes.fr/hal-01078098v2/document"	https://paperswithcode.com/dataset/simscenetvb-learning		SimSceneTVB Learning					
1090	SimSceneTVB Perception	"SimSceneTVB Perception  is a corpus of 100 sound scenes of 45s each representing urban sound environments, including: 6 scenes recorded in Paris, 19 scenes simulated using simScene to replicate recorded scenarios, 75 scenes simulated using simScene with diverse new scenarios, containing traffic, human voices and bird sources.The base audio files used for simulation are obtained from Freesound (https://freesound.org) and LibriSpeech (http://www.openslr.org/12).
Source: https://zenodo.org/record/3248734
Image Source: https://hal.archives-ouvertes.fr/hal-01078098v2/document"	https://paperswithcode.com/dataset/simscenetvb-perception		SimSceneTVB Perception					
1091	Sound Events for Surveillance Applications	"The Sound Events for Surveillance Applications (SESA) dataset files were obtained from Freesound. The dataset was divided between train (480 files) and test (105 files) folders. All audio files are WAV, Mono-Channel, 16 kHz, and 8-bit with up to 33 seconds. # Classes: 0 - Casual (not a threat) 1 - Gunshot 2 - Explosion 3 - Siren (also contains alarms).
Source: https://zenodo.org/record/3519845
Image Source: https://labs.freesound.org/datasets/"	https://paperswithcode.com/dataset/sound-events-for-surveillance-applications		Sound Events for Surveillance Applications					
1092	TUT Rare Sound Events 2017	"TUT Rare Sound events 2017, development dataset consists of source files for creating mixtures of rare sound events (classes baby cry, gun shot, glass break) with background audio, as well a set of readily generated mixtures and recipes for generating them. The ""source"" part of the dataset consists of two subsets: (a) background recordings from 15 different acoustic scenes, (b) recordings with the target rare sound events from three classes, accompanied by annotations of their temporal occurrences, (c) a set of meta files providing the cross-validation setup: lists of background and target event recordings split into training and test subsets (called ""devtrain"" and ""devtest"", respectively, indicating they are provided as the development dataset, as opposed to the evaluation dataset released separately).
The mixture set consists of two subsets (training and testing), each containing ~1500 mixtures (~500 per target class in each subset, with half of the mixtures not containing any target class events).
Diment, Aleksandr, Mesaros, Annamaria, Heittola, Toni, & Virtanen, Tuomas. (2017). TUT Rare sound events, Development dataset [Data set]. Zenodo. http://doi.org/10.5281/zenodo.401395
Source: https://zenodo.org/record/401395
Image Source: http://dcase.community/challenge2017/task-rare-sound-event-detection"	https://paperswithcode.com/dataset/tut-rare-sound-events-2017		TUT Rare Sound Events 2017					
1093	UrbanSound8K	"Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org.
Source: https://zenodo.org/record/401395
Image Source: https://urbansounddataset.weebly.com/urbansound8k.html"	https://paperswithcode.com/dataset/urbansound8k-1	01/01/2014	UrbanSound8K					
1094	VocalImitationSet	"The VocalImitationSet is a collection of crowd-sourced vocal imitations of a large set of diverse sounds collected from Freesound (https://freesound.org/), which were curated based on Google's AudioSet ontology (https://research.google.com/audioset/).
Source: https://zenodo.org/record/1340763
Image Source: https://www.researchgate.net/publication/332799163_VOCAL_IMITATION_SET_A_DATASET_OF_VOCALLY_IMITATED_SOUND_EVENTS_USING_THE_AUDIOSET_ONTOLOGY"	https://paperswithcode.com/dataset/vocalimitationset		VocalImitationSet					
1095	TUT Sound Events 2018	"The TUT Sounds Event 2018 dataset consists of real-life first order Ambisonic (FOA) format recordings with stationary point sources each associated with a spatial coordinate. The dataset was generated by collecting impulse responses (IR) from a real environment using the Eigenmike spherical microphone array. The measurement was done by slowly moving a Genelec G Two loudspeaker continuously playing a maximum length sequence around the array in circular trajectory in one elevation at a time. The playback volume was set to be 30 dB greater than the ambient sound level. The recording was done in a corridor inside the university with classrooms around it during work hours. The IRs were collected at elevations −40 to 40 with 10-degree increments at 1 m from the Eigenmike and at elevations −20 to 20 with 10-degree increments at 2 m.
Source: https://zenodo.org/record/1237793
Image Source: https://www.cs.tut.fi/~mesaros/pubs/mesaros_eusipco2016-dcase.pdf"	https://paperswithcode.com/dataset/tut-sound-events-2018		TUT Sound Events 2018					
1096	aGender	"The aGender corpus contains audio recordings of predefined utterances and free speech produced by humans of different age and gender. Each utterance is labeled as one of four age groups: Child, Youth, Adult, Senior, and as one of three gender classes: Female, Male and Child.
Source: Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data
Image Source: http://www.lrec-conf.org/proceedings/lrec2010/pdf/262_Paper.pdf"	https://paperswithcode.com/dataset/agender	01/01/2010	aGender					
1097	TUT Sound Events 2017	"The TUT Sound Events 2017 dataset contains 24 audio recordings in a street environment and contains 6 different classes. These classes are: brakes squeaking, car, children, large vehicle, people speaking, and people walking.
Source: Language Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling
Image Source: https://hal.inria.fr/hal-02067935/document"	https://paperswithcode.com/dataset/tut-sound-events-2017	01/01/2016	TUT Sound Events 2017					
1098	DCASE 2014	"DCASE2014 is an audio classification benchmark.
Source: Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization"	https://paperswithcode.com/dataset/dcase-2014	01/01/2015	DCASE 2014					
1099	LOCATA	"The LOCATA dataset is a dataset for acoustic source localization. It consists of real-world ambisonic speech recordings with optically tracked azimuth-elevation labels.
Source: Regression and Classification for Direction-of-Arrival Estimation with Convolutional Recurrent Neural Networks
Image Source: https://www.locata.lms.tf.fau.de/files/2018/05/LOCATA_Paper_SAM_Workshop_2018.pdf"	https://paperswithcode.com/dataset/locata	01/01/2018	LOCATA					
1100	ChestX-ray8	"ChestX-ray8 is a medical imaging dataset which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques.
Source: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610
Image Source: https://nihcc.app.box.com/v/ChestXray-NIHCC"	https://paperswithcode.com/dataset/chestx-ray8	01/01/2017	ChestX-ray8					
1101	PPMI	"The Parkinson’s Progression Markers Initiative (PPMI) dataset originates from an observational clinical and longitudinal study comprising evaluations of people with Parkinson’s disease (PD), those people with high risk, and those who are healthy.
Source: Time-Guided High-Order Attention Model of Longitudinal Heterogeneous Healthcare Data
Image Source: https://www.ppmi-info.org/2013/08/imaging-inventory-whats-in-the-ppmi-database-2/"	https://paperswithcode.com/dataset/ppmi		Parkinson’s Progression Markers Initiative					
1102	BraTS 2018	"BraTS 2018 is a dataset which provides multimodal 3D brain MRIs and ground truth brain tumor segmentations annotated by physicians, consisting of 4 MRI modalities per case (T1, T1c, T2, and FLAIR). Annotations include 3 tumor subregions—the enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core. The annotations were combined into 3 nested subregions—whole tumor (WT), tumor core (TC), and enhancing tumor (ET). The data were collected from 19 institutions, using various MRI scanners
Source: End-to-End Boundary Aware Networks forMedical Image Segmentation
Image Source: https://www.med.upenn.edu/sbia/brats2018/tasks.html"	https://paperswithcode.com/dataset/brats-2018-1	01/01/2015	BraTS 2018					
1103	ISIC 2017 Task 1	"The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 1 challenge dataset for lesion segmentation contains 2,000 images for training with ground truth segmentations (2000 binary mask images).
Source: https://challenge.isic-archive.com/landing/2017/42
Image Source: https://challenge.isic-archive.com/landing/2017/42"	https://paperswithcode.com/dataset/isic-2017-task-1	01/01/2018	ISIC 2017 Task 1					
1104	ISIC 2017 Task 2	"The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 challenge dataset for lesion dermoscopic feature extraction contains the original lesion image, a corresponding superpixel mask, and superpixel-mapped expert annotations of the presence and absence of the following features: (a) network, (b) negative network, (c) streaks and (d) milia-like cysts.
Source: https://challenge.isic-archive.com/landing/2017/43
Image Source: https://challenge.isic-archive.com/landing/2017/43"	https://paperswithcode.com/dataset/isic-2017-task-2	01/01/2018	ISIC 2017 Task 2					
1105	ISIC 2017 Task 3	"The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 challenge dataset for lesion classification contains 2,000 images for training including 374 melanoma, 254 seborrheic keratosis and the remainder as benign nevi (1372).
Source: https://challenge.isic-archive.com/landing/2017/42
Image Source: https://challenge.isic-archive.com/landing/2017/44"	https://paperswithcode.com/dataset/isic-2017-task-3	01/01/2018	ISIC 2017 Task 3					
1106	ISIC 2018 Task 1	"The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. This Task 1 dataset is the challenge on lesion segmentation. It includes 2594 images.
Source: Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions
Image Source: https://challenge2018.isic-archive.com/task1/"	https://paperswithcode.com/dataset/isic-2018-task-1	01/01/2019	ISIC 2018 Task 1					
1107	ISIC 2018 Task 2	"The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 dataset is the challenge on lesion attribute detection. It includes 2594 images. The task is to detect the following dermoscopic attributes: pigment network, negative network, streaks, mila-like cysts and globules (including dots).
Source: Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions
Image Source: https://challenge2018.isic-archive.com/task2/"	https://paperswithcode.com/dataset/isic-2018-task-2	01/01/2019	ISIC 2018 Task 2					
1108	ISIC 2018 Task 3	"The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 dataset is the challenge on lesion classification. It includes 2594 images. The task is to classify the dermoscopic images into one of the following categories: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen’s disease, benign keratosis, dermatofibroma, and vascular lesion.
Source: Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions
Image Source: https://challenge2018.isic-archive.com/task3/"	https://paperswithcode.com/dataset/isic-2018-task-3	01/01/2019	ISIC 2018 Task 3					
1109	HAM10000	"HAM10000 is a dataset of 10000 training images for detecting pigmented skin lesions. The authors collected dermatoscopic images from different populations, acquired and stored by different modalities.
Source: https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000
Image Source: https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000"	https://paperswithcode.com/dataset/ham10000-1	01/01/2018	HAM10000					
1110	BCN_20000	"BCN_20000 is a dataset composed of 19,424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Clínic in Barcelona. The dataset can be used for lesion recognition tasks such as lesion segmentation, lesion detection and lesion classification.
Source: https://arxiv.org/abs/1908.02288
Image Source: https://arxiv.org/abs/1908.02288"	https://paperswithcode.com/dataset/bcn-20000	01/01/2019	BCN_20000					
1111	MSK	"The MSK dataset is a dataset for lesion recognition from the Memorial Sloan-Kettering Cancer Center. It is used as part of the ISIC lesion recognition challenges.
Source: https://arxiv.org/pdf/1710.05006.pdf
Image Source: https://arxiv.org/pdf/1902.03368.pdf"	https://paperswithcode.com/dataset/msk	01/01/2018	MSK					
1112	NeuB1	"NeuB1 is a microscopic neuronal image dataset for retinal vessel segmentation, which contains 112 images of size 512 x 152. The train/test split is 37/75.
Image Source: https://web.bii.a-star.edu.sg/~zhaoh/Jaydeep_Tracing/"	https://paperswithcode.com/dataset/neub1	01/01/2016	NeuB1					
1113	BraTS 2017	"The BRATS2017 dataset. It contains 285 brain tumor MRI scans, with four MRI modalities as T1, T1ce, T2, and Flair for each scan. The dataset also provides full masks for brain tumors, with labels for ED, ET, NET/NCR. The segmentation evaluation is based on three tasks: WT, TC and ET segmentation.
Source: Scribble-based Hierarchical Weakly Supervised Learning for Brain Tumor Segmentation
Image Source: https://www.google.com/search?q=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&oq=A+Modified+U-Net+Convolutional+Network+Featuring+a+Nearest-neighbor+Re-sampling-based+Elastic-Transformation+for+Brain+Tissue+Characterization+and+Segmentation&aqs=chrome..69i57j69i64l3.296j0j4&sourceid=chrome&ie=UTF-8"	https://paperswithcode.com/dataset/brats-2017-1	01/01/2015	BraTS 2017					
1114	BraTS 2015	"The BraTS 2015 dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented “ground truth” is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis.
Source: Brain MRI Tumor Segmentation with Adversarial Networks
Image Source: https://sites.google.com/site/braintumorsegmentation/home/brats2015"	https://paperswithcode.com/dataset/brats-2015-1	01/01/2015	BraTS 2015					
1115	PROMISE12	"The PROMISE12 dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols.
Source: Constrained Deep Networks: Lagrangian Optimization via Log-Barrier Extensions
Image Source: https://promise12.grand-challenge.org/"	https://paperswithcode.com/dataset/promise12	01/01/2014	PROMISE12					
1116	LUNA16	"The LUNA16 (LUng Nodule Analysis) dataset is a dataset for lung segmentation. It consists of 1,186 lung nodules annotated in 888 CT scans.
Source: Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets
Image Source: https://luna16.grand-ch"	https://paperswithcode.com/dataset/luna16	01/01/2016	LUNA16					
1117	BraTS 2013	"BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available.
Source: Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization
Image Source: https://arxiv.org/pdf/1708.00377.pdf"	https://paperswithcode.com/dataset/brats-2013-1	01/01/2015	BraTS 2013					
1118	ISRUC-Sleep	"ISRUC-Sleep is a polysomnographic (PSG) dataset. The data were obtained from human adults, including healthy subjects, and subjects with sleep disorders under the effect of sleep medication. The dataset, which is structured to support different research objectives, comprises three groups of data: (a) data concerning 100 subjects, with one recording session per subject, (b) data gathered from 8 subjects; two recording sessions were performed per subject, which are useful for studies involving changes in the PSG signals over time, (c) data collected from one recording session related to 10 healthy subjects, which are useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders.
Source: https://sleeptight.isr.uc.pt/
Image Source: https://sleeptight.isr.uc.pt/"	https://paperswithcode.com/dataset/isruc-sleep		ISRUC-Sleep					
1119	LiTS17	"LiTS17 is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans.
Image Source: https://arxiv.org/pdf/1707.07734.pdf"	https://paperswithcode.com/dataset/lits17	01/01/2019	Liver Tumor Segmentation Challenge 2017					
1120	NIH-LN	"NIH-Lymph Node (NIH-LN) contains 388 mediastinal LNs in 90 CT scans and 595 abdominal LNs in 86 scans.
Source: https://sleeptight.isr.uc.pt/"	https://paperswithcode.com/dataset/nih-ln		NIH-Lymph Node					
1121	BraTS 2014	"BRATS 2014 is a brain tumor segmentation dataset.
Source: Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization
Image Source: http://people.csail.mit.edu/menze/papers/proceedings_miccai_brats_2014.pdf"	https://paperswithcode.com/dataset/brats-2014-1	01/01/2015	BraTS 2014					
1122	BraTS 2016	"BRATS 2016 is a brain tumor segmentation dataset. It shares the same training set as BRATS 2015, which consists of 220 HHG and 54 LGG. Its testing dataset consists of 191 cases with unknown grades.
Image Source: https://sites.google.com/site/braintumorsegmentation/home/brats_2016"	https://paperswithcode.com/dataset/brats-2016	01/01/2015	BraTS 2016					
1123	DBP15K	"DBP15k contains four language-specific KGs that are respectively extracted from English (En), Chinese (Zh), French (Fr) and Japanese (Ja) DBpedia, each of which contains around 65k-106k entities. Three sets of 15k alignment labels are constructed to align entities between each of the other three languages and En.
Source: Cross-lingual Entity Alignment for Knowledge Graphs with Incidental Supervision from Free Text"	https://paperswithcode.com/dataset/dbp15k	01/01/2017	DBP15K					
1124	MedDialog	"The MedDialog dataset (Chinese) contains conversations (in Chinese) between doctors and patients. It has 1.1 million dialogues and 4 million utterances. The data is continuously growing and more dialogues will be added. The raw dialogues are from haodf.com. All copyrights of the data belong to haodf.com.
Source: GitHub"	https://paperswithcode.com/dataset/meddialog	16/11/2020	MedDialog					
1125	Conceptual Captions	"Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).
Google's Conceptual Captions dataset has more than 3 million images, paired with natural-language captions. In contrast with the curated style of the MS-COCO images, Conceptual Captions images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions are harvested from the Alt-text HTML attribute associated with web images. The authors developed an automatic pipeline that extracts, filters, and transforms candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.
Source: Conceptual Captions
Image Source: Sharma et al"	https://paperswithcode.com/dataset/conceptual-captions	01/07/2018	Conceptual Captions					
1126	CNN/Daily Mail	"CNN/Daily Mail is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites.
In all, the corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as defined by their scripts. The source documents in the training set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. 
Source: Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"	https://paperswithcode.com/dataset/cnn-daily-mail-1	16/08/2016	CNN/Daily Mail					
1127	EuroSAT	"Eurosat is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images.
Source: EuroSAT
Image Source: EuroSAT"	https://paperswithcode.com/dataset/eurosat	31/08/2017	EuroSAT					
1128	RESISC45	RESISC45 dataset is a dataset for Remote Sensing Image Scene Classification (RESISC). It contains 31,500 RGB images of size 256×256 divided into 45 scene classes, each class containing 700 images. Among its notable features, RESISC45 contains varying spatial resolution ranging from 20cm to more than 30m/px.	https://paperswithcode.com/dataset/resisc45	01/03/2017	RESISC45					
1129	Country211	Country211 is an internal OpenAI dataset designed to assess the geolocation capability of visual representations. It filters the YFCC100m dataset (Thomee et al., 2016) to find 211 countries (defined as having an ISO-3166 country code) that have at least 300 photos with GPS coordinates. OpenAI built a balanced dataset with 211 categories, by sampling 200 photos for training and 100 photos for testing, for each country.	https://paperswithcode.com/dataset/country211		Country211					
1130	Hateful Memes	The Hateful Memes data set is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. Images were licensed from Getty Images so that researchers can use the data set to support their work.	https://paperswithcode.com/dataset/hateful-memes	10/05/2020	Hateful Memes					
1131	Rendered SST2	"The Rendered SST2 dataset is an internal OpenAI dataset that measures the optical character recognition capability of visual representations.
It uses sentences from the Stanford Sentiment Treebank dataset and renders them into images, with black texts on a white background, in a
448×448 resolution."	https://paperswithcode.com/dataset/rendered-sst2		Rendered SST2					
1132	AccentDB	AccentDB is a database that contains samples of 4 Indian-English accents, and a compilation of samples from 4 native-English, and a metropolitan Indian-English accent.	https://paperswithcode.com/dataset/accentdb	16/05/2020	AccentDB					
1133	Common Voice	Common Voice is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages.	https://paperswithcode.com/dataset/common-voice	13/12/2019	Common Voice					
1134	CREMA-D	"CREMA-D is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified).
Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).
Participants rated the emotion and emotion levels based on the combined audiovisual presentation, the video alone, and the audio alone. Due to the large number of ratings needed, this effort was crowd-sourced and a total of 2443 participants each rated 90 unique clips, 30 audio, 30 visual, and 30 audio-visual. 95% of the clips have more than 7 ratings."	https://paperswithcode.com/dataset/crema-d		CREMA-D					
1135	DementiaBank	DementiaBank is a shared database of multimedia interactions for the study of communication in dementia. The dataset contains 117 people diagnosed with Alzheimer Disease, and 93 healthy people, reading a description of an image. The principal task and benchmark is to classify each group.	https://paperswithcode.com/dataset/dementiabank		DementiaBank					
1136	FUSS	The Free Universal Sound Separation (FUSS) dataset is a database of arbitrary sound mixtures and source-level references, for use in experiments on arbitrary sound separation. FUSS is based on FSD50K corpus.	https://paperswithcode.com/dataset/fuss	02/11/2020	Free Universal Sound Separation					
1137	Groove	The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming.	https://paperswithcode.com/dataset/groove-midi-dataset	14/05/2019	Groove MIDI Dataset					
1138	GTZAN	"The gtzan8 audio dataset contains 1000 tracks of 30 second length. There are 10 genres, each containing 100 tracks which are all 22050Hz Mono 16-bit audio files in .wav format. The genres are:

blues
classical
country
disco
hiphop
jazz
metal
pop
reggae
rock"	https://paperswithcode.com/dataset/gtzan							
1139	gtzan_music_speech	gtzan_music_speech is a dataset for music/speech discrimination. It consists of 120 tracks of 30 second length. Each class (music/speech) has 60 samples. The tracks are all 22050Hz Mono 16-bit audio files in .wav format.	https://paperswithcode.com/dataset/gtzan-music-speech		gtzan_music_speech					
1140	iVQA	"An open-ended VideoQA benchmark that aims to: i) provide a well-defined evaluation by including five correct answer annotations per question and ii) avoid questions which can be answered without the video. 
iVQA contains 10,000 video clips with one question and five corresponding answers per clip. Moreover, we manually reduce the language bias by excluding questions that could be answered without watching the video.  
Source: Just Ask: Learning to Answer Questions from Millions of Narrated Videos"	https://paperswithcode.com/dataset/ivqa	01/12/2020	Instructional Video Question Answering					
1141	HowTo100M	"HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:

136M video clips with captions sourced from 1.2M Youtube videos (15 years of video)
23k activities from domains such as cooking, hand crafting, personal care, gardening or fitness

Each video is associated with a narration available as subtitles automatically downloaded from Youtube.
Source: HowTo100M"	https://paperswithcode.com/dataset/howto100m	07/06/2019	HowTo100M					
1142	LibriTTS	"LibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus. The main differences from the LibriSpeech corpus are listed below:

The audio files are at 24kHz sampling rate.
The speech is split at sentence breaks.
Both original and normalized texts are included.
Contextual information (e.g., neighbouring sentences) can be extracted.
Utterances with significant background noise are excluded."	https://paperswithcode.com/dataset/libritts	05/04/2019	LibriTTS					
1143	SAVEE	The Surrey Audio-Visual Expressed Emotion (SAVEE) dataset was recorded as a pre-requisite for the development of an automatic emotion recognition system. The database consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically-balanced for each emotion. The data were recorded in a visual media lab with high quality audio-visual equipment, processed and labeled. To check the quality of performance, the recordings were evaluated by 10 subjects under audio, visual and audio-visual conditions. Classification systems were built using standard features and classifiers for each of the audio, visual and audio-visual modalities, and speaker-independent recognition rates of 61%, 65% and 84% achieved respectively.	https://paperswithcode.com/dataset/savee		Surrey Audio-Visual Expressed Emotion					
1144	Speech Commands	Speech Commands is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems.	https://paperswithcode.com/dataset/speech-commands	09/04/2018	Speech Commands					
1145	FSDD	Free Spoken Digit Dataset (FSDD) is a simple audio/speech dataset consisting of recordings of spoken digits in wav files at 8kHz. The recordings are trimmed so that they have near minimal silence at the beginnings and ends. It contains data from 6 speakers, 3,000 recordings (50 of each digit per speaker), and English pronunciations.	https://paperswithcode.com/dataset/fsdd		Free Spoken Digit Dataset					
1146	HowToVQA69M	"A dataset of 69,270,581 video clip, question and answer triplets (v, q, a). HowToVQA69M is two orders of magnitude larger than any of the currently available VideoQA datasets.
On average, each original video results in 43 video clips, where each clip lasts 12.1 seconds and is associated to 1.2 question-answer pairs. Questions and answers contain 8.7 and 2.4 words on average respectively. HowToVQA69M is highly diverse and contains over 16M unique answers, where over 2M unique answers appear more than once and over 300K unique answers appear more than ten times.
Source: Just Ask: Learning to Answer Questions from Millions of Narrated Videos"	https://paperswithcode.com/dataset/sqa69m	01/12/2020	HowToVQA69M					
1147	TED-LIUM 3	"TED-LIUM 3 is an audio dataset collected from TED Talks. It contains:

2351 audio talks in NIST sphere format (SPH), including talks from TED-LIUM 2: be careful, same talks but not same audio files (only these audio file must be used with the TED-LIUM 3 STM files)
452 hours of audio
2351 aligned automatic transcripts in STM format
TEDLIUM 2 dev and test data: 19 TED talks in SPH format with corresponding manual transcriptions (cf. ‘legacy’ distribution below).
Dictionary with pronunciations (159848 entries), same file as the one included in TED-LIUM 2
Selected monolingual data for language modeling from WMT12 publicly available corpora: these files come from the TED-LIUM 2 release, but have been modified to get a tokenization more relevant for English language"	https://paperswithcode.com/dataset/ted-lium-3	12/05/2018	TED-LIUM 3					
1148	Yesno	Yesno is an audio dataset consisting of 60 recordings of one individual saying yes or no in Hebrew; each recording is eight words long. It was created for the Kaldi audio project by an author who wishes to remain anonymous.	https://paperswithcode.com/dataset/yesno		Yesno					
1149	AbstractReasoning	"AbstractReasoning is a dataset for abstract reasoning, where the goal is to infer the correct answer from the context panels based on abstract reasoning.
Image Source: Barrett et al"	https://paperswithcode.com/dataset/abstractreasoning	11/07/2018	AbstractReasoning					
1150	BCCD	BCCD is a small-scale dataset for blood cells detection.	https://paperswithcode.com/dataset/bccd		BCCD					
1151	M-VAD Names	"The dataset contains the annotations of characters' visual appearances, in the form of tracks of face bounding boxes, and the associations with characters' textual mentions, when available. The detection and annotation of the visual appearances of characters in each video clip of each movie was achieved through a semi-automatic approach. The released dataset contains more than 24k annotated video clips, including 63k visual tracks and 34k textual mentions, all associated with their character identities.
Source: M-VAD Names Dataset"	https://paperswithcode.com/dataset/m-vad-names	04/03/2019	M-VAD Names Dataset					
1152	TGIF	"The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques.
Source: GitHub"	https://paperswithcode.com/dataset/tgif	10/04/2016	Tumblr GIF					
1153	TGIF-QA	"The TGIF-QA dataset contains 165K QA pairs for the animated GIFs from the TGIF dataset [Li et al. CVPR 2016]. The question & answer pairs are collected via crowdsourcing with a carefully designed user interface to ensure quality. The dataset can be used to evaluate video-based Visual Question Answering techniques.
Source: GitHub
Image Source: https://github.com/YunseokJANG/tgif-qa"	https://paperswithcode.com/dataset/tgif-qa	14/04/2017	TGIF-QA					
1154	TutorialVQA	"TutorialVQA is a new type of dataset used to find answer spans in tutorial videos. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.
Source: TutorialVQA: Question Answering Dataset for Tutorial Videos"	https://paperswithcode.com/dataset/tutorialvqa	02/12/2019	TutorialVQA					
1155	How2R	"Amazon Mechanical Turk (AMT) is used to collect annotations on HowTo100M videos. 30k 60-second clips are randomly sampled from 9,421 videos and present each clip to the turkers, who are asked to select a video segment containing a single, self-contained scene. After this segment selection step, another group of workers are asked to write descriptions for each displayed segment. Narrations are not provided to the workers to ensure that their written queries are based on visual content only. These final video segments are 10-20 seconds long on average, and the length of queries ranges from 8 to 20 words. From this process, 51,390 queries are collected for 24k 60-second clips from 9,371 videos in HowTo100M, on average 2-3 queries per clip. The video clips and its associated queries are split into 80% train, 10% val and 10% test.
Source: HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"	https://paperswithcode.com/dataset/how2r	01/05/2020	How2R					
1156	How2QA	"To collect How2QA for video QA task, the same set of selected video clips are presented to another group of AMT workers for multichoice QA annotation. Each worker is assigned with one video segment and asked to write one question with four answer candidates (one correctand three distractors). Similarly, narrations are hidden from the workers to ensure the collected QA pairs are not biased by subtitles. Similar to TVQA, the start and end points are provided for the relevant moment for each question. After filtering low-quality annotations, the final dataset contains 44,007 QA pairs for 22k 60-second clips selected from 9035 videos.
Source: HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training"	https://paperswithcode.com/dataset/how2qa	01/05/2020	How2QA					
1157	CLIC	CLIC is a dataset for learned image compression. The dataset contains both RGB and grayscale images.	https://paperswithcode.com/dataset/clic		Challenge on Learned Image Compression					
1158	3DMatch	"The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. 
The pixel size of each 2D patch is determined by the projection of the 0.3m3 local 3D patch around the interest point onto the image plane. 
Source: 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions"	https://paperswithcode.com/dataset/3dmatch							
1159	BSD100		https://paperswithcode.com/dataset/bsd100							
1160	CIFAR-FS	"CIFAR100 few-shots (CIFAR-FS) is randomly sampled from CIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has been generated. The average inter-class similarity is sufficiently high to represent a challenge for the current state of the art. Moreover, the limited original resolution of 32×32 makes the task harder and at the same time allows fast prototyping.
Source: Bertinetto et al.
Image source: Bertinetto et al."	https://paperswithcode.com/dataset/cifar-fs	21/05/2018	CIFAR100 few-shots					
1161	Cornell		https://paperswithcode.com/dataset/cornell							
1162	Google		https://paperswithcode.com/dataset/google							
1163	PyBullet	PyBullet is an easy to use Python module for physics simulation, robotics and deep reinforcement learning based on the Bullet Physics SDK. With PyBullet you can load articulated bodies from URDF, SDF and other file formats. PyBullet provides forward dynamics simulation, inverse dynamics computation, forward and inverse kinematics and collision detection and ray intersection queries. Aside from physics simulation, PyBullet supports to rendering, with a CPU renderer and OpenGL visualization and support for virtual reality headsets.	https://paperswithcode.com/dataset/pybullet							
1164	SemEval 2013		https://paperswithcode.com/dataset/semeval-2013							
1165	ELI5	"ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question.
ELI5 is also a task in Dodecadialogue.
Source: ELI5
Image Source: https://arxiv.org/pdf/1907.09190v1.pdf"	https://paperswithcode.com/dataset/eli5	22/07/2019	ELI5					
1166	PixelHelp	"PixelHelp includes 187 multi-step instructions of 4 task categories deined in https://support.google.com/pixelphone and annotated by human. This dataset includes 88 general tasks, such as configuring accounts, 38 Gmail tasks, 31 Chrome tasks, and 30 Photos related tasks. This dataset is an updated opensource version of the original PixelHelp dataset, which was used for testing the end-to-end grounding quality of the model in paper ""Mapping Natural Language Instructions to Mobile UI Action Sequences"". The similar accuracy is acquired on this version of the dataset.
Source: PixelHelp"	https://paperswithcode.com/dataset/pixelhelp	07/05/2020	PixelHelp					
1167	RicoSCA	"Rico is a public UI corpus with 72K Android UI screens mined from 9.7K Android apps (Deka et al., 2017). Each screen in Rico comes with a screenshot image and a view hierarchy of a collection of UI objects. Authors manually removed screens whose view hierarchies do not match their screenshots by asking annotators to visually verify whether the bounding boxes of view hierarchy leaves match each UI object on the corresponding screenshot image. This filtering results in 25K unique screens.
In total, RICOSCA contains 295,476 single-step synthetic commands for operating 177,962 different target objects across 25,677 Android screens.
Source: Google Research"	https://paperswithcode.com/dataset/ricosca	07/05/2020	RicoSCA					
1168	AndroidHowTo	"AndroidHowTo contains 32,436 data points from 9,893 unique How-To instructions and split into training (8K), validation (1K) and test (900). All test examples have perfect agreement across all three annotators for the entire sequence. In total, there are 190K operation spans, 172K object spans, and 321 input spans labeled. The lengths of the instructions range from 19 to 85 tokens, with median of 59. They describe a sequence of actions from one to 19 steps, with a median of 5.
Source: Google Research"	https://paperswithcode.com/dataset/androidhowto	07/05/2020	AndroidHowTo					
1169	MIND	"MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website. The mission of MIND is to serve as a benchmark dataset for news recommendation and facilitate the research in news recommendation and recommender systems area.
MIND contains about 160k English news articles and more than 15 million impression logs generated by 1 million users. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression. To protect user privacy, each user was de-linked from the production system when securely hashed into an anonymized ID.
Source: MIND"	https://paperswithcode.com/dataset/mind	01/07/2020	MIcrosoft News Dataset					
1170	MLQA	"MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average.
Source: Facebook Research
Image Source: https://github.com/facebookresearch/mlqa"	https://paperswithcode.com/dataset/mlqa	16/10/2019	MultiLingual Question Answering					
1171	KdConv	"KdConv is a Chinese multi-domain Knowledge-driven Conversation dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics, while the corpus can also used for exploration of transfer learning and domain adaptation.
Source: KdConv"	https://paperswithcode.com/dataset/kdconv	08/04/2020	Knowledge-driven Conversation					
1172	STACKEX	"STACKEX expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. 
Source: One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases"	https://paperswithcode.com/dataset/stackex	11/10/2018	STACKEX					
1173	SciREX	"SCIREX is a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. The dataset is annotated by integrating automatic and human annotations, leveraging existing scientific knowledge resources.
Source: SCIREX: A Challenge Dataset for Document-Level Information Extraction"	https://paperswithcode.com/dataset/scirex	01/05/2020	SciREX					
1174	CH-SIMS	"CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.
Source: CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality"	https://paperswithcode.com/dataset/ch-sims	01/07/2020	CH-SIMS					
1175	WCEP	"The WCEP dataset for multi-document summarization (MDS) consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the Common Crawl News dataset. 
Source: WCEP"	https://paperswithcode.com/dataset/wcep	20/05/2020	Wikipedia Current Events Portal					
1176	MATINF	"Maternal and Infant (MATINF) Dataset is a large-scale dataset jointly labeled for classification, question answering and summarization in the domain of maternity and baby caring in Chinese. An entry in the dataset includes four fields: question (Q), description (D), class (C) and answer (A).
Nearly two million question-answer pairs are collected with fine-grained human-labeled classes from a large Chinese maternity and baby caring QA site. Authors conduct both automatic and manual data cleansing and remove: (1) classes with insufficient samples; (2) entries in which the length of the description filed is less than the length of the question field; (3) data with any field longer than 256 characters; (4) human-spotted ill-formed data. After the data cleansing, MATINF is constructed with the remaining 1.07 million entries
Source: MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization"	https://paperswithcode.com/dataset/matinf	26/04/2020	Maternal and Infant Dataset					
1177	FOBIE	"The Focused Open Biology Information Extraction (FOBIE) dataset aims to support IE from Computer-Aided Biomimetics. The dataset contains ~1,500 sentences from scientific biological texts. These sentences are annotated with TRADE-OFFS and syntactically similar relations between unbounded arguments, as well as argument-modifiers.
The FOBIE dataset has been used to explore Semi-Open Relation Extraction (SORE). The code for this and instructions can be found inside the SORE folder Readme.md, or in the ReadTheDocs documentations.
Source: FOBIE"	https://paperswithcode.com/dataset/fobie	15/05/2020	Focused Open Biological Information Extraction					
1178	CODA-19	"CODA-19 is a human-annotated dataset that denotes the Background, Purpose, Method, Finding/Contribution, and Other for 10,966 English abstracts in the COVID-19 Open Research Dataset.
CODA-19 was created by 248 crowd workers from Amazon Mechanical Turk collectively within ten days. Each abstract was annotated by nine different workers, and the final labels were obtained by majority voting.
CODA-19's labels have an accuracy of 82% and an inter-annotator agreement (Cohen's kappa) of 0.74 when compared against expert labels on 129 abstracts.
Source: CODA-19"	https://paperswithcode.com/dataset/coda-19	05/05/2020	CODA-19					
1179	RWWD	"Real World Worry Dataset (RWWD) captures the emotional responses of UK residents to COVID-19 at a point in time where the impact of the COVID19 situation affected the lives of all individuals in the UK. The data were collected on the 6th and 7th of April 2020, a time at which the UK was under lockdown (news, 2020), and death tolls were increasing. On April 6, 5,373 people in the UK had died of the virus, and 51,608 tested positive. On the day before data collection, the Queen addressed the nation via a television broadcast. Furthermore, it was also announced that Prime Minister Boris Johnson was admitted to intensive care in a hospital for COVID-19 symptoms.
The RWWD is a ground truth dataset that used a direct survey method and obtained written accounts of people alongside data of their felt emotions while writing. As such, the dataset does not rely on third-person annotation but can resort to direct self-reported emotions. Two versions of RWWD are presented, each consisting of 2,500 English
texts representing the participants’ genuine emotional responses to Corona situation in the UK: the Long RWWD consists of texts that were openended in length and asked the participants to express their feelings as they wish. The Short RWWD asked the same people also to express their feelings in Tweet-sized texts. The latter was chosen
to facilitate the use of this dataset for Twitter data research.
Source: Measuring Emotions in the COVID-19 Real World Worry Dataset"	https://paperswithcode.com/dataset/rwwd	08/04/2020	Real World Worry Dataset					
1180	COVID-Q	"COVID-Q consists of COVID-19 questions which have been annotated into a broad category (e.g. Transmission, Prevention) and a more specific class such that questions in the same class are all asking the same thing.
Source: COVID-Q"	https://paperswithcode.com/dataset/covid-q	26/05/2020						
1181	WT-WT	"Will-They-Won't-They (WT-WT) is a large dataset of English tweets targeted at stance detection for the rumor verification task. The dataset is constructed based on tweets that discuss five recent merger and acquisition (M&A) operations of US companies, mainly from the healthcare sector.
All the annotations are carried out by domain experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection.
Source: WT-WT"	https://paperswithcode.com/dataset/wt-wt	01/05/2020	Will-They-Won't-They					
1182	iSarcasm	"iSarcasm is a dataset of tweets, each labelled as either sarcastic or non_sarcastic. Each sarcastic tweet is further labelled for one of the following types of ironic speech:

sarcasm: tweets that contradict the state of affairs and are critical towards an addressee;
irony: tweets that contradict the state of affairs but are not obviously critical towards an addressee;
satire: tweets that appear to support an addressee, but contain underlying disagreement and mocking;
understatement: tweets that undermine the importance of the state of affairs they refer to;
overstatement: tweets that describe the state of affairs in obviously exaggerated terms;
rhetorical question: tweets that include a question whose invited inference (implicature) is obviously contradicting the state of affairs.

For each sarastic tweet, there's also:

an explanation, in English sentences, as to why it is sarcastic, and
a rephrase that conveys the same meaning non-sarcastically. Both have been provided by the author of the tweet.

iSarcasm contains 4,484 tweets, out of which 777 are labelled as sarcastic and 3,707 as non-sarcastic. You'll find two files, isarcasm_train.csv and isarcasm_test.csv, each containing 80% and 20% of the examples chosen at random, respectively. Each line in a file has the format tweet_id,sarcasm_label,sarcasm_type, where sarcasm_type are only defined for sarcastic tweets, as specified above.
Source: iSarcasm"	https://paperswithcode.com/dataset/isarcasm	08/11/2019	iSarcasm					
1183	KLEJ	"The KLEJ benchmark (Kompleksowa Lista Ewaluacji Językowych) is a set of nine evaluation tasks for the Polish language understanding task.
Key benchmark features:

It contains a diverse set of tasks from different domains and with different objectives.
Most tasks are created from existing datasets but the authors also released the new sentiment analysis dataset from an e-commerce domain.
It includes tasks which have relatively small datasets and require extensive external knowledge to solve them. It promotes the usage of transfer learning instead of training separate models from scratch.

The name KLEJ (English: GLUE) is an abbreviation for Kompleksowa Lista Ewaluacji Językowych (English: Comprehensive List of Language Evaluations) and refers to the GLUE benchmark.
Source: KLEJ"	https://paperswithcode.com/dataset/klej	01/05/2020						
1184	XQuAD	"XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages.
Source: XQuAD
Image Source: https://arxiv.org/pdf/1910.11856v3.pdf"	https://paperswithcode.com/dataset/xquad	25/10/2019	XQuAD					
1185	Microsoft Research Multimodal Aligned Recipe Corpus	"To construct the MICROSOFT RESEARCH MULTIMODAL ALIGNED RECIPE CORPUS the authors first extract a large number of text and video recipes from the web. The goal is to find joint alignments between multiple text recipes and multiple video recipes for the same dish. The task is challenging, as different recipes vary in their order of instructions and use of ingredients. Moreover, video instructions can be noisy, and text and video instructions include different levels of specificity in their descriptions.
Source: A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks"	https://paperswithcode.com/dataset/microsoft-research-multimodal-aligned-recipe	19/05/2020	Microsoft Research Multimodal Aligned Recipe Corpus					
1186	ClarQ	"ClarQ, consists of ∼2M examples distributed across 173 domains of stackexchange. This dataset is meant for training and evaluation of Clarification Question Generation Systems.
Source: ClarQ: A large-scale and diverse dataset for Clarification Question Generation"	https://paperswithcode.com/dataset/clarq	10/06/2020	ClarQ					
1187	TechQA	"TECHQA is a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size – 600 training, 310 dev, and 490 evaluation question/answer pairs – thus reflecting the cost of creating large labeled datasets with actual data. Consequently, TECHQA is meant to stimulate research in domain adaptation rather than being a resource to build QA systems from scratch. The dataset was obtained by crawling the IBM Developer and IBM DeveloperWorks forums for questions with accepted answers that appear in a published IBM Technote—a technical document that addresses a specific technical issue.
Source: The TechQA Dataset"	https://paperswithcode.com/dataset/techqa	08/11/2019	The TechQA Dataset					
1188	Refer360°	"Refer360° is a novel large-scale referring expression recognition dataset consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes.
Source: Refer360° : A Referring Expression Recognition Dataset in 360° Images"	https://paperswithcode.com/dataset/refer360deg	01/07/2020	Refer360°					
1189	MUStARD	"We release the MUStARD dataset which is a multimodal video corpus for research in automated sarcasm discovery. The dataset is compiled from popular TV shows including Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics Anonymous. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context, which provides additional information on the scenario where the utterance occurs.
Source: MUStARD"	https://paperswithcode.com/dataset/mustard	01/07/2019	Multimodal Sarcasm Detection Dataset					
1190	ChID	"ChID is a large-scale Chinese IDiom dataset for cloze test. ChID contains 581K passages and 729K blanks, and covers multiple domains. In ChID, the idioms in a passage were replaced with blank symbols. For each blank, a list of candidate idioms including the golden idiom are provided as choice. 
Source: ChID: A Large-scale Chinese IDiom Dataset for Cloze Test
Image Source: https://arxiv.org/pdf/1906.01265v3.pdf"	https://paperswithcode.com/dataset/chid	04/06/2019	Chinese IDiom dataset					
1191	XQA	"XQA is a data which consists of a total amount of 90k question-answer pairs in nine languages for cross-lingual open-domain question answering.
Source: XQA: A Cross-lingual Open-domain Question Answering Dataset"	https://paperswithcode.com/dataset/xqa	01/07/2019	XQA					
1192	TalkSumm	"The TalkSumm dataset contains 1705 automatically-generated summaries of scientific papers from ACL, NAACL, EMNLP, SIGDIAL (2015-2018), and ICML (2017-2018).
The dataset is provided as a list of titles and URLs and the corresponding summaries.
Source: GitHub"	https://paperswithcode.com/dataset/talksumm	04/06/2019	TalkSumm					
1193	CONAN	"COunter NArratives through Nichesourcing (CONAN) is a dataset that consists of 4,078 pairs over the 3 languages. Additionally, 3 types of metadata are provided: expert demographics, hate speech sub-topic and counter-narrative type. The dataset is augmented through translation (from Italian/French to English) and paraphrasing, which brought the total number of pairs to 14.988.
Source: CONAN
Image Source: https://www.aclweb.org/anthology/P19-1271"	https://paperswithcode.com/dataset/conan	01/07/2019	COunter NArratives through Nichesourcing					
1194	VIST-Edit	"The dataset, VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions.
Source: Visual Story Post-Editing"	https://paperswithcode.com/dataset/vist-edit	05/06/2019	VIST-Edit					
1195	OQGend	"Dataset OQRanD and OQGenD for paper ""Asking the crowd: Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums"" by Zi Chai, Xinyu Xing, Xiaojun Wan and Bo Huang. This paper is accepted by ACL'19.
The OQGenD dataset can be viewed at ""OQGenD.xml"". Each data (NQ-pairs) contains a certain piece of news with multiple related open-answered questions.
The OQRanD dataset can be viewed at ""OQRanD.xml"". Each data (Question Pairs) contains two questions, Q2 has more answers than Q1.
Source: GitHub"	https://paperswithcode.com/dataset/oqgend	01/07/2019	OQGend					
1196	OQRanD	"Dataset OQRanD and OQGenD for paper ""Asking the crowd: Asking the Crowd: Question Analysis, Evaluation and Generation for Open Discussion on Online Forums"" by Zi Chai, Xinyu Xing, Xiaojun Wan and Bo Huang. This paper is accepted by ACL'19.
The OQGenD dataset can be viewed at ""OQGenD.xml"". Each data (NQ-pairs) contains a certain piece of news with multiple related open-answered questions.
The OQRanD dataset can be viewed at ""OQRanD.xml"". Each data (Question Pairs) contains two questions, Q2 has more answers than Q1.
Source: GitHub"	https://paperswithcode.com/dataset/oqrand	01/07/2019	OQRanD					
1197	PAWS	"Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset.
Source: PAWS"	https://paperswithcode.com/dataset/paws	01/04/2019	Paraphrase Adversaries from Word Scrambling					
1198	LitBank	"LitBank is an annotated dataset of 100 works of English-language fiction to support tasks in natural language processing and the computational humanities, described in more detail in the following publications:

David Bamman, Sejal Popat and Sheng Shen (2019), ""An Annotated Dataset of Literary Entities,"" NAACL 2019.
Matthew Sims, Jong Ho Park and David Bamman (2019), ""Literary Event Detection,"" ACL 2019.
David Bamman, Olivia Lewke and Anya Mansoor (2020), ""An Annotated Dataset of Coreference in English Literature"", LREC.

LitBank currently contains annotations for entities, events, entity coreference, and quotation attribution in a sample of ~2,000 words from each of those texts, totaling 210,532 tokens.
LitBank is licensed under a Creative Commons Attribution 4.0 International License."	https://paperswithcode.com/dataset/litbank	01/06/2019	LitBank					
1199	Discovery Dataset	"The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus.
Markers prediction can be used in order to train a sentence encoders. Discourse markers can be considered as noisy labels for various semantic tasks, such as entailment (y=therefore), subjectivity analysis (y=personally) or sentiment analysis (y=sadly), similarity (y=similarly), typicality, (y=curiously) ...
The specificity of this dataset is the diversity of the markers, since previously used data used only ~10 imbalanced classes. The author of the dataset provide:

a list of the 174 discourse markers
a Base version of the dataset with 1.74 million pairs (10k examples per marker)
a Big version with 3.4 million pairs
a Hard version with 1.74 million pairs where the connective couldn't be predicted with a fastText linear model

Source: GitHub"	https://paperswithcode.com/dataset/discovery	28/03/2019						
1200	CLEVR-Dialog	"CLEVR-Dialog is a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, that authors construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs.
The CLEVR-Dialog is used to benchmark performance of standard visual dialog models; in particular, on visual coreference resolution (as a function of the coreference distance). This is the first analysis of its kind for visual dialog models that was not possible without this dataset. 
CLEVR-Dialog is aims to help inform the development of future models for visual dialog.
Source: CLEVR-Dialog"	https://paperswithcode.com/dataset/clevr-dialog	07/03/2019	CLEVR-Dialog					
1201	MultiSense	"MultiSense is a dataset of 9,504 images annotated with an English verb and its translation in Spanish and German.
Source: Cross-lingual Visual Verb Sense Disambiguation"	https://paperswithcode.com/dataset/multisense	10/04/2019	MultiSense					
1202	SciQ	"The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/sciq	19/07/2017	SciQ					
1203	MedHop	"With the same format as WikiHop, the MedHop dataset is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins.
Source: QAngaroo"	https://paperswithcode.com/dataset/medhop	17/10/2017	MedHop					
1204	NEWSROOM	"CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction.
Source: CORNELL NEWSROOM"	https://paperswithcode.com/dataset/newsroom	30/04/2018	CORNELL NEWSROOM					
1205	ListOps	"The ListOps examples are comprised of summary operations on lists of single digit integers, written in prefix notation. The full sequence has a corresponding solution which is
also a single-digit integer, thus making it a ten-way balanced classification problem. For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9. Each operation has a corresponding closing square bracket that defines the list of numbers for the operation. In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}. 
Source: ListOps: A Diagnostic Dataset for Latent Tree Learning"	https://paperswithcode.com/dataset/listops	17/04/2018	ListOps					
1206	DuoRC	"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.
Why another RC dataset?
DuoRC pushes the NLP community to address challenges on incorporating knowledge and reasoning in neural architectures for reading comprehension. It poses several interesting challenges such as:

DuoRC using parallel plots is especially designed to contain a large number of questions with low lexical overlap between questions and their corresponding passages
It requires models to go beyond the content of the given passage itself and incorporate world-knowledge, background knowledge, and common-sense knowledge to arrive at the answer
It revolves around narrative passages from movie plots describing complex events and therefore naturally require complex reasoning (e.g. temporal reasoning, entailment, long-distance anaphoras, etc.) across multiple sentences to infer the answer to questions
Several of the questions in DuoRC, while seeming relevant, cannot actually be answered from the given passage. This requires the model to detect the unanswerability of questions. This aspect is important for machines to achieve in industrial settings in particular

Source: DuoRC"	https://paperswithcode.com/dataset/duorc	21/04/2018	DuoRC					
1207	PAWS-X	"PAWS-X contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki.
Source: Google Research
Image Source: https://arxiv.org/pdf/1908.11828v1.pdf"	https://paperswithcode.com/dataset/paws-x	30/08/2019	PAWS-X					
1208	KnowledgeNet	"KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction).
For instance, the dataset contains text expressing the fact (Gennaro Basile; RESIDENCE; Moravia), in the passage: ""Gennaro Basile was an Italian painter, born in Naples but active in the German-speaking countries. He settled at Brünn, in Moravia, and lived about 1756...""
Source: KnowledgeNet"	https://paperswithcode.com/dataset/knowledgenet	01/11/2019	KnowledgeNet					
1209	CLINC150	"This dataset is for evaluating the performance of intent classification systems in the presence of ""out-of-scope"" queries, i.e., queries that do not fall into any of the system-supported intent classes. The dataset includes both in-scope and out-of-scope data.
Source: CLINC150"	https://paperswithcode.com/dataset/clinc150	04/09/2019	CLINC150					
1210	WikiCREM	"An unsupervised dataset for co-reference resolution. Presented in the publication: Kocijan et. al, WikiCREM: A Large Unsupervised Corpus for Coreference Resolution, presented at EMNLP 2019.
Source: WikiCREM
Image Source: https://arxiv.org/pdf/1908.08025v3.pdf"	https://paperswithcode.com/dataset/wikicrem	21/08/2019	WikiCREM					
1211	BiPaR	"BiPaR is a manually annotated bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support monolingual, multilingual and cross-lingual reading comprehension on novels. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written in parallel in two languages. BiPaR is diverse in prefixes of questions, answer types and relationships between questions and passages. Answering the questions requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality.
Source: BiPaR"	https://paperswithcode.com/dataset/bipar	11/10/2019	BiPaR					
1212	PASTEL	"PASTEL is a parallelly annotated stylistic language dataset. The dataset consists of ~41K parallel sentences and 8.3K parallel stories annotated across different personas.
Source: PASTEL"	https://paperswithcode.com/dataset/pastel	31/08/2019	PASTEL					
1213	PubMedQA	"The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts.
PubMedQA has 1k expert labeled, 61.2k unlabeled and 211.3k artificially generated QA instances.
Source: PubMedQA
Image Source: https://arxiv.org/pdf/1909.06146v1.pdf"	https://paperswithcode.com/dataset/pubmedqa	13/09/2019	PubMedQA					
1214	JuICe	"JuICe is a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data.
Source: JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"	https://paperswithcode.com/dataset/juice	05/10/2019	JuICe Dataset					
1215	VisPro	"VisPro dataset contains coreference annotation of 29,722 pronouns from 5,000 dialogues.
Source: VisPro"	https://paperswithcode.com/dataset/vispro	01/09/2019	VisPro					
1216	RUN	"The RUN dataset  is based on OpenStreetMap (OSM). The map contains rich layers and an abundance of entities of different types. Each entity is complex and can contain (at least) four labels: name, type, is building=y/n, and house number. An entity can spread over several tiles. As the maps do not overlap, only very few entities are shared among them. The RUN dataset aligns NL navigation instructions to coordinates of their corresponding route on the OSM map.
Source: RUN"	https://paperswithcode.com/dataset/run	19/09/2019	The RUN Dataset					
1217	CrossWOZ	"CrossWOZ is the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides.
Source: CrossWOZ"	https://paperswithcode.com/dataset/crosswoz	27/02/2020	CrossWOZ					
1218	TyDi QA	"TyDi QA is a question answering dataset covering 11 typologically diverse languages with 200K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology — the set of linguistic features that each language expresses — such that the authors expect models performing well on this set to generalize across a large number of the languages in the world.
Source: Google Research"	https://paperswithcode.com/dataset/tydi-qa	10/03/2020	Typologically Diverse Question Answering					
1219	BLiMP	"BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars. Aggregate human agreement with the labels is 96.4%.
Source: BLiMP 
Image Source: https://arxiv.org/pdf/1912.00582v3.pdf"	https://paperswithcode.com/dataset/blimp	02/12/2019	Benchmark of Linguistic Minimal Pairs					
1220	BREAK	"Break is a question understanding dataset, aimed at training models to reason over complex questions. It features 83,978 natural language questions, annotated with a new meaning representation, Question Decomposition Meaning Representation (QDMR). Each example has the natural question along with its QDMR representation. Break contains human composed questions, sampled from 10 leading question-answering benchmarks over text, images and databases. This dataset was created by a team of NLP researchers at Tel Aviv University and Allen Institute for AI.
Source: BREAK"	https://paperswithcode.com/dataset/break	31/01/2020	BREAK					
1221	OLPBENCH	"OLPBENCH is a large Open Link Prediction benchmark, which was derived from the state-of-the-art Open Information Extraction corpus OPIEC (Gashteovski et al., 2019). OLPBENCH contains 30M open triples, 1M distinct open relations and 2.5M distinct mentions of approximately 800K entities. 
Open Link Prediction is defined as follows: Given an Open Knowledge Graph and a question consisting of an entity mention and an open relation, predict mentions as answers. A predicted mention is correct if it is a mention of the correct answer entity. For example, given the question (“NBC-TV”, “has office in”, ?), correct answers include “NYC” and “New York”.
Source: OLPBENCH"	https://paperswithcode.com/dataset/olpbench	01/07/2020	OLPBENCH					
1222	LIAR	"LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM’s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness. 
Source: “Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection"	https://paperswithcode.com/dataset/liar	01/05/2017	LIAR					
1223	STAIR Captions	"STAIR Captions is a large-scale dataset containing 820,310 Japanese captions.
This dataset can be used for caption generation, multimodal retrieval, and image generation.
Source: STAIR Captions"	https://paperswithcode.com/dataset/stair-captions	02/05/2017	STAIR Captions					
1224	BillSum	"BillSum is the first dataset for summarization of US Congressional and California state bills.
The BillSum dataset consists of three parts: US training bills, US test bills and California test bills. The US bills were collected from the Govinfo service provided by the United States Government Publishing Office (GPO). The corpus consists of bills from the 103rd-115th (1993-2018) sessions of Congress. The data was split into 18,949 train bills and 3,269 test bills. For California, bills from the 2015-2016 session were scraped directly from the legislature’s website; the summaries were written by their Legislative Counsel.
The BillSum corpus focuses on mid-length legislation from 5,000 to 20,000 character in length. The authors chose to measure the text length in characters, instead of words or sentences, because the texts have complex structure that makes it difficult to consistently measure words. The range was chosen because on one side, short bills introduce minor changes and do not require summaries. While the CRS produces summaries for them, they often contain most of the text of the bill. On the
other side, very long legislation is often composed of several large sections.
Source: BillSum: A Corpus for Automatic Summarization of US Legislation"	https://paperswithcode.com/dataset/billsum	01/10/2019	BillSum					
1225	Business Scene Dialogue	"The Japanese-English business conversation corpus, namely Business Scene Dialogue corpus, was constructed in 3 steps:

selecting business scenes,
writing monolingual conversation scenarios according to the selected scenes, and
translating the scenarios into the other language.

Half of the monolingual scenarios were written in Japanese and the other half were written in English. The whole construction process was supervised by a person who satisfies the following conditions to guarantee the conversations to be natural:

has the experience of being engaged in language learning programs, especially for business conversations
is able to smoothly communicate with others in various business scenes both in Japanese and English
has the experience of being involved in business

The BSD corpus is split into balanced training, development and evaluation sets. The documents in these sets are balanced in terms of scenes and original languages. In this repository we publicly share the full development and evaluation sets and a part of the training data set.
Source: BSD"	https://paperswithcode.com/dataset/business-scene-dialogue	05/08/2020						
1226	X-WikiRE	"X-WikiRE is a new, large-scale multilingual relation extraction dataset in which relation extraction is framed as a problem of reading comprehension to allow for generalization to unseen relations. 
Source: X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension"	https://paperswithcode.com/dataset/x-wikire	14/08/2019	X-WikiRE					
1227	ProofWriter	"The ProofWriter dataset contains many small rulebases of facts and rules, expressed in English. Each rulebase also has a set of questions (English statements) which can either be proven true or false using proofs of various depths, or the answer is “Unknown” (in open-world setting, OWA) or assumed negative (in closed-world setting, CWA).
The dataset includes full proofs with intermediate conclusions, which models can try to reproduce.
The dataset supports various tasks:

Given rulebase + question, what is answer + proof (w/intermediates)?
Given rulebase, what are all the provable implications?
Given rulebase + question without proof, what single fact can be added to make the question true?

Source: Allen AI"	https://paperswithcode.com/dataset/proofwriter	24/12/2020	ProofWriter					
1228	Open PI	"Open PI is the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. The dataset comprises 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com.
The state tracking task assumes new formulation in which just the text is provided, from which a set of state changes (entity, attribute, before, after) is generated for each step, where the entity, attribute, and values must all be predicted from an open vocabulary.
Source: Allen Institute of AI"	https://paperswithcode.com/dataset/open-pi	31/10/2020	Open PI					
1229	hasPart KB	"This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old’s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/haspart-kb	12/06/2020	hasPart KB					
1230	SciDocs	"SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/scidocs	15/04/2020	SciDocs					
1231	GenericsKB	"The GenericsKB contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as ""Dogs bark,"" and ""Trees remove carbon dioxide from the atmosphere."" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/genericskb	02/05/2020	GenericsKB					
1232	CORD-19	"CORD-19 is a free resource of tens of thousands of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses for use by the global research community.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/cord-19	22/04/2020	CORD-19					
1233	Quoref	"Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/quoref	16/08/2019	Quoref					
1234	ROPES	"ROPES is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s), a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the back-ground passage in the context of the situation.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/ropes	16/08/2019	Reasoning Over Paragraph Effects in Situations					
1235	QASC	"QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/qasc	25/10/2019	Question Answering via Sentence Composition					
1236	QuaRTz	"QuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).
The QuaRTz dataset V1 contains 3864 questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).
The dataset is split into train (2696), dev (384) and test (784). A background sentence will only appear in a single split.
Each line in a dataset file is a question specified as a json object, e.g., (with extra whitespace for readability).
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/quartz	08/09/2019	QuaRTz Dataset					
1237	WIQA	"The WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph. The dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/wiqa	10/09/2019	What-If Question Answering					
1238	QuaRel	"QuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/quarel	20/11/2018	QuaRel					
1239	ProPara	"The ProPara dataset is designed to train and test comprehension of simple paragraphs describing processes (e.g., photosynthesis), designed for the task of predicting, tracking, and answering questions about how entities change during the process.
ProPara aims to promote the research in natural language understanding in the context of procedural text. This requires identifying the actions described in the paragraph and tracking state changes happening to the entities involved. The comprehension task is treated as that of predicting, tracking, and answering questions about how entities change during the procedure. The dataset contains 488 paragraphs and 3,300 sentences. Each paragraph is richly annotated with the existence and locations of all the main entities (the “participants”) at every time step (sentence) throughout the procedure (~81,000 annotations).
ProPara paragraphs are natural (authored by crowdsourcing) rather than synthetic (e.g., in bAbI). Workers were given a prompt (e.g., “What happens during photosynthesis?”) and then asked to author a series of sentences describing the sequence of events in the procedure. From these sentences, participant entities and their existence and locations were identified. The goal of the challenge is to predict the existence and location of each participant, based on sentences in the paragraph.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/propara	17/05/2018						
1240	ComplexWebQuestions	"ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways:

By interacting with a search engine;
As a reading comprehension task: the authors release 12,725,989 web snippets that are relevant for the questions, and were collected during the development of their model;
As a semantic parsing task: each question is paired with a SPARQL query that can be executed against Freebase to retrieve the answer.

Source: Allen Institute for AI
Image Source: Talmor et al"	https://paperswithcode.com/dataset/complexwebquestions	18/03/2018	ComplexWebQuestions					
1241	ScienceExamCER	"ScienceExamCER is a collection of resources for studying explanation-centered inference, including explanation graphs for 1,680 questions, with 4,950 tablestore rows, and other analyses of the knowledge required to answer elementary and middle-school science questions.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/scienceexamcer	24/11/2019						
1242	TupleInf Open IE Dataset	"The TupleInf Open IE dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in “Answering Complex Questions Using Open Information Extraction” (referred as Tuple KB, T). These sentences were collected from a large Web corpus using training questions from 4th and 8th grade as queries. This dataset contains 156K sentences collected for 4th grade questions and 107K sentences for 8th grade questions. Each sentence is followed by the Open IE v4 tuples using their simple format.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/tupleinf-open-ie-dataset	19/04/2017	TupleInf Open IE Dataset					
1243	TQA	"The TextbookQuestionAnswering (TQA) dataset is drawn from middle school science curricula. It consists of 1,076 lessons from Life Science, Earth Science and Physical Science textbooks. This includes 26,260 questions, including 12,567 that have an accompanying diagram.
The TQA dataset encourages work on the task of Multi-Modal Machine Comprehension (M3C) task. The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images. The dataset constructed to showcase this task has been built from a middle school science curriculum that pairs a given question to a limited span of knowledge needed to answer it.
Source: Allen Institute for AI"	https://paperswithcode.com/dataset/tqa	01/07/2017	Textbook Question Answering					
1244	Countix	"Countix is a real world dataset of repetition videos collected in the wild (i.e.YouTube) covering a wide range of semantic settings with significant challenges such as camera and object motion, diverse set of periods and counts, and changes in the speed of repeated actions. Countix include repeated videos of workout activities (squats, pull ups, battle rope training, exercising arm), dance moves (pirouetting, pumping fist), playing instruments (playing ukulele), using tools repeatedly (hammer hitting objects, chainsaw cutting wood, slicing onion), artistic performances (hula hooping, juggling soccer ball), sports (playing ping pong and tennis) and many others. Figure 6 illustrates some examples from the dataset as well as the distribution of repetition counts and period lengths.
Source: Counting Out Time: Class Agnostic Video Repetition Counting in the Wild"	https://paperswithcode.com/dataset/countix	27/06/2020	Countix					
1245	RL Unplugged	"RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, the datasets are provided with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning.
In this suite of benchmarks, the authors try to focus on the following problems:

High dimensional action spaces, for example the locomotion humanoid domains, there are 56 dimensional actions.
High dimensional observations.
Partial observability, observations have egocentric vision.
Difficulty of exploration, using states of the art algorithms and imitation to generate data for difficult environments.
Real world challenges.

Source: DeepMind"	https://paperswithcode.com/dataset/rl-unplugged	01/12/2020	RL Unplugged					
1246	MineRL	"MineRLis an imitation learning dataset with over 60 million frames of recorded human player data. The dataset includes a set of tasks which highlights many of the hardest problems in modern-day Reinforcement Learning: sparse rewards and hierarchical policies.
Source: MineRL"	https://paperswithcode.com/dataset/minerl	22/04/2019	MineRL					
1247	Mathematics Dataset	"This dataset code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models.
Source: DeepMind"	https://paperswithcode.com/dataset/mathematics	02/04/2019						
1248	PGM	"PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.
Source: Measuring abstract reasoning in neural networks
Image Source: Barrett et al"	https://paperswithcode.com/dataset/pgm	11/07/2018	Procedurally Generated Matrices (PGM)					
1249	Slim	"This dataset consists of virtual scenes rendered in MuJoCo with multiple views each presented in multiple modalities: image, and synthetic or natural language descriptions. Each scene consists of two or three objects placed on a square walled room, and for each of the 10 camera viewpoint the authors rendered a 3D view of the scene as seen from that viewpoint as well as a synthetically generated description of the scene.
Source: GitHub"	https://paperswithcode.com/dataset/slim	04/07/2018	Slim					
1250	TableBank	"To address the need for a standard open domain table benchmark dataset, the author propose a novel weak supervision approach to automatically create the TableBank, which is orders of magnitude larger than existing human labeled datasets for table analysis. Distinct from traditional weakly supervised training set, our approach can obtain not only large scale but also high quality training data.
Nowadays, there are a great number of electronic documents on the web such as Microsoft Word (.docx) and Latex (.tex) files. These online documents contain mark-up tags for tables in their source code by nature. Intuitively, one can manipulate these source code by adding bounding box using the mark-up language within each document. For Word documents, the internal Office XML code can be modified where the borderline of each table is identified. For Latex documents, the tex code can be also modified where bounding boxes of tables are recognized. In this way, high-quality labeled data is created for a variety of domains such as business documents, official fillings, research papers etc, which is tremendously beneficial for large-scale table analysis tasks.
The TableBank dataset totally consists of 417,234 high quality labeled tables as well as their original documents in a variety of domains.
Source: TableBank"	https://paperswithcode.com/dataset/tablebank	05/03/2019	TableBank					
1251	GitHub Typo Corpus	"Are you the kind of person who makes a lot of typos when writing code? Or are you the one who fixes them by making ""fix typo"" commits? Either way, thank you—you contributed to the state-of-the-art in the NLP field.
GitHub Typo Corpus is a large-scale dataset of misspellings and grammatical errors along with their corrections harvested from GitHub. It contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date.
Source: GitHub"	https://paperswithcode.com/dataset/github-typo-corpus	28/11/2019	GitHub Typo Corpus					
1252	word2word	"word2word contains easy-to-use word translations for 3,564 language pairs.

A large collection of freely & publicly available bilingual lexicons for 3,564 language pairs across 62 unique languages.
Easy-to-use Python interface for accessing top-k word translations and for building a new bilingual lexicon from a custom parallel corpus.
Constructed using a simple approach that yields bilingual lexicons with high coverage and competitive translation quality.

Source: word2word"	https://paperswithcode.com/dataset/word2word	27/11/2019	word2word					
1253	Dakshina	"The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon which consists of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet.
Source: Google Research"	https://paperswithcode.com/dataset/dakshina	02/07/2020	Dakshina					
1254	Dataset of Legal Documents	"Dataset of Legal Documents consists of court decisions from 2017 and 2018 were selected for the dataset, published online by the Federal Ministry of Justice and Consumer Protection. The documents originate from seven federal courts: Federal Labour Court (BAG), Federal Fiscal Court (BFH), Federal Court of Justice (BGH), Federal Patent Court (BPatG), Federal Social Court (BSG), Federal Constitutional Court (BVerfG) and Federal Administrative Court (BVerwG).
The dataset consists of 66,723 sentences with 2,157,048 tokens. The sizes of the seven court-specific datasets varies between 5,858 and 12,791 sentences, and 177,835 to 404,041 tokens. The distribution of annotations on a per-token basis corresponds to approx. 19-23 %.
Source: GitHub"	https://paperswithcode.com/dataset/dataset-of-legal-documents	29/03/2020						
1255	ChrEn	"Cherokee-English Parallel Dataset is a low-resource dataset of 14,151 pairs of sentences with around
313K English tokens and 206K Cherokee tokens. The parallel corpus is accompanied by a monolingual Cherokee dataset of 5,120 sentences. Both datasets are mostly derived from Cherokee monolingual books."	https://paperswithcode.com/dataset/chren	09/10/2020	Cherokee-English Parallel Dataset					
1256	C4	"C4 is a colossal, cleaned version of Common Crawl's web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models.
The dataset can be downloaded in a pre-processed form from allennlp."	https://paperswithcode.com/dataset/c4	23/10/2019	Colossal Clean Crawled Corpus					
1257	Image网	"Image网 (pronounced Imagewang; 网 means ""net"" in Chinese) is an image classification dataset combined from Imagenette and Imagewoof datasets in a way to make it into a semi-supervised unbalanced classification problem:


the validation set is the same as the validation set of Imagewoof; there are no Imagenette images in the validation set (they're all in the training set),


only 10% of Imagewoof images are in the training set. The remaining images are in the ""unsupervised"" split.


Source: fast.ai"	https://paperswithcode.com/dataset/imagewang	11/02/2020						
1258	CCAligned	"CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to multiple documents in different target language, it is possible to join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French).
Source: CCAligned"	https://paperswithcode.com/dataset/ccaligned	10/11/2019	CCAligned					
1259	WikiTableT	"WikiTableT contains Wikipedia article sections and their corresponding tabular data and various metadata. WikiTableT contains millions of instances while covering a broad range of topics and a variety of kinds of generation tasks.
Source: WikiTableT"	https://paperswithcode.com/dataset/wikitablet	29/12/2020	WikiTableT					
1260	AutoWeakS	"Collects all the courses from XuetangX5, one of the largest MOOCs in China, and this results in 1951 courses. The collected courses involve seven areas: computer science, economics, engineering, foreign language, math, physics, and social science. Each course contains 131 words in its descriptions on average. Contains 706 job postings from the recruiting website operated by JD.com (JD) and 2,456 job postings from the website owned by Tencent corporation (Tencent). The collected job postings involve six areas: technical post, financial post, product post, design post, market post, supply chain and engineering post. 
Source: AutoWeakS"	https://paperswithcode.com/dataset/autoweaks	28/12/2020	AutoWeakS					
1261	MMDB	"Multimodal Dyadic Behavior (MMDB) dataset is a unique collection of multimodal (video, audio, and physiological) recordings of the social and communicative behavior of toddlers. The MMDB contains 160 sessions of 3-5 minute semi-structured play interaction between a trained adult examiner and a child between the age of 15 and 30 months. The MMDB dataset supports a novel problem domain for activity recognition, which consists of the decoding of dyadic social interactions between adults and children in a developmental context.
Source: MMDB"	https://paperswithcode.com/dataset/mmdb	01/06/2013	Multimodal Dyadic Behavior					
1262	GazeFollow	"GazeFollow is a large-scale dataset annotated with the location of where people in images are looking. It uses several major datasets that contain people as a source of images: 1, 548 images from SUN, 33, 790 images from MS COCO, 9, 135 images from Actions 40, 7, 791 images from PASCAL, 508 images from the ImageNet detection challenge and 198, 097 images from the Places dataset. This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios.
Source: GazeFollow"	https://paperswithcode.com/dataset/gazefollow	01/12/2015	GazeFollow					
1263	4DFAB	"4DFAB is a large scale database of dynamic high-resolution 3D faces which consists of recordings of 180 subjects captured in four different sessions spanning over a five-year period (2012 - 2017), resulting in a total of over 1,800,000 3D meshes. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour.
Source: ibug"	https://paperswithcode.com/dataset/4dfab	01/06/2018	4DFAB					
1264	iQIYI-VID-2019	"iQIYI-VID-2019 dataset is the first video dataset for multi-model person identification. This dataset aims to encourage the research of multi-modal based person identification. To get close to real applications, video clips are extracted from real online videos of extensive types. All the clips are labeled by human annotators, and use automatic algorithms to accelerate the collection and labeling process. The iQIYI-VID-2019 dataset is more challenging comparing to the iQIYI-VID-2018 dataset, since most hard examples are selected from iQIYI-VID-2018 while more person ids is added. The dataset contains 100K~200K video clips, divided into three parts, 40% for training, 30% for validation, and 30% for test. The dataset contains about 10, 000 identities, 5,000 of which come from the iQIYI celebrity database and mainly extracts from iQIYI-VID-2018.
Source: 2019 iQIYI Celebrity Video Identification Challenge"	https://paperswithcode.com/dataset/iqiyi-vid-2019	10/05/2019	iQIYI-VID-2019					
1265	iQIYI-VID	"iQIYI-VID dataset, which comprises video clips from iQIYI variety shows, films, and television dramas. The whole dataset contains 500,000 videos clips of 5,000 celebrities. The length of each video is 1~30 seconds. 
Source: iQIYI-VID"	https://paperswithcode.com/dataset/iqiyi-vid	19/11/2018	iQIYI-VID					
1266	ELFW	"Extended Labeled Faces in-the-Wild (ELFW) is a dataset supplementing with additional face-related categories —and also additional faces— the originally released semantic labels in the vastly used Labeled Faces in-the-Wild (LFW) dataset. Additionally, two object-based data augmentation techniques are deployed to synthetically enrich under-represented categories which, in benchmarking experiments, reveal that not only segmenting the augmented categories improves, but also the remaining ones benefit.
Source: Extended Labeled Faces in-the-Wild 
Image Source: https://multimedia-eurecat.github.io/2020/06/22/extended-faces-in-the-wild.html"	https://paperswithcode.com/dataset/elfw	24/06/2020	Extended Labeled Faces in-the-Wild					
1267	KANFace	"KANFace consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The dataset is manually annotated in terms of identity, exact age, gender and kinship.
Source: KANFace Dataset"	https://paperswithcode.com/dataset/kanface	15/05/2020	KANFace Dataset					
1268	BAVL	"Blind Audio-Visual Localization (BAVL) Dataset consists of 20 audio-visual recordings of  sound sources, which could be talking faces or music instruments. Most audio-visual recordings (19) are videos from Youtube except V8, which is from [1]. Besides, the video V7 was also used in[2][3], and V16 used in [3]. All 20 videos are annotated by ourselves in a uniform manner. Details of the video sequences are listed in Table 1. 
The videos in the dataset have average duration of 10 seconds, and they are all recorded by one camera and one microphone. The audio files (.wav) was sampled at a 16 kHz for V7, V8, V16, and 44.1 kHz for the rest. The video frames contain the sound-making object (sound source) and distracting objects (e.g. pedestrian on the street), while the audio signals  consists of the sound produced by the sound source (human speech or instrumental music), environmental noise and sometimes other sounds. The distracting objects and other irrelevant noise/sounds do not exist in all videos. The primary usage of the dataset is to evaluate the performance of sound source localization method, in the presence of distracting motions and noise.
[1] Kidron, Einat, Yoav Y. Schechner, and Michael Elad. ""Pixels that sound.""Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Vol. 1. IEEE, 2005.
[2] Izadinia, Hamid, Imran Saleemi, and Mubarak Shah. ""Multimodal analysis for identification and segmentation of moving-sounding objects.""IEEE Transactions on Multimedia 15.2 (2013): 378-390.
[3] Li, Kai, Jun Ye, and Kien A. Hua. ""What's making that sound?.""Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014.
Source: Sound of Pixels"	https://paperswithcode.com/dataset/bavl	19/06/2017	Blind Audio-Visual Localization (BAVL)					
1269	AV Digits Database	"AV Digits Database is an audiovisual database which contains normal, whispered and silent speech. 53 participants were recorded from 3 different views (frontal, 45 and profile) pronouncing digits and phrases in three speech modes.
The database consists of two parts: digits and short phrases. In the first part, participants were asked to read 10 digits, from 0 to 9, in English in random order five times. In case of non-native English speakers this part was also repeated in the participant’s native language. In total, 53 participants (41 males and 12 females) from 16 nationalities, were recorded with a mean age and standard deviation of 26.7 and 4.3 years, respectively.
In the second part, participants were asked to read 10 short phrases. The phrases are the same as the ones used in the OuluVS2 database: “Excuse me”, “Goodbye”, “Hello”, “How are you”, “Nice to meet you”, “See you”, “I am sorry”,   “Thank you”, “Have a good time”, “You are welcome”. Again, each phrase was repeated five times in 3 different modes, neutral, whisper and silent speech. Thirty nine participants (32 males and 7 females) were recorded for this part with a mean age and standard deviation of 26.3 and 3.8 years, respectively.
Source: AV Digits Database"	https://paperswithcode.com/dataset/av-digits-database	18/02/2018	AV Digits Database					
1270	Fabrics Dataset	"The Fabrics Dataset consists of about 2000 samples of garments and fabrics. A small patch of each surface has been captured under 4 different illumination conditions using a custom made, portable photometric stereo sensor. All images have been acquired ""in the field"" (at clothes shops) and the dataset reflects the distribution of fabrics in real world, hence it is not balanced. The majority of clothes are made of specific fabrics, such as cotton and polyester, while some other fabrics, such as silk and linen, are more rare. Also, a large number of clothes are not composed of a single fabric but two or more fabrics are used to give the garment the desired properties (blended fabrics). For every garment there is information (attributes) about its material composition from the manufacturer label and its type (pants, shirt, skirt etc.).
Source: Fabrics Dataset"	https://paperswithcode.com/dataset/fabrics-dataset	16/09/2016	The Fabrics Dataset					
1271	MobiFace	"MobiFace is the first dataset for single face tracking in mobile situations. It consists of 80 unedited live-streaming mobile videos captured by 70 different smartphone users in fully unconstrained environments. Over 95K bounding boxes are manually labelled. The videos are carefully selected to cover typical smartphone usage. The videos are also annotated with 14 attributes, including 6 newly proposed attributes and 8 commonly seen in object tracking.
Source: MobiFace"	https://paperswithcode.com/dataset/mobiface	24/05/2018	MobiFace					
1272	LSFM	"The Large Scale Facial Model (LSFM) is a 3D statistical model of facial shape built from nearly 10,000 individuals.
Source: LSFM"	https://paperswithcode.com/dataset/lsfm	08/04/2017	Large Scale Facial Model (LSFM)					
1273	FaceScape	"FaceScape dataset provides 3D face models, parametric models and multi-view images in large-scale and high-quality. The camera parameters, the age and gender of the subjects are also included. The data have been released to public for non-commercial research purpose.
Source: FaceScape"	https://paperswithcode.com/dataset/facescape	31/03/2020	FaceScape					
1274	AgeDB	"AgeDB contains 16, 488 images of various famous people, such as actors/actresses, writers, scientists, politicians, etc. Every image is annotated with respect to the identity, age and gender attribute. There exist a total of 568 distinct subjects. The average number of images per subject is 29. The minimum and maximum age is 1 and 101, respectively. The average age range for each subject is 50.3 years.
Source: AgeDB"	https://paperswithcode.com/dataset/agedb	01/07/2017	AgeDB					
1275	AFEW-VA	"The AFEW-VA databaset is a collection of highly accurate per-frame annotations levels of valence and arousal, along with per-frame annotations of 68 facial landmarks for 600 challenging video clips. These clips are extracted from feature films and were also annotated in terms of discrete emotion categories in the form of the AFEW database (that can be obtained there).
Source: AFEW-VA"	https://paperswithcode.com/dataset/afew-va	07/02/2017	AFEW-VA Database for Valence and Arousal Estimation In-The-Wild					
1276	KILT	"KILT (Knowledge Intensive Language Tasks) is a benchmark consisting of 11 datasets representing 5 types of tasks:

Fact-checking (FEVER),
Entity linking (AIDA CoNLL-YAGO, WNED-WIKI, WNED-CWEB),
Slot filling (T-Rex, Zero Shot RE),
Open domain QA (Natural Questions, HotpotQA, TriviaQA, ELI5),
Dialog generation (Wizard of Wikipedia).

All these datasets have been grounded in a single pre-processed wikipedia snapshot, allowing for fairer and more consistent evaluation as well as enabling new task setups such as multitask and transfer learning.
Source: KILT Benchmarking"	https://paperswithcode.com/dataset/kilt	04/09/2020	KILT Benchmark					
1277	SOREL-20M	"SOREL-20M is a large-scale dataset consisting of nearly 20 million files with pre-extracted features and metadata, high-quality labels derived from multiple sources, information about vendor detections of the malware samples at the time of collection, and additional “tags” related to each malware sample to serve as additional targets.
Source: SOREL-20M"	https://paperswithcode.com/dataset/sorel-20m	14/12/2020	Sophos/ReversingLabs-20 Million					
1278	Relational Pattern Similarity Dataset	"The relational pattern similarity dataset is a new dataset upon the work of Zeichner et al. (2012), which consists of relational patterns with semantic inference labels annotated. The dataset includes 5,555 pairs extracted by Reverb (Fader et al., 2011), 2,447 pairs with inference relation and 3,108 pairs (the rest) without one.
Source: Composing Distributed Representations of Relational Patterns"	https://paperswithcode.com/dataset/relational-pattern-similarity-dataset	23/07/2017	Relational Pattern Similarity Dataset					
1279	PHM2017	"PHM2017 is a new dataset consisting of 7,192 English tweets across six diseases and conditions: Alzheimer’s Disease, heart attack (any severity), Parkinson’s disease, cancer (any type), Depression (any severity), and Stroke. The Twitter search API was used to retrieve the data using the colloquial disease names as search keywords, with the expectation of retrieving a high-recall, low precision dataset. After removing the re-tweets and replies, the tweets were manually annotated. The labels are:

self-mention. The tweet contains a health mention with a health self-report of the Twitter account owner, e.g., ""However, I worked hard and ran for Tokyo Mayer Election Campaign in January through February, 2014, without publicizing the cancer.""
other-mention. The tweet contains a health mention of a health report about someone other than the account owner, e.g., ""Designer with Parkinson’s couldn’t work then engineer invents bracelet + changes her world""
awareness. The tweet contains the disease name, but does not mention a specific person, e.g., ""A Month Before a Heart Attack, Your Body Will Warn You With These 8 Signals""
non-health. The tweet contains the disease name, but the tweet topic is not about health. ""Now I can have cancer on my wall for all to see <3""

Source: Did You Really Just Have a Heart Attack? Towards Robust Detection of Personal Health Mentions in Social Media"	https://paperswithcode.com/dataset/phm2017	26/02/2018	PHM2017					
1280	ORVS	"The ORVS dataset has been newly established as a collaboration between the computer science and visual-science departments at the University of Calgary.
This dataset contains 49 images (42 training and seven testing images) collected from a clinic in Calgary-Canada. All images were acquired with a Zeiss Visucam 200 with 30 degrees field of view (FOV). The image size is 1444×1444 with 24 bits per pixel. Images and are stored in JPEG format with low compression, which is common in ophthalmology practice. All images were manually traced by an expert who a has been working in the field of retinal-image analysis and went through training. The expert was asked to label all pixels belonging to retinal vessels. The Windows Paint 3D tool was used to manually label the images.
Source: Transfer Learning Through Weighted Loss Function and Group Normalization for Vessel Segmentation from Retinal Images"	https://paperswithcode.com/dataset/orvs	16/12/2020	Online Retinal image for Vessel Segmentation (ORVS)					
1281	DR HAGIS	"The DR HAGIS database has been created to aid the development of vessel extraction algorithms suitable for retinal screening programmes. Researchers are encouraged to test their segmentation algorithms using this database.
All thirty-nine fundus images were obtained from a diabetic retinopathy screening programme in the UK. Hence, all images were taken from diabetic patients. Since patients attending these screening programmes exhibit other co-morbidities, the DR HAGIS database consists of following four co-morbidity subgroups:

Images 1-10: Glaucoma subgroup
Images 11-20: Hypertension subgroup
Images 21-30: Diabetic retinopathy subgroup
Images 31-40: Age-related macular degeneration subgroup

Besides the fundus images, the manual segmentation of the retinal surface vessels is provided by an expert grader. These manually segmented images can be used as the ground truth to compare and assess the automatic vessel extraction algorithms. Masks of the FOV are provided as well to quantify the accuracy of vessel extraction within the FOV only.
The images were acquired in different screening centers, therefore reflecting the range of image resolutions, digital cameras and fundus cameras used in the clinic. The fundus images were captured using a Topcon TRC-NW6s, Topcon TRC-NW8 or a Canon CR DGi fundus camera with a horizontal 45 degree field-of-view (FOV). The images are 4752x3168 pixels, 3456x2304 pixels, 3126x2136 pixels, 2896x1944 pixels or 2816x1880 pixels in size.
The fundus images are saved as compressed JPEG files with 8 bits per colour plane. The ground truth and mask images are saved as binary PNG files.
Source: DR HAGIS"	https://paperswithcode.com/dataset/dr-hagis	09/02/2017	Diabetic Retinopathy, Hypertension, Age-related macular degeneration and Glacuoma ImageS					
1282	ARIA	"This data set was collected in 2004 to 2006 in the United Kingdom. Subjects were adult males and females, some of whom were healthy (control group), some with age-related macular degeneration (AMD group), and some were diabetic patients (diabetic group). Unfortunately, no other information from this time exists about this subjects.
Source: ARIA"	https://paperswithcode.com/dataset/aria	15/10/2008	Automated Retinal Image Analysis (ARIA) Data Set					
1283	VICAVR	"The VICAVR database is a set of retinal images used for the computation of the A/V Ratio. The database currently includes 58 images. The images have been acquired with a TopCon non-mydriatic camera NW-100 model and are optic disc centered with a resolution of 768x584. The database includes the caliber of the vessels measured at different radii from the optic disc as well as the vessel type (artery/vein) labelled by three experts.
Source: VICAVR"	https://paperswithcode.com/dataset/vicavr	18/04/2014	VICAVR Database					
1284	OCTAGON	"The OCTAGON dataset is a set of Angiography by Octical Coherence Tomography images (OCT-A) used to the segmentation of the Foveal Avascular Zone (FAZ). The dataset includes 144 healthy OCT-A images and 69 diabetic OCT-A images, divided into four groups, each one with 36 and about 17 OCT-A images, respectively. These groups are: 3x3 superficial, 3x3 deep, 6x6 superficial and 6x6 deep, where 3x3 and 6x6 are the zoom of the image and superficial/deep are the depth level of the extracted image. The healthy dataset includes OCT-A images from people classified in 6 age ranges: 10-19 years, 20-29 years, 30-39 years, 40-49 years, 50-59 years and 60-69 years. Each age range includes 3 different patients with information of left and right eyes for each one. Finally, for each eye, there are four different images: one 3x3 superficial image, one 3x3 deep image, one 6x6 superficial image and one 6x6 deep image. Each image have two manual labelled of expert clinicians of the FAZ and their quantification in the healthy OCT-A images, and one manual labelled in the diabetic OCT-A images.
Source: OCTAGON dataset"	https://paperswithcode.com/dataset/octagon	26/11/2018	OCTAGON Dataset					
1285	CLOUD	"The CLOUD dataset is a set of Optical Coherence Tomography of the Anterior Segment images (AS-OCT) used to the automatic identification and representation of the cornea-contact lens relationship. The dataset includes 112 AS-OCT images that were captured from 16 different patients. In particular, the images were obtained by an OCT Cirrus 500 scanner model of Carl Zeiss Meditec with an anterior segment module for users of scleral contact lens (SCL).
Source: CLOUD Dataset"	https://paperswithcode.com/dataset/cloud	16/10/2019	CLOUD Dataset					
1286	MESSIDOR	"The Messidor database has been established to facilitate studies on computer-assisted diagnoses of diabetic retinopathy. The research community is welcome to test its algorithms on this database. In this section, you will find instructions on how to download the database.
Source: MESSIDOR"	https://paperswithcode.com/dataset/messidor-1	31/07/2014	MESSIDOR DATABASE					
1287	DIARETDB1	"The database consists of 89 colour fundus images of which 84 contain at least mild non-proliferative signs (Microaneurysms) of the diabetic retinopathy, and 5 are considered as normal which do not contain any signs of the diabetic retinopathy according to all experts who participated in the evaluation. Images were captured using the same 50 degree field-of-view digital fundus camera with varying imaging settings. The data correspond to a good (not necessarily typical) practical situation, where the images are comparable, and can be used to evaluate the general performance of diagnostic methods. This data set is referred to as ""calibration level 1 fundus images"".
Source: DIARETDB1"	https://paperswithcode.com/dataset/diaretdb1	10/09/2007	DIARETDB1					
1288	UDA-CH	"UDA-CH contains 16 objects that cover a variety of artworks which can be found in a museum like sculptures, paintings and books. Specifically, the dataset has been collected inside the cultural site “Galleria Regionale di Palazzo Bellomo” located in Siracusa, Italy.
Source: An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites"	https://paperswithcode.com/dataset/uda-ch	04/08/2020	Unsupervised Domain Adaptation on Cultural Heritage					
1289	EGO-CH	"EGO-CH is a dataset of egocentric videos for visitors’ behavior understanding. The dataset has been collected in two different cultural sites and includes more than 27 hours of video acquired by 70 subjects, including volunteers and 60 real visitors. The overall dataset includes labels for 26 environments and over 200 Points of Interest (POIs). Specifically, each video of EGO-CH has been annotated with 1) temporal labels specifying the current location of the visitor and the observed POI, 2) bounding box annotations around POIs. A large subset of the dataset, consisting of 60 videos, is also associated with surveys filled out by the visitors at the end of each visit.
Source: EGO-CH"	https://paperswithcode.com/dataset/ego-ch	03/02/2020	EGOcentric-Cultural Heritage					
1290	MAP	"Maybe Ambiguous Pronoun is a dataset similar to GAP dataset, but without binary gender constraints.
Source: Toward Gender-Inclusive Coreference Resolution"	https://paperswithcode.com/dataset/map	30/10/2019	Maybe Ambiguous Pronoun					
1291	GICoref	"GICoref is a fully annotated coreference resolution dataset written by and about trans people.
Source: Toward Gender-Inclusive Coreference Resolution"	https://paperswithcode.com/dataset/gicoref	30/10/2019	Gender Inclusive Coreference					
1292	NAF	"This dataset was created with images provided by the United States National Archive and FamilySearch.
The goal of this data is to capture relationships between text/handwriting entities on form images. It will include transcriptions in the future, but doesn't currently.
The form images are organized into ""groups"", each group containing images of the same form type.
Source: NAF Dataset"	https://paperswithcode.com/dataset/naf	05/09/2019	National Archives Forms Dataset					
1293	ImageNet-P	"ImageNet-P consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used.
Source: Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"	https://paperswithcode.com/dataset/imagenet-p	28/03/2019						
1294	Combinatorial 3D Shape Dataset	"The combinatorial 3D shape dataset is composed of 406 instances of 14 classes. Specifically, each object in the dataset is considered equivalent to a sequence of primitive placement.
Source: Combinatorial 3D Shape Generation via Sequential Assembly"	https://paperswithcode.com/dataset/combinatorial-3d-shape-dataset	16/04/2020						
1295	AI2D	"AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams with over 150000 rich annotations, their ground truth syntactic parses, and more than 15000 corresponding multiple choice questions.
Source: A Diagram Is Worth A Dozen Images
Image Source: Kembhavi et al"	https://paperswithcode.com/dataset/ai2d	24/03/2016	AI2 Diagrams					
1296	Chart2Text	"Chart2Text is a dataset that was crawled from 23,382 freely accessible pages from statista.com in early March of 2020, yielding a total of 8,305 charts, and associated summaries. For each chart, the chart image, the underlying data table, the title, the axis labels, and a human-written summary describing the statistic was downloaded.
Source: Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model"	https://paperswithcode.com/dataset/chart2text	18/10/2020	Chart Summarization Dataset					
1297	DENSE	"DENSE (Depth Estimation oN Synthetic Events) is a new dataset with synthetic events and perfect ground truth.
Source: Learning Monocular Dense Depth from Events"	https://paperswithcode.com/dataset/dense	16/10/2020	Depth Estimation oN Synthetic Events					
1298	PixelShift200	"Advanced pixel shift technology is employed to perform a full color sampling of the image. Pixel shift technology takes four samples of the same image at nearly the same time, and physically controls the camera sensor to move one pixel horizontally or vertically at each sampling to capture all color information at each pixel. The pixel shift technology ensures that the sampled images follow the distribution of natural images sampled by the camera, and the full information of the color (R, Gr, Gb, B channel) is completely obtained without any need of interpolation. In this way, the collected RGB images are artifacts-free, which leads to better training results for demosaicing related tasks.
PixelShift200 Dataset contains 210 high quality 4K images.

Training: 200 images
Testing: 10 images
Key Features: fully colored, demosiacing artifacts free
Camera: SONY α7R III

Source: PixelShift200"	https://paperswithcode.com/dataset/pixelshift200	07/05/2019						
1299	VLEP	"VLEP contains 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. Each example (see Figure 1) consists of a Premise Event (a short video clip with dialogue), a Premise Summary (a text summary of the premise event), and two potential natural language Future Events (along with Rationales) written by people. These clips are on average 6.1 seconds long and are harvested from diverse event-rich sources, i.e., TV show and YouTube Lifestyle Vlog videos.
Source: What is More Likely to Happen Next? Video-and-Language Future Event Prediction"	https://paperswithcode.com/dataset/vlep	15/10/2020	Video-and-Language Event Prediction					
1300	Cata7	"Cata7 is the first cataract surgical instrument dataset for semantic segmentation. The dataset consists of seven videos while each video records a complete cataract surgery. All videos are from Beijing Tongren Hospital. Each video is split into a sequence of images, where resolution is 1920×1080 pixels. To reduce redundancy, the videos are downsampled from 30 fps to 1 fps. Also, images without surgical instruments are manually removed. Each image is labeled with precise edges and types of surgical instruments. This dataset contains 2,500 images, which are divided into training and test sets. The training set consists of five video sequences and test set consists of two video sequence.
Source: RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments
Image Source: Ni et al"	https://paperswithcode.com/dataset/cata7	23/09/2019						
1301	UCC	"The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.
Each comment is labelled as either 'healthy' or 'unhealthy', in addition to binary labels for the presence of six potentially 'unhealthy' sub-attributes: (1) hostile; (2) antagonistic, insulting, provocative or trolling; (3) dismissive; (4) condescending or patronising; (5) sarcastic; and/or (6) an unfair generalisation. Each label also has an associated confidence score.
The UCC contributes further high quality data on attributes like sarcasm, hostility, and condescension, adding to existing datasets on these and related attributes, and provides the first dataset of this scale with labels for dismissiveness, unfair generalisations, antagonistic behavior, and overall assessments of whether those comments fall within 'healthy' conversation.
Source: UCC"	https://paperswithcode.com/dataset/ucc	14/10/2020	Unhealthy Comments Corpus					
1302	Satire Dataset	"The satire dataset is a new multi-modal dataset of satirical and regular news articles. The satirical news is collected from four websites that explicitly declare themselves to be satire, and the regular news is collected from six mainstream news websites. Specifically, the satirical news websites the articles were collected from are The Babylon Bee, Clickhole, Waterford Whisper News, and The DailyER. The regular news websites are Reuters, The Hill, Politico, New York Post, Huffington Post, and Vice News. The headlines
and the thumbnail images of the latest 1000 articles for each of the publications are collected. The dataset contains a total of 4000 satirical and 6000 regular news articles.
Source: A Multi-Modal Method for Satire Detection using Textual and Visual Cues"	https://paperswithcode.com/dataset/satire-dataset	13/10/2020						
1303	Headcam	"This dataset contains panoramic video captured from a helmet-mounted camera while riding a bike through suburban Northern Virginia. 
Source: Headcam"	https://paperswithcode.com/dataset/headcam	04/01/2019						
1304	OCNLI	"OCNLI stands for Original Chinese Natural Language Inference. It is corpus for Chinese Natural Language Inference, collected following closely the procedures of MNLI, but with enhanced strategies aiming for more challenging inference pairs. No human/machine translation is used in creating the dataset, and thus the Chinese texts are original and not translated.
OCNLI has roughly 50k pairs for training, 3k for development and 3k for test. Only the test data is released but not its labels.
OCNLI is part of the CLUE benchmark.
Source: OCNLI"	https://paperswithcode.com/dataset/ocnli	12/10/2020	Original Chinese Natural Language Inference					
1305	QReCC	"QReCC contains 14K conversations with 81K question-answer pairs. QReCC is built on questions from TREC CAsT, QuAC and Google Natural Questions. While TREC CAsT and QuAC datasets contain multi-turn conversations, Natural Questions is not a conversational dataset. Questions in NQ dataset were used as prompts to create conversations explicitly balancing types of context-dependent questions, such as anaphora (co-references) and ellipsis.
For each query the authors collect query rewrites by resolving references, the resulting query rewrite is a context-independent version of the original (context-dependent) question. The rewritten query is then used to with a search engine to answer the question. Each query is also annotated with answer, link to the web page that used to produce the answer.
Each conversation in the dataset contains a unique Conversation_no, Turn_no unique within a conversation, the original Question, Context, Rewrite and Answer with Answer_URL.
Source: QReCC"	https://paperswithcode.com/dataset/qrecc	10/10/2020						
1306	PHD²	"The dataset contains information on what video segments a specific user considers a highlight. Having this kind of data allows for strong personalization models, as specific examples of what a user is interested in help models obtain a fine-grained understanding of that specific user.
The data consists of YouTube videos, from which gifs.com users manually extracted their highlights, by creating GIFs from a segment of the full video. Thus, the dataset is similar to PHD-GIFS, with two major differences.

Each selection is associated with a user, which is what allows personalization.
instead of visual matching to find the position in the video from which a GIF was selected, PHD-GIFS uses the timestamps. Thus, the ground truth is free from any alignment errors.

The training set contains highlights from 12,972 users. The test set contains highlights from 850 users. 
Source: Personalized Highlight Detection Dataset"	https://paperswithcode.com/dataset/phd2	18/04/2018	Personalized Highlight Detection Dataset					
1307	Video2GIF	"The Video2GIF dataset contains over 100,000 pairs of GIFs and their source videos. The GIFs were collected from two popular GIF websites (makeagif.com, gifsoup.com) and the corresponding source videos were collected from YouTube in Summer 2015. IDs and URLs of the GIFs and the videos are provided, along with temporal alignment of GIF segments to their source videos. The dataset shall be used to evaluate GIF creation and video highlight techniques.
In addition to the 100K GIF-video pairs, the dataset contains 357 pairs of GIFs and their source videos as the test set. The 357 videos come with a Creative Commons CC-BY license, which allowed the authors to redistribute the material with appropriate credit to make the results on test set reproducible even when some of the videos become unavailable.
Source: Video2GIF"	https://paperswithcode.com/dataset/video2gif	16/05/2016						
1308	VAST	"VAST consists of a large range of topics covering broad themes, such as politics (e.g., ‘a Palestinian state’), education (e.g., ‘charter schools’), and public health (e.g., ‘childhood vaccination’). In addition, the data includes a wide range of similar expressions (e.g., ‘guns on campus’ versus ‘firearms on campus’). This variation captures how humans might realistically describe the same topic and contrasts with the lack of variation in existing datasets.
Source: Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations"	https://paperswithcode.com/dataset/vast	07/10/2020	VAried Stance Topics					
1309	Silent Speech EMG	"Facial electromyography recordings during both silent and vocalized speech.
Source: Silent Speech EMG"	https://paperswithcode.com/dataset/silent-speech-emg	06/10/2020						
1310	SMOT	"The SMOT dataset, Single sequence-Multi Objects Training, is collected to represent a practical scenario of collecting training images of new objects in the real world, i.e. a mobile robot with an RGB-D camera collects a sequence of frames while driving around a table to learning multiple objects and tries to recognize objects in different locations.
Source: SMOT"	https://paperswithcode.com/dataset/smot	07/05/2020	Single sequence-Multi Objects Training					
1311	3DNet	"The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10, 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor.
Source: 3DNet"	https://paperswithcode.com/dataset/3dnet							
1312	ARID	"ARID is a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot.
Source: ARID"	https://paperswithcode.com/dataset/arid	18/09/2017	Autonomous Robot Indoor Dataset					
1313	OCID	"Developing robot perception systems for handling objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms.
The Object Cluttered Indoor Dataset is an RGBD-dataset containing point-wise labeled point-clouds for each object. The data was captured using two ASUS-PRO Xtion cameras that are positioned at different heights. It captures diverse settings of objects, background, context, sensor to scene distance, viewpoint angle and lighting conditions. The main purpose of OCID is to allow systematic comparison of existing object segmentation methods in scenes with increasing amount of clutter. In addition OCID does also provide ground-truth data for other vision tasks like object-classification and recognition.
Source: OCID"	https://paperswithcode.com/dataset/ocid	05/02/2019	Object Clutter Indoor Dataset					
1314	LfED-6D	"The LfED-6D dataset is a collection of 6D grasp annotations acquired through experience (with a robot platform) or by human demonstration. For known objects, the annotated grasps can be directly applied given the pose of the object model is correctly computed. For unknown objects, the grasps can be generalized using methods for shape matching, for example the Dense Geometrical Correspondence Network.
Source: LfED-6D"	https://paperswithcode.com/dataset/lfed-6d	15/01/2020	Learning from Experience and Demonstration for 6-DOF Grasping Dataset					
1315	NYU-VP	"NYU-VP is a new dataset for multi-model fitting, vanishing point (VP) estimation in this case. Each image is annotated with up to eight vanishing points, and pre-extracted line segments are provided which act as data points for a robust estimator. Due to its size, the dataset is the first to allow for supervised learning of a multi-model fitting task.
Source: CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus"	https://paperswithcode.com/dataset/nyu-vp	08/01/2020						
1316	YUD+	"YUD+ is a dataset containing additional Vanishing Point Labels for the York Urban Database.
Source: YUD+"	https://paperswithcode.com/dataset/yud	08/01/2020	Additional Vanishing Point Labels for the York Urban Database					
1317	NText	"NText is an eight million words dataset extracted and preprocessed from nuclear research papers and thesis.
Source: NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain"	https://paperswithcode.com/dataset/ntext	30/03/2020						
1318	NQuAD	"NQuAD is a Nuclear Question Answering Dataset, which contains 700+ nuclear Question Answer pairs developed and verified by expert nuclear researchers.
Source: NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain"	https://paperswithcode.com/dataset/nquad	30/03/2020	Nuclear Question Answering Dataset					
1319	Indoor and outdoor DFD dataset	"The dfd_indoor dataset contains 110 images for training and 29 images for testing. The dfd_outdoor dataset contains 34 images for tests; no ground truth was given for this dataset, as the depth sensor only works on indoor scenes.
Source: DFD dataset"	https://paperswithcode.com/dataset/indoor-and-outdoor-dfd-dataset	05/09/2018						
1320	Lorenz Dataset	"The Lorenz dataset contains 100000 time-series with length 24. The data has 5 modes and it is obtained using the Lorenz equation with 5 different seed values.
Source: Lorenz Dataset"	https://paperswithcode.com/dataset/lorenz-dataset-1	29/03/2019						
1321	EHR-Rel	"EHR-RelB is a benchmark dataset for biomedical concept relatedness, consisting of 3630 concept pairs sampled from electronic health records (EHRs). EHR-RelA is a smaller dataset of 111 concept pairs, which are mainly unrelated.
Source: EHR-Rel"	https://paperswithcode.com/dataset/ehr-rel	30/10/2020						
1322	MLGESTURE DATASET	"MlGesture is a dataset for hand gesture recognition tasks, recorded in a car with 5 different sensor types at two different viewpoints. The dataset contains over 1300 hand gesture videos from 24 participants and features 9 different hand gesture symbols. One sensor cluster with five different cameras is mounted in front of the driver in the center of the dashboard. A second sensor cluster is mounted on the ceiling looking straight down.
Source: MLGESTURE DATASET"	https://paperswithcode.com/dataset/mlgesture-dataset	24/04/2020						
1323	NYT-H	"NYT-H is a dataset for distantly-supervised relation extraction, in which DS-labelled training data is used and several annotators to label test data are hired. NYT-H can serve as a benchmark of distantly-supervised relation extraction.
Source: Towards Accurate and Consistent Evaluation: A Dataset for Distantly-Supervised Relation Extraction"	https://paperswithcode.com/dataset/nyt-h	30/10/2020						
1324	CSAW-S	"CSAW-S is a dataset of mammography images which includes expert annotations of tumors and non-expert annotations of breast anatomy and artifacts in the image.
Source: CSAW-S"	https://paperswithcode.com/dataset/csaw-s	20/07/2020						
1325	2D-3D Match Dataset	"2D-3D Match Dataset is a new dataset of 2D-3D correspondences by leveraging the availability of several 3D datasets from RGB-D scans. Specifically, the data from SceneNN and 3DMatch are used. The training dataset consists of 110 RGB-D scans, of which 56 scenes are from SceneNN and 54 scenes are from 3DMatch. The 2D-3D correspondence data is generated as follows. Given a 3D point which is randomly sampled from a 3D point cloud, a set of 3D patches from different scanning views are extracted. To find a 2D-3D correspondence, for each 3D patch, its 3D position is re-projected into all RGB-D frames for which the point lies in the camera frustum, taking occlusion into account. The corresponding local 2D patches around the re-projected point are extracted. In total, around 1.4 millions 2D-3D correspondences are collected.
Source: 2D-3D Match Dataset"	https://paperswithcode.com/dataset/2d-3d-match-dataset	21/11/2019						
1326	FIGR-8	"The FIGR-8 database is a dataset containing 17,375 classes of 1,548,256 images representing pictograms, ideograms, icons, emoticons or object or conception depictions. Its aim is to set a benchmark for Few-shot Image Generation tasks, albeit not being limited to it. Each image is represented by 192x192 pixels with grayscale value of 0-255. Classes are not balanced (they do not all contain the same number of elements), but they all do contain at the very least 8 images.
Source: https://github.com/marcdemers/FIGR-8
Image Source: https://github.com/marcdemers/FIGR-8"	https://paperswithcode.com/dataset/figr-8	08/01/2019						
1327	ImageNet-Sketch	"ImageNet-Sketch data set consists of 50000 images, 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries ""sketch of "", where  is the standard class name. Only within the ""black and white"" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images.
Source: ImageNet-Sketch
Image Source: https://github.com/HaohanWang/ImageNet-Sketch"	https://paperswithcode.com/dataset/imagenet-sketch	29/05/2019						
1328	YouTubeVIS	"YouTubeVIS is a new dataset tailored for tasks like simultaneous detection, segmentation and tracking of object instances in videos and is collected based on the current largest video object segmentation dataset YouTubeVOS.
Source: YouTubeVIS"	https://paperswithcode.com/dataset/youtubevis	12/05/2019						
1329	EURLEX57K	"EURLEX57K is a new publicly available legal LMTC dataset, dubbed EURLEX57K, containing 57k English EU legislative documents from the EUR-LEX portal, tagged with ∼4.3k labels (concepts) from the European Vocabulary (EUROVOC).
Source: Large-Scale Multi-Label Text Classification on EU Legislation"	https://paperswithcode.com/dataset/eurlex57k	05/06/2019						
1330	Anonymized Keystrokes Dataset	"Includes two datasets for this task, one for English-French (En-Fr) and another for English-German (En-De). For each dataset, the action sequences for full documents are provided, along with an editor identifier. The dataset contains document-level post-editing action sequences, including edit operations from keystrokes, mouse actions, and waiting times.
Source: Translator2Vec: Understanding and Representing Human Post-Editors"	https://paperswithcode.com/dataset/anonymized-keystrokes-dataset	24/07/2019						
1331	METU-VIREF Dataset	"METU-VIREF is a video referring expression dataset comprising of videos from VIRAT Ground and ILSVRC2015 VID datasets. VIRAT is a surveillance dataset and contains mainly people and vehicles. To line up with this and restrict the domain, only videos that contain vehicles from the ILSVRC dataset are used. The METU-VIREF dataset does not contain whole videos from these datasets (the videos need to be downloaded from the respective sources) but just referring expressions for video sequences containing an object pair. For this, object pairs are chosen which had a relation that a meaningful referring expression could be written for.
Source: VIREF"	https://paperswithcode.com/dataset/metu-viref-dataset	03/08/2019						
1332	UNDD	"UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset.
Source: UNDD
Image Source: https://github.com/sauradip/night_image_semantic_segmentation"	https://paperswithcode.com/dataset/undd	24/09/2019	Urban Night Driving Dataset					
1333	Mapillary Vistas Dataset	"Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world.
Source: Mapillary Vistas Dataset
Image Source: Neuhold et al"	https://paperswithcode.com/dataset/mapillary-vistas-dataset	01/10/2017						
1334	Food.com Recipes and Interactions	"Food.com Recipes and Interactions consists of 270K recipes and 1.4M user-recipe interactions (reviews) scraped from Food.com, covering a period of 18 years (January 2000 to December 2018).
Source: Generating Personalized Recipes from Historical User Preferences"	https://paperswithcode.com/dataset/food-com-recipes-and-interactions	31/08/2019						
1335	SYNTHIA-PANO	"SYNTHIA-PANO is the panoramic version of SYNTHIA dataset. Five sequences are included: Seqs02-summer, Seqs02-fall, Seqs04-summer, Seqs04-fall and Seqs05-summer. Panomaramic images with fine annotation for semantic segmentation.
Source: SYNTHIA-PANO
Image Source: https://github.com/Francis515/SYNTHIA-PANO"	https://paperswithcode.com/dataset/synthia-pano	02/09/2019						
1336	SUIM	"The Segmentation of Underwater IMagery (SUIM) dataset contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants.
Source: Semantic Segmentation of Underwater Imagery: Dataset and Benchmark
Image Source: http://irvlab.cs.umn.edu/resources/suim-dataset"	https://paperswithcode.com/dataset/suim	02/04/2020	Segmentation of Underwater IMagery					
1337	CSAbstruct Dataset	"CSAbstruct is a new dataset of annotated computer science abstracts with sentence labels according to their rhetorical roles. The key difference between this dataset and PUBMED-RCT is that PubMed abstracts are written according to a predefined structure, whereas computer science papers are free-form. Therefore, there is more variety in writing styles in CSABSTRUCT. CSABSTRUCT is collected from the Semantic Scholar corpus (Ammar et al., 2018). Each sentence is annotated by 5 workers on the Figure-eight platform,6 with one of 5 categories {BACKGROUND, OBJECTIVE, METHOD, RESULT, OTHER}.
Source: Pretrained Language Models for Sequential Sentence Classification"	https://paperswithcode.com/dataset/csabstrcut-dataset	09/09/2019						
1338	Pesteh-Set	"Pesteh-Set is made of two parts. The first part includes 423 images with ground truth. The pistachios are sorted into two classes: Open-mouth and closed-mouth. The ground truth of the images is a CSV file that consists of the bounding boxes of the two classes of pistachios in the images. There are between 1 to 27 pistachios in each image, and 3927 pistachios in total. The second part includes 6 videos with a total length of 167 seconds and 561 moving pistachios.
Source: Pesteh-Set"	https://paperswithcode.com/dataset/pesteh-set	08/05/2020						
1339	CelebAGaze	"CelebAGaze consists of 25283 high-resolution celebrity images that are collected from CelebA and the Internet. It consists of 21832 face images with eyes staring at the camera and 3451 face images with eyes staring somewhere else. All images (256 × 256) are cropped and the eye mask region by dlib is computed. Specifically, dlib is used to extract 68 facial landmarks and calculate the mean of 6 points near the eye region, which will be the center point of the mask. The size of the mask is fixed to 30×50. As described above, 300 samples from domain Y are randomly selected, 100 samples from domain X as the test set, the remaining as the training set. Note that this dataset is unpaired and it is not labeled with the specific eye angle or the head pose information.
Source: Dual In-painting Model for Unsupervised Gaze Correction and Animation in the Wild"	https://paperswithcode.com/dataset/celebagaze	09/08/2020						
1340	IQUAD	"IQUAD is a dataset for Visual Question Answering in interactive environments. It is built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive object. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration.
Source: IQA: Visual Question Answering in Interactive Environments"	https://paperswithcode.com/dataset/iquad	09/12/2017	Interactive Question Answering Dataset					
1341	EVE	"EVE (End-to-end Video-based Eye-tracking) is a dataset for eye-tracking. It is collected from 54 participants and consists of 4 camera views, over 12 million frames and 1327 unique visual stimuli (images, video, text), adding up to approximately 105 hours of video data in total.
Official competition on Codalab: https://competitions.codalab.org/competitions/28954"	https://paperswithcode.com/dataset/eve	26/07/2020	End-to-end Video-based Eye-tracking					
1342	OpoSum	"OPOSUM is a dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums.
The six training collections were created by downsampling from the Amazon Product Dataset introduced in McAuley et al. (2015) and contain reviews and their respective ratings. 
A subset of the dataset has been manually annotated, specifically, for each domain, 10 different products were uniformly sampled (across ratings) with 10 reviews each, amounting to a total of 600 reviews, to be used only for development (300) and testing (300).
Source: Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised"	https://paperswithcode.com/dataset/oposum	27/08/2018						
1343	ForecastQA	"ForecastQA is a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. The forecasting problem for this dataset is formulated as a restricted-domain, multiple-choice, question-answering (QA) task that simulates the forecasting scenario.
Source: ForecastQA: A Question Answering Challenge for Event Forecasting"	https://paperswithcode.com/dataset/forecastqa	02/05/2020						
1344	TSU	"Toyota Smarthome Untrimmed (TSU) is a dataset for activity detection in long untrimmed videos. The dataset contains 536 videos with an average duration of 21 mins. Since this dataset is based on the same footage video as Toyota Smarthome Trimmed version, it features the same challenges and introduces additional ones. The dataset is annotated with 51 activities.
The dataset has been recorded in an apartment equipped with 7 Kinect v1 cameras. It contains common daily living activities of 18 subjects. The subjects are senior people in the age range 60-80 years old. The dataset has a resolution of 640×480 and offers 3 modalities: RGB + Depth + 3D Skeleton. The 3D skeleton joints were extracted from RGB. For privacy-preserving reasons, the face of the subjects is blurred.
Source: Toyota Smarthome"	https://paperswithcode.com/dataset/tsu	28/10/2020	Toyota Smarthome Untrimmed					
1345	AUTSL	"The Ankara University Turkish Sign Language Dataset (AUTSL) is a large-scale, multimode dataset that contains isolated Turkish sign videos. It contains 226 signs that are performed by 43 different signers. There are 38,336 video samples in total. The samples are recorded using Microsoft Kinect v2 in RGB, depth and skeleton formats. The videos are provided at a resolution of 512×512. The skeleton data contains spatial coordinates, i.e. (x, y), of the 25 junction points on the signer body that are aligned with 512×512 data.
Source: AUTSL Dataset"	https://paperswithcode.com/dataset/autsl	03/08/2020	Ankara University Turkish Sign Language Dataset					
1346	WikiHowQA	"WikiHowQA is a Community-based Question Answering dataset, which can be used for both answer selection and abstractive summarization tasks. It contains 76,687 questions in the train set, 8,000 in the development set and 22,354 in the test set.
Source: WikiHowQA"	https://paperswithcode.com/dataset/wikihowqa	22/11/2019						
1347	DRealSR	"DRealSR establishes a Super Resolution (SR) benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. 
It has been collected from five DSLR cameras in natural scenes and cover indoor and outdoor scenes avoiding moving objects, e.g., advertising posters, plants, offices, buildings. The training images are cropped into 380×380, 272×272 and 192×192 patches, resulting in 31,970 patches.
Source: Component Divide-and-Conquer for Real-World Image Super-Resolution"	https://paperswithcode.com/dataset/drealsr	05/08/2020	Diverse Real-world image Super-Resolution					
1348	EDEN	"EDEN (Enclosed garDEN) is a multimodal synthetic dataset, a dataset for nature-oriented applications. The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. 
Source: EDEN: Multimodal Synthetic Dataset of Enclosed GarDEN Scenes
Image Source: https://lhoangan.github.io/eden/"	https://paperswithcode.com/dataset/eden	09/11/2020						
1349	Wiki-CS	"Wiki-CS is a Wikipedia-based dataset for benchmarking Graph Neural Networks. The dataset is constructed from Wikipedia categories, specifically 10 classes corresponding to branches of computer science, with very high connectivity. The node features are derived from the text of the corresponding articles.  They were calculated as the average of pretrained GloVe word embeddings (Pennington et al., 2014), resulting in 300-dimensional node features.
The dataset has 11,701 nodes and 216,123 edges.
Source: Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks
Image Source: https://arxiv.org/pdf/2007.02901v1.pdf"	https://paperswithcode.com/dataset/wiki-cs	06/07/2020						
1350	ChaosNLI	"Chaos NLI is a Natural Language Inference (NLI) dataset with 100 annotations per example (for a total of 464,500 annotations) for some existing data points in the development sets of SNLI, MNLI, and Abductive NLI. The dataset provides additional labels for NLI annotations that reflect the distribution of human annotators, instead of picking the majority label as the gold standard label.
Source: ChaosNLI Github Repository"	https://paperswithcode.com/dataset/chaosnli	07/10/2020						
1351	VideoForensicsHQ	"VideoForensicsHQ is a benchmark dataset for face video forgery detection, providing high quality visual manipulations.  It is one of the first face video manipulation benchmark sets that also contains audio and thus complements existing datasets along a new challenging dimension. VideoForensicsHQ shows manipulations at much higher video quality and resolution, and shows manipulations that are provably much harder to detect by humans than videos in other datasets. 
VideoForensicsHQ contains 1,737 videos of speaking faces (44% male, 56% female), with 8 different emotions, most of them of “HD” resolution. The videos amount to 1,666,816 frames.
Source: VideoForensicsHQ: Detecting High-quality Manipulated Face Videos"	https://paperswithcode.com/dataset/videoforensicshq	20/05/2020						
1352	SOLO	"The SOLO Corpus comprises over 4 million English tweets, each of which contains at least one of the following tokens: solitude, lonely, and loneliness. The corpus has been collected to analyze the language and emotions associated with the state of being alone in English tweets.
Tweets related to the state of being alone were collected by polling the Twitter API from August 28, 2018 to July 10, 2019 with the following query terms: loneliness, lonely, and solitude. Duplicate tweets, short tweets (containing less than three words), and tweets with external URLs were discarded. Further, only up to three tweets per user are kept. This minimizes the impact of prolific tweeters and bots on the corpus.
Source: SOLO: A Corpus of Tweets for Examining the State of Being Alone"	https://paperswithcode.com/dataset/solo	04/06/2020						
1353	EXPO-HD	"The EXPO-HD Dataset is a dataset of Expo whiteboard markers for the purpose of instance segmentation. The dataset contains two subsets (both include instances segmentation labels):

Photorealistic synthetic image dataset with 5000 images.
Real image dataset with 200 images (used for validation and test).

The dataset can be used for testing domain adaptation techniques, as the training set consists of only synthetic images, and the validation and test sets consist of real images.
Source: EXPO-HD: Exact Object Perception using High Distraction Synthetic Data"	https://paperswithcode.com/dataset/expo-hd	28/07/2020						
1354	Twitter Death Hoaxes	"This is a dataset for detection fake death hoaxes. It consists of of death reports collected from Twitter between 1st January, 2012 and 31st December, 2014. It was collected by tracking the keyword 'RIP', and matching those tweets in which a name is mentioned next to RIP. Matching names were identified by using Wikidata as a database of names. 
The dataset contains 4,007 death reports, of which 2,301 are real deaths, 1,092 are commemorations and 614 are fake deaths.
Source: Early Detection of Social Media Hoaxes at Scale"	https://paperswithcode.com/dataset/twitter-death-hoaxes	22/01/2018						
1355	KACC	"The KACC benchmark consists of three subtasks that can be applied to knowledge graphs: knowledge abstraction, knowledge concretization and knowledge completion. 

The knowledge abstraction subtask contains tasks of concept inference, schema prediction and concept graph completion on the two-view KG. 
The knowledge concretization subtask requires models to do entity graph completion based on the two subgraphs. The concretization ability can be further examined by the results of long-tail entity link prediction. 
The knowledge completion subtask consists of typical single-view knowledge graph completion tasks for each subgraph.

KACC contains 999,902 entities in the entity graph, with 691 types of relations . The concept graph contains 21,293 concepts with 198 types of meta-relations. There are 2,367,971 cross-links between the two.
Source: KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion"	https://paperswithcode.com/dataset/kacc	28/04/2020						
1356	MSSD	"The Spotify Music Streaming Sessions Dataset (MSSD) consists of 160 million streaming sessions with associated user interactions, audio features and metadata describing the tracks streamed during the sessions, and snapshots of the playlists listened to during the sessions. 
This dataset enables research on important problems including how to model user listening and interaction behaviour in streaming, as well as Music Information Retrieval (MIR), and session-based sequential recommendations."	https://paperswithcode.com/dataset/mssd	31/12/2018	Music Streaming Sessions Dataset					
1357	InterHand2.6M	"The InterHand2.6M dataset is a large-scale real-captured dataset with accurate GT 3D interacting hand poses, used for 3D hand pose estimation The dataset contains 2.6M labeled single and interacting hand frames.
Source: InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image"	https://paperswithcode.com/dataset/interhand2-6m	21/08/2020						
1358	TaxiNLI	"TaxiNLI is a dataset collected based on the principles and categorizations of the aforementioned taxonomy. A subset of examples are curated from MultiNLI (Williams et al., 2018) by sampling uniformly based on the entailment label and the domain. The dataset is annotated with finegrained category labels.
Source: TaxiNLI: Taking a Ride up the NLU Hill"	https://paperswithcode.com/dataset/taxinli	30/09/2020						
1359	AllMusic Mood Subset	"The AllMusic Mood Subset (AMS) is a dataset for mood classification from songs. It is created by matching a subset of the Million Song Dataset (MSD), totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic.
Since the AMS is a subset of the MSD, the audio data is gathered by obtaining the 7-digital 30 second previews associated with all MSD tracks. These are 128kbps mp3 stereo
files sampled at 44.1kHz.
Source: Mood Classification Using Listening Data"	https://paperswithcode.com/dataset/allmusic-mood-subset	22/10/2020						
1360	NISP	"This dataset contains speech recordings along with speaker physical parameters (height, weight, shoulder size, age ) as well as regional information and linguistic information.
There are a total of 345 speakers (219 male and 126 female). The dataset contains sentences that are taken out from newspapers. Each speaker has contributed about 4-5 minutes of data that includes recordings in both English and their mother tongue. The transcript for the text is provided in UTF-8 format.
Source: NISP"	https://paperswithcode.com/dataset/nisp	12/07/2020	NITK-IISc Multilingual Multi-accent Speaker Profiling					
1361	EDUVSUM	"EDUVSUM contains educational videos with subtitles from three popular e-learning platforms: Edx,YouTube, and TIB AV-Portal that cover the following topics: crash course on history of science and engineering, computer science, python and web programming, machine learning and computer vision, Internet of things (IoT), and software engineering. In total, the current version of the dataset contains 98 videos with ground truth values annotated by a user with an academic background in computer science.
Source: EDUVSUM"	https://paperswithcode.com/dataset/eduvsum	26/10/2020	Educational Video Summarization					
1362	ADVANCE	"The AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE) is a brand-new multimodal learning dataset, which aims to explore the contribution of both audio and conventional visual messages to scene recognition. This dataset in summary contains 5075 pairs of geotagged aerial images and sounds, classified into 13 scene classes, i.e., airport, sports land, beach, bridge, farmland, forest, grassland, harbor, lake, orchard, residential area, shrub land, and train station.
Source:"	https://paperswithcode.com/dataset/advance	18/05/2020	AuDio Visual Aerial sceNe reCognition datasEt					
1363	Multi-Modal CelebA-HQ	"Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background.
Multi-Modal-CelebA-HQ can be used to train and evaluate algorithms of text-to-image-generation, text-guided image manipulation, sketch-to-image generation, and GANs for face generation and editing.
Source: Multi-Modal CelebA-HQ Dataset
Image Source: Xia et al"	https://paperswithcode.com/dataset/multi-modal-celeba-hq-1	06/12/2020						
1364	Short Text Font Dataset	"The proposed dataset includes 1,309 short text instances from Adobe Spark. The dataset is a collection of publicly available sample texts created by different designers. It covers a variety of topics found in posters, flyers, motivational quotes and advertisements.
Source: Short Text Font Dataset"	https://paperswithcode.com/dataset/short-text-font-dataset	03/05/2020						
1365	SPHERE-calorie	"The dataset contains both RGB and depth images, and the data from two accelerometers, together with ground truth calorie values from a calorimeter for calorie expenditure estimation in home environments. 
Source: SPEHERE Calorie"	https://paperswithcode.com/dataset/sphere-calorie	27/07/2016						
1366	SmartCity	"SmartCity consists of 50 images in total collected from ten city scenes including office entrance, sidewalk, atrium, shopping mall etc.. Unlike the existing crowd counting datasets with images of hundreds/thousands of pedestrians and nearly all the images being taken outdoors, SmartCity has few pedestrians in images and consists of both outdoor and indoor scenes: the average number of pedestrians is only 7.4 with minimum being 1 and maximum being 14.
Source: SmartCity"	https://paperswithcode.com/dataset/smartcity	13/11/2017						
1367	ErhuPT	"This dataset is an audio dataset containing about 1500 audio clips recorded by multiple professional players.
Source: Zenodo"	https://paperswithcode.com/dataset/erhupt	20/10/2019	Erhu Playing Technique Dataset					
1368	VideoNavQA	"The VideoNavQA dataset contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the Embodied Question Answering (EQA) task.
VideoNavQA contains approximately 101,000 pairs of videos and questions, 28 types of questions belonging to 8 categories, with 70 possible answers. Each question type is
associated with a template that facilitates programmatic generation using ground truth information extracted from the video. The complexity of the questions in the dataset is far beyond that of other similar tasks using this generation method (such as CLEVR): the questions involve single or multiple object/room existence, object/room counting, object color recognition and localization, spatial reasoning, object/room size comparison and equality of object attributes (color, room location).
Source: VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering"	https://paperswithcode.com/dataset/videonavqa	14/08/2019						
1369	OSTD	"This dataset consists of 18 movies with duration range between 10 and 104 minutes leveraged from the OVSD dataset (Rotman et al., 2016). For these videos, the summary length limit is set to be the minimum between 4 minutes and 10% of the video length.
Source: ILS-SUMM: ITERATED LOCAL SEARCH FOR UNSUPERVISED VIDEO SUMMARIZATION"	https://paperswithcode.com/dataset/ostd	08/12/2019	Open-Source-Total-Distance					
1370	PolarRR	"PolarRR is a new dataset with more than 100 types of glass in which obtained transmission images are perfectly aligned with input mixed images.
Source: Polarized Reflection Removal with Perfect Alignment in the Wild"	https://paperswithcode.com/dataset/polarrr	28/03/2020						
1371	Lytro Illum	"Lytro Illum is a new light field dataset using a Lytro Illum camera. 640 light fields are collected with significant variations in terms of size, textureness, background clutter and illumination, etc. Micro-lens image arrays and central viewing images are generated, and corresponding ground-truth maps are produced.
Source: Lytro Illum"	https://paperswithcode.com/dataset/lytro-illum	19/06/2019						
1372	UFPR-Eyeglasses	"The UFPR-Eyeglasses dataset has 1,135 images of both eyes (2,270 cropped images of each eye) from 83 subjects (166 classes). The dataset is used to evaluate the effect of the occlusion caused by eyeglasses in periocular recognition.
Source: Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization"	https://paperswithcode.com/dataset/ufpr-eyeglasses	10/02/2020						
1373	Circa	"The Circa (meaning ‘approximately’) dataset aims to help machine learning systems to solve the problem of interpreting indirect answers to polar questions.
The dataset contains pairs of yes/no questions and indirect answers, together with annotations for the interpretation of the answer. The data is collected in 10 different social conversational situations (eg. food preferences of a friend). Examples:
```
Q: Are you vegan?
A: I love burgers too much. [No]
Q: Do you like spicy food?
A: I put hot sauce on everything. [Yes] 
Q: Would you like to go see live music?
A: If it’s not too crowded. [Yes, upon a condition]
```
Currently, the Circa annotations focus on a few classes such as ‘yes’, ‘no’ and ‘yes, upon condition’. The data can be used to build machine learning models which can replicate these classes on new question-answer pairs, and allow evaluation of methods for doing so.
Source: Circa"	https://paperswithcode.com/dataset/circa	07/10/2020						
1374	QUVA Repetition	"QUVA Repetition dataset consists of 100 videos displaying a wide variety of repetitive video dynamics, including swimming, stirring, cutting, combing and music-making. All videos have been annotated with individual cycle bounds and a total repetition count.
Source: Real-world Repetition Estimation by Div, Grad and Curl"	https://paperswithcode.com/dataset/quva-repetition	27/02/2018						
1375	NH-HAZE	"NN-HAZE is an image dehazing dataset. Since in many real cases haze is not uniformly distributed NH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and corresponding haze-free images. This is the first non-homogeneous image dehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has been introduced in the scene using a professional haze generator that imitates the real conditions of hazy scenes.
Source: NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and Haze-Free Images"	https://paperswithcode.com/dataset/nh-haze	07/05/2020						
1376	PVDN	"PVDN is a dataset of vehicle detection at night, using light reflections caused by their headlamps. It contains 59,746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e. g., headlamps), and their respective light reflections (e. g., light reflections on guardrails) are labeled. With this information, this dataset enables research into new methods of detecting oncoming vehicles based on the light reflections they cause, long before they are directly visible. 
Source: Provident Vehicle Detection at Night: The PVDN Dataset"	https://paperswithcode.com/dataset/pvdn	31/12/2020	Provident Vehicle Detection at Night					
1377	JEC-QA	"JEC-QA is a LQA (Legal Question Answering) dataset collected from the National Judicial Examination of China. It contains 26,365 multiple-choice and multiple-answer questions in total. The task of the dataset is to predict the answer using the questions and relevant articles. To do well on JEC-QA, both retrieving and answering are important.
Source: JEC-QA Website"	https://paperswithcode.com/dataset/jec-qa	27/11/2019						
1378	TrashCan	"The TrashCan dataset is an instance-segmentation dataset of underwater trash. It is comprised of annotated images (7,212 images) which contain observations of trash, ROVs, and a wide variety of undersea flora and fauna. The annotations in this dataset take the format of instance segmentation annotations: bitmaps containing a mask marking which pixels in the image contain each object. The imagery in TrashCan is sourced from the J-EDI (JAMSTEC E-Library of Deep-sea Images) dataset, curated by the Japan Agency of Marine Earth Science and Technology (JAMSTEC). 
Source: TrashCan 1.0 An Instance-Segmentation Labeled Dataset of Trash Observations
Image Source: https://conservancy.umn.edu/handle/11299/214865"	https://paperswithcode.com/dataset/trashcan	16/07/2020						
1379	UCLA Aerial Event Dataset	"The UCLA Aerial Event Dataest has been captured by a low-cost hex-rotor with a GoPro camera, which is able to eliminate the high frequency vibration of the camera and hold in air autonomously through a GPS and a barometer. It can also fly 20 ∼ 90m above the ground and stays 5 minutes in air. 
This hex-rotor has been used to take the set of videos in the dataset, captured in different places: hiking routes, parking lots, camping sites, picnic areas with shelters, restrooms, tables, trash bins and BBQ ovens. By detecting/tracking humans and objects in the videos, the videos can be annotated with events.
The original videos are pre-processed, including camera calibration and frame registration. After pre-processing, there are totally 27 videos in the dataset, the length of which ranges from 2 minutes to 5 minutes. Each video has annotations with hierarchical semantic information of objects, roles, events and groups in the videos.
Source: Joint Inference of Groups, Events and Human Roles in Aerial Videos"	https://paperswithcode.com/dataset/ucla-aerial-event-dataset	22/05/2015						
1380	CQASUMM	"CQASUMM is a dataset for CQA (Community Question Answering) summarization, constructed from the 4.4 million Yahoo! Answers L6 dataset. The dataset contains ~300k annotated samples.
Source: CQASUMM"	https://paperswithcode.com/dataset/cqasumm	12/11/2018						
1381	NeuralNews	"NeuralNews is a dataset for machine-generated news detection. It consists of human-generated and machine-generated articles. The human-generated articles are extracted from the GoodNews dataset, which is extracted from the New York Times. It contains 4 types of articles:

Real Articles and Real Captions
Real Articles and Generated Captions
Generated Articles and Real Captions
Generated Articles and Generated Captions

In total, it contains about 32K samples of each article type (resulting in about 128K total).
Source: Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News"	https://paperswithcode.com/dataset/neuralnews	16/09/2020						
1382	EyeCar	"EyeCar is a dataset of driving videos of vehicles involved in rear-end collisions paired with eye fixation data captured from human subjects. It contains 21 front-view videos that were captured in various traffic, weather, and day light conditions. Each video is 30sec in length and contains typical driving tasks (e.g., lanekeeping, merging-in, and braking) ending to rear-end collisions.
Source: MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy Deep Inverse Reinforcement Learning"	https://paperswithcode.com/dataset/eyecar	17/12/2019						
1383	WordNet-feelings	"WordNet-feelings, is an affective dataset that identifies 3664 word senses as feelings, and associates each of these with one of the 9 categories of feeling. The 9 different categories are: Actions, Anger, Attention, Attraction, Hedonics, Other, Physiological, Social, Wellbeing.
Source: WordNet-feelings: A linguistic categorisation of human feelings"	https://paperswithcode.com/dataset/wordnet-feelings	06/11/2018						
1384	Doc3DShade	"Doc3DShade extends Doc3D with realistic lighting and shading. Follows a similar synthetic rendering procedure using captured document 3D shapes but final image generation step combines real shading of different types of paper materials under numerous illumination conditions.
Source: Doc3DShade"	https://paperswithcode.com/dataset/doc3dshade	29/11/2020						
1385	Deep Fakes Dataset	"The Deep Fakes Dataset is a collection of ""in the wild"" portrait videos for deepfake detection. The videos in the dataset are diverse real-world samples in terms of the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context. They originate from various sources such as news articles, forums, apps, and research presentations; totalling up to 142 videos, 32 minutes, and 17 GBs. Synthetic videos are matched with their original counterparts when possible. 
Source: Deepfakes dataset"	https://paperswithcode.com/dataset/deep-fakes-dataset	08/01/2019	inamibora					
1386	GSL	"Dataset Description
The Greek Sign Language (GSL) is a large-scale RGB+D dataset, suitable for Sign Language Recognition (SLR) and Sign Language Translation (SLT). The video captures are conducted using an Intel RealSense D435 RGB+D camera at a rate of 30 fps. Both the RGB and the depth streams are acquired in the same spatial resolution of 848×480 pixels. To increase variability in the videos, the camera position and orientation is slightly altered within subsequent recordings. Seven different signers are employed to perform 5 individual and commonly met scenarios in different public services. The average length of each scenario is twenty sentences.
The dataset contains 10,290 sentence instances, 40,785 gloss instances, 310 unique glosses (vocabulary size) and 331 unique sentences, with 4.23 glosses per sentence on average. Each signer is asked to perform the pre-defined dialogues five consecutive times. In all cases, the simulation considers a deaf person communicating with a single public service employee. The involved signer performs the sequence of glosses of both agents in the discussion. For the annotation of each gloss sequence, GSL linguistic experts are involved. The given annotations are at individual gloss and gloss sequence level. A translation of the gloss sentences to spoken Greek is also provided.
Evaluation
The GSL dataset includes the 3 evaluation setups:


Signer-dependent continuous sign language recognition (GSL SD) – roughly 80% of videos are used for training, corresponding to 8,189 instances. The rest 1,063 (10%) were kept for validation and 1,043 (10%) for testing.


Signer-independent continuous sign language recognition (GSL SI) – the selected test gloss sequences are not used in the training set, while all the individual glosses exist in the training set. In GSL SI, the recordings of one signer are left out for validation and testing (588 and 881 instances, respectively). The rest 8821 instances are utilized for training.


Isolated gloss sign language recognition (GSL isol.) – The validation set consists of 2,231 gloss instances, the test set 3,500, while the remaining 34,995 are used for training. All 310 unique glosses are seen in the training set.


For more info and results, advice our paper
Paper Abstract: A Comprehensive Study on Sign Language Recognition Methods, Adaloglou et al. 2020
In this paper, a comparative experimental assessment of computer vision-based methods for sign language recognition is conducted. By implementing the most recent deep neural network methods in this field, a thorough evaluation on multiple publicly available datasets is performed. The aim of the present study is to provide insights on sign language recognition, focusing on mapping non-segmented video streams to glosses. For this task, two new sequence training criteria, known from the fields of speech and scene text recognition, are introduced. Furthermore, a
plethora of pretraining schemes are thoroughly discussed. Finally, a new RGB+D dataset for the Greek sign language is created. To the best of our knowledge, this is the first sign language dataset where sentence and gloss level annotations are provided for every video capture.
Arxiv link"	https://paperswithcode.com/dataset/gsl	24/07/2020	Greek Sign Language					
1387	SketchyScene	"SketchyScene is a large-scale dataset of scene sketches to advance research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. 
Source: SketchyScene: Richly-Annotated Scene Sketches
Image Source: Zoe et al"	https://paperswithcode.com/dataset/sketchyscene	07/08/2018						
1388	ECHR	"ECHR is an English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). The dataset contains ~11.5k cases, including the raw text.
For each case, the dataset provides a list of facts extracted using regular expressions from the case description. Each case is also mapped to articles of the Convention that were violated (if any). An importance score is also assigned by ECHR.
Source: Neural Legal Judgment Prediction in English"	https://paperswithcode.com/dataset/echr	05/06/2019						
1389	AmazonQA	"AmazonQA consists of 923k questions, 3.6M answers and 14M reviews across 156k products. Building on the well-known Amazon dataset, additional annotations are collected, marking each question as either answerable or unanswerable based on the available reviews. 
Source: AmazonQA: A Review-Based Question Answering Task"	https://paperswithcode.com/dataset/amazonqa	12/08/2019						
1390	emrQA	"emrQA has 1 million question-logical form and 400,000+ questionanswer evidence pairs.
Source: emrQA: A Large Corpus for Question Answering on Electronic Medical Records
Image Source: https://arxiv.org/pdf/1809.00732v1.pdf"	https://paperswithcode.com/dataset/emrqa	03/09/2018						
1391	SuperGLUE	"SuperGLUE is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE. SuperGLUE has the same high-level motivation as GLUE: to provide a simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English. SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number
performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:

More challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks were identified from those submitted to an open call for task proposals and were selected based on difficulty for current NLP approaches.
More diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair classification. The authors expand the set of task formats in SuperGLUE to include
coreference resolution and question answering (QA).
Comprehensive human baselines: the authors include human performance estimates for all benchmark tasks, which verify that substantial headroom exists between a strong BERT-based baseline and human performance.
Improved code support: SuperGLUE is distributed with a new, modular toolkit for work on pretraining, multi-task learning, and transfer learning in NLP, built around standard tools including PyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).
Refined usage rules: The conditions for inclusion on the SuperGLUE leaderboard were revamped to ensure fair competition, an informative leaderboard, and full credit
assignment to data and task creators."	https://paperswithcode.com/dataset/superglue	02/05/2019	SuperGLUE					
1392	TurkQA	"TurkQA consists of a selection of sentences from English Wikipedia articles, with questions and answers crowdsourced from workers on Amazon Mechanical Turk.
Source: TurkQA"	https://paperswithcode.com/dataset/turkqa	01/08/2013						
1393	XTREME	"The Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark was introduced to encourage more research on multilingual transfer learning,. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics.
The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. The languages in XTREME are selected to maximize language diversity, coverage in existing tasks, and availability of training data. Among these are many under-studied languages, such as the Dravidian languages Tamil (spoken in southern India, Sri Lanka, and Singapore), Telugu and Malayalam (spoken mainly in southern India), and the Niger-Congo languages Swahili and Yoruba, spoken in Africa."	https://paperswithcode.com/dataset/xtreme	01/01/2020	Cross-Lingual Transfer Evaluation of Multilingual Encoders					
1394	WikiMovies	"WikiMovies is a dataset for question answering for movies content. It contains ~100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb),
Source:  WikiMovies"	https://paperswithcode.com/dataset/wikimovies	09/06/2016						
1395	MDD	"Movie Dialog dataset (MDD) is designed to measure how well models can perform at goal and non-goal orientated dialog centered around the topic of movies (question answering, recommendation and discussion).
Source: Movie Dialog dataset"	https://paperswithcode.com/dataset/mdd	21/11/2015	Movie Dialog dataset					
1396	CBT	"Children’s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg. 
Source: CBT
Image Source: https://research.fb.com/downloads/babi/"	https://paperswithcode.com/dataset/cbt	07/11/2015	Children’s Book Test					
1397	Dialog-based Language Learning dataset	"Dialog-based Language Learning dataset is designed to measure how well models can perform at learning as a student given a teacher’s textual responses to the student’s answer (as well as potentially receiving an external real-valued reward signal). 
Source: Dialog-based Language Learning dataset"	https://paperswithcode.com/dataset/dialog-based-language-learning-dataset	01/12/2016						
1398	TyDiQA-GoldP	"TyDiQA is the gold passage version of the Typologically Diverse Question Answering (TyDiWA) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3× and 2× less lexical overlap compared to XQuAD and MLQA respectively.
Source: XTREME"	https://paperswithcode.com/dataset/tydiqa-goldp	10/03/2020	TyDiQA-GoldP					
1399	WikiReading	"WikiReading is a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs).
Source: WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia
Image Source: Hewlett et al"	https://paperswithcode.com/dataset/wikireading	11/08/2016						
1400	Tatoeba	"The Tatoeba dataset consists of up to 1,000 English-aligned sentence pairs covering 122 languages.
Image Source: https://arxiv.org/pdf/1812.10464v2.pdf"	https://paperswithcode.com/dataset/tatoeba	26/12/2018	Tatoeba					
1401	DeeperForensics-1.0	"DeeperForensics-1.0 represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. The full dataset includes 48,475 source videos and 11,000 manipulated videos. The source videos are collected on 100 paid and consented actors from 26 countries, and the manipulated videos are generated by a newly proposed many-to-many end-to-end face swapping method, DF-VAE. 7 types of real-world perturbations at 5 intensity levels are employed to ensure a larger scale and higher diversity.
Image Source: https://github.com/EndlessSora/DeeperForensics-1.0"	https://paperswithcode.com/dataset/deeperforensics-1-0	09/01/2020	DeeperForensics-1.0					
1402	WikiSuggest	"To collect WikiSuggest, Google Suggest API is used to harvest natural language questions and submit them to Google Search. Whenever Google Search returns a box with a short answer from Wikipedia, an example from the question, answer, and the Wikipedia document are created. If the answer string is missing from the document this often implies a spurious question-answer pair, such as (‘what time is half time in rugby’, ‘80 minutes, 40 minutes’). Question-answer pairs without the exact answer string are pruned. Fifty examples after filtering are examined and 54% were found to be well-formed question-answer pairs where answers in the document can be grounded, 20% contained answers without textual evidence in the document (the answer string exists in an irreleveant context), and 26% contain incorrect QA pairs.
Source: Coarse-to-Fine Question Answering for Long Documents"	https://paperswithcode.com/dataset/wikisuggest	01/07/2017						
1403	FineGym	"FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a ""balance beam"" event will be annotated as a sequence of elementary sub-actions derived from five sets: ""leap-jumphop"", ""beam-turns"", ""flight-salto"", ""flight-handspring"", and ""dismount"", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes."	https://paperswithcode.com/dataset/finegym	14/04/2020	FineGym					
1404	Shmoop Corpus	"Shmoop Corpus is a dataset of 231 stories that are paired with detailed multi-paragraph summaries for each individual chapter (7,234 chapters), where the summary is chronologically aligned with respect to the story chapter. From the corpus, a set of common NLP tasks are constructed, including Cloze-form question answering and a simplified form of abstractive summarization, as benchmarks for reading comprehension on stories.
Source: Shmoop Corpus"	https://paperswithcode.com/dataset/shmoop-corpus	30/12/2019						
1405	BookTest	"BookTest is a new dataset similar to the popular Children’s Book Test (CBT), however more than 60 times larger.
Source: BookTest"	https://paperswithcode.com/dataset/booktest	04/10/2016						
1406	MovieNet	MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style.	https://paperswithcode.com/dataset/movienet	21/07/2020	MovieNet					
1407	DREAM	"DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding.
DREAM contains 10,197 multiple choice questions for 6,444 dialogues, collected from English-as-a-foreign-language examinations designed by human experts. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge.
Source: DREAM"	https://paperswithcode.com/dataset/dream	01/03/2019						
1408	MessyTable	"MessyTable features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics. The dataset challenges existing methods in mining subtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association.
There are 50,211 images and 5,579 scenes in the dataset."	https://paperswithcode.com/dataset/messytable	29/07/2020	MessyTable					
1409	MCTest	"MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. 
MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.
Source: MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text
Image Source: Richardson et al"	https://paperswithcode.com/dataset/mctest	01/10/2013						
1410	TweetQA	"With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effectiveness of many applications that rely on real-time knowledge. While previous question answering (QA) datasets have concentrated on formal text like news and Wikipedia, the first large-scale dataset for QA over social media data is presented. To make sure the tweets are meaningful and contain interesting information, tweets used by journalists to write news articles are gathered. Then human annotators are asked to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, the answer are allowed to be abstractive. The task requires model to read a short tweet and a question and outputs a text phrase (does not need to be in the tweet) as the answer.
Source: TWEETQA: A Social Media Focused Question Answering Dataset"	https://paperswithcode.com/dataset/tweetqa	14/07/2019						
1411	UCF Sports	"The UCF Sports dataset consists of a set of actions collected from various sports which are typically featured on broadcast television channels such as the BBC and ESPN. The video sequences were obtained from a wide range of stock footage websites including BBC Motion gallery and GettyImages.
The dataset includes a total of 150 sequences with the resolution of 720 x 480. The collection represents a natural pool of actions featured in a wide range of scenes and viewpoints.
Source: UCF Sports Action"	https://paperswithcode.com/dataset/ucf-sports							
1412	MSRA-B	"The MSRA-B dataset is a dataset for salient object detection. It contains 5,000 images with a variety of image contents. Most of the images have a single salient object. There is a large variation among images including natural scenes, animals, indoor, outdoor, etc.
Source: Deep Contrast Learning for Salient Object Detection"	https://paperswithcode.com/dataset/msra-b							
1413	VOT2015	"VOT2015 is a visual object tracking dataset. The dataset comprises 60 short sequences showing various objects in challenging backgrounds. The sequences were chosen from a large pool of sequences from different sources.
Source: VOT2015"	https://paperswithcode.com/dataset/vot2015		Visual Object Tracking Challenge 2015					
1414	VOT2014	"The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool.
Source: VOT2014"	https://paperswithcode.com/dataset/vot2014		Visual Object Tracking Challenge 2014					
1415	UCF50	"UCF50 is an action recognition data set with 50 action categories, consisting of realistic videos taken from youtube. This data set is an extension of YouTube Action data set (UCF11) which has 11 action categories.
UCF50 data set's 50 action categories collected from youtube are: Baseball Pitch, Basketball Shooting, Bench Press, Biking, Biking, Billiards Shot,Breaststroke, Clean and Jerk, Diving, Drumming, Fencing, Golf Swing, Playing Guitar, High Jump, Horse Race, Horse Riding, Hula Hoop, Javelin Throw, Juggling Balls, Jump Rope, Jumping Jack, Kayaking, Lunges, Military Parade, Mixing Batter, Nun chucks, Playing Piano, Pizza Tossing, Pole Vault, Pommel Horse, Pull Ups, Punch, Push Ups, Rock Climbing Indoor, Rope Climbing, Rowing, Salsa Spins, Skate Boarding, Skiing, Skijet, Soccer Juggling, Swing, Playing Tabla, TaiChi, Tennis Swing, Trampoline Jumping, Playing Violin, Volleyball Spiking, Walking with a dog, and Yo Yo.
Source: UCF50"	https://paperswithcode.com/dataset/ucf50							
1416	MSD	"The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks.
The core of the dataset is the feature analysis and metadata for one million songs, provided by The Echo Nest. The dataset does not include any audio, only the derived features. Note, however, that sample audio can be fetched from services like 7digital, using code provided by the authors.
Source: http://millionsongdataset.com/
Paper: The Million Song Dataset"	https://paperswithcode.com/dataset/msd		Million Song Dataset					
1417	CASME II	"The Chinese Academy of Sciences Micro-Expression dataset (CASME II) consists of 255 videos, elicited from 26 participants. The videos are recorded using Point Gray GRAS-03K2C camera which has a frame rate of 200fps. The average video length is 0.34s, equivalent to 68 frames. Each video’s emotion label is annotated by two coders, where the reliability is 0.846. 
All the images are cropped to 170×140 pixels. The ground-truth information provided by the database include the emotion state, the action unit, the onset, apex and offset frame indices. The videos are grouped into seven categories: others (99 videos), disgust (63 videos), happiness (32 videos), repression (27 videos), surprise (25 videos), sadness (7 videos) and fear (2 videos).
Source: OFF-ApexNet on Micro-expression Recognition System"	https://paperswithcode.com/dataset/casme-ii		Chinese Academy of Sciences Micro-Expression II					
1418	UCF-Crime	"The UCF-Crime dataset is a large-scale dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety. 
This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities.
Source: Video Anomaly Dection Dataset"	https://paperswithcode.com/dataset/ucf-crime	12/01/2018						
1419	VOT2013	"The dataset comprises 16 short sequences showing various objects in challenging backgrounds. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 16 sequences sample evenly well the existing pool. The sequences were annotated by the VOT committee using axis-aligned bounding boxes.
Source: VOT2013"	https://paperswithcode.com/dataset/vot2013		Visual Object Tracking Challenge 2013					
1420	Medical Segmentation Decathlon	"The Medical Segmentation Decathlon is a collection of medical image segmentation datasets. It contains a total of 2,633 three-dimensional images collected across multiple anatomies of interest, multiple modalities and multiple sources. Specifically, it contains data for the following body organs or parts: Brain, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen and Colon.
Source: A large annotated medical image dataset for the development and evaluation of segmentation algorithms
Image Source: Simpson et al"	https://paperswithcode.com/dataset/medical-segmentation-decathlon	25/02/2019						
1421	CrossNER	"CrossNER is a cross-domain NER (Named Entity Recognition) dataset, a fully-labeled collection of NER data spanning over five diverse domains (Politics, Natural Science, Music, Literature, and Artificial Intelligence) with specialized entity categories for different domains. Additionally, CrossNER also includes unlabeled domain-related corpora for the corresponding five domains. 
Source: CrossNER"	https://paperswithcode.com/dataset/crossner	08/12/2020						
1422	11k Hands	"A large dataset of human hand images (dorsal and palmar sides) with detailed ground-truth information for gender recognition and biometric identification.
Source: 11K Hands: Gender recognition and biometric identification using a large dataset of hand images"	https://paperswithcode.com/dataset/11k-hands	12/11/2017						
1423	2-PM Vessel Dataset	"2-PM Vessel is an open-source volumetric brain vasculature dataset obtained with two-photon microscopy at Focused Ultrasound Lab, at Sunnybrook Research Institute (affiliated with University of Toronto by Dr. Alison Burgess, Charissa Poon and Marc Santos. The dataset contains a total of 12 volumetric stacks consisting of images of mouse brain vasculature and tumour vasculature.
Source: https://github.com/petteriTeikari/vesselNN_dataset
Image Source: Teikari et al"	https://paperswithcode.com/dataset/2-pm-vessel-dataset							
1424	Placepedia	Placepedia contains 240K places with 35M images from all over the world. Each place is associated with its district, city/town/village, state/province, country, continent, and a large amount of diverse photos. Both administrative areas and places have rich side information, e.g. discription, population, category, function. In addition, two cleaned subsets (Places-Coarse and Places-Fine) for experiments are provided.	https://paperswithcode.com/dataset/placepedia	07/07/2020	Placepedia					
1425	2WikiMultiHopQA	"Uses structured and unstructured data. The dataset introduces the evidence information containing a reasoning path for multi-hop questions.
Source: Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps"	https://paperswithcode.com/dataset/2wikimultihopqa							
1426	30MQA	"An enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions.
Source: Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus"	https://paperswithcode.com/dataset/30mqa		30M Factoid Question-Answer Corpus					
1427	360-SOD	"360-SOD contains 500 high-resolution equirectangular images.
Source: Distortion-adaptive Salient Object Detection in 360$^\circ$ Omnidirectional Images
Image Source: http://cvteam.net/projects/JSTSP20_DDS/DDS.html"	https://paperswithcode.com/dataset/360-sod							
1428	3D60	"Collects high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. 
Source: OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas"	https://paperswithcode.com/dataset/3d60							
1429	3D Hand Pose	"3D Hand Pose is a multi-view hand pose dataset consisting of color images of hands and different kind of annotations for each: the bounding box and the 2D and 3D location on the joints in the hand. 
Source: Large-scale Multiview 3D Hand Pose Dataset"	https://paperswithcode.com/dataset/3d-hand-pose							
1430	3D Ken Burns Dataset	"Provides a large-scale synthetic dataset which contains accurate ground truth depth of various photo-realistic scenes.
Source: 3D Ken Burns Effect from a Single Image"	https://paperswithcode.com/dataset/3d-ken-burns-dataset							
1431	3DMAD	"The 3D Mask Attack Database (3DMAD) is a biometric (face) spoofing database. It currently contains 76500 frames of 17 persons, recorded using Kinect for both real-access and spoofing attacks. Each frame consists of:

a depth image (640x480 pixels – 1x11 bits)
the corresponding RGB image (640x480 pixels – 3x8 bits)
manually annotated eye positions (with respect to the RGB image).

Source: 3D Mask Attack Dataset"	https://paperswithcode.com/dataset/3dmad		3D Mask Attack Dataset					
1432	3DPeople Dataset	"A large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits.
Source: 3DPeople: Modeling the Geometry of Dressed Humans"	https://paperswithcode.com/dataset/3dpeople-dataset							
1433	3DSeg-8	The 3DSeg-8 is a collection of several publicly available 3D segmentation datasets from different medical imaging modalities, e.g. magnetic resonance imaging (MRI) and computed tomography (CT), with various scan regions, target organs and pathologies.	https://paperswithcode.com/dataset/3dseg-8							
1434	3D-ZeF	"3D-ZeF dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes.
Source: 3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset"	https://paperswithcode.com/dataset/3d-zef		3D ZebraFish Tracking Benchmark					
1435	3RScan	"A novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. 
Source: RIO: 3D Object Instance Re-Localization in Changing Indoor Environments"	https://paperswithcode.com/dataset/3r-scan							
1436	4Seasons	"4Seasons is adataset covering seasonal and challenging perceptual conditions for autonomous driving.
Source: 4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous Driving
Image Source: Wenzel et al"	https://paperswithcode.com/dataset/4seasons							
1437	A2D2	"Audi Autonomous Driving Dataset (A2D2) consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus.
Source: A2D2: Audi Autonomous Driving Dataset
Image Source: https://www.a2d2.audi/a2d2/en/dataset.html"	https://paperswithcode.com/dataset/a2d2		Audi Autonomous Driving Dataset					
1438	A*3D	"The A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world.
Characteristics:
* 230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images.
* Captured at different times (day, night) and weathers (sun, cloud, rain).
Source: https://github.com/I2RDL2/ASTAR-3D
Image Source: https://github.com/I2RDL2/ASTAR-3D"	https://paperswithcode.com/dataset/a-3d							
1439	Aachen Day-Night	"Aachen Day-Night is a dataset designed for benchmarking 6DOF outdoor visual localization in changing conditions. It focuses on localizing high-quality night-time images against a day-time 3D model. There are 14,607 images with changing conditions of weather, season and day-night cycles.
Source: Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions"	https://paperswithcode.com/dataset/aachen-day-night							
1440	AADB	"Contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. 
Source: Photo Aesthetics Ranking Network with Attributes and Content Adaptation"	https://paperswithcode.com/dataset/aadb							
1441	AAVE/SAE Paired Dataset	"AAVE/SAE Paired Dataset contains 2019 intent-equivalent AAVE/SAE pairs. The AAVE (African-American Vernacular English) samples are sampled from Blodgett et. al. (2016)'s TwitterAAE, with their corresponding SAE (Standard American English) samples annotated by Amazon MTurk.
Source: https://github.com/sophiegroenwold/AAVE_SAE_dataset"	https://paperswithcode.com/dataset/aave-sae-paired-dataset							
1442	ABC Dataset	The ABC Dataset is a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms.	https://paperswithcode.com/dataset/abc-dataset-1							
1443	ACL ARC	"ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:

files that look like not full papers, paper fragments, foreign-language papers (e.g., French), or pure junk.
headers (title and author information; NOT abstract).
footers (""References"" line and the actual references).
some bad characters (spurious characters).
some page numbers (i.e., a single number appearing on a line, with nothing else attached to it).
significant foreign-language (e.g., French) content in an otherwise English paper.

The cleaned corpus has 10,628 documents.
Source: ACL ARC"	https://paperswithcode.com/dataset/acl-arc-1							
1444	ACRONYM	"A dataset for robot grasp planning based on physics simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872 objects from 262 different categories, each labeled with the grasp result obtained from a physics simulator. 
Source: ACRONYM: A Large-Scale Grasp Dataset Based on Simulation"	https://paperswithcode.com/dataset/acronym							
1445	Acronym Identification	"Is an acronym disambiguation (AD) dataset for scientific domain with 62,441 samples which is significantly larger than the previous scientific AD dataset.
Source: What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation"	https://paperswithcode.com/dataset/acronym-identification							
1446	ActioNet	"ActioNet is a video task-based dataset collected in a synthetic 3D environment. It contains 3,038 annotated videos and hierarchical task structures over 65 individual household tasks from 120 different scenes. Each task is annotated across three to five different scenes by 10 different annotators. The tasks can be broken down into four categories: living room, bedroom, bathroom, kitchen.
Source: https://github.com/SamsonYuBaiJian/actionet"	https://paperswithcode.com/dataset/actionet							
1447	ActivityNet Entities	"ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase. This allows training video description models with this data, and importantly, evaluate how grounded or ""true"" such model are to the video they describe.
Source: https://github.com/facebookresearch/ActivityNet-Entities
Image Source: https://github.com/facebookresearch/ActivityNet-Entities"	https://paperswithcode.com/dataset/activitynet-entities-1							
1448	ActivityNet-QA	"The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning.
Source: ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"	https://paperswithcode.com/dataset/activitynet-qa							
1449	ActivityNet Thumbnails	"Consists of 10,000+ video-sentence pairs with each accompanied by an annotated sentence specified video thumbnail. 
Source: Sentence Specified Dynamic Video Thumbnail Generation"	https://paperswithcode.com/dataset/activitynet-thumbnails							
1450	ADHA	"ADHA: “Adverbs Describing Human Actions” is the first benchmark for a new problem — recognizing human action adverbs (HAA). This is the first step for computer vision to change over from pattern recognition to real AI. Some key features of ADHA are: a semantically complete set of adverbs describing human actions, a set of common, describable human actions, and an exhaustive labeling of simultaneously emerging actions in each video.
Source: ADHA"	https://paperswithcode.com/dataset/adha							
1451	ADL Piano MIDI	"The ADL Piano MIDI is a dataset of 11,086 piano pieces from different genres. This dataset is based on the Lakh MIDI dataset, which is a collection on 45,129 unique MIDI files that have been matched to entries in the Million Song Dataset. Most pieces in the Lakh MIDI dataset have multiple instruments, so for each file the authors of ADL Piano MIDI dataset extracted only the tracks with instruments from the ""Piano Family"" (MIDI program numbers 1-8). This process generated a total of 9,021 unique piano MIDI files. Theses 9,021 files were then combined with other approximately 2,065 files scraped from publicly-available sources on the internet. All the files in the final collection were de-duped according to their MD5 checksum.
Source: ADL Piano MIDI"	https://paperswithcode.com/dataset/adl-piano-midi							
1452	Advice Seeking Questions	"The Advice-Seeking Questions (ASQ) dataset is a collection of personal narratives with advice-seeking questions. The dataset has been split into train, test, heldout sets, with 8865, 2500, 10000 test instances each. This dataset is used to train and evaluate methods that can infer what is the advice-seeking goal behind a personal narrative. This task is formulated as a cloze test, where the goal is to identify which of two advice-seeking questions was removed from a given narrative.
Source: https://github.com/CornellNLP/ASQ"	https://paperswithcode.com/dataset/advice-seeking-questions							
1453	ADVIO	"Provides a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track. 
Source: ADVIO: An authentic dataset for visual-inertial odometry"	https://paperswithcode.com/dataset/advio							
1454	AeroRIT	"AeroRIT is a hyperspectral dataset to facilitate aerial hyperspectral scene understanding.
Source: AeroRIT: A New Scene for Hyperspectral Image Analysis
Image Source: https://github.com/aneesh3108/AeroRIT"	https://paperswithcode.com/dataset/aerorit							
1455	AESLC	"To study the task of email subject line generation: automatically generating an email subject line from the email body. 
Source: This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation"	https://paperswithcode.com/dataset/aeslc							
1456	Aesthetics Text Corpus	"An exhaustive list of stop lemmas created from 12 corpora across multiple domains, consisting of over 13 million words, from which more than 200,000 lemmas were generated, and 11 publicly available stop word lists comprising over 1000 words, from which nearly 400 unique lemmas were generated.
Source: Novel Language Resources for Hindi: An Aesthetics Text Corpus and a Comprehensive Stop Lemma List"	https://paperswithcode.com/dataset/aesthetics-text-corpus							
1457	Affective Text	"Affective Text (Test Corpus of SemEval 2007) by Carlo Strapparava & Rada Mihalcea.
Source: Affective Text"	https://paperswithcode.com/dataset/affective-text							
1458	Aff-Wild	"Aff-Wild is a dataset for emotion recognition from facial images in a variety of head poses, illumination conditions and occlusions.
Source: Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond"	https://paperswithcode.com/dataset/aff-wild							
1459	Aff-Wild2	"Aff-Wild2 is an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. 
Source: Aff-Wild2: Extending the Aff-Wild Database for Affect Recognition"	https://paperswithcode.com/dataset/aff-wild2							
1460	AfroMNIST	"A set of synthetic MNIST-style datasets for four orthographies used in Afro-Asiatic and Niger-Congo languages: Ge`ez (Ethiopic), Vai, Osmanya, and N'Ko. These datasets serve as ""drop-in"" replacements for MNIST. 
Source: Afro-MNIST: Synthetic generation of MNIST-style datasets for low-resource languages"	https://paperswithcode.com/dataset/afromnist							
1461	Agriculture-Vision	"A large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. Collects 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. 
Source: Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis"	https://paperswithcode.com/dataset/agriculture-vision							
1462	AGRR-2019	"Consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. 
Source: AGRR-2019: A Corpus for Gapping Resolution in Russian"	https://paperswithcode.com/dataset/agrr-2019							
1463	AI2D-RST	"AI2D-RST is a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelligence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. 
Source: AI2D-RST: A multimodal corpus of 1000 primary school science diagrams"	https://paperswithcode.com/dataset/ai2d-rst							
1464	AIDER	"Dataset aimed to do automated aerial scene classification of disaster events from on-board a UAV.
Source: Deep-Learning-Based Aerial Image Classification for Emergency Response Applications Using Unmanned Aerial Vehicles"	https://paperswithcode.com/dataset/aider							
1465	AIRS	"The AIRS (Aerial Imagery for Roof Segmentation) dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. 
Source: Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards Automatic Mapping of Buildings"	https://paperswithcode.com/dataset/airs							
1466	AirSim	"AirSim is a simulator for drones, cars and more, built on Unreal Engine. It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, there exists an experimental version for a Unity plugin.
Source: AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles"	https://paperswithcode.com/dataset/airsim							
1467	AISHELL-1	"AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin. 
Source: AISHELL-1: An Open-Source Mandarin Speech Corpus and A Speech Recognition Baseline"	https://paperswithcode.com/dataset/aishell-1							
1468	AISHELL-2	"AISHELL-2 contains 1000 hours of clean read-speech data from iOS is free for academic usage. 
Source: AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale
Image Source: https://arxiv.org/pdf/1808.10583v2.pdf"	https://paperswithcode.com/dataset/aishell-2							
1469	Alexa Point of View	"The Alexa Point of View dataset is point of view conversion dataset, a parallel corpus of messages spoken to a virtual assistant and the converted messages for delivery.
The dataset contains parallel corpus of input (input column) message and POV converted messages (output column). An example of a pair is tell @CN@ that i'll be late [\t] hi @CN@, @SCN@ would like you to know that they'll be late. The input and pov-converted output pair is tab separated. @CN@ tag is a placeholder for the contact name (receiver) and @SCN@ tag is a placeholder for source contact name (sender).
The total dataset has 46563 pairs. This data is then test/train/dev split into 6985 pairs/32594 pairs/6985 pairs.
Source: https://github.com/alexa/alexa-point-of-view-dataset"	https://paperswithcode.com/dataset/alexa-point-of-view							
1470	ALFRED	"ALFRED (Action Learning From Realistic Environments and Directives), is a new benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks.
Source: ALFRED"	https://paperswithcode.com/dataset/alfred	03/12/2019	Action Learning From Realistic Environments and Directives					
1471	ALGAD	"Repository of a generative art dataset by computer artist Andy Lomas.
Source: https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset
Image Source: https://github.com/SensiLab/Andy-Lomas-Generative-Art-Dataset"	https://paperswithcode.com/dataset/algad		Andy Lomas Generative Art Dataset					
1472	Allegro Reviews	"A comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. 
Source: KLEJ: Comprehensive Benchmark for Polish Language Understanding"	https://paperswithcode.com/dataset/allegro-reviews							
1473	AlloCine	"A new dataset for sentiment analysis, scraped from Allociné.fr user reviews. It contains 100k positive and 100k negative reviews divided into 3 balanced splits: train (160k reviews), val (20k) and test (20k). 
Source: AlloCine"	https://paperswithcode.com/dataset/allocine							
1474	ALT	"The ALT project aims to advance the state-of-the-art Asian natural language processing (NLP) techniques through the open collaboration for developing and using ALT. It was first conducted by NICT and UCSY as described in Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch and Eiichiro Sumita (2016). Then, it was developed under ASEAN IVO as described in this Web page. The process of building ALT began with sampling about 20,000 sentences from English Wikinews, and then these sentences were translated into the other languages. ALT now has 13 languages: Bengali, English, Filipino, Hindi, Bahasa Indonesia, Japanese, Khmer, Lao, Malay, Myanmar (Burmese), Thai, Vietnamese, Chinese (Simplified Chinese).
Source: ALT"	https://paperswithcode.com/dataset/alt		Asian Language Treebank					
1475	AMASS	AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning.	https://paperswithcode.com/dataset/amass							
1476	Amazon Product Data	"This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.
This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs)."	https://paperswithcode.com/dataset/amazon-product-data	04/02/2016						
1477	AmbigQA	"Is a new open-domain question answering task which involves predicting a set of question-answer pairs, where every plausible answer is paired with a disambiguated rewrite of the original question. A dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.
Source: AmbigQA"	https://paperswithcode.com/dataset/ambigqa							
1478	AML Robot Cutting Dataset	"The AML Robot Cutting Dataset consists of approximately 1500 seconds of real data collected on Kinova Jaco 2 robot retrofitted with a custom end-effector fixture and dremel performing cutting tasks on wood specimens for 5 materials and 5 thicknesses.
Source: AML Robot Cutting Dataset"	https://paperswithcode.com/dataset/aml-robot-cutting-dataset							
1479	ANETAC	"An English-Arabic named entity transliteration and classification dataset built from freely available parallel translation corpora. The dataset contains 79,924 instances, each instance is a triplet (e, a, c), where e is the English named entity, a is its Arabic transliteration and c is its class that can be either a Person, a Location, or an Organization. The ANETAC dataset is mainly aimed for the researchers that are working on Arabic named entity transliteration, but it can also be used for named entity classification purposes.
Source: ANETAC: Arabic Named Entity Transliteration and Classification Dataset"	https://paperswithcode.com/dataset/anetac		Arabic Named Entity Transliteration and Classification					
1480	Animal-Pose Dataset	"Animal-Pose Dataset is an animal pose dataset to facilitate training and evaluation. This dataset provides animal pose annotations on five categories are provided: dog, cat, cow, horse, sheep, with in total 6,000+ instances in 4,000+ images. Besides, the dataset also contains bounding box annotations for other 7 animal categories.
Source: Cross-Domain Adaptation for Animal Pose Estimation
Image Source: https://sites.google.com/view/animal-pose/"	https://paperswithcode.com/dataset/animal-pose-dataset							
1481	AnimalWeb	"A large-scale, hierarchical annotated dataset of animal faces, featuring 21.9K faces from 334 diverse species and 21 animal orders across biological taxonomy. These faces are captured `in-the-wild' conditions and are consistently annotated with 9 landmarks on key facial features. The proposed dataset is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours.
Source: AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces"	https://paperswithcode.com/dataset/animalweb							
1482	ANTIQUE	ANTIQUE is a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset  contains 34,011 manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing.	https://paperswithcode.com/dataset/antique							
1483	AO-CLEVr	"AO-CLEVr is a new synthetic-images dataset containing images of ""easy"" Attribute-Object categories, based on the CLEVr. AO-CLEVr has attribute-object pairs created from 8 attributes: { red, purple, yellow, blue, green, cyan, gray, brown } and 3 object shapes {sphere, cube, cylinder}, yielding 24 attribute-object pairs. Each pair consists of 7500 images. Each image has a single object that consists of the attribute-object pair. The object is randomly assigned one of two sizes (small/large), one of two materials (rubber/metallic), a random position, and random lightning according to CLEVr defaults.
Source: https://github.com/nv-research-israel/causal_comp
Image Source: https://github.com/nv-research-israel/causal_comp"	https://paperswithcode.com/dataset/ao-clevr							
1484	ApartmenTour	"Contains a large number of online videos and subtitles. 
Source: Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision"	https://paperswithcode.com/dataset/apartmentour							
1485	APE	"APE is useful to evaluate Machine Translation automatic post-editing (APE), which is the task of improving the output of a blackbox MT system by automatically fixing its mistakes. The act of post-editing text can be fully specified as a sequence of delete and insert actions in given positions.
Source: https://github.com/antoniogois/keystrokes_ape
Image Source: Gois et al"	https://paperswithcode.com/dataset/ape		Automatic Post-Editing					
1486	APRICOT	"APRICOT is a collection of over 1,000 annotated photographs of printed adversarial patches in public locations. The patches target several object categories for three COCO-trained detection models, and the photos represent natural variation in position, distance, lighting conditions, and viewing angle. 
Source: APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection
Image Source: https://apricot.mitre.org/"	https://paperswithcode.com/dataset/apricot							
1487	APT-Malware	"The APT Malware dataset is used to train classifiers to predict if a given malware belongs to the “Advanced Persistent Threat” (APT) type or not. It contains 3131 samples spread over 24 different unique malware classes.
Source: https://arxiv.org/pdf/1810.07321.pdf"	https://paperswithcode.com/dataset/apt-malware							
1488	AQUA	"The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. The dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. 
Source: A Dataset and Baselines for Visual Question Answering on Art"	https://paperswithcode.com/dataset/aqua							
1489	Aqualoc	"A new underwater dataset that has been recorded in an harbor and provides several sequences with synchronized measurements from a monocular camera, a MEMS-IMU and a pressure sensor.
Source: The Aqualoc Dataset: Towards Real-Time Underwater Localization from a Visual-Inertial-Pressure Acquisition System"	https://paperswithcode.com/dataset/aqualoc							
1490	aquamuse	"5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl.
Source: AQuaMuSe: Automatically Generating Datasets for Query-Based Multi-Document Summarization"	https://paperswithcode.com/dataset/aquamuse							
1491	AQUA-RAT	"Algebra Question Answering with Rationales (AQUA-RAT) is a dataset that contains algebraic word problems with rationales. The dataset consists of about 100,000 algebraic word problems with natural language rationales. Each problem is a json object consisting of four parts:
* question - A natural language definition of the problem to solve
* options - 5 possible options (A, B, C, D and E), among which one is correct
* rationale - A natural language description of the solution to the problem
* correct - The correct option
Source: https://github.com/deepmind/AQuA"	https://paperswithcode.com/dataset/aqua-rat		Algebra Question Answering with Rationales					
1492	Arabic Dataset for Commonsense Validation¬†	"A benchmark Arabic dataset for commonsense understanding and validation as well as a baseline research and models trained using the same dataset. 
Source: Is this sentence valid? An Arabic Dataset for Commonsense Validation"	https://paperswithcode.com/dataset/arabic-dataset-for-commonsense-validation							
1493	Arabic Handwritten Digits Dataset	"Contain Arabic handwritten digits images (60000 training and 10000 testing images).
Source: Arabic Handwritten Digits Dataset"	https://paperswithcode.com/dataset/arabic-handwritten-digits-dataset							
1494	Arabic Text Diacritization	"Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words.
Source: https://github.com/AliOsm/arabic-text-diacritization"	https://paperswithcode.com/dataset/arabic-text-diacritization							
1495	ArabicWeb16	"It includes 150M (150,211,934) Arabic Web pages.
Web pages in ArabicWeb16 are collected into files that conform to the WARC ISO 28500 version 0.18 standard (""WARC files"").
Source: ArabicWeb16"	https://paperswithcode.com/dataset/arabicweb16							
1496	ARCD	"Composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). 
Source: Neural Arabic Question Answering"	https://paperswithcode.com/dataset/arcd							
1497	ArCOV-19	"ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes over 1M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked).
Source: ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks"	https://paperswithcode.com/dataset/arcov-19							
1498	ArCOV19-Rumors	"ArCOV19-Rumors is an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. 
Source: ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation Detection"	https://paperswithcode.com/dataset/arcov19-rumors							
1499	ARDIS	"This is a new image-based handwritten historical digit dataset named ARDIS (Arkiv Digital Sweden). The images in ARDIS dataset are extracted from 15.000 Swedish church records which were written by different priests with various handwriting styles in the nineteenth and twentieth centuries. The constructed dataset consists of three single digit datasets and one digit strings dataset. The digit strings dataset includes 10.000 samples in Red-Green-Blue (RGB) color space, whereas, the other datasets contain 7.600 single digit images in different color spaces.
Source: ARDIS"	https://paperswithcode.com/dataset/ardis							
1500	Armenian Paraphrase Detection Corpus	"This dataset contains 2,360 paraphrases in Armenian that can be used for paraphrase detection. The dataset is constructed by back-translating sentences from Armenian to English twice, and manually filtering the result.
Source: https://github.com/ivannikov-lab/arpa-paraphrase-corpus"	https://paperswithcode.com/dataset/armenian-paraphrase-detection-corpus							
1501	ArraMon	"A dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. 
Source: ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments"	https://paperswithcode.com/dataset/arramon							
1502	ArSentD-LEV	"The Arabic Sentiment Twitter Dataset for the Levantine dialect (ArSenTD-LEV) is a dataset of 4,000 tweets with the following annotations: the overall sentiment of the tweet, the target to which the sentiment was expressed, how the sentiment was expressed, and the topic of the tweet. 
Source: ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in Arabic Levantine Tweets"	https://paperswithcode.com/dataset/arsentd-lev							
1503	ART Dataset	ART consists of over 20k commonsense narrative contexts and 200k explanations.	https://paperswithcode.com/dataset/art-dataset		Abductive Reasoning in narrative Text					
1504	Arxiv Academic Paper Dataset	"A dataset to enable automatic academic paper rating.
Source: Automatic Academic Paper Rating Based on Modularized Hierarchical Convolutional Neural Network"	https://paperswithcode.com/dataset/arxiv-academic-paper-dataset							
1505	arXiv Summarization Dataset	"This is a dataset for evaluating summarisation methods for research papers.
Source: A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"	https://paperswithcode.com/dataset/arxiv-summarization-dataset							
1506	ASAYAR	"The first public dataset dedicated for Latin (French) and Arabic Scene Text Detection in Highway panels. It comprises more than 1800 well-annotated images. The dataset was colleted from Moroccan Highway and it has been manually annotated. ASAYAR data can be used to develop and evaluate traffic signs detection and French or Arabic text detection in different languages.
Source: ASAYAR"	https://paperswithcode.com/dataset/asayar							
1507	AskParents	"AskParents is a dataset for advice classification extracted from Reddit. In this dataset, posts are annotated for whether they contain advice or not. It contains 8,701 samples for training, 802 for validation and 1,091 for testing.
Source: https://github.com/venkatasg/Advice-EMNLP2020"	https://paperswithcode.com/dataset/askparents							
1508	ASNQ	"A large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. 
Source: TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection"	https://paperswithcode.com/dataset/asnq		Answer Sentence Natural Questions					
1509	ASSET Corpus	"A crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.
Source: ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations"	https://paperswithcode.com/dataset/asset-corpus							
1510	ASSIN	"ASSIN (Avaliação de Similaridade Semântica e INferência textual) is a dataset with semantic similarity score and entailment annotations. It was used in a shared task in the PROPOR 2016 conference.
The full dataset has 10,000 sentence pairs, half of which in Brazilian Portuguese and half in European Portuguese. Either language variant has 2,500 pairs for training, 500 for validation and 2,000 for testing. This is different from the split used in the shared task, in which the training set had 3,000 pairs and there was no validation set. The shared task training set can be reconstructed by simply merging both sets.
Source: ASSIN"	https://paperswithcode.com/dataset/assin							
1511	ASSIN2	"ASSIN 2 is the second Semantic Similarity Assessment and Textual Inference, and was a workshop held in conjunction with STIL 2019 .
Source: ASSIN2"	https://paperswithcode.com/dataset/assin2							
1512	Astyx HiRes2019	"A radar-centric automotive dataset based on radar, lidar and camera data for the purpose of 3D object detection.
Source: Automotive Radar Dataset for Deep Learning Based 3D Object Detection"	https://paperswithcode.com/dataset/astyx-hires2019							
1513	Atari Grand Challenge	"The Atari Grand Challenge dataset is a large dataset of human Atari 2600 replays. It consists of replays for 5 different games:
* Space Invaders (445 episodes, 2M frames)
* Q*bert (659 episodes, 1.6M frames)
* Ms.Pacman (384 episodes, 1.7M frames)
* Video Pinball (211 episodes, 1.5M frames)
* Montezuma’s revenge (668 episodes, 2.7M frames)
Source: https://arxiv.org/pdf/1705.10998.pdf
Image Source: https://arxiv.org/pdf/1705.10998.pdf"	https://paperswithcode.com/dataset/atari-grand-challenge							
1514	Atlas	"Atlas is a dataset for e-commerce clothing product categorization. The Atlas dataset consists of a high-quality product taxonomy dataset focusing on clothing products which contain 186,150 images under clothing category with 3 levels and 52 leaf nodes in the taxonomy.
Source: https://github.com/vumaasha/atlas
Image Source: Umaashankaret et al"	https://paperswithcode.com/dataset/atlas							
1515	ATOMIC	"ATOMIC is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., ""if X pays Y a compliment, then Y will likely return the compliment"").
Source: ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning
Image Source: https://homes.cs.washington.edu/~msap/atomic/"	https://paperswithcode.com/dataset/atomic							
1516	ATRW	"The ATRW Dataset contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. 
Source: ATRW: A Benchmark for Amur Tiger Re-identification in the Wild"	https://paperswithcode.com/dataset/atrw		Amur Tiger Re-identification in the Wild					
1517	AU-AIR	"The AU-AIR is a multi-modal aerial dataset captured by a UAV. Having visual data, object annotations, and flight data (time, GPS, altitude, IMU sensor data, velocities), AU-AIR meets vision and robotics for UAVs.
Source: https://github.com/bozcani/auairdataset
Image Source: https://github.com/bozcani/auairdataset"	https://paperswithcode.com/dataset/au-air							
1518	Automatic Keyphrase Extraction Dataset	"Dataset for automatic keyphrase extraction task.
Source: Automatic Keyphrase Extraction Dataset"	https://paperswithcode.com/dataset/automatic-keyphrase-extraction-dataset							
1519	Automating Dynamic Consent	"This dataset is used to evaluate a predictive consent model for users’ information shared in social media. In this task, the goal is to predict whether the users will give their consent to share that data with different hypothetical audiences within a medical context. The dataset is built from information the users posted on Facebook and their consent answers about each piece of information.
Source: https://github.com/cnorval/automating-dynamic-consent-dataset"	https://paperswithcode.com/dataset/automating-dynamic-consent							
1520	AuxAD	"AuxAD is a a distantly supervised dataset for acronym disambiguation.
Source: https://github.com/PrimerAI/sdu-data"	https://paperswithcode.com/dataset/auxad							
1521	AVA-ActiveSpeaker	"Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. 
Source: AVA-ActiveSpeaker: An Audio-Visual Dataset for Active Speaker Detection"	https://paperswithcode.com/dataset/ava-activespeaker							
1522	AVA-LAEO	"Dataset to address the problem of detecting people Looking At Each Other (LAEO) in video sequences.
Source: LAEO-Net: revisiting people Looking At Each Other in videos"	https://paperswithcode.com/dataset/ava-laeo							
1523	AVA-Speech	"Contains densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task. 
Source: AVA-Speech: A Densely Labeled Dataset of Speech Activity in Movies"	https://paperswithcode.com/dataset/ava-speech							
1524	AVD	"AVD focuses on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes.
Source: A Dataset for Developing and Benchmarking Active Vision"	https://paperswithcode.com/dataset/avd		Active Vision Dataset					
1525	AVE	"To investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization.
Source: Audio-Visual Event Localization in Unconstrained Videos"	https://paperswithcode.com/dataset/ave		Audio-Visual Event Localization					
1526	AVECL-UMons	"A dataset for audio-visual event classification and localization in the context of office environments. The audio-visual dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. Two types of sequences are recorded according to the number of events in the sequence. The dataset comprises 2662 unilabel sequences and 2724 multilabel sequences corresponding to a total of 5.24 hours. 
Source: AVECL-UMONS database for audio-visual event classification and localization"	https://paperswithcode.com/dataset/avecl-umons							
1527	BanFakeNews	"An annotated dataset of ~50K news that can be used for building automated fake news detection systems for a low resource language like Bangla. 
Source: BanFakeNews: A Dataset for Detecting Fake News in Bangla"	https://paperswithcode.com/dataset/banfakenews							
1528	BanglaLekha-Isolated	"This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district. 
Source: BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character Dataset"	https://paperswithcode.com/dataset/banglalekha-isolated							
1529	BanglaWriting	"The BanglaWriting dataset contains single-page handwritings of 260 individuals of different personalities and ages. Each page includes bounding-boxes that bounds each word, along with the unicode representation of the writing. This dataset contains 21,234 words and 32,787 characters in total. Moreover, this dataset includes 5,470 unique words of Bangla vocabulary. Apart from the usual words, the dataset comprises 261 comprehensible overwriting and 450 incomprehensible overwriting. All of the bounding boxes and word labels are manually-generated. The dataset can be used for complex optical character/word recognition, writer identification, and handwritten word segmentation. Furthermore, this dataset is suitable for extracting age-based and gender-based variation of handwriting.
Source: https://github.com/QuwsarOhi/BanglaWriting
Image Source: https://github.com/QuwsarOhi/BanglaWriting"	https://paperswithcode.com/dataset/banglawriting							
1530	BAR	"Biased Action Recognition (BAR) dataset is a real-world image dataset categorized as six action classes which are biased to distinct places. The authors settle these six action classes by inspecting imSitu, which provides still action images from Google Image Search with action and place labels. In detail, the authors choose action classes where images for each of these candidate actions share common place characteristics. At the same time, the place characteristics of action class candidates should be distinct in order to classify the action only from place attributes. The select pairs are six typical action-place pairs: (Climbing, RockWall), (Diving, Underwater), (Fishing, WaterSurface), (Racing, APavedTrack), (Throwing, PlayingField),and (Vaulting, Sky).
Source: https://github.com/alinlab/BAR
Image Source: https://github.com/alinlab/BAR"	https://paperswithcode.com/dataset/bar		Biased Action Recognition					
1531	BASIL	"300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias.
Source: In Plain Sight: Media Bias Through the Lens of Factual Reporting"	https://paperswithcode.com/dataset/basil							
1532	BBDB	"A new large-scale baseball video dataset which is produced semi-automatically by using play-by-play texts available online. The BBDB contains 4200 hours of baseball game videos with 400k temporally annotated activity segments.
Source: BBDB"	https://paperswithcode.com/dataset/bbdb							
1533	BCWS	"Dataset for evaluating English-Chinese Bilingual Contextual Word Similarity. The dataset consists of 2,091 English-Chinese word pairs with the corresponding sentential contexts and their similarity scores annotated by the human. 
Source: BCWS: Bilingual Contextual Word Similarity"	https://paperswithcode.com/dataset/bcws		Bilingual Contextual Word Similarity					
1534	BD-4SK-ASR	"The Basic Dataset for Sorani Kurdish Automatic Speech Recognition (BD-4SK-ASR) is a dataset for automatic speech recognition for Sorani Kurdish.
Source: https://arxiv.org/abs/1911.13087"	https://paperswithcode.com/dataset/bd-4sk-asr		Basic Dataset for Sorani Kurdish Automatic Speech Recognition					
1535	BdSLImset	"Bangladeshi Sign Language Image Dataset (BdSLImset) is a dataset that contains images of different Bangladeshi sign letters.
Source: https://github.com/imruljubair/BdSLImset
Image Source: https://arxiv.org/pdf/1811.12813v1.pdf"	https://paperswithcode.com/dataset/bdslimset		Bangladeshi Sign Language Image Dataset					
1536	Bengali Hate Speech	"Introduces three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. 
Source: Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network"	https://paperswithcode.com/dataset/bengali-hate-speech							
1537	Berkeley DeepDrive Video	"A dataset comprised of real driving videos and GPS/IMU data. The BDDV dataset contains diverse driving scenarios including cities, highways, towns, and rural areas in several major cities in US.
Source: End-to-end Learning of Driving Models from Large-scale Video Datasets"	https://paperswithcode.com/dataset/berkeley-deepdrive-video							
1538	Bianet	"Bianet is a parallel news corpus in Turkish, Kurdish and English
It contains 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper.
Source: Bianet: A Parallel News Corpus in Turkish, Kurdish and English"	https://paperswithcode.com/dataset/bianet							
1539	BigEarthNet	"BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. 
Source: BigEarthNet: A Large-Scale Benchmark Archive For Remote Sensing Image Understanding"	https://paperswithcode.com/dataset/bigearthnet							
1540	BigHand2.2M Benchmark	"A large-scale hand pose dataset, collected using a novel capture method.
Source: BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis"	https://paperswithcode.com/dataset/bighand2-2m-benchmark							
1541	Billion Word Benchmark	The One Billion Word dataset is a dataset for language modeling. The training/held-out data was produced from the WMT 2011 News Crawl data using a combination of Bash shell and Perl scripts.	https://paperswithcode.com/dataset/billion-word-benchmark							
1542	BIMCV COVID-19	"BIMCV-COVID19+ dataset is a large dataset with chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19 patients along with their radiographic findings, pathologies, polymerase chain reaction (PCR), immunoglobulin G (IgG) and immunoglobulin M (IgM) diagnostic antibody tests and radiographic reports from Medical Imaging Databank in Valencian Region Medical Image Bank (BIMCV). The findings are mapped onto standard Unified Medical Language System (UMLS) terminology and they cover a wide spectrum of thoracic entities, contrasting with the much more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels in a Medical Imaging Data Structure (MIDS) format. In addition, 23 images were annotated by a team of expert radiologists to include semantic segmentation of radiographic findings. Moreover, extensive information is provided, including the patient’s demographic information, type of projection and acquisition parameters for the imaging study, among others. These iterations of the database include 7,377 CR, 9,463 DX and 6,687 CT studies.
Source: https://github.com/BIMCV-CSUSP/BIMCV-COVID-19
Image Source: https://github.com/BIMCV-CSUSP/BIMCV-COVID-19"	https://paperswithcode.com/dataset/bimcv-covid-19							
1543	BIOMRC	"A large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018).
Source: BIOMRC: A Dataset for Biomedical Machine Reading Comprehension"	https://paperswithcode.com/dataset/biomrc							
1544	Blackbird	"The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception. The Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms. 
Source: The Blackbird Dataset: A large-scale dataset for UAV perception in aggressive flight"	https://paperswithcode.com/dataset/blackbird							
1545	BlendedMVS	"BlendedMVS is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps. 
Source: BlendedMVS: A Large-scale Dataset for Generalized Multi-view Stereo Networks"	https://paperswithcode.com/dataset/blendedmvs							
1546	Blended Skill Talk	"To analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. 
Source: Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills"	https://paperswithcode.com/dataset/blended-skill-talk		Blended Skill Talk					
1547	Blog Authorship Corpus	"The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.  
Source: Blog Authorship Corpus"	https://paperswithcode.com/dataset/blog-authorship-corpus							
1548	BLUE	"The BLUE benchmark consists of five different biomedicine text-mining tasks with ten corpora. These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges.
Source: Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets"	https://paperswithcode.com/dataset/blue		Biomedical Language Understanding Evaluation					
1549	BLVD	"BLVD is a large scale 5D semantics dataset collected by the Visual Cognitive Computing and Intelligent Vehicles Lab.
This dataset contains 654 high-resolution video clips owing 120k frames extracted from Changshu, Jiangsu Province, China, where the Intelligent Vehicle Proving Center of China (IVPCC) is located. The frame rate is 10fps/sec for RGB data and 3D point cloud. The dataset contains fully annotated frames which yield 249,129 3D annotations, 4,902 independent individuals for tracking with the length of overall 214,922 points, 6,004 valid fragments for 5D interactive event recognition, and 4,900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime).
Source: BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving"	https://paperswithcode.com/dataset/blvd							
1550	Book Cover Dataset	"A new challenging dataset that can be used for many pattern recognition tasks.
Source: Judging a Book By its Cover"	https://paperswithcode.com/dataset/book-cover-dataset							
1551	BotNet	"The BotNet dataset is a set of topological botnet detection datasets forgraph neural networks.
Source: https://github.com/harvardnlp/botnet-detection
Image Source: https://github.com/harvardnlp/botnet-detection"	https://paperswithcode.com/dataset/botnet							
1552	BreizhCrops	"BreizhCrops is a satellite image time series dataset for crop type classification. It consists on aggregated label data and Sentinel-2 top-of-atmosphere as well as bottom-of-atmosphere time series in the region of Brittany (Breizh in local language), north-east France.
Source: https://github.com/TUM-LMF/BreizhCrops"	https://paperswithcode.com/dataset/breizhcrops							
1553	Brno-Urban-Dataset	"This self-driving dataset collected in Brno, Czech Republic contains data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy.
Source: https://arxiv.org/abs/1909.06897
Image Source: https://github.com/RoboticsBUT/Brno-Urban-Dataset"	https://paperswithcode.com/dataset/brno-urban-dataset							
1554	BRWAC	"Composed by 2.7 billion tokens, and has been annotated with tagging and parsing information. 
Source: The brWaC Corpus: A New Open Resource for Brazilian Portuguese"	https://paperswithcode.com/dataset/brwac							
1555	BSTLD	"This dataset contains 13427 camera images at a resolution of 1280x720 pixels and contains about 24000 annotated traffic lights. The annotations include bounding boxes of traffic lights as well as the current state (active light) of each traffic light.
The camera images are provided as raw 12bit HDR images taken with a red-clear-clear-blue filter and as reconstructed 8-bit RGB color images. The RGB images are provided for debugging and can also be used for training. However, the RGB conversion process has some drawbacks. Some of the converted images may contain artifacts and the color distribution may seem unusual.
Source: BSTLD"	https://paperswithcode.com/dataset/bstld		Bosch Small Traffic Lights Dataset					
1556	BVI-DVC	"Contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools.
Source: BVI-DVC: A Training Database for Deep Video Compression"	https://paperswithcode.com/dataset/bvi-dvc							
1557	C3	C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset.	https://paperswithcode.com/dataset/c3							
1558	CADC	"Collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ.
Source: Canadian Adverse Driving Conditions Dataset"	https://paperswithcode.com/dataset/cadc		Canadian Adverse Driving Conditions					
1559	CADP	"A novel dataset for traffic accidents analysis. 
Source: CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis"	https://paperswithcode.com/dataset/cadp							
1560	CAIL2019-SCM	"Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets.
Source: CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain"	https://paperswithcode.com/dataset/cail2019-scm							
1561	CALFW	"A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. 
Source: CALFW"	https://paperswithcode.com/dataset/calfw		Cross-Age LFW					
1562	Caltech Pedestrian Dataset	"The Caltech Pedestrian Dataset consists of approximately 10 hours of 640x480 30Hz video taken from a vehicle driving through regular traffic in an urban environment. About 250,000 frames (in 137 approximately minute long segments) with a total of 350,000 bounding boxes and 2300 unique pedestrians were annotated. The annotation includes temporal correspondence between bounding boxes and detailed occlusion labels.
Source: Caltech Pedestrian Dataset"	https://paperswithcode.com/dataset/caltech-pedestrian-dataset							
1563	capes	"Approximately 240,000 documents were collected and aligned using the Hunalign tool.
Source: A Parallel Corpus of Theses and Dissertations Abstracts"	https://paperswithcode.com/dataset/capes							
1564	CapGaze	"Consists of eye movements and verbal descriptions recorded synchronously over images.
Source: Human Attention in Image Captioning: Dataset and Analysis"	https://paperswithcode.com/dataset/capgaze							
1565	CARD-660	"An expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques. 
Source: Card-660: Cambridge Rare Word Dataset - a Reliable Benchmark for Infrequent Word Representation Models"	https://paperswithcode.com/dataset/card-660							
1566	CARRADA	"CARRADA is a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. 
Source: CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations
Image Source: https://github.com/valeoai/carrada_dataset"	https://paperswithcode.com/dataset/carrada							
1567	CASIA-SURF	"Dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of  subjects with  videos and each sample has  modalities (i.e., RGB, Depth and IR). 
Source: CASIA-SURF: A Large-scale Multi-modal Benchmark for Face Anti-spoofing"	https://paperswithcode.com/dataset/casia-surf							
1568	CATER	"Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning.
Source: CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning
Image Source: CATER"	https://paperswithcode.com/dataset/cater							
1569	CATS	"A dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. The authors scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes.
Source: CATS: A Color and Thermal Stereo Benchmark"	https://paperswithcode.com/dataset/cats		Color and Thermal Stereo Benchmark					
1570	caWaC	"The corpus represents the largest existing corpus of Catalan containing 687 million words, which is a significant increase given that until now the biggest corpus of Catalan, CuCWeb, counts 166 million words. 
Source: caWaC -- A web corpus of Catalan and its application to language modeling and machine translation"	https://paperswithcode.com/dataset/cawac							
1571	CC100	"This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository.
Source: Unsupervised Cross-lingual Representation Learning at Scale"	https://paperswithcode.com/dataset/cc100							
1572	CC-19	"CC-19 is a small new dataset related to the latest family of coronavirus i.e. COVID-19. The proposed dataset “CC-19” contains 34,006 CT scan slices (images) belonging to 98 subjects out of which 28,395 CT scan slices belong to positive COVID patients.
Source: https://github.com/abdkhanstd/COVID-19
Image Source: https://github.com/abdkhanstd/COVID-19"	https://paperswithcode.com/dataset/cc-19							
1573	CCD	"Car Crash Dataset (CCD) is collected for traffic accident analysis. It contains real traffic accident videos captured by dashcam mounted on driving vehicles, which is critical to developing safety-guaranteed self-driving systems. CCD is distinguished from existing datasets for diversified accident annotations, including environmental attributes (day/night, snowy/rainy/good weather conditions), whether ego-vehicles involved, accident participants, and accident reason descriptions.
Source: https://github.com/Cogito2012/CarCrashDataset
Image Source: https://github.com/Cogito2012/CarCrashDataset"	https://paperswithcode.com/dataset/ccd		Car Crash Dataset					
1574	CCMatrix	"CCMatrix uses ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences. 
Source: CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB"	https://paperswithcode.com/dataset/ccmatrix							
1575	CCPD	"The Chinese City Parking Dataset (CCPD) is a dataset for license plate detection and recognition. It contains over 250k unique car images, with license plate location annotations.
Source: Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline
Image Source: Xu et al"	https://paperswithcode.com/dataset/ccpd		Chinese City Parking Dataset					
1576	Polish CDSCorpus	"Consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for the evaluation of compositional distributional semantics models of Polish.
Source: Polish evaluation dataset for compositional distributional semantics models"	https://paperswithcode.com/dataset/polish-cdscorpus							
1577	CECW	"The CECW dataset is a color-extended version of the Cleanup World (CW) borrowed from the mobile-manipulation robot domain. CW refers to a world equipped with a movable object as well as four rooms in four colors, including ""blue,"" ""green,"" ""red,"" and ""yellow,"" which is designed as a simulation environment where the agent can act based on the instructions received. CW obeys a particular Geometric Linear Temporal Logic (GLTL) to parse commands by grammatical syntax, resulting in a total of 3,382 commands reflecting 39 GLTL expressions.
Source: https://github.com/MrShininnnnn/CECW
Image Source: https://github.com/MrShininnnnn/CECW"	https://paperswithcode.com/dataset/cecw		Colorful Extended Cleanup World					
1578	CED	"Contains 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes.
Source: CED: Color Event Camera Dataset"	https://paperswithcode.com/dataset/ced		Color Event Camera Dataset					
1579	CelebAMask-HQ	"CelebAMask-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA.
Source: https://github.com/switchablenorms/CelebAMask-HQ
Image Source: https://github.com/switchablenorms/CelebAMask-HQ"	https://paperswithcode.com/dataset/celebamask-hq							
1580	CelebA-Spoof	"CelebA-Spoof is a large-scale face anti-spoofing dataset with the following properties: 

Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, significantly larger than the existing datasets. 
Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination conditions) with more than 10 sensors. 
Annotation Richness: CelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute annotations inherited from the original CelebA dataset.

Source: CelebA-Spoof: Large-Scale Face Anti-Spoofing Dataset with Rich Annotations"	https://paperswithcode.com/dataset/celeba-spoof							
1581	Celeb-DF	"Celeb-DF is a large-scale challenging dataset for deepfake forensics. It includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding DeepFake videos.
Source: https://github.com/danmohaha/celeb-deepfakeforensics
Image Source: https://github.com/danmohaha/celeb-deepfakeforensics"	https://paperswithcode.com/dataset/celeb-df							
1582	Cervix93 Cytology Dataset	"The dataset has 93 image stacks and their corresponding Extended Depth of Field (EDF) image acquired from cases with grades Nagative, LSIL or HSIL (The Bethesda System):
- Negative: 16
- LSIL: 46
- HSIL: 31
The ground truth includes the grade labels for each frame and manually marked points inside cervical cells in each frame. There are in total 2705 manually marked points inside all frames:
- Negative: 238
- LSIL: 1536
- HSIL: 931
Source: https://github.com/parham-ap/cytology_dataset"	https://paperswithcode.com/dataset/cervix93-cytology-dataset							
1583	CFQ	"A large and realistic natural language question answering dataset.
Source: Measuring Compositional Generalization: A Comprehensive Method on Realistic Data"	https://paperswithcode.com/dataset/cfq		Compositional Freebase Questions					
1584	CDNET	"A video database for testing change detection algorithms. 
Source: CDNET"	https://paperswithcode.com/dataset/cdnet		Change Detection					
1585	Charades-Ego	"Contains 68,536 activity instances in 68.8 hours of first and third-person video, making it one of the largest and most diverse egocentric datasets available. Charades-Ego furthermore shares activity classes, scripts, and methodology with the Charades dataset, that consist of additional 82.3 hours of third-person video with 66,500 activity instances. 
Source: Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos"	https://paperswithcode.com/dataset/charades-ego							
1586	CHECKED	"Chinese dataset on COVID-19 misinformation. CHECKED provides ground-truth on credibility, carefully obtained by ensuring the specific sources are used. CHECKED includes microblogs related to COVID-19, identified by using a specific list of keywords, covering a total 2120 microblogs published from December 2019 to August 2020. The dataset contains a rich set of multimedia information for each microblog including ground-truth label, textual, visual, response, and social network information.
Source: CHECKED: Chinese COVID-19 Fake News Dataset"	https://paperswithcode.com/dataset/checked							
1587	CHiME-5	"The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning.
Source: The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines"	https://paperswithcode.com/dataset/chime-5		CHiME Speech Separation and Recognition Challenge					
1588	Chinese AI and Law (CAIL) 2018	"Large-scale Chinese legal dataset for judgment prediction. \dataset contains more than 2.6  million criminal cases published by the Supreme People's Court of China, which are several times larger than other datasets in existing works on judgment prediction.
Source: CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction"	https://paperswithcode.com/dataset/chinese-ai-and-law-cail-2018							
1589	ChineseFoodNet	"ChineseFoodNet aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In the dataset, images of each food category of the dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. 
Source: ChineseFoodNet: A large-scale Image Dataset for Chinese Food Recognition
Image Source: https://sites.google.com/view/chinesefoodnet"	https://paperswithcode.com/dataset/chinesefoodnet							
1590	Chinese Literature NER RE	"Chinese Literature NER RE is a Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text. It is constructed from hundreds of Chinese literature articles.
Source: https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset
Image Source: https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset"	https://paperswithcode.com/dataset/chinese-literature-ner-re							
1591	Chinese Text in the Wild	Chinese Text in the Wild is a dataset of Chinese text with about 1 million Chinese characters from 3850 unique ones annotated by experts in over 30000 street view images. This is a challenging dataset with good diversity containing planar text, raised text, text under poor illumination, distant text, partially occluded text, etc.	https://paperswithcode.com/dataset/chinese-text-in-the-wild							
1592	Chinese Traditional Painting dataset	"The Chinese Traditional Painting dataset for style transfer contains 1000 content images and 100 style images.
The content images are mostly the photorealistic scenes of mountain, lake, river, bridge, and buildings in regions south of the Yangtze River. It includes not only the scenes of China, but also beautiful pictures of Rhine, Alps, Yellow Stone, Grand Canyon, etc. The content images include diverse types of Chinese traditional paintings.
Source: https://github.com/lbsswu/Chinese_style_transfer
Image Source: https://github.com/lbsswu/Chinese_style_transfer"	https://paperswithcode.com/dataset/chinese-traditional-painting-dataset							
1593	CIC	"The dataset is annotated with stance towards one topic, namely, the independence of Catalonia.
Source: Multilingual Stance Detection: The Catalonia Independence Corpus"	https://paperswithcode.com/dataset/cic		Catalonia Independence Corpus					
1594	CITE	"CITE is a crowd-sourced resource for multimodal discourse: this resource characterises inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations.
Source: CITE: A Corpus of Image-Text Discourse Relations"	https://paperswithcode.com/dataset/cite							
1595	CITIUS Video Database	"This eye tracking video database can be used to validate visual attention models. This dataset includes 72 videos downloaded from Internet and some synthetic videos generated in the lab. The videos can be classified in four categories, natural and synthetic, with fixed or movement camera. It includes 27 synthetic videos with dynamic pop-out effects. The videos have been selected in order to minimize the influence of the top-down effects.
Source: CITIUS Video Database"	https://paperswithcode.com/dataset/citius-video-database							
1596	CITR Dataset	"CITR Dataset consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI. DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration.
Source: Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus
Image Source: Yang et al"	https://paperswithcode.com/dataset/citr-dataset							
1597	CityFlow	"CityFlow is a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. 
Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). 
Source: CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification"	https://paperswithcode.com/dataset/cityflow							
1598	Cityscapes Panoptic Parts	"The Cityscapes Panoptic Parts dataset introduces part-aware panoptic segmentation annotations for the Cityscapes dataset. It extends the original panoptic annotations for the Cityscapes dataset with part-level annotations for selected scene-level classes. 
Source: https://arxiv.org/pdf/2004.07944.pdf
Image Source: https://github.com/pmeletis/panoptic_parts"	https://paperswithcode.com/dataset/cityscapes-panoptic-parts	11/06/2021						
1599	CLAD	"CLAD (Compled and Long Activities Dataset) is an activity dataset which exhibits real-life and diverse scenarios of complex, temporally-extended human activities and actions. The dataset consists of a set of videos of actors performing everyday activities in a natural and unscripted manner. The dataset was recorded using a static Kinect 2 sensor which is commonly used on many robotic platforms. The dataset comprises of RGB-D images, point cloud data, automatically generated skeleton tracks in addition to crowdsourced annotations. 
Source: Complex and Long Activities Dataset"	https://paperswithcode.com/dataset/clad		Complex and Long Activities Dataset					
1600	ClaimBuster	"Consist of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. 
Source: A Benchmark Dataset of Check-worthy Factual Claims"	https://paperswithcode.com/dataset/claimbuster							
1601	ClariQ	"ClariQ is an extension of the Qulac dataset with additional new topics, questions, and answers in the training set. The test set is completely unseen and newly collected. Like Qulac, ClariQ consists of single-turn conversations (initial_request, followed by clarifying question and answer). In addition, it comes with synthetic multi-turn conversations (up to three turns). ClariQ features approximately 18K single-turn conversations, as well as 1.8 million multi-turn conversations. 
Source: ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue Systems (ClariQ)"	https://paperswithcode.com/dataset/clariq							
1602	CLEVR-Ref+	"CLEVR-Ref+ is a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators. 
Source: CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions"	https://paperswithcode.com/dataset/clevr-ref							
1603	Climate Claims	"The Climate Change Claims dataset for generating fact checking summaries contains claims broadly related to climate change and global warming from climatefeedback.org. It contains 1k documents from 104 different claims from 97 different domains.
Source: https://arxiv.org/abs/2010.08570"	https://paperswithcode.com/dataset/climate-claims							
1604	CLIMATE-FEVER	"A new publicly available dataset for verification of climate change-related claims. 
Source: CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims"	https://paperswithcode.com/dataset/climate-fever							
1605	ClipShots	"ClipShots is a large-scale dataset for shot boundary detection collected from Youtube and Weibo covering more than 20 categories, including sports, TV shows, animals, etc. In contrast to previous shot boundary detection datasets, e.g. TRECVID and RAI, which only consist of documentaries or talk shows where the frames are relatively static, ClipShots contains moslty short videos from Youtube and Weibo. Many short videos are home-made, with more challenges, e.g. hand-held vibrations and large occlusion. The types of these videos are various, including movie spotlights, competition highlights, family videos recorded by mobile phones etc. Each video has a length of 1-20 minutes. The gradual transitions in the dataset include dissolve, fade in fade out, and sliding in sliding out.
Source: https://github.com/Tangshitao/ClipShots"	https://paperswithcode.com/dataset/clipshots							
1606	CLIRMatrix	"CLIRMatrix is a large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval. It includes:

BI-139: A bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs,
MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.

In total, 49 million unique queries and 34 billion (query, document, label) triplets were mined, making CLIRMatrix the largest and most comprehensive CLIR dataset to date.
Source: CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval"	https://paperswithcode.com/dataset/clirmatrix							
1607	CloudCast	"A satellite-based dataset called ""CloudCast"". It consists of 70080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928  × 1530 pixels (3  × 3 km per pixel) with 15-min intervals between frames for the period January 1, 2017, to December 31, 2018. All frames are centered and projected over Europe.
Source: CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds"	https://paperswithcode.com/dataset/cloudcast	02/03/2021	CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds					
1608	ClovaCall	"ClovaCall is a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people. The raw dataset of ClovaCall includes approximately 112,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain.
Source: https://github.com/ClovaAI/ClovaCall"	https://paperswithcode.com/dataset/clovacall							
1609	CLUE	CLUE is a Chinese Language Understanding Evaluation benchmark. It consists of different NLU datasets. It is a community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text.	https://paperswithcode.com/dataset/clue		Chinese Language Understanding Evaluation Benchmark					
1610	CLUECorpus2020	"CLUECorpus2020 is a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. 
Source: CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model"	https://paperswithcode.com/dataset/cluecorpus2020							
1611	CLUENER2020	"CLUENER2020 is a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories. 
Source: CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark for Chinese"	https://paperswithcode.com/dataset/cluener2020							
1612	CMCNC	"The Coherent Multiple Choice Narrative Cloze (CMCNC) dataset is an evaluation dataset for the multi-choice narrative cloze task, where the goal is to distinguish which event has been held out from a document from a small set of randomly drawn events.
Source: https://arxiv.org/pdf/1711.07611.pdf"	https://paperswithcode.com/dataset/cmcnc		Coherent Multiple Choice Narrative Cloze					
1613	CMD	"Consists of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face-tracks, and metadata about the movie. The dataset is scalable, obtained automatically from YouTube, and is freely available for anybody to download and use. 
Source: Condensed Movies: Story Based Retrieval with Contextual Embeddings"	https://paperswithcode.com/dataset/cmd		Condensed Movies Dataset					
1614	CMRC 2017	"Contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set.
Source: Dataset for the First Evaluation on Chinese Machine Reading Comprehension"	https://paperswithcode.com/dataset/cmrc-2017		Chinese Machine Reading Comprehension 2017					
1615	CMRC 2018	"CMRC 2018 is a dataset for Chinese Machine Reading Comprehension. Specifically, it is a span-extraction reading comprehension dataset that is similar to SQuAD.
Source: http://ymcui.com/cmrc2018/"	https://paperswithcode.com/dataset/cmrc-2018		Chinese Machine Reading Comprehension 2018					
1616	CMRC 2019	"CMRC 2019 is a Chinese Machine Reading Comprehension dataset that was used in The Third Evaluation Workshop on Chinese Machine Reading Comprehension. Specifically, CMRC 2019 is a sentence cloze-style machine reading comprehension dataset that aims to evaluate the sentence-level inference ability.
Source: http://ymcui.com/cmrc2019/"	https://paperswithcode.com/dataset/cmrc-2019		Chinese Machine Reading Comprehension 2019					
1617	CMU DoG	"This is a document grounded dataset for text conversations. ""Document Grounded Conversations"" are conversations that are about the contents of a specified document. In this dataset the specified documents are Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation.
Source: https://github.com/festvox/datasets-CMU_DoG
Image Source: https://arxiv.org/pdf/1809.07358v1.pdf"	https://paperswithcode.com/dataset/cmu-dog		CMU Document Grounded Conversations Dataset					
1618	CMU Panoptic Studio Dataset	"CMU Panoptic Studio is a large scale dataset capturing more than 3 hours of group interaction scenes using 521 heterogeneous sensors. 65 sequences (5.5 hours) and 1.5 millions of 3D skeletons are available.
Source: Panoptic Studio: A Massively Multiview System for Social Interaction Capture"	https://paperswithcode.com/dataset/cmu-panoptic-studio-dataset							
1619	CN-CELEB	"CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world.
Source: CN-CELEB: a challenging Chinese speaker recognition dataset"	https://paperswithcode.com/dataset/cn-celeb							
1620	Coached Conversational Preference Elicitation	"Coached Conversational Preference Elicitation is a dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'.
Image Source: https://www.aclweb.org/anthology/W19-5941"	https://paperswithcode.com/dataset/coached-conversational-preference-elicitation							
1621	CoAID	"CoAID include diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 4,251 news, 296,000 related user engagements, 926 social platform posts about COVID-19, and ground truth labels.
Source: CoAID: COVID-19 Healthcare Misinformation Dataset"	https://paperswithcode.com/dataset/coaid							
1622	Coarse Discourse	"A large corpus of discourse annotations and relations on ~10K forum threads.
Source: Coarse Discourse"	https://paperswithcode.com/dataset/coarse-discourse							
1623	CoarseWSD-20	"The CoarseWSD-20 dataset is a coarse-grained sense disambiguation dataset built from Wikipedia (nouns only) targeting 2 to 5 senses of 20 ambiguous words. It was specifically designed to provide an ideal setting for evaluating Word Sense Disambiguation (WSD) models (e.g. no senses in test sets missing from training), both quantitively and qualitatively.
Source: https://github.com/danlou/bert-disambiguation"	https://paperswithcode.com/dataset/coarsewsd-20							
1624	COCO-QA	"COCO-QA is a dataset for visual question answering. It consists of:

123287 images
78736 train questions
38948 test questions
4 types of questions: object, number, color, location
Answers are all one-word.

Source: Exploring Models and Data for Image Question Answering"	https://paperswithcode.com/dataset/coco-qa							
1625	COCO-Tasks	"Comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated.
Source: What Object Should I Use? - Task Driven Object Detection"	https://paperswithcode.com/dataset/coco-tasks							
1626	COCO-WholeBody	"Extends COCO dataset with whole-body annotations. 
Source: Whole-Body Human Pose Estimation in the Wild
Image source: https://arxiv.org/pdf/2007.11858v1.pdf"	https://paperswithcode.com/dataset/coco-wholebody							
1627	CODEBRIM	"Dataset for multi-target classification of five commonly appearing concrete defects.
Source: Meta-learning Convolutional Neural Architectures for Multi-target Concrete Defect Classification with the COncrete DEfect BRidge IMage Dataset"	https://paperswithcode.com/dataset/codebrim		COncrete DEfect BRidge IMage Dataset					
1628	CodeSwitch-Reddit	"A diverse dataset of written code-switched productions, curated from topical threads of multiple bilingual communities on the Reddit discussion platform, and explore questions that were mainly addressed in the context of spoken language thus far.
Source: CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums"	https://paperswithcode.com/dataset/codeswitch-reddit							
1629	CoDEx Small	"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. 
Source: CoDEx: A Comprehensive Knowledge Graph Completion Benchmark"	https://paperswithcode.com/dataset/codex	16/11/2020						
1630	CoDraw	"The Collaborative Drawing game (CoDraw) dataset contains ~10K dialogs consisting of ~138K messages exchanged between human players in the CoDraw game. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language.
Source: https://github.com/facebookresearch/CoDraw
Image Source: https://github.com/facebookresearch/CoDraw"	https://paperswithcode.com/dataset/codraw							
1631	COG	"A configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory -- problems that remain challenging for modern deep learning architectures. 
Source: A Dataset and Architecture for Visual Reasoning with a Working Memory"	https://paperswithcode.com/dataset/cog							
1632	Colorectal Adenoma	"Colorectal Adenoma contains 177 whole slide images (156 contain adenoma) gathered and labelled by pathologists from the Department of Pathology, The Chinese PLA General Hospital.
Source: https://github.com/ThoroughImages/CAMEL"	https://paperswithcode.com/dataset/colorectal-adenoma							
1633	COMETA	"Consists of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph.
Source: COMETA: A Corpus for Medical Entity Linking in the Social Media"	https://paperswithcode.com/dataset/cometa							
1634	comma 2k19	"comma 2k19 is a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. The dataset was collected using comma EONs that have sensors similar to those of any modern smartphone including a road-facing camera, phone GPS, thermometers and a 9-axis IMU. 
Source: A Commute in Data: The comma2k19 Dataset"	https://paperswithcode.com/dataset/comma-2k19							
1635	comma.ai	The comma.ai dataset consists of 7 and a quarter hours of largely highway driving.	https://paperswithcode.com/dataset/comma-ai-1							
1636	COMP6	"COMP6 is a benchmark for evaluating the extensibility of machine-learning based molecular potentials. It contains a diverse set of organic molecules.
Source: https://github.com/isayev/COMP6"	https://paperswithcode.com/dataset/comp6		COmprehensive Machine-learning Potential					
1637	CompGuessWhat?!	"CompGuessWhat?! extends the original GuessWhat?! datasets with a rich semantic representations in the form of scene graphs associated with every image used as reference scene for the guessing games.
Source: CompGuessWhat?!"	https://paperswithcode.com/dataset/compguesswhat							
1638	Composable activities dataset	"The Composable activities dataset consists of 693 videos that contain activities in 16 classes performed by 14 actors. Each activity is composed of 3 to 11 atomic actions. RGB-D data for each sequence is captured using a Microsoft Kinect sensor and estimate position of relevant body joints.
The dataset provides annotations of the activity for each video and the actions for each of the four human parts (left/right arm and leg) for each frame in every video.
Source: Discriminative Hierarchical Modeling of Spatio-Temporally Composable Human Activities
Image Source: https://ialillo.sitios.ing.uc.cl/ActionsCVPR2014/"	https://paperswithcode.com/dataset/composable-activities-dataset							
1639	Composed Quora	"The Composed Quora dataset consists of questions extracted from Quora that are grouped together if they are asking the same thing. The dataset contains 60,400 groups of questions, each group with at least 3 questions that are asking the same.
Source: https://arxiv.org/pdf/1911.02747.pdf"	https://paperswithcode.com/dataset/composed-quora							
1640	ComQA	"ComQA is a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology.
Source: ComQA: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters
Image Source: http://qa.mpi-inf.mpg.de/comqa/"	https://paperswithcode.com/dataset/comqa							
1641	ConceptNet	"ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. 
Source: ConceptNet 5.5: An Open Multilingual Graph of General Knowledge
Image Source: Speer et al"	https://paperswithcode.com/dataset/conceptnet	11/12/2018						
1642	CONCODE	"A new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment.
Source: Mapping Language to Code in Programmatic Context"	https://paperswithcode.com/dataset/concode							
1643	CoNLL-2000	"CoNLL-2000 is a dataset for dividing text into syntactically related non-overlapping groups of words, so-called text chunking. 
Source: https://www.clips.uantwerpen.be/conll2000/chunking/"	https://paperswithcode.com/dataset/conll-2000-1							
1644	ContentWise Impressions	"The ContentWise Impressions dataset is a collection of implicit interactions and impressions of movies and TV series from an Over-The-Top media service, which delivers its media contents over the Internet. The dataset is distinguished from other already available multimedia recommendation datasets by the availability of impressions, i.e., the recommendations shown to the user, its size, and by being open-source.
The items in the dataset represent the multimedia content that the service provided to the users and are represented by an anonymized numerical identifier. The items refer to television and cinema products belonging to four mutually exclusive categories: movies, movies and clips in series, TV movies or shows, and episodes of TV series.
The interactions represent the actions performed by users on items in the service and are associated with the timestamp when it occurred. Interactions contain the identifier of the impressions, except in those cases where the recommendations came from a row added by the service provider. The interactions are categorized in four different types: views, detail, ratings, and purchases.
The impressions refer to the recommended items that were presented to the user and are identified by their series. Impressions consist of a numerical identifier, the list position on the screen, the length of the recommendation list, and an ordered list of recommended series identifiers, where the most relevant item is in the first position.
Source: https://github.com/ContentWise/contentwise-impressions"	https://paperswithcode.com/dataset/contentwise-impressions							
1645	Contour Drawing Dataset	"A new dataset of contour drawings.
Source: Photo-Sketching: Inferring Contour Drawings from Images"	https://paperswithcode.com/dataset/contour-drawing-dataset							
1646	Controversial News Topic Datasets	"Corpus of controversial news articles extracted from Twitter. Contains news from three different topics: Beef Ban – controversy over the slaughter and sale of beef on religious grounds (1543
articles) is localised to a particular region, mainly Indian subcontinent, while Gun Control – restrictions on carrying, using, or purchasing firearms (6494 articles) and Capital Punishment – use of the death penalty (7905 articles) are
topical in various regions around the world.
Source: Comparative Document Summarisation via Classification"	https://paperswithcode.com/dataset/controversial-news-topic-datasets							
1647	CONVERSE	"A novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors.
Source: From Pose to Activity: Surveying Datasets and Introducing CONVERSE"	https://paperswithcode.com/dataset/converse							
1648	Cookie	"The dataset is constructed from an Amazon review corpus by integrating both user-agent dialogue and custom knowledge graphs for recommendation.
Source: COOKIE: A Dataset for Conversational Recommendation over Knowledge Graphs in E-commerce"	https://paperswithcode.com/dataset/cookie							
1649	Cops-Ref	"Cops-Ref is a dataset for visual reasoning in context of referring expression comprehension with two main features.
Source: Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension
Image Source: https://github.com/zfchenUnique/Cops-Ref"	https://paperswithcode.com/dataset/cops-ref							
1650	COQE	"Contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models.
Source: See the Glass Half Full: Reasoning about Liquid Containers, their Volume and Content"	https://paperswithcode.com/dataset/coqe		Containers Of liQuid contEnt					
1651	CORe50	"CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context.
Source: CORe50: a New Dataset and Benchmark for Continuous Object Recognition"	https://paperswithcode.com/dataset/core50							
1652	Cornell Movie-Dialogs Corpus	"This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:

220,579 conversational exchanges between 10,292 pairs of movie characters
involves 9,035 characters from 617 movies
in total 304,713 utterances
movie metadata included:
genres
release year
IMDB rating
number of IMDB votes
IMDB rating


character metadata included:
gender (for 3,774 characters)
position on movie credits (3,321 characters)



Source: Cornell Movie-Dialogs Corpus"	https://paperswithcode.com/dataset/cornell-movie-dialogs-corpus							
1653	Cornell Movie-Quotes Corpus	"A corpus of movie quotes, annotated with memorability information, in which one is able to control for both the speaker and the setting of the quotes.
Source: You Had Me at Hello: How Phrasing Affects Memorability"	https://paperswithcode.com/dataset/cornell-movie-quotes-corpus							
1654	COS960	"A benchmark dataset with 960 pairs of Chinese wOrd Similarity, where all the words have two morphemes in three Part of Speech (POS) tags with their human annotated similarity rather than relatedness. 
Source: COS960: A Chinese Word Similarity Dataset of 960 Word Pairs"	https://paperswithcode.com/dataset/cos960							
1655	CoS-E	"CoS-E consists of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations
Source: Explain Yourself! Leveraging Language Models for Commonsense Reasoning"	https://paperswithcode.com/dataset/cos-e		Commonsense Explanations Dataset					
1656	CO-SKEL dataset	"A benchmark dataset for the co-skeletonization task.
Source: Object Co-Skeletonization With Co-Segmentation"	https://paperswithcode.com/dataset/co-skel-dataset							
1657	CoSQL	"CoSQL is a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. 
Source: CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"	https://paperswithcode.com/dataset/cosql		Conversational Text-to-SQL Challenge					
1658	COSTRA 1.0	"COSTRA 1.0 is a dataset of complex sentence transformations. The dataset is intended for the study of sentence-level embeddings beyond simple word alternations or standard paraphrasing. The first version of the dataset is limited to sentences in Czech but the construction method is universal and the authors plan to use it also for other languages. The dataset consist of 4,262 unique sentences with average length of 10 words, illustrating 15 types of modifications such as simplification, generalization, or formal and informal language variation.
Source: COSTRA 1.0: A Dataset of Complex Sentence Transformations"	https://paperswithcode.com/dataset/costra-1-0							
1659	COUGH	"A large challenging dataset, COUGH, for COVID-19 FAQ retrieval. Specifically, similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO).
Source: COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval"	https://paperswithcode.com/dataset/cough							
1660	COUNTER	"The COUNTER (COrpus of Urdu News TExt Reuse) corpus contains 600 source-derived document pairs collected from the field of journalism. It can be used to evaluate mono-lingual text reuse detection systems in general and specifically for Urdu language.
The corpus has 600 source and 600 derived documents. It contains in total 275,387 words (tokens), 21,426 unique words and 10,841 sentences. It has been manually annotated at document level with three levels of reuse: wholly derived (135), partially derived (288) and non derived (177).
Source: COUNTER"	https://paperswithcode.com/dataset/counter							
1661	COVERAGE	"COVERAGE contains copymove forged (CMFD) images and their originals with similar but genuine objects (SGOs). COVERAGE is designed to highlight and address tamper detection ambiguity of popular methods, caused by self-similarity within natural images. In COVERAGE, forged–original pairs are annotated with (i) the duplicated and forged region masks, and (ii) the tampering factor/similarity metric. For benchmarking, forgery quality is evaluated using (i) computer vision-based methods, and (ii) human detection performance.
Source: COVERAGE"	https://paperswithcode.com/dataset/coverage		Copy-Move Forgery Database with Similar but Genuine Objects					
1662	COVID19-Algeria-and-World-Dataset	"A coronavirus dataset with 98 countries constructed from different reliable sources, where each row represents a country, and the columns represent geographic, climate, healthcare, economic, and demographic factors that may contribute to accelerate/slow the spread of the COVID-19. The assumptions for the different factors are as follows:
Source: https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset
Image Source: https://github.com/SamBelkacem/COVID19-Algeria-and-World-Dataset"	https://paperswithcode.com/dataset/covid19-algeria-and-world-dataset							
1663	COVID19-CountryImage	"The Covid19-CountryImage dataset is a Twitter dataset which contains COVID-19-related tweets.
Source: https://github.com/thunlp/COVID19-CountryImage"	https://paperswithcode.com/dataset/covid19-countryimage							
1664	COVID-19-CT-CXR	"A public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. 
Source: COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature"	https://paperswithcode.com/dataset/covid-19-ct-cxr							
1665	COVID-19 Image Data Collection	"Contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19.
Source: COVID-19 Image Data Collection"	https://paperswithcode.com/dataset/covid-19-image-data-collection	25/03/2020						
1666	COVID-19 Twitter Chatter Dataset	"A large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing.
Source: A large-scale COVID-19 Twitter chatter dataset for open scientific research -- an international collaboration"	https://paperswithcode.com/dataset/covid-19-twitter-chatter-dataset							
1667	COVID-CQ	"COVID-CQ is a stance data set of user-generated content on Twitter in the context of COVID-19.
Source: A Stance Data Set on Polarized Conversations on Twitter about the Efficacy of Hydroxychloroquine as a Treatment for COVID-19
Image Source: https://arxiv.org/pdf/2009.01188v2.pdf"	https://paperswithcode.com/dataset/covid-cq							
1668	COVID-CT	"Contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic. 
Source: COVID-CT-Dataset: A CT Scan Dataset about COVID-19"	https://paperswithcode.com/dataset/covid-ct							
1669	COVIDGR	"Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the COVIDGR-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered.
Source: https://github.com/ari-dasci/covidgr"	https://paperswithcode.com/dataset/covidgr							
1670	Covid-HeRA	"Covid-HeRA is a dataset for health risk assessment and severity-informed decision making in the presence of COVID19 misinformation. It is a benchmark dataset for risk-aware health misinformation detection, related to the 2019 coronavirus pandemic. Social media posts (Twitter) are annotated based on the perceived likelihood of health behavioural changes and the perceived corresponding risks from following unreliable advice found online.
Source: https://github.com/TIMAN-group/covid19_misinformation"	https://paperswithcode.com/dataset/covid-hera							
1671	CovidQA	"The beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. 
Source: Rapidly Bootstrapping a Question Answering Dataset for COVID-19"	https://paperswithcode.com/dataset/covidqa							
1672	COVIDx	"An open access benchmark dataset comprising of 13,975 CXR images across 13,870 patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge.
Source: COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images"	https://paperswithcode.com/dataset/covidx		COVIDx CRX-2					
1673	CoVoST	"CoVoST is a large-scale multilingual speech-to-text translation corpus. Its latest 2nd version covers translations from 21 languages into English and from English into 15 languages. It has total 2880 hours of speech and is diversified with 78K speakers and 66 accents.
Source: CoVoST 2 and Massively Multilingual Speech-to-Text Translation"	https://paperswithcode.com/dataset/covost	04/02/2020						
1674	COWC	"The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars.
Source: A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning
Image Source: https://gdo152.llnl.gov/cowc/"	https://paperswithcode.com/dataset/cowc		Cars Overhead With Context					
1675	CPCXR	"The COVID-19 Posteroanterior Chest X-Ray fused (CPCXR) dataset is generated by the fusion of three publicly available datasets: COVID-19 cxr image, Radiological Society of North America (RSNA), and U.S. national library of medicine (USNLM) collected Montgomery country - NLM(MC). The dataset consists of samples of diseases labeled as COVID-19, Tuberculosis, Other pneumonia (SARS, MERS, etc.), and Normal. The dataset can be utilized to train an evaulate deep learning and machine learning models as binary and multi-class classification problem.
Source: https://github.com/nspunn1993/COVID-19-PA-CXR-fused-dataset"	https://paperswithcode.com/dataset/cpcxr		COVID-19 Posteroanterior Chest X-Ray fused					
1676	CPH	"A large-scale database including substantial CU partition data for HEVC intra- and inter-modes. This enables deep learning on the CU partition.
Source: Reducing Complexity of HEVC: A Deep Learning Approach"	https://paperswithcode.com/dataset/cph							
1677	CPLFW	"A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. 
There are three motivations behind the construction of CPLFW benchmark as follows:
1.Establishing a relatively more difficult database to evaluate the performance of real world face verification so the effectiveness of several face verification methods can be fully justified.
2.Continuing the intensive research on LFW with more realistic consideration on pose intra-class variation and fostering the research on cross-pose face verification in unconstrained situation. The challenge of CPLFW emphasizes pose difference to further enlarge intra-class variance. Also, negative pairs are deliberately selected to avoid different gender or race. CPLFW considers both the large intra-class variance and the tiny inter-class variance simultaneously.
3.Maintaining the data size, the face verification protocol which provides a 'same/different' benchmark and the same identities in LFW, so one can easily apply CPLFW to evaluate the performance of face verification.
Source: CPLFW"	https://paperswithcode.com/dataset/cplfw		Cross-Pose LFW					
1678	CPP	"A benchmark dataset that consists of 99,000+ sentences for Chinese polyphone disambiguation. 
Source: g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset"	https://paperswithcode.com/dataset/cpp		Chinese Polyphones with Pinyin					
1679	CQR	"CQR is an extension to the Stanford Dialogue Corpus. It contains crowd-sourced rewrites to facilitate research in dialogue state tracking using natural language as the interface.
Source: A dataset for resolving referring expressions in spoken dialogue via contextual query rewrites (CQR)"	https://paperswithcode.com/dataset/cqr		Contextual Query Rewrite					
1680	CraigslistBargains	"A richer dataset based on real items on Craigslist.
Source: Decoupling Strategy and Generation in Negotiation Dialogues"	https://paperswithcode.com/dataset/craigslistbargains							
1681	Common Crawl Domain Names	"Corpus of domain names scraped from Common Crawl and manually annotated to add word boundaries (e.g. ""commoncrawl"" to ""common crawl"").
Source: Common Crawl Domain Names"	https://paperswithcode.com/dataset/common-crawl-domain-names							
1682	CRD3	"The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction.
Source: Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset"	https://paperswithcode.com/dataset/crd3		Critical Role Dungeons and Dragons Dataset					
1683	Creative Flow+ Dataset	"Includes 3000 animated sequences rendered using styles randomly selected from 40 textured line styles and 38 shading styles, spanning the range between flat cartoon fill and wildly sketchy shading. The dataset includes 124K+ train set frames and 10K test set frames rendered at 1500x1500 resolution, far surpassing the largest available optical flow datasets in size.
Source: Creative Flow+ Dataset"	https://paperswithcode.com/dataset/creative-flow-dataset							
1684	CRL-Person	"Provides two large-scale multi-step benchmarks for biometric identification, where the visual appearance of different classes are highly relevant.
Source: Continual Representation Learning for Biometric Identification"	https://paperswithcode.com/dataset/crl-person							
1685	Crowd Dataset	"A dense crowd dataset with manually annotated groundtruth, collected from different public datasets. This dataset comprises 20 videos that exhibit a multitude of motion behaviors that cover both the obvious and subtle instabilities.
Source: Crowd Dataset"	https://paperswithcode.com/dataset/crowd-dataset							
1686	CrowdFix	"Contributes dataset: (1) reviewing the dynamics behind saliency and crowds. (2) using eye tracking to create a dynamic human eye fixation dataset over a new set of crowd videos gathered from the Internet. The videos are annotated into three distinct density levels. 
Source: CrowdFix: An Eyetracking Dataset of Real Life Crowd Videos"	https://paperswithcode.com/dataset/crowdfix							
1687	CrowdFlow	"The TUB CrowdFlow is a synthetic dataset that contains 10 sequences showing 5 scenes. Each scene is rendered twice: with a static point of view and a dynamic camera to simulate drone/UAV based surveillance. The scenes are render using Unreal Engine at HD resolution (1280x720) at 25 fps, which is typical for current commercial CCTV surveillance systems. The total number of frames is 3200.
Each sequence has the following ground-truth data:

Optical flow fields
Person trajectories (up to 1451)
Dense pixel trajectories

Source: Optical Flow Dataset and Benchmark for Visual Crowd Analysis"	https://paperswithcode.com/dataset/crowdflow		TUB CrowdFlow					
1688	CrowS-Pairs	"CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.
Source: CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"	https://paperswithcode.com/dataset/crows-pairs							
1689	CRVD	"The CRVD dataset consists of 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600.
Source: Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic Scenes"	https://paperswithcode.com/dataset/crvd		Captured Raw Video Denoising					
1690	CS	"This dataset is constructed and based on the online free-access fictions that are tagged with sci-fi, urban novel, love story, youth, etc. It is used for Writing Polishment with Smile (WPS) a task that aims to polish plain text with similes.
All similes are extracted by rich regular expression, and the extraction precision is estimated as 92% by labelling 500 random extracted samples.
It contains 5M samples for training and 2.5k for validation and test respectively.
Source: https://github.com/mrzjy/writing-polishment-with-simile"	https://paperswithcode.com/dataset/cs		Chinese Simile					
1691	CSD	"Comprises 4 different subsets - Flat, House, Priory and Lab - each containing a number of different sequences that can be successfully relocalised against each other. 
Source: CSD"	https://paperswithcode.com/dataset/csd		Collaborative SLAM Dataset					
1692	CSPubSum	"CSPubSum is a dataset for summarisation of computer science publications, created by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. 
Source: A Supervised Approach to Extractive Summarisation of Scientific Papers"	https://paperswithcode.com/dataset/cspubsum							
1693	CSQA	"Contains around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in the dialogs require a larger subgraph of the KG. 
Source: Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph"	https://paperswithcode.com/dataset/csqa							
1694	CSS10	"A collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts.
Source: CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages"	https://paperswithcode.com/dataset/css10							
1695	CTC	"A dataset that allows exploration of cross-modal retrieval where images contain scene-text instances. 
Source: StacMR: Scene-Text Aware Cross-Modal Retrieval"	https://paperswithcode.com/dataset/ctc		COCO-Text Captioned					
1696	CTPelvic1K	"Curates a large pelvic CT dataset pooled from multiple sources and different manufacturers, including 1, 184 CT volumes and over 320, 000 slices with different resolutions and a variety of the above-mentioned appearance variations.
Source: Deep Learning to Segment Pelvic Bones: Large-scale CT Datasets and Baseline Models"	https://paperswithcode.com/dataset/ctpelvic1k							
1697	Cube++	"Cube++ is a novel dataset for the color constancy problem that continues on the Cube+ dataset. It includes 4890 images of different scenes under various conditions. For calculating the ground truth illumination, a calibration object with known surface colors was placed in every scene.
Source: https://github.com/Visillect/CubePlusPlus
Image Source: https://github.com/Visillect/CubePlusPlus"	https://paperswithcode.com/dataset/cube							
1698	CubiCasa5K	"CubiCasa5K is a large-scale floorplan image dataset containing 5000 samples annotated into over 80 floorplan object categories. The dataset annotations are performed in a dense and versatile manner by using polygons for separating the different objects.
Source: https://github.com/CubiCasa/CubiCasa5k"	https://paperswithcode.com/dataset/cubicasa5k							
1699	CUHK-QA	"CUHK-QA is a dataset for natural language-based person search using iterative questioning. 
The dataset consists of 400 images of 360 people, and  20 participants answered 5 specific questions about the appearance of each person. So, each person has labelled 20 images. Average length of combined description for each image is 39.15. All the images have been taken from the test set of CUHK-PEDES dataset.
Source: Interactive Natural Language-based Person Search"	https://paperswithcode.com/dataset/cuhk-qa							
1700	CUHK-Shadow	"Collects shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. The dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. 
Source: Revisiting Shadow Detection: A New Benchmark Dataset for Complex World"	https://paperswithcode.com/dataset/cuhk-shadow							
1701	Cumulo	"A benchmark dataset for training and evaluating global cloud classification models. It consists of one year of 1km resolution MODIS hyperspectral imagery merged with pixel-width 'tracks' of CloudSat cloud labels. 
Source: Cumulo: A Dataset for Learning Cloud Classes"	https://paperswithcode.com/dataset/cumulo							
1702	Curated AFD	"The Curated AFD dataset is a curated version of the Asian Face Dataset (AFD) for face recognition research. The original AFD dataset has been curated to remove wrong identity labels, duplicate images and duplicate subjects.
Source: https://arxiv.org/abs/2004.03074"	https://paperswithcode.com/dataset/curated-afd							
1703	Curation Corpus	"The Curation Corpus is a collection of 40,000 professionally-written summaries of news articles, with links to the articles themselves. 
Source: Curation Corpus"	https://paperswithcode.com/dataset/curation-corpus							
1704	CURE-TSD	"Based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. 
Source: Traffic Sign Detection under Challenging Conditions: A Deeper Look Into Performance Variations and Spectral Characteristics"	https://paperswithcode.com/dataset/cure-tsd		CURE Traffic Sign Detection					
1705	CURE-TSR	"Includes more than two million traffic sign images that are based on real-world and simulator data. 
Source: CURE-TSR: Challenging Unreal and Real Environments for Traffic Sign Recognition"	https://paperswithcode.com/dataset/cure-tsr		CURE Traffic Sign Recognition					
1706	Curiosity	"The Curiosity dataset consists of 14K dialogs (with 181K utterances) with fine-grained knowledge groundings, dialog act annotations, and other auxiliary annotation. In this dataset users and virtual assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages.
Source: https://github.com/facebookresearch/curiosity
Image Source: https://www.pedro.ai/curiosity"	https://paperswithcode.com/dataset/curiosity							
1707	Czech restaurant information	"Czech restaurant information is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as a translation of the English San Francisco Restaurants dataset by Wen et al. (2015).
Source: https://github.com/UFAL-DSG/cs_restaurant_dataset"	https://paperswithcode.com/dataset/czech-restaurant-information							
1708	CzEng 2.0 Parallel Corpus	"Czech-English parallel corpus CzEng 2.0 consisting of over 2 billion words (2 ""gigawords"") in each language. The corpus contains document-level information and is filtered with several techniques to lower the amount of noise.
Source: Announcing CzEng 2.0 Parallel Corpus with over 2 Gigawords"	https://paperswithcode.com/dataset/czeng-2-0-parallel-corpus							
1709	D2City	"A large-scale comprehensive collection of dashcam videos collected by vehicles on DiDi's platform. D2-City contains more than 10000 video clips which deeply reflect the diversity and complexity of real-world traffic scenarios in China.
Source: D$^2$-City: A Large-Scale Dashcam Video Dataset of Diverse Traffic Scenarios"	https://paperswithcode.com/dataset/d2city							
1710	MVTec D2S	"MVTec D2S is a benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21,000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse.
Source: MVTec D2S: Densely Segmented Supermarket Dataset
Image Source: https://www.mvtec.com/company/research/datasets/mvtec-d2s"	https://paperswithcode.com/dataset/mvtec-d2s		MVTec Densely Segmented Supermarket					
1711	DAD	"Contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving. 
Source: Driver Anomaly Detection: A Dataset and Contrastive Learning Approach"	https://paperswithcode.com/dataset/dad		Driver Anomaly Detection					
1712	DAIS	"A large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. 
Source: Investigating representations of verb bias in neural language models"	https://paperswithcode.com/dataset/dais							
1713	DAiSEE	"DAiSEE is a multi-label video classification dataset comprising of 9,068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration ""in the wild"". The dataset has four levels of labels namely - very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists. 
Source: DAiSEE: Towards User Engagement Recognition in the Wild"	https://paperswithcode.com/dataset/daisee							
1714	Danbooru2020	"A large-scale anime image database with 4.2m+ images annotated with 130m+ text tags describing image contents in detail; it can be useful for machine learning purposes such as image recognition and generation. It has been applied to a wide variety of applications, particularly generative modeling.
Danbooru20xx is updated annually with the previous years' images & metadata improvements. Previous iterations: Danbooru2017, Danbooru2018, Danbooru2019.
Source: Danbooru2020."	https://paperswithcode.com/dataset/danbooru2020	03/02/2018						
1715	DaNE	"Danish Dependency Treebank (DaNE) is a named entity annotation for the Danish Universal Dependencies treebank using the CoNLL-2003 annotation scheme.
Source: DaNE: A Named Entity Resource for Danish"	https://paperswithcode.com/dataset/dane	01/05/2020	Danish Dependency Treebank					
1716	DART	"DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.
Source: DART: Open-Domain Structured Data Record to Text Generation"	https://paperswithcode.com/dataset/dart							
1717	DAVANet	"A large-scale multi-scene dataset for stereo deblurring, containing 20,637 blurry-sharp stereo image pairs from 135 diverse sequences and their corresponding bidirectional disparities.
Source: DAVANet: Stereo Deblurring with View Aggregation"	https://paperswithcode.com/dataset/davanet							
1718	Da Vinci Dataset	"A line drawing restoration dataset which consists of 71 line drawing sketches by Leonardo Da Vinci.
Source: Da Vinci Dataset"	https://paperswithcode.com/dataset/da-vinci-dataset							
1719	DAWN	"DAWN emphasizes a diverse traffic environment (urban, highway and freeway) as well as a rich variety of traffic flow. The DAWN dataset comprises a collection of 1000 images from real-traffic environments, which are divided into four sets of weather conditions: fog, snow, rain and sandstorms. The dataset is annotated with object bounding boxes for autonomous driving and video surveillance scenarios. This data helps interpreting effects caused by the adverse weather conditions on the performance of vehicle detection systems.
Source: DAWN: Vehicle Detection in Adverse Weather Nature Dataset
Image Source: Kenk and Hassaballah"	https://paperswithcode.com/dataset/dawn							
1720	DAWT	"The DAWT dataset consists of Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. 
Source: DAWT: Densely Annotated Wikipedia Texts across multiple languages"	https://paperswithcode.com/dataset/dawt		Densely Annotated Wikipedia Texts					
1721	DBpedia NIF	"The dataset provides the content of all articles for 128 Wikipedia languages. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. 
Source: DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus"	https://paperswithcode.com/dataset/dbpedia-nif							
1722	DDAD	"DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba).
Source: https://github.com/TRI-ML/DDAD
Image Source: https://github.com/TRI-ML/DDAD"	https://paperswithcode.com/dataset/ddad		Dense Depth for Autonomous Driving					
1723	DDD17	"DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface. 
Source: DDD17: End-To-End DAVIS Driving Dataset
Image Source: DVS Driving Dataset 2017 (DDD17) README"	https://paperswithcode.com/dataset/ddd17		DAVIS Driving Dataset 2017					
1724	DDD20	"The dataset was captured with a DAVIS camera that concurrently streams both dynamic vision sensor (DVS) brightness change events and active pixel sensor (APS) intensity frames. DDD20 is the longest event camera end-to-end driving dataset to date with 51h of DAVIS event+frame camera and vehicle human control data collected from 4000km of highway and urban driving under a variety of lighting conditions.
Source: DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction"	https://paperswithcode.com/dataset/ddd20		DAVIS Driving Dataset 2020					
1725	DDI-100	"The DDI-100 dataset is a synthetic dataset for text detection and recognition based on 7000 real unique document pages and consists of more than 100000 augmented images. The ground truth comprises text and stamp masks, text and characters bounding boxes with relevant annotations.
Source: DDI-100"	https://paperswithcode.com/dataset/ddi-100		Distorted Document Images					
1726	DECADE	"DECADE is a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements.
Source: Who Let The Dogs Out? Modeling Dog Behavior From Visual Data"	https://paperswithcode.com/dataset/decade							
1727	DeepFashion2	"DeepFashion2 is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs
Source: DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images
Image Source: https://github.com/switchablenorms/DeepFashion2"	https://paperswithcode.com/dataset/deepfashion2							
1728	Deep Fashion3D	"A novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations.
Source: Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images"	https://paperswithcode.com/dataset/deep-fashion3d							
1729	DeepFish	"DeepFish as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 habitats in the marine environments of tropical Australia. It contains classification labels as well as point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes.
Source: https://github.com/alzayats/DeepFish
Image Source: https://github.com/alzayats/DeepFish"	https://paperswithcode.com/dataset/deepfish	04/09/2020						
1730	OST	"Is one of the largest egocentric datasets in the object search task with eyetracking information available
Source: Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks"	https://paperswithcode.com/dataset/deep-future-gaze		Egocentric Dataset					
1731	DeepScores	"DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. For advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding.
Source: DeepScores -- A Dataset for Segmentation, Detection and Classification of Tiny Objects
Image Source: https://tuggeluk.github.io/deepscores/"	https://paperswithcode.com/dataset/deepscores							
1732	DeepWeeds	"The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora.
Source: https://github.com/AlexOlsen/DeepWeeds
Image Source: https://github.com/AlexOlsen/DeepWeeds"	https://paperswithcode.com/dataset/deepweeds							
1733	DeepWriting	"A new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation.
Source: DeepWriting: Making Digital Ink Editable via Deep Generative Modeling"	https://paperswithcode.com/dataset/deepwriting							
1734	Definite Pronoun Resolution Dataset	"Composes sentence pairs (i.e., twin sentences).
Source: Resolving Complex Cases of Definite Pronouns: The Winograd Schema Challenge"	https://paperswithcode.com/dataset/definite-pronoun-resolution-dataset							
1735	DEFT Corpus	"A SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. 
Source: SemEval-2020 Task 6: Definition extraction from free text with the DEFT corpus"	https://paperswithcode.com/dataset/deft-corpus	01/08/2019						
1736	DemCare	"Dem@Care is providing the following datasets, which are collected during lab and home experiments. The data collection took place in the Greek Alzheimer’s Association for Dementia and Related Disorders in Thessaloniki, Greece and in participants’ homes. The datasets include video and audio recordings as well as data from physiological sensors. Moreover, they include data from sleep, motion and plug sensors.
Source: DemCare"	https://paperswithcode.com/dataset/demcare							
1737	Dengue	"Benchmark dataset for low-resource multiclass classification, with 4,015 training, 500 testing, and 500 validation examples, each labeled as part of five classes. Each sample can be a part of multiple classes. Collected as tweets and originally used in Livelo & Cheng (2018).
Source: Dengue"	https://paperswithcode.com/dataset/dengue							
1738	DensePose	"DensePose-COCO is a large-scale ground-truth dataset with image-to-surface correspondences manually annotated
on 50K COCO images and train DensePose-RCNN, to densely regress part-specific UV coordinates within every human
region at multiple frames per second.
Source: DensePose: Dense Human Pose Estimation In The Wildc
Image Source: Guler et al"	https://paperswithcode.com/dataset/densepose	01/02/2018	DensePose-COCO					
1739	DesireDB	"Includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context.
Source: Modelling Protagonist Goals and Desires in First-Person Narrative"	https://paperswithcode.com/dataset/desiredb							
1740	DeSMOG	"A dataset of stance-labeled GW sentences.
Source: DeSMOG: Detecting Stance in Media On Global Warming"	https://paperswithcode.com/dataset/desmog							
1741	DET	"DET is a lane detection dataset that consists of the raw event data, accumulated images over 30ms and corresponding lane labels. Contains 17,103 lane instances, each of which is labeled pixel by pixel manually. 
Source: DET"	https://paperswithcode.com/dataset/det							
1742	UA-DETRAC	"Consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. 
Source: UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking
Image Source: UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking"	https://paperswithcode.com/dataset/ua-detrac							
1743	DFDC	"The DFDC (Deepfake Detection Challenge) is a dataset for deepface detection consisting of more than 100,000 videos.
The DFDC dataset consists of two versions:

Preview dataset. with 5k videos. Featuring two facial modification algorithms.
Full dataset, with 124k videos. Featuring eight facial modification algorithms

Source: The Deepfake Detection Challenge (DFDC) Preview Dataset"	https://paperswithcode.com/dataset/dfdc		Deepfake Detection Challenge					
1744	DFW	"Contains over 11000 images of 1000 identities with different types of disguise accessories. The dataset is collected from the Internet, resulting in unconstrained face images similar to real world settings. 
Source: Recognizing Disguised Faces in the Wild"	https://paperswithcode.com/dataset/dfw		Disguised Faces in the Wild					
1745	DHP19	"DHP19 is the first human pose dataset with data collected from DVS event cameras. 
It has recordings from 4 synchronized 346x260 pixel DVS cameras and marker positions in 3D space from Vicon motion capture system. The files have event streams and 3D positions recorded from 17 subjects each performing 33 movements.
Source: DHP19"	https://paperswithcode.com/dataset/dhp19		Dynamic Vision Sensor 3D Human Pose Dataset					
1746	Diabetes60	"RGB-D images of 60 western dishes, home made. Data was recorded using a Microsoft Kinect V2.
Source: Diabetes60 Dataset"	https://paperswithcode.com/dataset/diabetes60							
1747	Diabetic Foot Ulcers Classification Datasets	"Contains Diabetic Foot Ulcers (DFU) from different patients.
Source: DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer Classification"	https://paperswithcode.com/dataset/diabetic-foot-ulcers-classification-datasets							
1748	Diabetic Retinopathy Detection Dataset	"A large scale of retina image dataset.
Source: Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation"	https://paperswithcode.com/dataset/diabetic-retinopathy-detection-dataset							
1749	DiaBLa	"A new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and reference translations produced a posteriori. 
Source: DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation"	https://paperswithcode.com/dataset/diabla							
1750	DialoGLUE	"DialoGLUE is a natural language understanding nenchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks.
Source: DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue"	https://paperswithcode.com/dataset/dialoglue							
1751	DialogueFairness	"The Dialogue Fairness dataset is used to evaluate and understand fairness in dialogue models, focusing on gender and racial biases.
Source: https://github.com/zgahhblhc/DialogueFairness"	https://paperswithcode.com/dataset/dialoguefairness							
1752	DIB-10K	"Is a challenging image dataset which has more than 10 thousand different types of birds. It was created to enable the study of machine learning and also ornithology research.
Source: The DongNiao International Birds 10000 Dataset"	https://paperswithcode.com/dataset/dib-10k		DongNiao International Birds 10000					
1753	DIPS	"Contains biases but is two orders of magnitude larger than those used previously. 
Source: End-to-End Learning on 3D Protein Structure for Interface Prediction"	https://paperswithcode.com/dataset/dips							
1754	Diseases in Neurology Case Reports Dataset	"Extracts diseases and syndromes (DsSs) from more than 65,000 neurology case reports from 66 journals in PubMed over the last six decades from 1955 to 2017.
Source: Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to 2017 with Text Mining"	https://paperswithcode.com/dataset/diseases-in-neurology-case-reports-dataset							
1755	DiveFace	"A new face annotation dataset with balanced distribution between genders and ethnic origins. 
Source: SensitiveNets: Learning Agnostic Representations with Application to Face Images"	https://paperswithcode.com/dataset/diveface							
1756	DLBCL-Morph	"DLBCL-Morph is a dataset containing 42 digitally scanned high-resolution tissue microarray (TMA) slides accompanied by clinical, cytogenetic, and geometric features from 209 DLBCL cases.
Source: https://github.com/stanfordmlgroup/DLBCL-Morph"	https://paperswithcode.com/dataset/dlbcl-morph							
1757	dMelodies	"dMelodies is dataset of simple 2-bar melodies generated using 9 independent latent factors of variation where each data point represents a unique melody based on the following constraints:
- Each melody will correspond to a unique scale (major, minor, blues, etc.).
- Each melody plays the arpeggios using the standard I-IV-V-I cadence chord pattern.
- Bar 1 plays the first 2 chords (6 notes), Bar 2 plays the second 2 chords (6 notes).
- Each played note is an 8th note.
Source: https://github.com/ashispati/dmelodies_dataset"	https://paperswithcode.com/dataset/dmelodies							
1758	DMQA	The DeepMind Q&A Dataset consists of two datasets for Question Answering, CNN and DailyMail. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.	https://paperswithcode.com/dataset/dmqa		DeepMind Q&A					
1759	doc2dial	"A new dataset of goal-oriented dialogues that are grounded in the associated documents.
Source: doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset"	https://paperswithcode.com/dataset/doc2dial							
1760	DocBank	"A benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the \LaTeX{} documents available on the arXiv.com.
Source: DocBank: A Benchmark Dataset for Document Layout Analysis"	https://paperswithcode.com/dataset/docbank							
1761	DocVQA	"DocVQA consists of 50,000 questions defined on 12,000+ document images.
Source: DocVQA: A Dataset for VQA on Document Images"	https://paperswithcode.com/dataset/docvqa							
1762	DOGC	"Intended to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies.
Source: Parallel Data, Tools and Interfaces in OPUS"	https://paperswithcode.com/dataset/dogc							
1763	DoMSEV	"The Dataset of Multimodal Semantic Egocentric Video (DoMSEV) contains 80-hours of multimodal (RGB-D, IMU, and GPS) data related to First-Person Videos with annotations for recorder profile, frame scene, activities, interaction, and attention.
Source: A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos"	https://paperswithcode.com/dataset/domsev		Dataset of Multimodal Semantic Egocentric Video					
1764	DoQA	"A dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing.
Source: DoQA -- Accessing Domain-Specific FAQs via Conversational QA"	https://paperswithcode.com/dataset/doqa							
1765	DOTmark	"DOTmark is a benchmark for discrete optimal transport, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy.
Source: DOTmark - A Benchmark for Discrete Optimal Transport"	https://paperswithcode.com/dataset/dotmark		Discrete Optimal Transport Benchmark					
1766	DPC-Captions	"This is an open-source image captions dataset for the aesthetic evaluation of images.
The dataset is called DPC-Captions, which contains comments of up to five aesthetic attributes of one image through knowledge transfer from a full-annotated small-scale dataset.
Source: https://github.com/BestiVictory/DPC-Captions"	https://paperswithcode.com/dataset/dpc-captions							
1767	DPED	"A large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera.
Source: DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks"	https://paperswithcode.com/dataset/dped		DSLR Photo Enhancement Dataset					
1768	DpgMedia2019	"DpgMedia2019 is a Dutch news dataset for partisanship detection. It contains more than 100K articles that are labelled on the publisher level and 776 articles that were crowdsourced using an internal survey platform and labelled on the article level.
Source: https://github.com/dpgmedia/partisan-news2019"	https://paperswithcode.com/dataset/dpgmedia2019							
1769	DramaQA	"The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. The dataset is built upon the TV drama ""Another Miss Oh"" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels.
Source: DramaQA: Character-Centered Video Story Understanding with Hierarchical QA"	https://paperswithcode.com/dataset/dramaqa							
1770	Dreaddit	"Consists of 190K posts from five different categories of Reddit communities.
Source: Dreaddit: A Reddit Dataset for Stress Analysis in Social Media"	https://paperswithcode.com/dataset/dreaddit							
1771	DR(eye)VE	"DR(eye)VE is a large dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500,000 registered frames, matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements.
Source: Predicting the Driver's Focus of Attention: the DR(eye)VE Project
Image Source: http://imagelab.ing.unimore.it/dreyeve"	https://paperswithcode.com/dataset/dr-eye-ve							
1772	Drive&Act	"The Drive&Act dataset is a state of the art multi modal benchmark for driver behavior recognition. The dataset includes 3D skeletons in addition to frame-wise hierarchical labels of 9.6 Million frames captured by 6 different views and 3 modalities (RGB, IR and depth).
It offers following key features:

12h of video data in 29 long sequences
Calibrated multi view camera system with 5 views
Multi modal videos: NIR, Depth and Color data
Markerless motion capture: 3D Body Pose and Head Pose
Model of the static interior of the car
83 manually annotated hierarchical activity labels:
Level 1: Long running tasks (12)
Level 2: Semantic actions (34)
Level 3: Object Interaction tripplets [action|object|location] (6|17|14)



Source: Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles"	https://paperswithcode.com/dataset/drive-act							
1773	DrivingStereo	"DrivingStereo contains over 180k images covering a diverse set of driving scenarios, which is hundreds of times larger than the KITTI Stereo dataset. High-quality labels of disparity are produced by a model-guided filtering strategy from multi-frame LiDAR points. 
Source: DrivingStereo: A Large-Scale Dataset for Stereo Matching in Autonomous Driving Scenarios
Image Source: https://drivingstereo-dataset.github.io/"	https://paperswithcode.com/dataset/drivingstereo							
1774	Drone Tracking	"This dataset contains videos where a flying drone (hexacopter) is captured with multiple consumer-grade cameras (smartphones, compact cameras, gopro,...) with highly accurate 3D drone trajectory ground truth recorderd by a precise real-time RTK system from Fixposition. In some videos, the ground truth temporal synchronization and ground truth camera locations are also provided.
Source: https://github.com/CenekAlbl/drone-tracking-datasets
Image Source: https://github.com/CenekAlbl/drone-tracking-datasets"	https://paperswithcode.com/dataset/drone-tracking							
1775	DSBI	"The Double-Sided Braille Image dataset (DSBI) is a large-scale dataset for Braille image recognition. It has detailed Braille recto dots, verso dots and Braille cells annotation.
Source: https://arxiv.org/abs/1811.10893
Image Source: https://github.com/yeluo1994/DSBI"	https://paperswithcode.com/dataset/dsbi		Double-Sided Braille Image					
1776	dSprites	"dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.
All possible combinations of these latents are present exactly once, generating N = 737280 total images.
Source: dSprites"	https://paperswithcode.com/dataset/dsprites		Disentanglement testing Sprites dataset					
1777	DublinCity	"A novel benchmark dataset that includes a manually annotated point cloud for over 260 million laser scanning points into 100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects are labelled into 13 classes using hierarchical levels of detail from large (i.e., building, vegetation and ground) to refined (i.e., window, door and tree) elements. 
Source: DublinCity: Annotated LiDAR Point Cloud and its Applications"	https://paperswithcode.com/dataset/dublincity							
1778	Dunhuang Grottoes Painting Dataset	"This dataset provides a large number of training and testing example which is sufficient for a deep learning approach to address Dunhuang Grotto Painting restoration.
Source: Dunhuang Grottoes Painting Dataset and Benchmark"	https://paperswithcode.com/dataset/dunhuang-grottoes-painting-dataset							
1779	DuRecDial	"A human-to-human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). 
Source: Towards Conversational Recommendation over Multi-Type Dialogs"	https://paperswithcode.com/dataset/durecdial							
1780	DVQA	DVQA is a synthetic question-answering dataset on images of bar-charts.	https://paperswithcode.com/dataset/dvqa		Data Visualizations via Question Answering					
1781	DVS128 Gesture	"Comprises 11 hand gesture categories from 29 subjects under 3 illumination conditions.
Source: A Low Power, Fully Event-Based Gesture Recognition System"	https://paperswithcode.com/dataset/dvs128-gesture-dataset							
1782	DWIE	"The 'Deutsche Welle corpus for Information Extraction' (DWIE) is a multi-task dataset that combines four main Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document.
Source: https://arxiv.org/abs/2009.12626"	https://paperswithcode.com/dataset/dwie		Deutsche Welle corpus for Information Extraction					
1783	Dynamic FAUST	"Dynamic FAUST extends the FAUST dataset to dynamic 4D data. It consists of high-resolution 4D scans of human subjects in motion, captured at 60 fps.
Source: Dynamic FAUST: Registering Human Bodies in Motion"	https://paperswithcode.com/dataset/dynamic-faust							
1784	DynaSent	"DynaSent is an English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers.
Source: DynaSent: A Dynamic Benchmark for Sentiment Analysis"	https://paperswithcode.com/dataset/dynasent							
1785	E2E	"End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena.
Source: Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge"	https://paperswithcode.com/dataset/e2e	28/06/2017	End-to-End NLG Challenge					
1786	ECUSTFD	"The ECUST Food Dataset is a food recognition dataset that contains 2978 images
Source: https://github.com/Liang-yc/ECUSTFD-resized-
Image Source: https://github.com/Liang-yc/ECUSTFD-resized-"	https://paperswithcode.com/dataset/ecustfd		ECUST Food Dataset					
1787	Edge-Map-345C	"Edge-Map-345C is a large-scale edge-map dataset including 290,281 edge-maps corresponding to 345 object categories of QuickDraw dataset. In particular, these 345 categories are corresponding to the 345 free-hand sketch categories of Google QuickDraw dataset.
Source: https://github.com/PengBoXiangShang/EdgeMap345C_Dataset
Image Source: https://github.com/PengBoXiangShang/EdgeMap345C_Dataset"	https://paperswithcode.com/dataset/edge-map-345c							
1788	Edina-DR	"Edina-DR is a novel corpus of discourse relation pairs; the first of its kind to attempt to identify the discourse relations connecting the dialogic turns in open-domain discourse. 
Source: Implicit Discourse Relation Identification for Open-domain Dialogues"	https://paperswithcode.com/dataset/edina-dr							
1789	EdNet	"A large-scale hierarchical dataset of diverse student activities collected by Santa, a multi-platform self-study solution equipped with artificial intelligence tutoring system. EdNet contains 131,441,538 interactions from 784,309 students collected over more than 2 years, which is the largest among the ITS datasets released to the public so far.
Source: EdNet: A Large-Scale Hierarchical Dataset in Education"	https://paperswithcode.com/dataset/ednet							
1790	EDUB-Seg	Egocentric Dataset of the University of Barcelona – Segmentation (EDUB-Seg) is a dataset for egocentric event segmentation acquired by the Narrative Clip, which takes a picture every 30 seconds. The dataset contains a total of 18,735 images captured by 7 different users during overall 20 days. To ensure diversity, all users were wearing the camera in different contexts: while attending a conference, on holiday, during the weekend, and during the week.	https://paperswithcode.com/dataset/edub-seg		Egocentric Dataset of the University of Barcelona – Segmentation					
1791	EgoCap	"EgoCap is a dataest of 100,000 egocentric images of eight people in different clothing, with 75,000 images from six people used for training. The images have been captured with two fisheye cameras.
Source: EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras"	https://paperswithcode.com/dataset/egocap							
1792	EgoHands	"The EgoHands dataset contains 48 Google Glass videos of complex, first-person interactions between two people. The main intention of this dataset is to enable better, data-driven approaches to understanding hands in first-person computer vision. The dataset offers

high quality, pixel-level segmentations of hands
the possibility to semantically distinguish between the observer’s hands and someone else’s hands, as well as left and right hands
virtually unconstrained hand poses as actors freely engage in a set of joint activities
lots of data with 15,053 ground-truth labeled hands

Source: Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions"	https://paperswithcode.com/dataset/egohands							
1793	EGOK360	"Contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve. 
Source: Egok360: A 360 Egocentric Kinetic Human Activity Video Dataset"	https://paperswithcode.com/dataset/egok360							
1794	EgoShots	"Egoshots is a 2-month Ego-vision Dataset with Autographer Wearable Camera annotated ""for free"" with transfer learning. Three state of the art pre-trained image captioning models are used. The dataset represents the life of 2 interns while working at Philips Research (Netherlands) (May-July 2015) generously donating their data.
Source: https://github.com/NataliaDiaz/Egoshots"	https://paperswithcode.com/dataset/egoshots							
1795	EYTH	"Includes egocentric videos containing hands in the wild.
Source: Analysis of Hand Segmentation in the Wild"	https://paperswithcode.com/dataset/eyth		EgoYouTubeHands					
1796	Egyptian Arabic Segmentation Dataset	"Contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs.
Source: A Neural Architecture for Dialectal Arabic Segmentation"	https://paperswithcode.com/dataset/egyptian-arabic-segmentation-dataset							
1797	EiTB-ParCC	"A large comparable corpus for Basque-Spanish was prepared, on the basis of independently-produced news by the Basque public broadcaster EiTB.
Source: Handle with Care: A Case Study in Comparable Corpora Exploitation for Neural Machine Translation"	https://paperswithcode.com/dataset/eitb-parcc							
1798	Electro-Magnetic Emanations Interception Dataset	"An open data corpus of 123.610 labeled samples, 
Source: Electro-Magnetic Side-Channel Attack Through Learned Denoising and Classification"	https://paperswithcode.com/dataset/electro-magnetic-emanations-interception							
1799	Elsevier OA CC-BY	"An open corpus of Scientific Research papers which has a representative sample from across scientific disciplines. This corpus not only includes the full text of the article, but also the metadata of the documents, along with the bibliographic information for each reference.
Source: Elsevier OA CC-By Corpus"	https://paperswithcode.com/dataset/elsevier-oa-cc-by							
1800	EMBER	"A labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign).
Source: EMBER: An Open Dataset for Training Static PE Malware Machine Learning Models"	https://paperswithcode.com/dataset/ember							
1801	EmoBank	"EmoBank is a corpus of 10k English sentences balancing multiple genres, annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. 
Source: EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis"	https://paperswithcode.com/dataset/emobank							
1802	EMOTIC	"The EMOTIC dataset, named after EMOTions In Context, is a database of images with people in real environments, annotated with their apparent emotions. The images are annotated with an extended list of 26 emotion categories combined with the three common continuous dimensions Valence, Arousal and Dominance.
Source: Context Based Emotion Recognition using EMOTIC Dataset"	https://paperswithcode.com/dataset/emotic	30/03/2020	EMOTIons in Context					
1803	CARER	"CARER is an emotion dataset collected through noisy labels, annotated via distant supervision as in (Go et al., 2009). 
The subset of data provided here corresponds to the six emotions variant described in the paper. The six emotions are anger, fear, joy, love, sadness, and surprise.
Source: CARER: Contextualized Affect Representations for Emotion Recognition"	https://paperswithcode.com/dataset/emotion	01/11/2018	Contextualized Affect Representations for Emotion Recognition					
1804	EMU	"48k question-answer pairs written in rich natural language.
Source: Edited Media Understanding: Reasoning About Implications of Manipulated Images"	https://paperswithcode.com/dataset/emu		Edited Media Understanding					
1805	EndoSLAM	"The endoscopic SLAM dataset (EndoSLAM) is a dataset for depth estimation approach for endoscopic videos. It consists of both ex-vivo and synthetically generated data. The ex-vivo part of the dataset includes standard as well as capsule endoscopy recordings. The dataset is divided into 35 sub-datasets. Specifically, 18, 5 and 12 sub-datasets exist for colon, small intestine and stomach respectively.
Source: https://github.com/CapsuleEndoscope/EndoSLAM
Image Source: https://github.com/CapsuleEndoscope/EndoSLAM"	https://paperswithcode.com/dataset/endoslam		Endoscopic SLAM dataset					
1806	ENT-DESC	"ENT-DESC involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (KG), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions.
Source: ENT-DESC: Entity Description Generation by Exploring Knowledge Graph"	https://paperswithcode.com/dataset/ent-desc							
1807	EORSSD	"The Extended Optical Remote Sensing Saliency Detection (EORSSD) dataset is an extension of the ORSSD dataset. This new dataset is larger and more varied than the original. It contains 2,000 images and corresponding pixel-wise ground truth, which includes many semantically meaningful but challenging images.
Source: https://github.com/rmcong/EORSSD-dataset
Image Source: Zhang et al"	https://paperswithcode.com/dataset/eorssd		Extended Optical Remote Sensing Saliency Detection					
1808	EPIE	"Corpus containing 25206 sentences labelled with lexical instances of 717 idiomatic expressions. These spans also cover literal usages for the given set of idiomatic expressions. 
Source: EPIE Dataset: A Corpus For Possible Idiomatic Expressions"	https://paperswithcode.com/dataset/epie							
1809	ERA	"Consists of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in various circumstances and at dramatically various scales.
Source: ERA: A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos"	https://paperswithcode.com/dataset/era		Event Recognition in Aerial videos					
1810	ESAD	"ESAD is a large-scale dataset designed to tackle the problem of surgeon action detection in endoscopic minimally invasive surgery. ESAD aims at contributing to increase the effectiveness and reliability of surgical assistant robots by realistically testing their awareness of the actions performed by a surgeon. The dataset provides bounding box annotation for 21 action classes on real endoscopic video frames captured during prostatectomy, and was used as the basis of a recent MIDL 2020 challenge. 
Source: ESAD: Endoscopic Surgeon Action Detection Dataset"	https://paperswithcode.com/dataset/esad	07/04/2021	SARAS Endoscopic Surgeon Action Detection					
1811	eSCAPE	"Consists of millions of entries in which the MT element of the training triplets has been obtained by translating the source side of publicly-available parallel corpora, and using the target side as an artificial human post-edit. Translations are obtained both with phrase-based and neural models.
Source: eSCAPE: a Large-scale Synthetic Corpus for Automatic Post-Editing"	https://paperswithcode.com/dataset/escape							
1812	e-SNLI	"e-SNLI is used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets.
Source: e-SNLI: Natural Language Inference with Natural Language Explanations
Image Source: https://arxiv.org/pdf/1812.01193v2.pdf"	https://paperswithcode.com/dataset/e-snli							
1813	eSports Sensors Dataset	"The eSports Sensors dataset contains sensor data collected from 10 players in 22 matches in League of Legends. The sensor data collected includes:

Hand/head/chair movements.
Heart rate.
Muscle activity.
Gaze movement on the monitor.
Galvanic skin response(GSR).
Electroencephalography (EEG).
Mouse and keyboard activity.
Facial skin temperature.
Environmental data.

The data were collected for one team of 5 people simultaneously. In-game logs and meta information for each match are also provided for each match.
Source: https://github.com/smerdov/eSports_Sensors_Dataset"	https://paperswithcode.com/dataset/esports-sensors-dataset	02/11/2020						
1814	esXNLI	"esXNLI is a bilingual NLI dataset. It comprises 2,490 examples from 5 different genres that were originally annotated in Spanish, and translated into English by professional translators. It serves as a counterpoint to XNLI, which was originally annotated in English and translated into 14 other languages, including Spanish. The dataset was conceived to be used in conjunction with the XNLI development set to analyse the effect of translation in cross-lingual transfer learning.
Source: https://github.com/artetxem/esxnli"	https://paperswithcode.com/dataset/esxnli							
1815	ETH3D	"ETHD is a multi-view stereo benchmark / 3D reconstruction benchmark that covers a variety of indoor and outdoor scenes.
Ground truth geometry has been obtained using a high-precision laser scanner.
A DSLR camera as well as a synchronized multi-camera rig with varying field-of-view was used to capture images.
Source: A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos"	https://paperswithcode.com/dataset/eth3d							
1816	ETHICS	"A new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality.
Source: Aligning AI With Shared Human Values"	https://paperswithcode.com/dataset/ethics-1							
1817	ETHOS	"ETHOS is a hate speech detection dataset. It is built from YouTube and Reddit comments validated through a crowdsourcing platform. It has two subsets, one for binary classification and the other for multi-label classification. The former contains 998 comments, while the latter contains fine-grained hate-speech annotations for 433 comments.
Source: https://arxiv.org/abs/2006.08328"	https://paperswithcode.com/dataset/ethos		multi-labEl haTe speecH detectiOn dataSet					
1818	ETH Py150 Open	"A massive, deduplicated corpus of 7.4M Python files from GitHub.
Source: Learning and Evaluating Contextual Embedding of Source Code"	https://paperswithcode.com/dataset/eth-py150-open							
1819	ETH-XGaze	"Consists of over one million high-resolution images of varying gaze under extreme head poses. The dataset is collected from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. 
Source: ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation
Official competition on Codalab: https://competitions.codalab.org/competitions/28930"	https://paperswithcode.com/dataset/eth-xgaze							
1820	eTRIMS Image Database	"The database is comprised of two datasets, the 4-Class eTRIMS Dataset with 4 annotated object classes and the 8-Class eTRIMS Dataset with 8 annotated object classes.
Source: eTRIMS Image Database"	https://paperswithcode.com/dataset/etrims-image-database							
1821	ETT	"The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value ”oil temperature” and 6 power load features. The train/val/test is 12/4/4 months.
Source: https://arxiv.org/pdf/2012.07436.pdf
Image Source: https://github.com/zhouhaoyi/ETDataset"	https://paperswithcode.com/dataset/ett		Electricity Transformer Temperature					
1822	EuroCity Persons	The EuroCity Persons dataset provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211,200).	https://paperswithcode.com/dataset/eurocity-persons							
1823	Europarl ConcoDisco Dataset	"The ConcoDisco Corpus is an English-French parallel corpus with discourse relations (DRs) and discourse connectives (DCs) annotations.
Source: https://github.com/mjlaali/Europarl-ConcoDisco
Image Source: https://github.com/mjlaali/Europarl-ConcoDisco"	https://paperswithcode.com/dataset/europarl-concodisco-dataset							
1824	Europarl-ST	"Europarl-ST is a multilingual SLT corpus containing paired audio-text samples for SLT from and into 6 European languages, for a total of 30 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. 
Source: Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates"	https://paperswithcode.com/dataset/europarl-st							
1825	Europeana Newspapers	"Europeana Newspapers consists of four datasets with 100 pages each for the languages Dutch, French, German (including Austrian) as part of the Europeana Newspapers project is expected to contribute to the further development and improvement of named entity recognition systems with a focus on historical content.
Source: An Open Corpus for Named Entity Recognition in Historic Newspapers"	https://paperswithcode.com/dataset/europeana-newspapers							
1826	European Flood 2013 Dataset	"This dataset consists of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution).
Source: https://github.com/cvjena/eu-flood-dataset
Image Source: https://github.com/cvjena/eu-flood-dataset"	https://paperswithcode.com/dataset/european-flood-2013-dataset							
1827	Event-Camera Dataset	"The Event-Camera Dataset is a collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files.
Source: The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM"	https://paperswithcode.com/dataset/event-camera-dataset							
1828	Event-focused Emotion Corpora for German and English	"A corpus designed in analogy to the well-established English ISEAR emotion dataset.
Source: Crowdsourcing and Validating Event-focused Emotion Corpora for German and English"	https://paperswithcode.com/dataset/event-focused-emotion-corpora-for-german-and							
1829	EventKG+Click	"Builds upon the event-centric EventKG knowledge graph and language-specific information on user interactions with events, entities, and their relations derived from the Wikipedia clickstream.
Source: EventKG+Click: A Dataset of Language-specific Event-centric User Interaction Traces"	https://paperswithcode.com/dataset/eventkg-click							
1830	Event-QA	"Contains 1000 semantic queries and the corresponding English, German and Portuguese verbalizations for EventKG - an event-centric knowledge graph with more than 970 thousand events.
Source: Event-QA: A Dataset for Event-Centric Question Answering over Knowledge Graphs"	https://paperswithcode.com/dataset/event-qa							
1831	Evidence Inference	"Evidence Inference is a corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. 
Source: Inferring Which Medical Treatments Work from Reports of Clinical Trials"	https://paperswithcode.com/dataset/evidence-inference							
1832	EV-IMO	"Includes accurate pixel-wise motion masks, egomotion and ground truth depth. 
Source: EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event Cameras"	https://paperswithcode.com/dataset/ev-imo							
1833	EXAMS	"A new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. Collects more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. EXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. 
Source: EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering"	https://paperswithcode.com/dataset/exams							
1834	ExDark	"The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes.
Source: https://github.com/cs-chan/Exclusively-Dark-Image-Dataset"	https://paperswithcode.com/dataset/exdark		Exclusively Dark Image Dataset					
1835	EXEQ-300k	"The EXEQ-300k dataset contains 290,479 detailed questions with corresponding math headlines from Mathematics Stack Exchange. The dataset can be used to generate concise math headlines from detailed math questions.
Source: https://arxiv.org/pdf/1912.00839.pdf"	https://paperswithcode.com/dataset/exeq-300k							
1836	Explainable Abstract Trains	"An image dataset containing simplified representations of trains. It aims to provide a platform for the application and research of algorithms for justification and explanation extraction. The dataset is accompanied by an ontology that conceptualizes and classifies the depicted trains based on their visual characteristics, allowing for a precise understanding of how each train was labeled. Each image in the dataset is annotated with multiple attributes describing the trains' features and with bounding boxes for the train elements. 
Source: Explainable Abstract Trains Dataset"	https://paperswithcode.com/dataset/explainable-abstract-trains							
1837	ExPose	"Curates a dataset of SMPL-X fits on in-the-wild images.
Source: Monocular Expressive Body Regression through Body-Driven Attention"	https://paperswithcode.com/dataset/expose		EXpressive POse and Shape rEgression					
1838	ExpW	The Expression in-the-Wild (ExpW) dataset is for facial expression recognition and contains 91,793 faces manually labeled with expressions. Each of the face images is annotated as one of the seven basic expression categories: “angry”, “disgust”, “fear”, “happy”, “sad”, “surprise”, or “neutral”.	https://paperswithcode.com/dataset/expw	21/09/2016	Expression in-the-Wild					
1839	ExtremeWeather	"Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change.
Source: ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events"	https://paperswithcode.com/dataset/extremeweather							
1840	EyeQ	"Dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., Good',Usable' and `Reject') for evaluating RIQA methods. 
Source: Evaluation of Retinal Image Quality Assessment Networks in Different Color-spaces"	https://paperswithcode.com/dataset/eyeq							
1841	Facebook Post Reactions	"Collects posts (and their reactions) from Facebook pages of large supermarket chains.
Source: Social Emotion Mining Techniques for Facebook Posts Reaction Prediction"	https://paperswithcode.com/dataset/facebook-post-reactions							
1842	FaceForensics++	"FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries.
Source: https://github.com/ondyari/FaceForensics
Image Source: https://github.com/ondyari/FaceForensics"	https://paperswithcode.com/dataset/faceforensics-1							
1843	FairFace	"FairFace is a face image dataset which is race balanced. It contains 108,501 images from 7 different race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups.
Source: https://github.com/joojs/fairface"	https://paperswithcode.com/dataset/fairface							
1844	FAKBAT	"The Freebase Annotations of TREC KBA 2014 Stream Corpus with Timestamps (FAKBAT) is an extension of the FAKBA1 dataset that contains entity age and entity timestamp. It comprises roughly 1.2 billion timestamped documents from global public news wires, blogs, forums, and shortened links shared on social media. It spans 572 days (October 7, 2011–May 1, 2013).
Source: https://arxiv.org/pdf/1701.04039.pdf"	https://paperswithcode.com/dataset/fakbat							
1845	Fakeddit	"Fakeddit is a novel multimodal dataset for fake news detection consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision.
Source: https://fakeddit.netlify.app/"	https://paperswithcode.com/dataset/fakeddit							
1846	Fake News Filipino Dataset	"Expertly-curated benchmark dataset for fake news detection in Filipino.
Source: Localization of Fake News Detection via Multitask Transfer Learning"	https://paperswithcode.com/dataset/fake-news-filipino-dataset							
1847	FPDS	"A benchmark for detecting fallen people lying on the floor. It consists of 6982 images, with a total of 5023 falls and 2275 non falls corresponding to people in conventional situations (standing up, sitting, lying on the sofa or bed, walking, etc). Almost all the images have been captured in indoor environments with very different situations: variation of poses and sizes, occlusions, lighting changes, etc.
Source: FPDS"	https://paperswithcode.com/dataset/fpds		Fallen People Data Set					
1848	FAS100K	"FAS100K is a large-scale visual localization dataset. This dataset is comprised of two traverses of 238 and 130 kms respectively where the latter is a partial repeat of the former. The data was collected using stereo cameras in Australia under sunny day conditions. It covers a variety of road and environment types including urban and rural areas. The raw image data from one of the cameras streaming at 5 Hz constitutes 63,650 and 34,497 image frames for the two traverses respectively.
Source: https://arxiv.org/pdf/2001.08434.pdf"	https://paperswithcode.com/dataset/fas100k							
1849	Fashion 144K	"Fashion 144K is a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information.
Source: Neuroaesthetics in Fashion: Modeling the Perception of Fashionability"	https://paperswithcode.com/dataset/fashion-144k							
1850	Fashion-Gen	"Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles.
Source: Fashion-Gen: The Generative Fashion Dataset and Challenge
Image Source: Rostamzadeh et al"	https://paperswithcode.com/dataset/fashion-gen							
1851	Fashion IQ	"Fashion IQ support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images.
Source: Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback"	https://paperswithcode.com/dataset/fashion-iq							
1852	Fashionpedia	"Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. 
Source: Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset"	https://paperswithcode.com/dataset/fashionpedia							
1853	FAT	"Falling Things (FAT) is a dataset for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. It consists of generated photorealistic images with accurate 3D pose annotations for all objects in 60k images.
The 60k annotated photos of 21 household objects are taken from the YCB objects set. For each image, the dataset contains the 3D poses, per-pixel class segmentation, and 2D/3D bounding box coordinates for all objects.
Source: Falling Things: A Synthetic Dataset for 3D Object Detection and Pose Estimation"	https://paperswithcode.com/dataset/fat		Falling Things					
1854	FB15k-237-low	"The FB15k-237-low dataset is a variation of the FB15k-237 dataset where relations with a low number of triplets are kept.
Source: https://arxiv.org/pdf/1911.03091.pdf"	https://paperswithcode.com/dataset/fb15k-237-low							
1855	FCDB	"Consists of 76 million geo-tagged images in 16 cosmopolitan cities.
Source: Changing Fashion Cultures"	https://paperswithcode.com/dataset/fcdb		Fashion Culture DataBase					
1856	FDDB-360	"A 360-degree fisheye-like version of the popular FDDB face detection dataset.
Source: FDDB-360: Face Detection in 360-degree Fisheye Images"	https://paperswithcode.com/dataset/fddb-360							
1857	FDF	"A diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds. 
Source: DeepPrivacy: A Generative Adversarial Network for Face Anonymization"	https://paperswithcode.com/dataset/fdf		Flickr Diverse Faces					
1858	FDST	"The Fudan-ShanghaiTech dataset (FDST) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes
Source: https://arxiv.org/abs/1907.07911"	https://paperswithcode.com/dataset/fdst		Fudan-ShanghaiTech					
1859	FeathersV1	"The FeatherV1 dataset is a dataset for fine-grained visual classification. It contains 28,272 images of feathers categorized by 595 bird species.
Source: https://github.com/feathers-dataset/feathersv1-dataset
Image Source: https://github.com/feathers-dataset/feathersv1-dataset"	https://paperswithcode.com/dataset/feathersv1							
1860	FewGlue	"FewGLUE consists of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.
Source: FewGlue"	https://paperswithcode.com/dataset/fewglue							
1861	FewRel 2.0	"A more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations?
Source: FewRel 2.0: Towards More Challenging Few-Shot Relation Classification"	https://paperswithcode.com/dataset/fewrel-2-0							
1862	FFHQ-Aging	"FFHQ-Aging is a Dataset of human faces designed for benchmarking age transformation algorithms as well as many other possible vision tasks.
This dataset is an extention of the NVIDIA FFHQ dataset, on top of the 70,000 original FFHQ images, it also contains the following information for each image:
* Gender information (male/female with confidence score)
* Age group information (10 classes with confidence score)
* Head pose (pitch, roll & yaw)
* Glasses type (none, normal or dark)
* Eye occlusion score (0-100, different score for each eye)
* Full semantic map (19 classes, based on CelebAMask-HQ labels)
Source: https://github.com/royorel/FFHQ-Aging-Dataset
Image Source: https://github.com/royorel/FFHQ-Aging-Dataset"	https://paperswithcode.com/dataset/ffhq-aging							
1863	FGADR	"This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis.
Source: A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability"	https://paperswithcode.com/dataset/fgadr							
1864	FIGRIM	"This is a dataset of 9428 images, 1754 of which are target images with memorability scores.
The images span 21 scene categories from the SUN database. Each scene category was chosen to contain at least 300 images of size 700x700 or greater. All images were cropped to 700x700 pixels.
Source: FIGRIM"	https://paperswithcode.com/dataset/figrim		FIne-GRained Image Memorability					
1865	FigureQA	"FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. 
Source: FigureQA: An Annotated Figure Dataset for Visual Reasoning"	https://paperswithcode.com/dataset/figureqa							
1866	FinChat	"Finnish chat conversation corpus and includes unscripted conversations on seven topics from people of different ages. 
Source: FinChat: Corpus and evaluation setup for Finnish chat conversations on everyday topics"	https://paperswithcode.com/dataset/finchat		Finnish Chat Conversations on Everyday Topics					
1867	Fine-grained 3D Pose	"A new large-scale dataset that consists of 409 fine-grained categories and 31,881 images with accurate 3D pose annotation.
Source: Improving Annotation for 3D Pose Dataset of Fine-Grained Object Categories"	https://paperswithcode.com/dataset/fine-grained-3d-pose							
1868	Fine-Grained R2R	"This dataset enriches the benchmark Room-to-Room (R2R) dataset by dividing the instructions into sub-instructions and pairing each of those with their corresponding viewpoints in the path. The overall instruction and trajectory of each sample remains the same.
Source: https://github.com/YicongHong/Fine-Grained-R2R"	https://paperswithcode.com/dataset/fine-grained-r2r							
1869	Finer	Finnish News Corpus for Named Entity Recognition (Finer) is a corpus that consists of 953 articles (193,742 word tokens) with six named entity classes (organization, location, person, product, event,and date). The articles are extracted from the archives of Digitoday, a Finnish online technology news source.	https://paperswithcode.com/dataset/finer		Finnish News Corpus for Named Entity Recognition					
1870	FinnSentiment	"FinnSentiment introduces a 27,000 sentence dataset (in Finnish) annotated independently with sentiment polarity by three native annotators. 
Source: FinnSentiment -- A Finnish Social Media Corpus for Sentiment Polarity Annotation"	https://paperswithcode.com/dataset/finnsentiment							
1871	First-Person Hand Action Benchmark	"First-Person Hand Action Benchmark is a collection of RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. 
Source: First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations"	https://paperswithcode.com/dataset/first-person-hand-action-benchmark							
1872	FIVR-200K	The FIVR-200K dataset has been collected to simulate the problem of Fine-grained Incident Video Retrieval (FIVR). The dataset comprises 225,960 videos associated with 4,687 Wikipedia events and 100 selected video queries.	https://paperswithcode.com/dataset/fivr-200k							
1873	FIW-MM	"A large-scale dataset for recognizing kinship in multimedia which extend FIW with multimedia data (i.e., video, audio, and contextual transcripts). 
Source: Families In Wild Multimedia (FIW-MM): A Multi-Modal Database for Recognizing Kinship"	https://paperswithcode.com/dataset/fiw-mm		Families In Wild Multimedia					
1874	FLAME	"FLAME is a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms.
Source: Aerial Imagery Pile burn detection using Deep Learning: the FLAME dataset"	https://paperswithcode.com/dataset/flame		Fire Luminosity Airborne-based Machine learning Evaluation					
1875	Flickr1024	"Contains 1024 pairs of high-quality images and covers diverse scenarios.
Source: Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution"	https://paperswithcode.com/dataset/flickr1024							
1876	Flickr Cropping Dataset	"The Flick Cropping Dataset consists of high quality cropping and pairwise ranking annotations used to evaluate the performance of automatic image cropping approaches.
Source: https://arxiv.org/abs/1701.01480"	https://paperswithcode.com/dataset/flickr-cropping-dataset							
1877	Flightmare Simulator	"Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently from each other. Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) an integration with a virtual-reality headset for interaction with the simulated environment. Flightmare can be used for various applications, including path-planning, reinforcement learning, visual-inertial odometry, deep learning, human-robot interaction, etc.
Source: Flightmare Simulator"	https://paperswithcode.com/dataset/flightmare-simulator							
1878	FLoRes	FLoRes is a benchmark dataset for machine translation between English and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia.	https://paperswithcode.com/dataset/flores	01/11/2019	Facebook Low Resource MT Benchmark					
1879	Florence 3D Faces	"This dataset is being constructed specifically to support research on techniques that bridge the gap between 2D, appearance-based recognition techniques, and fully 3D approaches. It is designed to simulate, in a controlled fashion, realistic surveillance conditions and to probe the efficacy of exploiting 3D models in real scenarios.
Source: FLORENCE 3D FACES"	https://paperswithcode.com/dataset/florence-3d-faces							
1880	FMD	"The Fluorescence Microscopy Denoising (FMD) dataset is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. Image averaging is used to effectively obtain ground truth images and 60,000 noisy images with different noise levels.
Source: https://arxiv.org/abs/1812.10366
Image Source: https://github.com/bmmi/denoising-fluorescence"	https://paperswithcode.com/dataset/fmd		Fluorescence Microscopy Denoising					
1881	fMoW	"Functional Map of the World (fMoW) is a dataset that aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. 
Source: Functional Map of the World
Image Source: Christie et al"	https://paperswithcode.com/dataset/fmow		Functional Map of the World					
1882	FocusPath	"FocusPath is a dataset compiled from diverse Whole Slide Image (WSI) scans in different focus (z-) levels. Images are naturally blurred by out-of-focus lens provided with GT scores of focus levels. The dataset can be used for No-Reference Focus Quality assessment of Digital Pathology/Microscopy images.
Source: FocusPath"	https://paperswithcode.com/dataset/focuspath							
1883	FollowUp	"1000 query triples on 120 tables.
Source: FANDA: A Novel Approach to Perform Follow-up Query Analysis"	https://paperswithcode.com/dataset/followup							
1884	Fon-French Dataset	"FFR Dataset is an ongoing project to collect, clean and store corpora of Fon and French sentences for machine translation from Fon-French. Fon (also called Fongbe) is an African-indigenous language spoken mostly in Benin, by about 1.7 million people. As training data is crucial to the high performance of a machine learning model, the aim of the project is to compile the largest set of training corpora for the research and design of translation and NLP models involving Fon. There are 117,029 parallel Fon-French sentences at the moment.
Source: FFR Dataset"	https://paperswithcode.com/dataset/fon-french-dataset							
1885	FoodX-251	"FoodX-251 is a dataset of 251 fine-grained classes with 118k training, 12k validation and 28k test images. Human verified labels are made available for the training and test images. The classes are fine-grained and visually similar, for example, different types of cakes, sandwiches, puddings, soups, and pastas.
Source: FoodX-251: A Dataset for Fine-grained Food Classification
Image Source: Kaur et al"	https://paperswithcode.com/dataset/foodx-251							
1886	Ford AV Dataset	"A challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. 
Source: Ford Multi-AV Seasonal Dataset"	https://paperswithcode.com/dataset/ford-av-dataset							
1887	FPL	"Supports new task that predicts future locations of people observed in first-person videos.
Source: Future Person Localization in First-Person Videos"	https://paperswithcode.com/dataset/fpl		First-Person Locomotion					
1888	Frames Dataset	"This dataset is dialog dataset collected in a Wizard-of-Oz fashion. Two humans talked to each other via a chat interface. One was playing the role of the user and the other one was playing the role of the conversational agent. The latter is called a wizard as a reference to the Wizard of Oz, the man behind the curtain. The wizards had access to a database of 250+ packages, each composed of a hotel and round-trip flights. The users were asked to find the best deal. This resulted in complex dialogues where a user would often consider different options, compare packages, and progressively build the description of her ideal trip.
Source: A Frame Tracking Model for Memory-Enhanced Dialogue Systems"	https://paperswithcode.com/dataset/frames-dataset							
1889	Fraxtil	"Fraxtil is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 90 songs choreographed by a single author, with 450 charts for the 90 songs.
Source: https://arxiv.org/pdf/1703.06891.pdf
Image Source: https://github.com/chrisdonahue/ddc"	https://paperswithcode.com/dataset/fraxtil							
1890	FreebaseQA	"FreebaseQA is a data set for open-domain QA over the Freebase knowledge graph. The question-answer pairs in this data set are collected from various sources, including the TriviaQA data set and other trivia websites (QuizBalls, QuizZone, KnowQuiz), and are matched against Freebase to generate relevant subject-predicate-object triples that were further verified by human annotators. As all questions in FreebaseQA are composed independently for human contestants in various trivia-like competitions, this data set shows richer linguistic variation and complexity than existing QA data sets, making it a good test-bed for emerging KB-QA systems.
Source: FreebaseQA: A New Factoid QA Data Set Matching Trivia-Style Question-Answer Pairs with Freebase"	https://paperswithcode.com/dataset/freebaseqa							
1891	French CASS dataset	"Composed of judgments from the French Court of cassation and their corresponding summaries.
Source: STRASS: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings"	https://paperswithcode.com/dataset/french-cass-dataset							
1892	FRSign	"A large-scale and accurate dataset for vision-based railway traffic light detection and recognition.The recordings were made on selected running trains in France and benefited from carefully hand-labeled annotations.
Source: FRSign: A Large-Scale Traffic Light Dataset for Autonomous Trains"	https://paperswithcode.com/dataset/frsign							
1893	FSOCO	"FSOCO is a collaborative dataset for vision-based cone detection systems in Formula Student Driverless competitions. It contains human annotated ground truth labels for both bounding boxes and instance-wise segmentation masks. The data buy-in philosophy of FSOCO asks student teams to contribute to the database first before being granted access ensuring continuous growth. By providing clear labeling guidelines and tools for a sophisticated raw image selection, new annotations are guaranteed to meet the desired quality.
Source: https://github.com/fsoco/fsoco-dataset
Image Source: https://github.com/fsoco/fsoco-dataset"	https://paperswithcode.com/dataset/fsoco							
1894	FT Speech	"FT Speech is a speech corpus created from the recorded meetings of the Danish Parliament, otherwise known as the Folketing (FT). The corpus contains over 1,800 hours of transcribed speech by a total of 434 speakers. It is significantly larger in duration, vocabulary, and amount of spontaneous speech than the existing public speech corpora for Danish, which are largely limited to read-aloud and dictation data. 
Source: FT Speech: Danish Parliament Speech Corpus"	https://paperswithcode.com/dataset/ft-speech							
1895	FUNSD	"Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking.
Source: FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents
Image source: https://guillaumejaume.github.io/FUNSD/"	https://paperswithcode.com/dataset/funsd		Form Understanding in Noisy Scanned Documents					
1896	Fusion 360 Gallery	"The Fusion 360 Gallery Dataset contains rich 2D and 3D geometry data derived from parametric CAD models. The dataset is produced from designs submitted by users of the CAD package Autodesk Fusion 360 to the Autodesk Online Gallery. The dataset provides valuable data for learning how people design, including sequential CAD design data, designs segmented by modelling operation, and design hierarchy and connectivity data.
Source: https://github.com/AutodeskAILab/Fusion360GalleryDataset
Image Source: https://github.com/AutodeskAILab/Fusion360GalleryDataset"	https://paperswithcode.com/dataset/fusion-360-gallery							
1897	FVI	"The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes.
Source: https://arxiv.org/abs/1904.10247
Image Source: https://github.com/amjltc295/Free-Form-Video-Inpainting"	https://paperswithcode.com/dataset/fvi		Free-form Video Inpainting					
1898	G1020	"A large publicly available retinal fundus image dataset for glaucoma classification called G1020. The dataset is curated by conforming to standard practices in routine ophthalmology and it is expected to serve as standard benchmark dataset for glaucoma detection. This database consists of 1020 high resolution colour fundus images and provides ground truth annotations for glaucoma diagnosis, optic disc and optic cup segmentation, vertical cup-to-disc ratio, size of neuroretinal rim in inferior, superior, nasal and temporal quadrants, and bounding box location for optic disc. 
Source: G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided Glaucoma Detection"	https://paperswithcode.com/dataset/g1020							
1899	GamePad Environment	"GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant.
Source: GamePad: A Learning Environment for Theorem Proving"	https://paperswithcode.com/dataset/gamepad-environment							
1900	GameWikiSum	"GameWikiSum is a domain-specific (video game) dataset for multi-document summarization, which is one hundred times larger than commonly used datasets, and in another domain than news. Input documents consist of long professional video game reviews as well as references of their gameplay sections in Wikipedia pages.
Source: https://github.com/Diego999/GameWikiSum"	https://paperswithcode.com/dataset/gamewikisum							
1901	GAP Coreference Dataset	"GAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia and released by Google AI Language for the evaluation of coreference resolution in practical applications.
Source: GAP Coreference Dataset
Image Source: https://arxiv.org/pdf/1810.05201.pdf"	https://paperswithcode.com/dataset/gap-coreference-dataset	11/10/2018						
1902	GASP	"GASP is a dataset composed by a list of cited abstracts associated with the corresponding source abstract. The dataset is composed by a training set of 100000 elements, a test set and a validation set of 10000 each.
The goal is to generate a paper abstract given cited paper's abstracts and model the human creativity behind the process.
Source: https://github.com/ART-Group-it/GASP"	https://paperswithcode.com/dataset/gasp							
1903	Gazeta	"Gazeta is a dataset for automatic summarization of Russian news. The dataset consists of 63,435 text-summary pairs. To form training, validation, and test datasets, these pairs were sorted by time and the first 52,400 pairs are used as the training dataset, the proceeding 5,265 pairs as the validation dataset, and the remaining 5,770 pairs as the test dataset.
Source: https://github.com/IlyaGusev/gazeta"	https://paperswithcode.com/dataset/gazeta							
1904	GCDC	"A corpus of real-world texts.
Source: Discourse Coherence in the Wild: A Dataset, Evaluation and Methods"	https://paperswithcode.com/dataset/gcdc		Grammarly Corpus of Discourse Coherence					
1905	GDXray+	"GDXray+ is a collection of more than 21.100 X-ray images for the development, testing, and evaluation of image analysis and computer vision algorithms.
GDXray includes five groups of images:

Castings,
Welds,
Baggage,
Nature,
Settings.

Each group has several series, and each series several X-ray images.
Source: GDXray"	https://paperswithcode.com/dataset/gdxray							
1906	GeBioCorpus	"A high-quality dataset for machine translation evaluation that aims at being one of the first non-synthetic gender-balanced test datasets.
Source: GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies"	https://paperswithcode.com/dataset/gebiocorpus							
1907	General-100	The General-100 dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small).	https://paperswithcode.com/dataset/general-100	01/08/2016	General-100					
1908	Real Bacteria Dataset	"A genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem.
Source: Likelihood Ratios for Out-of-Distribution Detection"	https://paperswithcode.com/dataset/real-bacteria-dataset							
1909	GeoCoV19	GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content.	https://paperswithcode.com/dataset/geocov19							
1910	GeoFaces	"A large database of geotagged face images.
Source: GeoFaces"	https://paperswithcode.com/dataset/geofaces							
1911	GeoWebNews	"GeoWebNews provides test/train examples and enable fine-grained Geotagging and Toponym Resolution (Geocoding). This dataset is also suitable for prototyping and evaluating machine learning NLP models.
Source: A Pragmatic Guide to Geoparsing Evaluation
Image Source: https://arxiv.org/pdf/1810.12368.pdf"	https://paperswithcode.com/dataset/geowebnews							
1912	Get it #OffMyChest	"Corpus and annotations for the CL-Aff Shared Task - Get it #OffMyChest - from Nanyang Technological University Singapore.
Source: Get it #OffMyChest"	https://paperswithcode.com/dataset/get-it-offmychest							
1913	GGPONC	"German Guideline Program in Oncology NLP Corpus (GGPONC) is a German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions.
Source: GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines
Image Source: https://arxiv.org/pdf/2007.06400.pdf"	https://paperswithcode.com/dataset/ggponc		German Guideline Program in Oncology NLP Corpus					
1914	GiantMIDI-Piano	"GiantMIDI-Piano contains 10,854 unique piano solo pieces composed by 2,786 composers. GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata information of each music piece.
Source: GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music"	https://paperswithcode.com/dataset/giantmidi-piano							
1915	Gibson Environment	Gibson is an opensource perceptual and physics simulator to explore active and real-world perception. The Gibson Environment is used for Real-World Perception Learning.	https://paperswithcode.com/dataset/gibson-environment							
1916	Gigaword Entailment	"The Gigaword Entailment dataset is a dataset for entailment prediction between an article and its headline. It is built from the Gigaword dataset.
Source: https://github.com/nlp-titech/headline-entailment"	https://paperswithcode.com/dataset/gigaword-entailment							
1917	Global Voices	"Global Voices is a multilingual dataset for evaluating cross-lingual summarization methods. It is extracted from social-network descriptions of Global Voices news articles to cheaply collect evaluation data for into-English and from-English summarization in 15 languages. 
Source: Global Voices: Crossing Borders in Automatic News Summarization"	https://paperswithcode.com/dataset/global-voices							
1918	GloREPlus	"A distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. 
Source: Global Textual Relation Embedding for Relational Understanding"	https://paperswithcode.com/dataset/gloreplus							
1919	Goldfinch	"Goldfinch is a dataset for fine-grained recognition challenges. It contains a list of bird, butterfly, aircraft, and dog categories with relevant Google image search and Flickr search URLs. In addition, it also includes a set of active learning annotations on dog categories.
Source: The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition"	https://paperswithcode.com/dataset/goldfinch		GOogLe image-search Dataset					
1920	GolfDB	"GolfDB is a high-quality video dataset created for general recognition applications in the sport of golf, and specifically for the task of golf swing sequencing.
Source: GolfDB: A Video Database for Golf Swing Sequencing"	https://paperswithcode.com/dataset/golfdb							
1921	Sentence Compression	"Sentence Compression is a dataset where the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence where supervised systems which require a structural alignment between the input and output can be successfully trained. 
Source: Overcoming the Lack of Parallel Data in Sentence Compression"	https://paperswithcode.com/dataset/sentence-compression							
1922	Google Landmarks Dataset v2	"This is the second version of the Google Landmarks dataset (GLDv2), which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test
Source: https://github.com/cvdfoundation/google-landmark
Image Source: https://github.com/cvdfoundation/google-landmark"	https://paperswithcode.com/dataset/google-landmarks-dataset-v2							
1923	Google Refexp	"A new large-scale dataset for referring expressions, based on MS-COCO.
Source: Generation and Comprehension of Unambiguous Object Descriptions"	https://paperswithcode.com/dataset/google-refexp							
1924	GOZ	"The Generix Object Zero-shot Learning (GOZ) dataset is a benchmark dataset for zero-shot learning.
Source: https://github.com/TristHas/GOZ"	https://paperswithcode.com/dataset/goz		Generic Object ZSL Dataset					
1925	GRAB	"GRAB is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. It contains 5 male and 5 female participants and 4 different motion intents.
The GRAB dataset also contains binary contact maps between the body and objects.
Source: https://github.com/otaheri/GRAB
Image Source: https://github.com/otaheri/GRAB"	https://paperswithcode.com/dataset/grab							
1926	GRAL	"A new dataset containing over 550K pairs (covering 143 km^2 area) of RGB and aerial LIDAR depth images. 
Source: RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization"	https://paperswithcode.com/dataset/gral							
1927	GraspNet	"A large-scale grasp pose detection dataset with a unified evaluation system. The dataset contains 87,040 RGBD images with over 370 million grasp poses. 
Source: GraspNet: A Large-Scale Clustered and Densely Annotated Dataset for Object Grasping"	https://paperswithcode.com/dataset/graspnet							
1928	Grocery Store	"Grocery Store is a dataset of natural images of grocery items. All natural images were taken with a smartphone camera in different grocery stores. It contains 5,125 natural images from 81 different classes of fruits, vegetables, and carton items (e.g. juice, milk, yoghurt). The 81 classes are divided into 42 coarse-grained classes, where e.g. the fine-grained classes 'Royal Gala' and 'Granny Smith' belong to the same coarse-grained class 'Apple'. Additionally, each fine-grained class has an associated iconic image and a product description of the item.
Source: https://github.com/marcusklasson/GroceryStoreDataset
Image Source: https://github.com/marcusklasson/GroceryStoreDataset"	https://paperswithcode.com/dataset/grocery-store							
1929	Groningen Meaning Bank	"Groningen Meaning Bank is a semantic resource that anyone can edit and that integrates various semantic phenomena, including predicate-argument structure, scope, tense, thematic roles, animacy, pronouns, and rhetorical relations.
Source: The Groningen Meaning Bank"	https://paperswithcode.com/dataset/groningen-meaning-bank							
1930	GTA-IM Dataset	"The GTA Indoor Motion dataset (GTA-IM) that emphasizes human-scene interactions in the indoor environments. It consists of HD RGB-D image sequences of 3D human motion from a realistic game engine. The dataset has clean 3D human pose and camera pose annotations, and large diversity in human appearances, indoor environments, camera views, and human activities.
Source: https://github.com/ZheC/GTA-IM-Dataset
Image Source: https://github.com/ZheC/GTA-IM-Dataset"	https://paperswithcode.com/dataset/gta-im-dataset		GTA Indoor Motion					
1931	Gumar Corpus	"A large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels.
Source: A Large Scale Corpus of Gulf Arabic"	https://paperswithcode.com/dataset/gumar-corpus							
1932	Gutenberg Dialog Dataset	"This is a high-quality dataset consisting of 14.8M utterances in English, extracted from processed dialogues from publicly available online books.
Source: https://github.com/ricsinaruto/gutenberg-dialog"	https://paperswithcode.com/dataset/gutenberg-dialog-dataset							
1933	Gutenberg Time Dataset	"A data set of hourly time phrases from 52,183 fictional books. 
Source: What time is it? Temporal Analysis of Novels"	https://paperswithcode.com/dataset/gutenberg-time-dataset							
1934	H3D	"The H3D is a large scale full-surround 3D multi-object detection and tracking dataset. It is gathered from HDD dataset, a large scale naturalistic driving dataset collected in San Francisco Bay Area. H3D consists of following features:

Full 360 degree LiDAR dataset (dense pointcloud from Velodyne-64)
160 crowded and highly interactive traffic scenes
1,071,302 3D bounding box labels
8 common classes of traffic participants (Manually annotated every 2Hz and linearly propagated for 10 Hz data)
Benchmarked on state-of-the art algorithms for 3D only detection and tracking algorithms."	https://paperswithcode.com/dataset/h3d		Honda Research Institute 3D					
1935	HAA500	"HAA500 is a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, e.g., ""Throw"", HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, e.g., ""Baseball Pitching"" vs ""Free Throw in Basketball"", to minimize ambiguities in action classification. HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. 
Source: HAA500: Human-Centric Atomic Action Dataset with Curated Videos"	https://paperswithcode.com/dataset/haa500		Human-Centric Atomic Action Dataset					
1936	Habitat Platform	"A platform for research in embodied artificial intelligence (AI).
Source: Habitat: A Platform for Embodied AI Research"	https://paperswithcode.com/dataset/habitat-platform							
1937	HAM	"HAM is a dataset for molecular graph partitioning. This dataset contains coarse-grained (CG) mappings of 1206 organic molecules with less than 25 heavy atoms. Each molecule was downloaded from the PubChem database as SMILES. One molecule was assigned to two annotators to compare the human agreement between CG mappings. Downloaded SMILES were hand-mapped. The completed annotations were reviewed by a third person, to identify and remove unreasonable mappings (eg: one bead mappings) which did not agree with the given guidelines. Hence, there are 1.68 annotations per molecule in the current database (16% removed).
Source: https://github.com/rochesterxugroup/HAM_dataset"	https://paperswithcode.com/dataset/ham		Human-annotated Mappings					
1938	HANS	"The HANS (Heuristic Analysis for NLI Systems) dataset which contains many examples where the heuristics fail. 
Source: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"	https://paperswithcode.com/dataset/hans		Heuristic Analysis for NLI Systems					
1939	Hanabi Learning Environment	"A new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information.
Source: The Hanabi Challenge: A New Frontier for AI Research"	https://paperswithcode.com/dataset/hanabi-learning-environment							
1940	HandNet	"The HandNet dataset contains depth images of 10 participants' hands non-rigidly deforming in front of a RealSense RGB-D camera. The annotations are generated by a magnetic annotation technique. 6D pose is available for the center of the hand as well as the five fingertips (i.e. position and orientation of each).
Source: Rule Of Thumb: Deep derotation for improved fingertip detection"	https://paperswithcode.com/dataset/handnet							
1941	HappyDB	"HyppyDB is a corpus of 100,000 crowdsourced happy moments.
Source: HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments"	https://paperswithcode.com/dataset/happydb							
1942	HarperValleyBank	"The data simulate simple consumer banking interactions, containing about 23 hours of audio from 1,446 human-human conversations between 59 unique speakers.
Source: HarperValleyBank: A Domain-Specific Spoken Dialog Corpus"	https://paperswithcode.com/dataset/harpervalleybank							
1943	HASY	"HASY is a dataset of single symbols similar to MNIST. It contains 168,233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge.
Source: The HASYv2 dataset"	https://paperswithcode.com/dataset/hasy							
1944	Hateful Memes Challenge	"A new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes.
Source: The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"	https://paperswithcode.com/dataset/hateful-memes-challenge							
1945	Hate Speech	"Dataset of hate speech annotated on Internet forum posts in English at sentence-level. The source forum in Stormfront, a large online community of white nacionalists. A total of 10,568 sentence have been been extracted from Stormfront and classified as conveying hate speech or not.
Source: https://arxiv.org/pdf/1809.04444.pdf"	https://paperswithcode.com/dataset/hate-speech							
1946	Hate Speech and Offensive Language	"HSOL is a dataset for hate speech detection. The authors begun with a hate speech lexicon containing words and
phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API they searched
for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. They extracted
the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus they took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech.
Source: Automated Hate Speech Detection and the Problem of Offensive Language"	https://paperswithcode.com/dataset/hate-speech-and-offensive-language							
1947	HCU400	"The dataset consists of the features associated with 402 5-second sound samples.
The 402 sounds range from easily identifiable everyday sounds to intentionally obscured artificial ones. The dataset aims to lower the barrier for the study of aural phenomenology as the largest available audio dataset to include an analysis of causal attribution. Each sample has been annotated with crowd-sourced descriptions, as well as familiarity, imageability, arousal, and valence ratings.
Source: https://github.com/mitmedialab/HCU400"	https://paperswithcode.com/dataset/hcu400							
1948	HDD	"Honda Research Institute Driving Dataset (HDD) is a dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors.
Source: Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning
Image Source: https://usa.honda-ri.com/hdd"	https://paperswithcode.com/dataset/hdd		Honda Research Institute Driving Dataset					
1949	HDR+ Burst Photography Dataset	"The dataset consists of 3640 bursts (made up of 28461 images in total), organized into subfolders, plus the results of an image processing pipeline. Each burst consists of the raw burst input (in DNG format) and certain metadata not present in the images, as sidecar files.
Source: HDR+ Burst Photography Dataset"	https://paperswithcode.com/dataset/hdr-burst-photography-dataset							
1950	Headlines dataset	"The Headlines dataset for sarcasm detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events. The dataset includes all the headlines from News in Brief and News in Photos categories (which are sarcastic) and real (and non-sarcastic) news headlines from HuffPost.
This dataset has following advantages over the existing Twitter datasets:

Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.
Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, the dataset has high-quality labels with much less noise as compared to Twitter datasets.
Unlike tweets which are replies to other tweets, the obtained news headlines are self-contained.

Source: https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection
Image Source: https://github.com/rishabhmisra/Headlines-Dataset-For-Sarcasm-Detection"	https://paperswithcode.com/dataset/headlines-dataset							
1951	HeadQA	"HeadQA is a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. 
Source: HEAD-QA: A Healthcare Dataset for Complex Reasoning
Image Source: https://arxiv.org/pdf/1906.04701v1.pdf"	https://paperswithcode.com/dataset/headqa							
1952	HELP	"The HELP dataset is an automatically created natural language inference (NLI) dataset that embodies the combination of lexical and logical inferences focusing on monotonicity (i.e., phrase replacement-based reasoning). The HELP (Ver.1.0) has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction.
Source: HELP"	https://paperswithcode.com/dataset/help							
1953	HHOI	"A new RGB-D video dataset, i.e., UCLA Human-Human-Object Interaction (HHOI) dataset, which includes 3 types of human-human interactions, i.e., shake hands, high-five, pull up, and 2 types of human-object-human interactions, i.e., throw and catch, and hand over a cup. On average, there are 23.6 instances per interaction performed by totally 8 actors recorded from various views. Each interaction lasts 2-7 seconds presented at 10-15 fps.
Source: Learning Social Affordance for Human-Robot Interaction"	https://paperswithcode.com/dataset/hhoi							
1954	HIDE	"Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. 
Source: Human-Aware Motion Deblurring"	https://paperswithcode.com/dataset/hide							
1955	HiEve	"A new large-scale dataset for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches.
Source: Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events"	https://paperswithcode.com/dataset/hieve		Human-in-Events					
1956	HIGGS Data Set	"The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.
Source: HIGGS Data Set"	https://paperswithcode.com/dataset/higgs-data-set							
1957	HindEnCorp	"A parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences.
Source: HindEnCorp - Hindi-English and Hindi-only Corpus for Machine Translation"	https://paperswithcode.com/dataset/hindencorp							
1958	Hindi Visual Genome	"Hindi Visual Genome is a multimodal dataset consisting of text and images suitable for English-Hindi multimodal machine translation task and multimodal research.
Source: Hindi Visual Genome: A Dataset for Multimodal English-to-Hindi Machine Translation
Image Source: https://ufal.mff.cuni.cz/hindi-visual-genome"	https://paperswithcode.com/dataset/hindi-visual-genome							
1959	HINT3	"HINT3 is a dataset for intent detection. It consists of 3 different datasets each containing a diverse set of intents in a single domain - mattress products retail, fitness supplements retail and online gaming named SOFMattress, Curekart and Powerplay11.
Source: https://github.com/hellohaptik/HINT3"	https://paperswithcode.com/dataset/hint3							
1960	Bulgarian Reading Comprehension Dataset	"A dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. 
Source: Beyond English-Only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian"	https://paperswithcode.com/dataset/bulgarian-reading-comprehension-dataset							
1961	HJDataset	"HJDataset is a large dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. 
Source: A Large Dataset of Historical Japanese Documents with Complex Layouts
Image Source: https://dell-research-harvard.github.io/HJDataset/"	https://paperswithcode.com/dataset/hjdataset							
1962	Hong Kong Cantonese corpus	"The Hong Kong Cantonese Corpus was collected from transcribed conversations that were recorded between March 1997 and August 1998. About 230,000 Chinese words were collected in the annotated corpus. It contains recordings of spontaneous speech (51 texts) and radio programmes (42 texts), which involve 2 to 4 speakers, with 1 text of monologue. The text were word-segmented, annotated with part-of-speech tagging and Cantonese pronunciation using the romanisation scheme of Linguistic Society of Hong Kong (LSHK).
Source: Hong Kong Cantonese corpus"	https://paperswithcode.com/dataset/hong-kong-cantonese-corpus							
1963	HLA-Chat	"Models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs.
Source: ALOHA: Artificial Learning of Human Attributes for Dialogue Agents"	https://paperswithcode.com/dataset/hla-chat							
1964	Hollywood 3D dataset	"A dataset for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use of the 3D data, and five new interest point detection strategies are also proposed, that extend to the 3D data. 
Source: Hollywood 3D: Recognizing Actions in 3D Natural Scenes"	https://paperswithcode.com/dataset/hollywood-3d-dataset							
1965	Holopix50k	"An in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform.
Source: Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset"	https://paperswithcode.com/dataset/holopix50k							
1966	HolStep	"HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. 
Source: HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving"	https://paperswithcode.com/dataset/holstep							
1967	HoME	HoME (Household Multimodal Environment) is a multimodal environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more.	https://paperswithcode.com/dataset/home		Household Multimodal Environment					
1968	HORAE	"A new dataset of annotated pages from books of hours, a type of handwritten prayer books owned and used by rich lay people in the late middle ages. The dataset was created for conducting historical research on the evolution of the religious mindset in Europe at this period since the book of hours represent one of the major sources of information thanks both to their rich illustrations and the different types of religious sources they contain. 
Source: HORAE: an annotated dataset of books of hours"	https://paperswithcode.com/dataset/horae							
1969	Horne 2017 Fake News Data	"The Horne 2017 Fake News Data contains two independed news datasets:


Buzzfeed Political News Data:

News originally analyzed by Craig Silverman of Buzzfeed News in article entitled "" This Analysis Shows How Viral Fake Election News Stories Outperformed Real News On Facebook.""
BuzzFeed News used keyword search on the content analysis tool BuzzSumo to find news stories
Post the analysis of Buzzfeed News, the authors collect the body text and body title of all articles and use the ground truth as set by Buzzfeed as actual ground truth.
This data set has fewer clear restrictions on the ground truth, including opinion-based real stories and satire-based fake stories. In our study, the authors manually filter this data set down to contain only ""hard"" news stories and malicious fake news stories. This repository contains the whole dataset with no filtering.



Random Political News Data:

Randomly collected from three types of sources during 2016.
Sources ground truth determined through: Business Insider’s “Most Trusted” list and Zimdars 2016 Fake news list
Sources:
Real: Wall Street Journal, The Economist, BBC, NPR, ABC, CBS, USA Today, The Guardian, NBC, The Washington Post
Satire: The Onion, Huffington Post Satire, Borowitz Report, The Beaverton, Satire Wire, and Faking News
Fake: Ending The Fed, True Pundit, abcnews.com.co, DC Gazette, Liberty Writers News, Before its News, InfoWars, Real News Right Now





Source: https://github.com/BenjaminDHorne/fakenewsdata1"	https://paperswithcode.com/dataset/horne-2017-fake-news-data							
1970	HotelRec	"Publicly available dataset in the hotel domain (50M versus 0.9M) and additionally, the largest recommendation dataset in a single domain and with textual reviews (50M versus 22M).
Source: HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset"	https://paperswithcode.com/dataset/hotelrec							
1971	Hotels-50K	"The Hotels-50K dataset consists of over 1 million images from 50,000 different hotels around the world. These images come from both travel websites, as well as the TraffickCam mobile application, which allows every day travelers to submit images of their hotel room in order to help combat trafficking. The TraffickCam images are more visually similar to images from trafficking investigations than the images from travel websites.
The training dataset includes 1,027,871 images from 50,000 hotels, and 92 major hotel chains. Of the 50,000 hotels, 13,900 include user contributed images from the TraffickCam application (a total of 55,061 TraffickCam images are included in the training set).
The test dataset includes 17,954 TraffickCam images from 5,000 different hotels (as well as versions of the test images that have medium and large occlusions to replicate the occlusions seen in real world trafficking victim photographs).
Source: Hotels-50K: A Global Hotel Recognition Dataset"	https://paperswithcode.com/dataset/hotels-50k							
1972	Houses Dataset	"This dataset is used for predicting house prices from both images and textual information. It is composed of 535 sample houses from California, USA.
Source: https://github.com/emanhamed/Houses-dataset"	https://paperswithcode.com/dataset/houses-dataset							
1973	House3D Environment	"A rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.)
Source: Building Generalizable Agents with a Realistic and Rich 3D Environment"	https://paperswithcode.com/dataset/house3d-environment							
1974	HouseExpo	"A large-scale indoor layout dataset containing 35,357 2D floor plans including 252,550 rooms in total.
Source: HouseExpo: A Large-scale 2D Indoor Layout Dataset for Learning-based Algorithms on Mobile Robots"	https://paperswithcode.com/dataset/houseexpo							
1975	Houses3K	"Houses3K is a dataset of 3000 textured 3D house models. Houses3K is divided into twelve batches, each containing 50 unique house geometries. For each batch, five different textures were applied forming the sets (A, B, C, D, E).
Source: https://github.com/darylperalta/Houses3K
Image Source: https://github.com/darylperalta/Houses3K"	https://paperswithcode.com/dataset/houses3k							
1976	HoVer	"Is a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes.
Source: HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification"	https://paperswithcode.com/dataset/hover							
1977	HRA	"A verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. 
Source: Exploring object-centric and scene-centric CNN features and their complementarity for human rights violations recognition in images"	https://paperswithcode.com/dataset/hra		Human Rights Archive Database					
1978	HSD	"An annotated dataset is released to enable dynamic scene classification that includes 80 hours of diverse high quality driving video data clips collected in the San Francisco Bay area. The dataset includes temporal annotations for road places, road types, weather, and road surface conditions. 
Source: Dynamic Traffic Scene Classification with Space-Time Coherence"	https://paperswithcode.com/dataset/hsd		Honda Scenes Dataset					
1979	HS-SOD	"HS-SOD is a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB).
Source: Hyperspectral Image Dataset for Benchmarking on Salient Object Detection"	https://paperswithcode.com/dataset/hs-sod		HyperSpectral Salient Object Detection Dataset					
1980	Human-Parts	"The Human-Parts dataset is a dataset for human body, face and hand detection with ~15k images. It contains ~106k different annotations, with multiple annotations per image.
Source: https://github.com/xiaojie1017/Human-Parts
Image Source: https://github.com/xiaojie1017/Human-Parts"	https://paperswithcode.com/dataset/human-parts							
1981	HUMBI	"A new large multiview dataset for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and physical condition. 
Source: HUMBI: A Large Multiview Dataset of Human Body Expressions"	https://paperswithcode.com/dataset/humbi							
1982	Humicroedit	"Humicroedit is a humorous headline dataset. The data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. The authors carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline.
Source: ""President Vows to Cut <Taxes> Hair"": Dataset and Analysis of Creative Text Editing for Humorous Headlines
Image Source: https://arxiv.org/pdf/1906.00274v1.pdf"	https://paperswithcode.com/dataset/humicroedit							
1983	HurricaneEmo	"HurricaneEmo is an emotion dataset that contains 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria.
Source: https://github.com/shreydesai/hurricane"	https://paperswithcode.com/dataset/hurricaneemo							
1984	HybridQA	"A new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. 
Source: HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data"	https://paperswithcode.com/dataset/hybridqa							
1985	Hypersim	"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. Hypersim is a photorealistic synthetic dataset for holistic indoor scene understanding. It contains 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry.
Source: https://github.com/apple/ml-hypersim
Image Source: https://github.com/apple/ml-hypersim"	https://paperswithcode.com/dataset/hypersim							
1986	Hyperspectral City	"Propose a dataset which adopts multi-channel visual input.
Source: Hyperspectral City"	https://paperswithcode.com/dataset/hyperspectral-city							
1987	iCartoonFace	"The iCartoonFace dataset is a large-scale dataset that can be used for two different tasks: cartoon face detection and cartoon face recognition.
Source: https://github.com/luxiangju-PersonAI/iCartoonFace
Image Source: https://github.com/luxiangju-PersonAI/iCartoonFace"	https://paperswithcode.com/dataset/icartoonface							
1988	Ice Hockey News Dataset	"Ice Hockey News Dataset is a corpus of Finnish ice hockey news, edited to be suitable for training of end-to-end news generation methods, as well as demonstrate generation of text, which was judged by journalists to be relatively close to a viable product.
Source: Template-free Data-to-Text Generation of Finnish Sports News"	https://paperswithcode.com/dataset/ice-hockey-news-dataset							
1989	Icentia11K	"Public ECG dataset of continuous raw signals for representation learning containing 11 thousand patients and 2 billion labelled beats.
Source: Icentia11K: An Unsupervised Representation Learning Dataset for Arrhythmia Subtype Discovery"	https://paperswithcode.com/dataset/icentia11k							
1990	ICLabel	"An Independent components (IC) dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings.
Source: ICLabel: An automated electroencephalographic independent component classifier, dataset, and website"	https://paperswithcode.com/dataset/iclabel							
1991	Icons-50	"Icons-50 is a dataset for studying surface variation robustness.
Source: Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations"	https://paperswithcode.com/dataset/icons-50							
1992	ICubWorld	iCubWorld datasets are collections of images recording the visual experience of iCub while observing objects in its typical environment, a laboratory or an office. The acquisition setting is devised to allow a natural human-robot interaction, where a teacher verbally provides the label of the object of interest and shows it to the robot, by holding it in the hand; the iCub can either track the object while the teacher moves it, or take it in its hand.	https://paperswithcode.com/dataset/icubworld							
1993	IDD	"IDD is a dataset for road scene understanding in unstructured environments used for semantic segmentation and object detection for autonomous driving. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. 
Source: IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments
Image Source: Varma et al"	https://paperswithcode.com/dataset/idd		Indian Driving Dataset					
1994	iFakeFaceDB	"iFakeFaceDB is a face image dataset for the study of synthetic face manipulation detection, comprising about 87,000 synthetic face images generated by the Style-GAN model and transformed with the GANprintR approach. All images were aligned and resized to the size of 224 x 224.
Source: https://github.com/socialabubi/iFakeFaceDB
Image Source: https://github.com/socialabubi/iFakeFaceDB"	https://paperswithcode.com/dataset/ifakefacedb							
1995	IgboNLP Datasets	"IgboNLP is a standard machine translation benchmark dataset for Igbo. It consists of 10,000 English-Igbo human-level quality sentence pairs mostly from the news domain.
Source: Igbo-English Machine Translation: An Evaluation Benchmark"	https://paperswithcode.com/dataset/igbonlp-datasets							
1996	iHarmony4	"iHarmony4 is a synthesized dataset for Image Harmonization. It contains 4 sub-datasets: HCOCO, HAdobe5k, HFlickr, and Hday2night (based on COCO, Adobe5k, Flickr, day2night datasets respectively), each of which contains synthesized composite images, foreground masks of composite images and corresponding real images. 
Source: Image_Harmonization_Datasets"	https://paperswithcode.com/dataset/iharmony4							
1997	IIIT-AR-13K	"IIIT-AR-13K is created by manually annotating the bounding boxes of graphical or page objects in publicly available annual reports. This dataset contains a total of 13k annotated page images with objects in five different popular categories - table, figure, natural image, logo, and signature. It is the largest manually annotated dataset for graphical object detection.
Source: IIIT-AR-13K: A New Dataset for Graphical Object Detection in Documents
Image Source: http://cvit.iiit.ac.in/usodi/iiitar13k.php"	https://paperswithcode.com/dataset/iiit-ar-13k							
1998	IIRC	"Contains more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. 
Source: IIRC: A Dataset of Incomplete Information Reading Comprehension Questions"	https://paperswithcode.com/dataset/iirc		Incomplete Information Reading Comprehension					
1999	IITB Corridor	"An abnormal activity data-set for research use that contains 4,83,566 annotated frames.
Source: Multi-timescale Trajectory Prediction for Abnormal Human Activity Detection"	https://paperswithcode.com/dataset/iitb-corridor							
2000	IIW	"Intrinsic Images in the Wild is a large scale, public dataset for intrinsic image decompositions of real-world scenes selected from the OpenSurfaces dataset. Each image is annotated with crowdsourced pairwise comparisons of material properties. 
Source: IIW"	https://paperswithcode.com/dataset/iiw		Intrinsic Images in the Wild					
2001	IKEA ASM	"A three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. 
Source: The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose"	https://paperswithcode.com/dataset/ikea-asm							
2002	iLur News Texts	"iLur News Texts is a dataset of over 12000 news articles from iLur.am, categorized into 7 classes: sport, politics, weather, economy, accidents, art, society. The articles are split into train (2242k tokens) and test sets (425k tokens).
Source: iLur News Texts"	https://paperswithcode.com/dataset/ilur-news-texts							
2003	Image and Video Advertisements	"The Image and Video Advertisements collection consists of an image dataset of 64,832 image ads, and a video dataset of 3,477 ads. The data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer (""What should I do according to this ad, and why should I do it? ""), and symbolic references ads make (e.g. a dove symbolizes peace). 
Source: Automatic Understanding of Image and Video Advertisements"	https://paperswithcode.com/dataset/image-and-video-advertisements							
2004	Image Caption Quality Dataset	"Image Caption Quality Dataset is a dataset of crowdsourced ratings for machine-generated image captions. It contains more than 600k ratings of image-caption pairs.
Source: https://arxiv.org/abs/1909.03396"	https://paperswithcode.com/dataset/image-caption-quality-dataset							
2005	Image Editing Request Dataset	"A new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. 
Source: Expressing Visual Relationships via Language"	https://paperswithcode.com/dataset/image-editing-request-dataset							
2006	Image Memorability	"A database of images with measured probabilities that each picture will be remembered after a single view. 
Source: Image Memorability"	https://paperswithcode.com/dataset/image-memorability							
2007	IMDb-Face	"IMDb-Face is  large-scale noise-controlled dataset for face recognition research. The dataset contains about 1.7 million faces, 59k identities, which is manually cleaned from 2.0 million raw images. All images are obtained from the IMDb website. 
Source: The Devil of Face Recognition is in the Noise"	https://paperswithcode.com/dataset/imdb-face							
2008	IMEMNET	"The Image-MusicEmotion-Matching-Net (IMEMNet) dataset is a dataset for continuous emotion-based image and music matching. It has over 140K image-music pairs.
Source: https://github.com/linkAmy/IMEMNet"	https://paperswithcode.com/dataset/imemnet		Image-MusicEmotion-Matching-Net					
2009	Immediacy Dataset	"Consists of 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated.
Source: Multi-Task Recurrent Neural Network for Immediacy Prediction"	https://paperswithcode.com/dataset/immediacy-dataset							
2010	iMoCap	"A dataset that consists of 20 actions of various actors, such as tennis serves, yoga and Tai Chi. Take tennis serves as an example. The publicly available videos of some tennis players from YouTube are downloaded, and manually crop the videos roughly to obtain a set of video clips of serves for each player.
Source: Motion Capture from Internet Videos"	https://paperswithcode.com/dataset/imocap							
2011	IMPPRES	"An IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types.
Source: Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition"	https://paperswithcode.com/dataset/imppres							
2012	iNaturalist Fine-Grained Geolocation	"The iNaturalist Fine-Grained Geolocation dataset is an extension of the iNaturalist dataset with complementary geolocation information.
Source: https://github.com/visipedia/fg_geo
Image Source: https://github.com/visipedia/fg_geo"	https://paperswithcode.com/dataset/inaturalist-fine-grained-geolocation							
2013	Incidents	"Contains 446,684 images annotated by humans that cover 43 incidents across a variety of scenes. 
Source: Detecting natural disasters, damage, and incidents in the wild"	https://paperswithcode.com/dataset/incidents							
2014	Incremental Dialog Dataset	"Simulates unanticipated user needs in the deployment stage.
Source: Incremental Learning from Scratch for Task-Oriented Dialogue Systems"	https://paperswithcode.com/dataset/incremental-dialog-dataset							
2015	IndicNLP Corpus	"The IndicNLP corpus is a large-scale, general-domain corpus containing 2.7 billion words for 10 Indian languages from two language families.
Source: https://arxiv.org/abs/2005.00085"	https://paperswithcode.com/dataset/indicnlp-corpus							
2016	IndoNLU Benchmark	"The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It is a joint venture from many Indonesia NLP enthusiasts from different institutions such as Gojek, Institut Teknologi Bandung, HKUST, Universitas Multimedia Nusantara, Prosa.ai, and Universitas Indonesia.
Source: IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"	https://paperswithcode.com/dataset/indonlu-benchmark							
2017	IndoSum	"The IndoSum dataset is a benchmark dataset for Indonesian text summarization. The dataset consists of news articles and manually constructed summaries.
Source: https://github.com/kata-ai/indosum"	https://paperswithcode.com/dataset/indosum							
2018	Industrial Benchmark	"A benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. 
Source: A Benchmark Environment Motivated by Industrial Control Problems"	https://paperswithcode.com/dataset/industrial-benchmark							
2019	InfoTabS	"InfoTabS comprises of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.
Source: INFOTABS: Inference on Tables as Semi-structured Data"	https://paperswithcode.com/dataset/infotabs							
2020	InLoc	"InLoc is a dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario.
Source: InLoc: Indoor Visual Localization with Dense Matching and View Synthesis"	https://paperswithcode.com/dataset/inloc							
2021	INQUISITIVE	"A dataset of ~19K questions that are elicited while a person is reading through a document.
Source: Inquisitive Question Generation for High Level Text Comprehension"	https://paperswithcode.com/dataset/inquisitive							
2022	INRIA Holidays Dataset	"The Holidays dataset is a set of images which mainly contains some of the authors' personal holidays photos. The remaining ones were taken on purpose to test the robustness to various attacks: rotations, viewpoint and illumination changes, blurring, etc. The dataset includes a very large variety of scene types (natural, man-made, water and fire effects, etc) and images are in high resolution. The dataset contains 500 image groups, each of which represents a distinct scene or object. The first image of each group is the query image and the correct retrieval results are the other images of the group.
Source: INRIA Holidays Dataset"	https://paperswithcode.com/dataset/inria-holidays-dataset							
2023	Inspired	"A new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations.
Source: INSPIRED: Toward Sociable Recommendation Dialog Systems"	https://paperswithcode.com/dataset/inspired							
2024	InstaFake	"Includes two datasets published for the detection of fake and automated accounts. 
Source: Instagram Fake and Automated Account Detection"	https://paperswithcode.com/dataset/instafake							
2025	InsuranceQA	"InsuranceQA is a question answering dataset for the insurance domain, the data stemming from the website Insurance Library. There are 12,889 questions and 21,325 answers in the training set. There are 2,000 questions and 3,354 answers in the validation set. There are 2,000 questions and 3,308 answers in the test set.
Source: APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK"	https://paperswithcode.com/dataset/insuranceqa							
2026	INTEL-TAU	"A new large dataset for illumination estimation. This dataset, called INTEL-TAU, contains 7022 images in total, which makes it the largest available high-resolution dataset for illumination estimation research. 
Source: INTEL-TAU: A Color Constancy Dataset"	https://paperswithcode.com/dataset/intel-tau							
2027	INTERACTION Dataset	"The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. The dataset can serve for many behavior-related research areas, such as 

1) intention/behavior/motion prediction, 
2) behavior cloning and imitation learning,
3) behavior analysis and modeling,
4) motion pattern and representation learning,
5) interactive behavior extraction and categorization,
6) social and human-like behavior generation,
7) decision-making and planning algorithm development and verification,
8) driving scenario/case generation, etc. 

Source: https://interaction-dataset.com/
Image Source: https://interaction-dataset.com/"	https://paperswithcode.com/dataset/interaction-dataset							
2028	InteriorNet	"InteriorNet is a RGB-D for large scale interior scene understanding and mapping. The dataset contains 20M images created by pipeline:

(A) the authors collected around 1 million CAD models provided by world-leading furniture manufacturers.
(B) based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations.
(C) For each layout, authors generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life.
(D) Authors provide an interactive simulator (ViSim) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory.
(E) All supported image sequences and ground truth.

Source: InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset
Image Source: InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset"	https://paperswithcode.com/dataset/interiornet							
2029	Interpretable STS	"A dataset of sentence pairs annotated following the formalization.
Source: Interpretable Semantic Textual Similarity: Finding and explaining differences between sentences"	https://paperswithcode.com/dataset/interpretable-sts							
2030	Interview	"A large-scale (105K conversations) media dialog dataset collected from news interview transcripts.
Source: Interview: A Large-Scale Open-Source Corpus of Media Dialog"	https://paperswithcode.com/dataset/interview							
2031	IntPhys 2019	"A benchmark for visual intuitive physics reasoning.
Source: IntPhys 2019"	https://paperswithcode.com/dataset/intphys-2019							
2032	IntrA	"IntrA is an open-access 3D intracranial aneurysm dataset that makes the application of points-based and mesh-based classification and segmentation models available. This dataset can be used to diagnose intracranial aneurysms and to extract the neck for a clipping operation in medicine and other areas of deep learning, such as normal estimation and surface reconstruction.
103 3D models of entire brain vessels are collected by reconstructing scanned 2D MRA images of patients (the raw 2D MRA images are not published due to medical ethics).
1909 blood vessel segments are generated automatically from the complete models, including 1694 healthy vessel segments and 215 aneurysm segments for diagnosis.
116 aneurysm segments are divided and annotated manually by medical experts; the scale of each aneurysm segment is based on the need for a preoperative examination.
Geodesic distance matrices are computed and included for each annotated 3D segment, because the expression of the geodesic distance is more accurate than Euclidean distance according to the shape of vessels.
Source: https://github.com/intra3d2019/IntrA
Image Source: https://github.com/intra3d2019/IntrA"	https://paperswithcode.com/dataset/intra							
2033	IP102	"IP102 contains more than 75,000 images belonging to 102 categories, which exhibit a natural long-tailed distribution.
Source: IP102: A Large-Scale Benchmark Dataset for Insect Pest Recognition"	https://paperswithcode.com/dataset/ip102							
2034	IPN Hand	"The IPN Hand dataset is a benchmark video dataset with sufficient size, variation, and real-world elements able to train and evaluate deep neural networks for continuous Hand Gesture Recognition (HGR).
Source: https://github.com/GibranBenitez/IPN-hand"	https://paperswithcode.com/dataset/ipn-hand							
2035	IPOD	"Comprises 192k job titles belonging to 56k LinkedIn users. 
Source: A Large-scale Industrial and Professional Occupation Dataset"	https://paperswithcode.com/dataset/ipod		Industrial and Professional Occupation Dataset					
2036	IPRE	"A dataset for inter-personal relationship extraction which aims to facilitate information extraction and knowledge graph construction research. In total, IPRE has over 41,000 labeled sentences for 34 types of relations, including about 9,000 sentences annotated by workers.
Source: IPRE: a Dataset for Inter-Personal Relationship Extraction"	https://paperswithcode.com/dataset/ipre							
2037	irc-disentanglement	"This is a dataset for disentangling conversations on IRC, which is the task of identifying separate conversations in a single stream of messages. It contains disentanglement information for 77,563 messages or IRC.
Source: https://github.com/jkkummerfeld/irc-disentanglement
Image Source: https://github.com/jkkummerfeld/irc-disentanglement"	https://paperswithcode.com/dataset/irc-disentanglement							
2038	IRS	"IRS is an open dataset for indoor robotics vision tasks, especially disparity and surface normal estimation. It contains totally 103,316 samples covering a wide range of indoor scenes, such as home, office, store and restaurant.
Source: https://github.com/HKBU-HPML/IRS
Image Source: https://github.com/HKBU-HPML/IRS"	https://paperswithcode.com/dataset/irs		Indoor Robotics Stereo					
2039	IS-A	"The IS-A dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the “is a” relation. For example, ‘acute leukemia’ is a ‘leukemia’. The dataset has 294,693 nodes with 356,541 edges between them.
Source: https://arxiv.org/pdf/1906.05939.pdf"	https://paperswithcode.com/dataset/is-a							
2040	ISBDA	"Consists of user-generated aerial videos from social media with annotations of instance-level building damage masks. This provides the first benchmark for quantitative evaluation of models to assess building damage using aerial videos.
Source: MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos"	https://paperswithcode.com/dataset/isbda							
2041	ISIA Food-500	"Includes 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume.
Source: ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network"	https://paperswithcode.com/dataset/isia-food-500							
2042	ISR-UoL 3D Social Activity Dataset	"This is a social interaction dataset between two subjects. This dataset consists of RGB and depth images, and tracked skeleton data (i.e. joints 3D coordinates and rotations) acquired by an RGB-D sensor. It includes 8 social activities: {handshake, greeting hug, help walk, help stand-up, fight, push, conversation, call attention}. Each activity was recorded in a period around 40 to 60 seconds of repetitions within the same session at a frame rate of 30 frames per second. The only exceptions are help walking (at a short distance) and help stand-up, which were recorded 4 times to the same session, regardless of the time spent on it.
Source: ISR-UoL 3D Social Activity Dataset"	https://paperswithcode.com/dataset/isr-uol-3d-social-activity-dataset							
2043	IStego100K	"Contains 208,104 images with the same size of 1024*1024. Among them, 200,000 images (100,000 cover-stego image pairs) are divided as the training set and the remaining 8,104 as testing set.
Source: IStego100K: Large-scale Image Steganalysis Dataset"	https://paperswithcode.com/dataset/istego100k							
2044	IU ShareView	"IU ShareView dataset consists of 9 sets of two 5-10 minute first-person videos. Each set contains 3-4 participants performing a variety of everyday activities (shaking hands, chatting, eating, etc.) in one of six indoor environments. 
Source: IU ShareView"	https://paperswithcode.com/dataset/iu-shareview							
2045	IWSLT 2019	"The IWSLT 2019 dataset contains source, Machine Translated, reference and Post-Edited text, which can be used to quantify and evaluate Post-editing effort after automatic MT.
Source: https://arxiv.org/abs/1910.06204"	https://paperswithcode.com/dataset/iwslt-2019							
2046	Jamendo Lyrics	"Dataset for lyrics alignment and transcription evaluation. It contains 20 music pieces under CC license from the Jamendo website along with their lyrics, with:

Manual annotations indicating the start time of each word in the audio file
Predictions of start and end times for each word from both of the models presented in the paper"	https://paperswithcode.com/dataset/jamendo-lyrics							
2047	JAMUL	"A large-scale evaluation dataset for headlines of three different lengths composed by professional editors.
Source: A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation"	https://paperswithcode.com/dataset/jamul		JApanese MUlti-Length Headline Corpus					
2048	Japanese Word Similarity	"This dataset contains information about Japanese word similarity including rare words. The dataset is constructed following the Stanford Rare Word Similarity Dataset. 10 annotators annotated word pairs with 11 levels of similarity.
Source: https://github.com/tmu-nlp/JapaneseWordSimilarityDataset"	https://paperswithcode.com/dataset/japanese-word-similarity							
2049	JESC	"Japanese-English Subtitle Corpus is a large Japanese-English parallel corpus covering the underrepresented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. 
Source: JESC: Japanese-English Subtitle Corpus"	https://paperswithcode.com/dataset/jesc		Japanese-English Subtitle Corpus					
2050	JHU CoSTAR Block Stacking Dataset	"Involves data where a robot interacts with 5.1 cm colored blocks to complete an order-fulfillment style block stacking task. It contains dynamic scenes and real time-series data in a less constrained environment than comparable datasets. There are nearly 12,000 stacking attempts and over 2 million frames of real data. 
Source: The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints"	https://paperswithcode.com/dataset/jhu-costar-block-stacking-dataset							
2051	JHU-CROWD	(JHU-CROWD) a crowd counting dataset that contains 4,250 images with 1.11 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level.	https://paperswithcode.com/dataset/jhu-crowd							
2052	JIT Dataset	"The Jejueo Interview Transcripts (JIT) dataset is a parallel corpus containing 170k+ Jejueo-Korean sentences.
Source: https://arxiv.org/abs/1911.12071"	https://paperswithcode.com/dataset/jit-dataset		Jejueo Interview Transcripts					
2053	JRDB	"A novel egocentric dataset collected from social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 degrees RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 degrees spherical image from a fisheye camera and encoder values from the robot's wheels.
Source: JRDB: A Dataset and Benchmark of Egocentric Visual Perception for Navigation in Human Environments"	https://paperswithcode.com/dataset/jrdb		JackRabbot Dataset and Benchmark					
2054	JSUT Corpus	"JSUT Corpus is a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist.
Source: JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis"	https://paperswithcode.com/dataset/jsut-corpus							
2055	JTA	"JTA is a dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. 
Source: Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World"	https://paperswithcode.com/dataset/jta		Joint Track Auto					
2056	JW300	"A parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average.
Source: JW300: A Wide-Coverage Parallel Corpus for Low-Resource Languages"	https://paperswithcode.com/dataset/jw300							
2057	KAIST Multispectral Pedestrian Detection Benchmark	"The KAIST Multispectral Pedestrian Dataset consists of 95k color-thermal pairs (640x480, 20Hz) taken from a vehicle. All the pairs are manually annotated (person, people, cyclist) for the total of 103,128 dense annotations and 1,182 unique pedestrians. The annotation includes temporal correspondence between bounding boxes like Caltech Pedestrian Dataset. 
Source: KAIST Multispectral Pedestrian Detection Benchmark"	https://paperswithcode.com/dataset/kaist-multispectral-pedestrian-detection							
2058	Kannada-MNIST	"The Kannada-MNIST dataset is a drop-in substitute for the standard MNIST dataset for the Kannada language.
Source: https://github.com/vinayprabhu/Kannada_MNIST
Image Source: https://github.com/vinayprabhu/Kannada_MNIST"	https://paperswithcode.com/dataset/kannada-mnist							
2059	KaoKore	"Consists of faces extracted from pre-modern Japanese artwork.
Source: KaoKore: A Pre-modern Japanese Art Facial Expression Dataset"	https://paperswithcode.com/dataset/kaokore							
2060	KaWAT	"A new word analogy task dataset for Indonesian.
Source: KaWAT: A Word Analogy Task Dataset for Indonesian"	https://paperswithcode.com/dataset/kawat							
2061	KELM	"KELM is a large-scale synthetic corpus of Wikidata KG as natural text.
Source: KELM"	https://paperswithcode.com/dataset/kelm							
2062	K-EmoCon	"A multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. 
Source: K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations"	https://paperswithcode.com/dataset/k-emocon							
2063	KenyanFood13	"The Kenyan Food Type Dataset (KenyanFood13) is an image classification dataset for Kenyan food. The images are categorized into 13 different labels.
Source: https://github.com/monajalal/Kenyan-Food
Image Source: https://github.com/monajalal/Kenyan-Food"	https://paperswithcode.com/dataset/kenyanfood13							
2064	KeypointNet	"KeypointNet is a large-scale and diverse 3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations, based on ShapeNet models.
Source: https://github.com/qq456cvb/KeypointNet
Image Source: https://github.com/qq456cvb/KeypointNet"	https://paperswithcode.com/dataset/keypointnet							
2065	KinectFaceDB	"The Dataset consists of the multimodal facial images of 52 people (14 females, 38 males) obtained by Kinect. The data is captured in two sessions happened at different time period (about half month). In each session, the dataset provides the facial images of each person in 9 states of different facial expressions, different lighting and occlusion conditions: neutral, smile, open mouth, left profile, right profile, occlusion eyes, occlusion mouth, occlusion paper and light on [Figure 1]. All the images are provided in three sources of information: the RGB color image, the depth map (provided in two forms of the bitmap depth image and the text file containing the original depth levels sensed by Kinect) as well as 3D. In addition, the dataset comes with the manual landmarks of 6 positions in the face: left eye, right eye, the tip of nose, left side of mouth, right side of mouth and the chin [Figure 2]. Other information of the person such as gender, year of birth, glasses (this person wears the glasses or not), capture time of each session are also available.
Source: KinectFaceDB"	https://paperswithcode.com/dataset/kinectfacedb							
2066	KINNEWS and KIRNEWS	"Two news datasets (KINNEWS and KIRNEWS) for multi-class classification of news articles in Kinyarwanda and Kirundi, two low-resource African languages. The two languages are mutually intelligible.
Source: KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for Kinyarwanda and Kirundi"	https://paperswithcode.com/dataset/kinnews-and-kirnews							
2067	KINS	"Augments the KITTI with more instance pixel-level annotation for 8 categories.
Source: Amodal Instance Segmentation with KINS Dataset"	https://paperswithcode.com/dataset/kins							
2068	Kinteract	"Explicitly created for Human Computer Interaction (HCI).
Source: Fast Gesture Recognition with Multiple Stream Discrete HMMs on 3D Skeletons"	https://paperswithcode.com/dataset/kinteract							
2069	Kitchen Scenes	"Kitchen Scenes is a multi-view RGB-D dataset of nine kitchen scenes, each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset. The viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3D point cloud. 
Source: Multiview RGB-D Dataset for Object Instance Detection"	https://paperswithcode.com/dataset/kitchen-scenes							
2070	KIT Motion-Language	"The KIT Motion-Language is a dataset linking human motion and natural language.
Source: The KIT Motion-Language Dataset"	https://paperswithcode.com/dataset/kit-motion-language							
2071	KnowIT VQA	"KnowIT VQA is a video dataset with 24,282 human-generated question-answer pairs about The Big Bang Theory. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered.
Source: KnowIT VQA: Answering Knowledge-Based Questions about Videos"	https://paperswithcode.com/dataset/knowit-vqa							
2072	Korean HateSpeech Dataset	"Presents 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea.
Source: BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection"	https://paperswithcode.com/dataset/korean-hatespeech-dataset							
2073	KorNLI	"KorNLI is a Korean Natural Language Inference (NLI) dataset. The dataset is constructed by automatically translating the training sets of the SNLI, XNLI and MNLI datasets. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other’s translation afterward.
It contains 942,854 training examples translated automatically and 7,500 evaluation (development and test) examples translated manually
Source: https://github.com/kakaobrain/KorNLUDatasets"	https://paperswithcode.com/dataset/kornli							
2074	KPTimes	"KPTimes is a large-scale dataset of news texts paired with editor-curated keyphrases. 
Source: KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents"	https://paperswithcode.com/dataset/kptimes							
2075	KQA Pro	"A large-scale dataset for Complex KBQA.
Source: KQA Pro: A Large-Scale Dataset with Interpretable Programs and Accurate SPARQLs for Complex Question Answering over Knowledge Base"	https://paperswithcode.com/dataset/kqa-pro							
2076	Kuzushiji-49	"Kuzushiji-49 is an MNIST-like dataset that has 49 classes (28x28 grayscale, 270,912 images) from 48 Hiragana characters and one Hiragana iteration mark.
Source: Deep Learning for Classical Japanese Literature
Image Source: Clanuwat et al"	https://paperswithcode.com/dataset/kuzushiji-49							
2077	Kvasir-Instrument	"Consists of  annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists.
Source: Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy"	https://paperswithcode.com/dataset/kvasir-instrument							
2078	LABR	"LABR is a large sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. 
Source: LABR: A Large Scale Arabic Book Reviews Dataset"	https://paperswithcode.com/dataset/labr		Large-Scale Arabic Book Reviews					
2079	LAD	"LAD (Large-scale Attribute Dataset) has 78,017 images of 5 super-classes and 230 classes. The image number of LAD is larger than the sum of the four most popular attribute datasets (AwA, CUB, aP/aY and SUN). 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level.
Source: A Large-scale Attribute Dataset for Zero-shot Learning"	https://paperswithcode.com/dataset/lad		Large-scale Attribute Dataset					
2080	LAG	"Includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432).
Source: Attention Based Glaucoma Detection: A Large-scale Database and CNN Model"	https://paperswithcode.com/dataset/lag		Large-scale Attention based Glaucoma					
2081	LAMA	"LAnguage Model Analysis (LAMA) consists of a set of knowledge sources, each comprised of a set of facts. LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models.
Source: Language Models as Knowledge Bases?"	https://paperswithcode.com/dataset/lama		LAnguage Model Analysis					
2082	LaMem	"An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources).
Source: Understanding and Predicting Image Memorability at a Large Scale"	https://paperswithcode.com/dataset/lamem							
2083	LAOFIW Dataset	"An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe.
Source: Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings"	https://paperswithcode.com/dataset/laofiw-dataset		Labeled Ancestral Origin Faces in the Wild					
2084	LaPa	"A large-scale Landmark guided face Parsing dataset (LaPa) for face parsing. It consists of more than 22,000 facial images with abundant variations in expression, pose and occlusion, and each image of LaPa is provided with a 11-category pixel-level label map and 106-point landmarks.
Source: LaPa"	https://paperswithcode.com/dataset/lapa							
2085	LasVR	"A large-scale video database for rain removal (LasVR), which consists of 316 rain videos.
Source: Removing Rain in Videos: A Large-scale Database and A Two-stream ConvLSTM Approach"	https://paperswithcode.com/dataset/lasvr							
2086	Lazaro Corpus	"A corpus of 21,570 newspaper headlines written in European Spanish annotated with emergent anglicisms.
Source: An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines"	https://paperswithcode.com/dataset/lazaro-corpus							
2087	LC25000	"The LC25000 dataset contains 25,000 color images with 5 classes of 5,000 images each. All images are 768 x 768 pixels in size and are in jpeg file format. The 5 classes are: colon adenocarcinomas, benign colonic tissues, lung adenocarcinomas, lung squamous cell carcinomas and bening lung tissues.
Source: https://github.com/tampapath/lung_colon_image_set"	https://paperswithcode.com/dataset/lc25000		Lung And Colon Histopathological Image Dataset					
2088	LCCC	"Contains a base version (6.8million dialogues) and a large version (12.0 million dialogues). 
Source: A Large-Scale Chinese Short-Text Conversation Dataset"	https://paperswithcode.com/dataset/lccc		Large-scale Cleaned Chinese Conversation corpus					
2089	LC-QuAD 2.0	"LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version.
Source: LC-QuAD 2.0"	https://paperswithcode.com/dataset/lc-quad-2-0		Largescale Complex Question Answering Dataset					
2090	LCSTS	"LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. The authors also manually tagged the relevance of 10,666 short summaries with their corresponding short texts 10,666 short summaries with their corresponding short texts.
Source: LCSTS: A Large Scale Chinese Short Text Summarization Dataset"	https://paperswithcode.com/dataset/lcsts							
2091	LEAF Benchmark	"A suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.
Source: LEAF: A Benchmark for Federated Settings"	https://paperswithcode.com/dataset/leaf-benchmark							
2092	Leaf counting dataset	"Dataset containing  9372 RGB images of weeds with the number of leaves counted. The images are collected in fields across Denmark using Nokia and Samsung cell phone cameras; Samsung, Nikon, Canon and Sony consumer cameras; and a Point Grey industrial camera.
Source: Leaf counting dataset"	https://paperswithcode.com/dataset/leaf-counting-dataset							
2093	LectureBank	"LectureBank Dataset is a manually collected dataset of lecture slides. It contains 1,352 online lecture files from 60 courses covering 5 different domains, including Natural Language Processing (nlp), Machine Learning (ml), Artificial Intelligence (ai), Deep Learning (dl) and Information Retrieval (ir). In addition, it also contains the corresponding annotations for each slide.
Source: https://github.com/Yale-LILY/LectureBank
Image Source: https://github.com/Yale-LILY/LectureBank"	https://paperswithcode.com/dataset/lecturebank							
2094	Legal Documents Entity Recognition	"Court decisions from 2017 and 2018 were selected for the dataset, published online by the Federal Ministry of Justice and Consumer Protection. The documents originate from seven federal courts: Federal Labour Court (BAG), Federal Fiscal Court (BFH), Federal Court of Justice (BGH), Federal Patent Court (BPatG), Federal Social Court (BSG), Federal Constitutional Court (BVerfG) and Federal Administrative Court (BVerwG).
Source: Legal Documents Entity Recognition"	https://paperswithcode.com/dataset/legal-documents-entity-recognition							
2095	LEMMA	"The LEMMA dataset aims to explore the essence of complex human activities in a goal-directed, multi-agent, multi-task setting with ground-truth labels of compositional atomic-actions and their associated tasks. By quantifying the scenarios to up to two multi-step tasks with two agents, the authors strive to address human multi-task and multi-agent interactions in four scenarios: single-agent single-task (1 x 1), single-agent multi-task (1 x 2), multi-agent single-task (2 x 1), and multi-agent multi-task (2 x 2). Task instructions are only given to one agent in the 2 x 1 setting to resemble the robot-helping scenario, hoping that the learned perception models could be applied in robotic tasks (especially in HRI) in the near future.
Both the third-person views (TPVs) and the first-person views (FPVs) were recorded to account for different perspectives of the same activities. The authors densely annotate atomic-actions (in the form of compositional verb-noun pairs) and tasks of each atomic-action, as well as the spatial location of each participating agent (bounding boxes) to facilitate the learning of multi-agent multi-task task scheduling and assignment.
Source: LEMMA"	https://paperswithcode.com/dataset/lemma							
2096	Lenta Short Sentences	"The Lenta Short Sentences dataset is a text dataset for language modelling for the Russian language. It consists of 236K sentences sampled from the Lenta News dataset.
Source: https://arxiv.org/pdf/2005.02470.pdf"	https://paperswithcode.com/dataset/lenta-short-sentences							
2097	Libri-Adapt	"Libri-Adapt aims to support unsupervised domain adaptation research on speech recognition models.
Source: Libri-Adapt: A New Speech Dataset for Unsupervised Domain Adaptation"	https://paperswithcode.com/dataset/libri-adapt							
2098	LibriCSS	"Continuous speech separation (CSS) is an approach to handling overlapped speech in conversational audio signals. A real recorded dataset, called LibriCSS, is derived from LibriSpeech by concatenating the corpus utterances to simulate a conversation and capturing the audio replays with far-field microphones.
Source: https://github.com/chenzhuo1011/libri_css"	https://paperswithcode.com/dataset/libricss							
2099	Libri-Light	"Libri-Light is a collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio.
Source: Libri-Light: A Benchmark for ASR with Limited or No Supervision"	https://paperswithcode.com/dataset/libri-light							
2100	LibriMix	"LibriMix is an open-source alternative to wsj0-2mix. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. 
Source: LibriMix: An Open-Source Dataset for Generalizable Speech Separation"	https://paperswithcode.com/dataset/librimix							
2101	LibriVoxDeEn	"LibriVoxDeEn is a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audiobooks. The speech translation data consist of 110 hours of audio material aligned to over 50k parallel sentences. An even larger dataset comprising 547 hours of German speech aligned to German text is available for speech recognition. The audio data is read speech and thus low in disfluencies.
Source: LibriVoxDeEn: A Corpus for German-to-English Speech Translation and German Speech Recognition"	https://paperswithcode.com/dataset/librivoxdeen							
2102	LiMiT	"The limit dataset of ~24K sentences that describe literal motion (~14K sentences), and sentences not describing motion or other type of motion (e.g. fictive motion). Senteces were extracted from electronic books categorized as fiction or novels, and a portion from the NetActivity Captions Dataset.
Source: LiMiT"	https://paperswithcode.com/dataset/limit		Literal Motion in Text Dataset					
2103	LinCE	"A centralized benchmark for Linguistic Code-switching Evaluation (LinCE) that combines ten corpora covering four different code-switched language pairs (i.e., Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks (i.e., language identification, named entity recognition, part-of-speech tagging, and sentiment analysis). 
Source: LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation"	https://paperswithcode.com/dataset/lince		Linguistic Code-switching Evaluation Dataset					
2104	Liputan6	"A large-scale Indonesian summarization dataset consisting of harvested articles from Liputan6.com, an online news portal, resulting in 215,827 document-summary pairs.
Source: Liputan6: A Large-scale Indonesian Dataset for Text Summarization"	https://paperswithcode.com/dataset/liputan6							
2105	LISA Gaze Dataset	"LISA Gaze is a dataset for driver gaze estimation comprising of 11 long drives, driven by 10 subjects in two different cars. 
Source: Driver Gaze Zone Estimation using Convolutional Neural Networks: A General Framework and Ablative Analysis"	https://paperswithcode.com/dataset/lisa-gaze-dataset	08/02/2018						
2106	Live Comment Dataset	"The Live Comment Dataset is a large-scale dataset with 2,361 videos and 895,929 live comments that were written while the videos were streamed.
Source: LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts"	https://paperswithcode.com/dataset/live-comment-dataset							
2107	LiveQA	"A new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu (https://nba.hupu.com/games) website.
Source: LiveQA: A Question Answering Dataset over Sports Live"	https://paperswithcode.com/dataset/liveqa							
2108	LIVE-YT-HFR	"LIVE-YT-HFR comprises of 480 videos having 6 different frame rates, obtained from 16 diverse contents.
Source: Subjective and Objective Quality Assessment of High Frame Rate Videos"	https://paperswithcode.com/dataset/live-yt-hfr		LIVE YouTube High Frame Rate					
2109	LKS	"LKS is a dataset of 684 Liver-Kidney-Stomach immunofluorescence whole slide images (WSIs) used in the investigation of autoimmune liver disease.
Source: https://arxiv.org/abs/2003.05080
Image Source: https://github.com/cradleai/LKS-Dataset"	https://paperswithcode.com/dataset/lks		Liver Kidney Stomach					
2110	WI-LOCNESS	"WI-LOCNESS is part of the Building Educational Applications 2019 Shared Task for Grammatical Error Correction. It consists of two datasets:

LOCNESS: is a corpus consisting of essays written by native English students. 
Cambridge English Write & Improve (W&I): Write & Improve (Yannakoudakis et al., 2018) is an online web platform that assists non-native English students with their writing. Specifically, students from around the world submit letters, stories, articles and essays in response to various prompts, and the W&I system provides instant feedback. Since W&I went live in 2014, W&I annotators have manually annotated some of these submissions and assigned them a CEFR level.

Source: The BEA-2019 Shared Task on Grammatical Error Correction
Image source: WI-LOCNESS"	https://paperswithcode.com/dataset/locness-corpus		Cambridge English Write & Improve & LOCNESS					
2111	LoDoPaB-CT	"LoDoPaB-CT is a dataset of computed tomography images and simulated low-dose measurements. It contains over 40,000 scan slices from around 800 patients selected from the LIDC/IDRI Database. 
Source: The LoDoPaB-CT Dataset: A Benchmark Dataset for Low-Dose CT Reconstruction Methods"	https://paperswithcode.com/dataset/lodopab-ct							
2112	Logic2Text	"Logic2Text is a large-scale dataset with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics.
Source: https://github.com/czyssrs/Logic2Text"	https://paperswithcode.com/dataset/logic2text							
2113	LogiQA	"LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting.
Source: LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning
Image Source: https://arxiv.org/pdf/2007.08124v1.pdf"	https://paperswithcode.com/dataset/logiqa							
2114	Logo-2K+	"Logo-2K+:A Large-Scale Logo Dataset for Scalable Logo Classiﬁcation
The Logo-2K+ dataset contains a diverse range of logo classes from real-world logo images. It contains 167,140 images with 10 root categories and 2,341 leaf categories.
The 10 different root categories are: Food, Clothes, Institution, Accessories, Transportation, Electronic, Necessities, Cosmetic, Leisure and Medical.
Source: https://github.com/msn199959/Logo-2k-plus-Dataset
Image Source: https://github.com/msn199959/Logo-2k-plus-Dataset"	https://paperswithcode.com/dataset/logo-2k							
2115	LogoDet-3K	"A logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. 
Source: LogoDet-3K: A Large-Scale Image Dataset for Logo Detection"	https://paperswithcode.com/dataset/logodet-3k							
2116	LOGO-Net	"A large-scale logo image database for logo detection and brand recognition from real-world product images. 
Source: LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with Deep Region-based Convolutional Networks"	https://paperswithcode.com/dataset/logo-net							
2117	Long-Term Crowd Flow	"A synthetic dataset of procedurally generated environments, dynamically simulated crowd flows, and statically derived “proxy” crowd flows (which have more error but are more efficient to compute), for model training and evaluation.
Source: Laying the Foundations of Deep Long-Term Crowd Flow Prediction"	https://paperswithcode.com/dataset/long-term-crowd-flow							
2118	Long-term visual localization	"Long-term visual localization provides a benchmark datasets aimed at evaluating 6 DoF pose estimation accuracy over large appearance variations caused by changes in seasonal (summer, winter, spring, etc.) and illumination (dawn, day, sunset, night) conditions. Each dataset consists of a set of reference images, together with their corresponding ground truth poses, and a set of query images.
Source: Long-term visual localization
Image Source: https://www.visuallocalization.net/"	https://paperswithcode.com/dataset/long-term-visual-localization							
2119	Lost and Found	"Lost and Found is a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic.
Source: Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles
Image Source: http://www.6d-vision.com/lostandfounddataset"	https://paperswithcode.com/dataset/lost-and-found							
2120	LPW	Labeled Pedestrian in the Wild (LPW) is a pedestrian detection dataset that contains 2,731 pedestrians in three different scenes where each annotated identity is captured by from 2 to 4 cameras. The LPW features a notable scale of 7,694 tracklets with over 590,000 images as well as the cleanliness of its tracklets. It distinguishes from existing datasets in three aspects: large scale with cleanliness, automatically detected bounding boxes and far more crowded scenes with greater age span. This dataset provides a more realistic and challenging benchmark, which facilitates the further exploration of more powerful algorithms.	https://paperswithcode.com/dataset/lpw	23/11/2017	Labeled Pedestrian in the Wild					
2121	LRS3-TED	"LRS3-TED is a multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research. 
Source: LRS3-TED: a large-scale dataset for visual speech recognition"	https://paperswithcode.com/dataset/lrs3-ted							
2122	LS3D-W	"A 3D facial landmark dataset of around 230,000 images.
Source: How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)"	https://paperswithcode.com/dataset/ls3d-w							
2123	LSHTC	"LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist
of either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal). 
Source: LSHTC: A Benchmark for Large-Scale Text Classification"	https://paperswithcode.com/dataset/lshtc							
2124	LSICC	"Large Scale Informal Chinese Corpus (LSICC) is a large-scale corpus of informal Chinese. This corpus contains around 37 million book reviews and 50 thousand netizen's comments to the news.
Source: LSICC: A Large Scale Informal Chinese Corpus"	https://paperswithcode.com/dataset/lsicc		Large Scale Informal Chinese Corpus					
2125	LSLF	"Consists of a large number of unconstrained multi-view and partially occluded faces.
Source: Large-scale Datasets: Faces with Partial Occlusions and Pose Variations in the Wild"	https://paperswithcode.com/dataset/lslf		Large-scale Labeled Face					
2126	LSMDC-Context	"The Large Scale Movie Description Challenge (LSMDC) - Context is an augmented version of the original LSMDC dataset with movie scripts as contextual text.
Source: https://github.com/primle/LSMDC-Context"	https://paperswithcode.com/dataset/lsmdc-context							
2127	LVIS	"LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images.
Source: LVIS"	https://paperswithcode.com/dataset/lvis							
2128	Lyft Level 5 Prediction	"A self-driving dataset for motion prediction, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. 
Source: One Thousand and One Hours: Self-driving Motion Prediction Dataset"	https://paperswithcode.com/dataset/lyft-level-5-prediction							
2129	m2caiSeg	"Created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene.
Source: m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks"	https://paperswithcode.com/dataset/m2caiseg							
2130	M2E2	"Aims to extract events and their arguments from multimedia documents. Develops the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.
Source: Cross-media Structured Common Space for Multimedia Event Extraction"	https://paperswithcode.com/dataset/m2e2							
2131	Machine Number Sense	"Consists of visual arithmetic problems automatically generated using a grammar model--And-Or Graph (AOG). These visual arithmetic problems are in the form of geometric figures: each problem has a set of geometric shapes as its context and embedded number symbols. 
Source: Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning"	https://paperswithcode.com/dataset/machine-number-sense							
2132	Mafiascum	"A collection of over 700 games of Mafia, in which players are randomly assigned either deceptive or non-deceptive roles and then interact via forum postings. Over 9000 documents were compiled from the dataset, which each contained all messages written by a single player in a single game. This corpus was used to construct a set of hand-picked linguistic features based on prior deception research, as well as a set of average word vectors enriched with subword information. 
Source: The Mafiascum Dataset: A Large Text Corpus for Deception Detection"	https://paperswithcode.com/dataset/mafiascum							
2133	Makeup Datasets	"A dataset of female face images assembled for studying the impact of makeup on face recognition.
Source: Makeup Datasets"	https://paperswithcode.com/dataset/makeup-datasets							
2134	Malaria Dataset	"The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells.
Source: Malaria Dataset"	https://paperswithcode.com/dataset/malaria-dataset							
2135	MalayalamMixSentiment	"MalayalamMixSentiment is a Sentiment Analysis Dataset for Code-Mixed Malayalam-English.
Source: https://github.com/bharathichezhiyan/MalayalamMixSentiment"	https://paperswithcode.com/dataset/malayalammixsentiment							
2136	MAMe	"The MAMe dataset contains images of high-resolution and variable shape of artworks from 3 different museums:

The Metropolitan Museum of Art of New York
The Los Angeles County Museum of Art
The Cleveland Museum of Art

Source: https://github.com/HPAI-BSC/MAMe-baselines
Image Source: Pares´ et al"	https://paperswithcode.com/dataset/mame		Museum Art Medium dataset					
2137	MANTRA	"An annotated dataset of 4869 transient and 71207 non-transient object lightcurves built from the Catalina Real Time Transient Survey. 
Source: MANTRA: A Machine Learning reference lightcurve dataset for astronomical transient event recognition"	https://paperswithcode.com/dataset/mantra							
2138	ManyModalQA	"Collects the data by scraping Wikipedia and then utilize crowdsourcing to collect question-answer pairs. 
Source: ManyModalQA: Modality Disambiguation and QA over Diverse Inputs"	https://paperswithcode.com/dataset/manymodalqa							
2139	Market1203-Reid-Dataset	"This dataset contains 1203 individuals captured from two disjoint camera views. To each person, one to twelve images are captured from one to six different orientations under one camera view and are normalized to 128x64 pixels. This dataset is constructed based on the Market-1501 benchmark data and the orientation label for each image has been manually annotated.
Source: https://github.com/charliememory/Market1203-Reid-Dataset"	https://paperswithcode.com/dataset/market1203-reid-dataset							
2140	Marmara Turkish Coreference Resolution Corpus	"Describe the Marmara Turkish Coreference Corpus, which is an annotation of the whole METU-Sabanci Turkish Treebank with mentions and coreference chains.
Source: Marmara Turkish Coreference Corpus and Coreference Resolution Baseline"	https://paperswithcode.com/dataset/marmara-turkish-coreference-resolution-corpus							
2141	MASATI	"The MASATI dataset contains color images in dynamic marine environments, and it can be used to evaluate ship detection methods. Each image may contain one or multiple targets in different weather and illumination conditions. The datasets is composed of 7,389 satellite images labeled according to the following seven classes: land, coast, sea, ship, multi, coast-ship, and detail. In addition, labeling with the bounding box for the location of the vessels is also included.
Source: MASATI"	https://paperswithcode.com/dataset/masati		MAritime SATellite Imagery dataset					
2142	MaskedFace-Net	"Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net).
Source: MaskedFace-Net -- A Dataset of Correctly/Incorrectly Masked Face Images in the Context of COVID-19"	https://paperswithcode.com/dataset/maskedface-net							
2143	MASRI-HEADSET	"MASRI-HEADSET is a corpus that was developed by the MASRI project at the University of Malta. It consists of 8 hours of speech paired with text, recorded by using short text snippets in a laboratory environment. The speakers were recruited from different geographical locations all over the Maltese islands, and were roughly evenly distributed by gender. 
Source: MASRI-HEADSET: A Maltese Corpus for Speech Recognition"	https://paperswithcode.com/dataset/masri-headset							
2144	MaSS	"MaSS (Multilingual corpus of Sentence-aligned Spoken utterances) is an extension of the CMU Wilderness Multilingual Speech Dataset, a speech dataset based on recorded readings of the New Testament.
MaSS extends it by providing a large and clean dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). The covered languages are: Basque, English, Finnish, French, Hungarian, Romanian, Russian and Spanish.
Source: MaSS: A Large and Clean Multilingual Corpus of Sentence-aligned Spoken Utterances Extracted from the Bible
Image Source: https://arxiv.org/pdf/1907.12895v3.pdf"	https://paperswithcode.com/dataset/mass							
2145	MathQA	"MathQA significantly enhances the AQuA dataset with fully-specified operational programs. 
Source: MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"	https://paperswithcode.com/dataset/mathqa							
2146	MatterportLayout	"MatterportLayout extends the Matterport3D dataset with general Manhattan layout annotations. It has 2,295 RGBD panoramic images from Matterport3D which are extended with ground truth 3D layouts.
Source: https://github.com/ericsujw/Matterport3DLayoutAnnotation"	https://paperswithcode.com/dataset/matterportlayout							
2147	MAVEN	"Contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. 
Source: MAVEN: A Massive General Domain Event Detection Dataset"	https://paperswithcode.com/dataset/maven							
2148	MCAD	"Designed to evaluate the open view classification problem under the surveillance environment. In total, MCAD contains 14,298 action samples from 18 action categories, which are performed by 20 subjects and independently recorded with 5 cameras.
Source: Multi-Camera Action Dataset for Cross-Camera Action Recognition Benchmarking"	https://paperswithcode.com/dataset/mcad		Multi-Camera Action Dataset					
2149	MC-AFP	"A dataset of around 2 million examples for machine reading-comprehension.
Source: Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors"	https://paperswithcode.com/dataset/mc-afp							
2150	MCIC-COCO	"A large-scale machine comprehension dataset (based on the COCO images and captions).
Source: Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task"	https://paperswithcode.com/dataset/mcic-coco							
2151	MC-TACO	"MC-TACO is a dataset of 13k question-answer pairs that require temporal commonsense comprehension. The dataset contains five temporal properties, (1) duration (how long an event takes), (2) temporal ordering (typical order of events), (3) typical time (when an event occurs), (4) frequency (how often an event occurs), and (5) stationarity (whether a state is maintained for a very long time or indefinitely). 
Source: MC-TACO"	https://paperswithcode.com/dataset/mc-taco	06/09/2019						
2152	MD4K	"A small-scale training set, which only contains 4K images.
Source: A Deeper Look at Salient Object Detection: Bi-stream Network with a Small Training Dataset"	https://paperswithcode.com/dataset/md4k							
2153	MD Gender	"Provides eight automatically annotated large scale datasets with gender information. 
Source: Multi-Dimensional Gender Bias Classification"	https://paperswithcode.com/dataset/md-gender		Multi-Dimensional Gender Bias Datasets					
2154	mEBAL	"A multimodal database for eye blink detection and attention level estimation. 
Source: mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level Estimation"	https://paperswithcode.com/dataset/mebal							
2155	MED	"MED is a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications. The dataset was constructed by collecting naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications.
It consists of 5,382 examples.
Source: https://github.com/verypluming/MED
Image Source: https://www.aclweb.org/anthology/W19-4804v2.pdf"	https://paperswithcode.com/dataset/med		Monotonicity Entailment Dataset					
2156	MeDAL	"The  Medical Dataset for Abbreviation Disambiguation for Natural Language Understanding (MeDAL) is a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. It was published at the ClinicalNLP workshop at EMNLP.
Source: https://github.com/BruceWen120/medal
Image Source: https://github.com/BruceWen120/medal"	https://paperswithcode.com/dataset/medal							
2157	MedDG	"MedDG is a large-scale high-quality Medical Dialogue dataset related to 12 types of common Gastrointestinal diseases. It contains more than 17K conversations collected from the online health consultation community. Five different categories of entities, including diseases, symptoms, attributes, tests, and medicines, are annotated in each conversation of MedDG as additional labels.
Two kinds of medical dialogue tasks are proposed for this dataset:
* Next entity prediction
* Doctor response generation
Source: https://github.com/lwgkzl/MedDG"	https://paperswithcode.com/dataset/meddg							
2158	Medical Case Report Corpus	"Medical Case Report Corpus is a new corpus comprising annotations of medical entities in case reports, originating from PubMed Central's open access library. 
Source: Named Entities in Medical Case Reports: Corpus and Experiments"	https://paperswithcode.com/dataset/medical-case-report-corpus							
2159	MedICaT	"MedICaT is a dataset of medical images, captions, subfigure-subcaption annotations, and inline textual references.
Figures and captions are extracted from open access articles in PubMed Central and corresponding reference text is derived from S2ORC.
The dataset consists of:
217,060 figures from 131,410 open access papers
7507 subcaption and subfigure annotations for 2069 compound figures
Inline references for ~25K figures in the ROCO dataset
Source: https://github.com/allenai/medicat"	https://paperswithcode.com/dataset/medicat							
2160	MEDIQA-AnS	"The first summarization collection containing question-driven summaries of answers to consumer health questions. This dataset can be used to evaluate single or multi-document summaries generated by algorithms using extractive or abstractive approaches. 
Source: Question-Driven Summarization of Answers to Consumer Health Questions"	https://paperswithcode.com/dataset/mediqa-ans		MEDIQA-Answer Summarization					
2161	medisim	"medisim is a collection of new large-scale medical term similarity datasets based on SNOMED-CT.
Source: https://github.com/babylonhealth/medisim"	https://paperswithcode.com/dataset/medisim							
2162	Medley2K	"A dataset called Medley2K that consists of 2,000 medleys and 7,712 labeled transitions. 
Source: Medley2K: A Dataset of Medley Transitions"	https://paperswithcode.com/dataset/medley2k							
2163	MedMentions	"MedMentions is a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. 
Source: MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts"	https://paperswithcode.com/dataset/medmentions							
2164	MedMNIST	"A collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge. 
Source: MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis"	https://paperswithcode.com/dataset/medmnist							
2165	MedQuAD	"MedQuAD includes 47,457 medical question-answer pairs created from 12 NIH websites (e.g. cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). The collection covers 37 question types (e.g. Treatment, Diagnosis, Side Effects) associated with diseases, drugs and other medical entities such as tests.  
Source: A Question-Entailment Approach to Question Answering"	https://paperswithcode.com/dataset/medquad		Medical Question Answering Dataset					
2166	MegaAge	MegaAge is a large dataset that consists of 41,941 faces annotated with age posterior distributions.	https://paperswithcode.com/dataset/megaage							
2167	Mega-COV	"Mega-COV is a billion-scale dataset from Twitter for studying COVID-19. The dataset is diverse (covers 234 countries), longitudinal (goes as back as 2007), multilingual (comes in 65 languages), and has a significant number of location-tagged tweets (~32M tweets).
Source: https://github.com/UBC-NLP/megacov
Image Source: https://github.com/UBC-NLP/megacov"	https://paperswithcode.com/dataset/mega-cov							
2168	MegaDepth	"The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS.
Source: MegaDepth: Learning Single-View Depth Prediction from Internet Photos"	https://paperswithcode.com/dataset/megadepth							
2169	MeGlass	"MeGlass is an eyeglass dataset originally designed for eyeglass face recognition evaluation. All the face images are selected and cleaned from MegaFace. Each identity has at least two face images with eyeglass and two face images without eyeglass. It contains 47,817 images from 1,710 different identities.
Source: https://github.com/cleardusk/MeGlass
Image Source: https://github.com/cleardusk/MeGlass"	https://paperswithcode.com/dataset/meglass							
2170	MEIR	"MEIR is a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr.
Source: Deep Multimodal Image-Repurposing Detection"	https://paperswithcode.com/dataset/meir	20/08/2018	Multimodal Entity Image Repurposing					
2171	Melinda	"Introduces a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt methoD clAssification. The dataset is collected in a fully automated distant supervision manner, where the labels are obtained from an existing curated database, and the actual contents are extracted from papers associated with each of the records in the database. 
Source: MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification"	https://paperswithcode.com/dataset/melinda							
2172	Memeify	"A large-scale dataset of memes with captions and class labels. The dataset consists of 1.1 million meme captions from 128 classes.
Source: Memeify: A Large-Scale Meme Generation System"	https://paperswithcode.com/dataset/memeify							
2173	MetaLWOz	"Collected by leveraging background knowledge from a larger, more highly represented dialogue source.
Source: Few-Shot Dialogue Generation Without Annotated Data: A Transfer Learning Approach"	https://paperswithcode.com/dataset/metalwoz		Meta-Learning Wizard-of-Oz					
2174	Metaphorics	"Metaphorics is a newly introduced non-contextual skeleton action dataset. All the datasets introduced so far in the skeleton human action recognition have categories based only on verb-based actions.
Source: Metaphorics"	https://paperswithcode.com/dataset/metaphorics							
2175	Meta-World Benchmark	"An open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.
Source: Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"	https://paperswithcode.com/dataset/meta-world-benchmark							
2176	methods2test	"methods2test is a supervised dataset consisting of Test Cases and their corresponding Focal Methods from a set of Java software repositories.
Methods2test was constructed by parsing the Java projects to obtain classes and methods with their associated metadata. Next each Test Class was matched to its corresponding Focal Class. Finally, each Test Case within a Test Class was mapped to the related Focal Method to obtain a set of Mapped Test Cases.
Source: https://github.com/microsoft/methods2test"	https://paperswithcode.com/dataset/methods2test							
2177	#MeTooMA	"The dataset consists of tweets belonging to #MeToo movement on Twitter, labelled into different categories. 
Source: #MeTooMA"	https://paperswithcode.com/dataset/metooma							
2178	AraMeter	"A dataset to identify the meters of Arabic poems.
Source: https://github.com/zaidalyafeai/MetRec"	https://paperswithcode.com/dataset/metrec							
2179	METU-ALET	"METU-ALET is an image dataset for the detection of the tools in the wild. The dataset has annotations for tools that belongs to the categories such as farming, gardening, office, stonemasonry, vehicle, woodworking and workshop. The images in the dataset contains a total of 22,841 bounding boxes and 49 different tool categories.
Source: https://github.com/metu-kovan/METU-ALET
Image Source: https://github.com/metu-kovan/METU-ALET"	https://paperswithcode.com/dataset/metu-alet							
2180	MEVA	"Large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. The dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity.
Source: MEVA: A Large-Scale Multiview, Multimodal Video Dataset for Activity Detection"	https://paperswithcode.com/dataset/meva		Multiview Extended Video with Activities					
2181	Mewsli-9	"A large new multilingual dataset for multilingual entity linking.
Source: Entity Linking in 100 Languages"	https://paperswithcode.com/dataset/mewsli-9							
2182	MEx	"A multi-sensor, multi-modal dataset, implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion algorithms. Collection of this dataset was inspired by the need for recognising and evaluating quality of exercise performance to support patients with Musculoskeletal Disorders(MSD).
Source: MEx: Multi-modal Exercises Dataset for Human Activity Recognition"	https://paperswithcode.com/dataset/mex							
2183	Microsoft Research Social Media Conversation Corpus	"Microsoft Research Social Media Conversation Corpus consists of 127M context-message-response triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012. Only those triples where context and response were generated by the same user were extracted. To minimize noise, only triples that contained at least one frequent bigram that appeared more than 3 times in the corpus was selected. This produced a corpus of 29M Twitter triples.
Source: A Neural Network Approach to Context-Sensitive Generation of Conversational Responses"	https://paperswithcode.com/dataset/microsoft-research-social-media-conversation							
2184	Mid-Air Dataset	"Mid-Air, The Montefiore Institute Dataset of Aerial Images and Records, is a multi-purpose synthetic dataset for low altitude drone flights. It provides a large amount of synchronized data corresponding to flight records for multi-modal vision sensors and navigation sensors mounted on board of a flying quadcopter. Multi-modal vision sensors capture RGB pictures, relative surface normal orientation, depth, object semantics and stereo disparity.
Source: Mid-Air Dataset"	https://paperswithcode.com/dataset/mid-air-dataset							
2185	MIDAS-KIKI	"Consists of manually annotated dangerous and non-dangerous Kiki challenge videos. 
Source: Kiki Kills: Identifying Dangerous Challenge Videos from Social Media"	https://paperswithcode.com/dataset/midas-kiki							
2186	MIDV-2019	"Contains video clips shot with modern high-resolution mobile cameras, with strong projective distortions and with low lighting conditions.
Source: MIDV-2019: Challenges of the modern mobile-based document OCR"	https://paperswithcode.com/dataset/midv-2019							
2187	MilkQA	"A question answering dataset from the dairy domain dedicated to the study of consumer questions. The dataset contains 2,657 pairs of questions and answers, written in the Portuguese language and originally collected by the Brazilian Agricultural Research Corporation (Embrapa). All questions were motivated by real situations and written by thousands of authors with very different backgrounds and levels of literacy, while answers were elaborated by specialists from Embrapa's customer service. 
Source: MilkQA: a Dataset of Consumer Questions for the Task of Answer Selection"	https://paperswithcode.com/dataset/milkqa							
2188	MIMII	"Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection (MIMII) is a sound dataset
of industrial machine sounds.
Source: MIMII Dataset: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection"	https://paperswithcode.com/dataset/mimii							
2189	MINC	"MINC is a large-scale, open dataset of materials in the wild.
Source: Material Recognition in the Wild with the Materials in Context Database
Image Source: Bell et al"	https://paperswithcode.com/dataset/minc		Materials in Context Database					
2190	MinneApple	"MinneApple is a benchmark dataset for apple detection and segmentation. The fruits are labelled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, the dataset also contains data for patch-based counting of clustered fruits. The dataset contains over 41, 000 annotated object instances in 1000 images.
Source: https://github.com/nicolaihaeni/MinneApple
Image Source: https://github.com/nicolaihaeni/MinneApple"	https://paperswithcode.com/dataset/minneapple							
2191	MitoEM	"Contains mitochondria instances.
Source: MitoEM"	https://paperswithcode.com/dataset/mitoem							
2192	MITOS_WSI_CMC	"A dataset of 21 WSIs of CMC completely annotated for MF. For this, a pathologist screened all WSIs for potential MF and structures with a similar appearance.
Source: A completely annotated whole slide image dataset of canine breast cancer to aid human breast cancer research"	https://paperswithcode.com/dataset/mitos-wsi-cmc							
2193	MIZAN	"Persian-English parallel corpus with more than one million sentence pairs collected from masterpieces of literature.
Source: MIZAN: A Large Persian-English Parallel Corpus"	https://paperswithcode.com/dataset/mizan							
2194	MJU-Waste	"MJU-Waste is an RGBD waste object segmentation dataset that is made public to facilitate future research in this area.
Source: https://github.com/realwecan/mju-waste
Image Source: Wang et al"	https://paperswithcode.com/dataset/mju-waste							
2195	MKQA	Multilingual Knowledge Questions and Answers (MKQA) is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). The goal of this dataset is to provide a challenging benchmark for question answering quality across a wide set of languages. Answers are based on a language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering.	https://paperswithcode.com/dataset/mkqa		Multilingual Knowledge Questions and Answers					
2196	MK-SQuIT	"An example dataset of 110,000 question/query pairs across four WikiData domains.
Source: MK-SQuIT: Synthesizing Questions using Iterative Template-filling"	https://paperswithcode.com/dataset/mk-squit							
2197	MLB Dataset	"A new dataset on the baseball domain.
Source: Data-to-text Generation with Entity Modeling"	https://paperswithcode.com/dataset/mlb-dataset							
2198	MLB-YouTube Dataset	"The MLB-YouTube dataset is a new, large-scale dataset consisting of 20 baseball games from the 2017 MLB post-season available on YouTube with over 42 hours of video footage. The dataset consists of two components: segmented videos for activity recognition and continuous videos for activity classification. It is quite challenging as it is created from TV broadcast baseball games where multiple different activities share the camera angle. Further, the motion/appearance difference between the various activities is quite small.
Source: https://github.com/piergiaj/mlb-youtube
Image Source: https://github.com/piergiaj/mlb-youtube"	https://paperswithcode.com/dataset/mlb-youtube-dataset							
2199	MLM	"A new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. 
Source: MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities"	https://paperswithcode.com/dataset/mlm							
2200	MLMA Hate Speech	"A new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches.
Source: Multilingual and Multi-Aspect Hate Speech Analysis"	https://paperswithcode.com/dataset/mlma-hate-speech							
2201	MLQE	"The MLQE dataset is a dataset for sentence-level Machine Translation Quality Estimation. It consists of 6 language pairs representing NMT training in high, medium, and low-resource scenarios. The corpus is extracted from Wikipedia, and 10K segments per language pair are annotated.
Source: https://github.com/facebookresearch/mlqe"	https://paperswithcode.com/dataset/mlqe		MultiLingual Quality Estimation					
2202	MLQE-PE	"The Multilingual Quality Estimation and Automatic Post-editing (MLQE-PE) Dataset is a dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains seven language pairs, with human labels for 9,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level good/bad labels. It also contains the post-edited sentences, as well as titles of the articles where the sentences were extracted from, and the neural MT models used to translate the text.
Source: https://github.com/sheffieldnlp/mlqe-pe"	https://paperswithcode.com/dataset/mlqe-pe		Multilingual Quality Estimation and Automatic Post-editing Dataset					
2203	MLRSNet	"MLRSNet is a a multi-label high spatial resolution remote sensing dataset for semantic scene understanding. It provides different perspectives of the world captured from satellites. That is, it is composed of high spatial resolution optical satellite images. MLRSNet contains 109,161 remote sensing images that are annotated into 46 categories, and the number of sample images in a category varies from 1,500 to 3,000. The images have a fixed size of 256×256 pixels with various pixel resolutions (~10m to 0.1m). Moreover, each image in the dataset is tagged with several of 60 predefined class labels, and the number of labels associated with each image varies from 1 to 13. The dataset can be used for multi-label based image classification, multi-label based image retrieval, and image segmentation.
Source: https://github.com/cugbrs/MLRSNet
Image Source: Qi et al"	https://paperswithcode.com/dataset/mlrsnet							
2204	MLS	"The Multiple Light Source dataset (MLS) is a collection of 24 multiple object scenes each recorded under 18 multiple light source illumination scenarios. The illuminants are varying in dominant spectral colours, intensity and distance from the scene. The dataset can be used for the evaluation of computational colour constancy algorithms. Along with the images of the scenes the spectral characteristics of the camera, light sources and the objects are also provided, and each image includes pixel-by-pixel ground truth annotation of uniformly coloured object surfaces thus making this useful for benchmarking colour-based image segmentation algorithms.
Source: https://arxiv.org/abs/1908.06126
Image Source: https://github.com/Visillect/mls-dataset"	https://paperswithcode.com/dataset/mls		Multiple Light Source					
2205	MLSUM	"A large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community.
Source: MLSUM: The Multilingual Summarization Corpus"	https://paperswithcode.com/dataset/mlsum		MultiLingual SUMmarization					
2206	MMAct	"MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. 
Source: MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding"	https://paperswithcode.com/dataset/mmact							
2207	MMD	"The MMD (MultiModal Dialongs) dataset is a dataset for multimodal domain-aware conversations. It consists of over 150K conversation sessions between shoppers and sales agents, annotated by a group of in-house annotators using a semi-automated manually intense iterative process. 
Source: Towards Building Large Scale Multimodal Domain-Aware Conversation Systems"	https://paperswithcode.com/dataset/mmd		Multimodal Dialogs					
2208	MMED	"Contains 25,165 textual news articles collected from hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and 76,516 image posts shared on Flickr social media, which are annotated according to 412 real-world events. The dataset is collected to explore the problem of organizing heterogeneous data contributed by professionals and amateurs in different data domains, and the problem of transferring event knowledge obtained from one data domain to heterogeneous data domain, thus summarizing the data with different contributors.
Source: MMED: A Multi-domain and Multi-modality Event Dataset"	https://paperswithcode.com/dataset/mmed							
2209	MMID	"A large-scale multilingual corpus of images, each labeled with the word it represents. The dataset includes approximately 10,000 words in each of 100 languages.
Source: Learning Translations via Images with a Massively Multilingual Image Dataset"	https://paperswithcode.com/dataset/mmid		Massively Multilingual Image Dataset					
2210	MNIST-1D	"A minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94% accuracy respectively (these models obtain 94, 99+, and 99+% on MNIST).
Source: Scaling down Deep Learning"	https://paperswithcode.com/dataset/mnist-1d							
2211	MNIST-MIX	"MNIST-MIX is a multi-language handwritten digit recognition dataset. It contains digits from 10 different languages.
Source: https://github.com/jwwthu/MNIST-MIX"	https://paperswithcode.com/dataset/mnist-mix							
2212	Mo2Cap2	"A large ground truth training corpus of top-down fisheye images.
Source: Mo2Cap2"	https://paperswithcode.com/dataset/mo2cap2							
2213	MobiBits	"A novel database comprising representations of five different biometric characteristics, collected in a mobile, unconstrained or semi-constrained setting with three different mobile devices, including characteristics previously unavailable in existing datasets, namely hand images, thermal hand images, and thermal face images, all acquired with a mobile, off-the-shelf device.
Source: MobiBits: Multimodal Mobile Biometric Database"	https://paperswithcode.com/dataset/mobibits		Multimodal Mobile Biometric Database					
2214	MOCHA	"Contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. 
Source: MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics"	https://paperswithcode.com/dataset/mocha							
2215	MOD++	"Includes challenging sequences and extensive data stratification in-terms of camera and object motion, velocity magnitudes, direction, and rotational speeds.
Source: 0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera"	https://paperswithcode.com/dataset/mod							
2216	ModaNet	"ModaNet is a street fashion images dataset consisting of annotations related to RGB images. ModaNet provides multiple polygon annotations for each image. Each polygon is associated with a label from 13 meta fashion categories. The annotations are based on images in the PaperDoll image set, which has only a few hundred images annotated by the superpixel-based tool.
Source: ModaNet
Image Source: ModaNet"	https://paperswithcode.com/dataset/modanet							
2217	Modern Hebrew Sentiment Dataset	"Modern Hebrew Sentiment Dataset is a sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances of these data: in token-based and morpheme-based settings.
Source: Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew"	https://paperswithcode.com/dataset/modern-hebrew-sentiment-dataset							
2218	Molweni	"A machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni's source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. 
Source: Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure"	https://paperswithcode.com/dataset/molweni							
2219	MonoPerfCap Dataset	MonoPerfCap is a benchmark dataset for human 3D performance capture from monocular video input  consisting of around 40k frames, which covers a variety of different scenarios.	https://paperswithcode.com/dataset/monoperfcap-dataset							
2220	Moral Stories	"A crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning.
Source: Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences"	https://paperswithcode.com/dataset/moral-stories							
2221	MOROCO	"The MOldavian and ROmanian Dialectal COrpus (MOROCO) is a corpus that contains 33,564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21,719 samples for training, 5,921 samples for validation and another 5,924 samples for testing. 
Source: MOROCO: The Moldavian and Romanian Dialectal Corpus"	https://paperswithcode.com/dataset/moroco		MOldavian and ROmanian Dialectal COrpus					
2222	MOR-UAV	"A large-scale video dataset for MOR in aerial videos.
Source: MOR-UAV: A Benchmark Dataset and Baselines for Moving Object Recognition in UAV Videos"	https://paperswithcode.com/dataset/mor-uav							
2223	MosMedData	"MosMedData contains anonymised human lung computed tomography (CT) scans with COVID-19 related findings, as well as without such findings. A small subset of studies has been annotated with binary pixel masks depicting regions of interests (ground-glass opacifications and consolidations). CT scans were obtained between 1st of March, 2020 and 25th of April, 2020, and provided by municipal hospitals in Moscow, Russia.
Source: MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset"	https://paperswithcode.com/dataset/mosmeddata							
2224	MOTChallenge	"The MOTChallenge datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20.
Source: MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking"	https://paperswithcode.com/dataset/motchallenge							
2225	MotionSense	"This dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket using SensingKit which collects information from Core Motion framework on iOS devices. All data is collected in 50Hz sample rate. A total of 24 participants in a range of gender, age, weight, and height performed 6 activities in 15 trials in the same environment and conditions: downstairs, upstairs, walking, jogging, sitting, and standing.
Source: https://github.com/mmalekzadeh/motion-sense
Image Source: https://github.com/mmalekzadeh/motion-sense"	https://paperswithcode.com/dataset/motionsense							
2226	Mouse Embryo Tracking Database	The Mouse Embryo Tracking Database is a dataset for tracking mouse embryos. The dataset contains, for each of the 100 examples: (1) the uncompressed frames, up to the 10th frame after the appearance of the 8th cell; (2) a text file with the trajectories of all the cells, from appearance to division (for cells of generations 1 to 3), where a trajectory is a sequence of pairs (center, radius); (3) a movie file showing the trajectories of the cells.	https://paperswithcode.com/dataset/mouse-embryo-tracking-database							
2227	Mouse Reach	"A large, annotated video dataset of mice performing a sequence of actions. The dataset was collected and labeled by experts for the purpose of neuroscience research. 
Source: Detecting the Starting Frame of Actions in Video"	https://paperswithcode.com/dataset/mouse-reach							
2228	MoVi	"Contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement.
Source: MoVi: A Large Multipurpose Motion and Video Dataset"	https://paperswithcode.com/dataset/movi							
2229	MovieFIB	"A quantitative benchmark for developing and understanding video of fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. 
Source: A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering"	https://paperswithcode.com/dataset/moviefib		Movie Fill-in-the-Blank					
2230	MovieGraphs	"Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions.
Source: MovieGraphs: Towards Understanding Human-Centric Situations from Videos"	https://paperswithcode.com/dataset/moviegraphs							
2231	Moviescope	"Moviescope is a large-scale dataset of 5,000 movies with corresponding video trailers, posters, plots and metadata. Moviescope is based on the IMDB 5000 dataset consisting of 5.043 movie records. It is augmented by crawling video trailers associated with each movie from YouTube and text plots from Wikipedia.
Source: Moviescope: Large-scale Analysis of Movies using Multiple Modalities"	https://paperswithcode.com/dataset/moviescope							
2232	Moving Symbols	"A parameterized synthetic dataset called Moving Symbols to support the objective study of video prediction networks. 
Source: A Dataset To Evaluate The Representations Learned By Video Prediction Models"	https://paperswithcode.com/dataset/moving-symbols							
2233	MPI3D Disentanglement	"A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position.
Source: On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset"	https://paperswithcode.com/dataset/mpi3d-disentanglement							
2234	MPI FAUST Dataset	"Contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology.
Source: FAUST: Dataset and Evaluation for 3D Mesh Registration"	https://paperswithcode.com/dataset/mpi-faust-dataset							
2235	MPII Cooking 2 Dataset	"A dataset which provides detailed annotations for activity recognition.
Source: Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data"	https://paperswithcode.com/dataset/mpii-cooking-2-dataset							
2236	MQR	"A multi-domain question rewriting dataset is constructed from human contributed Stack Exchange question edit histories. The dataset contains 427,719 question pairs which come from 303 domains.
Source: How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions"	https://paperswithcode.com/dataset/mqr							
2237	MRNet	"The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports. 
Source: MRNet"	https://paperswithcode.com/dataset/mrnet							
2238	MRQA 2019	"The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems.
Source: MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension"	https://paperswithcode.com/dataset/mrqa-2019							
2239	MS-ASL	"MS-ASL is a real-life large-scale sign language data set comprising over 25,000 annotated videos.
Source: MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language"	https://paperswithcode.com/dataset/ms-asl							
2240	MSC	"MSC is a dataset for Macro-Management in StarCraft 2 based on the platfrom SC2LE. It consists of well-designed feature vectors, pre-defined high-level actions and final result of each match. It contains 36,619 high quality replays, which are unbroken and played by relatively professional players.
Source: https://github.com/wuhuikai/MSC"	https://paperswithcode.com/dataset/msc							
2241	MSeg	"A composite dataset that unifies semantic segmentation datasets from different domains. 
Source: MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation"	https://paperswithcode.com/dataset/mseg	27/12/2021						
2242	MSR ActionPairs	"This is a 3D action recognition dataset, also known as 3D Action Pairs dataset. The actions in this dataset are selected in pairs such that the two actions of each pair are similar in motion (have similar trajectories) and shape (have similar objects); however, the motion-shape relation is different. 
Source: HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences"	https://paperswithcode.com/dataset/msr-actionpairs							
2243	MTNT	"The Machine Translation of Noisy Text (MTNT) dataset is a Machine Translation dataset that consists of noisy comments on Reddit and professionally sourced translation. The translation are between French, Japanese and French, with between 7k and 37k sentence per language pair.
Source: https://arxiv.org/abs/1809.00388
Image Source: https://github.com/pmichel31415/mtnt"	https://paperswithcode.com/dataset/mtnt							
2244	MuCo-3DHP	"MuCo-3DHP is a large scale training data set showing real images of sophisticated multi-person interactions and occlusions. 
Source: Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB"	https://paperswithcode.com/dataset/muco-3dhp							
2245	Multi30k	"Multi30K is a dataset to stimulate multilingual multimodal research for English-German. It is based on the Flickr30k dataset, which contains 31,014 images sourced from online photo-sharing websites. Each image is paired with
five English descriptions, which were collected from Amazon Mechanical Turk. The dataset contains 145,000 training, 5,070 development, and 5,000 test descriptions. The Multi30K dataset extends the Flickr30K dataset with translated and independent German sentences.
Source: Multi30K: Multilingual English-German Image Descriptions"	https://paperswithcode.com/dataset/multi30k-1							
2246	MultiBooked	"MultiBooked is a dataset for supervised aspect-level sentiment analysis in Basque and Catalan, both of which are under-resourced languages. 
Source: MultiBooked: A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification"	https://paperswithcode.com/dataset/multibooked							
2247	MultiFC	"Publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. 
Source: MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims"	https://paperswithcode.com/dataset/multifc							
2248	MultiReQA	"MultiReQA is a cross-domain evaluation for retrieval question answering models. Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus. MultiReQA is a new multi-domain ReQA evaluation suite composed of eight retrieval QA tasks drawn from publicly available QA datasets from the MRQA shared task.
MultiReQA contains the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. Five of these datasets, including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, contain both training and test data, and three, in cluding BioASQ, RelationExtraction, TextbookQA, contain only the test data.
Source: https://github.com/google-research-datasets/MultiReQA"	https://paperswithcode.com/dataset/multireqa							
2249	MultiSenti	"MultiSenti presents a labeled dataset called MultiSenti for sentiment classification of code-switched informal short text, (2) explore the feasibility of adapting resources from a resource-rich language for an informal one, and (3) propose a deep learning-based model for sentiment classification of code-switched informal short text.
Source: Adapting Deep Learning for Sentiment Classification of Code-Switched Informal Short Text"	https://paperswithcode.com/dataset/multisenti							
2250	Multi-species fruit flower detection datasets	"This dataset consists of four sets of flower images, from three different species: apple, peach, and pear, and accompanying ground truth images. The images were acquired under a range of imaging conditions. These datasets support work in an accompanying paper that demonstrates a flower identification algorithm that is robust to uncontrolled environments and applicable to different flower species. While this data is primarily provided to support that paper, other researchers interested in flower detection may also use the dataset to develop new algorithms. Flower detection is a problem of interest in orchard crops because it is related to management of fruit load.
Source: Multi-species fruit flower detection datasets"	https://paperswithcode.com/dataset/multi-species-fruit-flower-detection-datasets							
2251	MultiviewX	"MultiviewX is a synthetic Multiview pedestrian detection dataset. It is build using pedestrian models from PersonX, in Unity.
The MultiviewX dataset covers a square of 16 meters by 25 meters. The ground plane is quantized into a 640x1000 grid. There are 6 cameras with overlapping field-of-view in the MultiviewX dataset, each of which outputs a 1080x1920 resolution image. On average, 4.41 cameras are covering the same location.
Source: https://github.com/hou-yz/MVDet
Image Source: https://github.com/hou-yz/MVDet"	https://paperswithcode.com/dataset/multiviewx							
2252	MultiWOZ-coref	"MultiWOZ-coref, (or MultiWOZ 2.3) is an extension of the MultiWOZ dataset that adds co-reference annotations in addition to corrections of dialogue acts and dialogue states.
Source: https://github.com/lexmen318/MultiWOZ_2.3"	https://paperswithcode.com/dataset/multiwoz-coref							
2253	Multi-XScience	"Multi-XScience is a large-scale dataset for multi-document summarization of scientific articles. It has 30,369, 5,066 and 5,093 samples for the train, validation and test split respectively. The average document length is 778.08 words and the average summary length is 116.44 words.
Source: https://github.com/yaolu/Multi-XScience"	https://paperswithcode.com/dataset/multi-xscience							
2254	MURA	"A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. 
Source: MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs"	https://paperswithcode.com/dataset/mura							
2255	MUTLA	"This dataset includes time-synchronized multimodal data records of students (learning logs, videos, EEG brainwaves) as they work in various subjects from Squirrel AI Learning System (SAIL) to solve problems of varying difficulty levels. The dataset resources include user records from the learner records store of SAIL, brainwave data collected by EEG headset devices, and video data captured by web cameras while students worked in the SAIL products. 
Source: MUTLA: A Large-Scale Dataset for Multimodal Teaching and Learning Analytics"	https://paperswithcode.com/dataset/mutla							
2256	TAPOS	"TAPOS is a new dataset developed on sport videos with manual annotations of sub-actions, and conduct a study on temporal action parsing on top. A sport activity usually consists of multiple sub-actions and that the awareness of such temporal structures is beneficial to action recognition.
TAPOS contains 16,294 valid instances in total, across 21 action classes. These instances have a duration of 9.4
seconds on average. The number of instances within each class is different, where the largest class high jump has over
1,600 instances, and the smallest class beam has 200 instances. The average number of sub-actions also varies
from class to class, where parallel bars has 9 sub-actions on average, and long jump has 3 sub-actions on average. All instances are split into train, validation and test sets, of sizes 13094, 1790, and 1763, respectively."	https://paperswithcode.com/dataset/tapos	20/05/2020	TAPOS					
2257	MutualFriends	"In MutualFriends, two agents, A and B, each have a private knowledge base, which contains a list of friends with multiple attributes (e.g., name, school, major, etc.). The agents must chat with each other to find their unique mutual friend.
Source: Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"	https://paperswithcode.com/dataset/mutualfriends							
2258	MVOR	"Multi-View Operating Room (MVOR) is a dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter. 
Source: MVOR: A Multi-view RGB-D Operating Room Dataset for 2D and 3D Human Pose Estimation
Source: https://github.com/CAMMA-public/mvor
Image Source: https://github.com/CAMMA-public/mvor"	https://paperswithcode.com/dataset/mvor	24/08/2018	Multi-View Operating Room					
2259	MVS1K	"Contains about 1, 000 videos from 10 queries and their video tags, manual annotations, and associated web images.
Source: Query-Aware Sparse Coding for Multi-Video Summarization"	https://paperswithcode.com/dataset/mvs1k							
2260	MVSEC	"The Multi Vehicle Stereo Event Camera (MVSEC) dataset is a collection of data designed for the development of novel 3D perception algorithms for event based cameras. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images. 
Source: EV-FlowNet: Self-Supervised Optical Flow Estimation for Event-based Cameras"	https://paperswithcode.com/dataset/mvsec	30/01/2018	Multi Vehicle Stereo Event Camera					
2261	MWE-CWI	"Multiword expressions (MWEs) represent lexemes that should be treated as single lexical units due to their idiosyncratic nature. MWE-CWI is a dataset for MWE detection based on the Complex Word Identification Shared Task 2018 dataset.
Source: https://github.com/ekochmar/MWE-CWI"	https://paperswithcode.com/dataset/mwe-cwi							
2262	decaNLP	"Natural Language Decathlon Benchmark (decaNLP) is a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. The tasks as cast as question answering over a context.
Source: The Natural Language Decathlon: Multitask Learning as Question Answering
Image Source: http://decanlp.com/"	https://paperswithcode.com/dataset/decanlp		Natural Language Decathlon Benchmark					
2263	Nagoya University Extremely Low-resolution FIR Image Action Dataset	"A pedestrian dataset for Person Re-identification.
Source: Nagoya University Extremely Low-resolution FIR Image Action Dataset"	https://paperswithcode.com/dataset/nagoya-university-extremely-low-resolution							
2264	NAIST COVID	"NAIST COVID is a multilingual dataset of social media posts related to COVID-19, consisting of microblogs in English and Japanese from Twitter and those in Chinese from Weibo. The data cover microblogs from January 20, 2020, to March 24, 2020.
Source: NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset"	https://paperswithcode.com/dataset/naist-covid							
2265	NAS-Bench-101	"NAS-Bench-101 is the first public architecture dataset for NAS research. To build NASBench-101, the authors carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional
architectures. The authors trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the precomputed dataset. 
Source: NAS-Bench-101: Towards Reproducible Neural Architecture Search"	https://paperswithcode.com/dataset/nas-bench-101							
2266	NAS-Bench-1Shot1	"NAS-Bench-1Shot1 draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. 
Source: NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search"	https://paperswithcode.com/dataset/nas-bench-1shot1							
2267	NAS-Bench-201	"NAS-Bench-201 is a benchmark (and search space) for neural architecture search. Each architecture consists of a predefined skeleton with a stack of the searched cell. In this way, architecture search is transformed into the problem of searching a good cell.
Source: NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search"	https://paperswithcode.com/dataset/nas-bench-201							
2268	NatCat	"A general purpose text categorization dataset (NatCat) from three online resources: Wikipedia, Reddit, and Stack Exchange. These datasets consist of document-category pairs derived from manual curation that occurs naturally by their communities.
Source: Natcat: Weakly Supervised Text Classification with Naturally Annotated Datasets"	https://paperswithcode.com/dataset/natcat							
2269	NATS-Bench	"A unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. 
Source: NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size"	https://paperswithcode.com/dataset/nats-bench							
2270	Natural Stories	"The Natural Stories dataset consists of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data.
Source: The Natural Stories Corpus"	https://paperswithcode.com/dataset/natural-stories							
2271	NavigationNet	"NavigationNet is a computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation.
Source: NavigationNet: A Large-scale Interactive Indoor Navigation Dataset
Image Source: https://arxiv.org/pdf/1808.08374v1.pdf"	https://paperswithcode.com/dataset/navigationnet							
2272	N-CARS	"A large real-world event-based dataset for object classification.
Source: HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification"	https://paperswithcode.com/dataset/n-cars							
2273	NCBI Disease Corpus	"NCBI Disease Corpus is a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations, derived from an earlier corpus. The corpus contains rich annotations, was developed by a team of 12 annotators (two people per annotation) and covers all sentences in a PubMed abstract. Disease mentions are categorized into Specific Disease, Disease Class, Composite Mention and Modifier categories.
Source: An improved corpus of disease mentions in PubMed citations"	https://paperswithcode.com/dataset/ncbi-disease-corpus							
2274	NCD	"The Natural-Color Dataset (NCD) is an image colorization dataset where images are true to their colors. For example, a carrot will have an orange color in most images. Bananas will be either greenish or yellowish. It contains 723 images from the internet distributed in 20 categories. Each image has an object and a white background.
Source: https://github.com/saeed-anwar/ColorSurvey#dataset
Image Source: https://github.com/saeed-anwar/ColorSurvey"	https://paperswithcode.com/dataset/ncd		Natural-Color Dataset					
2275	NCLS	"Presents two high-quality large-scale CLS datasets based on existing monolingual summarization datasets.
Source: NCLS: Neural Cross-Lingual Summarization"	https://paperswithcode.com/dataset/ncls		Neural Cross-Lingual Summarization Corpora					
2276	NDD20	"Northumberland Dolphin Dataset 2020 (NDD20) is a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation.
Source: NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation
Image Source: Trotter et al"	https://paperswithcode.com/dataset/ndd20		Northumberland Dolphin Dataset 2020					
2277	N-Digit MNIST	N-Digit MNIST is a multi-digit MNIST-like dataset.	https://paperswithcode.com/dataset/n-digit-mnist							
2278	PSU NRTDB	"The PSU Near-Regular Texture Database is a texture dataset. It covers the spectrum of textures from completely regular to near-regular to irregular. It also includes video of near-regular textures in motion. The database also contains, or will include, test image sets with ground-truth for translation, rotation, reflection/glide-reflection symmetry detection algorithms.
Source: Computer Vision Online"	https://paperswithcode.com/dataset/psu-near-regular-texture-database		PSU Near-Regular Texture Database					
2279	NEEQ Annual Reports	"Business taxonomies automatically constructed from the content of corporate annual reports.
Source: Business Taxonomy Construction Using Concept-Level Hierarchical Clustering"	https://paperswithcode.com/dataset/neeq-annual-reports							
2280	Negotiation Dialogues Dataset	"This dataset consists of 5808 dialogues, based on 2236 unique scenarios. Each dialogue is converted into two training examples in the dataset, showing the complete conversation from the perspective of each agent. The perspectives differ on their input goals, output choice, and in special tokens marking whether a statement was read or written.
Source: Negotiation Dialogues Dataset"	https://paperswithcode.com/dataset/negotiation-dialogues-dataset							
2281	NERGRIT Corpus	"NERGRIT involves machine learning based NLP Tools and a corpus used for Indonesian Named Entity Recognition, Statement Extraction, and Sentiment Analysis. 
Source: NERGRIT Corpus"	https://paperswithcode.com/dataset/nergrit-corpus							
2282	NetiLook	"A large-scale clothing dataset named NetiLook to discover netizen-style comments.
Source: NetiLook"	https://paperswithcode.com/dataset/netilook							
2283	Neural Code Search Evaluation Dataset	"Neural-Code-Search-Evaluation-Dataset presents an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. 
Source: Neural Code Search Evaluation Dataset"	https://paperswithcode.com/dataset/neural-code-search-evaluation-dataset							
2284	Neural Conversational QA	"A modified data-set that has fewer spurious patterns than the original data-set, consequently allowing models to learn better.
Source: Neural Conversational QA: Learning to Reason vs Exploiting Patterns"	https://paperswithcode.com/dataset/neural-conversational-qa							
2285	PAF Benchmark	"Introduce three new neuromorphic vision datasets recorded by a novel neuromorphic vision sensor named Dynamic Vision Sensors (DVS).
Source: Neuromorphic Vision Datasets for Pedestrian Detection, Action Recognition, and Fall Detection"	https://paperswithcode.com/dataset/paf-benchmark							
2286	NewB	"A text corpus of more than 200,000 sentences from eleven news sources regarding Donald Trump.
Source: NewB: 200,000+ Sentences for Political Bias Detection"	https://paperswithcode.com/dataset/newb							
2287	New Brown Corpus	"A new dataset for training and evaluating grounded language models. 
Source: A Visuospatial Dataset for Naturalistic Verb Learning"	https://paperswithcode.com/dataset/new-brown-corpus							
2288	NewSHead	"The NewSHead dataset contains 369,940 English stories with 932,571 unique URLs, among which there are 359,940 stories for training, 5,000 for validation, and 5,000 for testing, respectively. Each news story contains at least three (and up to five) articles.
The dataset is collected from news stories published between May 2018 and May 2019, where a proprietary clustering algorithm iteratively loads articles published in a time window and groups them based on content similarity. Up to five representative articles are picked from the cluster for generating the story headline. Curators from a crowd-sourcing platform are requested to provide a headline of up to 35 characters to describe the major information covered by the story.
Source: NewSHead"	https://paperswithcode.com/dataset/newshead							
2289	Newspaper Navigator	"The largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model.
Source: The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content from 16 Million Historic Newspaper Pages in Chronicling America"	https://paperswithcode.com/dataset/newspaper-navigator							
2290	NewsPH-NLI	"NewsPH-NLI is a sentence entailment benchmark dataset in the low-resource Filipino language. 
Source: Investigating the True Performance of Transformers in Low-Resource Languages: A Case Study in Automatic Corpus Creation"	https://paperswithcode.com/dataset/newsph-nli							
2291	NIND	"An open dataset of real photographs with real noise, from identical scenes captured with varying ISO values.
Most images are taken with a Fujifilm X-T1 and XF18-55mm, other photographers are encouraged to contribute images for a more diverse crowdsourced effort.
Source: https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset
Image Source: https://commons.wikimedia.org/wiki/Natural_Image_Noise_Dataset"	https://paperswithcode.com/dataset/nind		Natural Image Noise Dataset					
2292	NLI-PT	"The first Portuguese dataset compiled for Native Language Identification (NLI), the task of identifying an author's first language based on their second language writing. The dataset includes 1,868 student essays written by learners of European Portuguese, native speakers of the following L1s: Chinese, English, Spanish, German, Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean, Romanian, and Swedish. NLI-PT includes the original student text and four different types of annotation: POS, fine-grained POS, constituency parses, and dependency parses. NLI-PT can be used not only in NLI but also in research on several topics in the field of Second Language Acquisition and educational NLP. 
Source: A Portuguese Native Language Identification Dataset"	https://paperswithcode.com/dataset/nli-pt							
2293	NLI-TR	"Natural Language Inference in Turkish (NLI-TR) provides translations of two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. 
Source: Data and Representation for Turkish Natural Language Inference
Image Source: https://arxiv.org/pdf/2004.14963.pdf"	https://paperswithcode.com/dataset/nli-tr		Natural Language Inference in Turkish					
2294	nocaps	"The nocaps benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets.
Source: nocaps: novel object captioning at scale
Image Source: https://nocaps.org/"	https://paperswithcode.com/dataset/nocaps							
2295	NoReC	"The Norwegian Review Corpus (NoReC) was created for the purpose of training and evaluating models for document-level sentiment analysis. More than 43,000 full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1–6, as provided by the rating of the original author.
Source: NoReC: The Norwegian Review Corpus"	https://paperswithcode.com/dataset/norec		Norwegian Review Corpus					
2296	NoReC_fine	"NoReC_fine is a dataset for fine-grained sentiment analysis in Norwegian, annotated with respect to polar expressions, targets and holders of opinion. 
Source: A Fine-Grained Sentiment Dataset for Norwegian"	https://paperswithcode.com/dataset/norec-fine							
2297	Noun-Noun Compound Dataset	"The noun–noun compounds dataset created by Fares (2016) consists of compounds annotated with two different taxonomies of relations; that is, for each noun–noun compound there are two distinct relations, drawing on different linguistic schools. The dataset was derived from existing linguistic resources, such as NomBank (Meyers et al., 2004) and the Prague Czech-English Dependency Treebank 2.0 (PCEDT; Hajič et al., 2012).
Source: Noun-Noun Compound Dataset"	https://paperswithcode.com/dataset/noun-ainoun-compound-dataset							
2298	NREC Agricultural Person-Detection	"A dataset to encourage research in these environments. It consists of labeled stereo video of people in orange and apple orchards taken from two perception platforms (a tractor and a pickup truck), along with vehicle position data from RTK GPS. 
Source: Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC Agricultural Person-Detection Dataset"	https://paperswithcode.com/dataset/nrec-agricultural-person-detection							
2299	NSMC	"This is a movie review dataset in the Korean language. Reviews were scraped from Naver Movies.
Source: NSMC"	https://paperswithcode.com/dataset/nsmc		Naver Sentiment Movie Corpus					
2300	NTPairs	"The NTPairs dataset consists of the pairs of news articles and their corresponding tweets that were published by eight media outlets in 2018. The eight outlets were selected to consider diverse outlets, which employ a different editing style for news sharing, in terms of publishing channels and political leaning.
Source: https://github.com/bywords/NTPairs
Image Source: https://github.com/bywords/NTPairs"	https://paperswithcode.com/dataset/ntpairs		News-Tweet Paired Dataset					
2301	Numeric Fused-Head	"The Numeric Fused-Head dataset consists of ~10K examples of crowd-sourced classified examples, labeled into 7 different categories, from two types. In the first type, Reference, the missing head is referenced explicitly somewhere else in the discourse, either in the same sentence or in surrounding sentences. In the second type, Implicit, the missing head does not appear in the text and needs to be inferred by the reader or hearer based on the context or world knowledge. This category was labeled into the 6 most common categories of the dataset. Models are evaluated based on accuracy.
Source: NLP Progress"	https://paperswithcode.com/dataset/numeric-fused-head							
2302	NumerSense	"Contains 13.6k masked-word-prediction probes, 10.5k for fine-tuning and 3.1k for testing.
Source: Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"	https://paperswithcode.com/dataset/numersense							
2303	NWPU-Crowd	"NWPU-Crowd consists of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0~20,033). 
Source: NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization"	https://paperswithcode.com/dataset/nwpu-crowd							
2304	NYC3DCars	"A vehicle detection database for vision tasks set in the real world.
Source: NYC3DCars"	https://paperswithcode.com/dataset/nyc3dcars							
2305	NYTWIT	"A collection of over 2,500 novel English words published in the New York Times between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). 
Source: NYTWIT: A Dataset of Novel Words in the New York Times"	https://paperswithcode.com/dataset/nytwit							
2306	NYU Symmetry Database	The NYU Symmetry database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files). Also included are a .m file to visualize the annotations on top of the images, and a .txt file with instructions on how to interpret the .mat annotations.	https://paperswithcode.com/dataset/nyu-symmetry-database							
2307	O4B	"O4B is a dataset of 17,458 open access business articles and their reference summaries. The dataset introduces a new challenge for summarization in the business domain, requiring highly abstractive and more concise summaries as compared to other existing datasets. 
Source: Open4Business(O4B): An Open Access Dataset for Summarizing Business Documents"	https://paperswithcode.com/dataset/o4b		Open4Business					
2308	OASIS	"A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images.
Source: OASIS: A Large-Scale Dataset for Single Image 3D in the Wild"	https://paperswithcode.com/dataset/oasis		Open Annotations of Single Image Surfaces					
2309	Objectron	"The Objectron dataset is a collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. In each video, the camera moves around the object, capturing it from different angles. The data also contain manually annotated 3D bounding boxes for each object, which describe the object’s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images in the following categories: bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes. To ensure geo-diversity, the dataset is collected from 10 countries across five continents.
Source: https://github.com/google-research-datasets/Objectron
Image Source: https://github.com/google-research-datasets/Objectron"	https://paperswithcode.com/dataset/objectron							
2310	Objects365	"Objects365 is a large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. 
Source: Objects365: A Large-Scale, High-Quality Dataset for Object Detection
Image Source: https://www.objects365.org/overview.html"	https://paperswithcode.com/dataset/objects365							
2311	OBP	"Open Bandit Dataset is a public real-world logged bandit feedback data. The dataset is provided by ZOZO, Inc., the largest Japanese fashion e-commerce company with over 5 billion USD market capitalization (as of May 2020). The company uses multi-armed bandit algorithms to recommend fashion items to users in a large-scale fashion e-commerce platform called ZOZOTOWN.
Source: OBP"	https://paperswithcode.com/dataset/obp		Open Bandit Dataset					
2312	Obstacle Tower	"Obstacle Tower is a high fidelity, 3D, 3rd person, procedurally generated environment for reinforcement learning. An agent playing Obstacle Tower must learn to solve both low-level control and high-level planning problems in tandem while learning from pixels and a sparse reward signal. Unlike other benchmarks such as the Arcade Learning Environment, evaluation of agent performance in Obstacle Tower is based on an agent’s ability to perform well on unseen instances of the environment.
Source: Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning"	https://paperswithcode.com/dataset/obstacle-tower							
2313	Occ-Traj120	"Occ-Traj120 is a trajectory dataset that contains occupancy representations of different local-maps with associated trajectories. This dataset contains 400 locally-structured maps with occupancy representation and roughly around 120K trajectories in total.
Source: https://github.com/soraxas/Occ-Traj120
Image Source: https://github.com/soraxas/Occ-Traj120"	https://paperswithcode.com/dataset/occ-traj120							
2314	OCR-VQA	"OCR-VQA dataset contains 207572 images and associated question-answer pairs.
Source: OCR-VQA"	https://paperswithcode.com/dataset/ocr-vqa							
2315	ODMS	"ODMS is a dataset for learning Object Depth via Motion and Segmentation. ODMS training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples in multiple domains, including robotics and driving.
Source: https://github.com/griffbr/ODMS
Image Source: https://github.com/griffbr/ODMS"	https://paperswithcode.com/dataset/odms		Object Depth via Motion and Segmentation					
2316	ODSQA	"The ODSQA dataset is a spoken dataset for question answering in Chinese. It contains more than three thousand questions from 20 different speakers.
Source: https://github.com/chiahsuan156/ODSQA"	https://paperswithcode.com/dataset/odsqa		Open-Domain Spoken Question Answering					
2317	OffComBR	"Offensive comments obtained from Brazilian website.
Source: OffComBR"	https://paperswithcode.com/dataset/offcombr		Offensive Comments in the Brazilian Web					
2318	Dataset of Structured Queries and Spatial Relations	"Provides 450, 000 relevance annotations and 53 structured queries.
Source: A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation"	https://paperswithcode.com/dataset/dataset-of-structured-queries-and-spatial							
2319	OGB	"The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner.
OGB is a community-driven initiative in active development.
Source: https://ogb.stanford.edu/
Image Source: https://ogb.stanford.edu/"	https://paperswithcode.com/dataset/ogb		Open Graph Benchmark					
2320	OGTD	"A manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. 
Source: Offensive Language Identification in Greek"	https://paperswithcode.com/dataset/ogtd		Offensive Greek Tweet Dataset					
2321	Oktoberfest Food Dataset	"A realistic, diverse, and challenging dataset for object detection on images. The data was recorded at a beer tent in Germany and consists of 15 different categories of food and drink items. 
Source: Oktoberfest Food Dataset"	https://paperswithcode.com/dataset/oktoberfest-food-dataset							
2322	Okutama-Action	"A new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors.
Source: Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection"	https://paperswithcode.com/dataset/okutama-action							
2323	OK-VQA	"Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer. 
Source: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge
Image Source: https://okvqa.allenai.org/"	https://paperswithcode.com/dataset/ok-vqa		Outside Knowledge Visual Question Answering					
2324	OmniArt	"Presents half a million samples and structured meta-data to encourage further research and societal engagement.
Source: OmniArt: Multi-task Deep Learning for Artistic Data Analysis"	https://paperswithcode.com/dataset/omniart							
2325	Omni-MOT	"The Omni-MOT is realistic CARLA based large-scale dataset with over 14M frames for multiple vehicle tracking . The dataset comprises 14M+ frames, 250K tracks, 110 million bounding boxes, three weather conditions, three crowd levels and three camera views in five simulated towns.
Source: Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking"	https://paperswithcode.com/dataset/omni-mot							
2326	One Million Posts Corpus	"An annotated data set consisting of user comments posted to an Austrian newspaper website (in German language).
DER STANDARD is an Austrian daily broadsheet newspaper. On the newspaper’s website, there is a discussion section below each news article where readers engage in online discussions. The data set contains a selection of user posts from the 12 month time span from 2015-06-01 to 2016-05-31. There are 11,773 labeled and 1,000,000 unlabeled posts in the data set. The labeled posts were annotated by professional forum moderators employed by the newspaper.
Source: One Million Posts Corpus"	https://paperswithcode.com/dataset/one-million-posts-corpus							
2327	OneStopEnglish	"Useful for through two applications - automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total).
Source: OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification"	https://paperswithcode.com/dataset/onestopenglish							
2328	OneStopQA	"OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.
Source: STARC: Structured Annotations for Reading Comprehension"	https://paperswithcode.com/dataset/onestopqa							
2329	OOVD	"This data set was created to understand the potential for machine learning, computer vision, and HPC to improve the energy efficiency aspects of traffic control by leveraging GRIDSMART traffic cameras as sensors for adaptive traffic control, with a sensitivity to the fuel consumption characteristics of the traffic in the camera’s visual field. GRIDSMART cameras—an existing, fielded commercial product—sense the presence of vehicles at intersections and replace more conventional sensors (such as inductive loops) to issue calls to traffic control. These cameras, which have horizon-to-horizon view, offer the potential for an improved view of the traffic environment which can be used to generate better control algorithms.
Source: OOVD"	https://paperswithcode.com/dataset/oovd		ORNL Overhead Vehicle Dataset					
2330	openDD	"Annotated using images taken by a drone in 501 separate flights, totalling in over 62 hours of trajectory data. As of today, openDD is by far the largest publicly available trajectory dataset recorded from a drone perspective, while comparable datasets span 17 hours at most.
Source: openDD: A Large-Scale Roundabout Drone Dataset"	https://paperswithcode.com/dataset/opendd							
2331	OpenDialKG	"OpenDialKG contains utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. 
Source: OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs"	https://paperswithcode.com/dataset/opendialkg							
2332	OpenEDS	"OpenEDS (Open Eye Dataset) is a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. 
Source: OpenEDS: Open Eye Dataset
Image Source: https://research.fb.com/programs/openeds-challenge"	https://paperswithcode.com/dataset/openeds							
2333	OpenEDS2020	"OpenEDS2020 is a dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display mounted with two synchronized eye-facing cameras. The dataset, which is anonymized to remove any personally identifiable information on participants, consists of 80 participants of varied appearance performing several gaze-elicited tasks, and is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560 sequences containing 550,400 eye-images and respective gaze vectors, created to foster research in spatio-temporal gaze estimation and prediction approaches; and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz, with up to 29,500 images, of which 5% contain a semantic segmentation label, devised to encourage the use of temporal information to propagate labels to contiguous frames. 
Source: OpenEDS2020: Open Eyes Dataset
Image Source: Palmero et al"	https://paperswithcode.com/dataset/openeds2020							
2334	OpenLORIS-object	"(L)ifel(O)ng (R)obotic V(IS)ion (OpenLORIS) - Object Recognition Dataset (OpenLORIS-Object) is designed for accelerating the lifelong/continual/incremental learning research and application，currently focusing on improving the continuous learning capability of the common objects in the home scenario.
Source: OpenLORIS-object"	https://paperswithcode.com/dataset/openloris-object	15/11/2019						
2335	Open MIC	"Open MIC (Open Museum Identification Challenge) contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Office 31.
Source: Museum Exhibit Identification Challenge for Domain Adaptation and Beyond"	https://paperswithcode.com/dataset/open-mic		Open Museum Identification Challenge					
2336	OpenSubtitles	OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.	https://paperswithcode.com/dataset/opensubtitles							
2337	OpenSurfaces	OpenSurfaces is a large database of annotated surfaces created from real-world consumer photographs. The  framework used for the annotation process draws on crowdsourcing to segment surfaces from photos, and then annotate them with rich surface properties, including material, texture and contextual information.	https://paperswithcode.com/dataset/opensurfaces							
2338	OpenViDial	"OpenViDial is a large-scale open-domain dialogue dataset with visual contexts. The dialogue turns and visual contexts are extracted from movies and TV series, where each dialogue turn is paired with the corresponding visual context in which it takes place. OpenViDial contains a total number of 1.1 million dialogue turns, and thus 1.1 million visual contexts stored in images.
Source: https://github.com/ShannonAI/OpenViDial
Image Source: https://github.com/ShannonAI/OpenViDial"	https://paperswithcode.com/dataset/openvidial							
2339	OPIEC	"OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time.
Source: OPIEC"	https://paperswithcode.com/dataset/opiec		Open Information Extraction Corpus					
2340	Opinosis	"This dataset contains sentences extracted from user reviews on a given topic. Example topics are “performance of Toyota Camry” and “sound quality of ipod nano”, etc. In total there are 51 such topics  with each topic having approximately 100 sentences (on average). The reviews were obtained from various sources – Tripadvisor (hotels), Edmunds.com (cars) and Amazon.com (various electronics).  This dataset was used for the following automatic text summarization project .
Source: Opinosis"	https://paperswithcode.com/dataset/opinosis							
2341	OPUS-100	"A novel multilingual dataset with 100 languages.
Source: Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation"	https://paperswithcode.com/dataset/opus-100							
2342	OrangeSum	"Source: BARThez: a Skilled Pretrained French Sequence-to-Sequence Model
OrangeSum is a single-document extreme summarization dataset with two tasks: title and abstract. Ground truth summaries are respectively 11.42 and 32.12 words in length on average, for the title and abstract tasks respectively, while document sizes are 315 and 350 words.
The motivation for OrangeSum was to put together a French equivalent of the XSum dataset.
Unlike the historical CNN, DailyMail, and NY Times datasets, OrangeSum requires the models to display a high degree of abstractivity to perform well.
OrangeSum was created by scraping articles and their titles and abstracts from the Orange Actu website.
Scraped pages cover almost a decade from Feb 2011 to Sep 2020, and belong to five main categories: France, world, politics, automotive, and society.
The society category is itself divided into 8 subcategories: health, environment, people, culture, media, high-tech, unsual (""insolite"" in French), and miscellaneous.
The dataset is publicly available at: https://github.com/Tixierae/OrangeSum."	https://paperswithcode.com/dataset/orangesum	23/10/2020						
2343	ORCAS	ORCAS is a click-based dataset. It covers 1.4 million of the TREC DL documents, providing 18 million connections to 10 million distinct queries.	https://paperswithcode.com/dataset/orcas							
2344	ORConvQA	"Enhances QuAC by adapting it to an open-retrieval setting. It is an aggregation of three existing datasets: (1) the QuAC dataset that offers information-seeking conversations, (2) the CANARD dataset that consists of context-independent rewrites of QuAC questions, and (3) the Wikipedia corpus that serves as the knowledge source of answering questions.
Source: ORConvQA"	https://paperswithcode.com/dataset/orconvqa		Open-Retrieval Conversational Question Answering					
2345	ORGaze	"A new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze.
Source: Object Referring in Videos with Language and Human Gaze"	https://paperswithcode.com/dataset/orgaze							
2346	ORKG-QA	"A preliminary dataset of related tables and a corresponding set of natural language questions.
Source: Question Answering on Scholarly Knowledge Graphs"	https://paperswithcode.com/dataset/orkg-qa							
2347	OTT-QA	"The Open Table-and-Text Question Answering (OTT-QA) dataset contains open questions which require retrieving tables and text from the web to answer. This dataset is re-annotated from the previous HybridQA dataset. The dataset is collected by UCSB NLP group and issued under MIT license.
Source: https://github.com/wenhuchen/OTT-QA
Image Source: https://github.com/wenhuchen/OTT-QA"	https://paperswithcode.com/dataset/ott-qa							
2348	Out the Window	"The Out the Window (OTW) dataset is a crowdsourced activity dataset containing 5,668 instances of 17 activities from the NIST Activities in Extended Video (ActEV) challenge. These videos are crowdsourced from workers on the Amazon Mechanical Turk using a novel scenario acting strategy, which collects multiple instances of natural activities per scenario. 
Source: Out the Window: A Crowd-Sourced Dataset for Activity Classification in Security Video"	https://paperswithcode.com/dataset/out-the-window							
2349	Oxford Radar RobotCar Dataset	"The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. It has been extended with data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset).
Source: The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset"	https://paperswithcode.com/dataset/oxford-radar-robotcar-dataset							
2350	Oxford RobotCar Dataset	"The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.
Source: Real-time Kinematic Ground Truth for the Oxford RobotCar Dataset"	https://paperswithcode.com/dataset/oxford-robotcar-dataset							
2351	OxUva	"OxUva is a dataset and benchmark for evaluating single-object tracking algorithms.
Source: Long-term Tracking in the Wild: A Benchmark"	https://paperswithcode.com/dataset/oxuva							
2352	PadChest	"PadChest is a labeled large-scale, high resolution chest x-ray dataset for the automated exploration
of medical images along with their associated reports. This dataset includes more than 160,000
images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital
San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional
information on image acquisition and patient demography. The reports were labeled with 174 different
radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical
taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of
these reports, 27% were manually annotated by trained physicians and the remaining set was labeled
using a supervised method based on a recurrent neural network with attention mechanisms. The labels
generated were then validated in an independent test set achieving a 0.93 Micro-F1 score."	https://paperswithcode.com/dataset/padchest							
2353	PA-HMDB51	"The Privacy Annotated HMDB51 (PA-HMDB51) dataset is a video-based dataset for evaluating pirvacy protection in visual action recognition algorithms. The dataset contains both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis.
Source: https://github.com/VITA-Group/PA-HMDB51"	https://paperswithcode.com/dataset/pa-hmdb51		Privacy Annotated HMDB51					
2354	PANDA	"PANDA is the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions.
Source: PANDA: A Gigapixel-level Human-centric Video Dataset"	https://paperswithcode.com/dataset/panda							
2355	PanNuke	"PanNuke is a semi automatically generated nuclei instance segmentation and classification dataset with exhaustive nuclei labels across 19 different tissue types. The dataset consists of 481 visual fields, of which 312 are randomly sampled from more than 20K whole slide images at different magnifications, from multiple data sources. In total the dataset contains 205,343 labeled nuclei, each with an instance segmentation mask. 
Source: PanNuke Dataset Extension, Insights and Baselines
Image Source: https://jgamper.github.io/PanNukeDataset/"	https://paperswithcode.com/dataset/pannuke							
2356	Spherical-Navi	"A novel 360◦ fisheye panoramas dataset, i.e., the Spherical-Navi image dataset is collected, with a unique labeling strategy enabling automatic generation of an arbitrary number of negative samples (wrong heading direction).
Source: Convolutional Neural Network-Based Robot Navigation Using Uncalibrated Spherical Images"	https://paperswithcode.com/dataset/panonavi-dataset							
2357	ParaBank	"A large-scale English paraphrase dataset that surpasses prior work in both quantity and quality.
Source: ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Translation"	https://paperswithcode.com/dataset/parabank							
2358	PARADE	"PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge.
Source: PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge"	https://paperswithcode.com/dataset/parade							
2359	Parallel Meaning Bank	"The Parallel Meaning Bank (PMB), developed at the University of Groningen and building upon the Groningen Meaning Bank, comprises sentences and texts in raw and tokenised format, syntactic analysis, word senses, thematic roles, reference resolution, and formal meaning representations. The main objective of the PMB is to provide fine-grained meaning representations for words, sentences and texts. Sentences are, in isolation, often ambiguous. The aim is to provide the most likely interpretation for a sentence, with a minimal use of underspecification.
The PMB annotations include gold standard data, which is fully manually corrected, as well as silver (partially manually corrected) and bronze (with no manual corrections) data. The releases so far contain documents for English, German, Italian and Dutch, but for future releases it is planned to include Chinese and Japanese.
Source: The Parallel Meaning Bank: A Framework for Semantically Annotating Multiple Languages"	https://paperswithcode.com/dataset/parallel-meaning-bank							
2360	Bilingual Corpus of Arabic-English Parallel Tweets	"A bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabic tweets regularly.
Source: Constructing a Bilingual Corpus of Parallel Tweets"	https://paperswithcode.com/dataset/bilingual-corpus-of-arabic-english-parallel							
2361	PARANMT-50M	"PARANMT-50M is a dataset for training paraphrastic sentence embeddings. It consists of more than 50 million English-English sentential paraphrase pairs. 
Source: ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations"	https://paperswithcode.com/dataset/paranmt-50m							
2362	ParaPat	"A parallel corpus from the open access Google Patents dataset in 74 language pairs, comprising more than 68 million sentences and 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm for the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned.
Source: ParaPat: The Multi-Million Sentences Parallel Corpus of Patents Abstracts"	https://paperswithcode.com/dataset/parapat		Parallel Corpus of Patents Abstracts					
2363	ParCorFull	"ParCorFull is a parallel corpus annotated with full coreference chains that has been created to address an important problem that machine translation and other multilingual natural language processing (NLP) technologies face -- translation of coreference across languages. This corpus contains parallel texts for the language pair English-German, two major European languages. Despite being typologically very close, these languages still have systemic differences in the realisation of coreference, and thus pose problems for multilingual coreference resolution and machine translation. This parallel corpus covers the genres of planned speech (public lectures) and newswire. It is richly annotated for coreference in both languages, including annotation of both nominal coreference and reference to antecedents expressed as clauses, sentences and verb phrases.
Source: ParCorFull: a Parallel Corpus Annotated with Full Coreference"	https://paperswithcode.com/dataset/parcorfull		Parallel Corpus Annotated with Full Coreference					
2364	Paris Art Deco Facades	"Anew dataset of facade images from Paris following the Art-deco style.
Source: Learning grammars for architecture-specific facade parsing"	https://paperswithcode.com/dataset/paris-art-deco-facades							
2365	Paris-Lille-3D	"The Paris-Lille-3D is a Benchmark on Point Cloud Classification. The Point Cloud has been labeled entirely by hand with 50 different classes. The dataset consists of around 2km of Mobile Laser System point cloud acquired in two cities in France (Paris and Lille).
Source: Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification
Image Source: https://npm3d.fr/paris-lille-3d"	https://paperswithcode.com/dataset/paris-lille-3d							
2366	Parkinson's Pose Estimation Dataset	"The data includes all movement trajectories extracted from the videos of Parkinson's assessments using Convolutional Pose Machines (CPM) as well as the confidence values from CPM. The dataset also includes ground truth ratings of parkinsonism and dyskinesia severity using the UDysRS, UPDRS, and CAPSIT.
Source: https://github.com/limi44/Parkinson-s-Pose-Estimation-Dataset"	https://paperswithcode.com/dataset/parkinson-s-pose-estimation-dataset							
2367	Pars-ABSA	"Pars-ABSA is a manually annotated Persian dataset, Pars-ABSA, which is verified by 3 native Persian speakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827 neutral data samples from 5,602 unique reviews.
Source: Pars-ABSA: an Aspect-based Sentiment Analysis dataset for Persian"	https://paperswithcode.com/dataset/pars-absa							
2368	PartNet	"PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others.
Source: PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding
Image Source: https://cs.stanford.edu/~kaichun/partnet/"	https://paperswithcode.com/dataset/partnet							
2369	PathTrack	"PathTrack is a dataset for person tracking which contains more than 15,000 person trajectories in 720 sequences.
Source: PathTrack: Fast Trajectory Annotation with Path Supervision"	https://paperswithcode.com/dataset/pathtrack							
2370	PathVQA	"PathVQA consists of 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness.
Source: PathVQA: 30000+ Questions for Medical Visual Question Answering"	https://paperswithcode.com/dataset/pathvqa							
2371	PCDS	"Contains over 4,500 videos recorded at the entrance doors of buses in normal and cluttered conditions. It also proposes an efficient method for counting people in real-world cluttered scenes related to public transportations using depth videos. 
Source: Benchmark data and method for real-time people counting in cluttered scenes using depth sensors"	https://paperswithcode.com/dataset/pcds		People Counting Dataset					
2372	P-DESTRE	"Provides consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions.
Source: The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking, Re-Identification and Search from Aerial Devices"	https://paperswithcode.com/dataset/p-destre							
2373	PEC	"A novel large-scale multi-domain dataset for persona-based empathetic conversations. 
Source: Towards Persona-Based Empathetic Conversational Models"	https://paperswithcode.com/dataset/pec		Persona-Based Empathetic Conversational					
2374	PedX	"PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors.
Source: pedx.io"	https://paperswithcode.com/dataset/pedx							
2375	People Snapshot Dataset	"Enables detailed human body model reconstruction in clothing from a single monocular RGB video without requiring a pre scanned template or manually clicked points.
Source: Video Based Reconstruction of 3D People Models"	https://paperswithcode.com/dataset/people-snapshot-dataset							
2376	PerKey	"A corpus of 553k news articles from six Persian news websites and agencies with relatively high quality author extracted keyphrases, which is then filtered and cleaned to achieve higher quality keyphrases. 
Source: PerKey: A Persian News Corpus for Keyphrase Extraction and Generation"	https://paperswithcode.com/dataset/perkey							
2377	Perlex	"Persian dataset for relation extraction, which is an expert-translated version of the ""Semeval-2010-Task-8"" dataset. 
Source: PERLEX: A Bilingual Persian-English Gold Dataset for Relation Extraction"	https://paperswithcode.com/dataset/perlex							
2378	Permuted bAbI dialog task	"The Permuted bAbi dialog task is an adaptation of the ""Dialog bAbI tasks data"" dataset released by Facebook. It is used for evaluating end-to-end dialog systems in the restaurant domain. This dataset introduces multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems in a more realistic setting.
Source: https://github.com/IBM/permuted-bAbI-dialog-tasks"	https://paperswithcode.com/dataset/permuted-babi-dialog-task							
2379	PerSenT	"PerSenT is a dataset of crowd-sourced annotations of the sentiment expressed by the authors towards the main entities in news articles. The dataset also includes paragraph-level sentiment annotations to provide more fine-grained supervision for the task. 
Source: Author's Sentiment Prediction"	https://paperswithcode.com/dataset/persent							
2380	Perspectrum	"Perspectrum is a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify the dataset. Crowd-sourcing was used to filter out noise and ensure high-quality data. The dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively.
Source: Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims"	https://paperswithcode.com/dataset/perspectrum							
2381	PEYMA	Peyma is a Persian NER dataset to train and test NER systems. It is constructed by collecting documents from ten news websites.	https://paperswithcode.com/dataset/peyma							
2382	PFN-PIC	"This dataset is a collection of spoken language instructions for a robotic system to pick and place common objects. Text instructions and corresponding object images are provided.
The dataset consists of situations where the robot is instructed by the operator to pick up a specific object and move it to another location: for example, Move the blue and white tissue box to the top right bin.
This dataset consists of RGBD images, bounding box annotations, destination box annotations, and text instructions.
Source: https://github.com/pfnet-research/picking-instruction
Image Source: https://github.com/pfnet-research/picking-instruction"	https://paperswithcode.com/dataset/pfn-pic		PFN Picking Instructions for Commodities Dataset					
2383	PG-19	"A new open-vocabulary language modelling benchmark derived from books.
Source: Compressive Transformers for Long-Range Sequence Modelling"	https://paperswithcode.com/dataset/pg-19							
2384	PGR	"Phenotype-Gene Relations (PGR) is a corpus that consists of 1712 abstracts, 5676 human phenotype annotations, 13835 gene annotations, and 4283 relations. 
Source: A Silver Standard Corpus of Human Phenotype-Gene Relations"	https://paperswithcode.com/dataset/pgr		Phenotype-Gene Relations					
2385	PheMT	"PheMT is a phenomenon-wise dataset designed for evaluating the robustness of Japanese-English machine translation systems. The dataset is based on the MTNT dataset, with additional annotations of four linguistic phenomena common in UGC; Proper Noun, Abbreviated Noun, Colloquial Expression, and Variant
Source: https://github.com/cl-tohoku/PheMT
Image Source: Fujii et al"	https://paperswithcode.com/dataset/phemt							
2386	PHINC	"PHINC is a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators. 
Source: PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation"	https://paperswithcode.com/dataset/phinc							
2387	Photi-LakeIce	"A new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations. 
Source: Lake Ice Monitoring with Webcams and Crowd-Sourced Images"	https://paperswithcode.com/dataset/photi-lakeice							
2388	PhotoBook	"A large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation.
Source: The PhotoBook Dataset: Building Common Ground through Visually-Grounded Dialogue"	https://paperswithcode.com/dataset/photobook							
2389	Photographic Defect Severity	"A large-scale dataset of user annotations on seven common photographic defects.
Source: Learning to Detect Multiple Photographic Defects"	https://paperswithcode.com/dataset/photographic-defect-severity							
2390	Photoswitch	"A benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths.
Source: The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the Advancement of Synthetic Chemistry"	https://paperswithcode.com/dataset/photoswitch							
2391	PhotoSynth	"The PhotoSynth (PS) dataset for patch matching consists of a total of 30 scenes with 25 scenes for training and 5 scenes for validation. The different image pairs are captured in different illumination conditions, at different scales and with different viewpoints.
Source: https://arxiv.org/abs/1801.01466
Image Source: https://github.com/rmitra/PS-Dataset"	https://paperswithcode.com/dataset/photosynth							
2392	PhraseCut	"PhraseCut is a dataset consisting of 77,262 images and 345,486 phrase-region pairs. The dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated.
Source: PhraseCut: Language-based Image Segmentation in the Wild
Image Source: https://people.cs.umass.edu/~chenyun/publication/phrasecut/"	https://paperswithcode.com/dataset/phrasecut							
2393	PHYRE	"Benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. 
Source: PHYRE: A New Benchmark for Physical Reasoning"	https://paperswithcode.com/dataset/phyre		PHYsical REasoning					
2394	pic2kcal	"The pic2kal benchmark for calorie prediction contains 308,000 images from over 70,000 recipes including photographs, ingredients and instructions, matched with nutritional information.
Source: https://arxiv.org/abs/2011.01082
Image Source: https://github.com/phiresky/pic2kcal"	https://paperswithcode.com/dataset/pic2kcal							
2395	PicTropes	"PicTropes is a dataset of films and the tropes that they use created from the database DBTropes.org.
Source: Overview of PicTropes, a film trope dataset"	https://paperswithcode.com/dataset/pictropes							
2396	Pinterest Complete The Look	"The Pinterest Complete the Look dataset consists of over 1 million outfits and 4 million objects. It can be used to predict style compatibility between fashion items in order to recommend complementary items that complete an outfit.
Source: https://arxiv.org/abs/2006.10792
Image Source: https://github.com/eileenforwhat/complete-the-look-dataset"	https://paperswithcode.com/dataset/pinterest-complete-the-look							
2397	pioNER	"The pioNER corpus provides gold-standard and automatically generated named-entity datasets for the Armenian language.
The automatically generated corpus is generated from Wikipedia. The gold-standard set is a collection of over 250 news articles from iLur.am with manual named-entity annotation. It includes sentences from political, sports, local and world news, and is comparable in size with the test sets of other languages.
Source: https://github.com/ispras-texterra/pioner"	https://paperswithcode.com/dataset/pioner							
2398	PIRM	The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution.	https://paperswithcode.com/dataset/pirm		Perceptual Image Restoration and Manipulation					
2399	PIT	"Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. 
Source: SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)"	https://paperswithcode.com/dataset/pit		Paraphrase and Semantic Similarity in Twitter					
2400	Plaintext Jokes	"There are about 208 000 jokes in this database scraped from three sources.
Source: Plaintext Jokes"	https://paperswithcode.com/dataset/plaintext-jokes							
2401	Planar Manipulator Dataset	"The dataset consists of 90 000 color videos that show a planar robot manipulator executing articulated manipulation tasks. More precisely, the manipulator grasps a circular object of random color and size and places it on top of a square object/platform of again random color and size. The initial conﬁgurations (location, size and color) of the objects were randomly sampled during generation. Different from other datasets such as the moving MNIST dataset, the samples comprise a goal-oriented task as described, making it more suitable for testing prediction capabilities of an ML model. For instance, one can use it as a toy dataset to investigate the capacity and output behavior of a deep neural network before testing it on real-world data.
Source: https://github.com/ferreirafabio/PlanarManipulatorDataset
Image Source: https://github.com/ferreirafabio/PlanarManipulatorDataset"	https://paperswithcode.com/dataset/planar-manipulator-dataset							
2402	PlantDoc	"PlantDoc is a dataset for visual plant disease detection. The dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images.
Source: PlantDoc: A Dataset for Visual Plant Disease Detection"	https://paperswithcode.com/dataset/plantdoc							
2403	Plant Seedlings Dataset	"A database of images of approximately 960 unique plants belonging to 12 species at several growth stages is made publicly available. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.
Source: A Public Image Database for Benchmark of Plant Seedling Classification Algorithms"	https://paperswithcode.com/dataset/plant-seedlings-dataset							
2404	PMC-SA	"PMC-SA (PMC Structured Abstracts) is a dataset of academic publications, used for the task of structured summarization.
Source: https://arxiv.org/abs/1905.07695"	https://paperswithcode.com/dataset/pmc-sa		PMC Structured Abstracts					
2405	PMIndia	"Consists of parallel sentences which pair 13 major languages of India with English. The corpus includes up to 56000 sentences for each language pair.
Source: PMIndia -- A Collection of Parallel Corpora of Languages of India"	https://paperswithcode.com/dataset/pmindia							
2406	PMLB	"The Penn Machine Learning Benchmarks (PMLB) is a large, curated set of benchmark datasets used to evaluate and compare supervised machine learning algorithms. These datasets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features.
Source: https://arxiv.org/abs/1703.00512"	https://paperswithcode.com/dataset/pmlb		Penn Machine Learning Benchmarks					
2407	pn-summary	"Pn-summary is a dataset for Persian abstractive text summarization.
Source: https://arxiv.org/abs/2012.11204
Image Source: https://github.com/hooshvare/pn-summary"	https://paperswithcode.com/dataset/pn-summary							
2408	PoC	"A dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. The dataset bridges the gap between coreference resolution and summarization.
Source: Understanding Points of Correspondence between Sentences for Abstractive Summarization"	https://paperswithcode.com/dataset/poc		Points of correspondence					
2409	PointDenoisingBenchmark	"The PointDenoisingBenchmark dataset features 28 different shapes, split into 18 training shapes and 10 test shapes.

PointDenoisingBenchmark for outliers removal: contains noisy point clouds with different levels of gaussian noise and the corresponding clean ground truths.
PointDenoisingBenchmark for denoising: contains noisy point clouds with different levels of noise and density of outliers and the corresponding clean ground truths.

Source: PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point Clouds"	https://paperswithcode.com/dataset/pointcleannet							
2410	PoKi	"PoKi is a corpus of 61,330 poems written by children from grades 1 to 12. PoKi is especially useful in studying child language because it comes with information about the age of the child authors (their grade).
Source: https://github.com/whipson/PoKi-Poems-by-Kids"	https://paperswithcode.com/dataset/poki							
2411	PolEmo 2.0	"PolEmo 2.0: Corpus of Multi-Domain Consumer Reviews, evaluation data for article presented at CoNLL.
Source: PolEmo 2.0 Sentiment Analysis Dataset for CoNLL"	https://paperswithcode.com/dataset/polemo-2-0							
2412	PolicyQA	"A dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices.
Source: PolicyQA: A Reading Comprehension Dataset for Privacy Policies"	https://paperswithcode.com/dataset/policyqa							
2413	Polish Political Advertising Dataset	"A dataset for detecting specific text chunks and categories of political advertising in the Polish language. It contains 1,705 human-annotated tweets tagged with nine categories, which constitute campaigning under Polish electoral law.
Source: Political Advertising Dataset: the use case of the Polish 2020 Presidential Elections"	https://paperswithcode.com/dataset/polish-political-advertising-dataset							
2414	PolitiFact	"Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from politifact.com.
Source: Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News"	https://paperswithcode.com/dataset/politifact							
2415	PolSF	"Collects five open polarimetric SAR images, which are images of the San Francisco area. These five images come from different satellites at different times, which has great scientific research value. 
Source: PolSF: PolSAR image dataset on San Francisco"	https://paperswithcode.com/dataset/polsf							
2416	POLUSA	"A dataset that represents the online media landscape as perceived by an average US news consumer. The dataset contains 0.9M articles covering policy topics published between Jan. 2017 and Aug. 2019 by 18 news outlets representing the political spectrum. Each outlet is labeled by its political leaning derived using a systematic aggregation of eight data sources. The news dataset is balanced with respect to publication date and outlet popularity. POLUSA enables studying a variety of subjects, e.g., media effects and political partisanship.
Source: The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and Outlet Popularity"	https://paperswithcode.com/dataset/polusa							
2417	Polyglot-NER	"Polyglot-NER builds massive multilingual annotators with minimal human expertise and intervention.
Source: POLYGLOT-NER: Massive Multilingual Named Entity Recognition"	https://paperswithcode.com/dataset/polyglot-ner							
2418	PoMo	"PoMo consists of more than 231K sentences with post-modifiers and associated facts extracted from Wikidata for around 57K unique entities.
Source: PoMo: Generating Entity-Specific Post-Modifiers in Context"	https://paperswithcode.com/dataset/pomo							
2419	Pow-Wow	"A dataset for studying situated goal-directed human communication.
Source: Pow-Wow: A Dataset and Study on Collaborative Communication in Pommerman"	https://paperswithcode.com/dataset/pow-wow							
2420	prachathai-67k	"The prachathai-67k dataset was scraped from the news site Prachathai excluding articles with less than 500 characters of body text (mostly images and cartoons). It contains 67,889 articles with 51,797 tags from August 24, 2004 to November 15, 2018.
Source: prachathai-67k"	https://paperswithcode.com/dataset/prachathai-67k							
2421	PreCo	"A large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. 
Source: PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution"	https://paperswithcode.com/dataset/preco							
2422	PRECOG	The PREdiction of Clinical Outcomes from Genomic profiles (or PRECOG) encompasses 166 cancer expression data sets, including overall survival data for ~18,000 patients diagnosed with 39 distinct malignancies.	https://paperswithcode.com/dataset/precog		PREdiction of Clinical Outcomes from Genomic Profiles					
2423	PRED18	"Twenty DAVIS recordings with a total duration of about 1.25 hour were obtained by driving the two robots in the robot arena of the University of Ulster in Londonderry.
Source: Steering a Predator Robot using a Mixed Frame/Event-Driven Convolutional Neural Network"	https://paperswithcode.com/dataset/pred18		PRED18: Predator/Prey DAVIS Dataset					
2424	PreSIL	"Consists of over 50,000 frames and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), and detailed annotations for all vehicles and people. 
Source: Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle Perception"	https://paperswithcode.com/dataset/presil		Precise Synthetic Image and LiDAR					
2425	PressurePose	"A synthetic dataset with 206K pressure images with 3D human poses and shapes.
Source: Bodies at Rest: 3D Human Pose and Shape Estimation from a Pressure Image using Synthetic Data"	https://paperswithcode.com/dataset/pressurepose							
2426	Procedural Human Action Videos	"Procedural Human Action Videos contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. 
Source: Procedural Generation of Videos to Train Deep Action Recognition Networks"	https://paperswithcode.com/dataset/procedural-human-action-videos							
2427	Procon20	"A novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. 
Source: Stance Prediction for Contemporary Issues: Data and Experiments"	https://paperswithcode.com/dataset/procon20							
2428	Products-10K	"Contains 10,000 fine-grained SKU-level products frequently bought by online customers in JD.com.
Source: Products-10K: A Large-scale Product Recognition Dataset"	https://paperswithcode.com/dataset/products-10k							
2429	Proposal Flow Datasets	"Dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. 
Source: Proposal Flow"	https://paperswithcode.com/dataset/proposal-flow-datasets							
2430	Prostate MRI Segmentation Dataset	"This prostate MRI segmentation dataset is collected from six different data sources.
Source: https://github.com/liuquande/SAML"	https://paperswithcode.com/dataset/prostate-mri-segmentation-dataset							
2431	ProtoQA	"ProtoQA is a question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show FAMILY- FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers.
Source: https://github.com/iesl/protoqa-data"	https://paperswithcode.com/dataset/protoqa							
2432	Proto Summ	"This is a large-scale court judgment dataset, where each judgment is a summary of the case description with a patternized style. It contains 2,003,390 court judgment documents. The case description is used as the input, and the court judgment as the summary. The average lengths of the input documents and summaries are 595.15 words and 273.57 words respectively.
Source: https://arxiv.org/pdf/1909.08837.pdf"	https://paperswithcode.com/dataset/proto-summ							
2433	PROX	"A dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. 
Source: PROX"	https://paperswithcode.com/dataset/prox							
2434	PS-Battles	"The PS-Battles dataset is gathered from a large community of image manipulation enthusiasts and provides a basis for media derivation and manipulation detection in the visual domain. The dataset consists of 102'028 images grouped into 11'142 subsets, each containing the original image as well as a varying number of manipulated derivatives.
Source: The PS-Battles Dataset - an Image Collection for Image Manipulation Detection
Image Source: Heller et al"	https://paperswithcode.com/dataset/ps-battles							
2435	PST900	"PST900 is a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge.
Source: https://arxiv.org/abs/1909.10980
Image Source: https://github.com/ShreyasSkandanS/pst900_thermal_rgb"	https://paperswithcode.com/dataset/pst900							
2436	PTB-TIR	"PTB-TIR is a Thermal InfraRed (TIR) pedestrian tracking benchmark, which provides 60  TIR sequences with mannuly annoations.  The benchmark is used to fair evaluate TIR trackers.
Source: PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark"	https://paperswithcode.com/dataset/ptb-tir							
2437	PTL	"A dataset of pedestrian traffic lights containing over 5000 photos taken at hundreds of intersections in Shanghai.
Source: LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic Lights and Zebra Crossing Recognition for the Visually Impaired"	https://paperswithcode.com/dataset/ptl		Pedestrian-Traffic-Lights					
2438	PubFig	"The PubFig database is a large, real-world face dataset consisting of 58,797 images of 200 people collected from the internet. Unlike most other existing face datasets, these images are taken in completely uncontrolled situations with non-cooperative subjects. Thus, there is large variation in pose, lighting, expression, scene, camera, imaging conditions and parameters, etc. The PubFig dataset is similar in spirit to the Labeled Faces in the Wild (LFW) dataset.
Source: PubFig: Public Figures Face Database"	https://paperswithcode.com/dataset/pubfig		Public Figures Face Database					
2439	PUBHEALTH	"PUBHEALTH is a comprehensive dataset for explainable automated fact-checking of public health claims. Each instance in the PUBHEALTH dataset has an associated veracity label (true, false, unproven, mixture). Furthermore each instance in the dataset has an explanation text field. The explanation is a justification for which the claim has been assigned a particular veracity label.
Source: https://github.com/neemakot/Health-Fact-Checking"	https://paperswithcode.com/dataset/pubhealth							
2440	PubLayNet	"PubLayNet is a dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated.
Source: PubLayNet: largest dataset ever for document layout analysis"	https://paperswithcode.com/dataset/publaynet							
2441	public_meetings	"The public_meetings corpus contains meetings, made of pairs of automatic transcriptions from audio recordings and meeting reports written by a professional. 22 aligned meetings are provided in total.
Source: public_meetings"	https://paperswithcode.com/dataset/public-meetings							
2442	Pump and dump dataset	"The Pump and dump dataset is an annotated set of messages to detect cryptocurrency market manipulations. It consists of a list of a list of pump and dumps arranged by groups on Telegram. All the pump and dumps in the dataset are on the trading pair SYM/BTC.
Source: https://github.com/SystemsLab-Sapienza/pump-and-dump-dataset"	https://paperswithcode.com/dataset/pump-and-dump-dataset							
2443	QBSUM	"A high-quality large-scale dataset consisting of 49,000+ data samples for the task of Chinese query-based document summarization. 
Source: QBSUM: a Large-Scale Query-Based Document Summarization Dataset from Real-world Applications"	https://paperswithcode.com/dataset/qbsum							
2444	QMUL-SurvFace	"QMUL-SurvFace is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time.
Source: Surveillance Face Recognition Challenge"	https://paperswithcode.com/dataset/qmul-survface							
2445	Q-Traffic	"Q-Traffic is a large-scale traffic prediction dataset, which consists of three sub-datasets: query sub-dataset, traffic speed sub-dataset and road network sub-dataset.
Source: https://github.com/JingqingZ/BaiduTraffic
Image Source: https://github.com/JingqingZ/BaiduTraffic"	https://paperswithcode.com/dataset/q-traffic							
2446	QuAIL	"A new kind of question-answering dataset that combines commonsense, text-based, and unanswerable questions, balanced for different genres and reasoning types. Reasoning type annotation for 9 types of reasoning: temporal, causality, factoid, coreference, character properties, their belief states, subsequent entity states, event durations, and unanswerable. Genres: CC license fiction, Voice of America news, blogs, user stories from Quora 800 texts, 18 questions for each (~14K questions).
Source: QuAIL - Question Answering for Artificial Intelligence"	https://paperswithcode.com/dataset/quail		Question Answering for Artificial Intelligence					
2447	Quda	"Aims to help V-NLIs recognize analytic tasks from free-form natural language by training and evaluating cutting-edge multi-label classification models. The dataset contains  diverse user queries, and each is annotated with one or multiple analytic tasks. 
Source: Quda: Natural Language Queries for Visual Data Analytics"	https://paperswithcode.com/dataset/quda							
2448	QuerYD	"A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content.
Source: QuerYD: A video dataset with high-quality textual and audio narrations"	https://paperswithcode.com/dataset/queryd							
2449	Query-Focused Video Summarization Dataset	"Collects dense per-video-shot concept annotations.
Source: Query-Focused Video Summarization: Dataset, Evaluation, and A Memory Network Based Approach"	https://paperswithcode.com/dataset/query-focused-video-summarization-dataset							
2450	Quick, Draw! Dataset	"The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.
Source: https://github.com/googlecreativelab/quickdraw-dataset
Image Source: https://github.com/googlecreativelab/quickdraw-dataset"	https://paperswithcode.com/dataset/quick-draw-dataset							
2451	QuickDraw-Extended	"Consists of 330,000 sketches and 204,000 photos spanning across 110 categories.
Source: Doodle to Search: Practical Zero-Shot Sketch-based Image Retrieval"	https://paperswithcode.com/dataset/quickdraw-extended							
2452	Quizbowl	"Consists of multiple sentences whose clues are arranged by difficulty (from obscure to obvious) and uniquely identify a well-known entity such as those found on Wikipedia.
Source: Quizbowl: The Case for Incremental Question Answering"	https://paperswithcode.com/dataset/quizbowl							
2453	Qulac	"A dataset on asking Questions for Lack of Clarity in open-domain information-seeking conversations. Qulac presents the first dataset and offline evaluation framework for studying clarifying questions in open-domain information-seeking conversational search systems.
Source: https://github.com/aliannejadi/qulac
Image Source: https://github.com/aliannejadi/qulac"	https://paperswithcode.com/dataset/qulac							
2454	RAD	"The dataset is useful for query-adaptive video summarization and annotated with diversity and query-specific relevance labels. 
Source: Query-adaptive Video Summarization via Quality-aware Relevance Estimation"	https://paperswithcode.com/dataset/rad		RELEVANCE AND DIVERSITY DATASET					
2455	RADIATE	"RADIATE (RAdar Dataset In Adverse weaThEr) is new automotive dataset created by Heriot-Watt University which includes Radar, Lidar, Stereo Camera and GPS/IMU.
The data is collected in different weather scenarios (sunny, overcast, night, fog, rain and snow) to help the research community to develop new methods of vehicle perception.
The radar images are annotated in 7 different scenarios: Sunny (Parked), Sunny/Overcast (Urban), Overcast (Motorway), Night (Motorway), Rain (Suburban), Fog (Suburban) and Snow (Suburban). The dataset contains 8 different types of objects (car, van, truck, bus, motorbike, bicycle, pedestrian and group of pedestrians).
Source: https://github.com/marcelsheeny/radiate_sdk
Image Source: https://github.com/marcelsheeny/radiate_sdk"	https://paperswithcode.com/dataset/radiate		RAdar Dataset In Adverse weaThEr					
2456	RadioTalk	"RadioTalk is a corpus of speech recognition transcripts sampled from talk radio broadcasts in the United States between October of 2018 and March of 2019. The corpus is intended for use by researchers in the fields of natural language processing, conversational analysis, and the social sciences. The corpus encompasses approximately 2.8 billion words of automatically transcribed speech from 284,000 hours of radio, together with metadata about the speech, such as geographical location, speaker turn boundaries, gender, and radio program information.
Source: https://github.com/social-machines/RadioTalk"	https://paperswithcode.com/dataset/radiotalk							
2457	RAF-ML	"Real-world Affective Faces Multi Label (RAF-ML) is a multi-label facial expression dataset with around 5K great-diverse facial images downloaded from the Internet with blended emotions and variability in subjects' identity, head poses, lighting conditions and occlusions. During annotation, 315 well-trained annotators are employed to ensure each image can be annotated enough independent times. And images with multi-peak label distribution are selected out to constitute the RAF-ML.
RAF-ML provides 4908 number of real-world images with blended emotions, 6-dimensional expression distribution vector for each image, 5 accurate landmark locations and 37 automatic landmark locations, and baseline classifier outputs for multi-label emotion recognition.
Source: Real-world Affective Faces Multi Label"	https://paperswithcode.com/dataset/raf-ml		Real-world Affective Faces Multi Label					
2458	Raindrop	"Raindrop is a set of image pairs, where
each pair contains exactly the same background scene, yet
one is degraded by raindrops and the other one is free from
raindrops. To obtain this, the images are captured through two pieces of exactly the
same glass: one sprayed with water, and the other is left
clean. The dataset consists of 1,119 pairs of images, with various
background scenes and raindrops. They were captured with a Sony A6000
and a Canon EOS 60.
Source: Attentive Generative Adversarial Network for Raindrop Removal from a Single Image"	https://paperswithcode.com/dataset/raindrop							
2459	RainNet	"RainNet is a real (non-simuated) large-scale spatial precipitation downscaling dataset that contains 62,424 pairs of low-resolution and high-resolution precipitation maps for 17 years. Contrary to simulated data, this real dataset covers various types of real meteorological phenomena (e.g., Hurricane, Squall, etc.), and shows the physical characters - Temporal Misalignment, Temporal Sparse and Fluid Properties - that challenge the downscaling algorithms.
Source: https://github.com/neuralchen/RainNet
Image Source: https://github.com/neuralchen/RainNet"	https://paperswithcode.com/dataset/rainnet							
2460	RareAct	"RareAct is a video dataset of unusual actions, including actions like “blend phone”, “cut keyboard” and “microwave shoes”. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately.
Source: https://github.com/antoine77340/RareAct
Image Source: https://github.com/antoine77340/RareAct"	https://paperswithcode.com/dataset/rareact							
2461	RarePlanes Dataset	"The dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. 
Source: RarePlanes: Synthetic Data Takes Flight"	https://paperswithcode.com/dataset/rareplanes-dataset							
2462	RAVEN	"RAVEN consists of 1,120,000 images and 70,000 RPM (Raven's Progressive Matrices)
problems, equally distributed in 7 distinct figure configurations."	https://paperswithcode.com/dataset/raven							
2463	RAVEN-FAIR	"RAVEN-FAIR is a modified version of the RAVEN dataset.
Source: https://github.com/yanivbenny/RAVEN_FAIR"	https://paperswithcode.com/dataset/raven-fair							
2464	RCTW-17	"Features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams.
Source: ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)"	https://paperswithcode.com/dataset/rctw-17		Reading Chinese Text in the Wild					
2465	Real Rain Dataset	"A large-scale dataset of ~29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes.
Source: Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset"	https://paperswithcode.com/dataset/real-rain-dataset							
2466	ReCO	"A human-curated ChineseReading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to the commercial search engine. The passages are provided by the crowdworkers who extract the support snippet from the retrieved documents. 
Source: ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion"	https://paperswithcode.com/dataset/reco							
2467	RED	"The Real Embodied Dataset (RED) is a computer vision large-scale dataset for grasping in cluttered scenes. It contains complete segmentation masks for partially occluded objects, with their order of occlusion.
Source: https://arxiv.org/pdf/2004.13358.pdf
Image Source: https://arxiv.org/pdf/2004.13358.pdf"	https://paperswithcode.com/dataset/red		Real Embodied Dataset					
2468	ReDial	"ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations. 
Source: Towards Deep Conversational Recommendations"	https://paperswithcode.com/dataset/redial							
2469	ReDWeb-S	"ReDWeb-S is a large-scale challenging dataset for Salient Object Detection. It has totally 3179 images with various real-world scenes and high-quality depth maps. The dataset is split into a training set with 2179 RGB-D image pairs and a testing set with the remaining 1000 image pairs.
Source: https://github.com/nnizhang/SMAC
Image Source: https://github.com/nnizhang/SMAC"	https://paperswithcode.com/dataset/redweb-s							
2470	redwood-3dscan	"A dataset of more than ten thousand 3D scans of real objects. 
Source: A Large Dataset of Object Scans"	https://paperswithcode.com/dataset/redwood-3dscan							
2471	RefCOCO	"This referring expression generation (REG) dataset was collected using the ReferitGame. In this two-player game, the first player is shown an image with a segmented target object and asked to write a natural language expression referring to the target object. The second player is shown only the image and the referring expression and asked to click on the corresponding object. If the players do their job correctly, they receive points and swap roles. If not, they are presented with a new object and image for description. Images in these collections were selected to contain two or more objects of the same object category. In the RefCOCO dataset, no restrictions are placed on the type of language used in the referring expressions. In a version of this dataset called RefCOCO+ players are disallowed from using location words in their referring expressions by adding “taboo” words to the ReferItGame. This dataset was collected to obtain a referring expression dataset focsed on purely appearance based description, e.g., “the man in the yellow polka-dotted shirt” rather than “the second man from the left”, which tend to be more interesting from a computer vision based perspective and are independent of viewer perspective. RefCOCO consists of 142,209 refer expressions for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for 49,856 objects in 19,992 images.
Source: https://arxiv.org/pdf/1608.00272.pdf
Image Source: https://github.com/lichengunc/refer"	https://paperswithcode.com/dataset/refcoco							
2472	REFreSD	"Consists of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.
Source: Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank"	https://paperswithcode.com/dataset/refresd		Rationalized English-French Semantic Divergences					
2473	REFUGE Challenge	"REFUGE Challenge provides a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one.
Source: REFUGE Challenge: A Unified Framework for Evaluating Automated Methods for Glaucoma Assessment from Fundus Photographs
Image Source: Orlando et al"	https://paperswithcode.com/dataset/refuge-challenge		Retinal Fundus Glaucoma Challenge					
2474	ReINTEL	"10,000 news collected from a social network in Vietnam.
Source: ReINTEL: A Multimodal Data Challenge for Responsible Information Identification on Social Network Sites"	https://paperswithcode.com/dataset/reintel							
2475	Relational Strategies in Customer Service (RSiCS) Dataset	"Corpus for improving the quality and relational abilities of Intelligent Virtual Agents (IVAs).
Source: An Annotated Corpus of Relational Strategies in Customer Service"	https://paperswithcode.com/dataset/relational-strategies-in-customer-service							
2476	Relative Size	"The Relative Size dataset contains 486 object pairs between 41 physical objects. Size comparisons are not available for all pairs of objects (e.g. bird and watermelon) because for some pairs humans cannot determine which object is bigger.
Dataset contains only object pairs that people have consistently agreed which one is bigger. Objects appear in 24 pairs on average, window with 13 pairs has the least, and eye with 35 pairs has the most number of comparisons.
Source: Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"	https://paperswithcode.com/dataset/relative-size							
2477	RELLIS-3D	"RELLIS-3D is a multi-modal dataset for off-road robotics. It was collected in an off-road environment containing annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. The dataset also provides full-stack sensor data in ROS bag format, including RGB camera images, LiDAR point clouds, a pair of stereo images, high-precision GPS measurement, and IMU data.
Source: https://github.com/unmannedlab/RELLIS-3D
Image Source: https://github.com/unmannedlab/RELLIS-3D"	https://paperswithcode.com/dataset/rellis-3d							
2478	RELX	"RELX is a benchmark dataset for cross-lingual relation classification in English, French, German, Spanish and Turkish.
Source: https://github.com/boun-tabi/RELX"	https://paperswithcode.com/dataset/relx							
2479	Rendered Handpose Dataset	"Rendered Handpose Dataset contains 41258 training and 2728 testing samples. Each sample provides:

RGB image (320x320 pixels)
Depth map (320x320 pixels)
Segmentation masks (320x320 pixels) for the classes: background, person, three classes for each finger and one for each palm
21 Keypoints for each hand with their uv coordinates in the image frame, xyz coordinates in the world frame and a visibility indicator
Intrinsic Camera Matrix K

Source: Learning to Estimate 3D Hand Pose from Single RGB Images
Image Source: https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html"	https://paperswithcode.com/dataset/rendered-handpose-dataset							
2480	Rendered WB dataset	"A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images.
Source: When Color Constancy Goes Wrong: Correcting Improperly White-Balanced Images"	https://paperswithcode.com/dataset/rendered-wb-dataset		Rendered WB dataset					
2481	Rent3D	"A dataset which contains over 200 apartments.
Source: Rent3D: Floor-Plan Priors for Monocular Layout Estimation"	https://paperswithcode.com/dataset/rent3d							
2482	Replica	"The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation. 
Source: The Replica Dataset: A Digital Replica of Indoor Spaces"	https://paperswithcode.com/dataset/replica							
2483	RESIDE	"A new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. 
Source: RESIDE"	https://paperswithcode.com/dataset/reside							
2484	Retail50K	"A dataset to encourage the community to adapt oriented bounding box (OBB) detectors for more complex environments.
Source: PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments"	https://paperswithcode.com/dataset/retail50k							
2485	ReviewQA	"ReviewQA is a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that a model is expected to master. Indeed, each question comes with an associated type that characterizes the required competency.
Source: ReviewQA: a relational aspect-based opinion reading dataset"	https://paperswithcode.com/dataset/reviewqa							
2486	RF signal	"This dataset is used for RF signal recognition, used to recognize different RF devices based on the signals they transmitted.
Source: https://arxiv.org/pdf/1908.09931.pdf"	https://paperswithcode.com/dataset/rf-signal							
2487	RFW	"To validate the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. 
Source: Racial Faces in-the-Wild: Reducing Racial Bias by Information Maximization Adaptation Network"	https://paperswithcode.com/dataset/rfw		Racial Faces in-the-Wild					
2488	DIML/CVl RGB-D Dataset	"This dataset contains synchronized RGB-D frames from both Kinect v2 and Zed stereo camera. For the outdoor scene, the authors first generate disparity maps using an accurate stereo matching method and convert them using calibration parameters. A per-pixel confidence map of disparity is also provided. The scenes are captured at various places, e.g., offices, rooms, dormitory, exhibition center, street, road etc., from Yonsei University and Ewha University.
Source: DIML/CVl RGB-D Dataset"	https://paperswithcode.com/dataset/diml-cvl-rgb-d-dataset							
2489	RGB-DAVIS Dataset	"Used to show systematic performance improvement in applications such as high frame-rate video synthesis, feature/corner detection and tracking, as well as high dynamic range image reconstruction.
Source: Joint Filtering of Intensity Images and Neuromorphic Events for High-Resolution Noise-Robust Imaging"	https://paperswithcode.com/dataset/rgb-davis-dataset							
2490	RGB-D Object dataset	"The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology.
Source: RGB-D Object dataset"	https://paperswithcode.com/dataset/rgb-d-object-dataset							
2491	RICE	"RICE is a remote sensing image dataset for cloud removal. The proposed dataset consists of two parts: RICE1 contains 500 pairs of images, each pair has images with cloud and cloudless size of 512512; RICE2 contains 450 sets of images, each set contains three 512512 size images, respectively, the reference picture without clouds, the picture of the cloud and the mask of its cloud.
Source: https://github.com/BUPTLdy/RICE_DATASET"	https://paperswithcode.com/dataset/rice		Remote sensing Image Cloud rEmoving					
2492	Rijksmuseum Challenge 2014	"Dataset used for the challenge to apply computer vision techniques on art objects (paintings, sculptures, drawings etc) from the Rijksmuseum (in Amsterdam, the Netherlands).
Source: Rijksmuseum Challenge 2014"	https://paperswithcode.com/dataset/rijksmuseum-challenge-2014							
2493	RiSAWOZ	"RiSAWOZ is a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labelled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, it also includes linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks.
Source: https://github.com/terryqj0107/RiSAWOZ"	https://paperswithcode.com/dataset/risawoz							
2494	RISE	"RISE is a large-scale video dataset for Recognizing Industrial Smoke Emissions. A citizen science approach was adopted to collaborate with local community members to annotate whether a video clip has smoke emissions. The dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons.
Source: https://arxiv.org/abs/2005.06111
Image Source: https://github.com/CMU-CREATE-Lab/deep-smoke-machine"	https://paperswithcode.com/dataset/rise							
2495	RIT-18	"The RIT-18 dataset was built for the semantic segmentation of remote sensing imagery. It was collected with the Tetracam Micro-MCA6 multispectral imaging sensor flown on-board a DJI-1000 octocopter. 
The features this dataset include 1) very-high resolution multispectral imagery from a drone, 2) six-spectral VNIR bands, and 3) 18 object classes (plus background) with a severely unbalanced class distribution.
Source: Algorithms for Semantic Segmentation of Multispectral Remote Sensing Imagery using Deep Learning
Image Source: https://github.com/rmkemker/RIT-18"	https://paperswithcode.com/dataset/rit-18							
2496	RLLab Framework	"A benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. 
Source: Benchmarking Deep Reinforcement Learning for Continuous Control"	https://paperswithcode.com/dataset/rllab-framework							
2497	RMFD	"Real-World Masked Face Dataset (RMFD) is a large dataset for masked face detection.
Source: https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset
Image Source: https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset"	https://paperswithcode.com/dataset/rmfd		Real-World Masked Face Dataset					
2498	Road Scene Graph	"A special scene-graph for intelligent vehicles. Different to classical data representation, this graph provides not only object proposals but also their pair-wise relationships. By organizing them in a topological graph, these data are explainable, fully-connected, and could be easily processed by GCNs (Graph Convolutional Networks).
Source: Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles"	https://paperswithcode.com/dataset/road-scene-graph							
2499	RoadText-1K	"A dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. The dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. 
Source: RoadText-1K: Text Detection & Recognition Dataset for Driving Videos"	https://paperswithcode.com/dataset/roadtext-1k							
2500	RoadTracer	"RoadTracer is a dataset for extraction of road networks from aerial images. It consists of a large
corpus of high-resolution satellite imagery and ground truth
road network graphs covering the urban core of forty cities
across six countries. For each city, the dataset covers a region of approximately 24 sq km around the city center. The satellite imagery is obtained from Google at 60 cm/pixel resolution, and the road network from OSM.
The dataset is split into a training set with 25 cities and a
test set with 15 other cities.
Source: RoadTracer: Automatic Extraction of Road Networks from Aerial Images
Image Source: Bastani et al"	https://paperswithcode.com/dataset/roadtracer							
2501	RoboCupSimData	"A large dataset from games of some of the top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D), where teams of 11 robots (agents) compete against each other. 
Source: RoboCupSimData: A RoboCup soccer research dataset"	https://paperswithcode.com/dataset/robocupsimdata							
2502	RoboNet	"An open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation.
Source: RoboNet: Large-Scale Multi-Robot Learning"	https://paperswithcode.com/dataset/robonet							
2503	Robot@Home dataset	"The Robot-at-Home dataset (Robot@Home) is a collection of raw and processed data from five domestic settings compiled by a mobile robot equipped with 4 RGB-D cameras and a 2D laser scanner. Its main purpose is to serve as a testbed for semantic mapping algorithms through the categorization of objects and/or rooms.
Paper: Robot@Home, a robotic dataset for semantic mapping of home environments
Source: Robot@Home dataset"	https://paperswithcode.com/dataset/robot-home-dataset	13/03/2017						
2504	Robotic Instruments	"Provides 8x 225-frame robotic surgical videos, captured at 2 Hz, where a trained team at Intuitive Surgical has manually labelled the different parts and types. The users are invited to test their algorithms on 8x 75-frame videos and 2x 300-frame videos which act as a test set.
Source: Robotic Instruments"	https://paperswithcode.com/dataset/robotic-instruments							
2505	RobustPointSet	"A dataset for robustness analysis of point cloud classification models (independent of data augmentation) to input transformations.
Source: RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers"	https://paperswithcode.com/dataset/robustpointset							
2506	Roll4Real	"Consists of real objects rolling on complex terrains (pool table, elliptical bowl, and random height-field). 
Source: Unsupervised Intuitive Physics from Visual Observations"	https://paperswithcode.com/dataset/roll4real							
2507	Roman Urdu Data Set	"Tagged for Sentiment (Positive, Negative, Neutral).
Source: Roman Urdu Data Set"	https://paperswithcode.com/dataset/roman-urdu-data-set							
2508	RONEC	"Romanian Named Entity Corpus is a named entity corpus for the Romanian language. The corpus contains over 26000 entities in ~5000 annotated sentences, belonging to 16 distinct classes. The sentences have been extracted from a copy-right free newspaper, covering several styles. This corpus represents the first initiative in the Romanian language space specifically targeted for named entity recognition. 
Source: Introducing RONEC -- the Romanian Named Entity Corpus"	https://paperswithcode.com/dataset/ronec	03/09/2019	Romanian Named Entity Corpus					
2509	ROSTD	"A dataset of 4K out-of-domain (OOD) examples for the publicly available dataset from (Schuster et al. 2019). In contrast to existing settings which synthesize OOD examples by holding out a subset of classes, the examples were authored by annotators with apriori instructions to be out-of-domain with respect to the sentences in an existing dataset. 
Source: Likelihood Ratios and Generative Classifiers for Unsupervised Out-of-Domain Detection In Task Oriented Dialog"	https://paperswithcode.com/dataset/rostd		Real Out-of-Domain Sentences From Task-oriented Dialog					
2510	Rotowire-Modified	"The RotoWire-Modified dataset is a cleaned extension of the RotoWire dataset, with writer information about each document. It contains 2705 samples for training, 532 for validation and 497 for testing.
Source: https://github.com/aistairc/rotowire-modified"	https://paperswithcode.com/dataset/rotowire-modified							
2511	RP2K	"A new large-scale retail product dataset for fine-grained image classification. Unlike previous datasets focusing on relatively few products, more than 500,000 images of retail products on shelves were collected, belonging to 2000 different products. The dataset aims to advance the research in retail object recognition, which has massive applications such as automatic shelf auditing and image-based product information retrieval.
Source: RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification"	https://paperswithcode.com/dataset/rp2k							
2512	RPC	"RPC is a large-scale retail product checkout dataset and collects 200 retail SKUs. The collected SKUs can be divided into 17 meta categories, i.e., puffed food, dried fruit, dried food, instant drink, instant noodles, dessert, drink, alcohol, milk, canned food, chocolate, gum, candy, seasoner, personal hygiene, tissue, stationery.
Source: RPC: A Large-Scale Retail Product Checkout Dataset
Image Source: Wei et al"	https://paperswithcode.com/dataset/rpc		Retail Product Checkout					
2513	RSDD-Time	"RSDD-Time is a dataset of 598 manually annotated self-reported depression diagnosis posts from Reddit that include temporal information about the diagnosis. Annotations include whether a mental health condition is present and how recently the diagnosis happened. Additionally, the dataset includes exact temporal spans that relate to the date of diagnosis. 
Source: RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses"	https://paperswithcode.com/dataset/rsdd-time							
2514	RSICD	"The Remote Sensing Image Captioning Dataset (RSICD) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image.
Source: https://github.com/201528014227051/RSICD_optimal
Image Source: https://github.com/201528014227051/RSICD_optimal"	https://paperswithcode.com/dataset/rsicd		Remote Sensing Image Captioning Dataset					
2515	RTN	"A corpus of real-world spoken personal narratives comprising 10,296 narrative clauses from 594 video transcripts. 
Source: Exploring aspects of similarity between spoken personal narratives by disentangling them into narrative clause types"	https://paperswithcode.com/dataset/rtn							
2516	RuBQ	"The first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd-assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification.
Source: RuBQ: A Russian Dataset for Question Answering over Wikidata"	https://paperswithcode.com/dataset/rubq		Russian Knowledge Base Questions					
2517	Runway	"A runway dataset, designing features suitable for capturing outfit appearance, collecting human judgments of outfit similarity, and learning similarity functions on the features to mimic those judgments.
Source: Runway"	https://paperswithcode.com/dataset/runway							
2518	Ruralscapes	"A dataset with high resolution (4K) images and manually-annotated dense labels every 50 frames.
Source: Semantics through Time: Semi-supervised Segmentation of Aerial Videos with Iterative Label Propagation"	https://paperswithcode.com/dataset/ruralscapes-dataset							
2519	RuSentRel	"RuSentRel is a corpus of analytical articles translated into Russian texts in the domain of international politics obtained from foreign authoritative sources. The collected articles contain both the author's opinion on the subject matter of the article and a large number of references mentioned between the participants of the described situations. In total, 73 large analytical texts were labeled with about 2000 relations.
Source: RuSentRel"	https://paperswithcode.com/dataset/rusentrel							
2520	RUSLAN	"RUSLAN is a Russian spoken language corpus for text-to-speech task. RUSLAN contains 22,200 audio samples with text annotations – more than 31 hours of high-quality speech of one person – being one of the largest annotated Russian corpus in terms of speech duration for a single speaker. 
Source: RUSLAN: Russian Spoken Language Corpus for Speech Synthesis"	https://paperswithcode.com/dataset/ruslan							
2521	RuStance	"Includes Russian tweets and news comments from multiple sources, covering multiple stories, as well as text classification approaches to stance detection as benchmarks over this data in this language.
Source: Stance Prediction for Russian: Data and Analysis"	https://paperswithcode.com/dataset/rustance							
2522	RWF-2000	"A database with 2,000 videos captured by surveillance cameras in real-world scenes. 
Source: RWF-2000: An Open Large Scale Video Database for Violence Detection"	https://paperswithcode.com/dataset/rwf-2000							
2523	RxR	"Room-Across-Room (RxR) is a multilingual dataset for Vision-and-Language Navigation (VLN) for Matterport3D environments. In contrast to related datasets such as Room-to-Room (R2R), RxR is 10x larger, multilingual (English, Hindi and Telugu), with longer and more variable paths, and it includes and fine-grained visual groundings that relate each word to pixels/surfaces in the environment.
Source: Room-Across-Room (RxR) Dataset"	https://paperswithcode.com/dataset/rxr		Room-across-Room					
2524	S2TLD	"S2TLD is a traffic light dataset, which contains 5,786 images of approximately 1,080 * 1,920 pixels and 720 * 1,280 pixels. It also contains 5 categories (include red, yellow, green, off and wait on) of 1,4130 instances. The scenes cover a decent variety of road scenes and typical:
* Busy street scenes inner-city,
* Dense stop-and-go traffic
* Strong changes in illumination/exposure
* Flickering/Fluctuating traffic lights
* Multiple visible traffic lights
* Image parts that can be confused with traffic lights (e.g. large round tail lights)
Source: https://github.com/Thinklab-SJTU/S2TLD
Image Source: https://github.com/Thinklab-SJTU/S2TLD"	https://paperswithcode.com/dataset/s2tld		SJTU Small Traffic Light Dataset					
2525	S3O4D	"The data consists of 100,000 renderings each of the Bunny and Dragon objects from the Stanford 3D Scanning Repository. More objects may be added in the future, but only the Bunny and Dragon are used in the paper. Each object is rendered with a uniformly sampled illumination from a point on the 2-sphere, and a uniformly sampled 3D rotation. The true latent states are provided as NumPy arrays along with the images. The lighting is given as a 3-vector with unit norm, while the rotation is provided both as a quaternion and a 3x3 orthogonal matrix.
Source: S3O4D"	https://paperswithcode.com/dataset/s3o4d		Stanford 3D Objects for Disentangling					
2526	SailX	"A dataset for grounded language learning that consists of navigational instructions and actions in a maze-like environment.
Source: A new dataset and model for learning to understand navigational instructions"	https://paperswithcode.com/dataset/sailx							
2527	Salient Closed Boundary Tracking	"This dataset contains nine video sequences captured by a webcam for salient closed boundary tracking evaluation. Each sequence is about 30 sec (30 fps) and the frame size is 640×480 (width×height). There are 9598 frames in total. In each sequence, different motion styles such as translation, rotation and viewpoint changing are all performed.
Source: https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset
Image Source: https://github.com/NathanUA/SalientClosedBoundaryTrackingDataset"	https://paperswithcode.com/dataset/salient-closed-boundary-tracking							
2528	Salient-KITTI	"Salient-KITTI is a saliency map prediction dataset based on KITTI.
Source: https://arxiv.org/pdf/2012.11863.pdf
Image Source: https://github.com/Saixiaoma/SBA-SLAM"	https://paperswithcode.com/dataset/salient-kitti							
2529	Salient Object Subitizing Dataset	"A salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. 
Source: Salient Object Subitizing"	https://paperswithcode.com/dataset/salient-object-subitizing-dataset							
2530	SALSA	"A novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis.
Source: SALSA: A Novel Dataset for Multimodal Group Behavior Analysis"	https://paperswithcode.com/dataset/salsa							
2531	SAMM Long Videos	"The SAMM Long Videos dataset consists of 147 long videos with 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded with detailed Action Units.
Source: SAMM Long Videos: A Spontaneous Facial Micro- and Macro-Expressions Dataset"	https://paperswithcode.com/dataset/samm-long-videos							
2532	SAMSum Corpus	"A new dataset with abstractive dialogue summaries.
Source: SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"	https://paperswithcode.com/dataset/samsum-corpus							
2533	San Francisco Landmark Dataset	"The San Francisco Landmark Dataset contains a database of 1.7 million images of buildings in San Francisco with ground truth labels, geotags, and calibration data, as well as a difficult query set of 803 cell phone images taken with a variety of different camera phones. The data is originally acquired by vehicle-mounted cameras with wide-angle lenses capturing spherical panoramic images. For all visible buildings in each panorama, a set of overlapping perspective images is generated.
Paper: D. Chen, G. Baatz, K. Koeser, S. Tsai, R. Vedantham, T. Pylvanainen, K. Roimela, X. Chen, J. Bach, M. Pollefeys, B. Girod, and R. Grzeszczuk, ""City-scale landmark identification on mobile devices"", IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2011."	https://paperswithcode.com/dataset/san-francisco-landmark-dataset							
2534	SARA	"A dataset for statutory reasoning in tax law entailment and question answering.
Source: A Dataset for Statutory Reasoning in Tax Law Entailment and Question Answering"	https://paperswithcode.com/dataset/sara		StAtutory Reasoning Assessment					
2535	SARC	"This dataset was designed for contextual investigations, with related works making considerable usage of said context. The dataset was constructed by scraping Reddit comments; with sarcastic entries being self-annotated by authors through the use of the \s token, which indicates sarcastic intent on the website. Posts on Reddit are often in response to another comment; SARC incorporates this information through the addition of the parent comment and further child comments surrounding a post. 
Source: DEEP AND DENSE SARCASM DETECTION"	https://paperswithcode.com/dataset/sarc	19/04/2017						
2536	SART	"SART is a collection of three datasets for Similarity, Analogies and Relatedness for the Tatar language.
The three subsets are:
* Similarity dataset - 202 pairs of words along with averaged human scores of similarity degree between the words (in 0-to-10 scale). For example, ""йорт, бина, 7.69"".
* Relatedness dataset - 252 pairs of words along with averaged human scores of relatedness degree between the words. For example, ""урам, балалар, 5.38"".
* Analogies dataset - set of analytical questions of the form A:B::C:D, meaning A to B as C to D, and D is to be predicted. For example, ""Әнкара Төркия Париж Франция"". Contains 34 categories, and in total 30 144 questions.
Source: https://github.com/tat-nlp/SART"	https://paperswithcode.com/dataset/sart							
2537	SatStereo	"Provides a set of stereo-rectified images and the associated groundtruthed disparities for 10 AOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS Challenge dataset and 2 AOIs from the CORE3D-Public dataset.
Source: A New Stereo Benchmarking Dataset for Satellite Images"	https://paperswithcode.com/dataset/satstereo							
2538	SAVOIAS	"A visual complexity dataset that compromises of more than 1,400 images from seven image categories relevant to the above research areas, namely Scenes, Advertisements, Visualization and infographics, Objects, Interior design, Art, and Suprematism. The images in each category portray diverse characteristics including various low-level and high-level features, objects, backgrounds, textures and patterns, text, and graphics. 
Source: SAVOIAS: A Diverse, Multi-Category Visual Complexity Dataset"	https://paperswithcode.com/dataset/savoias							
2539	SberQuAD	"A large scale analogue of Stanford SQuAD in the Russian language - is a valuable resource that has not been properly presented to the scientific community. 
Source: SberQuAD -- Russian Reading Comprehension Dataset: Description and Analysis
See DeepPavlov link
Model results
| Model config                 | EM (dev)    | F-1 (dev)   |
|------------------------------|-------------|-------------|
|DeepPavlov RuBERT            | 66.30+-0.24 | 84.60+-0.11 |
| DeepPavlov multilingual BERT | 64.35+-0.39 | 83.39+-0.08 |
| DeepPavlov R-Net             | 60.62       | 80.04       |"	https://paperswithcode.com/dataset/sberquad		Sberbank Question Answering Dataset					
2540	SBIC	"To support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.
Source: Social Bias Frames: Reasoning about Social and Power Implications of Language"	https://paperswithcode.com/dataset/sbic		Social Bias Inference Corpus					
2541	SBU Captions Dataset	"A collection that allows researchers to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results.
Source: Im2Text: Describing Images Using 1 Million Captioned Photographs"	https://paperswithcode.com/dataset/sbu-captions-dataset							
2542	SBWCE	"This resource consists of an unannotated corpus of the Spanish language of nearly 1.5 billion words, compiled from different corpora and resources from the web; and a set of word vectors (or embeddings), created from this corpus using the word2vec algorithm, provided by the gensim package. These embeddings were evaluated by translating to Spanish word2vec’s word relation test set.
Source: SBWCE"	https://paperswithcode.com/dataset/sbwce		Spanish Billion Word Corpus and Embeddings					
2543	SCAN	"SCAN is a dataset for grounded navigation which consists of a set of simple compositional navigation commands paired with the corresponding action sequences. 
Source: Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks"	https://paperswithcode.com/dataset/scan		Simplified versions of the CommAI Navigation tasks					
2544	Scan-CAD Object Similarity Dataset	"A dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. 
Source: Joint Embedding of 3D Scan and CAD Objects"	https://paperswithcode.com/dataset/scan-cad-object-similarity-dataset							
2545	ScanRefer Dataset	"Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.
Source: ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language"	https://paperswithcode.com/dataset/scanrefer-dataset							
2546	scb-mt-en-th-2020	"scb-mt-en-th-2020 is an English-Thai machine translation dataset with over 1 million segment pairs, curated from various sources, namely news, Wikipedia articles, SMS messages, task-based dialogs, web-crawled data and government documents.
Source: scb-mt-en-th-2020: A Large English-Thai Parallel Corpus
Image Source: Lowphansirikul et al"	https://paperswithcode.com/dataset/scb-mt-en-th-2020							
2547	SCDB	"Includes annotations for 10 distinguishable concepts.
Source: Explaining AI-based Decision Support Systems using Concept Localization Maps"	https://paperswithcode.com/dataset/scdb		Simple Concept DataBase					
2548	ScienceIE	"The shared task ScienceIE at SemEval 2017 deals with automatic extraction of keyphrases from Computer Science, Material Sciences and Physics publications, as well as extracting types of keyphrases and relations between keyphrases.
PROCESS, TASK and MATERIAL form the fundamental objects in scientific works. Scientific research and practice is founded upon gaining, maintaining and understanding the body of existing scientific work in specific areas related to such fundamental objects. 
Source: ScienceIE"	https://paperswithcode.com/dataset/scienceie							
2549	ScisummNet	"Large-scale manually-annotated corpus for 1,000 scientific papers (on computational linguistics) for automatic summarization. Summaries for each paper are constructed from the papers that cite that paper and from that paper's abstract.
Source: ScisummNet: A Large Annotated Corpus and Content-Impact Models for Scientific Paper Summarization with Citation Networks"	https://paperswithcode.com/dataset/scisummnet							
2550	SciTLDR	"A new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. 
Source: TLDR: Extreme Summarization of Scientific Documents"	https://paperswithcode.com/dataset/scitldr							
2551	SciTSR	"SciTSR is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files.
Source: https://github.com/Academic-Hammer/SciTSR"	https://paperswithcode.com/dataset/scitsr							
2552	Scruples	"Dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members.
Source: Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes"	https://paperswithcode.com/dataset/scruples							
2553	SCUT-HEAD	"Includes 4405 images with 111251 heads annotated.
Source: Detecting Heads using Feature Refine Net and Cascaded Multi-Scale Architecture"	https://paperswithcode.com/dataset/scut-head							
2554	Search4Code	"Search4Code is a large-scale web query based dataset of code search queries for C# and Java. The Search4Code data is mined from Microsoft Bing's anonymized search query logs using weak supervision technique.
Source: https://github.com/microsoft/Search4Code"	https://paperswithcode.com/dataset/search4code							
2555	SeasonDepth	"Aa new cross-season scaleless monocular depth prediction dataset from CMU Visual Localization dataset through structure from motion.
Source: SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments"	https://paperswithcode.com/dataset/seasondepth							
2556	SegTHOR	"SegTHOR (Segmentation of THoracic Organs at Risk) is a dataset dedicated to the segmentation of organs at risk (OARs) in the thorax, i.e. the organs surrounding the tumour that must be preserved from irradiations during radiotherapy. In this dataset, the OARs are the heart, the trachea, the aorta and the esophagus, which have varying spatial and appearance characteristics. The dataset includes 60 3D CT scans, divided into a training set of 40 and a test set of 20 patients, where the OARs have been contoured manually by an experienced radiotherapist. 
Source: SegTHOR: Segmentation of Thoracic Organs at Risk in CT images
Image Source: Lambert et al"	https://paperswithcode.com/dataset/segthor		Segmentation of THoracic Organs at Risk					
2557	SelQA	"SelQA is a dataset that consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. 
Source: SelQA: A New Benchmark for Selection-based Question Answering"	https://paperswithcode.com/dataset/selqa							
2558	SemanticPOSS	"The SemanticPOSS dataset for 3D semantic segmentation contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI.
Source: SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances"	https://paperswithcode.com/dataset/semanticposs							
2559	SemArt	"SemArt is a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. It contains 21,384 samples that provides artistic comments along with fine-art paintings and their attributes for studying semantic art understanding.
Source: How to Read Paintings: Semantic Art Understanding with Multi-Modal Retrieval"	https://paperswithcode.com/dataset/semart							
2560	SEMCAT	"Contains more than 6500 words semantically grouped under 110 categories.
Source: Semantic Structure and Interpretability of Word Embeddings"	https://paperswithcode.com/dataset/semcat							
2561	SemClinBr	"A semantically annotated corpus using clinical texts from multiple medical specialties, document types, and institutions.
Source: SemClinBr -- a multi institutional and multi specialty semantically annotated corpus for Portuguese clinical NLP tasks"	https://paperswithcode.com/dataset/semclinbr							
2562	SEN12MS	"A dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps.
Source: SEN12MS -- A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data Fusion"	https://paperswithcode.com/dataset/sen12ms							
2563	SEND	"SEND (Stanford Emotional Narratives Dataset) is a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models.
Source: Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset"	https://paperswithcode.com/dataset/send		Stanford Emotional Narratives Dataset					
2564	SensatUrban	"The SensatUrbat dataset is an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. The dataset consists of large areas from two UK cities, covering about 6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes, such as ground, vegetation, car, etc..
Source: https://github.com/QingyongHu/SensatUrban
Image Source: https://github.com/QingyongHu/SensatUrban"	https://paperswithcode.com/dataset/sensaturban							
2565	SentiCap	"The SentiCap dataset contains several thousand images with captions with positive and negative sentiments. These sentimental captions are constructed by the authors by re-writing factual descriptions. In total there are 2000+ sentimental captions.
Source: SentiCap: Generating Image Descriptions with Sentiments"	https://paperswithcode.com/dataset/senticap							
2566	Sentiment140	"Sentiment140 is a dataset that allows you to discover the sentiment of a brand, product, or topic on Twitter.
Source: Sentiment140"	https://paperswithcode.com/dataset/sentiment140							
2567	Sentimental LIAR	"The Sentimental LIAR dataset is a modified and further extended version of the LIAR extension introduced by Kirilin et al. In this dataset, the multi-class labeling of LIAR is converted to a binary annotation by changing half-true, false, barely-true and pants-fire labels to False, and the remaining labels to True. Furthermore, the speaker names are converted to numerical IDs in order to avoid bias with regards to the textual representation of names. The binary-label dataset is then extended by adding sentiments derived using the Google NLP API. Sentiment analysis determines the overall attitude of the text (i.e., whether it is positive or negative), and is quantified by a numerical score. If the sentiment score is positive, then the sample is tagged as Positive for the sentiment attribute, otherwise Negative is assigned.
A further extension is introduced by adding emotion scores extracted using the IBM NLP API for each claim, which determine the detected level of 6 emotional states namely anger, sadness, disgust, fear and joy. The score for each emotion is between the range of 0 and 1.
Source: https://github.com/UNHSAILLab/SentimentalLIAR"	https://paperswithcode.com/dataset/sentimental-liar							
2568	Serial Speakers	"An annotated dataset of 161 episodes from three popular American TV serials: Breaking Bad, Game of Thrones and House of Cards. Serial Speakers is suitable both for investigating multimedia retrieval in realistic use case scenarios, and for addressing lower level speech related tasks in especially challenging conditions.
Source: Serial Speakers: a Dataset of TV Series"	https://paperswithcode.com/dataset/serial-speakers							
2569	SEWA DB	"A database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies.
Source: SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild"	https://paperswithcode.com/dataset/sewa-db							
2570	SFU-Store-Nav	"A dataset collected in a set of experiments that involves human participants and a robot.
Source: SFU-Store-Nav: A Multimodal Dataset for Indoor Human Navigation"	https://paperswithcode.com/dataset/sfu-store-nav							
2571	SGD	"The Schema-Guided Dialogue (SGD) dataset consists of over 20k annotated multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, ranging from banks and events to media, calendar, travel, and weather. For most of these domains, the dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces, which reflects common real-world scenarios. The wide range of available annotations can be used for intent prediction, slot filling, dialogue state tracking, policy imitation learning, language generation, user simulation learning, among other tasks in large-scale virtual assistants. Besides these, the dataset has unseen domains and services in the evaluation set to quantify the performance in zero-shot or few shot settings.
Source: The Schema-Guided Dialogue Dataset"	https://paperswithcode.com/dataset/sgd		Schema-Guided Dialogue					
2572	SG-NLG	"The SG-NLG dataset is a pre-processed version of the DSTC8 Schema-Guided Dialogue SGD dataset, designed specifically for data-to-text Natural Language Generation (NLG). The original DSTC8 SGD contains ~20,000 dialogues spanning across ~20 domains.
This SG-NLG dataset is designed to make it easier to conduct NLG experiments on the SGD data. It consists of pre-processed SGD data by pairing the schema for each system turn with the corresponding set of natural language strings that realize it. It also “delexicalizes” the prompts (replace related values with fixed names) to convert them into templates that make them more generic for use within a dialog system.
The final SG-NLG dataset is composed of nearly 4K MRs and over 140K templates. 
Source: The Schema-Guided Natural Language Generation (SG-NLG) Dataset"	https://paperswithcode.com/dataset/sg-nlg		Schema-Guided Natural Language Generation					
2573	ShapenetRender	"ShapenetRenderer is an extension of the ShapeNet Core dataset which has more variation in camera angles. For each mesh model, the dataset provides 36 views with smaller variation and 36 views with larger variation. The resolution of the newly rendered images is 224x224 in contrast to the 137x137 original resolution. Additionally, each RGB image is paired with a depth image, a normal map and an albedo image.
Source: https://github.com/Xharlie/ShapenetRender_more_variation
Image Source: https://github.com/Xharlie/ShapenetRender_more_variation"	https://paperswithcode.com/dataset/shapenetrender							
2574	ShapeNet-Skeleton	"The ShapeNet-Skeleton dataset has ground-truth skeleton point sets and skeletal volumes for object instances in the ShapeNet dataset.
Source: https://arxiv.org/pdf/2008.05742.pdf"	https://paperswithcode.com/dataset/shapenet-skeleton							
2575	3D Shapes Dataset	"3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation.
Source: 3D Shapes Dataset"	https://paperswithcode.com/dataset/3d-shapes-dataset							
2576	ShapeStacks	"A simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. 
Source: ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking"	https://paperswithcode.com/dataset/shapestacks							
2577	ShEMO	"The database includes 3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state.
Source: ShEMO -- A Large-Scale Validated Database for Persian Speech Emotion Detection"	https://paperswithcode.com/dataset/shemo		Sharif Emotional Speech Database					
2578	SHIDC-BC-Ki-67	"Benchmark for BC Ki-67 stained cell detection and further annotated classification of cells.
Source: PathoNet: Deep learning assisted evaluation of Ki-67 and tumor infiltrating lymphocytes (TILs) as prognostic factors in breast cancer; A large dataset and baseline"	https://paperswithcode.com/dataset/shidc-bc-ki-67							
2579	ShopSign	"A newly developed natural scene text dataset of Chinese shop signs in street views. 
Source: ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views"	https://paperswithcode.com/dataset/shopsign							
2580	SIDD	"SIDD is an image denoising dataset containing 30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras. Ground truth images are provided along with the noisy images.
Source: A High-Quality Denoising Dataset for Smartphone Cameras"	https://paperswithcode.com/dataset/sidd		Smartphone Image Denoising Dataset					
2581	SidechainNet	"SidechainNet is a protein structure prediction dataset that directly extends ProteinNet. Specifically, SidechainNet adds measurements for protein angles and coordinates that describe the complete, all-atom protein structure (backbone and sidechain, excluding hydrogens) instead of the protein backbone alone.
Source: https://github.com/jonathanking/sidechainnet"	https://paperswithcode.com/dataset/sidechainnet							
2582	SIDOD	"SIDOD is a new, publicly-available image dataset generated by the NVIDIA Deep Learning Data Synthesizer intended for use in object detection, pose estimation, and tracking applications. This dataset contains 144k stereo image pairs that synthetically combine 18 camera viewpoints of three photorealistic virtual environments with up to 10 objects (chosen randomly from the 21 object models of the YCB dataset) and flying distractors. 
Source: SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with Distractors
Image Source: https://research.nvidia.com/publication/2019-06_SIDOD%3A-A-Synthetic"	https://paperswithcode.com/dataset/sidod							
2583	Simitate	"Simitate is a hybrid benchmarking suite targeting the evaluation of approaches for imitation learning. It consists on a dataset containing 1938 sequences where humans perform daily activities in a realistic environment. The dataset is strongly coupled with an integration into a simulator. RGB and depth streams with a resolution of 960×540 at 30Hz and accurate ground truth poses for the demonstrator's hand, as well as the object in 6 DOF at 120Hz are provided. Along with the dataset the 3D model of the used environment and labelled object images are also provided.
Source: https://arxiv.org/abs/1905.06002
Image Source: https://github.com/raphaelmemmesheimer/simitate"	https://paperswithcode.com/dataset/simitate							
2584	SIMMC	"Situated Interactive MultiModal Conversations (SIMMC) is the task of taking multimodal actions grounded in a co-evolving multimodal input content in addition to the dialog history. This dataset contains two SIMMC datasets totalling ~13K human-human dialogs (~169K utterances) using a multimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a) furniture (grounded in a shared virtual environment) and (b) fashion (grounded in an evolving set of images).
Source: https://github.com/facebookresearch/simmc
Image Source: https://github.com/facebookresearch/simmc"	https://paperswithcode.com/dataset/simmc		Situated and Interactive Multimodal Conversations					
2585	simply-CLEVR	"The simply-CLEVR dataset aims to provide a benchmark dataset that can be used for transparent quantitative evaluation of explanation methods (aka heatmaps/XAI methods).
It is made of simple Visual Question Answering (VQA) questions, which are derived from the original CLEVR task, and where each question is accompanied by two Ground Truth Masks that serve as a basis for evaluating explanations on the input image.
Source: https://github.com/ahmedmagdiosman/simply-clevr-dataset
Image Source: https://github.com/ahmedmagdiosman/simply-clevr-dataset"	https://paperswithcode.com/dataset/simply-clevr							
2586	SIS	"Comprises of 400 naturalistic usages of literature-informed verbs spanning the spectrum of symmetry-asymmetry.
Source: Inferring symmetry in natural language"	https://paperswithcode.com/dataset/sis		Symmetry inference sentence					
2587	SI-SCORE	"A synthetic dataset uses for a systematic analysis across common factors of variation.
Source: On Robustness and Transferability of Convolutional Neural Networks"	https://paperswithcode.com/dataset/si-score		Synthetic Interventions on Scenes for Robustness Evaluation					
2588	SIZER	"Dataset of clothing size variation which includes  different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. 
Source: SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing"	https://paperswithcode.com/dataset/sizer							
2589	SketchGraphs	"SketchGraphs is a dataset of 15 million sketches extracted from real-world CAD models intended to facilitate research in both ML-aided design and geometric program induction.
Each sketch is represented as a geometric constraint graph where edges denote designer-imposed geometric relationships between primitives, the nodes of the graph.
Source: https://github.com/PrincetonLIPS/SketchGraphs
Image Source: https://github.com/PrincetonLIPS/SketchGraphs"	https://paperswithcode.com/dataset/sketchgraphs							
2590	ShoeV2	"ShoeV2 is a dataset of 2,000 photos and 6648 sketches of shoes. The dataset is designed for fine-grained sketch-based image retrieval.
Source: Sketch Me That Shoe"	https://paperswithcode.com/dataset/sketch-me-that-shoe		ShoeV2					
2591	Skill2vec	"Collects a huge number of job descriptions from Dice.com - one of the most popular career website about Tech jobs in USA. From these job descriptions, skills are extracted for each one by using skills dictionary. Now, the dataset is presented by a list of collections of skills based on job descriptions. After crawling, there are a total of 5GB with more than 1,400,000 job descriptions. From these data, skills are extracted and performed as a list of skills in the same context, the context here includes skills in the same job description.
Source: Skill2vec: Machine Learning Approach for Determining the Relevant Skills from Job Description"	https://paperswithcode.com/dataset/skill2vec							
2592	SKU110K-R	"SKU110K-R is a dataset relabeled with oriented bounding boxes based on SKU110K. It is focused on evaluating oriented and densely packed object detection.
Source: https://github.com/Anymake/DRN_CVPR2020
Image Source: https://github.com/Anymake/DRN_CVPR2020"	https://paperswithcode.com/dataset/sku110k-r							
2593	SlowFlow	"SlowFlow is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur.
Source: Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data"	https://paperswithcode.com/dataset/slowflow							
2594	SLURP	"A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets.
Source: SLURP: A Spoken Language Understanding Resource Package"	https://paperswithcode.com/dataset/slurp		Spoken Language Understanding Resource Package					
2595	SMHD	"A novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users.
Source: SMHD: A Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions"	https://paperswithcode.com/dataset/smhd		Self-reported Mental Health Diagnoses					
2596	SmokEng	"SmokEng is a dataset of 3144 tweets, which are selected based on the presence of colloquial slang related to smoking and analyze it based on the semantics of the tweet. 
Source: SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media Text"	https://paperswithcode.com/dataset/smokeng							
2597	SMS Spam Collection Data Set	"This corpus has been collected from free or free for research sources at the Internet:

A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages.
A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available.
A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis.
the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages.

Source: SMS Spam Collection Data Set"	https://paperswithcode.com/dataset/sms-spam-collection-data-set							
2598	SMS-WSJ	"Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases this one considers all WSJ0+1 utterances and takes care of strictly separating the speaker sets present in the training, validation and test sets. 
Source: SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition"	https://paperswithcode.com/dataset/sms-wsj		Spatialized Multi-Speaker Wall Street Journal					
2599	SNLI-VE	"Visual Entailment (VE) consists of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. SNLI-VE is a dataset for VE which is based on the Stanford Natural Language Inference corpus and Flickr30k dataset.
Source: https://github.com/necla-ml/SNLI-VE"	https://paperswithcode.com/dataset/snli-ve							
2600	So2Sat LCZ42	"So2Sat LCZ42 consists of local climate zone (LCZ) labels of about half a million Sentinel-1 and Sentinel-2 image patches in 42 urban agglomerations (plus 10 additional smaller areas) across the globe. This dataset was labeled by 15 domain experts following a carefully designed labeling work flow and evaluation process over a period of six months. 
Source: So2Sat LCZ42: A Benchmark Dataset for Global Local Climate Zones Classification
Image Source: Zhu et al"	https://paperswithcode.com/dataset/so2sat-lcz42							
2601	SOBA	"A new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individual labeled masks.
Source: Instance Shadow Detection"	https://paperswithcode.com/dataset/soba		Shadow-OBject Association					
2602	SoccerData	"A dataset of 4562 images of which 4152 images contain a soccer ball.
Source: Utilizing Temporal Information in Deep Convolutional Network for Efficient Soccer Ball Detection and Tracking"	https://paperswithcode.com/dataset/soccerdata							
2603	SoccerDB	"Comprises of 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. 
Source: SoccerDB: A Large-Scale Database for Comprehensive Video Understanding"	https://paperswithcode.com/dataset/soccerdb							
2604	SoccerNet	"A benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). 
Source: SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos"	https://paperswithcode.com/dataset/soccernet							
2605	SoccerNet-v2	"A novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production.
Source: SoccerNet-v2 : A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos"	https://paperswithcode.com/dataset/soccernet-v2							
2606	Social-IQ	"Social-IQ is an unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies. By providing a rich source of open-ended questions and answers, Social-IQ opens the door to explainable social intelligence. The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ contains 1,250 natural in-the-wild social situations, 7,500 questions and 52,500 correct and incorrect answers. 
Source: Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence"	https://paperswithcode.com/dataset/social-iq							
2607	SMM4H	"Social Media Mining for Health (SMM4H) Shared Task is a massive data source for biomedical and public health applications.
Source: SMM4H"	https://paperswithcode.com/dataset/smm4h		Social Media Mining for Health Shared Task					
2608	SoloDance	"A large-scale HVMT dataset named SoloDance.
Source: C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer"	https://paperswithcode.com/dataset/solodance							
2609	Some Like it Hoax	"Some Like it Hoax is a fake news detection dataset consisting of 15,500 Facebook posts and 909,236 users.
Source: Some Like it Hoax: Automated Fake News Detection in Social Networks"	https://paperswithcode.com/dataset/some-like-it-hoax							
2610	SONYC-UST-V2	"A dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the ""Sounds of New York City"" (SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. 
Source: SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context"	https://paperswithcode.com/dataset/sonyc-ust-v2							
2611	SoyCultivarVein	"The SoyCultivarVein dataset is a publicly available dataset, which comprises 100 categories (cultivars) with 6 samples (leaf images) in each cultivar and thus has a total number of 100×6 = 600 images (Yu et al. 2019). The leaves in the SoyCultivarVein dataset are highly similar due to the fact that they all belong to the same species, making it a new and challenging dataset for the artificial intelligence and pattern analysis research community.
Source: Patchy Image Structure Classification Using Multi-Orientation Region Transform"	https://paperswithcode.com/dataset/soycultivarvein							
2612	SP-10K	"A large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English.
Source: SP-10K: A Large-scale Evaluation Set for Selectional Preference Acquisition"	https://paperswithcode.com/dataset/sp-10k							
2613	SpaceNet MVOI	"An open source Multi-View Overhead Imagery dataset with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance.
Source: SpaceNet MVOI: a Multi-View Overhead Imagery Dataset"	https://paperswithcode.com/dataset/spacenet-mvoi		SpaceNet Multi-View Overhead Imagery Dataset					
2614	Spaceship Dataset	"The Spaceship dataset is a dataset for evaluating agents’ ability to learn to solve a class of physics-based tasks. The tasks consist on a spaceship that has to reach a the mothership in 11 steps, in an environment where static planets exert gravitational forces on the spaceship, which induce complex non-linear dynamics on the motion over the 11 steps.
Source: https://arxiv.org/pdf/1705.02670.pdf"	https://paperswithcode.com/dataset/spaceship-dataset							
2615	SPair-71k	"SPair-71k contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations. 
Source: SPair-71k: A Large-scale Benchmark for Semantic Correspondence
Image Source: http://cvlab.postech.ac.kr/research/SPair-71k/"	https://paperswithcode.com/dataset/spair-71k							
2616	SPARE3D	"Contains three types of 2D-3D reasoning tasks on view consistency, camera pose, and shape generation, with increasing difficulty. 
Source: SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings"	https://paperswithcode.com/dataset/spare3d							
2617	SpatialSense Benchmark	"SpatialSense Benchmark is a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. 
Source: SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition"	https://paperswithcode.com/dataset/spatialsense-benchmark							
2618	SpeakingFaces	"SpeakingFaces is a publicly-available large-scale dataset developed to support multimodal machine learning research in contexts that utilize a combination of thermal, visual, and audio data streams; examples include human-computer interaction (HCI), biometric authentication, recognition systems, domain transfer, and speech recognition. SpeakingFaces is comprised of well-aligned high-resolution thermal and visual spectra image streams of fully-framed faces synchronized with audio recordings of each subject speaking approximately 100 imperative phrases.
Source: SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with Visual and Thermal Video Streams"	https://paperswithcode.com/dataset/speakingfaces							
2619	SpectroVision	"SpectroVision is a dataset of 14,400 high resolution texture images and spectral measurements collected from a PR2 mobile manipulator that interacted with 144 household objects from eight material categories.
Source: https://github.com/Healthcare-Robotics/spectrovision
Image Source: https://github.com/Healthcare-Robotics/spectrovision"	https://paperswithcode.com/dataset/spectrovision							
2620	SPEECH-COCO	"SPEECH-COCO contains speech captions that are generated using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) paired with images. 
Source: SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO Data Set"	https://paperswithcode.com/dataset/speech-coco							
2621	SPIRS	"A  first-of-its-kind large dataset of sarcastic/non-sarcastic tweets with high-quality labels and extra features: (1) sarcasm perspective labels (2) new contextual features. The dataset is expected to advance sarcasm detection research. 
Source: Reactive Supervision: A New Method for Collecting Sarcasm Data"	https://paperswithcode.com/dataset/spirs							
2622	SPLASH	"A dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback.
Source: Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback"	https://paperswithcode.com/dataset/splash							
2623	Spoken-SQuAD	"In SpokenSQuAD, the document is in spoken form, the input question is in the form of text and the answer to each question is always a span in the document. The following procedures were used to generate spoken documents from the original SQuAD dataset. First, the Google text-to-speech system was used to generate the spoken version of the articles in SQuAD. Then CMU Sphinx was sued to generate the corresponding ASR transcriptions. The SQuAD training set was used to generate the training set of Spoken SQuAD, and SQuAD development set was used to generate the testing set for Spoken SQuAD. If the answer of a question did not exist in the ASR transcriptions of the associated article, the question-answer pair was removed from the dataset because these examples are too difficult for listening comprehension machine at this stage.
Source: https://github.com/chiahsuan156/Spoken-SQuAD
Image Source: https://github.com/chiahsuan156/Spoken-SQuAD"	https://paperswithcode.com/dataset/spoken-squad							
2624	Spotify Podcast	"A set of approximately 100K podcast episodes comprised of raw audio files along with accompanying ASR transcripts. This represents over 47,000 hours of transcribed audio, and is an order of magnitude larger than previous speech-to-text corpora. 
Source: The Spotify Podcast Dataset"	https://paperswithcode.com/dataset/spotify-podcast							
2625	SQuAD-es	"Stanford Question Answering Dataset (SQuAD) into Spanish.
Source: SQuAD-es"	https://paperswithcode.com/dataset/squad-es							
2626	SQuAD-it	"SQuAD-it is derived from the SQuAD dataset and it is obtained through semi-automatic translation of the SQuAD dataset into Italian. It represents a large-scale dataset for open question answering processes on factoid questions in Italian. The dataset contains more than 60,000 question/answer pairs derived from the original English dataset.
Source: SQuAD-it"	https://paperswithcode.com/dataset/squad-it							
2627	SQuAD-shifts	"Provides four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. 
Source: The Effect of Natural Distribution Shift on Question Answering Models"	https://paperswithcode.com/dataset/squad-shifts							
2628	SQUID	"A dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images.
Source: Underwater Single Image Color Restoration Using Haze-Lines and a New Quantitative Dataset"	https://paperswithcode.com/dataset/squid		Stereo Quantitative Underwater Image Dataset					
2629	STAIR Actions Captions	"A large-scale Japanese video caption dataset consisting of 79,822 videos and 399,233 captions. Each caption in the dataset describes a video in the form of ""who does what and where.""
Source: Video Caption Dataset for Describing Human Actions in Japanese"	https://paperswithcode.com/dataset/stair-actions-captions							
2630	Standardized Project Gutenberg Corpus	"The Standardized Project Gutenberg Corpus (SPGC) is an open science approach to a curated version of the complete PG data containing more than 50,000 books and more than 3×109 word-tokens.
Source: https://arxiv.org/abs/1812.08092"	https://paperswithcode.com/dataset/standardized-project-gutenberg-corpus							
2631	StanfordExtra	"An 'in the wild' dataset of 20,580 dog images for which 2D joint and silhouette annotations were collected.
Source: Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop"	https://paperswithcode.com/dataset/stanfordextra							
2632	StaQC	"StaQC (Stack Overflow Question-Code pairs) is a large dataset of around 148K Python and 120K SQL domain question-code pairs, which are automatically mined from StackOverflow.
Source: https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset
Image Source: https://arxiv.org/pdf/1803.09371v1.pdf"	https://paperswithcode.com/dataset/staqc							
2633	STAR	"A schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog.
Source: STAR: A Schema-Guided Dialog Dataset for Transfer Learning"	https://paperswithcode.com/dataset/star							
2634	StereoMSI	"StereoMSI comprises of 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the PIRM2018 challenge.
Source: PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study
Image Source: Shoeiby et al"	https://paperswithcode.com/dataset/stereomsi							
2635	stickerchart	"The Stickerchat dataset is a large-scale real-world dialog dataset with stickers which contains 340K multi-turn dialog and sticker pairs.
Source: https://arxiv.org/abs/2003.04679"	https://paperswithcode.com/dataset/stickerchart							
2636	Store dataset	"The Store Dataset is a dataset for estimating 3D poses of multiple humans in real-time. It is captured inside two kinds of simulated stores with 12 and 28 cameras, respectively.
Source: https://arxiv.org/abs/2003.03972"	https://paperswithcode.com/dataset/store-dataset							
2637	Story Commonsense	"Story Commonsense is a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.
Source: Modeling Naive Psychology of Characters in Simple Commonsense Stories"	https://paperswithcode.com/dataset/story-commonsense							
2638	Stream-51	"A new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. 
Source: Stream-51: Streaming Classification and Novelty Detection from Videos"	https://paperswithcode.com/dataset/stream-51							
2639	Street Dataset	"A real-world image dataset that contains more than 900 images generated from 26 street cameras and 7 object categories annotated with detailed bounding box. The data distribution is non-IID and unbalanced, reflecting the characteristic real-world federated learning scenarios.
Source: Real-World Image Datasets for Federated Learning"	https://paperswithcode.com/dataset/street-dataset							
2640	Exact Street2Shop	"A dataset containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos.
Source: Where to Buy It: Matching Street Clothing Photos in Online Shops"	https://paperswithcode.com/dataset/exact-street2shop							
2641	StreetHazards	"StreetHazards is a synthetic dataset for anomaly detection, created by inserting a diverse array of foreign objects into driving scenes and re-render the scenes with these novel objects.
Source: Scaling Out-of-Distribution Detection for Real-World Settings"	https://paperswithcode.com/dataset/streethazards	25/11/2019						
2642	StreetLearn	"An interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task. 
Source: The StreetLearn Environment and Dataset"	https://paperswithcode.com/dataset/streetlearn							
2643	Street View Image, Pose, and 3D Cities Dataset	"A large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information.
Source: Generic 3D Representation via Pose Estimation and Matching"	https://paperswithcode.com/dataset/street-view-image-pose-and-3d-cities-dataset							
2644	Structured3D	"Structured3D is a large-scale photo-realistic dataset containing 3.5K house designs (a) created by professional designers with a variety of ground truth 3D structure annotations (b) and generate photo-realistic 2D images (c).
The dataset consists of rendering images and corresponding ground truth annotations (e.g., semantic, albedo, depth, surface normal, layout) under different lighting and furniture configurations.
Source: https://github.com/bertjiazheng/Structured3D
Image Source: https://github.com/bertjiazheng/Structured3D"	https://paperswithcode.com/dataset/structured3d							
2645	ST-VQA	"ST-VQA aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. 
Source: Scene Text Visual Question Answering"	https://paperswithcode.com/dataset/st-vqa							
2646	SubEdits	"SubEdits is a human-annnoated post-editing dataset of neural machine translation outputs, compiled from in-house NMT outputs and human post-edits of subtitles form Rakuten Viki. It is collected from English-German annotations and contains 160k triplets.
Source: https://github.com/shamilcm/pedra
Image Source: Chollampatt et al"	https://paperswithcode.com/dataset/subedits							
2647	SubjQA	"SubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as ""How much does this product weigh?"" is a factual question (i.e., low subjectivity), while ""Is this easy to use?"" is a subjective question (i.e., high subjectivity).
Source: https://github.com/megagonlabs/SubjQA"	https://paperswithcode.com/dataset/subjqa							
2648	Surveillance Camera Fight Dataset	"The dataset is collected from the Youtube videos that contains fight instances in it. Also, some non-fight sequences from regular surveillance camera videos are included.
* There are 300 videos in total as 150 fight + 150 non-fight
* Videos are 2-second long
* Only the fight related parts are included in the samples
Source: https://github.com/sayibet/fight-detection-surv-dataset
Image Source: https://github.com/sayibet/fight-detection-surv-dataset"	https://paperswithcode.com/dataset/surveillance-camera-fight-dataset							
2649	SuspectGuilt Corpus	"A corpus of annotated crime stories from English-language newspapers in the U.S. For SuspectGuilt, annotators read short crime articles and provided text-level ratings concerning the guilt of the main suspect as well as span-level annotations indicating which parts of the story they felt most influenced their ratings. SuspectGuilt thus provides a rich picture of how linguistic choices affect subjective guilt judgments.
Source: Modeling Subjective Assessments of Guilt in Newspaper Crime Narratives"	https://paperswithcode.com/dataset/suspectguilt-corpus							
2650	SVD	"SVD is a large-scale short video dataset, which contains over 500,000 short videos collected from http://www.douyin.com and over 30,000 labeled pairs of near-duplicate videos.
Source: SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval"	https://paperswithcode.com/dataset/svd	01/10/2019	Short Video Dataset					
2651	SVIRO	"Contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification. 
Source: SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and Benchmark"	https://paperswithcode.com/dataset/sviro		Synthetic Vehicle Interior Rear Seat Occupancy Dataset					
2652	SWAX	"Comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. 
Source: The SWAX Benchmark: Attacking Biometric Systems with Wax Figures"	https://paperswithcode.com/dataset/swax		Sense Wax Attack dataset					
2653	SweetRS	"Uses a  platform with 77 candies and sweets to rank. Over 2000 users submitted over 44000 grades resulting in a matrix with 28% coverage.
Source: SweetRS: Dataset for a recommender systems of sweets"	https://paperswithcode.com/dataset/sweetrs							
2654	Swiss3DCities	"Swiss3DCities is a dataset that is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras.
Source: Semantic Segmentation on Swiss3DCities: A Benchmark Study on Aerial Photogrammetric 3D Pointcloud Dataset
Image Source: Can et al"	https://paperswithcode.com/dataset/swiss3dcities							
2655	Synscapes	"Synscapes is a synthetic dataset for street scene parsing created using photorealistic rendering techniques, and show state-of-the-art results for training and validation as well as new types of analysis. 
Source: Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing
Image Source: https://7dlabs.com/synscapes-overview"	https://paperswithcode.com/dataset/synscapes							
2656	SynthCity	"SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Every point is assigned a label from one of nine categories.
Source: SynthCity: A large scale synthetic point cloud
Image Source: http://www.synthcity.xyz/"	https://paperswithcode.com/dataset/synthcity							
2657	Synthetic Human Model Dataset	"A synthetic dataset for evaluating non-rigid 3D human reconstruction based on conventional RGB-D cameras. The dataset consist of seven motion sequences of a single human model. 
Source: Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction"	https://paperswithcode.com/dataset/synthetic-human-model-dataset							
2658	Synthetic Keystroke	"This dataset is a large-scale synthetic dataset to simulate the attack scenario for a keystroke inference attack.
Source: https://arxiv.org/abs/2009.05796"	https://paperswithcode.com/dataset/synthetic-keystroke							
2659	SYNTHIA-AL	"Specially designed to evaluate active learning for video object detection in road scenes. 
Source: Temporal Coherence for Active Learning in Videos"	https://paperswithcode.com/dataset/synthia-al							
2660	Synthinel-1	"Synthinel-1 is a collection of synthetic overhead imagery with full pixel-wise building segmentation labels.
Source: https://github.com/timqqt/Synthinel
Image Source: https://github.com/timqqt/Synthinel"	https://paperswithcode.com/dataset/synthinel-1							
2661	SYSU-30k	"SYSU-30k contains 30k categories of persons, which is about 20 times larger than CUHK03 (1.3k categories) and Market1501 (1.5k categories), and 30 times larger than ImageNet (1k categories). SYSU-30k contains 29,606,918 images. Moreover, SYSU-30k provides not only a large platform for the weakly supervised ReID problem but also a more challenging test set that is consistent with the realistic setting for standard evaluation.
Source: https://github.com/wanggrun/SYSU-30k
Image Source: https://github.com/wanggrun/SYSU-30k"	https://paperswithcode.com/dataset/sysu-30k							
2662	SYSU-CEUS	"The SYSU-CEUS dataset consists of three types of Focal liver lesions (FLLs): 186 HCC instances, 109 HEM instances and 58 FNH instances (i.e.,186 malignant instances and 167 benign instances).
This dataset is collected from the First Affiliated Hospital, Sun Yat-sen University. The equipment used was Aplio SSA-770A (Toshiba Medical System).
All these instances with resolution 768*576 were taken from different patients, with large variations in appearance and enhancement patterns (e.g. sizes, contrasts, shapes and locations) of the FLLs.
Source: https://github.com/lemondan/Focal-liver-lesions-dataset-in-CEUS"	https://paperswithcode.com/dataset/sysu-ceus							
2663	TACO	"TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format.
Source: https://github.com/pedropro/TACO
Image Source: https://github.com/pedropro/TACO"	https://paperswithcode.com/dataset/taco							
2664	TACoS Multi-Level Corpus	"Augments the video-description dataset TACoS with short and single sentence descriptions.
Source: Coherent Multi-Sentence Video Description with Variable Level of Detail"	https://paperswithcode.com/dataset/tacos-multi-level-corpus							
2665	Talk2Car	"The Talk2Car dataset finds itself at the intersection of various research domains, promoting the development of cross-disciplinary solutions for improving the state-of-the-art in grounding natural language into visual space. The annotations were gathered with the following aspects in mind:
Free-form high quality natural language commands, that stimulate the development of solutions that can operate in the wild.
A realistic task setting. Specifically, the authors consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language.
The Talk2Car dataset was build on top of the nuScenes dataset to include an extensive suite of sensor modalities, i.e. semantic maps, GPS, LIDAR, RADAR and 360-degree RGB images annotated with 3D bounding boxes. Such variety of input modalities sets the object referral task on the Talk2Car dataset apart from related challenges, where additional sensor modalities are generally missing.
Source: https://talk2car.github.io/
Image Source: https://github.com/talk2car/Talk2Car"	https://paperswithcode.com/dataset/talk2car							
2666	Talk2Nav	"Talk2Nav is a large-scale dataset with verbal navigation instructions.
Source: Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory
Image Source: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html"	https://paperswithcode.com/dataset/talk2nav							
2667	TalkDown	"TalkDown is a labelled dataset for condescension detection in context. The dataset is derived from Reddit, a set of online communities that is diverse in content and tone. The dataset is built from COMMENT and REPLY pairs in which the REPLY targets a specific quoted span (QUOTED) in the COMMENT as being condescending. The dataset contains 3,255 positive (condescend) samples and 3,255 negative ones.
Source: https://arxiv.org/pdf/1909.11272.pdf"	https://paperswithcode.com/dataset/talkdown							
2668	Talk the Walk	"Talk The Walk is a large-scale dialogue dataset grounded in
action and perception. The task involves two agents (a “guide” and a “tourist”)
that communicate via natural language in order to achieve a common goal: having
the tourist navigate to a given target location."	https://paperswithcode.com/dataset/talk-the-walk							
2669	TAO	"TAO is a federated dataset for Tracking Any Object, containing 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. A bottom-up approach was used for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. 
The dataset was annotated by labelling tracks for objects that move at any point in the video, and giving names to them post factum. 
Source: TAO: A Large-Scale Benchmark for Tracking Any Object"	https://paperswithcode.com/dataset/tao		Tracking Any Object Dataset					
2670	TaoDescribe	"The TaoDescribe dataset contains 2,129,187 product titles and descriptions in Chinese.
Source: https://github.com/qibinc/KOBE"	https://paperswithcode.com/dataset/taodescribe							
2671	TaPaCo	"TaPaCo is a freely available paraphrase corpus for 73 languages extracted from the Tatoeba database.
Source: TaPaCo: A Corpus of Sentential Paraphrases for 73 Languages"	https://paperswithcode.com/dataset/tapaco							
2672	TArC	"A morpho-syntactically annotated Tunisian Arabish Corpus (TArC).
Source: TArC: Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus"	https://paperswithcode.com/dataset/tarc	20/03/2020						
2673	Taskmaster-2	The Taskmaster-2 dataset consists of 17,289 dialogs in seven domains: restaurants (3276), food ordering (1050), movies (3047), hotels (2355), flights (2481), music (1602), and sports (3478).	https://paperswithcode.com/dataset/taskmaster-2							
2674	Tasty Videos	"A collection of 2511 recipes for zero-shot learning, recognition and anticipation.
Source: Zero-Shot Anticipation for Instructional Activities"	https://paperswithcode.com/dataset/tasty-videos							
2675	TB-Places	"TB-Places is a data set of garden images for testing algorithms for visual place recognition. It contains images with ground truth camera pose recorded in two real gardens at different times, with a total of four different recording sessions, with varying light conditions.
Source: TB-Places"	https://paperswithcode.com/dataset/tb-places							
2676	TCG	"The TCG dataset is used to evaluate Traffic Control Gesture recognition for autonomous driving. The dataset is based on 3D body skeleton input to perform traffic control gesture classification on every time step. The dataset consists of 250 sequences from several actors, ranging from 16 to 90 seconds per sequence.
Source: https://arxiv.org/pdf/2007.16072.pdf
Image Source: https://github.com/againerju/tcg_recognition"	https://paperswithcode.com/dataset/tcg		Traffic Control Gesture					
2677	TCIA Test & Validation Radiotherapy CT Planning Scan	"A dataset of 663 deidentified computed tomography (CT) scans acquired in routine clinical practice and with both segmentations taken from clinical practice and segmentations.
Source: Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy"	https://paperswithcode.com/dataset/tcia-test-validation-radiotherapy-ct-planning							
2678	TE141K	"A new text effects dataset with 141,081 text effect/glyph pairs in total. The dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. 
Source: TE141K: Artistic Text Benchmark for Text Effect Transfer"	https://paperswithcode.com/dataset/te141k							
2679	Tencent ML-Images	"Tencent ML-Images is a large open-source multi-label image database, including 17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories.
Source: Tencent ML-Images"	https://paperswithcode.com/dataset/tencent-ml-images							
2680	TextCaps	"Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. 
Source: TextCaps: a Dataset for Image Captioning with Reading Comprehension"	https://paperswithcode.com/dataset/textcaps							
2681	TextSeg	"TextSeg is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions.
Source: https://github.com/SHI-Labs/Rethinking-Text-Segmentation
Image Source: https://github.com/SHI-Labs/Rethinking-Text-Segmentation"	https://paperswithcode.com/dataset/textseg							
2682	Textual Visual Semantic Dataset	"Extends the COCO-text [Veit et al. 2016] with information about the scene (such as objects and places appearing in the image) to enable researchers to include semantic relations between texts and scene in their Text Spotting systems, and to offer a common framework for such approaches.
Source: Textual Visual Semantic Dataset for Text Spotting"	https://paperswithcode.com/dataset/textual-visual-semantic-dataset							
2683	TextVQA	"TextVQA is a dataset to benchmark visual reasoning based on text in images.
TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.
Statistics
* 28,408 images from OpenImages
* 45,336 questions
* 453,360 ground truth answers"	https://paperswithcode.com/dataset/textvqa							
2684	TextWorld KG	"TextWorld KG is a dynamic Knowledge Graph (KG) extraction dataset. It is based on a set of text-based games generated using. That framework allows to extract the underlying partial KG for every state, i.e., the subgraph that represents the agent’s partial knowledge of the world – what it has observed so far. All games share the same overarching theme: the agent finds itself hungry in a simple modern house with the goal of gathering ingredients and cooking a meal.
Source: https://arxiv.org/abs/1910.09532"	https://paperswithcode.com/dataset/textworld-kg							
2685	TextZoom	"TextZoom is a super-resolution dataset that consists of paired Low Resolution – High Resolution scene text images. The images are captured by cameras with different focal length in the wild.
Source: https://github.com/JasonBoy1/TextZoom
Image Source: https://github.com/JasonBoy1/TextZoom"	https://paperswithcode.com/dataset/textzoom							
2686	Texygen Platform	"Texygen is a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and facilitate the sharing of fine-tuned open-source implementations among researchers for their work. As a consequence, this would help in improving the reproductivity and reliability of future research work in text generation.
Source: Texygen Platform"	https://paperswithcode.com/dataset/texygen-platform							
2687	TG-ReDial	"TG-ReDial is a a topic-guided conversational recommendation dataset for research on conversational/interactive recommender systems.
Source: https://github.com/RUCAIBox/TG-ReDial
Image Source: https://github.com/RUCAIBox/TG-ReDial"	https://paperswithcode.com/dataset/tg-redial							
2688	THCHS-30	"THCHS-30 is a free Chinese speech database
THCHS-30 that can be used to build a full-fledged
Chinese speech recognition system.
Source: THCHS-30 : A Free Chinese Speech Corpus"	https://paperswithcode.com/dataset/thchs-30							
2689	The RobotriX	"Photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. 
Source: The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions"	https://paperswithcode.com/dataset/the-robotrix							
2690	VLOG Dataset	"A large collection of interaction-rich video data which are annotated and analyzed.
Source: From Lifestyle Vlogs to Everyday Interactions"	https://paperswithcode.com/dataset/vlog-dataset							
2691	ThirdToFirst	"Two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos.
Source: From Third Person to First Person: Dataset and Baselines for Synthesis and Retrieval"	https://paperswithcode.com/dataset/thirdtofirst							
2692	TicketTalk	"A movie ticketing dialog dataset with 23,789 annotated conversations. The movie ticketing conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns. In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to ""make sense"" 86.5 percent of the time, almost the same as human responses in the same contexts.
Source: TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems"	https://paperswithcode.com/dataset/tickettalk							
2693	TikTok Comments	"TikTok Comments is a domain specific lexicon based on TikTok comments dataset.
<data>"	https://paperswithcode.com/dataset/tiktok-comments							
2694	Tilde MODEL Corpus	"Tilde MODEL Corpus is a multilingual corpora for European languages – particularly focused on the smaller languages. The collected resources have been cleaned, aligned, and formatted into a corpora standard TMX format useable for developing new Language technology products and services.
It contains over 10M segments of multilingual open data.
The data has been collected from sites allowing free use and reuse of its content, as well as from Public Sector web sites.
Source: Tilde MODEL - Multilingual Open Data for EU Languages"	https://paperswithcode.com/dataset/tilde-model-corpus		Tilde Multilingual Open Data for European Languages					
2695	Tilt-RGBD	"Includes considerable roll and pitch camera motion.
Source: Surface Normal Estimation of Tilted Images via Spatial Rectifier"	https://paperswithcode.com/dataset/tilt-rgbd							
2696	Time-Lapse Hyperspectral Radiance Images	"These sequences of hyperspectral radiance images have been taken from scenes undergoing natural illumination changes. In each scene, hyperspectral images were acquired at about 1-hour intervals. 
Source: Time-Lapse Hyperspectral Radiance Images of Natural Scenes 2015"	https://paperswithcode.com/dataset/time-lapse-hyperspectral-radiance-images							
2697	TimeTravel	"TimeTravel contains 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. 
Source: Counterfactual Story Reasoning and Generation"	https://paperswithcode.com/dataset/timetravel							
2698	TIM-Tremor	"Contains static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data.
Source: Hand-tremor frequency estimation in videos"	https://paperswithcode.com/dataset/tim-tremor		Technology in Motion Tremor					
2699	HAKE	HAKE is built upon existing activity datasets and provides human body part level atomic action labels (Part States).	https://paperswithcode.com/dataset/hake-large	02/04/2020						
2700	TinyPerson	"TinyPerson is a benchmark for tiny object detection in a long distance and with massive backgrounds. The images in TinyPerson are collected from the Internet. First, videos with a high resolution are collected from different websites. Second, images from the video are sampled every 50 frames. Then images with a certain repetition (homogeneity) are deleted, and the resulting images are annotated with 72,651 objects with bounding boxes by hand.
Source: https://arxiv.org/abs/1912.10664
Image Source: https://arxiv.org/pdf/1912.10664.pdf"	https://paperswithcode.com/dataset/tinyperson							
2701	TinySocial	"TinySocial is a dataset to enable research on Social Visual Question Answering.
Source: Characterizing Datasets for Social Visual Question Answering, and the New TinySocial Dataset"	https://paperswithcode.com/dataset/tinysocial							
2702	TinyVIRAT	"TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging.
Source: TinyVIRAT: Low-resolution Video Action Recognition"	https://paperswithcode.com/dataset/tinyvirat							
2703	TITAN	"TITAN consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. The dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. 
Source: TITAN: Future Forecast using Action Priors"	https://paperswithcode.com/dataset/titan							
2704	TJU-DHD	"TJU-DHD is a high-resolution dataset for object detection and pedestrian detection. The dataset contains 115,354 high-resolution images (52% images have a resolution of 1624×1200 pixels and 48% images have a resolution of at least 2,560×1,440 pixels) and 709,330 labelled objects in total with a large variance in scale and appearance.
Source: https://github.com/tjubiit/TJU-DHD
Image Source: https://github.com/tjubiit/TJU-DHD"	https://paperswithcode.com/dataset/tju-dhd							
2705	TLL	"Contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings.
Source: Totally Looks Like - How Humans Compare, Compared to Machines"	https://paperswithcode.com/dataset/tll		Totally-Looks-Like					
2706	TLP	"A new long video dataset and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking.
Source: Long-Term Visual Object Tracking Benchmark"	https://paperswithcode.com/dataset/tlp		Track Long and Prosper					
2707	TME Motorway Dataset	"The “Toyota Motor Europe (TME) Motorway Dataset” is composed by 28 clips for a total of approximately 27 minutes (30000+ frames) with vehicle annotation. Annotation was semi-automatically generated using laser-scanner data. Image sequences were selected from acquisition made in North Italian motorways in December 2011. This selection includes variable traffic situations, number of lanes, road curvature, and lighting, covering most of the conditions present in the complete acquisition.
The dataset comprises:

Image acquisition: stereo, 20 Hz frequency , 1024x768 grayscale losslessly compressed images, 32° horizontal field of view, bayer coded color information (in OpenCV use CV_BayerGB2GRAY and CV_BayerGB2BGR color conversion codes; please note that left camera was rotated upside down, convert to color/grayscale BEFORE flipping the image). A checkboard calibration sequence is made available.
Laser-scanner generated vehicle annotation and classification (car/truck).
A software evaluation toolkit (C++ source code).

Source: TME Motorway Dataset"	https://paperswithcode.com/dataset/tme-motorway-dataset							
2708	Topical-Chat	"A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.
Source: Topical-Chat"	https://paperswithcode.com/dataset/topical-chat	15/09/2019						
2709	TopLogo-10	"Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context.
Source: Deep Learning Logo Detection with Data Expansion by Synthesising Context"	https://paperswithcode.com/dataset/toplogo-10							
2710	Topology Optimization Dataset	"TOP is a synthetic dataset for topology optimization generated using Topy. The generated dataset has 10,000 objects which consist on 100 iterations of the optimization process for the problem defined on a regular 40 x 40 grid.
Source: https://arxiv.org/pdf/1709.09578.pdf
Image Source: https://github.com/ISosnovik/top"	https://paperswithcode.com/dataset/topology-optimization-dataset							
2711	Toronto-3D	"Toronto-3D is a large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of road and consists of about 78.3 million points. Point clouds has 10 attributes and classified in 8 labelled object classes.
Source: https://github.com/WeikaiTan/Toronto-3D
Image Source: https://github.com/WeikaiTan/Toronto-3D"	https://paperswithcode.com/dataset/toronto-3d							
2712	Torque	"Torque is an English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships.
Source: TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"	https://paperswithcode.com/dataset/torque							
2713	Touchdown Dataset	"Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear.
Source: https://github.com/lil-lab/touchdown
Image Source: https://github.com/lil-lab/touchdown"	https://paperswithcode.com/dataset/touchdown-dataset							
2714	Toulouse Vanishing Points Dataset	"Toulouse Vanishing Points Dataset is a public photographs database of Manhattan scenes taken with an iPad Air 1. The purpose of this dataset is the evaluation of vanishing points estimation algorithms. Its originality is the addition of Inertial Measurement Unit (IMU) data synchronized with the camera under the form of rotation matrices. Moreover, contrary to existing works which provide vanishing points of reference in the form of single points, there are computed uncertainty regions.
Source: Toulouse Vanishing Points Dataset"	https://paperswithcode.com/dataset/toulouse-vanishing-points-dataset	11/03/2015						
2715	Tour20	"Contains 140 videos with multiple human created summaries, which were acquired in a controlled experiment. 
Source: Diversity-aware Multi-Video Summarization"	https://paperswithcode.com/dataset/tour20							
2716	ToyADMOS	"ToyADMOS dataset is a machine operating sounds dataset of approximately 540 hours of normal machine operating sounds and over 12,000 samples of anomalous sounds collected with four microphones at a 48kHz sampling rate, prepared by Yuma Koizumi and members in NTT Media Intelligence Laboratories. The ToyADMOS dataset is designed for anomaly detection in machine operating sounds (ADMOS) research. It is designed for three tasks of ADMOS: product inspection (toy car), fault diagnosis for fixed machine (toy conveyor), and fault diagnosis for moving machine (toy train).
Source: https://github.com/YumaKoizumi/ToyADMOS-dataset"	https://paperswithcode.com/dataset/toyadmos							
2717	Toyota Smarthome	"A large scale dataset with daily-living activities performed in a natural manner.
Source: Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity Detection"	https://paperswithcode.com/dataset/toyota-smarthome							
2718	TPIC17	"Image dataset with about 600K Flickr photos.
Source: Sequential Prediction of Social Media Popularity with Deep Temporal Context Networks"	https://paperswithcode.com/dataset/tpic17		Temporal Popularity Image Collection					
2719	TRACT	"TRACT is a small scale manually annotated corpus for abuse classification problem.
Source: TRACT"	https://paperswithcode.com/dataset/tract		Tweets Reporting Abuse Classification Task Corpus					
2720	Traditional Chinese Landscape Painting Dataset	"This dataset consists of 2,192 high-quality traditional Chinese landscape paintings (中国山水画). All paintings are sized 512x512, from the following sources:
* Princeton University Art Museum, 362 paintings
* Harvard University Art Museum, 101 paintings
* Metropolitan Museum of Art, 428 paintings
* Smithsonian's Freer Gallery of Art, 1,301 paintings
Source: https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset
Image Source: https://github.com/alicex2020/Chinese-Landscape-Painting-Dataset"	https://paperswithcode.com/dataset/traditional-chinese-landscape-painting							
2721	CVL Traffic Signs Dataset	"A video dataset for recognising traffic signs hosted with the first IEEE Video and Image Processing (VIP) Cup within the IEEE Signal Processing Society.
Source: Challenging Environments for Traffic Sign Detection: Reliability Assessment under Inclement Conditions"	https://paperswithcode.com/dataset/cvl-traffic-signs-dataset							
2722	Trans10K	"A large-scale dataset for transparent object segmentation, named Trans10K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. 
Source: Segmenting Transparent Objects in the Wild"	https://paperswithcode.com/dataset/trans10k							
2723	Transient Biometrics Nails Dataset	"An extended version of an experimental dataset, called Transient Biometrics Nails Dataset (TBND), was created. TBND is composed of images of the right index finger. During acquisition the subject was instructed to lay her finger over a flat white surface and a simple point-and-shoot camera was used to acquire an image without the the use of a flash. No explicit instructions with respect to force applied were given and thus the results incorporate arbitrary force differences between users and capture sessions. Acquisition was thus done in a semi-controlled environment; apart from the white background and indirect lighting, the images present variation with respect to scale, focal plane and illumination. The dataset consists of three subsets, each one compromising the same 93 subjects, but varying on acquisition date. The first subset D01 consists of images acquired on the first acquisition day. The second subset D02 is composed of images acquired one day later. The third subset D30 was acquired 1 month after the first acquisition date. Given acquisition restrictions, the acquisitions of D30 have up to two days’ tolerance. This represents a massive expansion of the originally collected dataset TBND V01
Source: Transient Biometrics Nails Dataset"	https://paperswithcode.com/dataset/transient-biometrics-nails-dataset							
2724	TREK-100	"The dataset is composed of 100 video sequences densely annotated with 60K bounding boxes, 17 sequence attributes, 13 action verb attributes and 29 target object attributes. 
Source: Is First Person Vision Challenging for Object Tracking? The TREK-100 Benchmark Dataset"	https://paperswithcode.com/dataset/trek-100							
2725	T-REx	"A dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences).
Source: T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"	https://paperswithcode.com/dataset/t-rex							
2726	TriageSQL	"TriageSQL is a cross-domain text-to-SQL question intention classification benchmark that requires models to distinguish four types of unanswerable questions from answerable questions.
Source: https://github.com/chatc/TriageSQL"	https://paperswithcode.com/dataset/triagesql							
2727	TrMor2018	"A new high accuracy Turkish morphology dataset. 
Source: Morphological analysis using a sequence decoder"	https://paperswithcode.com/dataset/trmor2018							
2728	TSAC	"Tunisian Sentiment Analysis Corpus (TSAC) is a Tunisian Dialect corpus of 17.000 comments from Facebook. 
Source: Sentiment Analysis of Tunisian Dialects: Linguistic Ressources and Experiments"	https://paperswithcode.com/dataset/tsac		Tunisian Sentiment Analysis Corpus					
2729	TTPLA	"TTPLA is a public dataset which is a collection of aerial images on Transmission Towers (TTs) and Power Lines (PLs). It can be used for detection and segmentation of transmission towers and power lines. It consists of 1,100 images with the resolution of 3,840×2,160 pixels, as well as manually labelled 8,987 instances of TTs and PLs.
Source: https://github.com/r3ab/ttpla_dataset
Image Source: https://github.com/r3ab/ttpla_dataset"	https://paperswithcode.com/dataset/ttpla		Transmission Towers and Power Lines (TTPLA)					
2730	TTS-Portuguese Corpus	"The dataset has 10.5 hours from a single speaker.
Source: End-To-End Speech Synthesis Applied to Brazilian Portuguese"	https://paperswithcode.com/dataset/tts-portuguese-corpus							
2731	TUM Visual-Inertial Dataset	"A novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. 
Source: The TUM VI Benchmark for Evaluating Visual-Inertial Odometry"	https://paperswithcode.com/dataset/tum-visual-inertial-dataset							
2732	TUNIZI	"A sentiment analysis Tunisian Arabizi Dataset, collected from social networks, preprocessed for analytical studies and annotated manually by Tunisian native speakers.
Source: TUNIZI: a Tunisian Arabizi sentiment analysis Dataset"	https://paperswithcode.com/dataset/tunizi							
2733	TURBID Dataset	"The TURBID, is an open image dataset that has been generated to contribute with the underwater research area. TURBID consists in a collection of five different subsets of degraded images with its respective ground-truth.
Source: TURBID Dataset"	https://paperswithcode.com/dataset/turbid-dataset							
2734	Turing Change Point Dataset	"Specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. 
Source: An Evaluation of Change Point Detection Algorithms"	https://paperswithcode.com/dataset/turing-change-point-dataset							
2735	TuSimple Lane	"TuSimple Lane is an extension of the TuSimple dataset with 14,336 lane boundaries annotations. Each lane boundary in the dataset is annotated using 7 different classes such as “Single Dashed”, “Double Dashed” or “Single White Continuous”.
Source: https://github.com/fabvio/TuSimple-lane-classes
Image Source: Pizzati et al"	https://paperswithcode.com/dataset/tusimple-lane							
2736	TutorialBank	"TutorialBank is a publicly available dataset which aims to facilitate NLP education and research. The dataset consists of links to over 6,300 high-quality resources on NLP and related fields. The corpus’s magnitude, manual collection and focus on annotation for education in addition to research differentiates it from other corpora.
Source: TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation
Image Source: http://aan.how/browse/resources/"	https://paperswithcode.com/dataset/tutorialbank							
2737	TVC	"TV show Caption is a large-scale multimodal captioning dataset, containing 261,490 caption descriptions paired with 108,965 short video moments. TVC is unique as its captions may also describe dialogues/subtitles while the captions in the other datasets are only describing the visual content.
Source: https://tvr.cs.unc.edu/tvc.html
Image Source: https://github.com/jayleicn/TVCaption"	https://paperswithcode.com/dataset/tvc		TV show Captions					
2738	TVPR	"The TVPR (Top View Person Re-identification) dataset stores depth frames (640x480) collected using Asus Xtion Pro Live in top-view configuration. This setup choice is primarily due to the reduction of occlusions and it has also the advantage of being privacy preserving, because faces are not recorded by the camera. The use of an RGB-D camera allows to extract anthropometric features for the recognition of people passing under the camera.
Source: TVPR"	https://paperswithcode.com/dataset/tvpr		Top-View Person Re-Identification Dataset					
2739	TVSeries	"A realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances.
Source: Online Action Detection"	https://paperswithcode.com/dataset/tvseries							
2740	TweetEval	"TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks.
Source: TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification
Image Source: https://arxiv.org/pdf/2010.12421v2.pdf"	https://paperswithcode.com/dataset/tweeteval							
2741	Twitch-FIFA	"Twitch-FIFA is video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This dataset can be used to train visually-grounded dialogue models that generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history.
Source: https://github.com/ramakanth-pasunuru/video-dialogue"	https://paperswithcode.com/dataset/twitch-fifa							
2742	Twitter100k	"Twitter100k is a large-scale dataset for weakly supervised cross-media retrieval.
Source: Twitter100k: A Real-world Dataset for Weakly Supervised Cross-Media Retrieval
Image Source: https://arxiv.org/pdf/1703.06618v1.pdf"	https://paperswithcode.com/dataset/twitter100k							
2743	Twitter Conversations Dataset	"This dataset is used for the task of conversational document prediction. The dataset includes conversations that occurred between users and customer care agents in 25 organizations on the Twitter platform. Each conversation ends with a customer care agent providing a URL to a document to resolve the issue the user is facing. The task is to predict the document given a dialog context.
The train, dev and test datasets include 10000, 525 and 500 conversations respectively.
Source: https://github.com/IBM/twitter-customer-care-document-prediction
Image Source: https://arxiv.org/pdf/2010.02305v1.pdf"	https://paperswithcode.com/dataset/twitter-conversations-dataset							
2744	Twitter Cyberthreat Detection Dataset	"Twitter Cyberthreat Detection Dataset is a dataset that contains tweets from two sets of accounts related to cybersecurity. The tweets are annotated with different information such as whether they contain security-related information and named entities.
Source: https://arxiv.org/pdf/1904.01127.pdf"	https://paperswithcode.com/dataset/twitter-cyberthreat-detection-dataset							
2745	Twitter Flood	"This dataset contains two subsets of flood images from Twitter: The Harz17 dataset comprises images from tweets containing flood-related keywords during the occurrence of a flood in the Harz region in Germany in July 2017. Similarly, the Rhine18 dataset comprises images related to a flood of the river Rhine in January 2018.
Source: https://github.com/cvjena/twitter-flood-dataset"	https://paperswithcode.com/dataset/twitter-flood							
2746	TURL	"Twitter News URL Corpus is a human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification.
Source: A Continuously Growing Dataset of Sentential Paraphrases
Image Source: https://arxiv.org/pdf/1708.00391v1.pdf"	https://paperswithcode.com/dataset/twitter-news-url-corpus		Twitter News URL Corpus					
2747	TWT-16	"The TWT16 dataset contains ~30k conversations in Twitter, collected from January to June 2016.
Source: https://arxiv.org/pdf/1903.07319.pdf
Image Source: https://github.com/zengjichuan/Topic_Disc"	https://paperswithcode.com/dataset/twt-16							
2748	UAV-GESTURE	"UAV-GESTURE is a dataset for UAV control and gesture recognition. It is an outdoor recorded video dataset for UAV commanding signals with 13 gestures suitable for basic UAV navigation and command from general aircraft handling and
helicopter handling signals. It contains 119 high-definition video clips
consisting of 37,151 frames. 
Source: UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition"	https://paperswithcode.com/dataset/uav-gesture							
2749	UAVid	"UAVid is a high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. The UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. 
Source: UAVid: A Semantic Segmentation Dataset for UAV Imagery
Image Source: https://uavid.nl/"	https://paperswithcode.com/dataset/uavid							
2750	UBC3V Dataset	"~6 million synthetic depth frames for pose estimation from multiple cameras.
Source: Real-Time Human Motion Capture with Multiple Depth Cameras"	https://paperswithcode.com/dataset/ubc3v-dataset							
2751	UDC	"Ubuntu Dialogue Corpus (UDC) is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. 
Source: The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems"	https://paperswithcode.com/dataset/ubuntu-dialogue-corpus	04/02/2016	Ubuntu Dialogue Corpus					
2752	UCFRep	"The UCFRep dataset contains 526 annotated repetitive action videos. This dataset is built from the action recognition dataset UCF101.
Source: https://github.com/Xiaodomgdomg/Deep-Temporal-Repetition-Counting"	https://paperswithcode.com/dataset/ucfrep							
2753	UCLA Protest Image	"40,764 images (11,659 protest images and hard negatives) with various annotations of visual attributes and sentiments.
Source: Protest Activity Detection and Perceived Violence Estimation from Social Media Images"	https://paperswithcode.com/dataset/ucla-protest-image							
2754	UC Merced Land Use Dataset	"This is a 21 class land use image dataset meant for research purposes.
There are 100 images for each of the following classes:

agricultural
airplane
baseballdiamond
beach
buildings
chaparral
denseresidential
forest
freeway
golfcourse
harbor
intersection
mediumresidential
mobilehomepark
overpass
parkinglot
river
runway
sparseresidential
storagetanks
tenniscourt
Each image measures 256x256 pixels.

The images were manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country. The pixel resolution of this public domain imagery is 1 foot.
Source: UC Merced Land Use Dataset"	https://paperswithcode.com/dataset/uc-merced-land-use-dataset							
2755	UFDD	"Unconstrained Face Detection Dataset (UFDD) aims to fuel further research in unconstrained face detection.
Source: Pushing the Limits of Unconstrained Face Detection: a Challenge Dataset and Baseline Results
Image Source: https://ufdd.info/"	https://paperswithcode.com/dataset/ufdd		Unconstrained Face Detection Dataset					
2756	UFPR-AMR	"This dataset contains 2,000 images taken from inside a warehouse of the Energy Company of Paraná (Copel), which directly serves more than 4 million consuming units in the Brazilian state of Paraná.
The images were acquired with three different cameras and are available in the JPG format with a resolution between 2,340 × 4,160 and 3,120 × 4,160 pixels. The dataset is split into three sets: training (800 images), validation (400 images) and testing (800 images). 
Every image has the following annotations available in a text file: the camera in which the image was taken, the counter’s position (x,y,w,h) and reading, as well as the position of each digit. All counters of the dataset (regardless of meter type) have 5 digits, and thus 10,000 digits were manually annotated.
Source: Convolutional Neural Networks for Automatic Meter Reading"	https://paperswithcode.com/dataset/ufpr-amr-dataset	05/02/2019						
2757	UG^2	"Contains three difficult real-world scenarios: uncontrolled videos taken by UAVs and manned gliders, as well as controlled videos taken on the ground. Over 160,000 annotated frames forhundreds of ImageNet classes are available, which are used for baseline experiments that assess the impact of known and unknown image artifacts and other conditions on common deep learning-based object classification approaches.
Source: UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition"	https://paperswithcode.com/dataset/ug-2							
2758	UIT-ViNames	"This dataset comprises over 26,000 full names annotated with genders. 
Source: Gender Prediction Based on Vietnamese Names with Machine Learning Techniques"	https://paperswithcode.com/dataset/uit-vinames							
2759	UIT-ViNewsQA	"UIT-ViNewsQA is a new corpus for the Vietnamese language to evaluate healthcare reading comprehension models. The corpus comprises 22,057 human-generated question-answer pairs. Crowd-workers create the questions and their answers based on a collection of over 4,416 online Vietnamese healthcare news articles, where the answers comprise spans extracted from the corresponding articles. 
Source: New Vietnamese Corpus for Machine ReadingComprehension of Health News Articles"	https://paperswithcode.com/dataset/uit-vinewsqa							
2760	UIT-ViQuAD	"A new dataset for the low-resource language as Vietnamese to evaluate MRC models. This dataset comprises over 23,000 human-generated question-answer pairs based on 5,109 passages of 174 Vietnamese articles from Wikipedia. 
Source: A Vietnamese Dataset for Evaluating Machine Reading Comprehension"	https://paperswithcode.com/dataset/uit-viquad							
2761	UMC005 English-Urdu	"UMC005 English-Urdu is a parallel corpus of texts in English and Urdu language with sentence alignments. The corpus can be used for experiments with statistical machine translation.
The texts come from four different sources:

Quran
Bible
Penn Treebank (Wall Street Journal)
Emille corpus

Source: UMC005 English-Urdu"	https://paperswithcode.com/dataset/umc005-english-urdu							
2762	UMDFaces	"UMDFaces is a face dataset divided into two parts:

Still Images - 367,888 face annotations for 8,277 subjects.
Video Frames - Over 3.7 million annotated video frames from over 22,000 videos of 3100 subjects.

Part 1 - Still Images
The dataset contains 367,888 face annotations for 8,277 subjects divided into 3 batches. The annotations contain human curated bounding boxes for faces and estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network.
Part 2 - Video Frames
The second part contains 3,735,476 annotated video frames extracted from a total of 22,075 for 3,107 subjects. The annotations contain the estimated pose (yaw, pitch, and roll), locations of twenty-one keypoints, and gender information generated by a pre-trained neural network."	https://paperswithcode.com/dataset/umdfaces							
2763	UIEB	"Includes 950 real-world underwater images, 890 of which have the corresponding reference images.
Source: An Underwater Image Enhancement Benchmark Dataset and Beyond"	https://paperswithcode.com/dataset/uieb		Underwater Image Enhancement Benchmark Dataset					
2764	UniMiB SHAR	"Includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL, the age, the gender, and so on. 
Source: UniMiB SHAR: a new dataset for human activity recognition using acceleration data from smartphones"	https://paperswithcode.com/dataset/unimib-shar							
2765	United Nations Parallel Corpus	"The first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.
Source: The United Nations Parallel Corpus v1.0"	https://paperswithcode.com/dataset/united-nations-parallel-corpus							
2766	MultiUN	"The MultiUN parallel corpus is extracted from the United Nations Website , and then cleaned and converted to XML at Language Technology Lab in DFKI GmbH (LT-DFKI), Germany. The documents were published by UN from 2000 to 2009.
Source: MultiUN"	https://paperswithcode.com/dataset/multiun		Multilingual Corpus from United Nation Documents					
2767	Unite the People	Unite The People is a dataset for 3D body estimation. The images come from the Leeds Sports Pose dataset and its extended version, as well as the single person tagged people from the MPII Human Pose Dataset. The images are labeled with different types of annotations such as segmentation labels, pose or 3D.	https://paperswithcode.com/dataset/unite-the-people							
2768	UPIQ	"Contains over 4,000 images created by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets.
Source: Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality"	https://paperswithcode.com/dataset/upiq		Unified Photometric Image Quality					
2769	Urban Dict spelling variant	"Urban Dict spelling variant is a variant spelling dataset for use of NLP research in the informal domain. It consists of around 25k variant spelling pairs form UrbanDictionary.
Source: https://arxiv.org/abs/1911.04669
Image Source: https://arxiv.org/pdf/1911.04669.pdf"	https://paperswithcode.com/dataset/urban-dict-spelling-variant							
2770	Urban Environments	"The Urban Environments dataset is a dataset of 20 land use classes across 300 European cities paired with satellite imagery data.
Source: https://arxiv.org/abs/1704.02965
Image Source: https://github.com/adrianalbert/urban-environments"	https://paperswithcode.com/dataset/urban-environments-dataset							
2771	UrbanLoco	"UrbanLoco is a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers.
Source: UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes
Image Source: https://advdataset2019.wixsite.com/urbanloco"	https://paperswithcode.com/dataset/urbanloco							
2772	Bend the Truth	"""Bend the Truth"" dataset contains news in six different domains: technology, education, business, sports, politics, and entertainment. The real news included in the dataset were collected from a variety of mainstream news websites predominantly in Pakistan, India, UK, and the USA. These news channels are BBC Urdu News, CNN Urdu, Express-News, Jung News, Noway Waqat, and many other reliable news websites. The fake news included in this dataset consist of fake versions of the real news in the dataset, written by professional journalists. 
Source: Bend the Truth"	https://paperswithcode.com/dataset/bend-the-truth							
2773	Urdu Sentiment Corpus	"Consists of Urdu tweets for the sentiment analysis and polarity detection. The dataset is consisting of tweets, such that it casts a political shadow and presents a competitive environment between two separate political parties versus the government of Pakistan. Overall, the dataset is comprising over 17, 185 tokens with 52% records as positive, and 48% records as negative. 
Source: Urdu Sentiment Corpus (v1.0): Linguistic Exploration and Visualization of Labeled Dataset for Urdu Sentiment Analysis"	https://paperswithcode.com/dataset/urdu-sentiment-corpus							
2774	UR-FUNNY	"For understanding multimodal language used in expressing humor.
Source: UR-FUNNY: A Multimodal Language Dataset for Understanding Humor"	https://paperswithcode.com/dataset/ur-funny							
2775	US-4	"The US-4 is a dataset of Ultrasound (US) images. It is a video-based image dataset that contains over 23,000 high-resolution images from four US video sub-datasets, where two sub-datasets are newly collected by experienced doctors for this dataset.
Source: https://github.com/983632847/USCL
Image Source: https://github.com/983632847/USCL"	https://paperswithcode.com/dataset/us-4							
2776	UTA-RLDD	"Consists of around 30 hours of video, with contents ranging from subtle signs of drowsiness to more obvious ones.
Source: A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection"	https://paperswithcode.com/dataset/uta-rldd		University of Texas at Arlington Real-Life Drowsiness Dataset					
2777	UT Zappos50K	"UT Zappos50K is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories — shoes, sandals, slippers, and boots — followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis.
Source: UT Zappos50K"	https://paperswithcode.com/dataset/ut-zappos50k							
2778	UW IOM	"Comprises twenty individuals picking up and placing objects of varying weights to and from cabinet and table locations at various heights.
Source: Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks"	https://paperswithcode.com/dataset/uw-iom		University of Washington Indoor Object Manipulation					
2779	V2C	Contains ~9K videos of human agents performing various actions, annotated with 3 types of commonsense descriptions.	https://paperswithcode.com/dataset/v2c		Video-to-Commonsense					
2780	VDQG	The Visual Discriminative Question Generation (VDQG) dataset contains 11202 ambiguous image pairs collected from Visual Genome. Each image pair is annotated with 4.6 discriminative questions and 5.9 non-discriminative questions on average.	https://paperswithcode.com/dataset/vdqg	09/08/2017	Visual Discriminative Question Generation					
2781	VehicleX	"VehicleX is a large-scale synthetic dataset. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes.
Source: https://github.com/yorkeyao/VehicleX
Image Source: https://arxiv.org/pdf/1912.08855.pdf"	https://paperswithcode.com/dataset/vehiclex							
2782	VeRi Dataset	"To facilitate the research of vehicle re-identification (Re-Id), a large-scale benchmark dateset is built for vehicle Re-Id in the real-world urban surveillance scenario, named “VeRi”. The featured properties of VeRi include:

It contains over 50,000 images of 776 vehicles captured by 20 cameras covering an 1.0 km^2 area in 24 hours, which makes the dataset scalable enough for vehicle Re-Id and other related research.
The images are captured in a real-world unconstrained surveillance scene and labeled with varied attributes, e.g. BBoxes, types, colors, and brands. So complicated models can be learnt and evaluated for vehicle Re-Id.
Each vehicle is captured by 2 ∼ 18 cameras in different viewpoints, illuminations, resolutions, and occlusions, which provides high recurrence rate for vehicle Re-Id in practical surveillance environment.
It is also labeled with sufficient license plates and spatiotemporal information, such as the BBoxes of plates, plate strings, the timestamps of vehicles, and the distances between neighbouring cameras.

Source: VeRi Dataset"	https://paperswithcode.com/dataset/veri-dataset							
2783	VG-Depth	"Enable visual relation detection and serves as an extension to Visual Genome (VG).
Source: Improving Visual Relation Detection using Depth Maps"	https://paperswithcode.com/dataset/vg-depth							
2784	VGG-Sound	"Consists of more than 210k videos for 310 audio classes.
Source: VGGSound: A Large-scale Audio-Visual Dataset"	https://paperswithcode.com/dataset/vgg-sound							
2785	VIA	"The VIA dataset is a dataset for aiding the visually impaired. The proposed datase1 consists of 342 images divided into two classes: 175 of them are “clear-path” and 167 are “nonclear” path. They were taken using a smartphone camera and resized to 750 × 1000 pixels. The smartphone was placed in the user chest height and inclined approximately 30 to 60 from the ground, so it could capture a few meters of the path ahead, and beyond the reach of a regular white cane
Source: https://arxiv.org/abs/2005.04473"	https://paperswithcode.com/dataset/via							
2786	VideoMem	"Composed of 10,000 videos annotated with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes after memorization and again 24-72 hours later. 
Source: VideoMem: Constructing, Analyzing, Predicting Short-term and Long-term Video Memorability"	https://paperswithcode.com/dataset/videomem							
2787	Video Storytelling	"A new dataset describing textual stories for events. 
Source: Video Storytelling: Textual Summaries for Events"	https://paperswithcode.com/dataset/video-storytelling							
2788	VIDIT	"VIDIT  is a reference evaluation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images. The illumination settings are all the combinations of 5 color temperatures (2500K, 3500K, 4500K, 5500K and 6500K) and 8 light directions (N, NE, E, SE, S, SW, W, NW). Original image resolution is 1024x1024.
Source: VIDIT: Virtual Image Dataset for Illumination Transfer
Image source: https://github.com/majedelhelou/VIDIT"	https://paperswithcode.com/dataset/vidit		Virtual Image Dataset for Illumination Transfer					
2789	VidSet	"A large video dataset with dynamic content.
Source: Deep Homography Estimation for Dynamic Scenes"	https://paperswithcode.com/dataset/vidset							
2790	VidSTG	"The VidSTG dataset is a spatio-temporal video grounding dataset constructed based on the video relation dataset VidOR. VidOR contains 7,000, 835 and 2,165 videos for training, validation and testing, respectively. The goal of the Spatio-Temporal Video Grounding task (STVG) is to localize the spatio-temporal section of an untrimmed video that matches a given sentence depicting an object.
Source: https://github.com/Guaranteer/VidSTG-Dataset
Image Source: https://github.com/Guaranteer/VidSTG-Dataset"	https://paperswithcode.com/dataset/vidstg							
2791	VIENA2	"Covers 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25M frames, each annotated with an action label, corresponding to 600 samples per action class. 
Source: VIENA2: A Driving Anticipation Dataset"	https://paperswithcode.com/dataset/viena2							
2792	ViMMRC	"A challenging machine comprehension corpus with multiple-choice questions, intended for research on the machine comprehension of Vietnamese text. This corpus includes 2,783 multiple-choice questions and answers based on a set of 417 Vietnamese texts used for teaching reading comprehension for 1st to 5th graders. Answers may be extracted from the contents of single or multiple sentences in the corresponding reading text.
Source: ViMMRC"	https://paperswithcode.com/dataset/vimmrc		Vietnamese Multiple-choice Machine Reading Comprehension Corpus					
2793	Violin	"Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels.
Source: https://github.com/jimmy646/violin
Image Source: https://github.com/jimmy646/violin"	https://paperswithcode.com/dataset/violin		VIdeO-and-Language INference					
2794	VIPL-HR	"VIPL-HR database is a database for remote heart rate (HR) estimation from face videos under less-constrained situations. It contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Nine different conditions, including various head movements and illumination conditions are taken into consideration. All the videos are recorded using Logitech C310, RealSense F200 and the front camera of HUAWEI P9 smartphone, and the ground-truth HR is recorded using a CONTEC CMS60C BVP sensor (a FDA approved device). 
Source: VIPL-HR: A Multi-modal Database for Pulse Estimation from Less-constrained Face Video"	https://paperswithcode.com/dataset/vipl-hr							
2795	Virtual Gallery	"The Virtual Gallery dataset is a synthetic dataset that targets multiple challenges such as varying lighting conditions and different occlusion levels for various tasks such as depth estimation, instance segmentation and visual localization.
It consists of a scene containing 3-4 rooms, in which a total of 42 free-for-use famous paintings are placed on the walls.
The virtual model and the captured images were generated with Unity software, allowing us to extract ground-truth information such as depth, semantic and instance segmentation, 2D-2D and 2D-3D correspondences.
Source: Visual Localization by Learning Objects-Of-Interest Dense Match Regression
Image Source: https://europe.naverlabs.com/research/3d-vision/virtual-gallery-dataset/"	https://paperswithcode.com/dataset/virtual-gallery							
2796	VisDrone	"VisDrone is a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining, Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization.
Source: https://github.com/VisDrone/VisDrone-Dataset
Image Source: https://arxiv.org/pdf/2001.06303.pdf"	https://paperswithcode.com/dataset/visdrone	20/04/2018						
2797	Vistas-NP	"The Vistas-NP dataset is an out-of-distribution detection dataset based on the Mapillary Vistas dataset. The original Vistas dataset consists of 18,000 training images and 2,000 validation images with 66 classes. In Vistas-NP the human classes are used as outliers due to their dispersion across scenes and visual diversity from other objects. The dataset is created by excluding all images with class person and the three rider classes to the test subset. Consequently, the dataset has 8,003 train images and 830 validation images. The test set contains 11,167.
Source: https://github.com/matejgrcic/Vistas-NP"	https://paperswithcode.com/dataset/vistas-np							
2798	Visual Question Answering	Visual Question Answering (VQA) is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. The first version of the dataset was released in October 2015. VQA v2.0 was released in April 2017.	https://paperswithcode.com/dataset/visual-question-answering	03/05/2015	VQA					
2799	VQG	"VQG is a collection of datasets for visual question generation. VQG questions were collected by crowdsourcing the task on Amazon Mechanical Turk (AMT). The authors provided details on the prompt and the specific instructions for all the crowdsourcing tasks in this paper in the supplementary material. The prompt was successful at capturing nonliteral questions. Images were taken from the MSCOCO dataset.
Source: What BERT Sees: Cross-Modal Transfer for Visual Question Generation"	https://paperswithcode.com/dataset/vqg	19/03/2016	Visual Question Generation					
2800	Visual Relationship Detection Dataset	"A dataset containing 5000 images with 37,993 thousand relationships. The dataset contains 100 object categories and 70 predicate categories connecting those objects together.
Source: Visual Relationship Detection with Language prior and Softmax"	https://paperswithcode.com/dataset/visual-relationship-detection-dataset							
2801	ViText2SQL	"ViText2SQL is a dataset for the Vietnamese Text-to-SQL semantic parsing task, consisting of about 10K question and SQL query pairs.
Source: https://github.com/VinAIResearch/ViText2SQL"	https://paperswithcode.com/dataset/vitext2sql							
2802	ViTT	"The ViTT dataset consists of human produced segment-level annotations for 8,169 videos. Of these, 5,840 videos have been annotated once, and the rest of the videos have been annotated twice or more. A total of 12,461 sets of annotations are released. The videos in the dataset are from the Youtube-8M dataset.
An annotation has the following format:
{
  ""id"": ""FmTp"",
  ""annotations"": [
    {
      ""timestamp"": 260,
      ""tag"": ""Opening""
    },
    {
      ""timestamp"": 16000,
      ""tag"": ""Displaying technique""
    },
    {
      ""timestamp"": 23990,
      ""tag"": ""Showing foot positioning""
    },
    {
      ""timestamp"": 55530,
      ""tag"": ""Demonstrating crossover""
    },
    {
      ""timestamp"": 114100,
      ""tag"": ""Closing""
    }
  ]
}
Source: Video Timeline Tags (ViTT)"	https://paperswithcode.com/dataset/vitt		Video Timeline Tags					
2803	VizWiz-Captions	"Consists of over 39,000 images originating from people who are blind that are each paired with five captions.
Source: Captioning Images Taken by People Who Are Blind"	https://paperswithcode.com/dataset/vizwiz-captions							
2804	VizWiz-Priv	"VizWiz-Priv includes 8,862 regions showing private content across 5,537 images taken by blind people. Of these, 1,403 are paired with questions and 62% of those directly ask about the private content.
Source: VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People"	https://paperswithcode.com/dataset/vizwiz-priv		Visual Privacy dataset					
2805	VizWiz-QualityIssues	"A large-scale dataset that links the assessment of image quality issues to two practical vision tasks: image captioning and visual question answering.
Source: Assessing Image Quality Issues for Real-World Problems"	https://paperswithcode.com/dataset/vizwiz-qualityissues							
2806	VLEngagement	"A novel dataset that consists of content-based and video-specific features extracted from publicly available scientific video lectures and several metrics related to user engagement. 
Source: VLEngagement: A Dataset of Scientific Video Lectures for Evaluating Population-based Engagement"	https://paperswithcode.com/dataset/vlengagement							
2807	VMSMO	"The Video-based Multimodal Summarization with Multimodal Output (VMSMO) corpus consists of 184,920 document-summary pairs, with 180,000 training pairs, 2,460 validation and test pairs. The task for this dataset is generating and appropriate textual summary of an article and choosing a proper cover frame from a video accompanying the article.
Source: https://github.com/yingtaomj/VMSMO"	https://paperswithcode.com/dataset/vmsmo							
2808	VocalFolds	"The Vocal Folds dataset is a dataset for automatic segmentation of laryngeal endoscopic images.
The dataset consists of 8 sequences from 2 patients containing 536 hand segmented in vivo colour images of the larynx during two different resection interventions with a resolution of 512x512 pixels.
Source: https://github.com/imesluh/vocalfolds
Image Source: https://github.com/imesluh/vocalfolds"	https://paperswithcode.com/dataset/vocalfolds							
2809	VoxClamantis	"A large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. 
Source: A Corpus for Large-Scale Phonetic Typology"	https://paperswithcode.com/dataset/voxclamantis							
2810	VoxPopuli	"VoxPopuli is a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours.
Source: VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation"	https://paperswithcode.com/dataset/voxpopuli							
2811	VQA-OV	"Collects 60 reference sequences and 540 impaired sequences. 
Source: Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model"	https://paperswithcode.com/dataset/vqa-ov		Visual Quality Assessment of Omnidirectional Video					
2812	VT5000	"Includes 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms.
Source: RGBT Salient Object Detection: A Large-scale Dataset and Benchmark"	https://paperswithcode.com/dataset/vt5000							
2813	Ward2ICU	"Ward2ICU is a vital signs dataset of inpatients from the general ward. It contains vital signs with class labels indicating patient transitions from the ward to intensive care units
Source: https://github.com/3778/Ward2ICU"	https://paperswithcode.com/dataset/ward2icu							
2814	Waymo Open Dataset	"The Waymo Open Dataset is comprised of high resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. 
The Waymo Open Dataset currently contains 1,950 segments. The authors plan to grow this dataset in the future. Currently the datasets includes:

1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions
Sensor data
1 mid-range lidar
4 short-range lidars
5 cameras (front and sides)
Synchronized lidar and camera data
Lidar to camera projections
Sensor calibrations and vehicle poses


Labeled data
Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs
High-quality labels for lidar data in 1,200 segments
12.6M 3D bounding box labels with tracking IDs on lidar data
High-quality labels for camera data in 1,000 segments
11.8M 2D bounding box labels with tracking IDs on camera data"	https://paperswithcode.com/dataset/waymo-open-dataset							
2815	WeatherBench	"A benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientific interest for atmospheric and computer scientists alike. 
Source: WeatherBench: A benchmark dataset for data-driven weather forecasting"	https://paperswithcode.com/dataset/weatherbench							
2816	WebCaricature Dataset	"Aims to facilitate research in caricature recognition. All the caricatures and face images were collected from the Web. Compared with two existing datasets, this dataset is much more challenging, with a much greater number of available images, artistic styles and larger intra-personal variations. 
Source: WebCaricature: a benchmark for caricature recognition"	https://paperswithcode.com/dataset/webcaricature-dataset							
2817	WebChild	"One of the largest commonsense knowledge bases available, describing over 2 million disambiguated concepts and activities, connected by over 18 million assertions.
Source: WebChild 2.0 : Fine-Grained Commonsense Knowledge Distillation"	https://paperswithcode.com/dataset/webchild							
2818	WOS	"Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories.
Source: HDLTex: Hierarchical Deep Learning for Text Classification"	https://paperswithcode.com/dataset/web-of-science-dataset		Web of Science Dataset					
2819	WGISD	"Embrapa Wine Grape Instance Segmentation Dataset (WGISD) contains grape clusters properly annotated in 300 images and a novel annotation methodology for segmentation of complex objects in natural images.
Source: Grape detection, segmentation and tracking using deep neural networks and three-dimensional association"	https://paperswithcode.com/dataset/wgisd	26/07/2019	Embrapa Wine Grape Instance Segmentation Dataset					
2820	WHOI-Plankton	"WHOI-Plankton is a collection of annotated plankton images. It contains > 3.5 million images of microscopic marine plankton, organized according to category labels provided by researchers at the Woods Hole Oceanographic Institution (WHOI). The images are currently placed into one of 103 categories.
Source: WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification"	https://paperswithcode.com/dataset/whoi-plankton							
2821	WHU	"Created for MVS tasks and is a large-scale multi-view aerial dataset generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters.
Source: A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view Stereo Reconstruction from An Open Aerial Dataset"	https://paperswithcode.com/dataset/whu							
2822	WiC	"WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise.
Source: WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"	https://paperswithcode.com/dataset/wic		Words in Context					
2823	WIDER	WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels.	https://paperswithcode.com/dataset/wider	01/06/2015	Web Image Dataset for Event Recognition					
2824	WIDER Attribute Dataset	The WIDER Attribute dataset is a human attribute recognition dataset with human attribute and image event annotations. Images are selected from the WIDER dataset. There are a total of 13,789 images. A bounding box is annotated for each person in these images, with no more than 20 people (with top resolutions) in a crowd image, resulting in 57,524 boxes in total and 4+ boxes per image on average. For each bounding box, 14 distinct human attributes are labelled. There are 805,336 labels in total.	https://paperswithcode.com/dataset/wider-attribute-dataset							
2825	WikiDataSets	"Topical subsets of WikiData, assembled using the WikiDataSets python library. Extracted from Wikidata in April 2020.
Source: WikiDataSets"	https://paperswithcode.com/dataset/wikidatasets							
2826	Wiki-40B	"A new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families containing round 40 billion characters and aimed to accelerate the research of multilingual modeling.
Source: Wiki-40B: Multilingual Language Model Dataset"	https://paperswithcode.com/dataset/wiki-40b							
2827	WikiAnn	"WikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages.
Source: Cross-lingual Name Tagging and Linking for 282 Languages"	https://paperswithcode.com/dataset/wikiann-1							
2828	WikiAsp	"A large-scale dataset for multi-domain aspect-based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. 
Source: WikiAsp: A Dataset for Multi-domain Aspect-based Summarization"	https://paperswithcode.com/dataset/wikiasp							
2829	WikiAtomicEdits	"WikiAtomicEdits is a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. 
Source: WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse
Image Source: https://arxiv.org/pdf/1808.09422v1.pdf"	https://paperswithcode.com/dataset/wikiatomicedits							
2830	WikiCatSum	"WikiCatSum is a domain specific Multi-Document Summarisation (MDS) dataset. It assumes the summarisation task of generating Wikipedia lead sections for Wikipedia entities of a certain domain (e.g. Companies) from the set of documents cited in Wikipedia articles or returned by Google (using article titles as queries). The dataset includes three domains: Companies, Films, and Animals.
Source: https://datashare.ed.ac.uk/handle/10283/3368"	https://paperswithcode.com/dataset/wikicatsum							
2831	WikiConv	"A corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation.
Source: WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community"	https://paperswithcode.com/dataset/wikiconv							
2832	WikiCoref	"WikiCoref is an English corpus annotated for anaphoric relations, where all documents are from the English version of Wikipedia. 
Source: WikiCoref: An English Coreference-annotated Corpus of Wikipedia Articles"	https://paperswithcode.com/dataset/wikicoref							
2833	WikiDocEdits	"A dataset of single-sentence edits crawled from Wikipedia. 
Source: Text Editing by Command"	https://paperswithcode.com/dataset/wikidocedits							
2834	Wiki-en	"Wiki-en is an annotated English dataset for domain detection extracted from Wikipedia. It includes texts from 7 different domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW), “Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN).
Source: https://arxiv.org/pdf/1907.11499.pdf"	https://paperswithcode.com/dataset/wiki-en							
2835	Wiki-Flickr Event Dataset	"The Wiki-Flick Event dataset for cross-modal event retrieval is a well-labelled but weakly-aligned dataset collected for cross-modality event retrieval. The dataset consists of 28,825 images on Flickr and 11,960 text articles from hundreds of social media, belonging to 82 categories of events.
Source: https://github.com/zhengyang5/Wiki-Flickr-Event-Dataset"	https://paperswithcode.com/dataset/wiki-flickr-event-dataset							
2836	WikiLingua	"WikiLingua includes ~770k article and summary pairs in 18 languages from WikiHow. Gold-standard article-summary alignments across languages are extracted by aligning the images that are used to describe each how-to step in an article.
Source: https://github.com/esdurmus/Wikilingua
Image Source: Ladhak et al"	https://paperswithcode.com/dataset/wikilingua							
2837	WikiLinks	"A method for automatically gathering massive amounts of naturally-occurring cross-document reference data is used to create the Wikilinks dataset comprising of 40 million mentions over 3 million entities. 
Source: WikiLinks"	https://paperswithcode.com/dataset/wikilinks							
2838	WikiMatrix	"WikiMatrix is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:

85 different languages, 1620 language pairs
134M parallel sentences, out of which 34M are aligned with English

Source: WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"	https://paperswithcode.com/dataset/wikimatrix							
2839	Wikipedia Title	"Wikipedia Title is a dataset for learning character-level compositionality from the character visual characteristics. It consists of a collection of Wikipedia titles in Chinese, Japanese or Korean labelled with the category to which the article belongs.
Source: https://arxiv.org/abs/1704.04859"	https://paperswithcode.com/dataset/wikipedia-title							
2840	WikiQAar	"A publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering.
Source: WikiQA: A Challenge Dataset for Open-Domain Question Answering"	https://paperswithcode.com/dataset/wikiqaar		English-Arabic Wikipedia Question-Answering					
2841	WikiReading Recycled	"A newly developed public dataset and the task of multiple property extraction. It uses the same data as WikiReading but does not inherit its predecessor's identified disadvantages.
Source: From Dataset Recycling to Multi-Property Extraction and Beyond"	https://paperswithcode.com/dataset/wikireading-recycled							
2842	WikiSection	"A publicly available dataset with 242k labeled sections in English and German from two distinct domains: diseases and cities.
Source: SECTOR: A Neural Model for Coherent Topic Segmentation and Classification"	https://paperswithcode.com/dataset/wikisection							
2843	WikiSem500	"The WikiSem500 dataset contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases).
Source: https://arxiv.org/abs/1611.01547"	https://paperswithcode.com/dataset/wikisem500							
2844	WikiSplit	"Contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task.
Source: Learning To Split and Rephrase From Wikipedia Edit History"	https://paperswithcode.com/dataset/wikisplit							
2845	WikiSRS	"WikiSRS is a novel dataset of similarity and relatedness judgments of paired Wikipedia entities (people, places, and organizations), as assigned by Amazon Mechanical Turk workers.
Source: https://github.com/OSU-slatelab/WikiSRS"	https://paperswithcode.com/dataset/wikisrs							
2846	WikiText-TL-39	"WikiText-TL-39 is a benchmark language modeling dataset in Filipino that has 39 million tokens in the training set.
Source: Evaluating Language Model Finetuning Techniques for Low-resource Languages"	https://paperswithcode.com/dataset/wikitext-tl-39							
2847	Wilds	"Builds on top of recent data collection efforts by domain experts in these applications and provides a unified collection of datasets with evaluation metrics and train/test splits that are representative of real-world distribution shifts.
The v2.0 update adds unlabeled data to 8 datasets. The labeled data and evaluation metrics are exactly the same, so all previous results are directly comparable.
Source: WILDS: A Benchmark of in-the-Wild Distribution Shifts"	https://paperswithcode.com/dataset/wilds							
2848	WildDash	"WildDash is a benchmark evaluation method is presented that uses the meta-information to calculate the robustness of a given algorithm with respect to the individual hazards.
Source: WildDash - Creating Hazard-Aware Benchmarks
Image Source: https://wilddash.cc/"	https://paperswithcode.com/dataset/wilddash							
2849	WildDeepfake	"WildDeepfake is a dataset for real-world deepfakes detection which consists of 7,314 face sequences extracted from 707 deepfake videos that are collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop more effective detectors against real-world deepfakes.
Source: https://github.com/deepfakeinthewild/deepfake-in-the-wild
Image Source: https://github.com/deepfakeinthewild/deepfake-in-the-wild"	https://paperswithcode.com/dataset/wilddeepfake							
2850	WiLI-2018	"WiLI-2018 is a benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is.
Source: The WiLI benchmark dataset for written language identification"	https://paperswithcode.com/dataset/wili-2018							
2851	Winogender Schemas	"Winogender Schemas is a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.
Source: Gender Bias in Coreference Resolution"	https://paperswithcode.com/dataset/winogender-schemas							
2852	WinoGrande	"WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations.
Source: WinoGrande: An Adversarial Winograd Schema Challenge at Scale
Image Source: https://winogrande.allenai.org/"	https://paperswithcode.com/dataset/winogrande							
2853	WISDOM	"Synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. 
Source: Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data"	https://paperswithcode.com/dataset/wisdom		Warehouse Instance Segmentation Dataset for Object Manipulation					
2854	Wisesight Sentiment Corpus	"Social media message with sentiment label (positive, neutral, negative, question).
Source: Wisesight Sentiment Corpus"	https://paperswithcode.com/dataset/wisesight-sentiment-corpus							
2855	WLASL	"WLASL is a larege video dataset for Word-Level American Sign Language (ASL) recognition, which features 2,000 common different words in ASL.
Source: https://github.com/dxli94/WLASL"	https://paperswithcode.com/dataset/wlasl		Word-Level American Sign Language					
2856	WLD	"WildLife Documentary is an animal object detection dataset. It contains 15 documentary films that are downloaded from YouTube. The videos vary between 9 minutes to as long as 50 minutes, with resolution ranging from 360p
to 1080p. A unique property of this dataset is that all videos are accompanied with subtitles that are automatically generated from speech by YouTube. The subtitles are revised manually to correct obvious spelling mistakes. All the animals in the videos are annotated, resulting in more than 4098 object tracklets of 60 different visual
concepts, e.g., ‘tiger’, ‘koala’, ‘langur’, and ‘ostrich’."	https://paperswithcode.com/dataset/wld	30/07/2017	WildLife Documentary					
2857	WNLaMPro	"The WordNet Language Model Probing (WNLaMPro) dataset consists of relations between keywords and words. It contains 4 different kinds of relations: Antonym, Hypernym, Cohyponym and Corruption.
Source: https://arxiv.org/pdf/1904.06707.pdf"	https://paperswithcode.com/dataset/wnlampro		WordNet Language Model Probing					
2858	WoodScape	"Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of its prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images.
WoodScape is an extensive fisheye automotive dataset named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images.
Source: https://github.com/valeoai/WoodScape
Image Source: https://github.com/valeoai/WoodScape"	https://paperswithcode.com/dataset/woodscape							
2859	Workplace Sexual Harassment	"The goal of this dataset is to understand how people experience sexism and sexual harassment in the workplace by discovering themes in 2,362 experiences posted on the Everyday Sexism Project's website
Source: https://arxiv.org/abs/1907.00510"	https://paperswithcode.com/dataset/workplace-sexual-harassment							
2860	WritingPrompts	"WritingPrompts is a large dataset of 300K human-written stories paired with writing prompts from an online forum. 
Source: Hierarchical Neural Story Generation"	https://paperswithcode.com/dataset/writingprompts							
2861	WSVD	The Web Stereo Video Dataset consists of 553 stereoscopic videos from YouTube. This dataset has a wide variety of scene types, and features many nonrigid objects.	https://paperswithcode.com/dataset/wsvd		Web Stereo Video Dataset					
2862	xBD	"The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels.
Source: xBD
Image Source: Gupta et al"	https://paperswithcode.com/dataset/xbd							
2863	XED	"XED is a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. 
Source: XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection"	https://paperswithcode.com/dataset/xed							
2864	XKCDColors	"A balanced dataset of color names and RGB values for training classifiers.
Source: https://github.com/Smoltbob/XKCDColors-Dataset"	https://paperswithcode.com/dataset/xkcdcolors							
2865	XL-R2R	"The XL-R2R dataset is built upon the R2R dataset and extends it with Chinese instructions. XL-R2R preserves the same splits as in R2R and thus consists of train, val-seen, and val-unseen splits with both English and Chinese instructions, and test split with English instructions only.
Source: https://github.com/zzxslp/Crosslingual-VLN"	https://paperswithcode.com/dataset/xl-r2r		Cross-lingual Room-to-Room					
2866	XL-WiC	"A large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer.
Source: XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization"	https://paperswithcode.com/dataset/xl-wic							
2867	X-MARS	"The X-MARS dataset proposes new splits for the MARS dataset, to allow for cross-evaluation with the Market-1501 dataset without training and test overlap between the two datasets.
Source: https://github.com/andreas-eberle/x-mars"	https://paperswithcode.com/dataset/x-mars							
2868	XOR-TYDI QA	"A large-scale dataset built on questions from TyDi QA lacking same-language answers.
Source: XOR QA: Cross-lingual Open-Retrieval Question Answering"	https://paperswithcode.com/dataset/xor-tydi-qa							
2869	LAReQA	"A challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. 
Source: LAReQA: Language-agnostic answer retrieval from a multilingual pool"	https://paperswithcode.com/dataset/xquad-r							
2870	X-ray and Visible Spectra Circular Motion Images Dataset	"Collections of images of the same rotating plastic object made in X-ray and visible spectra. Both parts of the dataset contain 400 images. The images are maid every 0.5 degrees of the object axial rotation. The collection of images is designed for evaluation of the performance of circular motion estimation algorithms as well as for the study of X-ray nature influence on the image analysis algorithms such as keypoints detection and description.
Source: X-ray and Visible Spectra Circular Motion Images Dataset"	https://paperswithcode.com/dataset/x-ray-and-visible-spectra-circular-motion							
2871	xR-EgoPose	xR-EgoPose is an egocentric synthetic dataset for egocentric 3D human pose estimation. It consists of ~380 thousand photo-realistic egocentric camera images in a variety of indoor and outdoor spaces.	https://paperswithcode.com/dataset/xr-egopose							
2872	X-SRL	"SRL is the task of extracting semantic predicate-argument structures from sentences. X-SRL is a multilingual parallel Semantic Role Labelling (SRL) corpus for English (EN), German (DE), French (FR) and Spanish (ES) that is based on English gold annotations and shares the same labelling scheme across languages.
Source: https://github.com/Heidelberg-NLP/xsrl_mbert_aligner
Image Source: Daza et al"	https://paperswithcode.com/dataset/x-srl							
2873	x-stance	"A large-scale stance detection dataset from comments written by candidates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 political issues (targets).
Source: X-Stance: A Multilingual Multi-Target Dataset for Stance Detection"	https://paperswithcode.com/dataset/x-stance							
2874	YASO	"YASO is a crowd-sourced TSA evaluation dataset, collected using a new annotation scheme for labeling targets and their sentiments. The dataset contains 2,215 English sentences from movie, business and product reviews, and 7,415 terms and their corresponding sentiments annotated within these sentences. 
Source: YASO: A New Benchmark for Targeted Sentiment Analysis"	https://paperswithcode.com/dataset/yaso							
2875	YCBInEOAT Dataset	"A new dataset with significant occlusions related to object manipulation.
Source: se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains"	https://paperswithcode.com/dataset/ycbineoat-dataset							
2876	YFCC100M	"YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014.
Source: YFCC100M: The New Data in Multimedia Research
Image Source: Thomee et al"	https://paperswithcode.com/dataset/yfcc100m							
2877	Yoga-82	"Dataset for large-scale yoga pose recognition with 82 classes.
Source: Yoga-82: A New Dataset for Fine-grained Classification of Human Poses"	https://paperswithcode.com/dataset/yoga-82							
2878	YouCook	"This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions.
Source: A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching"	https://paperswithcode.com/dataset/youcook							
2879	Youtubean	"Youtbean is a dataset created from closed captions of YouTube product review videos. It can be used for aspect extraction and sentiment classification.
Source: https://arxiv.org/pdf/1708.02420.pdf"	https://paperswithcode.com/dataset/youtubean							
2880	YT-BB	"YouTube-BoundingBoxes (YT-BB) is a large-scale data set of video URLs with densely-sampled object bounding box annotations. The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. 
Source: YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video"	https://paperswithcode.com/dataset/youtube-boundingboxes		YouTube-BoundingBoxes					
2881	YouTube Movie Summaries	"This dataset contains 94 movie summary videos from various YouTube channels.
Source: YouTube Movie Summaries"	https://paperswithcode.com/dataset/youtube-movie-summaries							
2882	YUP++	"A new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se. 
Source: Temporal Residual Networks for Dynamic Scene Recognition"	https://paperswithcode.com/dataset/yup	01/07/2017	YUP++ Dynamic Scenes dataset					
2883	ZEST	"A new English language dataset structured for task-oriented evaluation on unseen tasks. 
Source: Learning from Task Descriptions"	https://paperswithcode.com/dataset/zest							
2884	Zooniverse	"The Humbug Zooinverse dataset is a dataset of mosquito audio recordings. With over a thousand contributors, it contains 195,434 labels of two second duration, of which approximately 10 percent signify mosquito events.
Source: https://github.com/HumBug-Mosquito/ZooniverseData"	https://paperswithcode.com/dataset/zooniverse		HumBug Zooniverse					
2885	ZuBuD+	"A more balanced version of ZuBuD.
Source: A location-aware embedding technique for accurate landmark recognition"	https://paperswithcode.com/dataset/zubud							
2886	Spot-the-diff	"Spot-the-diff is a dataset consisting of 13,192 image pairs along with corresponding human provided text annotations stating the differences between the two images.
Source: Learning to Describe Differences Between Pairs of Similar Images"	https://paperswithcode.com/dataset/spot-the-diff	31/08/2018						
2887	OST300	"OST300 is an outdoor scene dataset with 300 test images of outdoor scenes, and a training set of 7 categories of images with rich textures.
Source: Recovering Realistic Texture in Image Super-resolution by Deep Spatial Feature Transform
Image Source: http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/"	https://paperswithcode.com/dataset/ost300	09/04/2018	OST300					
2888	CoNSeP	"The colorectal nuclear segmentation and phenotypes (CoNSeP) dataset consists of 41 H&E stained image tiles, each of size 1,000×1,000 pixels at 40× objective magnification. The images were extracted from 16 colorectal adenocarcinoma (CRA) WSIs, each belonging to an individual patient, and scanned with an Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK.
Source: HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images
Image Source: Graham et al"	https://paperswithcode.com/dataset/consep	16/12/2018	Colorectal Nuclear Segmentation and Phenotypes					
2889	CUHK Image Cropping	CUHK Image Cropping  is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset.	https://paperswithcode.com/dataset/cuhk-image-cropping	01/06/2013	CUHK Image Cropping					
2890	MIT Traffic	"MIT Traffic is a dataset for research on activity analysis and crowded scenes. It includes a traffic video sequence of 90 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 480 and it is divided into 20 clips.
Source: MIT Traffic Dataset"	https://paperswithcode.com/dataset/mit-traffic		MIT Traffic					
2891	PCN	Pedestrian Color Naming (PCN) is a dataset for pedestrian color naming, which contains 14,213 images, each of which hand-labeled with color label for each pixel. All images in the PCN dataset are obtained from the Market- 1501 dataset.	https://paperswithcode.com/dataset/pcn		Pedestrian Color Naming					
2892	Social Relation Dataset	"Social Relation Dataset is a dataset for social relation trait prediction from face images. Traits are based on the interpersonal circle proposed by Kiesler, where human relations are divided into 16 segments. Each segment has its opposite side in the circle, such as 'friendly and hostile'. The dataset contains 8,306 images chosen from the internet and movies. Each image is labelled with faces’ bounding boxes and their pairwise relations. 
Source: Open MMLab"	https://paperswithcode.com/dataset/social-relation-dataset	14/09/2015	Social Relation Dataset					
2893	WWW Crowd	WWW Crowd provides 10,000 videos with over 8 million frames from 8,257 diverse scenes, therefore offering a comprehensive dataset for the area of crowd understanding.	https://paperswithcode.com/dataset/www-crowd	01/06/2015	WWW Crowd					
2894	PolyU Dataset	"PolyU Dataset is a large dataset of real-world noisy images with reasonably obtained corresponding “ground truth” images. The basic idea is to capture the same and unchanged scene for many (e.g., 500) times and compute their mean image, which can be roughly taken as the “ground truth” image for the real-world noisy images. The rational of this strategy is that for each pixel, the noise is generated randomly larger or smaller than 0. Sampling the same pixel many times and computing the average value will approximate the truth pixel value and alleviate significantly the noise.
Source: Real-world Noisy Image Denoising: A New Benchmark"	https://paperswithcode.com/dataset/polyu-dataset	07/04/2018						
2895	SPOT	"The SPOT dataset contains 197 reviews originating from the Yelp'13 and IMDB collections ([1][2]), annotated with segment-level polarity labels (positive/neutral/negative). Annotations have been gathered on 2 levels of granulatiry:

Sentences
Elementary Discourse Units (EDUs), i.e. sub-sentence clauses produced by a state-of-the-art RST parser

This dataset is intended to aid sentiment analysis research and, in particular, the evaluation of methods that attempt to predict sentiment on a fine-grained, segment-level basis.
Source: SPOT"	https://paperswithcode.com/dataset/spot	27/11/2017	Sentiment Polarity Annotations Dataset					
2896	ToM QA	"The data consists of a set of 3 task types and 4 question types, creating 12 total scenarios. The tasks are grouped into stories, which are denoted by the numbering at the start of each line.
Source: ToM QA"	https://paperswithcode.com/dataset/tom-qa	28/08/2018						
2897	Verse	"Verse is a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. 
Source: Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings"	https://paperswithcode.com/dataset/verse	30/03/2016						
2898	Holl-E	"Holl-E is a dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie.
Source: Towards Exploiting Background Knowledge for Building Conversation Systems"	https://paperswithcode.com/dataset/holl-e	21/09/2018						
2899	Wikidata-Disamb	"The Wikidata-Disamb dataset is intended to allow a clean and scalable evaluation of NED with Wikidata entries, and to be used as a reference in future research.
Source: Named Entity Disambiguation using Deep Learning on Graphs"	https://paperswithcode.com/dataset/wikidata-disamb	22/10/2018						
2900	Perceptual Similarity	"Perceptual Similarity is a dataset of human perceptual similarity judgments.
Source: Perceptual Similarity
Image Source: https://github.com/richzhang/PerceptualSimilarity"	https://paperswithcode.com/dataset/perceptual-similarity	11/01/2018						
2901	Gutenberg Poem Dataset	"Gutenberg Poem Dataset is used for the next verse prediction component.
Source: Gutenberg Poem Dataset"	https://paperswithcode.com/dataset/gutenberg-poem-dataset	05/11/2020						
2902	Visual Beliefs	"Visual Beliefs is a dataset of abstract scenes to study visual beliefs. The dataset consists of 8-frame scenes, and in each scene a person has a mistaken belief. The dataset can be used for two tasks: predicting who is mistaken and predicting when are they mistaken. 
Source: Who is Mistaken?"	https://paperswithcode.com/dataset/visual-beliefs	04/12/2016						
2903	MovieShots	"MovieShots is a dataset to facilitate the shot type analysis in videos. It is a large-scale shot type annotation set that contains 46K shots from 7,858 movies covering a wide
variety of movie genres to ensure the inclusion of all scale and movement types of shot. Each shot has two attributes, shot scale and shot movement.
Shot scale has five categories: 1) long shot (LS) is taken from a long distance, sometimes as far as a quarter of a mile away; 2) full shot (FS) barely includes the human body in full; 3) medium shot (MS) contains a figure from the knees or waist up; 4) close-up shot (CS) concentrates on a relatively small object, showing the face of the hand of a person; (5) extreme close-up shot (ECS) shows even smaller parts such as the image of an eye or a mouth.
Shot movement has four categories: 1) in static shot, the camera is fixed but the subject is flexible to move; 2) for motion shot, the camera moves or rotates; 3) the camera zooms in for push shot, and 4) zooms out for pull shot. While all the four movement types are widely used in movies, the use of push and pull shots only takes a very small portion. The usage of different shots usually depends on the movie genres and the preferences of the filmmakers.
Source: A Unified Framework for Shot Type Classification Based on Subject Centric Lens"	https://paperswithcode.com/dataset/movieshots	08/08/2020						
2904	VQA-HAT	"VQA-HAT (Human ATtention) is a dataset to evaluate the informative regions of an image depending on the question being asked about it. The dataset consists of human visual attention maps over the images in the original VQA dataset. It contains more than 60k attention maps.
Source: Human Attention in Visual Question Answering:"	https://paperswithcode.com/dataset/vqa-hat	17/06/2016	VQA Human Attention					
2905	RDD-2020	"The Road Damage Dataset 2020 (RDD-2020) Secondly is a large-scale heterogeneous dataset comprising 26620 images collected from multiple countries using smartphones. The images are collected from roads in India, Japan and the Czech Republic.
Source: Transfer Learning-based Road Damage Detection for Multiple Countries"	https://paperswithcode.com/dataset/rdd-2020	30/08/2020	Road Damage Dataset 2020					
2906	Worldtree	"Worldtree is a corpus of explanation graphs, explanatory role ratings, and associated tablestore. It contains explanation graphs for 1,680 questions, and 4,950 tablestore rows across 62 semi-structured tables are provided. This data is intended to be paired with the AI2 Mercury Licensed questions.
Source: Explanation Bank
Image Source: http://cognitiveai.org/explanationbank/"	https://paperswithcode.com/dataset/worldtree	08/02/2018						
2907	Sarcasm Corpus V2	"The Sarcasm Corpus contains sarcastic and non-sarcastic utterances of three different types, which are balanced with half of the samples being sarcastic and half non-sarcastic.
The three types are:

Generic: 6,520 samples
Rhetorical Questions: 1,702 samples
Hyperbole: 1,164 samples

Source: Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue"	https://paperswithcode.com/dataset/sarcasm-corpus-v2	15/09/2017						
2908	3D-FUTURE	"3D-FUTURE (3D FUrniture shape with TextURE) is a 3D dataset that contains 20,240 photo-realistic synthetic images captured in 5,000 diverse scenes, and 9,992 involved unique industrial 3D CAD shapes of furniture with high-resolution informative textures developed by professional designers.
Source: 3D-FUTURE
Image Source: https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future"	https://paperswithcode.com/dataset/3d-future	21/09/2020						
2909	RealNews	"RealNews is a large corpus of news articles from Common Crawl. Data is scraped from Common Crawl, limited to the 5000 news domains indexed by Google News. The authors used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019
were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression.
Image Source: https://arxiv.org/pdf/1905.12616v3.pdf"	https://paperswithcode.com/dataset/realnews	29/05/2019						
2910	CC-Stories	CC-Stories (or STORIES) is a dataset for common sense reasoning and language modeling. It was constructed by aggregating documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions in commonsense reasoning tasks. The top 1.0% of highest ranked documents is chosen as the new training corpus.	https://paperswithcode.com/dataset/cc-stories	07/06/2018	CC-Stories					
2911	xView	"xView is one of the largest publicly available datasets of overhead imagery. It contains images from complex scenes around the world, annotated using bounding boxes. It contains over 1M object instances from 60 different classes.
Source: xView dataset"	https://paperswithcode.com/dataset/xview	22/02/2018	mejdi					
2912	BrixIA	BrixIA Covid-19 is a large dataset of CXR images corresponding to the entire amount of images taken for both triage and patient monitoring in sub-intensive and intensive care units during one month (between March 4th and April 4th 2020) of pandemic peak at the ASST Spedali Civili di Brescia, and contains all the variability originating from a real clinical scenario. It includes 4,707 CXR images of COVID-19 subjects, acquired with both CR and DX modalities, in AP or PA projection, and retrieved from the facility RIS-PACS system.	https://paperswithcode.com/dataset/brixia	08/06/2020	BrixIA Covid-19					
2913	MaleX	MaleX is a curated dataset of malware and benign Windows executable samples for malware researchers. The dataset contains 1,044,394 Windows executable binaries with 864,669 labelled as malware and 179,725 as benign. This dataset has reasonable number of samples and is sufficient to test data-driven machine learning classification methods and also to measure the performance of the designed models in terms of scalability and adaptability.	https://paperswithcode.com/dataset/malex		MaleX					
2914	AIST++	"AIST++ is a 3D dance dataset which contains 3D motion reconstructed from real dancers paired with music. The AIST++ Dance Motion Dataset is constructed from the AIST Dance Video DB. With multi-view videos, an elaborate pipeline is designed to estimate the camera parameters, 3D human keypoints and 3D human dance motion sequences:

It provides 3D human keypoint annotations and camera parameters for 10.1M images, covering 30 different subjects in 9 views. These attributes makes it the largest and richest existing dataset with 3D human keypoint annotations.
It also contains 1,408 sequences of 3D human dance motion, represented as joint rotations along with root trajectories. The dance motions are equally distributed among 10 dance genres with hundreds of choreographies. Motion durations vary from 7.4 sec. to 48.0 sec. All the dance motions have corresponding music."	https://paperswithcode.com/dataset/aist							
2915	ScenicOrNot	"ScenicOrNot (SoN) is a dataset of 185,548 images with associated natural beauty rating histograms. Each image in the dataset was rated at least five times. The images also have metadata like title and location.
Source: Understanding and Mapping Natural Beauty"	https://paperswithcode.com/dataset/scenicornot	09/12/2016						
2916	TiMoS	"Tropes in Movie Synopses (TiMoS) is a dataset of movie tropes collected from a Wikipedia-style website, TVTropes3 with 5623 movie synopses associated with 95 most occurred tropes. The movies are diverse in genre, filming year, length, and style, making the task challenging and unable to rely on patterns from a specific domain. The tropes involve character trait, role interaction, situation, and storyline, which could be sensed by a non-expert human but remains challenging for machines that have more than 100 million parameters and pre-trained with 11,000 books and the whole
Wikipedia (23.97 F1 score while a human could reach 64.87)."	https://paperswithcode.com/dataset/timos		Tropes in Movie Synopses					
2917	Kite	"The Kite database is a multi-modal dataset for the control of unmanned aerial vehicles (UAVs). There are three modalities present in the dataset:

Language, represented by the commands issued to the UAV
Audio, represented by the spoken instantiation of the commands
Visual, represented by an image that is likely to be seen when the command is issued

The dataset was created by the members of the SpeeD team.
Source: Kite Dataset"	https://paperswithcode.com/dataset/kite	02/07/2019						
2918	COCO Earthquake	COCO Earthquake is a dataset similar to Common Objects in Context (COCO) used for cracking segmentation. The images selected in the dataset are at various scales, and the tool referred to as the COCO Annotator is used to label cracks for training. In these labeled images, cracks are in yellow and background is in purple. Size of the training and labeling images is varied from 168×300 to 4600×3070. By excluding steel structures, 2,021 images are labeled when surface cracks appeared on structural or nonstructural materials at various scales.	https://paperswithcode.com/dataset/coco-earthquake	05/11/2020						
2919	VLUC	"VLUC (Video-Like Urban Computing) is a benchmark for video-like computing on citywide traffic density and crowd prediction. It consists of two new datasets BousaiTYO and BousaiOSA and existing datasets TaxiBJ, BikeNYC I-II, and TaxiNYC.
Source: VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide Crowd and Traffic Prediction"	https://paperswithcode.com/dataset/vluc	16/11/2019	Video-Like Urban Computing					
2920	MM-WHS 2017	The MM-WHS 2017 dataset is a dataset for multi-modality whole heart segmentation. It provides 20 labeled and 40 unlabeled CT volumes, as well as 20 labeled and 40 unlabeled MR volumes. In total there are 120 multi-modality cardiac images acquired in a real clinical environment.	https://paperswithcode.com/dataset/mm-whs-2017		MM-WHS 2017					
2921	BosphorusSign22k	"BosphorusSign22k is a benchmark dataset for vision-based user-independent isolated Sign Language Recognition (SLR). The dataset is based on the BosphorusSign (Camgoz et al., 2016c) corpus which was collected with the purpose of helping both linguistic and computer science communities. It contains isolated videos of Turkish Sign Language glosses from three different domains: Health, finance and commonly used everyday signs. Videos in this dataset were performed by six native signers, which makes this dataset valuable for user independent sign language studies.
Source: BosphorusSign22k Sign Language Recognition Dataset"	https://paperswithcode.com/dataset/bosphorussign22k	02/04/2020						
2922	EMIDEC	"The MICCAI 2020 EMIDEC dataset is a dataset for classifying normal and pathological cases from the clinical information with or without DE-MRI, and secondly to automatically detect the different relevant areas (the myocardial contours, the infarcted area and the permanent microvascular obstruction area (no-reflow area)) from a series of short-axis DE-MRI covering the left ventricle. The segmentation allows one to make a quantification of the MI, in absolute value (mm3) or percentage of the myocardium.
The database consists of 150 exams (all from different patients) divided into 50 cases with normal MRI after injection of a contrast agent and 100 cases with myocardial infarction (and then with a hyperenhanced area on DE-MRI), whatever their inclusion in the cardiac emergency department. Along with MRI, clinical characteristics are provided to distinguish normal and pathological cases. The training set has 100 cases.
Lalande, A.; Chen, Z.; Decourselle, T.; Qayyum, A.; Pommier, T.; Lorgis, L.; de la Rosa, E.; Cochet, A.; Cottin, Y.; Ginhac, D.; Salomon, M.; Couturier, R.; Meriaudeau, F. Emidec: A Database Usable for the Automatic Evaluation of Myocardial Infarction from Delayed-Enhancement Cardiac MRI. Data 2020, 5, 89.
doi: https://doi.org/10.3390/data5040089"	https://paperswithcode.com/dataset/emidec		MICCAI 2020 EMIDEC					
2923	InstaCities1M	"InstaCities1M is a dataset of social media images with associated text. It consists of Instagram images associated associated with one of the 10 most populated English speaking cities all over the world. It has 100K images for each city, which makes a total of 1M images, split in 800K training images, 50K validation images and 150K testing images. All images were resized to 300x300 pixels.
Source: The InstaCities1M Dataset"	https://paperswithcode.com/dataset/instacities1m	20/08/2018						
2924	WHU-Hi	"WHU-Hi dataset (Wuhan UAV-borne hyperspectral image) is collected and shared by the RSIDEA research group of Wuhan University, and it could serve as a benchmark dataset for precise crop classification and hyperspectral image classification studies. The WHU-Hi dataset contains three individual UAV-borne hyperspectral datasets: WHU-Hi-LongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu. All the datasets were acquired in farming areas with various crop types in Hubei province, China, via a Headwall Nano-Hyperspec sensor mounted on a UAV platform. Compared with spaceborne and airborne hyperspectral platforms, unmanned aerial vehicle (UAV)-borne hyperspectral systems can acquire hyperspectral imagery with a high spatial resolution (which we refer to here as H2 imagery). The research was published in Remote Sensing of Environment.
Source: http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm
Image Source: http://rsidea.whu.edu.cn/resource_WHUHi_sharing.htm"	https://paperswithcode.com/dataset/whu-hi	27/12/2020	Wuhan UAV-borne hyperspectral image					
2925	Botswana	"Botswana is a hyperspectral image classification dataset. The NASA EO-1 satellite acquired a sequence of data over the Okavango Delta, Botswana in 2001-2004. The Hyperion sensor on EO-1 acquires data at 30 m pixel resolution over a 7.7 km strip in 242 bands covering the 400-2500 nm portion of the spectrum in 10 nm windows. Preprocessing of the data was performed by the UT Center for Space Research to mitigate the effects of bad detectors, inter-detector miscalibration, and intermittent anomalies. Uncalibrated and noisy bands that cover water absorption features were removed, and the remaining 145 bands were included as candidate features: [10-55, 82-97, 102-119, 134-164, 187-220]. The data analyzed in this study, acquired May 31, 2001, consist of observations from 14 identified classes representing the land cover types in seasonal swamps, occasional swamps, and drier woodlands located in the distal portion of the Delta.
Source: M Graña, MA Veganzons, B Ayerdi"	https://paperswithcode.com/dataset/botswana		Botswana					
2926	Houston	Houston is a hyperspectral image classification dataset. The hyperspectral imagery consists of 144 spectral bands in the 380 nm to 1050 nm region and has been calibrated to at-sensor spectral radiance units, SRU =$latex \mu \text{W} /( \text{cm}^2 \text{ sr nm})$. The corresponding co-registered DSM consists of elevation in meters above sea level (per the Geoid 2012A model).	https://paperswithcode.com/dataset/houston		Houston					
2927	Pavia Centre	Pavia Centre is a hyperspectral dataset acquired by the ROSIS sensor during a flight campaign over Pavia, northern Italy. The number of spectral bands is 102 for Pavia Centre. Pavia Centre is a 1096*1096 pixels image. The geometric resolution is 1.3 meters. Image groundtruths differentiate 9 classes each. Pavia scenes were provided by Prof. Paolo Gamba from the Telecommunications and Remote Sensing Laboratory, Pavia university (Italy).	https://paperswithcode.com/dataset/pavia-centre		Pavia Centre					
2928	SEVIR	"SEVIR is an annotated, curated and spatio-temporally aligned dataset containing over 10,000 weather events that each consist of 384 km x 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD vertically integrated liquid mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. Many events in SEVIR were selected and matched to the NOAA Storm Events database so that additional descriptive information such as storm impacts and storm descriptions can be linked to the rich imagery provided by the sensors.
Source: https://proceedings.neurips.cc//paper/2020/file/fa78a16157fed00d7a80515818432169-Paper.pdf"	https://paperswithcode.com/dataset/sevir		Storm EVent ImagRy					
2929	Chinese Classifier	"Classifiers are function words that are used to express quantities in Chinese and are especially difficult for language learners. This dataset of Chinese Classifiers can be used to predict Chinese classifiers from context.
The dataset contains a large collection of example sentences for Chinese classifier usage derived from three language corpora (Lancaster Corpus of Mandarin Chinese, UCLA Corpus of Written Chinese and Leiden Weibo Corpus). The data was cleaned and processed for a context-based classifier prediction task.
Source: https://github.com/wuningxi/ChineseClassifierDataset"	https://paperswithcode.com/dataset/chinese-classifier							
2930	SimpleDBpediaQA	"A new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. 
Source: Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia"	https://paperswithcode.com/dataset/simpledbpediaqa							
2931	Part Whole Relations	"The Part-Whole Relations dataset is a dataset of semantic relations between entities. It contains the following subtypes:
- Component-Of
- Member-Of
- Portion-Of
- Stuff-Of
- Located-In
- Contained-In
- Phase-Of
- Participates-In
Source: https://github.com/pvthuy/part-whole-relations"	https://paperswithcode.com/dataset/part-whole-relations							
2932	ISI-PPT	"This is a Dataset for Arabic/English text detection and optical character recognition. All image data are text-slides extracted from PowerPoint files downloaded from Internet through the Google API.
All annotations are automatically generated mainly through the WinCom32 Python API. Postprocess is also applied to place a more accurate text bounding box or to suppress false-alarms, e.g. a text box only containing spaces. Finally, all annotation results are briefly reviewed by human to reject extreme bad samples, e.g. a slide with a large portion of copied table as image. In summary, this dataset contains 10,692 images, and roughly 100K line samples.
Source: https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset
Image Source: https://gitlab.com/rex-yue-wu/ISI-PPT-Dataset"	https://paperswithcode.com/dataset/isi-ppt							
2933	MeQSum	"MeQSum is a dataset for medical question summarization. It contains 1,000 summarized consumer health questions.
Source: https://www.aclweb.org/anthology/P19-1215.pdf
Image Source: https://www.aclweb.org/anthology/P19-1215.pdf"	https://paperswithcode.com/dataset/meqsum							
2934	Visual Choice of Plausible Alternatives	"Visual Choice of Plausible Alternatives (VCOPA) is an evaluation dataset containing 380 VCOPA questions and over 1K images with various topics, which is amenable to automatic evaluation, and present the performance of baseline reasoning approaches as initial benchmarks for future systems.
Source: Visual Choice of Plausible Alternatives: An Evaluation of Image-based Commonsense Causal Reasoning"	https://paperswithcode.com/dataset/visual-choice-of-plausible-alternatives		VCOPA					
2935	QTuna	"The QTUNA dataset is the result of a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions in order to inform possible Natural Language Generation algorithms that mimic humans' use of quantified expressions.
Source: https://github.com/a-quei/qtuna"	https://paperswithcode.com/dataset/qtuna							
2936	StockNet	"The StockNet dataset is a comprehensive dataset for stock movement prediction from tweets and historical stock prices.
It consists of two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks, coming from all the 8 stocks in the Conglomerates sector and the top 10 stocks in capital size in each of the other 8 sectors.
Source: https://github.com/yumoxu/stocknet-dataset"	https://paperswithcode.com/dataset/stocknet-1							
2937	LDDRS	"The LWIR DoFP Dataset of Road Scene (LDDRS) is a road detection dataset with 2,113 annotated images. It contains both day and night scenes, with multiple cars and pedestrians per image.
Source: https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700460.pdf"	https://paperswithcode.com/dataset/lddrs		LWIR DoFP Dataset of Road Scene					
2938	VegFru	"VegFru is a domain-specific dataset for fine-grained visual categorization. VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class.
Source: https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf
Image Source: https://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf"	https://paperswithcode.com/dataset/vegfru							
2939	Para-Quality	"Used to investigate common crowdsourced paraphrasing issues and for detecting the quality issues.
Source: A Study of Incorrect Paraphrases in Crowdsourced User Utterances"	https://paperswithcode.com/dataset/para-quality							
2940	Metaphorical Connections	"The Metaphorical Connections dataset is a poetry dataset that contains annotations between metaphorical prompts and short poems. Each poem is annotated whether or not it successfully communicates the idea of the metaphorical prompt.
Source: https://github.com/kgero/metaphorical-connections"	https://paperswithcode.com/dataset/metaphorical-connections							
2941	German affixoid dataset	"German affixoids are a type of morpheme in between affixes and free stems.
Source: Distinguishing affixoid formations from compounds"	https://paperswithcode.com/dataset/german-affixoid-dataset		GerAff					
2942	Cross-Modal Comments Dataset	"Cross Modal Automatic Commenting (CMAC) is a task which aims to automatically generate comments for graphic news. The CMAC dataset is a large-scale dataset for this task which consists of 24,134 graphic news. Each instance is composed of several news photos, news title, news body, and corresponding high-quality comments.
Source: https://github.com/lancopku/CMAC"	https://paperswithcode.com/dataset/cross-modal-comments-dataset							
2943	STREETS	"A novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL.
Source: STREETS: A Novel Camera Network Dataset for Traffic Flow"	https://paperswithcode.com/dataset/streets							
2944	PubMed PICO Element Detection Dataset	"PICO is a framework to formulate a well-defined focused clinical question. This framework identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). The PubMed PICO Element Detection dataset is a dataset for evaluating models that automatically detect PICO elements.
Source: https://github.com/jind11/PubMed-PICO-Detection"	https://paperswithcode.com/dataset/pubmed-pico-element-detection-dataset							
2945	SpatialVOC2K	"A multilingual image dataset with spatial relation annotations and object features for image-to-text generation, built using 2,026 images from the PASCAL VOC2008 dataset. 
Source: SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects"	https://paperswithcode.com/dataset/spatialvoc2k							
2946	CC-DBP	"CC-DBP is a dataset for knowledge base population research using Common Crawl and DBpedia.
Source: https://github.com/IBM/cc-dbp"	https://paperswithcode.com/dataset/cc-dbp							
2947	Earnings Call	"The Earning Calls dataset consists of processed earning conference calls data (text and audio). It can be used to predict financial risk from both textual and vocal features from conference calls.
Source: https://www.aclweb.org/anthology/P19-1038/"	https://paperswithcode.com/dataset/earnings-call							
2948	Processed Twitter	"Processed Twitter is a dataset that is used for Twitter topic recognition.  It contains tweets from 6 different topics.
Source: https://arxiv.org/pdf/1908.09931.pdf"	https://paperswithcode.com/dataset/processed-twitter							
2949	Grasp MultiObject	"Robotic grasp dataset for multi-object multi-grasp evaluation with RGB-D data.
This dataset is annotated using the same protocol as Cornell Dataset, and can be used as multi-object extension of Cornell Dataset.
Source: https://github.com/ivalab/grasp_multiObject
Image Source: https://github.com/ivalab/grasp_multiObject"	https://paperswithcode.com/dataset/grasp-multiobject							
2950	OFEQ-10k	"The OFEQ-10k dataset contains 12,548 detailed questions with corresponding math headlines from MathOverflow.
Source: https://arxiv.org/pdf/1912.00839.pdf"	https://paperswithcode.com/dataset/ofeq-10k							
2951	Pascal Panoptic Parts	"The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset. It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part
Source: https://arxiv.org/abs/2106.06351
Image Source: https://github.com/tue-mps/panoptic_parts"	https://paperswithcode.com/dataset/pascal-panoptic-parts	11/06/2021						
2952	JSS Dataset	"The Jejueo Single Speaker Speech (JSS) dataset consists of 10k high-quality audio files recorded by a native Jejueo speaker and a transcript file.
Source: https://arxiv.org/abs/1911.12071"	https://paperswithcode.com/dataset/jss-dataset		Jejueo Single Speaker Speech					
2953	needadvice	"needadvice is a dataset for advice classification extracted from Reddit. In this dataset, posts are annotated for whether they contain advice or not. It contains 6,148 samples for training, 816 for validation and 898 for testing.
Source: https://github.com/venkatasg/Advice-EMNLP2020"	https://paperswithcode.com/dataset/needadvice							
2954	FB1.5M	"The FB1.5M dataset is a benchmark for Knowledge Graph Completion. It is based on Freebase and it contains 30 relations with less than 500 triplets as low-resource relations.
Source: https://arxiv.org/pdf/1911.03091.pdf"	https://paperswithcode.com/dataset/fb1-5m							
2955	Wiki-zh	"Wiki-zh is an annotated Chinese dataset for domain detection extracted from Wikipedia. It includes texts from 7 different domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW), “Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). It contains 26,280 documents split into training, validation and test.
Source: https://arxiv.org/pdf/1907.11499.pdf"	https://paperswithcode.com/dataset/wiki-zh							
2956	Simulated Flying Shapes	"The dataset consists of 90 000 grayscale videos that show two objects of equal shape and size in which one object approaches the other one. The object speed during the process of approaching is hereby modelled by a proportional-derivative controller. Overall, three different shapes (Rectangle, Triangle and Circle) are provided. Initial conﬁguration of the objects such as position and color were randomly sampled. Different from the moving MNIST dataset, the samples comprise a goal-oriented task, namely one object has to fully cover the other object rather than randomly moving, making it better suitable for testing prediction capabilities of an ML model.
For instance, one can use it as a toy dataset to investigate the capacity and output behavior of a deep neural network before testing it on real-world data.
Source: https://github.com/ferreirafabio/FlyingShapesDataset
Image Source: https://github.com/ferreirafabio/FlyingShapesDataset"	https://paperswithcode.com/dataset/simulated-flying-shapes							
2957	PKU-Reid	"This dataset contains 114 individuals including 1824 images captured from two disjoint camera views. For each person, eight images are captured from eight different orientations under one camera view and are normalized to 128x48 pixels. This dataset is also split into two parts randomly. One contains 57 individuals for training, and the other contains 57 individuals for testing.
Source: https://github.com/charliememory/PKU-Reid-Dataset
Image Source: https://arxiv.org/pdf/1605.02464.pdf"	https://paperswithcode.com/dataset/pku-reid							
2958	YFCC100M Fine-Grained Geolocation	"The YFCC100M Fine-Grained Geolocation dataset is a subset of 100 a set of 36,146 YFCC100M images that had Flickr tags that could be identified as corresponding to one of the labels in the iNaturalist 2017 dataset. The 36,146 images that were selected so have the following characteristics:
the image must have geolocation available,
the image must have at most one iNaturalist label,
at most ten examples were retained for each label.
Source: https://github.com/visipedia/fg_geo
Image Source: https://github.com/visipedia/fg_geo"	https://paperswithcode.com/dataset/yfcc100m-fine-grained-geolocation							
2959	AuxAI	"AuxAI is a distantly supervised dataset for acronym identification.
Source: https://github.com/PrimerAI/sdu-data"	https://paperswithcode.com/dataset/auxai							
2960	PART-OF	"The PART-OF dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are parts of the human body. The dataset has 16,894 nodes with 19,436 edges between them.
Source: https://arxiv.org/pdf/1906.05939.pdf"	https://paperswithcode.com/dataset/part-of							
2961	ITG	"In The Groove (ITG) is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 133 songs choreographed by a three different authors, with 652 charts for the 133 songs.
Source: https://arxiv.org/pdf/1703.06891.pdf
Image Source: https://github.com/chrisdonahue/ddc"	https://paperswithcode.com/dataset/itg		In The Groove					
2962	KorSTS	"KorSTS is a dataset for semantic textural similarity (STS) in Korean. The dataset is constructed by automatically the STS-B dataset. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other’s translation afterward.
The KorSTS dataset comprises 5,749 training examples translated automatically and 2,879 evaluation examples translated manually.
Source: https://github.com/kakaobrain/KorNLUDatasets
Image Source: https://github.com/kakaobrain/KorNLUDatasets"	https://paperswithcode.com/dataset/korsts							
2963	Detection of Traffic Anomaly	"Contains 4,677 videos with temporal, spatial, and categorical annotations.
Source: When, Where, and What? A New Dataset for Anomaly Detection in Driving Videos"	https://paperswithcode.com/dataset/detection-of-traffic-anomaly		DoTA					
2964	IN2LAAMA	"IN2LAAMA is a set of lidar-inertial datasets collected with a Velodyne VLP-16 lidar and a Xsens MTi-3 IMU.
Source: IN2LAAMA: INertial Lidar Localisation Autocalibration And MApping"	https://paperswithcode.com/dataset/in2laama	06/04/2020						
2965	TartanAir	"A dataset for robot navigation task and more. The data is collected in photo-realistic simulation environments in the presence of various light conditions, weather and moving objects.
Source: TartanAir: A Dataset to Push the Limits of Visual SLAM"	https://paperswithcode.com/dataset/tartanair							
2966	Snopes	"Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from snopes.com.
Source: Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News"	https://paperswithcode.com/dataset/snopes							
2967	DailyDialog++	"Consists of (i) five relevant responses for each context and (ii) five adversarially crafted irrelevant responses for each context.
Source: Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining"	https://paperswithcode.com/dataset/dailydialog-1							
2968	Multilingual LibriSpeech	"Multilingual LibriSpeech is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages. 
Source: MLS: A Large-Scale Multilingual Dataset for Speech Research"	https://paperswithcode.com/dataset/multilingual-librispeech		MLS					
2969	CITR & DUT	"Consists of two pedestrian trajectory datasets, CITR dataset and DUT dataset, so that the pedestrian motion models can be further calibrated and verified, especially when vehicle influence on pedestrians plays an important role. 
CITR dataset consists of experimentally designed fundamental VCI scenarios (front, back, and lateral VCIs) and provides unique ID for each pedestrian, which is suitable for exploring a specific aspect of VCI.
DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus, which can be used for more general purpose VCI exploration.
Source: Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus"	https://paperswithcode.com/dataset/vci-dut		CITR dataset and DUT dataset					
2970	JNC	"The JNC data provides common supervision data for headline generation.
Source: A Large-Scale Multi-Length Headline Corpus for Analyzing Length-Constrained Headline Generation Model Evaluation"	https://paperswithcode.com/dataset/jnc		Japanese News Corpus					
2971	JHU-CROWD++	JHU-CROWD++ is A large-scale unconstrained crowd counting dataset with 4,372 images and 1.51 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. In addition, the dataset provides comparatively richer set of annotations like dots, approximate bounding boxes, blur levels, etc.	https://paperswithcode.com/dataset/jhu-crowd-1							
2972	Action Recognition in the Dark	"ARID is a dataset for action recognition in dark videos. It consists of over 3,780 video clips with 11 action categories.
Source: ARID: A New Dataset for Recognizing Action in the Dark"	https://paperswithcode.com/dataset/action-recognition-in-the-dark		ARID					
2973	Kuzushiji-Kanji	"Kuzushiji-Kanji is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale, 140,426 images), ranging from 1,766 examples to only a single example per class. Kuzushiji is a Japanese cursive writing style.
Source: Deep Learning for Classical Japanese Literature
Image Source: Kuzushiji-MNIST"	https://paperswithcode.com/dataset/kuzushiji-kanji							
2974	UCO-LAEO	"A dataset for building models that detect people Looking At Each Other (LAEO) in video sequences.
Source: LAEO-Net: revisiting people Looking At Each Other in videos"	https://paperswithcode.com/dataset/uco-laeo							
2975	Skeletics 152	"A curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset.
Source: Quo Vadis, Skeleton Action Recognition ?"	https://paperswithcode.com/dataset/skeletics-152-1							
2976	TVQA+	"TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.
Source: TVQA+: Spatio-Temporal Grounding for Video Question Answering
Image Source: https://github.com/jayleicn/TVQAplus"	https://paperswithcode.com/dataset/tvqa-1							
2977	Spot the Difference Corpus	"Spot the Difference Corpus is a corpus of task-oriented spontaneous dialogues which contains 54 interactions between pairs of subjects interacting to find differences in two very similar scenes. The corpus includes rich transcriptions, annotations, audio and video.
Source: The Spot the Difference corpus: a multi-modal corpus of spontaneous task oriented spoken interactions"	https://paperswithcode.com/dataset/spot-the-difference-corpus	14/05/2018						
2978	CCPE-M	"A dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language.
The corpus was constructed from dialogues between two paid crowd-workers using a Wizard-of-Oz methodology. One worker plays the role of an ""assistant"", while the other plays the role of a ""user"". The ""assistant"" is tasked with eliciting the ""user"" preferences about movies following a Coached Conversational Preference Elicitation (CCPE) methodology. In particular, the assistant is required to ask questions designed so as to minimize the bias in the terminology the ""user"" employs to convey his or her preferences, and obtain these in as natural language as possible. Each dialog is annotated with entity mentions, preferences expressed about entities, descriptions of entities provided, and other statements of entities.
Source: CCPE-M: Coached Conversational Preference Elicitation dataset for Movies"	https://paperswithcode.com/dataset/ccpe-m	01/09/2019	Coached Conversational Preference Elicitation dataset for Movies					
2979	COCO-CN	"COCO-CN is a bilingual image description dataset enriching MS-COCO with manually written Chinese sentences and tags. The new dataset can be used for multiple tasks including image tagging, captioning and retrieval, all in a cross-lingual setting.
Source: COCO-CN"	https://paperswithcode.com/dataset/coco-cn	22/05/2018						
2980	T2 Guiding	"T2 Guiding is a dataset of 1000 images, each with six image labels. The images are from the Open Images Dataset (OID) and the dataset includes 2 sets of machine-generated labels for these images.

Object labels: Three random object labels generated by a FRCNN model trained on Visual Genome.
Image labels: Three random image labels obtained from Google Cloud Vision API.

Source: T2-Guiding"	https://paperswithcode.com/dataset/t2-guiding	04/12/2020						
2981	FarsBase-KBP	"FarsBase-KBP contains 22015 sentences, in which the entities and relation types are linked to the FarsBase ontology. This gold dataset can be reused for benchmarking KBP systems in the Persian language.
Source:"	https://paperswithcode.com/dataset/farsbase-kbp	04/05/2020						
2982	Visual Wake Words	"Visual Wake Words represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models.
Source: Visual Wake Words Dataset
Image Source: Chowdhery et al"	https://paperswithcode.com/dataset/visual-wake-words	12/06/2019						
2983	ReQA	"Retrieval Question-Answering (ReQA) benchmark tests a model’s ability to retrieve relevant answers efficiently from a large set of documents.
Source: ReQA: An Evaluation for End-to-End Answer Retrieval Models"	https://paperswithcode.com/dataset/reqa	10/07/2019	Retrieval Question-Answering					
2984	ROSE	"Retinal OCTA SEgmentation dataset (ROSE) consists of 229 OCTA images with vessel annotations at either centerline-level or pixel level.
Source: ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model
Image Source: https://imed.nimte.ac.cn/dataofrose.html"	https://paperswithcode.com/dataset/rose	10/07/2020	Retinal OCTA SEgmentation dataset					
2985	UDIVA	"UDIVA is a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self and peer-reported personality, internal state, and relationship profiling from participants. 
Source: Context-Aware Personality Inference in Dyadic Scenarios: Introducing the UDIVA Dataset"	https://paperswithcode.com/dataset/udiva	28/12/2020						
2986	MuST-Cinema	"MuST-Cinema is a Multilingual Speech-to-Subtitles corpus ideal for building subtitle-oriented machine and speech translation systems.
It comprises audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations.
MuST-Cinema was built by annotating MuST-C with subtitle breaks based on the original subtitle files. Special symbols have been inserted in the aligned sentences to mark subtitle breaks as follows:

<eob>: block break (breaks between subtitle blocks)
<eol>: line breaks (breaks between lines inside the same subtitle block)

Source: MuST-Cinema"	https://paperswithcode.com/dataset/must-cinema	25/02/2020						
2987	LIV360SV	"The dataset contains 26,645, 360 degree, street-level images collected via cycling with a GoPro Fusion camera, recorded Jan 14th -- 18th 2020. 10,106 advertisements were identified and classified as food (1335), alcohol (217), gambling (149) and other (8405) (e.g., cars and broadband).
Source: A deep learning approach to identify unhealthy advertisements in street view images"	https://paperswithcode.com/dataset/liv360sv	09/07/2020	Liverpool 360 degree Street View					
2988	i3-video	"The i3-video dataset contains ""is-it-instructional"" annotations for 6.4k videos from Youtube-8M. The videos are considered to be instructional if they focus on real-world human actions accompanied by procedural language that explains what’s happening on screen in reasonable details.
Source: i3-video"	https://paperswithcode.com/dataset/i3-video	29/04/2020	is-it-instructional-video					
2989	Open Images V4	"Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning.
Source: The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale
Image Source: https://storage.googleapis.com/openimages/web/index.html"	https://paperswithcode.com/dataset/open-images-v4	02/11/2018						
2990	CLaRO	"CLaRO is a new dataset of 234 Competency Questions that had been processed automatically into 106 patterns. The coverage of CLaRO, with its 93 main templates and 41 linguistic variants, is about 90% for unseen questions.
Source: CLaRO: a Data-driven CNL for Specifying Competency Questions"	https://paperswithcode.com/dataset/claro	17/07/2019						
2991	ImagiFilter	"ImagiFilter focusses on photographic and/or natural images, a very common use-case in computer vision research. Annotations for coarse prediction are provided, i.e. photographic vs. non-photographic, and smaller fine-grained prediction tasks where the non-photographic class is broken down into five classes: maps, drawings, graphs, icons, and sketches.
Source: ImagiFilter: A resource to enable the semi-automatic mining of images at scale"	https://paperswithcode.com/dataset/imagifilter	20/08/2020						
2992	CROSS	"Cross-Reference Omnidirectional Stitching IQA is a novel omnidirectional image dataset containing stitched images as well as dual-fisheye images captured from standard quarters of 0◦, 90◦ , 180◦ and 270◦. In this manner, when evaluating the quality of an image stitched from a pair of fisheye images (e.g., 0◦ and 180◦), the other pair of fisheye images (e.g., 90◦ and 270◦) can be used as the cross-reference to provide ground-truth observations of the stitching regions.
Source: Image Quality Assessment for Omnidirectional Cross-reference Stitching"	https://paperswithcode.com/dataset/cross	10/04/2019	Cross-Reference Omnidirectional Stitching IQA					
2993	BIRD	"Blocksworld Image Reasoning Dataset (BIRD) contains images of wooden blocks in different configurations, and the sequence of moves to rearrange one configuration to the other. 
Source: Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs"	https://paperswithcode.com/dataset/bird	28/05/2019	Blocksworld Image Reasoning Dataset					
2994	Almawave-SLU	"Almawave-SLU is the first Italian dataset for Spoken Language Understanding (SLU). It is derived through a semi-automatic procedure and is used as a benchmark of various open source and commercial systems.
Source: Almawave-SLU: A new dataset for SLU in Italian"	https://paperswithcode.com/dataset/almawave-slu	17/07/2019						
2995	EPIC30M	"EPIC30M contains a subset of 26.2 millions tweets related to three general diseases, namely Ebola, Cholera and Swine Flu, and another subset of 4.7 millions tweets of six global epidemic outbreaks, including 2009 H1N1 Swine Flu, 2010 Haiti Cholera, 2012 Middle-East Respiratory Syndrome (MERS), 2013 West African Ebola, 2016 Yemen Cholera and 2018 Kivu Ebola.
Source: EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets"	https://paperswithcode.com/dataset/epic30m	09/06/2020						
2996	The Spoken Wikipedia Corpora	"The SWC is a corpus of aligned Spoken Wikipedia articles from the English, German, and Dutch Wikipedia. This corpus has several outstanding characteristics:

hundreds of hours of aligned audio
from a diverse set of readers
about a diverse set of topics
in a well-researched textual genre
licensed under a free license (CC BY-SA 4.0)
Annotations can be mapped back to the original html
phoneme-level alignments"	https://paperswithcode.com/dataset/the-spoken-wikipedia-corpora		The Spoken Wikipedia Corpora					
2997	Flickr Audio Caption Corpus	"The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery. For a description of the corpus, see:
D. Harwath and J. Glass, ""Deep Multimodal Semantic Embeddings for Speech and Images,"" 2015 IEEE Automatic Speech Recognition and Understanding Workshop, pp. 237-244, Scottsdale, Arizona, USA, December 2015"	https://paperswithcode.com/dataset/flickr-audio-caption-corpus		Flickr Audio Caption Corpus					
2998	PCVC	The Persian Consonant Vowel Combination (PCVC) dataset is a phoneme based speech dataset, and also the first free Persian speech dataset to help Persian speech researchers. This dataset contains of 23 Persian consonants and 6 vowels. The sound samples are all possible combinations of vowels and consonants (138 samples for each speaker) with a length of 30000 data samples. The sample rate of all speech samples is 48000 which means there are 48000 sound samples in every 1 second. In each sample, sound starts with consonant and then there is a vowel sound and at last there is silent. length of silence is dependent on length of combination of consonant and vowel. For example if combination ends in 20000th data sample, so the rest of 10000 sample (until 30000, the length of each sound sample) are silence.	https://paperswithcode.com/dataset/pcvc		Persian Consonant Vowel Combination					
2999	DensePose-Track	"DensePose-Track is a dataset of videos where selected frames are annotated in the traditional DensePose manner.
Source: Slim DensePose: Thrifty Learning from Sparse Annotations and Motion Cues"	https://paperswithcode.com/dataset/densepose-track	13/06/2019						
3000	2000 HUB5 English	"2000 HUB5 English Evaluation Transcripts was developed by the Linguistic Data Consortium (LDC)  and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by NIST (National Institute of Standards and Technology). 
The Hub5 evaluation series focused on conversational speech over the telephone with the particular task of transcribing conversational speech into text. Its goals were to explore promising new areas in the recognition of conversational speech, to develop advanced technology incorporating those ideas and to measure the performance of new technology."	https://paperswithcode.com/dataset/2000-hub5-english		2000 HUB5 English					
3001	Parkinson Speech Dataset	Parkinson Speech Dataset is an audio dataset consisting of recordings of 20 Parkinson's Disease (PD) patients and 20 healthy subjects. From all subjects, multiple types of sound recordings (26) are taken. The goal is to classify which patients have Parkinson's.	https://paperswithcode.com/dataset/parkinson-speech-dataset		Parkinson Speech Dataset					
3002	MDID	"The Multimodal Document Intent Dataset (MDID) is a dataset for computing author intent from multimodal data from Instagram. It contains 1,299 Instagram posts covering a variety of topics, annotated with labels from three taxonomies. The samples are labelled with 7 labels of intent: Provocative, Informative, Advocative, Entertainment, Expositive, Expressive, Promotive
Source: Integrating Text and Image: Determining Multimodal Document Intent in Instagram Posts"	https://paperswithcode.com/dataset/mdid	19/04/2019	Multimodal Document Intent Dataset					
3003	Ciona17	"Ciona17 is a semantic segmentation dataset with pixel-level annotations pertaining to invasive species in a marine environment. Diverse outdoor illumination, a range of object shapes, colour, and severe occlusion provide a significant real world challenge for the computer vision community. 
Source: The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment
Image Source: Galloway et al"	https://paperswithcode.com/dataset/ciona17	18/02/2017						
3004	Arabic Speech Corpus	The Arabic Speech Corpus (1.5 GB) is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes The Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton. The corpus was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.	https://paperswithcode.com/dataset/arabic-speech-corpus		Arabic Speech Corpus					
3005	Mivia Audio Events Dataset	"The MIVIA audio events data set is composed of a total of 6000 events for surveillance applications, namely glass breaking, gun shots and screams. The 6000 events are divided into a training set (composed of 4200 events) and a test set (composed of 1800 events).
In audio surveillance applications, the events of interest (for instance a scream) can occur at different distances from the microphone that correspond to different levels of the signal-to-noise ratio. Moreover, in these applications the events are generally mixed with a complex background, usually composed of several types of different sounds depending on the specific environments both indoor and outdoor (household appliances, cheering of crowds, talking people, traffic jam, passing cars or motorbikes etc.).
The data set is designed to provide each audio event at 6 different values of signal-to-noise ratio (namely 5dB, 10dB, 15dB, 20dB, 25dB and 30dB) and overimposed to different combinations of environmental sounds in order to simulate their occurrence in different ambiences."	https://paperswithcode.com/dataset/mivia-audio-events-dataset		Mivia Audio Events Dataset					
3006	VRAI	"VRAI is a large-scale vehicle ReID dataset for UAV-based intelligent applications. The dataset consists of 137, 613 images of 13, 022 vehicle instances. The images of each vehicle instance are captured by cameras of two DJI consumer UAVs at different locations, with a variety of view angles and flight-altitudes (15m to 80m).
Source: Vehicle Re-identification in Aerial Imagery: Dataset and Approach"	https://paperswithcode.com/dataset/vrai	02/04/2019	Vehicle Re-identification for Aerial Image					
3007	RWCP Sound Scene Database	The RWCP Sound Scene Database includes non-speech sounds recorded in an anechoic room, reconstructed signals in various rooms, impulse responses for a microphone array, speech data recorded with the same array, and recordings of background noises. It is intended for use when simulating sound scenes. It was developed by the Real Acoustic Environments Working Group of the Real World Computing Partnership (RWCP). The data was recorded from 1998 to 2000.	https://paperswithcode.com/dataset/rwcp-sound-scene-database		RWCP Sound Scene Database					
3008	UIT-ViIC	"UIT-ViIC contains manually written captions for images from Microsoft COCO dataset relating to sports played with ball. UIT-ViIC consists of 19,250 Vietnamese captions for 3,850 images.
Source: UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning"	https://paperswithcode.com/dataset/uit-viic	01/02/2020						
3009	NAR	"NAR is a dataset  of audio recordings made with the humanoid robot Nao in real world conditions for sound recognition benchmarking. All the recordings were collected using the robot’s microphone and thus have the following characteristics:
- recorded with low-quality sensors (300 Hz – 18 kHz bandpass)
- suffering from typical fan noise from the robot’s internal hardware
- recorded in mutiple real domestic environments (no special acoustic charateristics, reverberations, presence of multiple sound sources and unknown locations)"	https://paperswithcode.com/dataset/nar		NAR					
3010	IISc VINE	"Indian Institute of Science VIdeo Naturalness Evaluation (IISc VINE) is a database consisting of 300 videos, obtained by applying different prediction models on different datasets, and accompanying human opinion scores. 
Source: A Naturalness Evaluation Database for Video Prediction Models"	https://paperswithcode.com/dataset/iisc-vine	01/05/2020	Indian Institute of Science VIdeo Naturalness Evaluation					
3011	MineNav	"MinNav is a synthetic dataset based on the sandbox game Minecraft. The dataset uses several plug-in program to generate rendered image sequences with time-aligned depth maps, surface normal maps and camera poses. Thanks for the large game's community, there is an extremely large number of 3D open-world environment, users can find suitable scenes for shooting and build data sets through it and they can also build scenes in-game. 
Source: MineNav: An Expandable Synthetic Dataset Based on Minecraft for Aircraft Visual Navigation"	https://paperswithcode.com/dataset/minenav	19/08/2020						
3012	Minecraft House	"Minecraft House is a crowd sourced dataset that collects examples of humans building houses in Minecraft.  Each user is asked to build a CraftAssist: A Framework for Dialogue-enabled Interactive Agents house on a fixed time budget (30 minutes), without any additional guidance or instructions. Every action of the user is recorded using the Cuberite server.
The data collection was performed in Minecraft’s creative mode, where the user is given unlimited resources, has access to all material block types and can freely move in
the game world. The action space of the environment is straight-forward: moving in x-y-z dimensions, choosing a block type, and placing or breaking a block.
There are 2586 houses in total."	https://paperswithcode.com/dataset/minecraft-house	19/07/2019	Minecraft House					
3013	ARVSU	"ARVSU contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario.
Source: Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes with Utterances"	https://paperswithcode.com/dataset/arvsu	12/09/2018	Addressee Recognition in Visual Scenes with Utterances					
3014	m2cai16-tool-locations	"The m2cai16-tool-locations dataset contains spatial tool annotations for 2,532 frames across the first 10 videos in the m2cai16-tool dataset, which includes 15 videos in total. The dataset consists of 3,141 annotations of 7 surgical instrument classes, with an average of 1.2 labels per frame and 7 instrument classes per video.
Source: http://ai.stanford.edu/~syyeung/tooldetection.html"	https://paperswithcode.com/dataset/m2cai16-tool-locations	24/02/2018						
3015	Minecraft Segmentation	"Minecraft Segmentation is a segmentation dataset for the Minecraft House that adds semantic
segmentation labels for sub-components of the house. There are 2050 houses in total and 1038 distinct labels of subcomponents."	https://paperswithcode.com/dataset/minecraft-segmentation	19/07/2019	Minecraft Segmentation					
3016	CryoNuSeg	"CryoNuSeg is a fully annotated FS-derived cryosectioned and H&E-stained nuclei instance segmentation dataset. The dataset contains images from 10 human organs that were not exploited in other publicly available datasets, and is provided with three manual mark-ups to allow measuring intra-observer and inter-observer variability.
Source: CryoNuSeg: A Dataset for Nuclei Instance Segmentation of Cryosectioned H&E-Stained Histological Images
Image Source: https://github.com/masih4/CryoNuSeg"	https://paperswithcode.com/dataset/cryonuseg	02/01/2021						
3017	Princeton Shape	The Princeton Shape dataset provides a repository of 3D models and software tools for evaluating shape-based retrieval and analysis algorithms.  The motivation is to promote the use of standardized data sets and evaluation methods for research in matching, classification, clustering, and recognition of 3D models.  Researchers are encouraged to use these resources to produce comparisons of competing algorithms in future publications. There are 1,814 models in total.	https://paperswithcode.com/dataset/princeton-shape		Princeton Shape					
3018	Opusparcus	"Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows.
For each target language, the Opusparcus data have been partitioned into three types of data sets: training, development and test sets. The training sets are large, consisting of millions of sentence pairs, and have been compiled automatically, with the help of probabilistic ranking functions. The development and test sets consist of sentence pairs that have been annotated manually; each set contains approximately 1000 sentence pairs that have been verified to be acceptable paraphrases by two annotators.
Source: Opusparcus"	https://paperswithcode.com/dataset/opusparcus	17/09/2018						
3019	IKEA 3D	IKEA 3D is a dataset of IKEA 3D models and aligned images, which is suitable for pose estimation. There are 759 images and 219 models including Sketchup (skp) and Wavefront (obj) files.	https://paperswithcode.com/dataset/ikea-3d		IKEA 3D					
3020	RSOC	"RSOC is a large-scale object counting dataset with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, large-vehicles and small-vehicles in parking lots.
Source :Counting from Sky: A Large-scale Dataset for Remote Sensing Object Counting and A Benchmark Method"	https://paperswithcode.com/dataset/rsoc	28/08/2020	Remote Sensing Object Counting					
3021	A Large Dataset of Object Scans	A Large Dataset of Object Scans is a dataset of more than ten thousand 3D scans of real objects. To create the dataset, the authors recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. The authors worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was placed in the public domain and is available freely.	https://paperswithcode.com/dataset/a-large-dataset-of-object-scans	08/02/2016	A Large Dataset of Object Scans					
3022	RGRS	"RGRS is a dataset for collaboratior recommendation on the ResearchGate academic social network. The data has been collected from Jan. 2019 to April 2019 and
includes raw data of 3980 RG users. 
Source: Presenting a Dataset for Collaborator Recommending Systems in Academic Social Network: a Case Study on ReseatchGate"	https://paperswithcode.com/dataset/rgrs	29/12/2020	ResearchGate dataset for Recommending Systems					
3023	ObjectNet3D	ObjectNet3D is a large scale database for 3D object recognition, named, that consists of 100 categories, 90,127 images, 201,888 objects in these images and 44,147 3D shapes. Objects in the images in the database are aligned with the 3D shapes, and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object. Consequently, the database is useful for recognizing the 3D pose and 3D shape of objects from 2D images. Authors also provide baseline experiments on four tasks: region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval, which can serve as baselines for future research.	https://paperswithcode.com/dataset/objectnet3d		ObjectNet3D					
3024	Event-Stream Dataset	"Event-Stream Dataset is a robotic grasping dataset with 91 objects.
Source: Event-based Robotic Grasping Detection with Neuromorphic Vision Sensor and Event-Stream Dataset"	https://paperswithcode.com/dataset/event-stream-dataset	28/04/2020						
3025	PersonalDialog	"PersonalDialog is a large-scale multi-turn dialogue dataset containing various traits from a large number of speakers. The dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers. Each utterance is associated with a speaker who is marked with traits like Age, Gender, Location, Interest Tags, etc. Several anonymization schemes are designed to protect the privacy of each speaker. 
Source: Personalized Dialogue Generation with Diversified Traits
Image Source: https://arxiv.org/pdf/1901.09672v2.pdf"	https://paperswithcode.com/dataset/personaldialog	28/01/2019						
3026	Thingi10K	Thingi10K is a dataset of 3D-Printing Models. Specifically there are 10,000 models from featured “things” on thingiverse.com, suitable for testing 3D printing techniques such as structural analysis , shape optimization, or solid geometry operations.	https://paperswithcode.com/dataset/thingi10k		Thingi10K					
3027	CocoDoom	"CocoDoom is a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format.
Source: ResearchDoom and CocoDoom: Learning Computer Vision with Games"	https://paperswithcode.com/dataset/cocodoom	07/10/2016						
3028	VOCASET	"VOCASET is a 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio. The dataset has 12 subjects and 480 sequences of about 3-4 seconds each with sentences chosen from an array of standard protocols that maximize phonetic diversity.
Source: timzhang642"	https://paperswithcode.com/dataset/vocaset	08/05/2019	VOCASET					
3029	MSAW	"Multi-Sensor All Weather Mapping (MSAW) is a dataset and challenge, which features two collection modalities (both SAR and optical). The dataset and challenge focus on mapping and building footprint extraction using a combination of these data sources. MSAW covers 120 km^2 over multiple overlapping collects and is annotated with over 48,000 unique building footprints labels, enabling the creation and evaluation of mapping algorithms for multi-modal data. 
Source: SpaceNet 6: Multi-Sensor All Weather Mapping Dataset"	https://paperswithcode.com/dataset/msaw	14/04/2020	Multi-Sensor All Weather Mapping					
3030	ADE-Affordance	"ADE-Affordance is a new dataset that builds upon ADE20k, which contains annotations enabling such rich visual reasoning. 
Source: Learning to Act Properly: Predicting and Explaining Affordances from Images"	https://paperswithcode.com/dataset/ade-affordance	20/12/2017						
3031	PISC	"The People in Social Context (PISC) dataset is a dataset that focuses on social relationships. It consists of 22,670 images of 9 types of social relationships. It has annotations for the bounding boxes of all people, as well as the social relationship between all pairs of people in the images. In addition, it also contains occupation annotation. 
Source: PISC"	https://paperswithcode.com/dataset/pisc	02/08/2017	People in Social Context					
3032	MINOS	MINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites.	https://paperswithcode.com/dataset/minos	11/12/2017	MINOS					
3033	WIKIOG	"WIKIOG is a public collection which consists of over 1.75 million document-outline pairs for research on the OG task. 
Source: Outline Generation: Understanding the Inherent Content Structure of Documents"	https://paperswithcode.com/dataset/wikiog	24/05/2019						
3034	SemanticUSL	"SemanticUSL is a dataset for domain adaptation for LiDAR point cloud semantic segmentation. The dataset has the same data format and ontology as SemanticKITTI. 
Source: LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud Semantic Segmentation
Image Source: https://unmannedlab.github.io/semanticusl"	https://paperswithcode.com/dataset/semanticusl	02/03/2020						
3035	Jericho	"Jericho is a learning environment for man-made Interactive Fiction (IF) games.
Source: Interactive Fiction Games: A Colossal Adventure"	https://paperswithcode.com/dataset/jericho	11/09/2019						
3036	3D-FRONT	"3D-FRONT (3D Furnished Rooms with layOuts and semaNTics) is large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, the dataset is freely available to the academic community and beyond. 
3D-FRONT contains 18,797 rooms diversely furnished by 3D objects. In addition, the 7,302 furniture objects all come with high-quality textures. While the floorplans and layout designs are directly sourced from professional creations, the interior designs in terms of furniture styles, color, and textures have been carefully curated based on a recommender system to attain consistent styles as expert designs."	https://paperswithcode.com/dataset/3d-front	18/11/2020	3D-FRONT					
3037	JParaCrawl	"JParaCrawl is a parallel corpus for English-Japanese, for which the amount of publicly available parallel corpora is still limited. The parallel corpus was constructed by broadly crawling the web and automatically aligning parallel sentences. The corpus amassed over 8.7 million sentence pairs.
Source: JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus"	https://paperswithcode.com/dataset/jparacrawl	25/11/2019						
3038	3ThreeDWorld	TDW is a 3D virtual world simulation platform, utilizing state-of-the-art video game engine technology. A TDW simulation consists of two components: a) the Build, a compiled executable running on the Unity3D Engine, which is responsible for image rendering, audio synthesis and physics simulations; and b) the Controller, an external Python interface to communicate with the build.	https://paperswithcode.com/dataset/3threedworld	09/07/2020	3ThreeDWorld					
3039	MUSIC	"The Multi-Spectral Imaging via Computed Tomography (MUSIC) dataset is a two-part (2D- and 3D spectral) open access dataset for advanced image analysis of spectral radiographic (x-ray) scans, their tomographic reconstruction and the detection of specific materials within such scans. The scans operate at a photon energy range of around 20 keV up to 160 keV.
The dataset includes — for 2D- as well as 3D spectral data — the corrected (e.g. calibrated) radiographic projections, their tomographic reconstructions (based on 37 projections of 256 detector pixels into a 100×100 pixel CT image per slice) and the corresponding set of segmentation variants.
Source: Multi-Spectral Imaging via Computed Tomography (MUSIC)"	https://paperswithcode.com/dataset/music	28/10/2018	Multi-Spectral Imaging via Computed Tomography					
3040	MuMu	"MuMu is a new dataset of more than 31k albums classified into 250 genre classes.
Source: Multi-label Music Genre Classification from Audio, Text, and Images Using Deep Features"	https://paperswithcode.com/dataset/mumu	16/07/2017						
3041	FSVQA	"Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset.
Source: The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)"	https://paperswithcode.com/dataset/fsvqa	21/09/2016	Full-Sentence Visual Question Answering					
3042	Taskmaster-1	"Taskmaster-1 is a dialog dataset consisting of 13,215 task-based dialogs in English, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.
Image Source: https://arxiv.org/pdf/1909.05358v1.pdf"	https://paperswithcode.com/dataset/taskmaster-1	01/09/2019	Taskmaster-1					
3043	RealEstate10K	RealEstate10K is a large dataset of camera poses corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. For each clip, the poses form a trajectory where each pose specifies the camera position and orientation along the trajectory. These poses are derived by running SLAM and bundle adjustment algorithms on a large set of videos.	https://paperswithcode.com/dataset/realestate10k	24/05/2018	RealEstate10K					
3044	Wikipedia Generation	Wikipedia Generation is a dataset for article generation from Wikipedia from references at the end of Wikipedia page and the top 10 search results for the Wikipedia topic.	https://paperswithcode.com/dataset/wikipedia-generation	30/01/2018	Wikipedia Generation					
3045	WildestFaces	"WildestFaces is tailored to study cross-domain recognition under a variety of adverse conditions. 
Source: Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos"	https://paperswithcode.com/dataset/wildestfaces	16/09/2020						
3046	FAD	"FAD is a dataset that have roughly 200,000 attribute labels for the above traits, for over 10,000 facial images.
Source: Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information"	https://paperswithcode.com/dataset/fad	29/05/2016	Face Attributes Dataset					
3047	VQA 360°	"VQA 360° is a dataset for visual question answering on 360° images containing around 17,000 real-world image-question-answer triplets for a variety of question types. 
Source: Visual Question Answering on 360° Images"	https://paperswithcode.com/dataset/vqa-360deg	10/01/2020						
3048	StreetStyle	"StreetStyle is a large-scale dataset of photos of people annotated with clothing attributes, and use this dataset to train attribute classifiers via deep learning.
Source: StreetStyle: Exploring world-wide clothing styles from millions of photos
Image Source: Matzen et al"	https://paperswithcode.com/dataset/streetstyle	06/06/2017						
3049	PHSPD	"PHSPD is a home-grown polarization image dataset of various human shapes and poses.
Source: Polarization Human Shape and Pose Dataset"	https://paperswithcode.com/dataset/phspd	30/04/2020	Polarization Human Shape and Pose Dataset					
3050	HARRISON	"HARRISON dataset is a benchmark on hashtag recommendation for real world images in social networks. The HARRISON dataset is a realistic dataset, composed of 57,383 photos from Instagram and an average of 4.5 associated hashtags for each photo.
Source: HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks"	https://paperswithcode.com/dataset/harrison	17/05/2016						
3051	OC20	Open Catalyst 2020 is a dataset for catalysis in chemical engineering. Focusing on molecules that are important in renewable energy applications, the OC20 data set comprises over 1.3 million relaxations of molecular adsorptions onto surfaces, the largest data set of electrocatalyst structures to date.	https://paperswithcode.com/dataset/oc20		Open Catalyst 2020					
3052	FSOD	"Few-Shot Object Detection Dataset (FSOD) is a high-diverse dataset specifically designed for few-shot object detection and intrinsically designed to evaluate thegenerality of a model on novel categories.
Source: FSOD"	https://paperswithcode.com/dataset/fsod	06/08/2019	Few-Shot Object Detection Dataset					
3053	DUS	The Daimler Urban Segmentation Dataset is a dataset for semantic segmentation. It consists of video sequences recorded in urban traffic. The dataset consists of 5000 rectified stereo image pairs with a resolution of 1024x440. 500 frames (every 10th frame of the sequence) come with pixel-level semantic class annotations into 5 classes: ground, building, vehicle, pedestrian, sky. Dense disparity maps are provided as a reference, however these are not manually annotated but computed using semi-global matching (sgm).	https://paperswithcode.com/dataset/dus		Daimler Urban Segmentation					
3054	HumanAct12	"HumanAct12 is a new 3D human motion dataset adopted from the polar image and 3D pose dataset PHSPD, with proper temporal cropping and action annotating. Statistically, there are 1191 3D motion clips(and 90,099 poses in total) which are categorized into 12 action classes, and 34 fine-grained sub-classes. The action types includes daily actions such as walk, run, sit down, jump up, warm up, etc. Fine-grained action types contain more specific information like Warm up by bowing left side, Warm up by pressing left leg, etc. 
Source: Action2Motion: Conditioned Generation of 3D Human Motions"	https://paperswithcode.com/dataset/humanact12	30/07/2020						
3055	Large Age-Gap	Large Age-Gap (LAG) is a dataset for face verification, The dataset contains 3,828 images of 1,010 celebrities. For each identity at least one child/young image and one adult/old image are present.	https://paperswithcode.com/dataset/large-age-gap	19/02/2016	Large Age-Gap					
3056	Interestingness	"The Interestingness dataset contains movie excerpts and key-frames and corresponding ground truth files based on classification into interesting and non-interesting samples. It is used for multimedia content interestingness classification. The dataset is composed of:

Shots and key-frames from a set of 78 Hollywood-like movie trailers of different genres
The corresponding ground truth
Additional low-level and mid-level features"	https://paperswithcode.com/dataset/interestingness							
3057	AKCES-GEC	"AKCES-GEC is a new dataset on grammatical error correction for Czech.
Source: Grammatical Error Correction in Low-Resource Scenarios"	https://paperswithcode.com/dataset/akces-gec	01/10/2019						
3058	LASIESTA	LASIESTA (Labeled and Annotated Sequences for Integral Evaluation of SegmenTation Algorithms) is a segmentation and detection dataset composed by many real indoor and outdoor sequences organized into categories, each of one covering a specific challenge in moving object detection strategies.	https://paperswithcode.com/dataset/lasiesta		LASIESTA					
3059	E-GMD	"Expanded Groove MIDI dataset (E-GMD) is an automatic drum transcription (ADT) dataset that contains 444 hours of audio from 43 drum kits, making it an order of magnitude larger than similar datasets, and the first with human-performed velocity annotations.
Source: Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset"	https://paperswithcode.com/dataset/e-gmd	01/04/2020	Expanded Groove MIDI Dataset					
3060	FIRE	Fundus Image Registration Dataset (FIRE) is a dataset consisting of 129 retinal images forming 134 image pairs. These image pairs are split into 3 different categories depending on their characteristics. The images were acquired with a Nidek AFC-210 fundus camera, which acquires images with a resolution of 2912x2912 pixels and a FOV of 45° both in the x and y dimensions. Images were acquired at the Papageorgiou Hospital, Aristotle University of Thessaloniki, Thessaloniki from 39 patients.	https://paperswithcode.com/dataset/fire		Fundus Image Registration Dataset					
3061	MVB	"MVB (Multi View Baggage) is a dataset for baggage ReID task which has some essential differences from person ReID. The features of MVB are three-fold. First, MVB is the first publicly released large-scale dataset that contains 4519 baggage identities and 22660 annotated baggage images as well as its surface material labels. Second, all baggage images are captured by specially-designed multi-view camera system to handle pose variation and occlusion, in order to obtain the 3D information of baggage surface as complete as possible. Third, MVB has remarkable inter-class similarity and intra-class dissimilarity, considering the fact that baggage might have very similar appearance while the data is collected in two real airport environments, where imaging factors varies significantly from each other.
Source: MVB: A Large-Scale Dataset for Baggage Re-Identification and Merged Siamese Networks"	https://paperswithcode.com/dataset/mvb	26/07/2019	Multi View Baggage					
3062	300-VW	300 Videos in the Wild (300-VW) is a dataset for evaluating facial landmark tracking algorithms in the wild. The dataset authors collected a large number of long facial videos recorded in the wild. Each video has duration of ~1 minute (at 25-30 fps). All frames have been annotated with regards to the same mark-up (i.e. set of facial landmarks) used in the 300 W competition as well (a total of 68 landmarks). The dataset includes 114 videos (circa 1 min each).	https://paperswithcode.com/dataset/300-vw		300 Videos in the Wild					
3063	Imp1k	"Imp1k is a new dataset of designs annotated with importance information.
Source: Predicting Visual Importance Across Graphic Design Types"	https://paperswithcode.com/dataset/imp1k	07/08/2020						
3064	PIROPO	The PIROPO database (People in Indoor ROoms with Perspective and Omnidirectional cameras) comprises multiple sequences recorded in two different indoor rooms, using both omnidirectional and perspective cameras. The sequences contain people in a variety of situations, including people walking, standing, and sitting. Both annotated and non-annotated sequences are provided, where ground truth is point-based (each person in the scene is represented by the point located in the center of its head). In total, more than 100,000 annotated frames are available.	https://paperswithcode.com/dataset/piropo		PIROPO					
3065	Million-AID	"Million-AID is a large-scale benchmark dataset containing a million instances for RS scene classification. There are 51 semantic scene categories in Million-AID. And the scene categories are customized to match the land-use classification standards, which greatly enhance the practicability of the constructed Million-AID. Different form the existing scene classification datasets of which categories are organized with parallel or uncertain relationships, scene categories in Million-AID are organized with systematic relationship architecture, giving it superiority in management and scalability. Specifically, the scene categories in Million-AID are organized by the hierarchical category network of a three-level tree: 51 leaf nodes fall into 28 parent nodes at the second level which are grouped into 8 nodes at the first level, representing the 8 underlying scene categories of agriculture land, commercial land, industrial land, public service land, residential land, transportation land, unutilized land, and water area. The scene category network provides the dataset with excellent organization of relationship among different scene categories and also the property of scalability. The number of images in each scene category ranges from 2,000 to 45,000, endowing the dataset with the property of long tail distribution. Besides, Million-AID has superiorities over the existing scene classification datasets owing to its high spatial resolution, large scale, and global distribution.
Source: DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation
Image Source: https://captain-whu.github.io/DiRS/"	https://paperswithcode.com/dataset/million-aid	22/06/2020						
3066	Edge Milling Heads	"The Edge Milling Heads data set comprises 144 images of an edge profile cutting head of a milling machine. The head tool contains a total of 30 cutting inserts. The cutting head is formed by 6 diagonals of inserts in radial direction along the tool perimeter, encompassing 5 inserts per diagonal in axial direction. Positions of the last and first inserts of consecutive diagonals are aligned in the same vertical. Therefore, even though there are 30 inserts in total, there are 24 equally spaced positions of inserts along the tool perimeter. Additionally, inserts are squared shape with four 90º indexable cutting edges. Inserts are fastened with a screw. Rake angle is 0.
Images were taken with a monochrome camera Genie M1280 1/3’’ with active resolution of 1280 × 960 pixels. AZURE-2514MM fixed lens with 25 mm focal length were used.
Source: Computer Vision Online"	https://paperswithcode.com/dataset/edge-milling-heads		Edge Milling Heads					
3067	MLe2e	"MLe2 is a dataset for the evaluation of scene text end-to-end reading systems and all intermediate stages such as text detection, script identification and text recognition. The dataset contains a total of 711 scene images covering four different scripts (Latin, Chinese, Kannada, and Hangul).
Source: A fine-grained approach to scene text script identification"	https://paperswithcode.com/dataset/mle2e	24/02/2016						
3068	VxC TSG	"The VXC TSG is based on samples taken from the ceramic tile industry and is comprised of 14 ceramic tile models, 42 surface grades and 960 pieces. It has been built in the VxC laboratory, at the Polytechnic University of Valencia, in collaboration with Keraben S.A., a large ceramic tile company located at Nules province of Castellón (Spain).
Source: Computer Vision Online"	https://paperswithcode.com/dataset/vxc-tsg		VxC TSG					
3069	DADA-2000	"DADA-2000 is a large-scale benchmark with 2000 video sequences (named as DADA-2000) is contributed with laborious annotation for driver attention (fixation, saccade, focusing time), accident objects/intervals, as well as the accident categories, and superior performance to state-of-the-arts are provided by thorough evaluations. 
Source: DADA: A Large-scale Benchmark and Model for Driver Attention Prediction in Accidental Scenarios
Image Source: Fang et al"	https://paperswithcode.com/dataset/dada-2000	18/12/2019						
3070	Panoramic Image Database	The Panoramic Image Database is a panoramic image dataset. The databases were collected by Andrew Vardy while visiting with the Computer Engineering group in February and March of 2004. Images were captured by a robot-mounted camera, pointed upwards at a hyperbolic mirror. The camera was an ImagingSource DFK 4303. The robot was an ActivMedia Pioneer 3-DX. The mirror was a large wide-view hyperbolic mirror from Accowle Ltd. The hyperbolic mirror expands the camera's field of view to allow the capture of panoramic images.	https://paperswithcode.com/dataset/panoramic-image-database		Panoramic Image Database					
3071	SESIV	"SEmantic Salient Instance Video (SESIV) dataset is obtained by augmenting the DAVIS-2017 benchmark dataset by assigning semantic ground-truth for salient instance labels. The SESIV dataset consists of 84 high-quality video sequences with pixel-wisely per-frame ground-truth labels.
Source: Semantic Instance Meets Salient Object: Study on Video Semantic Salient Instance Segmentation"	https://paperswithcode.com/dataset/sesiv	04/07/2018	SEmantic Salient Instance Video					
3072	OTCBVS	"OCTCBVS is a benchmark dataset for testing and evaluating novel and state-of-the-art computer vision algorithms. The benchmark contains videos and images recorded in and beyond the visible spectrum and is available for free to all researchers in the international computer vision communities.
Source: Computer Vision Online"	https://paperswithcode.com/dataset/otcbvs		OTCBVS					
3073	LEAF-QA	"LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering.
Source: LEAF-QA: Locate, Encode & Attend for Figure Question Answering"	https://paperswithcode.com/dataset/leaf-qa	30/07/2019						
3074	Multi Task Crowd	"Multi Task Crowd is a new 100 image dataset fully annotated for crowd counting, violent behaviour detection and density level classification.
Source: ResnetCrowd: A Residual Deep Learning Architecture for Crowd Counting, Violent Behaviour Detection and Crowd Density Level Classification"	https://paperswithcode.com/dataset/multi-task-crowd	30/05/2017						
3075	DogCentric Activity	"The DogCentric Activity dataset is composed of dog activity videos taken from a first-person animal viewpoint. The dataset contains 10 different types of activities, including activities performed by the dog himself/herself, interactions between people and the dog, and activities performed by people or cars. 
The authors attached a GoPro camera to the back of each of the four dogs, and their owners took them on a walk to their familiar walking routes. The walking routes are in various environments, such as residential area, a park along a river, a sand beach, a field, streets with traffic, etc. Thus even though different dogs do the same activity, their background varies.
The video contains various activities with 10 activities of interest chosen as target activities: 'playing with a ball', 'waiting for a car to passed by', 'drinking water', 'feeding', 'turning dog's head to the left', 'turning dog's head to the right', 'petting', 'shaking dog's body by himself', 'sniffing', and 'walking'. The videos are in 320*240 image resolution, 48 frames per second."	https://paperswithcode.com/dataset/dogcentric-activity		DogCentric Activity					
3076	Visual Question Answering v2.0	"Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset.

265,016 images (COCO and abstract scenes)
At least 3 questions (5.4 questions on average) per image
10 ground truth answers per question
3 plausible (but likely incorrect) answers per question
Automatic evaluation metric

The first version of the dataset was released in October 2015."	https://paperswithcode.com/dataset/visual-question-answering-v2-0	02/12/2016	VQA v2.0					
3077	Biwi Kinect Head Pose	"Biwi Kinect Head Pose is a challenging dataset mainly inspired by the automotive setup.  It is acquired with the Microsoft Kinect sensor, a structured IR light device. It contains about 15k frame, with RGB. (640 × 480) and depth maps (640 × 480). Twenty subjects have been involved in the recordings: four of them were recorded twice, for a total of 24 sequences. The ground truth of yaw, pitch and roll angles is reported together with the head center and the calibration matrix. 
Source: POSEidon: Face-from-Depth for Driver Pose Estimation"	https://paperswithcode.com/dataset/biwi-kinect-head-pose							
3078	ELAS	"ELAS is a dataset for lane detection. It contains more than 20 different scenes (in more than 15,000 frames) and considers a variety of scenarios (urban road, highways, traffic, shadows, etc.). The dataset was manually annotated for several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes).
Source: Ego-Lane Analysis System (ELAS): Dataset and Algorithms"	https://paperswithcode.com/dataset/elas	15/06/2018						
3079	100DOH	"The 100 Days Of Hands Dataset (100DOH) is a large-scale video dataset containing hands and hand-object interactions. It consists of 27.3K Youtube videos from 11 categories with nearly 131 days of footage of everyday interaction. The focus of the dataset is hand contact, and it includes both first-person and third-person perspectives. The videos in 100DOH are unconstrained and content-rich, ranging from records of daily life to specific instructional videos. To enforce diversity, the dataset contains no more than 20 videos from each uploader.
Source:"	https://paperswithcode.com/dataset/100doh	11/06/2020	100 Days Of Hands Dataset					
3080	SVLD	"The social vision and language dataset is a large-scale multimodal dataset designed for research into social contextual learning.
Source: A Dataset and Benchmarks for Multimedia Social Analysis
Image Source: https://cannylab.github.io/svld/"	https://paperswithcode.com/dataset/svld	05/06/2020	Social Vision and Language Dataset					
3081	TextComplexityDE	"TextComplexityDE is a dataset consisting of 1000 sentences in German language taken from 23 Wikipedia articles in 3 different article-genres to be used for developing text-complexity predictor models and automatic text simplification in German language. The dataset includes subjective assessment of different text-complexity aspects provided by German learners in level A and B. In addition, it contains manual simplification of 250 of those sentences provided by native speakers and subjective assessment of the simplified sentences by participants from the target group. The subjective ratings were collected using both laboratory studies and crowdsourcing approach.
Source: Subjective Assessment of Text Complexity: A Dataset for German Language"	https://paperswithcode.com/dataset/textcomplexityde	16/04/2019						
3082	Image Paragraph Captioning	"The Image Paragraph Captioning dataset allows researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2,487/2,489 images.
Since all the images are also part of the Visual Genome dataset, each image also contains 50 region descriptions (short phrases describing parts of an image), 35 objects, 26 attributes and 21 relationships and 17 question-answer pairs.
Source: A Hierarchical Approach for Generating Descriptive Image Paragraphs
Image Source: https://cs.stanford.edu/people/ranjaykrishna/im2p/index.html"	https://paperswithcode.com/dataset/image-paragraph-captioning	20/11/2016						
3083	Famulus	"This is a dataset for segmentation and classification of epistemic activities in diagnostic reasoning texts.
Source: Analysis of Automatic Annotation Suggestions for Hard Discourse-Level Tasks in Expert Domains"	https://paperswithcode.com/dataset/famulus	06/06/2019						
3084	CMU Wilderness Multilingual Speech Dataset	"The CMU Wilderness Multilingual Speech Dataset is a dataset of over 700 different languages providing audio, aligned text and word pronunciations. On average each language provides around 20 hours of sentence-lengthed transcriptions. 
Source: Alan W Black ""CMU Wilderness Multilingual Speech Dataset"" ICASSP 2019, Brighton, UK."	https://paperswithcode.com/dataset/cmu-wilderness-multilingual-speech-dataset							
3085	Aesthetic Visual Analysis	"Aesthetic Visual Analysis is a dataset for aesthetic image assessment that contains over 250,000 images along with a rich variety of meta-data including a
large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style."	https://paperswithcode.com/dataset/aesthetic-visual-analysis	01/01/2021	Aesthetic Visual Analysis					
3086	BigBIRD	"BigBIRD is a 3D dataset of 125 objects, with the following data for each object:

600 12 megapixel images, sampling the viewing hemisphere
600 registered RGB-D point clouds from a Carmine 1.09 sensor
Pose information for each of the above images and point clouds
Segmentation masks for each of the above images (and segmented point clouds)
Merged point clouds consisting of data from all 600 viewpoints
Reconstructed meshes from the merged point clouds

Paper: ICRA 2014 ""A Large-Scale 3D Database of Object Instances."""	https://paperswithcode.com/dataset/bigbird		(Big) Berkeley Instance Recognition Dataset					
3087	WSJ0-2mix	"WSJ0-2mix is a speech recognition corpus of speech mixtures using utterances from the Wall Street Journal (WSJ0) corpus.
Source: Deep clustering: Discriminative embeddings for segmentation and separation"	https://paperswithcode.com/dataset/wsj0-2mix-1	18/08/2015						
3088	WHAM!	"The WSJ0 Hipster Ambient Mixtures (WHAM!) dataset pairs each two-speaker mixture in the wsj0-2mix dataset with a unique noise background scene. It has an extension called WHAMR! that adds artificial reverberation to the speech signals in addition to the background noise.
The noise audio was collected at various urban locations throughout the San Francisco Bay Area in late 2018. The environments primarily consist of restaurants, cafes, bars, and parks. Audio was recorded using an Apogee Sennheiser binaural microphone on a tripod between 1.0 and 1.5 meters off the ground."	https://paperswithcode.com/dataset/wham	02/07/2019	WSJ0 Hipster Ambient Mixtures					
3089	CUHK Face Alignment Database	"The CUHK Face Alignment Database is dataset with 13,466 face images, among which 5, 590 images are from LFW and the remaining 7, 876 images are downloaded from the web. Each face is labeled with the positions of five facial keypoints. 10,000 images are used for training and the remaining 3,466 images for validation.
<paper>
Image Source: Deep Convolutional Network Cascade for Facial Point Detection"	https://paperswithcode.com/dataset/cuhk-face-alignment-database	01/06/2013						
3090	CUHK Square Dataset	"CUHK Square data set is for transfer learning research on adapting generic pedestrian detectors. It includes a traffic video sequence of 60 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 576. 
In order to evaluate the performance of human detection on this data set, ground truth of pedestrians of some sampled frames are manually labeled. 
Paper Source: Transferring a generic pedestrian detector towards specific scenes
Source: CUHK Square Dataset
Image Source: CUHK Square Dataset"	https://paperswithcode.com/dataset/cuhk-square-dataset	01/06/2012						
3091	CUHK Occlusion Dataset	"CUHK occlusion dataset includes 1,063 images with occluded pedestrians. It is used for Human Detection with occlusion handling in crowded scenes.
Paper: A discriminative deep model for pedestrian detection with occlusion handling
Source: CUHK Occlusion Dataset
Image Source: CUHK Occlusion Dataset"	https://paperswithcode.com/dataset/cuhk-occlusion-dataset							
3092	Grand Central Station Dataset	"The Grand central station dataset includes a video with 50,010 frames which is used for Scene Understanding and Crowd Analysis.
Paper: Understanding collective crowd behaviors: Learning a Mixture model of Dynamic pedestrian-Agents
Source: Train Station Dataset
Image Source: Train Station Dataset"	https://paperswithcode.com/dataset/grand-central-station-dataset							
3093	CUHK02	"CUHK02 is a dataset for person re-identification. It contains 1,816 identities from two disjoint camera views. Each identity has two samples per camera view making a total of 7,264 images. It is used for Person Re-identification.
Image Source: Locally Aligned Feature Transforms across Views"	https://paperswithcode.com/dataset/cuhk02	01/06/2013	CUHK Person Re-identification Dataset					
3094	ArtEmis	"ArtEmis is a large-scale dataset aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, this dataset focuses on the affective experience triggered by visual artworks an the annotators were asked to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. This leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., “freedom” or “love”), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. 
This dataset focuses on visual art (e.g., paintings, artistic photographs) as it is a prime example of imagery created to elicit emotional responses from its viewers. ArtEmis contains 439K emotion attributions and explanations from humans, on 81K artworks from WikiArt.
Paper: ArtEmis: Affective Language for Visual Art
Source: ArtEmis Dataest
Image Source: ArtEmis Dataest"	https://paperswithcode.com/dataset/artemis	01/01/2021						
3095	BreakHis	"The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X).  It contains 2,480  benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory - Pathological Anatomy and Cytopathology, Parana, Brazil.
Paper: F. A. Spanhol, L. S. Oliveira, C. Petitjean and L. Heutte, ""A Dataset for Breast Cancer Histopathological Image Classification,"" in IEEE Transactions on Biomedical Engineering, vol. 63, no. 7, pp. 1455-1462, July 2016, doi: 10.1109/TBME.2015.2496264
Source: https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/
Image Source: https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/"	https://paperswithcode.com/dataset/breakhis		Breast Cancer Histopathological Database					
3096	2D Hela	"2D HeLa is a dataset of fluorescence microscopy images of HeLa cells stained with various organelle-specific fluorescent dyes. The images include 10 organelles, which are DNA (Nuclei), ER (Endoplasmic reticulum), Giantin, (cis/medial Golgi), GPP130 (cis Golgi), Lamp2 (Lysosomes), Mitochondria, Nucleolin (Nucleoli), Actin, TfR (Endosomes), Tubulin.
The purpose of the dataset is to train a computer program to automatically identify sub-cellular organelles.
Paper: M. V. Boland and R. F. Murphy (2001). A Neural Network Classifier Capable of Recognizing the Patterns of all Major Subcellular Structures in Fluorescence Microscope Images of HeLa Cells. Bioinformatics 17:1213-1223
Source: Identifying Sub-cellular Organelles
Image Source: Identifying Sub-cellular Organelles"	https://paperswithcode.com/dataset/2d-hela							
3097	PointPattern	"PointPattern is a graph classification dataset constructed by simple point patterns from statistical mechanics. The authors simulated three point patterns in 2D: hard disks in equilibrium (HD), Poisson point process, and random sequential adsorption (RSA) of disks. The HD and Poisson distributions can be seen as simple models that describe the microstructures of liquids and gases while the RSA is a nonequilibrium stochastic process that introduces new particles one by one subject to nonoverlapping conditions. 
These systems are well known to be structurally different, while being easy to simulate, thus provides a solid and controllable classification task. For each point pattern, the
particles are treated as nodes, and edges are subsequently drawn according to whether two particles are within a threshold distance.
Source: Path Integral Based Convolution and Pooling for Graph Neural Networks
Image Source: Path Integral Based Convolution and Pooling for Graph Neural Networks"	https://paperswithcode.com/dataset/pointpattern	29/06/2020						
3098	Humans in 3D	"H3D (Humans in 3D) is a dataset of annotated people. The annotations include:

The joints and other keypoints (eyes, ears, nose, shoulders, elbows, wrists, hips, knees and ankles)
The 3D pose inferred from the keypoints.
Visibility boolean for each keypoint
Region annotations (upper clothes, lower clothes, dress, socks, shoes, hands, gloves, neck, face, hair, hat, sunglasses, bag, occluder)
Body type (male, female or child)

Paper: Poselets: Body part detectors trained using 3D human pose annotations
Source: H3D
Image Source: H3D"	https://paperswithcode.com/dataset/humans-in-3d	01/09/2009						
3099	BelgaLogos	"BelgaLogos is a dataset for logo detection and recognition. The images of BelgaLogos dataset have been provided and are copyrighted by BELGA press agency. They are freely available for research purpose only. The dataset is composed of 10,000 images covering all aspects of life and current affairs: politics and economics, finance and social affairs, sports, culture and personalities. All images are in JPEG format and have been re-sized with a maximum value of height and width equal to 800 pixels, preserving aspect ratio. 
Paper: Alexis Joly and Olivier Buisson, Logo retrieval with a contrario visual query expansion, In Proceedings of the Seventeen ACM international Conference on Multimedia, 2009.
Source: BelgaLogos dataset
Image Source: BelgaLogos dataset"	https://paperswithcode.com/dataset/belgalogos							
3100	Aspects dataset	"This dataset contains video shots for two different classes: tigers and cars. The shots were collected from 188 car ads (~1–2 min each) and 14 nature documentaries about tigers (~40 min), amounting to roughly 14 h of video. The videos were partitioned into shorter shots, and only those showing at least one instance of the class were kept. This produced 806 shots for the car and 1880 for the tiger class, typically 1–100 sec in length.
Paper: Discovering object aspects from video
Source: Aspects dataset
Image Source: Aspects dataset"	https://paperswithcode.com/dataset/aspects-dataset	01/06/2016						
3101	POET	"The POET (Pascal Objects Eye Tracking) is a dataset that consists of eye tracking data for the complete trainval set of ten objects classes (cat, dog, bicycle, motorbike, boat, aeroplane, horse, cow, sofa, dining table) from Pascal VOC 2012 (6,270 images in total). Each image is annotated with the eye movement record of five participants, whose task was to identify which object class was present in the image.
Paper: Training object class detectors from eye tracking data
Source: Pascal Objects Eye Tracking (POET) v1.1
Image Source: Pascal Objects Eye Tracking (POET) v1.1"	https://paperswithcode.com/dataset/poet	01/01/2014	Pascal Objects Eye Tracking					
3102	AMUSE	"The automotive multi-sensor (AMUSE) dataset consists of inertial and other complementary sensor data combined with monocular, omnidirectional, high frame rate visual data taken in real traffic scenes during multiple test drives.
Paper: A Multi-sensor Traffic Scene Dataset with Omnidirectional Video
Source: AMUSE
Image Source: AMUSE"	https://paperswithcode.com/dataset/amuse	01/06/2013	Automotive Multi-Sensor Dataset					
3103	IMO	"Dataset of annotated independently moving objects (IMO). This dataset contains left and right images, stereo images, stereo disparity from SGM, and vehicle labels as well as a ground truth annotations. 
Paper: Independently Moving Object Trajectories from Sequential Hierarchical Ransac
Source: IMO Dataset
Image Source: IMO Dataset"	https://paperswithcode.com/dataset/imo	01/01/2021	Independently Moving Objects					
3104	LTIR	"The LTIR dataset is a thermal infrared dataset for evaluation of Short-Term Single-Object (STSO) tracking.
The dataset contains

20 thermal infrared sequences, one .png per frame. Some sequences are available in both 8- and 16-bits.
Bounding box annotations of one object per sequence.
Local per-frame annotations.

Paper: A thermal Object Tracking benchmark
Source: The LTIR dataset v1.0
Image Source: The LTIR dataset v1.0"	https://paperswithcode.com/dataset/ltir	01/01/2016	Linköping Thermal InfraRed					
3105	Family101	"The Family101 dataset is the a large-scale dataset of families across several generations. It contains 101 different families with distinct family names, including 206 nuclear families, 607 individuals, with 14,816 images. The dataset are composed of renowned public families.
Paper: Kinship Classification by Modeling Facial Feature Heredity People
Source: Kinship Classification by Modeling Facial Feature Heredity People
Image Source: Kinship Classification by Modeling Facial Feature Heredity People"	https://paperswithcode.com/dataset/family101	28/05/2013						
3106	FIW	FIW is a large and comprehensive database available for kinship recognition. FIW is made up of 11,932 natural family photos of 1,000 families-- nearly 10x more than the next-to-largest, Family-101 database. Also, it contains 656,954 image pairs split between the 11 relationships, which is much larger than the 2nd to largest KinFaceW-II with 2,000 pairs for only 4 kinship types.	https://paperswithcode.com/dataset/fiw	07/04/2016	Families In The Wild					
3107	KinFaceW	"KinFaceW consists of two kinship datasets: KinFaceW-I and KinFaceW-II. Face images were collected from the internet, including some public figure face images as well as their parents' or children's face images. Face images are captured under uncontrolled environments in two datasets with no restriction in terms of pose, lighting, background, expression, age, ethnicity, and partial occlusion. The difference of KinFaceW-I and KinFaceW-II is that face images with a kin relation were acquired from different photos in KinFaceW-I and the same photo in KinFaceW-II in most cases.
Paper: Neighborhood Repulsed Metric Learning for Kinship Verification
Source: KinFaceW Database
Image Source: KinFaceW Database"	https://paperswithcode.com/dataset/kinfacew	01/06/2012						
3108	Boxy	"A large vehicle detection dataset with almost two million annotated vehicles for training and evaluating object detection methods for self-driving cars on freeways.
The dataset consists of:

200,000 images
1,990,000 annotated vehicles
5 Megapixel resolution
Sunshine, rain, dusk, night
Clear freeways, heavy traffic, traffic jams

Paper: Boxy Vehicle Detection in Large Images
Source: Boxy
Image Source: Boxy"	https://paperswithcode.com/dataset/boxy	04/06/2019	Boxy Vehicles Dataset					
3109	CASR	"CASR is a dataset for cyclist arm signal recognition in videos. It contains 219 annotated arm signal actions on videos of approximately 10 seconds each, containing one or two actions per video.
Source: Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation
Image Source: Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation"	https://paperswithcode.com/dataset/casr	09/10/2019	Cyclist Arm Signal Recognition					
3110	Driving Event Camera Dataset	"This dataset consists of a number of sequences that were recorded with a VGA (640x480) event camera (Samsung DVS Gen3) and a conventional RGB camera (Huawei P20 Pro) placed on the windshield of a car driving through Zurich.
Source: Driving Event Camera Dataset (Samsung DVS Gen3)
Image Source: Driving Event Camera Dataset (Samsung DVS Gen3)"	https://paperswithcode.com/dataset/driving-event-camera-dataset	15/06/2019						
3111	FRIDA	"FRIDA and FRIDA2 are databases of numerical images easily usable to evaluate in a systematic way the performance of visibility and contrast restoration algorithms. FRIDA comprises 90 synthetic images of 18 urban road scenes. FRIDA2 comprises 330 synthetic images of 66 diverse road scenes. The view point is closed to the one of the vehicle's driver. To each image without fog is associated 4 foggy images and a depthmap. Different kind of fog are added on each of the 4 associated images: uniform fog, heterogeneous fog, cloudy fog, and cloudy heterogeneous fog. These scenes can be used to test visibility and contrast restoration algorithms intensively and in an objective way, as well as ""shape from fog"" algorithms. The calibration parameters of the camera are given. 
Paper: Improved Visibility of Road Scene Images under Heterogeneous Fog
Source: FRIDA (Foggy Road Image DAtabase) image database
Image Source: FRIDA (Foggy Road Image DAtabase) image database"	https://paperswithcode.com/dataset/frida		Foggy Road Image Database					
3112	Ford Campus Vision and Lidar Data Set	"Ford Campus Vision and Lidar Data Set is a dataset collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. 
This dataset consists of the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research campus and downtown Dearborn, Michigan during November-December 2009. The vehicle path trajectory in these datasets contain several large and small-scale loop closures, which should be useful for testing various state of the art computer vision and SLAM (Simultaneous Localization and Mapping) algorithms.
Paper: Ford Campus vision and lidar data set
Source: Ford Campus vision and lidar data set
Image Source: Ford Campus vision and lidar data set"	https://paperswithcode.com/dataset/ford-campus-vision-and-lidar-data-set	02/03/2011						
3113	JAAD	"JAAD is a dataset for studying joint attention in the context of autonomous driving. The focus is on pedestrian and driver behaviors at the point of crossing and factors that influence them. To this end, JAAD dataset provides a richly annotated collection of 346 short video clips (5-10 sec long) extracted from over 240 hours of driving footage. These videos filmed in several locations in North America and Eastern Europe represent scenes typical for everyday urban driving in various weather conditions.
Bounding boxes with occlusion tags are provided for all pedestrians making this dataset suitable for pedestrian detection.
Behavior annotations specify behaviors for pedestrians that interact with or require attention of the driver. For each video there are several tags (weather, locations, etc.) and timestamped behavior labels from a fixed list (e.g. stopped, walking, looking, etc.). In addition, a list of demographic attributes is provided for each pedestrian (e.g. age, gender, direction of motion, etc.) as well as a list of visible traffic scene elements (e.g. stop sign, traffic signal, etc.) for each frame.
Paper: Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior
Source: JAAD
Image Source: Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior"	https://paperswithcode.com/dataset/jaad	12/02/2017	Joint Attention in Autonomous Driving					
3114	LISA Vehicle Detection	"This is a dataset for vehicle detection. It consists of:

Three color video sequences captured at different times of the day and illumination settings: morning, evening, sunny, cloudy, etc.
Different driving environments: highway and urban.
Varying traffic conditions: light to dense traffic

Paper: A General Active-Learning Framework for On-Road Vehicle Recognition and Tracking
Source: Vehicle Detection Dataset
Image Source: Vehicle Detection Dataset"	https://paperswithcode.com/dataset/lisa-vehicle-detection							
3115	LLAMAS	"The unsupervised Labeled Lane MArkerS dataset (LLAMAS) is a dataset for lane detection and segmentation. It contains over 100,000 annotated images, with annotations of over 100 meters at a resolution of 1276 x 717 pixels. The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. 
Paper: Unsupervised Labeled Lane Markers Using Maps
Source: Unsupervised Llamas Lane Marker Dataset
Image Source: Unsupervised Llamas Lane Marker Dataset"	https://paperswithcode.com/dataset/llamas		Labeled Lane Markers					
3116	VIsual PERception (VIPER)	"VIPER is a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. 
Source: Playing for Benchmarks
Image Source: Playing for Benchmarks"	https://paperswithcode.com/dataset/visual-perception-viper	21/09/2017						
3117	REC-COCO	"Relations in Captions (REC-COCO) is a new dataset that contains associations between caption tokens and bounding boxes in images. REC-COCO is based on the MS-COCO and V-COCO datasets. For each image in V-COCO, we collect their corresponding captions from MS-COCO and automatically align the concept triplet in V-COCO to the tokens in the caption. This requires finding the token for concepts such as PERSON. As a result, REC-COCO contains the captions and the tokens which correspond to each subject and object, as well as the bounding boxes for the subject and object.
Source: Inferring spatial relations from textual descriptions of images
Image Source: https://arxiv.org/pdf/2102.00997v1.pdf"	https://paperswithcode.com/dataset/rec-coco	01/02/2021	Relations in Captions					
3118	TRIPOD	"TRIPOD contains screenplays and plot synopses with turning point (TP) annotations for 99 movies. Each movie contains:

The Wikipedia plot synopsis (extended summary of 35 sentences on average) with sentence-level TP annotations.
The screenplay (all dialogue and description parts of the movie) segmented into scenes (selected from the Scriptbase dataset).
Gold scene-level TP labels for the screenplays of the test set.
The cast information (according to IMDb).

TRIPOD is extended in Movie Summarization via Sparse Graph Construction with more movies in the test set (122 now in total) and multimodal features extracted from the full-length movie videos. The multimodal version can be found here: https://datashare.ed.ac.uk/handle/10283/3819"	https://paperswithcode.com/dataset/tripod	27/08/2019	TuRnIng POint Dataset					
3119	CSI Screenplay Summarization Corpus	"The dataset contains gold-standard summary labels for 39 ""CSI: Crime Scene Investigation"" episodes from seasons 1-5. Each episode contains the full-length screenplay and human annotations for its summary. The annotations include:

scene-level binary labels denoting whether the scene belongs to the summary of the episode
aspect-based labels for the scenes that belong to the summary, i.e., which aspect of the summary the scene addresses (e.g., information about the victim, the crime scene, the perpetrator etc.)
sentence-level binary labels denoting the sentences of the screenplay that belong to the summary for 10 episodes of the dataset"	https://paperswithcode.com/dataset/csi-screenplay-summarization-corpus	27/04/2020						
3120	FPV-O	"FPV-O is a multi-subject first-person vision dataset of office activities. Office activities include person-to-person interactions, such as chatting and handshaking, person-to-object interactions, such as using a computer or a whiteboard, as well as generic activities such as walking. The videos in the dataset present a number of challenges that, in addition to intra-class differences and inter-class similarities, include frames with illumination changes, motion blur, and lack of texture. 
Paper: A First-Person Vision Dataset of Office Activities
Source: A first-person vision dataset of office activities
Image Source: A First-Person Vision Dataset of Office Activities"	https://paperswithcode.com/dataset/fpv-o	01/06/2018						
3121	MERL Shopping	"MERL Shopping is a dataset for training and testing action detection algorithms. The MERL Shopping Dataset consists of 106 videos, each of which is a sequence about 2 minutes long. The videos are from a fixed overhead camera looking down at people shopping in a grocery store setting. Each video contains several instances of the following 5 actions: ""Reach To Shelf"" (reach hand into shelf), ""Retract From Shelf "" (retract hand from shelf), ""Hand In Shelf"" (extended period with hand in the shelf), ""Inspect Product"" (inspect product while holding it in hand), and ""Inspect Shelf"" (look at shelf while not touching or reaching for the shelf).
Source: Action Detection in Videos
Image Source: Action Detection in Videos"	https://paperswithcode.com/dataset/merl-shopping	01/06/2016						
3122	A2D	A2D (Actor-Action Dataset) is a dataset for simultaneously inferring actors and actions in videos. A2D has seven actor classes (adult, baby, ball, bird, car, cat, and dog) and eight action classes (climb, crawl, eat, fly, jump, roll, run, and walk) not including the no-action class, which we also consider. The A2D has 3,782 videos with at least 99 instances per valid actor-action tuple and videos are labeled with both pixel-level actors and actions for sampled frames. The A2D dataset serves as a large-scale testbed for various vision problems: video-level single- and multiple-label actor-action recognition, instance-level object segmentation/co-segmentation, as well as pixel-level actor-action semantic segmentation to name a few.	https://paperswithcode.com/dataset/a2d	01/06/2015	Actor-Action Dataset					
3123	ASD	"The Annotated Semantic Dataset is composed of $11$ videos, divided in $3$ activity categories: Biking; Driving and Walking, according to their amount of semantic information. The classes are: $0p$, which represents the videos with approximately no semantic information; $25p$, for the videos containing relevant semantic information in ∼$25%$ of its frames ; the same ideia for the classes $50p$ and $75p$,
The videos were record using a GoPro Hero 3 camera mounted in a helmet for the Biking and Walking videos and attached to a head strap for the Driving videos."	https://paperswithcode.com/dataset/asd	01/10/2016	Annotated Semantic Dataset					
3124	l2d	"This dataset is composed of paired videos of people dancing 3 different music styles: Ballet, Michael Jackson and Salsa.
It contains multimodal data (visual data, temporal-graphs and audio) careful-selected from publicly available videos of dancers performing representative movements of the music style and audio data from the respective styles.
This dataset was used to train and evaluate methodologies for motion generation from audio. 
We split the samples into training and evaluation sets. 
The training set has 2352 samples of movements sequences with length 64. Which 525 are from Ballet style, 966 from Michael Jackson (MJ) and 861 from Salsa.
The evaluation set has 471 samples. 134 from Ballet, 102 from MJ and 235 from Salsa."	https://paperswithcode.com/dataset/l2d	25/11/2020	Learning to Dance					
3125	OccludedPASCAL3D+	"The OccludedPASCAL3D+ is a dataset is designed to evaluate the robustness to occlusion for a number of computer vision tasks, such as object detection, keypoint detection and pose estimation. In the OccludedPASCAL3D+ dataset, we simulate partial occlusion by superimposing objects cropped from the MS-COCO dataset on top of objects from the PASCAL3D+ dataset. We only use ImageNet subset in PASCAL3D+, which has 10812 testing images.
Source: OccludedPASCAL3D+
Image source: https://github.com/Angtian/OccludedPASCAL3D"	https://paperswithcode.com/dataset/occludedpascal3d	24/05/2020						
3126	THEODORE	Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high- resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization we reach an AP up to 0.84 for class person on High-Definition Analytics dataset.	https://paperswithcode.com/dataset/theodore	11/11/2020	Learning from THEODORE					
3127	MHRI dataset	"The dataset includes recordings from 10 different users teaching the robot different common kitchen objects, that consists of synchronized recordings from three cameras and a microphone mounted on the robot:
An RGB-d camera covers the user manipulation and interaction with the robot
An RGB-d camera mounted on the top of the robot provides a top view of the whole scenario
A HD-RGB camera points to the user head to capture face and expressions"	https://paperswithcode.com/dataset/mhri-dataset		Multimodal Human-Robot Interaction dataset					
3128	highD Dataseth	The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. Each vehicle's trajectory, including vehicle type, size and manoeuvres, is automatically extracted. Using state-of-the-art computer vision algorithms, the positioning error is typically less than ten centimeters. Although the dataset was created for the safety validation of highly automated vehicles, it is also suitable for many other tasks such as the analysis of traffic patterns or the parameterization of driver models.	https://paperswithcode.com/dataset/highd-dataseth		The Highway Drone Dataset Naturalistic Trajectories of 110 500 Vehicles Recorded at German Highways					
3129	inD Dataset	The inD dataset is a new dataset of naturalistic vehicle trajectories recorded at German intersections. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at four different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components.	https://paperswithcode.com/dataset/ind-dataset	18/11/2019	Intersection Drone Dataset					
3130	rounD Dataset	The rounD dataset is a new dataset of naturalistic road user trajectories recorded at German roundabouts. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at three different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components.	https://paperswithcode.com/dataset/round-dataset		The Roundabout Drone Dataset - Naturalistic Trajectories of Vehicles and Vulnerable Road Users Recorded at German Roundabouts					
3131	Localized Narratives	We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.	https://paperswithcode.com/dataset/localized-narratives	06/12/2019						
3132	CE4	"Given the difficulty to handle planetary data we provide downloadable files in PNG format from the missions Chang'E-3 and Chang'E-4. In addition to a set of scripts to do the conversion given a different PDS4 Dataset. 
This set of images constitute one of the first available datasets to tackle problems of Computer Vision and Learning in the context of space exploration."	https://paperswithcode.com/dataset/ce4	23/11/2020						
3133	MICC-SRI	The dataset contains 11,913 frame pairs of urban driving footage with and without moving objects, synthetically generated with the CARLA simulator. All frames are available both as RGB images and semantic segmentations. RGB images are non-photorealistic being rendered by a game engine, while semantic segmentations are similar to a real-world segmentations. The dataset is designed to provide a supervision for semantic road inpainting tasks.	https://paperswithcode.com/dataset/micc-sri	29/05/2018						
3134	KITTI-trajectory-prediction	"KITTI is a well established dataset in the computer vision community. It has often been used for trajectory prediction despite not having a well defined split, generating non comparable baselines in different works. This dataset aims at bridging this gap and proposes a well defined split of the KITTI data.
Samples are collected as 6 seconds chunks (2seconds for past and 4 for future) in a sliding window fashion from all trajectories in the dataset, including the egovehicle. There are a total of 8613 top-view trajectories for training and 2907 for testing.
Since top-view maps are not provided by KITTI, semantic labels of static categories obtained with DeepLab-v3+ from all frames are projected in a common top-view map using the Velodyne 3D point cloud and IMU. The resulting maps have a spatial resolution of 0.5 meters and are provided along with the trajectories."	https://paperswithcode.com/dataset/kitti-trajectory-prediction	05/06/2020						
3135	EmoContext	EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other.	https://paperswithcode.com/dataset/emocontext	01/06/2019						
3136	Glint360K	"The largest and cleanest face recognition dataset Glint360K, 
which contains 17,091,657 images of 360,232 individuals, baseline models trained on Glint360K can easily achieve state-of-the-art performance."	https://paperswithcode.com/dataset/glint360k	11/10/2020	Glint360K					
3137	IndicCorp	"IndicCorp is a large monolingual corpora with around 9 billion tokens covering 12 of the major Indian languages. It has been developed by discovering and scraping thousands of web sources - primarily news, magazines and books, over a duration of several months.
Languages covered: Assamese, Bengali, English, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu
Corpus Format: The corpus is a single large text file containing one sentence per line. The publicly released version is randomly shuffled, untokenized and deduplicated. 
Downloads
| Language | # News Articles* | Sentences     | Tokens        | Link     |
| -------- | ----------------- | ------------- | ------------- | -------- |
| as       | 0.60M             | 1.39M   |  32.6M  | link |
| bn       | 3.83M             | 39.9M | 836M  | link |
| en       | 3.49M             | 54.3M | 1.22B | link |
| gu       | 2.63M             | 41.1M | 719M  | link |
| hi       | 4.95M             | 63.1M |  1.86B | link |
| kn       | 3.76M             | 53.3M | 713M  | link |
| ml       | 4.75M             | 50.2M |  721M  | link |
| mr       | 2.31M             | 34.0M | 551M  | link |
| or       | 0.69M             | 6.94M   | 107M   | link |
| pa       | 2.64M             | 29.2M |  773M  | link |
| ta       | 4.41M             |  31.5M   |  582M  | link |
| te       | 3.98M             | 47.9M   |  674M  | link |
* Excluding articles obtained from the OSCAR corpus"	https://paperswithcode.com/dataset/indiccorp	08/11/2020						
3138	RuFa	"RuFa (Ruqaa-Farsi) dataset contains images of text written in one of two Arabic fonts: Ruqaa and Nastaliq (Farsi). The dataset contains 40,000 synthesized image and 516 real one, 40,516 in total. Images are in RGB JPG format at 100×100px. Text in the images has varying number of words, position, size, and opacity.
Real images were extracted from:


“The Rules of Arabic Calligraphy” by Hashem Al-Khatat - 1986.


“Ottman Fonts” by Muhammad Amin Osmanli Ketbkhana.


The synthetization process is described in detail in this post.
Dataset folder structure:
/rufa (40,516 images)


/real (516 images)
* /ruqaa (260 images)

* /farsi   (256 images)



/synth (40,000 images)
* /ruqaa (20,000 images)

* /farsi   (20,000 images)"	https://paperswithcode.com/dataset/rufa	17/07/2020						
3139	MERL-RAV	The MERL-RAV (MERL Reannotation of AFLW with Visibility) Dataset contains over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. The images were annotated by professional labelers, supervised by researchers at Mitsubishi Electric Research Laboratories (MERL).	https://paperswithcode.com/dataset/merl-rav-dataset	21/07/2020	MERL Reannotation of AFLW with Visibility					
3140	News Interactions on Globo.com	"Context
This large dataset with users interactions logs (page views) from a news portal was kindly provided by Globo.com, the most popular news portal in Brazil, for reproducibility of the experiments with CHAMELEON - a meta-architecture for contextual hybrid session-based news recommender systems. The source code was made available at GitHub.
The first version (v1) (download) of this dataset was released for reproducibility of the experiments presented in the following paper:
&gt; Gabriel de Souza Pereira Moreira, Felipe Ferreira, and Adilson Marques da Cunha. 2018.  News Session-Based Recommendations using Deep Neural Networks. In 3rd Workshop on Deep Learning for Recommender Systems (DLRS 2018), October 6, 2018, Vancouver, BC, Canada. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3270323.3270328
A second version (v2) (download) of this dataset was made available for reproducibility of the experiments presented in the following paper. Compared to the v1, the only differences are:

Included four additional user contextual attributes (click_os, click_country, click_region, click_referrer_type)
Removed repeated clicks (clicks in the same articles) within sessions. Those sessions with less than two clicks (minimum for the next-click prediction task) were removed

&gt; Gabriel de Souza Pereira Moreira, Dietmar Jannach, and Adilson Marques da Cunha. 2019.  Contextual Hybrid Session-based News Recommendation with Recurrent Neural Networks. arXiv preprint arXiv:1904.10367, 49 pages
You are not allowed to use this dataset for commercial purposes, only with academic objectives (like education or research). 
If used for research, please cite the above papers.
Content
The dataset contains a sample of user interactions (page views) in G1 news portal from Oct. 1 to 16, 2017, including about 3 million clicks, distributed in more than 1 million sessions from 314,000 users who read more than 46,000 different news articles during that period.
It is composed by three files/folders:

clicks.zip - Folder with CSV files (one per hour), containing user sessions interactions in the news portal.
articles_metadata.csv - CSV file with metadata information about all (364047) published articles 
articles_embeddings.pickle Pickle (Python 3) of a NumPy matrix containing the Article Content Embeddings (250-dimensional vectors), trained upon articles' text and metadata by the CHAMELEON's ACR module (see paper for details) for 364047 published articles.
 P.s. The full text of news articles could not be provided due to license restrictions, but those embeddings can be used by Neural Networks to represent their content. See this paper for a t-SNE visualization of these embeddings, colored by category.

Acknowledgements
I would like to acknowledge Globo.com for providing this dataset for this research and for the academic community, in special to Felipe Ferreira for preparing the original dataset by Globo.com.
Dataset banner photo by rawpixel on Unsplash
Inspiration
This dataset might be very useful if you want to implement and evaluate hybrid and contextual news recommender systems, using both user interactions and articles content and metadata to provide recommendations. You might also use it for analytics, trying to understand how users interactions in a news portal are distributed by user, by article, or by category, for example.
If you are interested in a dataset of user interactions on articles with the full text provided, to experiment with some different text representations using NLP, you might want to take a look in this smaller dataset."	https://paperswithcode.com/dataset/news-interactions-on-globo-com	31/07/2018	News Portal User Interactions by Globo.com - A large dataset for news recommendations offline evaluation and analytics					
3141	Synbols	"Synbols is a dataset generator designed for probing the behavior of learning algorithms. By defining the distribution over latent factors one can craft a dataset specifically tailored to answer specific questions about a given algorithm.
Default versions of these datasets are also materialized and can serve as benchmarks."	https://paperswithcode.com/dataset/synbols	14/09/2020						
3142	C&Z	"One of the first datasets (if not the first) to highlight the importance of bias and diversity in the community, which started a revolution afterwards. Introduced in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. It was later expanded by adding synthetic images generated by a GAN architecture at ETH Zürich (in HDCGAN by Curtó et al. 2017). Being then not only the pioneer of talking about the importance of balanced datasets for learning and vision but also for being the first GAN augmented dataset of faces. 
The original description goes as follows:
A bias-free dataset, containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions. C&Z is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces.
Dataset: https://github.com/curto2/c
Supplement (with scripts to handle the labels): https://github.com/curto2/graphics
1 https://www.curto.hk/c/decurto.pdf
2 https://www.zarza.hk/z/dezarza.pdf"	https://paperswithcode.com/dataset/c-and-z	17/11/2017						
3143	GEM	"Generation, Evaluation, and Metrics (GEM) is a benchmark environment for Natural Language Generation with a focus on its Evaluation, both through human annotations and automated Metrics.
GEM aims to:

measure NLG progress across 13 datasets spanning many NLG tasks and languages.
provide an in-depth analysis of data and models presented via data statements and challenge sets.
develop standards for evaluation of generated text using both automated and human metrics.

It is our goal to regularly update GEM and to encourage toward more inclusive practices in dataset development by extending existing data or developing datasets for additional languages.
Source: https://gem-benchmark.com/
Image Source: Gehrmann et al"	https://paperswithcode.com/dataset/gem	02/02/2021	Generation, Evaluation, and Metrics					
3144	ALFWorld	"ALFWorld contains interactive TextWorld environments (Côté et. al) that parallel embodied worlds in the ALFRED dataset (Shridhar et. al). The aligned environments allow agents to reason and learn high-level policies in an abstract space before solving embodied tasks through low-level actuation.   
Source: ALFWorld"	https://paperswithcode.com/dataset/alfworld	08/10/2020						
3145	HQ-WMCA	The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database consists of 2904 short multi-modal video recordings of both bona-fide and presentation attacks. There are 555 bonafide presentations from 51 participants and the remaining 2349 are presentation attacks. The data is recorded from several channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra).	https://paperswithcode.com/dataset/hq-wmca	22/07/2020	High-Quality Wide Multi-Channel Attack database					
3146	The Best Sarcasm Annotated Dataset in Spanish	"Content
This dataset contains all utterances of two episodes of South Park (Latin American voices) and two episodes of Archer (Spanish voices). The order of the utterances is shuffled. Each utterance has been annotated based on whether it is sarcastic or not. Sarcastic expressions also contain further annotation based on different theories on sarcasm.
This corpus is unique because it is annotated from primarily audiovisual media. It also contains a lot of negative examples of sentences that are meant to be humorous or outrageous, but not sarcastic. This data provides thus a closer to real life benchmark for any sarcasm detection system.
Cite
I annotated this data for my MA thesis, so please cite it if you use this data.
Hämäläinen, Mika (2016). Reconocimiento automático del sarcasmo: ¡Esto va a funcionar bien!. Helsinki: University of Helsinki, Department of Modern Languages.
Inspiration

Sarcasm detection
Prediction of the theoretical categories of sarcasm"	https://paperswithcode.com/dataset/the-best-sarcasm-annotated-dataset-in-spanish	01/06/2016						
3147	MIRACL-VC1	MIRACL-VC1 is a lip-reading dataset including both depth and color images. It can be used for diverse research fields like visual speech recognition, face detection, and biometrics. Fifteen speakers (five men and ten women) positioned in the frustum of an MS Kinect sensor and utter ten times a set of ten words and ten phrases (see the table below). Each instance of the dataset consists of a synchronized sequence of color and depth images (both of 640x480 pixels).  The MIRACL-VC1 dataset contains a total number of 3000 instances.	https://paperswithcode.com/dataset/miracl-vc1		MIRACL-VC1					
3148	XD-Violence	XD-Violence is a large-scale audio-visual dataset for violence detection in videos.	https://paperswithcode.com/dataset/xd-violence	09/07/2020						
3149	PatentMatch	We address the computer-assisted search for prior art by creating a training dataset for supervised machine learning called PatentMatch. It contains pairs of claims from patent applications and semantically corresponding text passages of different degrees from cited patent documents. Each pair has been labeled by technically-skilled patent examiners from the European Patent Office. Accordingly, the label indicates the degree of semantic correspondence (matching), i.e., whether the text passage is prejudicial to the novelty of the claimed invention or not.	https://paperswithcode.com/dataset/patentmatch	27/12/2020	PatentMatch					
3150	A Dataset of Journalists' Interactions with Their Readership	We present a dataset of dialogs in which journalists of The Guardian replied to reader comments and identify the reasons why. Based on this data, we formulate the novel task of recommending reader comments to journalists that are worth reading or replying to, i.e., ranking comments in such a way that the top comments are most likely to require the journalists' reaction.	https://paperswithcode.com/dataset/a-dataset-of-journalists-interactions-with	19/10/2020						
3151	Top Comment or Flop Comment?	"This dataset comprises four files of IDs of either strongly or weakly engaging online news comments (please see the paper for details):
""Top comments"" are 1) the top 10% comments in the politics section of The Guardian with the largest relative number of replies received (3111 samples) and 2) the top 10% comments in the politics section with the largest relative number of upvotes received (11081 samples)
""Flop comments"" are 1) the flop 10% comments in the politics section of The Guardian with the smallest relative number of replies received (3111 samples) and 2) the flop 10% comments in the politics section with the smallest relative number of upvotes received (11081 samples)"	https://paperswithcode.com/dataset/top-comment-or-flop-comment	26/03/2020	Top Comment or Flop Comment? User Engagement in Online News Discussions					
3152	HeartSeg	"The medaka (Oryzias latipes) and the zebrafish (Danio rerio) are used as a model organism for a variety of subjects in biomedical research. The presented work aims to study the potential of automated ventricular dimension estimation through heart segmentation in medaka. For more on this, it's time for a closer look on our paper and the supplementary materials.
See our paper here: https://www.liebertpub.com/doi/10.1089/zeb.2019.1754
See demonstration of our algorithm and framework on the test set data:
https://youtu.be/i5bX_XbwXq0
The raw data was provided by:
Dr. Jakob Gierten
Affiliated with:
Department of Pediatric Cardiology, University Hospital Heidelberg, Im Neuenheimer Feld 430, 69120 Heidelberg, Germany
Centre for Organismal Studies, Heidelberg University, Im Neuenheimer Feld 230, 69120 Heidelberg, Germany
Contributing
We hope this work sparks additional research in this direction. Either by contributing to this framework, deploying the framework, or reusing the annotated ground truth data.
In any case feel free to reach out and make sure to reference this work.
Schutera, M., Just, S., Gierten, J., Mikut, R., Reischl, M., & Pylatiuk, C. (2019). Machine learning methods for automated quantification of ventricular dimensions. Zebrafish, 16(6), 542-545.
Contact: mark.schutera@kit.edu and pylatiuk@kit.edu"	https://paperswithcode.com/dataset/heartseg	19/09/2019	HeartSeg					
3153	DNS Challenge	"The DNS Challenge at INTERSPEECH 2020 intended to promote collaborative research in single-channel Speech Enhancement aimed to maximize the perceptual quality and intelligibility of the enhanced speech. The challenge evaluated the speech quality using the online subjective evaluation framework ITU-T P.808. The challenge provides large datasets for training noise suppressors.
Source: Deep Noise Suppression Challenge – INTERSPEECH 2020"	https://paperswithcode.com/dataset/deep-noise-suppression-2020	23/01/2020	Deep Noise Suppression Challenge					
3154	Interspeech 2021 Deep Noise Suppression Challenge	"The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality.
This challenge has two two tracks:
Track 1: Real-Time Denoising track for wide band scenario
The noise suppressor must take less than the stride time Ts (in ms) to process a frame of size T (in ms) on an Intel Core i5 quad-core machine clocked at 2.4 GHz or equivalent processor. For example, Ts = T/2 for 50% overlap between frames. The total algorithmic latency allowed including the frame size T, stride time Ts, and any look ahead must be less than or equal to 40ms. For example, for a real-time system that receives 20ms audio chunks, if you use a frame length of 20ms with a stride of 10ms resulting in an algorithmic latency of 30ms, then you satisfy the latency requirements. If you use a frame of size 32ms with a stride of 16ms resulting in an algorithmic latency of 48ms, then your method does not satisfy the latency requirements as the total algorithmic latency exceeds 40ms. If your frame size plus stride T1=T+Ts is less than 40ms, then you can use up to (40-T1) ms future information.
Track 2: Real-Time Denoising track for full band scenario
Satisfy Track 1 requirements but at 48 kHz."	https://paperswithcode.com/dataset/interspeech-2021-deep-noise-suppression							
3155	TRN	"The Toulouse Road Network dataset describes patches of road maps from the city of Toulouse, represented both as spatial graphs G = (A, X) and as grayscale segmentation images. 
The TRN dataset contains 111,034 data points (map tiles), of which: 80,357 are in the training set (around 72.4%), 11,679 are in the validation set (around 10.5%), 18,998 are in the test set (around 17.1%). 
Each tile represents a squared region of side 0.001 degrees of latitude and longitude on the map, which corresponds to a square of around 110 meters. The semantic segmentation of each patch is represented as a 64 × 64 grayscale image. 
The dataset is generated starting from publicly available data from OpenStreetMap. More details on the dataset characteristic and generation methods are available in our blogpost.
Source: https://github.com/davide-belli/toulouse-road-network-dataset
Image Source: https://arxiv.org/pdf/1910.14388.pdf"	https://paperswithcode.com/dataset/trn	31/10/2019	Toulouse Road Network					
3156	WEB-FORUM-52	The WEB-FORUM-52 gold standard comprises (i) 13 web forums from the health domain, (ii) 15 forums obtained from a Wikipedia list of popular forums (https://en.wikipedia.org/wiki/List_of_Internet_forums), (iii) 13 forums mentioned on a list of popular German Web forums (https://www.beliebte-foren.de), (iv) nine forums obtained from WPressBlog (https://www.wpressblog.com/free-forum-posting-sites-list/) and (v) two additional forums. For most forums two web pages (from different threads) were used and stored together with gold standard annotations that have been manually created by domain experts and describe the post text, post date, post user and direct URL to the post.	https://paperswithcode.com/dataset/web-forum-52	27/10/2020	WEB-FORUM-52 gold standard					
3157	KorQuAD	KorQuAD is a large-scale question-and-answer dataset constructed for Korean machine reading comprehension, and investigate the dataset to understand the distribution of answers and the types of reasoning required to answer the question. This dataset benchmarks the data generating process of SQuAD to meet the standard.	https://paperswithcode.com/dataset/korquad	16/09/2019	The Korean Question Answering Dataset					
3158	MOBIO	"The MOBIO database consists of bi-modal (audio and video) data taken from 152 people. The database has a female-male ratio or nearly 1:2 (100 males and 52 females) and was collected from August 2008 until July 2010 in six different sites from five different countries. This led to a diverse bi-modal database with both native and non-native English speakers.
In total 12 sessions were captured for each client: 6 sessions for Phase I and 6 sessions for Phase II. The Phase I data consists of 21 questions with the question types ranging from: Short Response Questions, Short Response Free Speech, Set Speech, and Free Speech. The Phase II data consists of 11 questions with the question types ranging from:  Short Response Questions, Set Speech, and Free Speech. A more detailed description of the questions asked of the clients is provided below.
The database was recorded using two mobile devices: a mobile phone and a laptop computer. The mobile phone used to capture the database was a NOKIA N93i mobile while the laptop computer was a standard 2008 MacBook. The laptop was only used to capture part of the first session, this first session consists of data captured on both the laptop and the mobile phone."	https://paperswithcode.com/dataset/mobio							
3159	FRLL-Morphs	"FRLL-Morphs is a dataset of morphed faces based on images selected from the publicly available Face Research London Lab dataset 1.
We created the database by selecting similar looking pairs of people, and made 4 types of morphs for each pair using the following morphing tools: OpenCV 2, FaceMorpher 3, StyleGAN 2 3, WebMorpher 4.

1 https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666
2 https://www.learnopencv.com/face-morph-using-opencv-cpp-python
3 https://github.com/yaopang/FaceMorpher/tree/master/facemorpher
4 https://github.com/NVlabs/stylegan2"	https://paperswithcode.com/dataset/frll-morphs	21/10/2020	Face Research London Lab Morphs					
3160	VisualMRC	"VisualMRC is a visual machine reading comprehension dataset that proposes a task: given a question and a document image, a model produces an abstractive answer.
You can find more details, analyses, and baseline results in the paper, 
VisualMRC: Machine Reading Comprehension on Document Images, AAAI 2021.
Statistics:
10,197 images
30,562 QA pairs
10.53 average question tokens (tokenizing with NLTK tokenizer)
9.53 average answer tokens (tokenizing wit NLTK tokenizer)
151.46 average OCR tokens (tokenizing with NLTK tokenizer)"	https://paperswithcode.com/dataset/visualmrc	27/01/2021	VisualMRC: Machine Reading Comprehension on Document Images					
3161	FERET-Morphs	"FERET-Morphs is a dataset of morphed faces selected from the publicly available FERET dataset 1.
We created the database by selecting similar looking pairs of people, and made 3 types of morphs for each pair using the following morphing tools: OpenCV 2, FaceMorpher 3, StyleGAN 2 3.

1 https://www.nist.gov/itl/products-and-services/color-feret-database
2 https://www.learnopencv.com/face-morph-using-opencv-cpp-python
3 https://github.com/yaopang/FaceMorpher/tree/master/facemorpher
4 https://github.com/NVlabs/stylegan2"	https://paperswithcode.com/dataset/feret-morphs	21/10/2020						
3162	FRGC-Morphs	"FRGC-Morphs is a dataset of morphed faces selected from the publicly available FRGC dataset 1.
We created the database by selecting similar looking pairs of people, and made 3 types of morphs for each pair using the following morphing tools: OpenCV 2, FaceMorpher 3, StyleGAN 2 3.

1 https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc
2 https://www.learnopencv.com/face-morph-using-opencv-cpp-python
3 https://github.com/yaopang/FaceMorpher/tree/master/facemorpher
4 https://github.com/NVlabs/stylegan2"	https://paperswithcode.com/dataset/frgc-morphs	21/10/2020						
3163	NISP- A Multi-lingual Multi-accent Dataset for Speaker Profiling	We announce the release of a new multilingual speaker dataset called NITK-IISc Multilingual Multi-accent Speaker Profiling(NISP) dataset. The dataset contains speech in six different languages -- five Indian languages along with Indian English. The dataset contains speech data from 345 bilingual speakers in India. Each speaker has contributed about 4-5 minutes of data that includes recordings in both English and their mother tongue. The transcript for the text is provided in UTF-8 format. For every speaker, the dataset contains speaker meta-data such as L1, native place, medium of instruction, current residing place etc. In addition the dataset also contains physical parameter information of the speakers such as age, height, shoulder size and weight. We hope that the dataset is useful for a diverse set of research activities including multilingual speaker recognition, language and accent recognition, automatic speech recognition etc.	https://paperswithcode.com/dataset/nisp-a-multi-lingual-multi-accent-dataset-for							
3164	NinaPro DB2	"The second Ninapro database includes 40 intact subjects and it is thoroughly described in the paper: ""Manfredo Atzori, Arjan Gijsberts, Claudio Castellini, Barbara Caputo, Anne-Gabrielle Mittaz Hager, Simone Elsig, Giorgio Giatsidis, Franco Bassetto & Henning Müller. Electromyography data for non-invasive naturally-controlled robotic hand prostheses. Scientific Data, 2014"" (http://www.nature.com/articles/sdata201453).
Please, cite this paper for any work related to the Ninapro database.
Please, use also the paper by Gijsberts et al., 2014 (http://publications.hevs.ch/index.php/publications/show/1629) for more information about the database."	https://paperswithcode.com/dataset/ninapro-db2		DB2 - 40 Intact Subjects - Delsys Trigno electrodes					
3165	BuzzFeed-Webis Fake News Corpus 2016	The BuzzFeed-Webis Fake News Corpus 16 comprises the output of 9 publishers in a week close to the US elections. Among the selected publishers are 6 prolific hyperpartisan ones (three left-wing and three right-wing), and three mainstream publishers (see Table 1). All publishers earned Facebook’s blue checkmark, indicating authenticity and an elevated status within the network. For seven weekdays (September 19 to 23 and September 26 and 27), every post and linked news article of the 9 publishers was fact-checked by professional journalists at BuzzFeed. In total, 1,627 articles were checked, 826 mainstream, 256 left-wing and 545 right-wing. The imbalance between categories results from differing publication frequencies.	https://paperswithcode.com/dataset/buzzfeed-webis-fake-news-corpus-2016	01/07/2018						
3166	POLIT-FALSE-n-LEGIT NEWS DB 2016-2017	The LiT.RL POLIT-FALSE-n-LEGIT NEWS DB 2016-2017 contains a total of 274 news articles about U.S. Politics, content-matched in pairs of legitimate and falsified news. The database is free and released under an open license for educational and research purposes.	https://paperswithcode.com/dataset/polit-false-n-legit-news-db-2016-2017							
3167	GQN rooms-ring-camera	GQN rooms-ring-camera consist of scenes of a variable number of random objects captured in a square room of size 7x7 units. Wall textures, floor textures as well as the shapes of the objects are randomly chosen within a fixed pool of discrete options. There are 5 possible wall textures (red, green, cerise, orange, yellow), 3 possible floor textures (yellow, white, blue) and 7 possible object shapes (box, sphere, cylinder, capsule, cone, icosahedron and triangle). Each scene contains 1, 2 or 3 objects. In this simplified version of the dataset, the camera only moves on a fixed ring and always faces the center of the room.	https://paperswithcode.com/dataset/gqn-rooms-ring-camera							
3168	ISOT Fake News Dataset	The ISOT Fake News dataset is a compilation of several thousands fake news and truthful articles, obtained from different legitimate news sites and sites flagged as unreliable by Politifact.com.	https://paperswithcode.com/dataset/isot-fake-news-dataset							
3169	ObjectsRoom	"The ObjectsRoom dataset is based on the MuJoCo environment used by the Generative Query Network 4 and is a multi-object extension of the 3d-shapes dataset. The training set contains 1M scenes with up to three objects. We also provide ~1K test examples for the following variants:
2.1 Empty room: scenes consist of the sky, walls, and floor only.
2.2 Six objects: exactly 6 objects are visible in each image.
2.3 Identical color: 4-6 objects are placed in the room and have an identical, randomly sampled color.
Datapoints consist of an image and fixed number of masks. The first four masks correspond to the sky, floor, and two halves of the wall respectively. The remaining masks correspond to the foreground objects.
Source: Objects Room"	https://paperswithcode.com/dataset/objectsroom							
3170	SVDC Fake News Dataset	A labeled dataset that presents fake news surrounding the conflict in Syria. The dataset consists of a set of articles/news labeled by 0 (fake) or 1 (credible). Credibility of articles are computed with respect to a ground truth information obtained from the Syrian Violations Documentation Center  (VDC). In particular, for each article, we crowdsource the information extraction (e.g., date, location, Number of casualties) job using the crowdsourcing platform Figure Eight (formally CrowdFlower). Then, we match those articles against the VDC database to be able to deduce whether an article is fake or not. The dataset can be used to train machine learning models to detect fake news.	https://paperswithcode.com/dataset/svdc-fake-news-dataset							
3171	FakeNewsAMT & Celebrity	FakeNewsAMT & Celebrity include two novel datasets for the task of fake news detection, covering seven different news domains.	https://paperswithcode.com/dataset/fakenewsamt-celebrity	23/08/2017						
3172	DND	"Benchmarking Denoising Algorithms with Real Photographs
This dataset consists of 50 pairs of noisy and (nearly) noise-free images captured with four consumer cameras. Since the images are of very high-resolution, the providers extract 20 crops of size 512 × 512 from each image, thus yielding a total of 1000 patches."	https://paperswithcode.com/dataset/dnd		Darmstadt Noise Dataset					
3173	Cityscapes-VPS	Cityscapes-VPS is a video extension of the Cityscapes validation split. It provides 2500-frame panoptic labels that temporally extend the 500 Cityscapes image-panoptic labels. There are total 3000-frame panoptic labels which correspond to 5, 10, 15, 20, 25, and 30th frames of each 500 videos, where all instance ids are associated over time. It not only supports video panoptic segmentation (VPS) task, but also provides super-set annotations for video semantic segmentation (VSS) and video instance segmentation (VIS) tasks.	https://paperswithcode.com/dataset/cityscapes-vps	19/06/2020						
3174	METU Trademark	The METU Trademark Dataset is a large dataset (the largest publicly available logo dataset as of 2014, and the largest one not requiring any preprocessing as of 2017), which is composed of more than 900K real logos belonging to real companies worldwide. The dataset also includes query sets of varying difficulties, allowing Trademark Retrieval researchers to benchmark their methods against other methods to progress the field.	https://paperswithcode.com/dataset/metu-trademark-dataset	20/01/2017						
3175	PS-Plant dataset	"Automated leaf segmentation is a challenging area in computer vision. Recent advances in machine learning approaches allowed to achieve better results than traditional image processing techniques; however, training such systems often require large annotated data sets. To contribute with annotated data sets and help to overcome this bottleneck in plant phenotyping research, here we provide a novel photometric stereo (PS) data set with annotated leaf masks. This data set forms part of the work done in the BBSRC Tools and Resources Development project BB/N02334X/1.
Source: A photometric stereo-based 3D imaging system using computer vision and deep learning for tracking plant growth
Image Source: PS-Plant training dataset description"	https://paperswithcode.com/dataset/gytis	19/10/2018	PS-Plant dataset					
3176	Real SVBRDF	A total of 80 real material samples were captured in a dark room. For each material, multiple captures were collected at different distances from the camera (between 250 and 650 mm) to observe both macro- and micro-level details. The dataset is mostly comprised of planar specimens but also includes non-planar objects such as mugs, globes, crumpled paper, etc. As shown above, it contains a rich diversity of materials, including diffuse or specular wrapping papers, fabrics, anisotropic metals, plastics, rugs, ceramic and wood flooring samples, etc. Each capture set includes 12 LDR (8 bpp) RGB-D images at 4K pixel resolution. Each set is captured at 50% and 100% of maximum light intensity. In total, we captured 462 such image sets (combinations of light intensities, distances to the camera, and material sample).	https://paperswithcode.com/dataset/real-svbrdf	08/10/2020						
3177	MLPF	"Dataset of 50,000 top quark-antiquark (ttbar) events produced in proton-proton collisions at 14 TeV, overlaid with minimum bias events corresponding to a pileup of 200 on average. The dataset consists of detector hits as the input, generator particles as the ground truth and reconstructed particles from DELPHES for additional validation.
The DELPHES model corresponds to a CMS-like detector with a multi-layered charged particle tracker, an electromagnetic and hadron calorimeter. Pythia8 and Delphes3 were used for the simulation.
Each file contains a bzip2-compressed python pickle with the following contents:
```

data = pickle.load(bz2.BZ2File(""out/pythia8_ttbar/tev14_pythia8_ttbar_0_0.pkl.bz2"", ""rb""))

Each file contains lists of arrays X (detector elements), ygen (generator particles) and ycand (rule-based PF particles from Delphes) for 100 events

len(data[""ycand""]), len(data[""ygen""]), len(data[""X""])
100, 100, 100

Each element in the list corresponds to an event. The first event in the file contains 5992 detector elements, ygen and ycand are 0-padded to the same length as X

data[""X""][0].shape, data[""ygen""][0].shape, data[""ycand""][0].shape, 
((5992, 12), (5992, 7), (5992, 7))

The X rows are detector elements: calorimeter towers and tracks with the following 12-features (0-padded)
tower: [type==1, Et (GeV), eta, sin phi, cos phi, E (GeV), Eem (GeV), Ehad (GeV), 0, 0, 0, 0]
track: [type==2, pt (GeV), eta, sin phi, cos phi, P (GeV), eta_outer, sin phi_outer, cos phi_outer, charge, is_gen_muon, is_gen_electron]
The ygen (ycand) rows are generator-level truth particles (rule-based PF particles from Delphes) with the following features:
[pid, charge, pt (GeV), eta, sin phi, cos phi, E (GeV)]
pid==0: placeholder/padding entry
pid==1: charged hadrons
pid==2: neutral hadrons
pid==3: photons
pid==4: electrons
pid==5: muons
```"	https://paperswithcode.com/dataset/mlpf	21/01/2021	Simulated particle-level dataset of ttbar with PU200 using Pythia8+Delphes3 for machine learned particle flow (MLPF)					
3178	EXPLICIT 3D CHANGE DETECTION USING RAY-TRACING IN SPHERICAL COORDINATES	Real and simulated lidar data of indoor and outdoor scenes, before and after geometric scene changes have occurred. Data include lidar scans from multiple viewpoints with provided coordinate transforms, and manually annotated ground-truth regarding which parts of the scene have changed between subsequent scans.	https://paperswithcode.com/dataset/explicit-3d-change-detection-using-ray	01/05/2013						
3179	ACFR Orchard Fruit Dataset	"ACFR Orchard Fruit Dataset is an agricultural dataset containing images and annotations for different fruits, collected at different farms across Australia. The dataset was gathered by the agriculture team at the Australian Centre for Field Robotics, The University of Sydney, Australia.
Source: ACFR Orchard Fruit Dataset"	https://paperswithcode.com/dataset/acfr-orchard-fruit-dataset	12/10/2016						
3180	University of Washington/Northwestern University (UW/NU) Corpus	"The University of Washington/Northwestern University (UW/NU) Corpus contains recordings and textgrids of Pacific Northwest and Northern Cities speakers reading a subset of the IEEE ""Harvard"" sentences. The UW/NU Corpus Version 1.0 has been used to study the effects of dialectal variation on speech intelligibility, while version 2.0 is being used in ongoing research in speech intelligibility and gender interaction. Development is supported by the National Institutes of Health, National Institute on Deafness and Other Communication Disorders grant R01-DC006014. The PN/NC Corpus is well suited for both clinical and research studies where high-fidelity recordings and regional accent control are desirable."	https://paperswithcode.com/dataset/university-of-washington-northwestern							
3181	UNITOPATHO	Histopathological characterization of colorectal polyps allows to tailor patients' management and follow up with the ultimate aim of avoiding or promptly detecting an invasive carcinoma. Colorectal polyps characterization relies on the histological analysis of tissue samples to determine the polyps malignancy and dysplasia grade. Deep neural networks achieve outstanding accuracy in medical patterns recognition, however they require large sets of annotated training images. We introduce UniToPatho, an annotated dataset of 9536 hematoxylin and eosin stained patches extracted from 292 whole-slide images, meant for training deep neural networks for colorectal polyps classification and adenomas grading. The slides are acquired through a Hamamatsu Nanozoomer S210 scanner at 20× magnification (0.4415 μm/px)	https://paperswithcode.com/dataset/unitopatho	25/01/2021						
3182	WiC-TSV	WiC-TSV is a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, it is a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as a binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model.	https://paperswithcode.com/dataset/wic-tsv	30/04/2020	Words-in-Context: Target Sense Verification					
3183	Clinical Admission Notes from MIMIC-III	"This dataset is created from MIMIC-III (Medical Information Mart for Intensive Care III) and contains simulated patient admission notes. The clinical notes contain information about a patient at admission time to the ICU and are labelled for four outcome prediction tasks: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay.
To obtain the data one first has to gain access to the MIMIC-III dataset and then run the scripts introduced in the linked repository."	https://paperswithcode.com/dataset/hospital-admission-notes-from-mimic-iii	08/02/2021						
3184	IBC	"The Individual Brain Charting (IBC) project aims at providing a new generation of functional-brain atlases. To map cognitive mechanisms in a fine scale, task-fMRI data at high-spatial-resolution are being acquired on a fixed cohort of 12 participants, while performing many different tasks. These data—free from both inter-subject and inter-site variability—are publicly available as means to support the investigation of functional segregation and connectivity as well as individual variability with a view to establishing a better link between brain systems and behavior.
 What’s special about the IBC dataset?

Taskwise dataset: spanning the cognitive spectrum within subject
Fixed cohort over a 10-year span to minimize inter-subject variability
Fixed experimental setting to minimize inter-site variability
Multimodal: fMRI (task-based and resting state), DWI, structural

 Main characteristics 

12 healthy participants (aged 27-40 at the time of recruitment)
Spatial resolution: 1.5mm (isotropic); Temporal resolution: 2s
Scanner: Siemens 3T Magnetom Prisma; Coil: 64-channel
50 acquisitions per participant upon completion of the dataset in 2022"	https://paperswithcode.com/dataset/ibc		Individual Brain Charting					
3185	MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge	"Under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm. The standard registration data was generated by NiftyReg.
Source: MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge
Image source: MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge"	https://paperswithcode.com/dataset/miccai-2015-multi-atlas-abdomen-labeling	15/04/2015						
3186	Alchemy	"The DeepMind Alchemy environment is a meta-reinforcement learning benchmark that presents tasks sampled from a task distribution with deep underlying structure. It was created to test for the ability of agents to reason and plan via latent state inference, as well as useful exploration and experimentation. 
Alchemy is a single-player video game, implemented in Unity. The player sees a first-person view of a table with a number of objects on it, including a set of colored stones, a set of dishes containing colored potions, and a central cauldron. Stones have different point values, and points are collected when stones are added to the cauldron. By dipping stones into the potions, the player can transform the stones’ appearance, and thus their value, increasing the number of points that can be won.
Source: dm_alchemy: DeepMind Alchemy environment
Image Source: dm_alchemy: DeepMind Alchemy environment"	https://paperswithcode.com/dataset/alchemy	04/02/2021						
3187	Biase et al	Source: Cell fate inclination within 2-cell and 4-cell mouse embryos revealed by single-cell RNA sequencing	https://paperswithcode.com/dataset/biase-et-al	05/08/2014						
3188	Goolam et al	Source: Heterogeneity in Oct4 and Sox2 Targets Biases Cell Fate in 4-Cell Mouse Embryos	https://paperswithcode.com/dataset/goolam-et-al	23/03/2016						
3189	Yan et al	Source: Single-cell RNA-Seq profiling of human preimplantation embryos and embryonic stem cells	https://paperswithcode.com/dataset/yan-et-al	11/08/2013						
3190	Deng et al	Source: Single-cell RNA-seq reveals dynamic, random monoallelic gene expression in mammalian cells	https://paperswithcode.com/dataset/deng-et-al	10/01/2014						
3191	Pollen et al	"TPM values together with cell type annotations that were obtained from Alex Pollen on 15/10/15
Source: Low-coverage single-cell mRNA sequencing reveals cellular heterogeneity and activated signaling pathways in developing cerebral cortex"	https://paperswithcode.com/dataset/pollen-et-al	03/08/2014						
3192	Treutlein et al	Source: Reconstructing lineage hierarchies of the distal lung epithelium using single-cell RNA-seq	https://paperswithcode.com/dataset/treutlein-et-al							
3193	Synthetic Rain Datasets	"The Synthetic Rain Datasets consists of 13,712 clean-rain image pairs gathered from multiple datasets (Rain14000, Rain1800, Rain800, Rain12). With a single trained model, evaluation could be performed on various test sets, including Rain100H, Rain100L, Test100, Test2800, and Test1200.
PSNR and SSIM are computed on Y-channel in YCbCr color space."	https://paperswithcode.com/dataset/synthetic-rain-datasets	24/03/2020						
3194	EPIC-KITCHENS-100	"This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the ""test of time"" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit ""two years on"".
The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics."	https://paperswithcode.com/dataset/epic-kitchens-100	23/06/2020						
3195	UAVDT	"UAVDT is a large scale challenging UAV Detection and Tracking benchmark (i.e., about 80, 000 representative frames from 10 hours raw videos) for 3 important fundamental tasks, i.e., object DETection
(DET), Single Object Tracking (SOT) and Multiple Object Tracking (MOT).
The dataset is captured by UAVs in various complex scenarios. The objects of
interest in this benchmark are vehicles. The frames are manually annotated with bounding boxes and some useful attributes, e.g., vehicle category and occlusion. 
The UAVDT benchmark consists of 100 video sequences, which are selected
from over 10 hours of videos taken with an UAV platform at a number of locations in urban areas, representing various common scenes including squares, arterial streets, toll stations, highways, crossings and T-junctions. The videos
are recorded at 30 frames per seconds (fps), with the JPEG image resolution of 1080 × 540 pixels."	https://paperswithcode.com/dataset/uavdt	26/03/2018	Unmanned Aerial Vehicle Benchmark Object Detection and Tracking					
3196	The Contextual TV Dataset	"Using the Experience-Sampling Method (ESM), participants are asked to report TV consumption multiple times each day for a five week period. Through self-reported data, authors decrease uncertainty of exposure to content, and allow collection of non-trivial information, such as how much attention is paid to the TV. The data is structured to accommodate quantitative analyses, e.g. in the CARS community, and is publicly available under the name Contextual TV (CTV) dataset.
Source: Kristoffersen et al.
Image source: Kristoffersen et al."	https://paperswithcode.com/dataset/the-contextual-tv-dataset	30/07/2018	CTV					
3197	twitter politicians data	"Dataset based on Twitter usernames of American politicians. Data extracted from Wikidata.
The same politician can appear several times: if he has different pseudonyms on Twitter or Instagram, if he has been in several parties, or if several Twitter account IDs are associated with him. But the data is sorted in ascending order by name, so it is visible"	https://paperswithcode.com/dataset/twitter-politicians-data	14/05/2020						
3198	CoNLL-2012	"The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011.
Source: Pradhan et al."	https://paperswithcode.com/dataset/conll-2012-1	01/07/2012						
3199	3D Platelet EM	"The platelet-em dataset contains two 3D scanning electron microscope (EM) images of human platelets, as well as instance and semantic segmentations of those two image volumes.
This data has been reviewed by NIBIB, contains no PII or PHI, and is cleared for public release. All files use a multipage uint16 TIF format. A 3D image with size [Z, X, Y] is saved as Z pages of size [X, Y]. Image voxels are approximately 40x10x10 nm"	https://paperswithcode.com/dataset/3d-platelet-em	28/01/2021	Platelet Electron Microscopy					
3200	Sintel 4D LFV	A medium-scale synthetic 4D Light Field video dataset for depth (disparity) estimation. From the open-source movie Sintel. The dataset consists of 24 synthetic 4D LFVs with 1,204x436 pixels, 9x9 views, and 20–50 frames, and has ground-truth disparity values, so that can be used for training deep learning-based methods. Each scene was rendered with a clean pass after modifying the production file of Sintel with reference to the MPI Sintel dataset.	https://paperswithcode.com/dataset/sintel-4d-lfv	05/12/2020	Sintel 4D Light Field Video Dataset					
3201	Bee4Exp Honeybee Detection	"A dataset for flying honeybee detection introduced in ""A Method for Detection of Small Moving Objects in UAV Videos"". 
This dataset consists of three videos with flying honeybees in a natural environment."	https://paperswithcode.com/dataset/bee4exp-honeybee-detection	11/02/2021	Bee4Exp Honeybee Detection					
3202	MHSMA	"The MHSMA dataset is a collection of human sperm images from 235 patients with male factor infertility. Each image is labeled by experts for normal or abnormal sperm acrosome, head, vacuole, and tail.
The training, validation, and test sets contain 1000, 240, and 300 images, respectively.
Images are available in two different crop sizes: 128x128- and 64x64-pixel.
Paper: A novel deep learning method for automatic assessment of human sperm images"	https://paperswithcode.com/dataset/mhsma		The Modified Human Sperm Morphology Analysis					
3203	VITON	"VITON was a dataset for virtual try-on of clothing items. It consisted of 16,253 pairs of images of a person and a clothing item .
The authors have removed the dataset and it is no longer publicly available due to copyright issues."	https://paperswithcode.com/dataset/viton	22/11/2017	VITON-Zalando Dataset					
3204	Metric-Type of Numerical Tables	"Metric-Type of Numerical Tables is a dataset extracted from scientific papers (ACL anthology website) consisting of header tables, captions, and metric-types.
Image source: Suadaa et al."	https://paperswithcode.com/dataset/metric-type-numerical-table	01/02/2021						
3205	Deeply vocal characterizer	"Deeply vocal characterizer is a human nonverbal vocalization dataset. This sample dataset consists of about 0.6 hours(56.7 hours in the full set) of audio(16 kHz, 16-bit, mono) across 16 human nonverbal vocalization classes, including throat-clearing, coughing, laughing, panting, and etc. The audio contents are crowdsourced by the general public of South Korea.
The dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license."	https://paperswithcode.com/dataset/deeply-vocal-characterizer	27/01/2021						
3206	Deeply Korean read speech	"Deeply Korean read speech corpus contains pairs of Korean speakers reading a script with 3 distinct text sentiments (negative, neutral, positive), with 3 distinct voice sentiments (negative, neutral, positive), are recorded. The recordings took place in 3 different types of places, which are an anechoic chamber, studio apartment, and dance studio, of which the level of reverberation differs. And in order to examine the effect of the distance of mic from the source and device, every experiment is recorded at 3 distinct distances with 2 types of smartphone, iPhone X, and Galaxy S7.
This sample dataset consists of about 3 hours(290 hours in the full set) of audio(16 kHz, 16-bit, mono), and one pair of speakers. The dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license."	https://paperswithcode.com/dataset/deeply-korean-read-speech-corpus	27/01/2021						
3207	Deeply Parent-Child vocal interaction	"Deeply Parent-Child Vocal Interaction contains the interaction of 24 pairs of parent and child(total 48 speakers), such as reading fairy tales, singing children’s songs, conversing, and others, is recorded. The recordings took place in 3 different types of places, which are an anechoic chamber, studio apartment, and dance studio, of which the level of reverberation differs. And in order to examine the effect of the distance of mic from the source and device, every experiment is recorded at 3 distinct distances) with 2 types of smartphone, iPhone X, and Galaxy S7.
This sample dataset consists of about 3 hours(282 hours in the full set) of audio(16 kHz, 16-bit, mono), and one pair of speakers. The dataset is a subset(approximately 1%) of a much bigger dataset which were recorded under the same circumstances as these open-source datasets. Please contact us(contact@deeplyinc.com) for the full set with the research/commercial license."	https://paperswithcode.com/dataset/deeply-parent-child-vocal-interaction	27/01/2021						
3208	Lesion Boundary Segmentation Dataset	"Lesion Boundary Segmentation Dataset is a dataset for lesion segmentation from the ISIC2018 challenge. The dataset contains skin lesions and their corresponding annotations.
Image source :"	https://paperswithcode.com/dataset/lesion-boundary-segmentation-dataset	09/02/2019						
3209	HOC	"The Hallmarks of Cancer (*HOC) corpus consists of 1852 PubMed publication abstracts manually annotated by experts according to the Hallmarks of Cancer taxonomy. The taxonomy consists of 37 classes in a hierarchy. Zero or more class labels are assigned to each sentence in the corpus. 
Source: Hallmarks of Cancer Corpus
Image source: Hallmarks of Cancer Corpus"	https://paperswithcode.com/dataset/hoc-1	09/10/2015	Hallmarks of Cancer					
3210	MIT-BIH AFDB	"This database includes 25 long-term ECG recordings of human subjects with atrial fibrillation (mostly paroxysmal).
Of these, 23 records include the two ECG signals (in the .dat files); records 00735 and 03665 are represented only by the rhythm (.atr) and unaudited beat (.qrs annotation files.
The individual recordings are each 10 hours in duration, and contain two ECG signals each sampled at 250 samples per second with 12-bit resolution over a range of ±10 millivolts. The original analog recordings were made at Boston's Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) using ambulatory ECG recorders with a typical recording bandwidth of approximately 0.1 Hz to 40 Hz. The rhythm annotation files (with the suffix .atr) were prepared manually; these contain rhythm annotations of types (AFIB (atrial fibrillation), (AFL (atrial flutter), (J (AV junctional rhythm), and (N (used to indicate all other rhythms). (The original rhythm annotation files, still available in the old directory, used AF, AFL, J, and N to mark these rhythms; the atr annotations in this directory have been revised for consistency with those used for the MIT-BIH Arrhythmia Database.) Beat annotation files (with the suffix .qrs) were prepared using an automated detector and have not been corrected manually. For some records, manually corrected beat annotation files (with the suffix .qrsc) are available. (The .qrs annotations may be useful for studies of methods for automated AF detection, where such methods must be robust with respect to typical QRS detection errors. The .qrsc annotations may be preferred for basic studies of AF itself, where QRS detection errors would be confounding.) Note that in both .qrs and .qrsc files, no distinction is made among beat types (all beats are labelled as if normal).
Source: MIT-BIH Atrial Fibrillation Database"	https://paperswithcode.com/dataset/mit-bih-afdb	04/10/2000	MIT-BIH Atrial Fibrilation Database					
3211	ARC-DA	"ARC Direct Answer Questions (ARC-DA) dataset consists of 2,985 grade-school level, direct-answer (""open response"", ""free form"") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018.
How the dataset was built
These questions were derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018. The ARC Easy and ARC Challenge set questions in the original dataset were combined and then filtered/modified by the following process:


Turking: Each of the multiple-choice questions was presented as a direct answer question to five crowdsourced workers to gather additional answers.


Heuristic filtering: The questions were filtered based on the following heuristic filters:

Questions having a threshold number of turker answers, as a proxy for concreteness of the question.
Questions having at least two turker-provided answers with word overlap, as a measure of confidence in the correctness of the answers, and also straightforwardness of the question.
Other heuristics to identify questions that only make sense as multiple-choice questions, such as, questions starting with the phrase “Which of the following”.



Further manual vetting: We had volunteers in house do another pass of vetting where they:

Marked highly open-ended questions with too many answer choices, such as “Name an insect”, or otherwise invalid questions, for removal. These are filtered out.
Removed some of the bad answers gathered from turking.
Reworded questions to make them more suited to direct answer question format, for e.g., a question such as “What element is contained in table salt?” which would make sense as a multiple-choice question, needs be reworded to something like “Name an element present in table salt”.
Added any additional answers to the questions they could think of that were not present in the turker provided answers.



Image source: ARC-DA dataset"	https://paperswithcode.com/dataset/arc-da	05/02/2021	ARC Direct Answer Questions					
3212	Switchboard-1 Corpus	"The Switchboard-1 Telephone Speech Corpus (LDC97S62) consists of approximately 260 hours of speech and was originally collected by Texas Instruments in 1990-1, under DARPA sponsorship. The first release of the corpus was published by NIST and distributed by the LDC in 1992-3.
Switchboard is a collection of about 2,400 two-sided telephone conversations among 543 speakers (302 male, 241 female) from all areas of the United States. A computer-driven robot operator system handled the calls, giving the caller appropriate recorded prompts, selecting and dialing another person (the callee) to take part in a conversation, introducing a topic for discussion and recording the speech from the two subjects into separate channels until the conversation was finished. About 70 topics were provided, of which about 50 were used frequently. Selection of topics and callees was constrained so that: (1) no two speakers would converse together more than once and (2) no one spoke more than once on a given topic.
Source: https://catalog.ldc.upenn.edu/LDC97S62"	https://paperswithcode.com/dataset/switchboard-1-corpus	01/09/2008						
3213	MRDA	"The MRDA corpus consists of about 75 hours of speech from 75 naturally-occurring meetings among 53 speakers. The tagset used for labeling is a modified version of the SWBD-DAMSL tagset. It is annotated with three types of information: marking of the dialogue act segment boundaries, marking of the dialogue acts and marking of correspondences between dialogue acts.
Description from NLP Progress"	https://paperswithcode.com/dataset/mrda		ICSI Meeting Recorder Dialog Act Corpus					
3214	CLEVR-Humans	We collect  a  new  dataset  of  human-posed  free-form  natural  language  questions  about  CLEVR  images.    Many  of  these questions have out-of-vocabulary words and require reasoning skills that are absent from our model’s repertoire	https://paperswithcode.com/dataset/clevr-humans	10/05/2017						
3215	Funcom	"Funcom is a collection of ~2.1 million Java methods and their associated Javadoc comments. This data set was derived from a set of 51 million Java methods and only includes methods that have an associated comment, comments that are in the English language, and has had auto-generated files removed. Each method/comment pair also has an associated method_uid and project_uid so that it is easy to group methods by their parent project.
This dataset of function pairs is used for source code summarisation.
Source: Funcom"	https://paperswithcode.com/dataset/funcom	04/04/2019						
3216	Synthetic and Real Apache Log Records	"Each file contains a specific dataset described in the paper ""On Automatic Parsing of Log Records"". For example, T_E.txt contains the data for the dataset $T_E$. 
In a file, each log string resides on a separate line and contains a 2-tuple separated by tab (\t). The first element of the tuple is the actual log string that has to be parsed. The second element is the corresponding “translation” specifying the field name for each of the characters of the first element."	https://paperswithcode.com/dataset/synthetic-and-real-apache-log-records	12/02/2021	"Dataset for the paper ""On Automatic Parsing of Log Records"""					
3217	COVID-19 Fake News Dataset	Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\% F1-score with SVM.	https://paperswithcode.com/dataset/covid-19-fake-news-dataset	06/11/2020	COVID19 Fake News Detection in English					
3218	Real Blur Dataset	"The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs.
The deblurring result is first aligned to its ground truth sharp image using a homography estimated by the enhanced correlation coefficients method, and PSNR or SSIM is computed in sRGB color space."	https://paperswithcode.com/dataset/real-blur-dataset	01/08/2020						
3219	The Annotated Gumar Corpus		https://paperswithcode.com/dataset/the-annotated-gumar-corpus	01/05/2018						
3220	IG-3.5B-17k	IG-3.5B-17k is an internal Facebook AI Research dataset for training image classification models. It consists of hashtags for up to 3.5 billion public Instagram images.	https://paperswithcode.com/dataset/ig-3-5b-17k	02/05/2018	IG-3.5B-17k					
3221	IG-1B-Targeted	IG-1B-Targeted is an internal Facebook AI Research dataset that consists of 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets.	https://paperswithcode.com/dataset/ig-1b-targeted	02/05/2019	IG-1B-Targeted					
3222	DAGM2007	"This is a synthetic dataset for defect detection on textured surfaces. It was originally created for a competition at the 2007 symposium of the DAGM (Deutsche Arbeitsgemeinschaft für Mustererkennung e.V., the German chapter of the International Association for Pattern Recognition). The competition was hosted together with the GNSS (German Chapter of the European Neural Network Society).
After the competition, the dataset has been used as a test dataset in multiple projects and research papers. It is publicly available from the University of Heidelberg website (Heidelberg Collaboratory for Image Processing).
The data is artificially generated, but similar to real world problems. The first six out of ten datasets, denoted as development datasets, are supposed to be used for algorithm development. The remaining four datasets, which are referred to as competition datasets, can be used to evaluate the performance. Researchers should consider not using or analyzing the competition datasets before the development is completed as a code of honour."	https://paperswithcode.com/dataset/dagm2007		DAGM2007					
3223	DSTC 8 Track 2	"Dialog System Technology Challenges 8 (DSTC) Track 2 builds on the success of DSTC 7 Track 1 (NOESIS: Noetic End-to-End Response Selection Challenge). It proposes an extension of the task, incorporating new elements that are vital for the creation of a deployed task-oriented dialogue system. Specifically, three new dimensions are added to the challenge:

Conversations with more than 2 participants
Predicting whether a dialogue has solved the problem yet,
Handling multiple simultaneous conversations. Each of these adds an exciting new dimension and brings the task closer to the creation of systems able to handle the complexity of real-world conversation.
This challenge is offered with two goal oriented dialog datasets, used in 4 subtasks. 

Source: Dialog System Technology Challenges 8 (DSTC 8) - Track 2
Image source: DSTC 8 Track 2"	https://paperswithcode.com/dataset/dstc-8-track-2		Dialog System Technology Challenges 8 Track 2					
3224	Ubuntu Chat Corpus	"The Ubuntu Chat Corpus (UCC) is composed of archived chat logs from Ubuntu's Internet Relay Chat technical support channels. Ubuntu uses IRC as one of many modes of technical support -- it offers real-time problem solving. The authors have taken some of the archived messages (which are in the public domain), reorganized the file structure, removed some unnecessary system messages, and compressed them to make it easier to obtain.
Source: Ubuntu Chat Corpus
Image Source: Ubuntu Chat Corpus"	https://paperswithcode.com/dataset/ubuntu-chat-corpus							
3225	Liu et al. Corpus	The Liu et al. Corpus is a pretraining dataset for large language models. It consists of 160Gb of news, books, stories, and web text.	https://paperswithcode.com/dataset/liu-et-al-corpus	08/05/2019	Liu et al. Corpus					
3226	OSCAR	OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. The dataset used for training multilingual models such as BART incorporates 138 GB of text.	https://paperswithcode.com/dataset/oscar	11/06/2020	OSCAR					
3227	S-SOD	To validate the generalization abilities of SOD models, we create a small-scale dataset by collecting the most challenging images with varying brightness and contrast, background and foreground colors overlap, among many others. We conclude that the current models, including ours, are not trust-worthy for real-world practice, demanding extensive future research for more efficient and generalized SOD models.	https://paperswithcode.com/dataset/s-sod	12/02/2021	Surveillance Salient Object Detection					
3228	Reddit Corpus	"Reddit Corpus is part of a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using '1-of-100 accuracy'. The Reddit Corpus contains 726 million multi-turn dialogues from the Reddit board. 
Source: conversational-datasets
Image source: Henderson et al."	https://paperswithcode.com/dataset/reddit-corpus	13/04/2019						
3229	Advising Corpus	"Advising Corpus is a dataset based on an entirely new collection of dialogues in which university students are being advised which classes to take. These were collected at the University of Michigan with IRB approval. They were released as part of DSTC 7 track 1 and used again in DSTC 8 track 2.
Source: Gunasekara et al.
Image source: Gunasekara et al."	https://paperswithcode.com/dataset/advising-corpus	01/08/2019						
3230	CCNet	"CCNet is a dataset extracted from Common Crawl with a different filtering process than for OSCAR. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables. CCNet contains longer documents on average compared to OSCAR with smaller—and often noisier—documents weeded out.
Source: Martin et al"	https://paperswithcode.com/dataset/ccnet	01/11/2019	CCNet					
3231	French Wikipedia	French Wikipedia is a dataset used for pretraining the CamemBERT French language model. It uses the official 2019 French Wikipedia dumps	https://paperswithcode.com/dataset/french-wikipedia	10/11/2019	French Wikipedia					
3232	CEDAR Signature	"CEDAR Signature is a database of off-line signatures for signature verification. Each of 55 individuals contributed 24 signatures thereby creating 1,320 genuine signatures. Some were asked to forge three other writers’ signatures, eight times
per subject, thus creating 1,320 forgeries. Each signature was scanned at 300 dpi gray-scale and binarized using a gray-scale histogram. Salt pepper noise removal and slant normalization were two steps involved in image preprocessing. The database has 24 genuines and 24 forgeries available for each writer."	https://paperswithcode.com/dataset/cedar-signature	15/12/2006						
3233	BanglaLekhaImageCaptions	This dataset consists of images and annotations in Bengali. The images are human annotated in Bengali by two adult native Bengali speakers. All popular image captioning datasets have a predominant western cultural bias with the annotations done in English. Using such datasets to train an image captioning system assumes that a good English to target language translation system exists and that the original dataset had elements of the target culture. Both these assumptions are false, leading to the need of a culturally relevant dataset in Bengali, to generate appropriate image captions of images relevant to the Bangladeshi and wider subcontinental context. The dataset presented consists of 9,154 images.	https://paperswithcode.com/dataset/banglalekhaimagecaptions	02/09/2018						
3234	Multi-Domain Sentiment Dataset v2.0	"The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.
Source: Multi-Domain Sentiment Dataset (version 2.0)"	https://paperswithcode.com/dataset/multi-domain-sentiment-dataset-v2-0	01/06/2007						
3235	PAQ	"Probably Asked Questions (PAQ) is a very large resource of 65M automatically-generated QA-pairs. PAQ is a semi-structured Knowledge Base (KB) of 65M natural language QA-pairs, which models can memorise and/or learn to retrieve from. PAQ differs from traditional KBs in that questions and answers are stored in natural language, and that questions are generated such that they are likely to appear in ODQA datasets. PAQ is automatically constructed using a question generation model and Wikipedia.
Source: Lewis et al.
Image source: Lewis et al."	https://paperswithcode.com/dataset/paq	13/02/2021	Probably Asked Questions					
3236	BABEL Project	BABEL is a multilingual corpus of conversational telephone speech from IARPA, which includes Asian and African languages.	https://paperswithcode.com/dataset/babel		BABEL					
3237	AIDA CoNLL-YAGO	"AIDA CoNLL-YAGO contains assignments of entities to the mentions of named entities annotated for the original CoNLL 2003 entity recognition task. The entities are identified by YAGO2 entity name, by Wikipedia URL, or by Freebase mid. 
Source: AIDA CoNLL-YAGO"	https://paperswithcode.com/dataset/aida-conll-yago	01/07/2011						
3238	BTFDBB	"Reflectance measurements of Bidirectional Texture Functions (BTFs)
Database contains both flat samples:



as well as 3D geometry with texture mapped BTFs:


furthermore, there are some multispectral BTFs:"	https://paperswithcode.com/dataset/btfdbb	06/09/2014	BTF Database Bonn					
3239	CoNLL-2014 Shared Task: Grammatical Error Correction	"CoNLL-2014 will continue the CoNLL tradition of having a high profile shared task in natural language processing. This year's shared task will be grammatical error correction, a continuation of the CoNLL shared task in 2013. A participating system in this shared task is given short English texts written by non-native speakers of English. The system detects the grammatical errors present in the input texts, and returns the corrected essays. The shared task in 2014 will require a participating system to correct all errors present in an essay (i.e., not restricted to just five error types in 2013). Also, the evaluation metric will be changed to F0.5, weighting precision twice as much as recall.
The grammatical error correction task is impactful since it is estimated that hundreds of millions of people in the world are learning English and they benefit directly from an automated grammar checker. However, for many error types, current grammatical error correction methods do not achieve a high performance and thus more research is needed.
Source: CoNLL-2014 Shared Task: Grammatical Error Correction
Image source: Tou Ng et al."	https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error	01/06/2014						
3240	UBOFAB19	"A database of several hundred high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits.







Measurement HDR images are provided in OpenEXR format

SVBRDF fits are provided in X-Rite AxF format

Further geometric and radiometric calibration data is available as well."	https://paperswithcode.com/dataset/svbrdf-database-bonn	30/07/2019	SVBRDF Database Bonn					
3241	APPBENCH	"A database of 56 high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits. Used in the Fabric Appearance Challange.




Measurement HDR images are provided in OpenEXR format

SVBRDF fits are provided in X-Rite AxF format

Further geometric and radiometric calibration data is available as well."	https://paperswithcode.com/dataset/appbench	31/05/2020	SVBRDF Database Bonn					
3242	Chickenpox Cases in Hungary	"Chickenpox Cases in Hungary is a spatio-temporal dataset of weekly chickenpox (childhood disease) cases from Hungary. It can be used as a longitudinal dataset for benchmarking the predictive performance of spatiotemporal graph neural network architectures. The dataset consists of a county-level adjacency matrix and time series of the county-level reported cases between 2005 and 2015. There are 2 specific related tasks:

County level case count prediction.
National level case count prediction."	https://paperswithcode.com/dataset/chickenpox-cases-in-hungary	16/02/2021						
3243	Multimodal Opinionlevel Sentiment Intensity	"Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features.
Source: Zadeh et al
Image source: Zadeh et al"	https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity	20/06/2016	MOSI					
3244	WNUT 2017	"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet “so.. kktny in 30 mins?” - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text.
The goal of this task is to provide a definition of emerging and of rare entities, and based on that, also datasets for detecting these entities."	https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity	01/09/2017	WNUT 2017 Emerging and Rare entity recognition					
3245	RUSHOLD	"RUHSOLD is hate speech and offensive language dataset in Roman Urdu. The dataset contains over 10 thousand tweets that are hand labelled into the following categories:
1) Abusive/Offensive
2) Untargeted
3) Sexism
4) Religious
5) Neutral"	https://paperswithcode.com/dataset/rushold	30/11/2020	Roman Urdu Hate Speech and Offensive Language Dataset					
3246	Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison	"The Evaluation framework of Raganato et al. 2017 includes two training sets (SemCor-Miller et al., 1993- and OMSTI-Taghipour and Ng, 2015-) and five test sets from the Senseval/SemEval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015), standardized to the same format and sense inventory (i.e. WordNet 3.0).
Typically, there are two kinds of approach for WSD: supervised (which make use of sense-annotated training data) and knowledge-based (which make use of the properties of lexical resources).
Supervised: The most widely used training corpus used is SemCor, with 226,036 sense annotations from 352 documents manually annotated. All supervised systems in the evaluation table are trained on SemCor. Some supervised methods, particularly neural architectures, usually employ the SemEval 2007 dataset as development set (marked by *). The most usual baseline is the Most Frequent Sense (MFS) heuristic, which selects for each target word the most frequent sense in the training data.
Knowledge-based: Knowledge-based systems usually exploit WordNet or BabelNet as semantic network. The first sense given by the underlying sense inventory (i.e. WordNet 3.0) is included as a baseline.
Description from NLP Progress"	https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified	01/04/2017						
3247	PNT	"The Parsing Time Normalizations (PNT) corpus in SCATE format allows the representation of a wider variety of time expressions than previous approaches. This corpus was release with SemEval 2018 Task 6.
Source: NLP Progress"	https://paperswithcode.com/dataset/pnt		Parsing Time Normalizations					
3248	SemEval-2018 Task 9: Hypernym Discovery	"The SemEval-2018 hypernym discovery evaluation benchmark (Camacho-Collados et al. 2018) contains three domains (general, medical and music) and is also available in Italian and Spanish (not in this repository). For each domain a target corpus and vocabulary (i.e. hypernym search space) are provided. The dataset contains both concepts (e.g. dog) and entities (e.g. Manchester United) up to trigrams.
Source: NLP Progress
Image source: SemEval-2018 Task 9: Hypernym Discovery"	https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery	01/06/2018						
3249	WHU-Specular dataset	"WHU-Specular is a large dataset of annotated specular highlight regions created from real-world images. It can be used for specular highlight detection task. It contains 4310 image pairs (specular images and corresponding highlight masks). We randomly selected 3,017 images as the training set, and other 1293 images as the testing set.
Source: SHDNet"	https://paperswithcode.com/dataset/whu-specular-dataset	10/10/2020	WHU-Specular dataset					
3250	NAB	"The First Temporal Benchmark Designed to Evaluate  Real-time Anomaly Detectors Benchmark
The growth of the Internet of Things has created an abundance of streaming data. Finding anomalies in this data can provide valuable insights into opportunities or failures. Yet it’s difficult to achieve, due to the need to process data in real time, continuously learn and make predictions. How do we evaluate and compare various real-time anomaly detection techniques? 
The Numenta Anomaly Benchmark (NAB) provides a standard, open source framework for evaluating real-time anomaly detection algorithms on streaming data. Through a controlled, repeatable environment of open-source tools, NAB rewards detectors that find anomalies as soon as possible, trigger no false alarms, and automatically adapt to any changing statistics. 
NAB comprises two main components: a scoring system designed for streaming data and a dataset with labeled, real-world time-series data.
Source: Evaluating Real-time Anomaly Detection Algorithms – the Numenta Anomaly Benchmark
Image Source: https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/"	https://paperswithcode.com/dataset/nab	12/10/2015	Numenta Anomaly Benchmark					
3251	Newsela		https://paperswithcode.com/dataset/newsela	01/01/2015						
3252	B-T4SA		https://paperswithcode.com/dataset/b-t4sa	20/10/2017	B-T4SA					
3253	AVSpeech	"AVSpeech is a large-scale audio-visual dataset comprising
speech clips with no interfering background signals. The segments
are of varying length, between 3 and 10 seconds long, and in each clip
the only visible face in the video and audible sound in the soundtrack
belong to a single speaking person. In total, the dataset contains
roughly 4700 hours of video segments with approximately 150,000
distinct speakers, spanning a wide variety of people, languages
and face poses."	https://paperswithcode.com/dataset/avspeech	10/04/2018						
3254	Kinect-WSJ	Kinect-WSJ is a multichannel, multispeaker, reverberated, noisy dataset which extends the WSJ0-2mix singlechannel, non-reverberated, noiseless dataset to the strong reverberation and noise conditions and the Kinect-like microphone array geometry used in CHiME-5.	https://paperswithcode.com/dataset/kinect-wsj	24/10/2019						
3255	BiasBios	"The purpose of this dataset was to study gender bias in occupations. Online biographies, written in English, were collected to find the names, pronouns, and occupations. Twenty-eight most frequent occupations were identified based on their appearances.  The resulting dataset consists of 397,340 biographies spanning twenty-eight different occupations. Of these occupations, the professor is the most frequent, with 118,400 biographies, while the rapper is the least frequent, with 1,406 biographies. 
Important information about the biographies:
1. The longest biography is 194 tokens, while the shortest is eighteen; the median biography length is seventy-two tokens.
2. It should be noted that the demographics of online biographies’ subjects differ from those of the overall workforce and that this dataset does not contain all biographies on the Internet."	https://paperswithcode.com/dataset/biasbios	27/01/2019	Bias in Bios					
3256	AM-2k	AM-2k contains 2,000 high-resolution natural animal images from 20 categories along with manually labeled alpha mattes.	https://paperswithcode.com/dataset/am-2k-1	20/10/2020	Animal Matting - 2k					
3257	BG-20k	BG-20k contains 20,000 high-resolution background images excluded salient objects, which can be used to help generate high quality synthetic data.	https://paperswithcode.com/dataset/bg-20k	20/10/2020	Background Dataset - 20k					
3258	SPoC	"Pseudocode-to-Code (SPoC) is a program synthesis dataset, containing 18,356 programs with human-authored pseudocode and test cases.
Image source: https://sumith1896.github.io/spoc/"	https://paperswithcode.com/dataset/spoc	12/06/2019	Pseudocode-to-Code					
3259	OntoNotes 5.0	"OntoNotes 5.0 is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).
OntoNotes Release 5.0 contains the content of earlier releases - and adds source data from and/or additional annotations for, newswire, broadcast news, broadcast conversation, telephone conversation and web data in English and Chinese and newswire data in Arabic.
Source: https://catalog.ldc.upenn.edu/LDC2013T19"	https://paperswithcode.com/dataset/ontonotes-5-0	16/10/2013						
3260	WebText	"WebText is an internal OpenAI corpus created by scraping web pages with emphasis on
document quality. The authors scraped all outbound links from
Reddit which received at least 3
karma. The authors used the approach as a heuristic indicator for
whether other users found the link interesting, educational,
or just funny.
WebText contains the text subset of these 45 million links. It consists of over 8 million documents
for a total of 40 GB of text. All Wikipedia
documents were removed from WebText since it is a common data source
for other datasets."	https://paperswithcode.com/dataset/webtext	14/02/2019						
3261	MECCANO	"The MECCANO dataset is the first dataset of egocentric videos to study human-object interactions in industrial-like settings.
The MECCANO dataset has been acquired in an industrial-like scenario in which subjects built a toy model of a motorbike. We considered 20 object classes which include the 16 classes categorizing the 49 components, the two tools (screwdriver and wrench), the instructions booklet and a partial_model class.
Additional details related to the MECCANO:
20 different subjects in 2 countries (IT, U.K.)
Video Acquisition: 1920x1080 at 12.00 fps
11 training videos and 9 validation/test videos
8857 video segments temporally annotated indicating the verbs which describe the actions performed
64349 active objects annotated with bounding boxes
12 verb classes, 20 objects classes and 61 action classes"	https://paperswithcode.com/dataset/meccano	12/10/2020						
3262	ecoset	Ecoset, an ecologically motivated image dataset, is a large-scale image dataset designed for human visual neuroscience, which consists of over 1.5 million images from 565 basic-level categories. Category selection was based on English nouns that most frequently occur in spoken language (estimated on a set of 51 million words obtained from American television and film subtitles) and concreteness ratings from human observers. Ecoset consists of basic-level categories (including human categories man, woman, and child) that describe physical things in the world (rather than abstract concepts) that are important to humans.	https://paperswithcode.com/dataset/ecoset	15/02/2021						
3263	CoNLL-2003	"CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.
The data consists of eight files covering two languages: English and German.
For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.
The English data was taken from the Reuters Corpus. This corpus consists of Reuters news stories between August 1996 and August 1997.
For the training and development set, ten days worth of data were taken from the files representing the end of August 1996.
For the test set, the texts were from December 1996. The preprocessed raw data covers the month of September 1996.
The text for the German data was taken from the ECI Multilingual Text Corpus. This corpus consists of texts in many languages. The portion of data that
was used for this task, was extracted from the German newspaper Frankfurter Rundshau. All three of the training, development and test sets were taken
from articles written in one week at the end of August 1992.
The raw data were taken from the months of September to December 1992.
| English      data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 946      | 14,987    | 203,621 | 7140 | 3438 | 6321 | 6600 |
| Development  set  | 216      | 3,466     | 51,362  | 1837 | 922  | 1341 | 1842 |
| Test         set  | 231      | 3,684     | 46,435  | 1668 | 702  | 1661 | 1617 |
Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in English data files.
| German       data | Articles | Sentences | Tokens  | LOC  | MISC | ORG  | PER  |
|-------------------|----------|-----------|---------|------|------|------|------|
| Training     set  | 553      | 12,705    | 206,931 | 4363 | 2288 | 2427 | 2773 |
| Development  set  | 201      | 3,068     | 51,444  | 1181 | 1010 | 1241 | 1401 |
| Test         set  | 155      | 3,160     | 51,943  | 1035 | 670  | 773  | 1195 |
Number of articles, sentences, tokens and entities (locations, miscellaneous, organizations, and persons) in German data files."	https://paperswithcode.com/dataset/conll-2003	12/06/2003						
3264	QAMR	Question-Answer Meaning Representation (QAMR) represents a predicate-argument structure of a sentence with a set of question-answer pairs, so that annotations can be easily provided by non-experts. QAMR is a dataset of over 5,000 sentences and 100,000 questions created by crowdsourcing workers.	https://paperswithcode.com/dataset/qamr	16/11/2017	Question-Answer Meaning Representation Dataset					
3265	AW-OIE	All Words Open IE (AW-OIE) is an open information extraction dataset derived from Question-Answer Meaning Representation (QAMR) dataset.	https://paperswithcode.com/dataset/aw-oie	01/06/2018	All Words OpenIE					
3266	MSU Deinterlacer Benchmark	This is a dataset for video deinterlacing problem. The dataset contains 40 video sequences. Each sequence's length is 1 second. Resolution of all video sequences is 1920x1080. FPS varies from 24 to 60. TFF interlacing was used to get interlaced data from GT.	https://paperswithcode.com/dataset/msu-deinterlacer-benchmark-2020	26/11/2020						
3267	StrategyQA	"StrategyQA is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy.
It includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs.
Questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies."	https://paperswithcode.com/dataset/strategyqa	06/01/2021						
3268	Oxford 102 Flower	"Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images.
The images have large scale, pose and light variations. In addition, there are categories that have large variations within the category and several very similar categories."	https://paperswithcode.com/dataset/oxford-102-flower	01/12/2008	102 Category Flower Dataset					
3269	ICB	"A carefully chosen set of high-resolution high-precision natural images suited for compression algorithm evaluation.
The images historically used for compression research (lena, barbra, pepper etc...) have outlived their useful life and its about time they become a part of history only. They are too small, come from data sources too old and are available in only 8-bit precision.
These high-resolution high-precision images have been carefully selected to aid in image compression research and algorithm evaluation. These are photographic images chosen to come from a wide variety of sources and each one picked to stress different aspects of algorithms. Images are available in 8-bit, 16-bit and 16-bit linear variations, RGB and gray.
These Images are available without any prohibitive copyright restrictions.
These images are (c) there respective owners. You are granted full redistribution and publication rights on these images provided:

The origin of the pictures must not be misrepresented; you must not claim that you took the original pictures. If you use, publish or redistribute them, an acknowledgment would be appreciated but is not required.
Altered versions must be plainly marked as such, and must not be misinterpreted as being the originals.
No payment is required for distribution of this material, it must be available freely under the conditions stated here. That is, it is prohibited to sell the material.
This notice may not be removed or altered from any distribution.

For grayscale evaluation, use the Grayscale 8 bit dataset, for color evaluation, use the Color 8 bit dataset.
@online{icb,
  author = {Rawzor},
  title  = {Image Compression Benchmark},
  url    = {http://imagecompression.info/}
}"	https://paperswithcode.com/dataset/icb		Image Compression Benchmark					
3270	LIVE1	"Quality Assessment research strongly depends upon subjective experiments to provide calibration data as well as a testing mechanism. After all, the goal of all QA research is to make quality predictions that are in agreement with subjective opinion of human observers. In order to calibrate QA algorithms and test their performance, a data set of images and videos whose quality has been ranked by human subjects is required. The QA algorithm may be trained on part of this data set, and tested on the rest. 
```
@article{sheikh2006statistical,
  title={A statistical evaluation of recent full reference image quality assessment algorithms},
  author={Sheikh, Hamid R and Sabir, Muhammad F and Bovik, Alan C},
  journal={IEEE Transactions on image processing},
  volume={15},
  number={11},
  pages={3440--3451},
  year={2006},
  publisher={IEEE}
}
@online{sheikh2006live,
  title={LIVE image quality assessment database},
  author={Sheikh, HR and Wang, Z and Cormack, L and Bovik, AC},
  url={http://live.ece.utexas.edu/research/quality}
}
```"	https://paperswithcode.com/dataset/live1	16/10/2006	LIVE Public-Domain Subjective Image Quality Database					
3271	Classic5	Five classic grayscale images commonly used for image quality assessment tasks.	https://paperswithcode.com/dataset/classic5	04/09/2006						
3272	hls4ml LHC Jet dataset	"Dataset of high-pT jets from simulations of LHC proton-proton collisions
Prepared for FastML/HLS4ML studies: https://fastmachinelearning.org
Includes: High level features (see https://arxiv.org/abs/1804.06913)
Images: jet images with up to 100 particles/jet (see https://arxiv.org/abs/1908.05318)
List: list of jet features with up to 100 particles/jet (see https://arxiv.org/abs/1908.05318)"	https://paperswithcode.com/dataset/hls4ml-lhc-jet-dataset	16/04/2018	hls4ml LHC Jet dataset (100 particles)					
3273	RailEye3D Dataset	The RailEye3D dataset, a collection of train-platform scenarios for applications targeting passenger safety and automation of train dispatching, consists of 10 image sequences captured at 6 railway stations in Austria. Annotations for multi-object tracking are provided in both an unified format as well as the ground-truth format used in the MOTChallenge.	https://paperswithcode.com/dataset/raileye3d-dataset	23/02/2021						
3274	GraspNet-1Billion	GraspNet-1Billion provides large-scale training data and a standard evaluation platform for the task of general robotic grasping. The dataset contains 97,280 RGB-D image with over one billion grasp poses.	https://paperswithcode.com/dataset/graspnet-1billion	01/06/2020						
3275	MAEC	"MAEC is a new, large-scale multi-modal, text-audio paired, earnings-call dataset named MAEC, based on S&P 1500 companies. 
Source: MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction"	https://paperswithcode.com/dataset/maec		Multimodal Aligned Earnings Conference Call Dataset					
3276	NuCLS	"The NuCLS dataset contains over 220,000 labeled nuclei from breast cancer images from TCGA. These nuclei were annotated through the collaborative effort of pathologists, pathology residents, and medical students using the Digital Slide Archive. These data can be used in several ways to develop and validate algorithms for nuclear detection, classification, and segmentation, or as a resource to develop and evaluate methods for interrater analysis.
Data from both single-rater and multi-rater studies are provided. For single-rater data we provide both pathologist-reviewed and uncorrected annotations. For multi-rater datasets we provide annotations generated with and without suggestions from weak segmentation and classification algorithms.
Source: Amgad et al.
Image source:  Amgad et al."	https://paperswithcode.com/dataset/nucls	18/02/2021	Nucleus Classification, Localization and Segmentation					
3277	K-Hairstyle	"K-hairstyle is a novel large-scale Korean hairstyle dataset with 256,679 high-resolution images. In addition, K-hairstyle contains various hair attributes annotated by Korean expert hair stylists and hair segmentation masks.
Source: K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair editing and hairstyle classification
Image source: Kim et al."	https://paperswithcode.com/dataset/k-hairstyle	11/02/2021						
3278	CC12M	"Conceptual 12M (CC12M) is a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training.
Source: Changpinyo et al.
Image source: Changpinyo et al."	https://paperswithcode.com/dataset/cc12m	17/02/2021	Conceptual 12M					
3279	ACDC	"The goal of the Automated Cardiac Diagnosis Challenge (ACDC) challenge is to:

compare the performance of automatic methods on the segmentation of the left ventricular endocardium and epicardium as the right ventricular endocardium for both end diastolic and end systolic phase instances;
compare the performance of automatic methods for the classification of the examinations in five classes (normal case, heart failure with infarction, dilated cardiomyopathy, hypertrophic cardiomyopathy, abnormal right ventricle).

The overall ACDC dataset was created from real clinical exams acquired at the University Hospital of Dijon. Acquired data were fully anonymized and handled within the regulations set by the local ethical committee of the Hospital of Dijon (France). Our dataset covers several well-defined pathologies with enough cases to (1) properly train machine learning methods and (2) clearly assess the variations of the main physiological parameters obtained from cine-MRI (in particular diastolic volume and ejection fraction). The dataset is composed of 150 exams (all from different patients) divided into 5 evenly distributed subgroups (4 pathological plus 1 healthy subject groups) as described below. Furthermore, each patient comes with the following additional information : weight, height, as well as the diastolic and systolic phase instants.
The database is made available to participants through two datasets from the dedicated online evaluation website after a personal registration: i) a training dataset of 100 patients along with the corresponding manual references based on the analysis of one clinical expert; ii) a testing dataset composed of 50 new patients, without manual annotations but with the patient information given above. The raw input images are provided through the Nifti format.
Source: Automated Cardiac Diagnosis Challenge
Image source: Automated Cardiac Diagnosis Challenge"	https://paperswithcode.com/dataset/acdc	15/09/2021	Automated Cardiac Diagnosis Challenge					
3280	MoNuSeg	"The dataset for this challenge was obtained by carefully annotating tissue images of several patients with tumors of different organs and who were diagnosed at multiple hospitals. This dataset was created by downloading H&E stained tissue images captured at 40x magnification from TCGA archive. H&E staining is a routine protocol to enhance the contrast of a tissue section and is commonly used for tumor assessment (grading, staging, etc.). Given the diversity of nuclei appearances across multiple organs and patients, and the richness of staining protocols adopted at multiple hospitals, the training datatset will enable the development of robust and generalizable nuclei segmentation techniques that will work right out of the box.
Source: MoNuSeg"	https://paperswithcode.com/dataset/monuseg							
3281	GlaS	"The dataset used in this challenge consists of 165 images derived from 16 H&E stained histological sections of stage T3 or T42 colorectal adenocarcinoma. Each section belongs to a different patient, and sections were processed in the laboratory on different occasions. Thus, the dataset exhibits high inter-subject variability in both stain distribution and tissue architecture. The digitization of these histological sections into whole-slide images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel resolution of 0.465µm.
Source: Sirinukunwattana et al.
Image source: Sirinukunwattana et al."	https://paperswithcode.com/dataset/glas	01/03/2016	Gland Segmentation in Colon Histology Images Challenge					
3282	Brain US	"This brain anatomy segmentation dataset has 1300 2D US scans for training and 329 for testing. A total of 1629 in vivo B-mode US images were obtained from 20 different subjects (age<1 years old) who were treated between 2010 and 2016. The dataset contained subjects with IVH and without (healthy subjects but in risk of developing IVH). The US scans were collected using a Philips US machine with a C8-5 broadband curved array transducer using coronal and sagittal scan planes. For every collected image ventricles and septum pellecudi are manually segmented by an expert ultrasonographer. We split these images randomly into 1300 Training images and 329 Testing images for experiments. Note that these images are of size 512 × 512. 
Source: Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data
Image source: Learning to Segment Brain Anatomy from 2D Ultrasound with Less Data"	https://paperswithcode.com/dataset/brain-us	18/12/2019						
3283	PieAPP dataset	"The PieAPP dataset is a large-scale dataset used for training and testing perceptually-consistent image-error prediction algorithms.
The dataset can be downloaded from:  server containing a zip file with all data (2.2GB) or Google Drive (ideal for quick browsing). 
The dataset contains undistorted high-quality reference images and several distorted versions of these reference images. Pairs of distorted images corresponding to a reference image are labeled with probability of preference labels.
 These labels indicate the fraction of human population that considers one image to be visually closer to the reference over another in the pair.
To ensure reliable pairwise probability of preference labels, we query 40 human subjects via Amazon Mechanical Turk for each image pair.
We then obtain the percentage of people who selected image A over B as the ground-truth label for this pair, which is the probability of preference of A over B (the supplementary document explains the choice of using 40 human subjects to capture accurate probabilities).
This approach is more robust because it is easier to identify the visually closer image than to assign quality scores, and does not suffer from set-dependency or scalability issues like Swiss tournaments since we never label the images with  per-image quality scores (see the associated  paper and supplementary document for issues with such existing labeling schemes). 
A pairwise learning framework, discussed in the paper, can be used to train image error predictors on the PieAPP dataset.
Dataset statistics
We make this dataset available for non-commercial and educational purposes only. 
The dataset contains a total of 200 undistorted reference images, divided into train / validation / test split.
These reference images are derived from the Waterloo Exploration Dataset. We release the subset of 200 reference images used in PieAPP from the Waterloo Exploration Dataset with permissions for non-commercial, educational, use from the authors.
The users of the PieAPP dataset are requested to cite the Waterloo Exploration Dataset for the reference images, along with PieAPP dataset, as mentioned here.
The training + validation set contain a total of 160 reference images and test set contains 40 reference images.
A total of 19,680 distorted images are generated for the train/val set and pairwise probability of preference labels for 77,280 image pairs are made available (derived from querying 40 human subjects for a pairwise comparison + max-likelihood estimation of some missing pairs).
For test set, 15 distorted images per reference (total 600 distorted images) are created and all possible pairwise comparisons (total 4200) are performed to label each image pair with a probability of preference derived from 40 human subjects' votes.
Overall, the PieAPP dataset provides a total of 20,280 distorted images derived from 200 reference images, and 81,480 pairwise probability-of-preference labels.
More details of dataset collection can be found in Sec.4 of the paper and supplementary document."	https://paperswithcode.com/dataset/pieapp-dataset	06/06/2018	PieAPP dataset					
3284	AbstRCT - Neoplasm	"The AbstRCT dataset consists of randomized controlled trials retrieved from the MEDLINE database via PubMed search. The trials are annotated with argument components and argumentative relations.
Paper: Transformer-Based Argument Mining for Healthcare Applications"	https://paperswithcode.com/dataset/abstrct-neoplasm							
3285	CDCP	"The Cornell eRulemaking Corpus – CDCP is an argument mining corpus annotated with argumentative structure information capturing the evaluability of arguments. The corpus consists of 731 user comments on Consumer Debt Collection Practices (CDCP) rule by the Consumer Financial Protection Bureau (CFPB); the resulting dataset contains 4931 elementary unit and 1221 support relation annotations. It is a resource for building argument mining systems that can not only extract arguments from unstructured text, but also identify what additional information is necessary
for readers to understand and evaluate a given argument. Immediate applications include providing real-time feedback to commenters, specifying which types of support for which propositions can be added to construct better-formed arguments."	https://paperswithcode.com/dataset/cdcp	01/05/2018	Cornell eRulemaking Corpus					
3286	DRI Corpus	"The Dr. Inventor Multi-Layer Scientific Corpus (DRI Corpus) includes 40 Computer Graphics papers, selected by domain experts. Each paper of the Corpus has been annotated by three annotators by providing the following layers of annotations, each one characterizing a core aspect of scientific publications:

Scientific discourse: each sentence has been associated to a specific scientific discourse category (Background, Approach, Challenge, Future Work, etc.).
Subjective statements and novelty: each sentence has been characterized with respect to advantages, disadvantages and novel aspects presented.
Citation purpose: to each citation has been associated a purpose specifying the reason why the authors of the paper cited the specific piece of research.
Summary relevance of sentences and hand written summaries: each sentence of the paper has been characterized by an integer score ranging from 1 to 5, to point out the relevance of the same sentence for its inclusion in the summary of the paper. Sentences rated as 5 are the most relevant ones to summarize a paper. For each paper three hand-written summaries (max 250 words) are provided.

Source: Dr. Inventor Multi-layer Scientific Corpus"	https://paperswithcode.com/dataset/dr-inventor	01/05/2016	Dr. Inventor Multi-layer Scientific Corpus					
3287	PIPAL	PIPAL training set contains 200 reference images, 40 distortion types, 23k distortion images, and more than one million human ratings. Especially, we include GAN-based algorithms’ outputs as a new GAN-based distortion type. We employ the Elo rating system to assign the Mean Opinion Scores (MOS).	https://paperswithcode.com/dataset/pipal-perceptual-iqa-dataset	23/07/2020	Perceptual Image Processing ALgorithms IQA Dataset					
3288	PWDB	"Overview
This database of simulated arterial pulse waves is designed to be representative of a sample of pulse waves measured from healthy adults. It contains pulse waves for 4,374 virtual subjects, aged from 25-75 years old (in 10 year increments). The database contains a baseline set of pulse waves for each of the six age groups, created using cardiovascular properties (such as heart rate and arterial stiffness) which are representative of healthy subjects at each age group. It also contains 728 further virtual subjects at each age group, in which each of the cardiovascular properties are varied within normal ranges. This allows for extensive in silico analyses of haemodynamics and the performance of pulse wave analysis algorithms.
Data Description
The database contains the following pulse waves, sampled at 500 Hz:

arterial flow velocity (U),
luminal area (A),
pressure (P), and
photoplethysmogram (PPG).

These pulse waves are provided at a range of measurement sites, including:

aorta (ascending and descending)
carotid artery
brachial artery
radial artery
finger
femoral artery

The database also contains numerous reference variables, mostly relating to cardiovascular properties, such as:

heart rate
cardiac output
blood pressure
pulse wave velocity
age

The data are available in three formats: Matlab, CSV and WaveForm Database (WFDB) format. Further details of the formatting and contents of each file are available here.
Accompanying Publication
The database is described in the following publication:
Charlton P.H., Mariscal Harana, J., Vennin, S., Li, Y., Chowienczyk, P. & Alastruey, J., “Modelling arterial pulse waves in healthy ageing: a database for in silico evaluation of haemodynamics and pulse wave indices,” AJP Hear. Circ. Physiol., 317(5), pp.H1062-H1085, 2019. https://doi.org/10.1152/ajpheart.00218.2019
Please cite this publication when using the database.
Further Information
Further information on the Pulse Wave Database project can be found at the project homepage. In particular, an accompanying Wiki provides:

An introduction to the dataset here
The methods used to create and analyse the dataset here
An explanation of each of the variables in the dataset here
Case studies of analyses conducted on the dataset in Matlab here"	https://paperswithcode.com/dataset/pulse-wave-database	24/10/2019	Pulse Wave Database					
3289	ReCAM	"Tasks
Our shared task has three subtasks. Subtask 1 and 2 focus on evaluating machine learning models' performance with regard to two definitions of abstractness (Spreen and Schulz, 1966; Changizi, 2008), which we call imperceptibility and nonspecificity, respectively. Subtask 3 aims to provide some insights to their relationships.
• Subtask 1: ReCAM-Imperceptibility
Concrete words refer to things, events, and properties that we can perceive directly with our senses (Spreen and Schulz, 1966; Coltheart 1981; Turney et al., 2011), e.g., donut, trees, and red. In contrast, abstract words refer to ideas and concepts that are distant from immediate perception. Examples include objective, culture, and economy. In subtask 1, the participanting systems are required to perform reading comprehension of abstract meaning for imperceptible concepts.
Below is an example. Given a passage and a question, your model needs to choose from the five candidates the best one for replacing @placeholder.
• Subtask 2: ReCAM-Nonspecificity
Subtask 2 focuses on a different type of definition. Compared to concrete concepts like groundhog and whale, hypernyms such as vertebrate are regarded as more abstract (Changizi, 2008). 
• Subtask 3: ReCAM-Intersection
Subtask 3 aims to provide more insights to the relationship of the two views on abstractness, In this subtask, we test the performance of a system that is trained on one definition and evaluted on the other."	https://paperswithcode.com/dataset/recam	31/05/2021	SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning					
3290	VQA-E	"VQA-E is a dataset for Visual Question Answering with Explanation, where the models are required to generate and explanation with the predicted answer. The VQA-E dataset is automatically derived from the VQA v2 dataset by synthesizing a textual explanation for each image-question-answer triple.
Image Source: VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions"	https://paperswithcode.com/dataset/vqa-e	20/03/2018						
3291	RSPECT	"The RSNA Pulmonary Embolism CT (RSPECT) Dataset is composed of CT pulmonary angiogram images and annotations related to pulmonary embolism. It's part of the 2020 RSNA Pulmonary Embolism Detection Challenge which invited researchers to develop machine-learning algorithms to detect and characterize instances of pulmonary embolism (PE) on chest CT studies. The competition, conducted in collaboration with the Society of Thoracic Radiology (STR), involved creating the largest publicly available annotated PE dataset, comprised of more than 12,000 CT studies. Imaging data was contributed by five international research centers and labeled with detailed clinical annotations by a group of more than 80 expert thoracic radiologists. For the first time in an RSNA data challenge, the rules required competitors to submit and run their code in a standard shared environment, producing simpler, more readily usable models.
Source:"	https://paperswithcode.com/dataset/rspect		The RSNA Pulmonary Embolism CT					
3292	SEP-28k	"Stuttering Events in Podcasts (SEP-28k) is a dataset containing over 28k clips labeled with five event types including blocks, prolongations, sound repetitions, word repetitions, and interjections. Audio comes from public podcasts largely consisting of people who stutter interviewing other people who stutter. 
Source: Lea et al.
Image source: Lea et al."	https://paperswithcode.com/dataset/sep-28k	24/02/2021	Stuttering Events in Podcasts					
3293	FluencyBank	"FluencyBank is a shared database for the study of fluency development. Participants include typically-developing monolingual and bilingual children, children and adults who stutter (C/AWS) or who clutter (C/AWC), and second language learners.
Image Source: FluencyBank"	https://paperswithcode.com/dataset/fluencybank							
3294	MHIST	"The minimalist histopathology image analysis dataset (MHIST) is a binary classification dataset of 3,152 fixed-size images of colorectal polyps, each with a gold-standard label determined by the majority vote of seven board-certified gastrointestinal pathologists. MHIST also includes each image’s annotator agreement level. As a minimalist dataset, MHIST occupies less than 400 MB of disk space, and a ResNet-18 baseline can be trained to convergence on MHIST in just 6 minutes using approximately 3.5 GB of memory on a NVIDIA RTX 3090. As example use cases, the authors use MHIST to study natural questions that arise in histopathology image classification such as how dataset size, network depth, transfer learning, and high-disagreement examples affect model performance.
Source: Wei et al.
Image source: Wei et al."	https://paperswithcode.com/dataset/mhist	29/01/2021	Minimalist Histopathology image analysis dataset					
3295	CC-News	"CommonCrawl News is a dataset containing news articles from news sites all over the world. The dataset is available in form of Web ARChive (WARC) files that are released on a daily basis.
Source: https://commoncrawl.org/2016/10/news-dataset-available/"	https://paperswithcode.com/dataset/cc-news	04/10/2016	CommonCrawl News dataset					
3296	MalNet	"MalNet is a large public graph database, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families.
Image Source: Expore MalNet"	https://paperswithcode.com/dataset/malnet	16/11/2020						
3297	IBM-Rank-30k	"The IBM-Rank-30k is a dataset for the task of argument quality ranking. It is a corpus of 30,497 arguments carefully annotated for point-wise quality.
Image Source: A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis"	https://paperswithcode.com/dataset/ibm-rank-30k	26/11/2019	IBM-ArgQ-Rank-30kArgs					
3298	OTEANNv3	This dataset contains orthographic samples of words in 19 languages (ar, br, de, en, eno, ent, eo, es, fi, fr, fro, it, ko, nl, pt, ru, sh, tr, zh). Each sample contains two text features: a Word (the textual representation of the word according to its orthography) and a Pronunciation (the highest-surface IPA pronunciation of the word as pronunced in its language).	https://paperswithcode.com/dataset/oteannv3	31/12/2019						
3299	Maintenance of Wakefulness Test (MWT) recordings	"Maintenance of Wakefulness Test (MWT) is a dataset of recordings with microsleep episodes and drowsiness.
Cite as:
Hertig-Godeschalk Anneke, Skorucak Jelena, Malafeev Alexander, Achermann Peter, Mathis Johannes, & Schreier David R. (2019). Maintenance of Wakefulness Test (MWT) recordings (Version v1) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.325171
Each file contains a MWT trial (first trial after noon) recording of a patient. The data contains occipital EEG and EOG data. All signals were bandpass filtered between 0.5-45 Hz.
In each file, the data is structured as the following:
fs:   sampling rate.
eeg_O1:   EEG channel O1-M2 where M2 is the mastoid electrode on the opposite side.
eeg_O2:   EEG channel O2-M1 where M1 is the mastoid electrode on the opposite side.
E1 and E2:   EOG channels for left and right eye, both referenced to M1.
labels_O1 and labels_O2:   arrays with expert scoring (0-wake, 1-MSE, 2-MSEc, 3-ED, according to the BERN scoring criteria published in Hertig-Godeschalk et al. doi:10.1093/sleep/zsz163.); length of the arrays is the same as for other signals, i.e. there is a label per sample.
prec:   amount of signal samples per label, in this case it is 1. variables prec and half_prec were not used.
num_Labels:   length of the signal in samples.

Further descriptions, details, and outcomes can be found in the related studies. The published studies which are based on this data and address the borderland between wakefulness and sleep, i.e. microsleep episodes, are listed under related/alternative identifiers."	https://paperswithcode.com/dataset/maintenance-of-wakefulness-test-mwt	22/07/2020						
3300	darpa_sd2_perovskites	"Included in this content:

0045.perovksitedata.csv - main dataset used in this article.  A more detailed description can be found in the “dataset overview” section below
Chemical Inventory.csv - the hand curated file of all chemicals used in the construction of the perovskite dataset.  This file includes identifiers, chemical properties, and other information.
ExcessMolarVolumeData.xlsx - record of experimental data, computations, and final dataset used in the generation of the excess molar volume plots.
MLModelMetrics.xlsx - all of the ML metrics organized in one place (excludes reactant set specific breakdown, see ML_Logs.zip for those files).
OrganoammoniumDensityDataset.xlsx - complete set of the data used to generate the density values.  Example calculations included.
model_matchup_main.py - python pipeline used to generate all of the ML runs associated with the article.  More detailed instructions on the operation of this code is included in the “ML Code” Section below.  This file is also hosted on
GIT: https://github.com/ipendlet/MLScripts/blob/master/temp_densityconc/model_matchup_main_20191231.py 


SolutionVolumeDataset - complete set of 219 solutions in the perovskite dataset.  Tabs include the automatically generated reagent information from ESCALATE, hand curated reagent information from early runs, and the generation of the dataset used in the creation of Figure 5.
error_auditing.zip - code and historical datasets used for reporting the dataset auditing.
“AllCode.zip” which contains:
model_matchup_main_20191231.py - python pipeline used to generate all of the ML runs associated with the article.  More detailed instructions on the operation of this code is included in the “ML Code” Section below.  This file is also hosted on
GIT: https://github.com/ipendlet/MLScripts/blob/master/temp_densityconc/0045.perovskitedata.csv 
VmE_CurveFitandPlot.py - python code for generating the third order polynomial fit to the VmE vs mole fraction of FAH included in the main text. Requires the ‘MolFractionResults.csv’ to function (also included).
Calculation_Vm_Ve_CURVEFITTING.nb - mathematica code for generating the third order polynomial fit to the VmE vs mole fraction of FAH included in the main text.  
Covariance_Analysis.py - python code for ingesting and plotting the covariance of features and volumes in the perovskite dataset.  Includes renaming dictionaries used for the publication.
FeatureComparison_Plotting.py - python code for reading in and plotting features for the ‘GBT’ and ‘OHGBT’ folders in this directory.  The code parses the contents of these folders and generates feature comparison metrics used for Figure 9 and the associated Figure S8. Some assembly required.
Requirements.txt - all of the packages used in the generation of this paper
0045.perovskitedata.csv - the main dataset described throughout the article.  This file is required to run some of the code and is therefore kept near the code. 


“ML_Logs.zip” which contains:
A folder describing every model generated for this article.  In each folder there are a number of files:
Features_named_important.csv and features_value_importance.csv - these files are linked together and describe the weighted feature contributions from features (only present for GBT models)
AnalysisLog.txt - Log file of the run including all options, data curation and model training summaries        
LeaveOneOut_Summary.csv - Results of the leave-one-reactant set-out studies on the model (if performed)
LOOModelInfo.txt - Hyperparameter information for each model in the study (associated with the given dataset, sometimes includes duplicate runs).
STTSModelInfo.txt - Hyperparameter information for each model in the study (associated with the given dataset, sometimes includes duplicate runs).
StandardTestTrain_Summary.csv - Results of the 6 fold cross validation ML performance (for the hold out case)
LeaveOneOut_FullDataset_ByAmine.csv - Results of the leave-one-reactant set-out studies performed on the full dataset (all experiments) specified by reactant set (delineated by the amine)
LeaveOneOut_StratifiedData_ByAmine.csv -  Results of the leave-one-reactant set-out studies performed on a random stratified sample (96 random experiments) specified by reactant set (delineated by the amine)
model_matchup_main_*.py - code used to generate all of the runs contained in a particular folder.  The code is exactly what was used at run time to generate a given dataset (requires 0045.perovskitedata.csv file to run)."	https://paperswithcode.com/dataset/darpa-sd2-perovskites	26/05/2020	darpa_sd2_perovskites					
3301	Decagon	"Bio-decagon is a dataset for polypharmacy side effect identification problem framed as a multirelational link prediction problem in a two-layer multimodal graph/network of two node types: drugs and proteins. Protein-protein interaction
network describes relationships between proteins. Drug-drug interaction network contains 964 different types of edges (one for each side effect type) and describes which drug pairs lead to which side effects. Lastly,
drug-protein links describe the proteins targeted by a given drug.
The final network after linking entity vocabularies used by different databases has 645 drug and 19,085 protein nodes connected by 715,612 protein-protein, 4,651,131 drug-drug, and 18,596 drug-protein edges.
Source: Modeling polypharmacy side effects with graph convolutional networks
Image Source: Modeling polypharmacy side effects with graph convolutional networks"	https://paperswithcode.com/dataset/decagon	02/02/2018	Bio-decagon					
3302	TREC-10	"A question type classification dataset with 6 classes for questions about a person, location, numeric information, etc. The test split has 500 questions, and the training split has 5452 questions.
Paper: Learning Question Classifiers"	https://paperswithcode.com/dataset/trec-10	01/08/2002	TREC-10 Question Classification					
3303	Deep Thermal Imaging Dataset	"The Deep Thermal Imaging dataset consists of two main datasets:


DeepTherm I (Indoor materials) - 15 indoor materials were used to create the dataset DeepTherm I which consists of 14,860 processed thermal images (average count of data for each individual class: 990.7, SD=425.9; 400-600 images of each material per each variable). The dataset was created by recording thermal image sequences in a room with different lighting levels (bright / dark), with/without air-conditioning, different places (on a floor or a desk) and from different perspectives (Figure 5). The spatial temperature patterns were collected from different angles and different distances (between 10 and 50 cm, from the camera lens to the material). The data was collected five times in about 3 weeks. 


DeepTherm II (Outdoor materials) - 17 outdoor materials were targeted. The data collection process produced the DeepTherm II dataset which includes 26,584 labelled thermal images. The average number of collected spatial thermal patterns from each material was 1563.8 (SD=295.3; about 300-500 images of each material per each condition).


Image source: Cho et al.
Source: Cho et al."	https://paperswithcode.com/dataset/deep-thermal-imaging-dataset	06/03/2018						
3304	Fluent Speech Commands	"Fluent Speech Commands is an open source audio dataset for spoken language understanding (SLU) experiments. Each utterance is labeled with ""action"", ""object"", and ""location"" values; for example, ""turn the lights on in the kitchen"" has the label {""action"": ""activate"", ""object"": ""lights"", ""location"": ""kitchen""}. A model must predict each of these values, and a prediction for an utterance is deemed to be correct only if all values are correct. 
The task is very simple, but the dataset is large and flexible to allow for many types of experiments: for instance, one can vary the number of speakers, or remove all instances of a particular sentence and test whether a model trained on the remaining sentences can generalize."	https://paperswithcode.com/dataset/fluent-speech-commands	07/04/2019						
3305	Endotect Polyp Segmentation Challenge Dataset	"A challenge that consists of three tasks, each targeting a different requirement for in-clinic use. The first task involves classifying images from the GI tract into 23 distinct classes. The second task focuses on efficiant classification measured by the amount of time spent processing each image. The last task relates to automatcially segmenting polyps.
Source: EndoTect Challenge
Please cite ""The EndoTect 2020 Challenge: Evaluation andComparison of Classification, Segmentation and Inference Time for Endoscopy"" if you use the dataset."	https://paperswithcode.com/dataset/endotect-polyp-segmentation							
3306	Medico automatic polyp segmentation challenge (dataset)	"The “Medico automatic polyp segmentation challenge” aims to develop computer-aided diagnosis systems for automatic polyp segmentation to detect all types of polyps (for example, irregular polyp, smaller or flat polyps) with high efficiency and accuracy. The main goal of the challenge is to benchmark semantic segmentation algorithms on a publicly available dataset, emphasizing robustness, speed, and generalization.
Medico Multimedia Task at MediaEval 2020:Automatic Polyp Segmentation (https://arxiv.org/pdf/2012.15244.pdf)"	https://paperswithcode.com/dataset/medico-automatic-polyp-segmentation-challenge	30/12/2020						
3307	WIT	"Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal machine learning models.
Key Advantages
A few unique advantages of WIT:

The largest multimodal dataset (time of this writing) by the number of image-text examples.
A massively multilingual (first of its kind) with coverage for over 100+ languages.
A collection of diverse set of concepts and real world entities.
Brings forth challenging real-world test sets."	https://paperswithcode.com/dataset/wit	02/03/2021	Wikipedia-based Image Text					
3308	Unsplash Dataset	The Unsplash Dataset is created by over 200,000 contributing photographers and billions of searches across thousands of applications, uses, and contexts. It contains over 2M Unsplash images.	https://paperswithcode.com/dataset/unsplash-dataset	06/08/2020						
3309	IDRiD	Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.	https://paperswithcode.com/dataset/idrid	20/01/2018	Indian Diabetic Retinopathy Image Dataset					
3310	ReDWeb	The ReDWeb dataset consists of 3600 RGB-RD image pairs collected from the Web. This dataset covers a wide range of scenes and features various non-rigid objects.	https://paperswithcode.com/dataset/redweb	01/06/2018	Relative Depth from Web					
3311	HRWSI	The HRWSI dataset consists of about 21K diverse high-resolution RGB-D image pairs derived from the Web stereo images. Also, it provides sky segmentation masks, instance segmentation masks as well as invalid pixel masks.	https://paperswithcode.com/dataset/hrwsi	01/06/2020	High-Resolution Web Stereo Image					
3312	Fongbe  audio	"Fongbe Data collected by Fréjus A. A LALEYE
This dataset contains Fongbe speech corpus with audio data and transcriptions. 
Source: Fongbe dataset"	https://paperswithcode.com/dataset/fongbe-speech-recognition	21/01/2017	Fongbe dataset					
3313	DeepFluoroLabeling-IPCAI2020	This collection contains data and code associated with the IPCAI/IJCARS 2020 paper “Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and Efficient 2D/3D Registration.” The data hosted here consists of annotated datasets of actual hip fluoroscopy, CT and derived data from six lower torso cadaveric specimens. Documentation and examples for using the dataset and Python code for training and testing the proposed models are also included. Higher-level information, including clinical motivations, prior works, algorithmic details, applications to 2D/3D registration, and experimental details, may be found in the companion paper which is available at https://arxiv.org/abs/1911.07042 or https://doi.org/10.1007/s11548-020-02162-7. We hope that this code and data will be useful in the development of new computer-assisted capabilities that leverage fluoroscopy.	https://paperswithcode.com/dataset/deepfluorolabeling-ipcai2020	16/11/2019						
3314	Lens Flare Dataset	"The Lens Flare dataset is an internal dataset for Flare Spot detection used in the paper ""Automatic Flare Spot Artifact Detection and Removal in Photographs"" by Patricia Vitoria and Coloma Ballester.
The dataset consists of 405 natural images in which a minimum of one flare spot artifact appears. The sources of light can be the sun, light bulbs or specular surfaces, among others. The images have been captured by different cameras with different technical specifications."	https://paperswithcode.com/dataset/lens-flare-dataset	31/10/2018						
3315	SARA motion	"Sara motion is a 3D motion dataset, named Synthetic Actors and Real Actions (SARA), for training a model to produce motion embeddings suitable for reasoning about motion similarity. 
The motion sequence data for this dataset was generated by combining 18 different actors (i.e., action performing characters). The characters were rendered in a skeleton shape with Adobe Fuse software. Four action categories were selected (Combat, Adventure, Sport, and Dance) comprising a number of motion variations, where each action has a frame length of 32 or more. There are 4,428 base motions (e.g., dancing, jumping) in the SARA dataset."	https://paperswithcode.com/dataset/sara-motion	02/03/2021	Synthetic Actors and Real Actions					
3316	NTU RGB+D 120 motion similarity	Motion similarity annotations for NTU RGB+D 120 dataset to evaluate motion similarity in the real world.	https://paperswithcode.com/dataset/ntu-rgb-d-120-motion-similarity	02/03/2021						
3317	BU-BIL	"BU-BIL is an image library which includes six datasets that represent three imaging modalities and six object types. Providers of the datasets are instructed to choose images that capture the various environmental conditions and imaging noise that arose in their studies. These experts are asked to then select objects from those images that reflect the natural diversity of shape and appearances that these objects can exhibit. The image subregions containing the identified objects are cropped to create the image library. The outcome was a library with 305 objects from 235 images. Authors verify by visual inspection that the image library includes a variety of object appearances, backgrounds, and properties distinguishing objects from the background.
Paper: How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the Performance of Experts, Crowdsourced Non-Experts, and Algorithms
Image source: How to Collect Segmentations for Biomedical Images? A Benchmark Evaluating the Performance of Experts, Crowdsourced Non-Experts, and Algorithms"	https://paperswithcode.com/dataset/bu-bil		Boston University Biomedical Image Library					
3318	MTA-KDD'19	"Malware Traffic Analysis Knowledge Dataset 2019 (MTA-KDD'19) is an updated and refined dataset specifically tailored to train and evaluate machine learning based malware traffic analysis algorithms. To generate it, that authors started from the largest databases of network traffic captures available online, deriving a dataset with a set of widely-applicable features and then cleaning and preprocessing it to remove noise, handle missing data and keep its size as small as possible. The resulting dataset is not biased by any specific application (although specifically addressed to machine learning algorithms), and the entire process can run automatically to keep it updated.
Source: Letteri et al."	https://paperswithcode.com/dataset/mta-kdd-19	30/12/2020	Malware Traffic Analysis Knowledge Dataset 2019					
3319	Cuff-Less Blood Pressure Estimation	"Data Set Information:
The main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. The raw electrocardiogram (ECG), photoplethysmograph (PPG), and arterial blood pressure (ABP) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (For more information about the process please refer to our paper)
Attribute Information:
This database consists of a cell array of matrices, each cell is one record part. 
In each matrix each row corresponds to one signal channel:
1: PPG signal, FS=125Hz; photoplethysmograph from fingertip
2: ABP signal, FS=125Hz; invasive arterial blood pressure (mmHg)
3: ECG signal, FS=125Hz; electrocardiogram from channel II
Note: dataset is splitted to multiple parts to make it easier to load on machines with low memory. Each cell is a record. There might be more than one record per patient (which is not possible to distinguish). However, records of the same patient appear next to each other. N-fold cross test and train is suggested to reduce the chance of trainset being contaminated by test patients."	https://paperswithcode.com/dataset/cuff-less-blood-pressure-estimation	24/05/2015	Cuff-Less Blood Pressure Estimation. Pre-processed and cleaned vital signals for cuff-less BP estimation.					
3320	POTUS Corpus	"The POTUS Corpus is a Database of Weekly Addresses for the Study of Stance in Politics and Virtual Agents.
One of the main challenges in the field of Embodied Conversational Agent (ECA) is to generate socially believable agents. The common strategy for agent behaviour synthesis is to rely on dedicated corpus analysis. Such a corpus is composed of multimedia files of socio-emotional behaviors which have been annotated by external observers. The underlying idea is to identify interaction information for the agent’s socio-emotional behavior by checking whether the intended socio-emotional behavior is actually perceived by humans. Then, the annotations can be used as learning classes for machine learning algorithms applied to the social signals. This paper introduces the POTUS Corpus composed of high-quality audio-video files of political addresses to the American people. Two protagonists are present in this database. First, it includes speeches of former president Barack Obama to the American people. Secondly, it provides videos of these same speeches given by a virtual agent named Rodrigue. The ECA reproduces the original address as closely as possible using social signals automatically extracted from the original one. Both are annotated for social attitudes, providing information about the stance observed in each file. It also provides the social signals automatically extracted from Obama’s addresses used to generate Rodrigue’s ones."	https://paperswithcode.com/dataset/the-potus-copus	01/05/2020						
3321	ImageNet VIPriors subset	The training and validation data are subsets of the training split of the Imagenet 2012. The test set is taken from the validation split of the Imagenet 2012 dataset. Each data set includes 50 images per class.	https://paperswithcode.com/dataset/imagenet-vipriors-subset	05/03/2021						
3322	BiRD	"Bigram Relatedness Dataset (BiRD) is a large, fine-grained, bigram relatedness dataset, using a comparative annotation technique called Best Worst Scaling. Each of BiRD's 3,345 English term pairs involves at least one bigram. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed.
Image source: http://saifmohammad.com/WebPages/BiRD.html"	https://paperswithcode.com/dataset/big-bird	01/06/2019	Bigram Relatedness Dataset					
3323	Shiny dataset	"The shiny folder contains 8 scenes with challenging view-dependent effects used in our paper. We also provide additional scenes in the shiny_extended folder. 
The test images for each scene used in our paper consist of one of every eight images in alphabetical order.
Each scene contains the following directory structure:
scene/
    dense/
      cameras.bin
      images.bin
      points3D.bin
      project.ini
    images/
      image_name1.png
      image_name2.png
      ...
      image_nameN.png
    images_distort/
      image_name1.png
      image_name2.png
      ...
      image_nameN.png
    sparse/
      cameras.bin
      images.bin
      points3D.bin
      project.ini
    database.db
    hwf_cxcy.npy
    planes.txt
    poses_bounds.npy

dense/ folder contains COLMAP's output 1 after the input images are undistorted.
images/ folder contains undistorted images. (We use these images in our experiments.)
images_distort/ folder contains raw images taken from a smartphone.
sparse/ folder contains COLMAP's sparse reconstruction output 1.

Our poses_bounds.npy is similar to the LLFF2 file format with a slight modification. This file stores a Nx14 numpy array, where N is the number of cameras. Each row in this array is split into two parts of sizes 12 and 2. The first part, when reshaped into 3x4, represents the camera extrinsic (camera-to-world transformation), and the second part with two dimensions stores the distances from that point of view to the first and last planes (near, far). These distances are computed automatically based on the scene’s statistics using LLFF’s code. (For details on how these are computed, see this code) 
hwf_cxcy.npy stores the camera intrinsic (height, width, focal length, principal point x, principal point y) in a 1x5 numpy array.
planes.txt stores information about the MPI planes. The first two numbers are the distances from a reference camera to the first and last planes (near, far). The third number tells whether the planes are placed equidistantly in the depth space (0) or inverse depth space (1). The last number is the padding size in pixels on all four sides of each of the MPI planes. I.e., the total dimension of each plane is (H + 2 * padding, W + 2 * padding).
References:

1: COLMAP structure from motion (Schönberger and Frahm, 2016).
2: Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines (Mildenhall et al., 2019)."	https://paperswithcode.com/dataset/shiny-dataset	09/03/2021						
3324	MATH	"MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.
Source: Hendrycks et al.
Image source: Hendrycks et al."	https://paperswithcode.com/dataset/math	05/03/2021						
3325	PhysioNet Challenge 2016	"Introduction
The 2016 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify heart sound recordings collected from a variety of clinical or nonclinical (such as in-home visits) environments. The aim is to identify, from a single short recording (10-60s) from a single precordial location, whether the subject of the recording should be referred on for an expert diagnosis.
During the cardiac cycle, the heart firstly generates the electrical activity and then the electrical activity causes atrial and ventricular contractions. This in turn forces blood between the chambers of the heart and around the body. The opening and closure of the heart valves is associated with accelerations-decelerations of blood, giving rise to vibrations of the entire cardiac structure (the heart sounds and murmurs) 1. These vibrations are audible at the chest wall, and listening for specific heart sounds can give an indication of the health of the heart. The phonocardiogram (PCG) is the graphical representation of a heart sound recording. Figure 1 illustrates a short section of a PCG recording."	https://paperswithcode.com/dataset/physionet-challenge-2016	04/03/2016						
3326	IXI	"IXI Dataset is a collection of 600 MR brain images from normal, healthy subjects. The MR image acquisition protocol for each subject includes:

T1, T2 and PD-weighted images
MRA images
Diffusion-weighted images (15 directions)

The data has been collected at three different hospitals in London:

Hammersmith Hospital using a Philips 3T system (details of scanner parameters)
Guy’s Hospital using a Philips 1.5T system (details of scanner parameters)
Institute of Psychiatry using a GE 1.5T system (details of the scan parameters not available at the moment)

The data has been collected as part of the project:

IXI – Information eXtraction from Images (EPSRC GR/S21533/02)

The images in NIFTI format can be downloaded from here:
This data is made available under the Creative Commons CC BY-SA 3.0 license. If you use the IXI data please acknowledge the source of the IXI data."	https://paperswithcode.com/dataset/ixi-dataset		IXI Brain Development Dataset					
3327	LIFULL HOME'S	"The National Institute of Informatics provides LIFULL HOME'S Dataset to researchers, which was offered by LIFULL Co., Ltd. for promoting research in informatics and the related fields.
The dataset contains the data of LIFULL HOME'S, a Real Estate Information Service in Japan.


Snapshot Data of Rentals (snapshot of 2015-09)
Rental data (5.33 million all over Japan): rental fee, area, location, age, floor plan, structure, facilities, etc.; approx. 1.6GB .tsv format files.
Image data (83 million files): floor plan image, room view, etc. of all the above items; approx. 210GB .jpg format files, max size: 120x120.


High Resolution Floor Plan Image Data
High resolution version data of floor plan image (5.31 million files) included in Snapshot Data of Rentals; approx. 140GB .jpg format files. Additional application required to use this data (see Application section below).


Monthly Data of Rentals and Sales (2015-07 - 2017-06, 24 months)
Property data of rentals and sales (5.33 million all over Japan): rental fee/price, area, location, age, floor plan, structure, facilities, etc.; approx. 1.7 - 4.5GB .tsv format files for respective months.
In addition, LIFULL Co., Ltd. provides a sample script for classifying image types via Github:
https://github.com/Littel-Laboratory/homes-dataset-tools"	https://paperswithcode.com/dataset/lifull-home-s	07/09/2020						
3328	CosmoFlow	The latest CosmoFlow dataset includes around 10,000 cosmological N-body dark matter simulations. The simulations are run using MUSIC to generate the initial conditions, and are evolved with pyCOLA, a multithreaded Python/Cython N-body code. The output of these simulations is then binned into a 3D histogram of particle counts in a cube of size 512x512x512, which is sampled at 4 different redshifts.	https://paperswithcode.com/dataset/cosmoflow	14/08/2018						
3329	Sketch2aia (Mobile User Interface Sketches)	"Dataset of 374 photos of hand-drawn sketches of App Inventor apps used for development of the Sketch2aia model for automatic generation of App Inventor wireframes from hand-drawn sketches.
Data format
Training:2 37 images in JPG (.jpg) format with 720×1280 pixels, each accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), used to train the Sketch2aia model.
Validation: 42 images in JPG (.jpg) format with 720×1280 pixels, each accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), used to test the Sketch2aia model.
Additional Images: 95 images in JPG (.jpg) format with 720×1280 pixels. Some images are accompanied by a JSON (.json) file with manually attributed bounding box annotation for 10 different classes of UI elements (Screen, Label, Button, Switch, Slider, TextBox, CheckBox, ListPicker, Image and Map), while others have not yet been labeled. This portion of the dataset was collected during user evaluation of the Sketch2aia model, and have not been directly used to train or test the object detection model."	https://paperswithcode.com/dataset/sketch2aia-mobile-user-interface-sketches	09/03/2021						
3330	An Amharic News Text classification Dataset	In NLP, text classification is one of the primary problems we try to solve and its uses in language analyses are indisputable. The lack of labeled training data made it harder to do these tasks in low resource languages like Amharic. The task of collecting, labeling, annotating, and making valuable this kind of data will encourage junior researchers, schools, and machine learning practitioners to implement existing classification models in their language. In this short paper, we aim to introduce the Amharic text classification dataset that consists of more than 50k news articles that were categorized into 6 classes. This dataset is made available with easy baseline performances to encourage studies and better performance experiments.	https://paperswithcode.com/dataset/an-amharic-news-text-classification-dataset	10/03/2021						
3331	PHOENIX14T	"Over a period of three years (2009 - 2011) the daily news and weather forecast airings of the German public tv-station PHOENIX featuring sign language interpretation have been recorded and the weather forecasts of a subset of 386 editions have been transcribed using gloss notation. Furthermore, we used automatic speech recognition with manual cleaning to transcribe the original German speech. As such, this corpus allows to train end-to-end sign language translation systems from sign language video input to spoken language.
The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only."	https://paperswithcode.com/dataset/phoenix14t	01/06/2018	RWTH-PHOENIX-Weather-2014T					
3332	CUAD	"Contract Understanding Atticus Dataset (CUAD) is a dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project
and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review."	https://paperswithcode.com/dataset/cuad	10/03/2021	Contract Understanding Atticus Dataset					
3333	BIKED	BIKED is a dataset comprised of 4500 individually designed bicycle models sourced from hundreds of designers. BIKED enables a variety of data-driven design applications for bicycles and generally supports the development of data-driven design methods. The dataset is comprised of a variety of design information including assembly images, component images, numerical design parameters, and class labels.	https://paperswithcode.com/dataset/biked	10/03/2021	BIKED					
3334	THEOStereo	"THEOStereo is a dataset providing synthetic stereo image pairs and their corresponding scene depth and will be published along with 1. All images follow the omnidirectional camera model. In total, there are 31,250 omnidirectional images pairs. The training set contains 25,000 image pairs. For validation and testing there are 3,125 image pairs, respectively. For each pair, there is a ground truth depth map describing the pixel-wise distance of the object along the left camera's z-axis. The virtual omnidirectional cameras exhibit a FOV of 180 degrees and can be described using Kannala's camera model 2. The distortion parameters are k_1 = 1 and k_2 = k_3 = k_4 = k_5 = 0. The baseline of the stereo camera was 0.3 m. Please do not forget to cite 1 if you use the dataset in your work. Thank you.
Structure of the Dataset
.
├── README.md
├── test
│   ├── depth_exr_abs
│   ├── img_stereo_webp
│   └── img_webp
├── train
│   ├── depth_exr_abs
│   ├── img_stereo_webp
│   └── img_webp
└── valid
    ├── depth_exr_abs
    ├── img_stereo_webp
    └── img_webp
The directory depth_exr_abs contain the depth maps given in meters. The depth reference to the image of the left camera. All images of the left camera are stored in the img_webp. The right camera's images can be found in img_stereo_webp.
License
This dataset is licensed under CC BY 4.0.
For details, please visit https://creativecommons.org/licenses/by/4.0/.

Conference paper
The conference paper can be downloaded from here.
BibTex
If you use the dataset in your work, we would kindly ask you to cite 1.
You might want to use the following BibTex entry:
bibtex
@inproceedings{seuffert_study_2021,
    address = {Online Conference},
    title = {A {Study} on the {Influence} of {Omnidirectional} {Distortion} on {CNN}-based {Stereo} {Vision}},
    isbn = {978-989-758-488-6},
    doi = {10.5220/0010324808090816},
    booktitle = {Proceedings of the 16th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}, {VISIGRAPP} 2021, {Volume} 5: {VISAPP}},
    publisher = {SciTePress},
    author = {Seuffert, Julian Bruno and Perez Grassi, Ana Cecilia and Scheck, Tobias and Hirtz, Gangolf},
    year = {2021},
    month = {2},
    pages = {809--816}
}
References
1 J. B. Seuffert, A. C. Perez Grassi, T. Scheck, and G. Hirtz, “A Study on the Influence of Omnidirectional Distortion on CNN-based Stereo Vision,” in Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021, Volume 5: VISAPP, Online Conference, Feb. 2021, pp. 809–816, doi: 10.5220/0010324808090816.
2 J. Kannala, J. Heikkilä, and S. S. Brandt, “Geometric Camera Calibration,” in Wiley Encyclopedia of Computer Science
and Engineering, B. W. Wah, Ed. Hoboken, NJ, USA: John Wiley & Sons, Inc., 2008."	https://paperswithcode.com/dataset/theostereo	01/02/2021						
3335	PCD	The Arabic dataset is scraped mainly from الموسوعة الشعرية and الديوان. After merging both, the total number of verses is 1,831,770 poetic verses. Each verse is labeled by its meter, the poet who wrote it, and the age which it was written in. There are 22 meters, 3701 poets and 11 ages: Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman, Andalusian, era between Umayyad and Abbasid, Fatimid, and finally the modern age. We are only interested in the 16 classic meters which are attributed to Al-Farahidi, and they comprise the majority of the dataset with a total number around 1.7M verses. It is important to note that the verses diacritic states are not consistent. This means that a verse can carry full, semi diacritics, or it can carry nothing.	https://paperswithcode.com/dataset/pcd	07/05/2019	Poem Comprehensive Dataset					
3336	ARCH	"ARCH is a computational pathology (CP) multiple instance captioning dataset to facilitate dense supervision of CP tasks. Existing CP datasets focus on narrow tasks; ARCH on the other hand contains dense diagnostic and morphological descriptions for a range of stains, tissue types and pathologies. 
Source: Gamper et al.
Image source: Gamper et al."	https://paperswithcode.com/dataset/arch	08/03/2021						
3337	UASOL	"The UASOL an RGB-D stereo dataset, that contains 160902 frames, filmed at 33 different scenes, each with between 2 k and 10 k frames. The frames show different paths from the perspective of a pedestrian, including sidewalks, trails, roads, etc. The images were extracted from video files with 15 fps at HD2K resolution with a size of 2280 × 1282 pixels. The dataset also provides a GPS geolocalization tag for each second of the sequences and reflects different climatological conditions. It also involved up to 4 different persons filming the dataset at different moments of the day.
We propose a train, validation and test split to train the network. 
Additionally, we introduce a subset of 676 pairs of RGB Stereo images and their respective depth, which we extracted randomly from the entire dataset. This given test set is introduced to make comparability possible between the different methods trained with the dataset."	https://paperswithcode.com/dataset/uasol	29/08/2019	A large-scale high-resolution outdoor stereo dataset					
3338	SUM	"SUM is a new benchmark dataset of semantic urban meshes which covers about 4 km2 in Helsinki (Finland), with six classes: Ground, Vegetation, Building, Water, Vehicle, and Boat.
The authors used Helsinki 3D textured meshes as input and annotated them as a benchmark dataset of semantic urban meshes. The Helsinki's raw dataset covers about 12 km2 and was generated in 2017 from oblique aerial images that have about a 7.5 cm ground sampling distance (GSD) using an off-the-shelf commercial software namely ContextCapture.
The entire region of Helsinki is split into tiles, and each of them covers about 250 m2. 
Image source: Gao et al."	https://paperswithcode.com/dataset/sum	27/02/2021						
3339	BLURB	"BLURB is a collection of resources for biomedical natural language processing. In general domains such as newswire and the Web, comprehensive benchmarks and leaderboards such as GLUE have greatly accelerated progress in open-domain NLP. In biomedicine, however, such resources are ostensibly scarce. In the past, there have been a plethora of shared tasks in biomedical NLP, such as BioCreative, BioNLP Shared Tasks, SemEval, and BioASQ, to name just a few. These efforts have played a significant role in fueling interest and progress by the research community, but they typically focus on individual tasks. The advent of neural language models such as BERTs provides a unifying foundation to leverage transfer learning from unlabeled text to support a wide range of NLP applications. To accelerate progress in biomedical pretraining strategies and task-specific methods, it is thus imperative to create a broad-coverage benchmark encompassing diverse biomedical tasks.
Inspired by prior efforts toward this direction (e.g., BLUE), BLURB (short for Biomedical Language Understanding and Reasoning Benchmark) was created. BLURB comprises of a comprehensive benchmark for PubMed-based biomedical NLP applications, as well as a leaderboard for tracking progress by the community. BLURB includes thirteen publicly available datasets in six diverse tasks. To avoid placing undue emphasis on tasks with many available datasets, such as named entity recognition (NER), BLURB reports the macro average across all tasks as the main score. The BLURB leaderboard is model-agnostic. Any system capable of producing the test predictions using the same training and development data can participate. The main goal of BLURB is to lower the entry barrier in biomedical NLP and help accelerate progress in this vitally important field for positive societal and human impact.
Source: BLURB
Image source: BLURB"	https://paperswithcode.com/dataset/blurb	31/07/2020	Biomedical Language Understanding and Reasoning Benchmark					
3340	GAD	GAD, or Gene Associations Database, is a corpus of gene-disease associations curated from genetic association studies.	https://paperswithcode.com/dataset/gad		Gene Associations Database					
3341	BC2GM	Created by Smith et al. at 2008, the BioCreative II Gene Mention Recognition (BC2GM) Dataset contains data where participants are asked to identify a gene mention in a sentence by giving its start and end characters. The training set consists of a set of sentences, and for each sentence a set of gene mentions (GENE annotations). [registration required for access], in English language. Containing 20 in n/a file format.	https://paperswithcode.com/dataset/bc2gm		BC2GM					
3342	Kaggle EyePACS	"Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people.
retina
The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment.
Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment.
Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient.
The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible – ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection.
Acknowledgements
This competition is sponsored by the California Healthcare Foundation.

Retinal images were provided by EyePACS, a free platform for retinopathy screening."	https://paperswithcode.com/dataset/kaggle-eyepacs	17/02/2015	Kaggle EyePACS. Diabetic Retinopathy Detection Identify signs of diabetic retinopathy in eye images					
3343	THFOOD-50	"Fine-Grained Thai Food Image Classification Datasets
THFOOD-50 containing 15,770 images of 50 famous Thai dishes."	https://paperswithcode.com/dataset/thfood-50	01/08/2017	Thai Food 50 Image Classification					
3344	SRD	SRD is a dataset for shadow removal that contains 3088 shadow and shadow-free image pairs.	https://paperswithcode.com/dataset/srd	01/07/2017	Shadow Removal Dataset					
3345	BL30K	BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. The videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories in a greedy fashion to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See MiVOS for details.	https://paperswithcode.com/dataset/bl30k	14/03/2021						
3346	BIG	A high-resolution semantic segmentation dataset with 50 validation and 100 test objects. Image resolution in BIG ranges from 2048×1600 to 5000×3600. Every image in the dataset has been carefully labeled by a professional while keeping the same guidelines as PASCAL VOC 2012 without the void region.	https://paperswithcode.com/dataset/big	06/05/2020						
3347	COCO Object Detection VIPriors subset	The training and validation data are subsets of the training split of the MS COCO dataset (2017 release, bounding boxes only). The test set is taken from the validation split of the MS COCO dataset.	https://paperswithcode.com/dataset/coco-object-detection-vipriors-subset	05/03/2021						
3348	Cityscapes VIPriors subset	The training and validation data are subsets of the training split of the Cityscapes dataset. The test set is taken from the validation split of the Cityscapes dataset.	https://paperswithcode.com/dataset/cityscapes-vipriors-subset	05/03/2021						
3349	UCF-101 VIPriors subset	"The VIriors Action Recognition Challenge uses a subset of the UCF101 action recognition dataset:
Train set: ~4.8K clips.
Validation set: ~4.7K clips.
Test set: ~3.8K clips."	https://paperswithcode.com/dataset/ucf-101-vipriors-subset	05/03/2021						
3350	CLEVR-Hans	"The CLEVR-Hans data set is a novel confounded visual scene data set, which captures complex compositions of different objects. This data set consists of CLEVR images divided into several classes. 
The membership of a class is based on combinations of objects’ attributes and relations. Additionally, certain classes within the data set are confounded. Thus, within the data set, consisting of train, validation, and test splits, all train, and validation images of confounded classes will be confounded with a specific attribute or combination of attributes.
Each class is represented by 3000 training images, 750 validation images, and 750 test images. The training, validation, and test set splits contain 9000, 2250, and 2250 samples, respectively, for CLEVR-Hans3 and 21000, 5250, and 5250 samples for CLEVR-Hans7. The class distribution is balanced for all data splits.
For CLEVR-Hans classes for which class rules contain more than three objects, the number of objects to be placed per scene was randomly chosen between the minimal required number of objects for that class and ten, rather than between three and ten, as in the original CLEVR data set.
Finally, the images were created such that the exact combinations of the class rules did not occur in images of other classes. It is possible that a subset of objects from one class rule occur in an image of another class. However, it is not possible that more than one complete class rule is contained in an image.
Source: CLEVR-Hans
Image Source: CLEVR-Hans"	https://paperswithcode.com/dataset/clevr-hans	25/11/2020						
3351	Tsinghua Dogs	Tsinghua Dogs is a fine-grained classification dataset for dogs, over 65% of whose images are collected from people's real life. Each dog breed in the dataset contains at least 200 images and a maximum of 7,449 images, basically in proportion to their frequency of occurrence in China, so it significantly increases the diversity for each breed over existing dataset. Furthermore, Tsinghua Dogs annotated bounding boxes of the dog’s whole body and head in each image, which can be used for supervising the training of learning algorithms as well as testing them.	https://paperswithcode.com/dataset/tsinghua-dogs	01/10/2020						
3352	ADAM	"ADAM is organized as a half day Challenge, a Satellite Event of the ISBI 2020 conference in Iowa City, Iowa, USA.
The ADAM challenge focuses on the investigation and development of algorithms associated with the diagnosis of Age-related Macular degeneration (AMD) and segmentation of lesions in fundus photos from AMD patients. The goal of the challenge is to evaluate and compare automated algorithms for the detection of AMD on a common dataset of retinal fundus images. We invite the medical image analysis community to participate by developing and testing existing and novel automated fundus classification and segmentation methods.
Instructions: 
ADAM: Automatic Detection challenge on Age-related Macular degeneration
Link: https://amd.grand-challenge.org
Age-related macular degeneration, abbreviated as AMD, is a degenerative disorder in the macular region. It mainly occurs in people older than 45 years old and its incidence rate is even higher than diabetic retinopathy in the elderly.  
The etiology of AMD is not fully understood, which could be related to multiple factors, including genetics, chronic photodestruction effect, and nutritional disorder. AMD is classified into Dry AMD and Wet AMD. Dry AMD (also called nonexudative AMD) is not neovascular. It is characterized by progressive atrophy of retinal pigment epithelium (RPE). In the late stage, drusen and the large area of atrophy could be observed under ophthalmoscopy. Wet AMD (also called neovascular or exudative AMD), is characterized by active neovascularization under RPE, subsequently causing exudation, hemorrhage, and scarring, and will eventually cause irreversible damage to the photoreceptors and rapid vision loss if left untreated.
An early diagnosis of AMD is crucial to treatment and prognosis. Fundus photo is one of the basic examinations. The current dataset is composed of AMD and non-AMD (myopia, normal control, etc.) photos. Typical signs of AMD that can be found in these photos include drusen, exudation, hemorrhage, etc. 
The ADAM challenge has 4 tasks:
Task 1: Classification of AMD and non-AMD fundus images.
Task 2: Detection and segmentation of optic disc.
Task 3: Localization of fovea.
Task 4: Detection and Segmentation of lesions from fundus images."	https://paperswithcode.com/dataset/adam	16/02/2022	Adam: automatic detection challenge on age-related macular degeneration					
3353	DiCOVA	"The DiCOVA Challenge dataset is derived from the Coswara dataset, a crowd-sourced dataset of sound recordings from COVID-19 positive and non-COVID-19 individuals. The Coswara data is collected using a web-application2, launched in April-2020, accessible through the internet by anyone around the globe. The volunteering subjects are advised to record their respiratory sounds in a quiet environment. 
Each subject provides 9 audio recordings, namely, (a) shallow and deep breathing (2 nos.), (b) shallow and heavy cough (2 nos.), (c) sustained phonation of vowels [æ] (as in bat), [i] (as in beet), and [u] (as in boot) (3 nos.), and (d) fast and normal pace 1 to 20 number counting (2 nos.). 
The DiCOVA Challenge has two tracks. The participants also provided metadata corresponding to their current health status (includes COVID19 status, any other respiratory ailments, and symptoms), demographic information, age and gender. From this Coswara dataset, two datasets have been created: 
(a) Track-1 dataset: composed of cough sound recordings. It t is composed of cough audio data from 1040 subjects.
(b) Track-2 dataset: composed of deep breathing, vowel [i], and number counting (normal pace) speech recordings. It is composed of audio data from 1199 subjects."	https://paperswithcode.com/dataset/dicova	16/03/2021						
3354	Digital Peter	"Digital Peter is a dataset of Peter the Great's manuscripts annotated for segmentation and text recognition. The dataset may be useful for researchers to train handwriting text recognition models as a benchmark for comparing different models. It consists of 9,694 images and text files corresponding to lines in historical documents. The dataset includes Peter’s handwritten materials covering the period from 1709 to 1713. 
The open machine learning competition Digital Peter was held based on the considered dataset."	https://paperswithcode.com/dataset/digital-peter	16/03/2021						
3355	OGB-LSC	"OGB Large-Scale Challenge (OGB-LSC) is a collection of three real-world datasets for advancing the state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that are orders of magnitude larger than existing ones and covers three core graph learning tasks -- link prediction, graph regression, and node classification. 
OGB-LSC consists of three datasets: MAG240M-LSC, WikiKG90M-LSC, and PCQM4M-LSC. Each dataset offers an independent task.

MAG240M-LSC is a heterogeneous academic graph, and the task is to predict the subject areas of papers situated in the heterogeneous graph (node classification).
WikiKG90M-LSC is a knowledge graph, and the task is to impute missing triplets (link prediction).
PCQM4M-LSC is a quantum chemistry dataset, and the task is to predict an important molecular property, the HOMO-LUMO gap, of a given molecule (graph regression)."	https://paperswithcode.com/dataset/ogb-lsc	17/03/2021	OGB Large-Scale Challenge					
3356	TeachMyAgent	TeachMyAgent (TA) is a benchmark for Automatic Curriculum Learning (ACL) algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment.	https://paperswithcode.com/dataset/teachmyagent	17/03/2021						
3357	L1000	"The L1000 dataset consists of ~1,400,000 gene-expression profiles on the responses of ~50 human cell lines to one of ~20,000 compounds across a range of concentrations. The L1000 dataset and its normalization versions10 were recently widely used in drug repurposing and discovery.
Description from: A deep learning framework for high-throughput mechanism-driven phenotype compound screening and its application to COVID-19 drug repurposing
Publication introducing the dataset: A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles"	https://paperswithcode.com/dataset/l1000							
3358	DSBEC	"The data set consists of 6257 labeled images of Bose-Einstein condensates (BECs) with and without solitonic excitations, including kink solitons and solitonic vortices. Each element of the data set contains a masked image (132x164 pixels) of 2D atomic density used to train the machine learning model used in the paper ""Machine-learning enhanced dark soliton detection in Bose-Einstein condensates,"" (https://arxiv.org/abs/2101.05404), and a label indicating the class a given image belongs to (0 indicates no solitons, 1 indicates a single soliton, and 2 indicates other excitations). The data structure file and project description are included with the data.
This data set was used to train a deep convolutional neural network to automatically recognize whether or not a lone dark soliton has been created in BECs that was then implemented within an automated soliton detection and positioning system (see https://arxiv.org/abs/2101.05404 for details)."	https://paperswithcode.com/dataset/dsbec	14/01/2021	Dark solitons in BECs dataset					
3359	VESSEL12		https://paperswithcode.com/dataset/vessel12		VESsel SEgmentation in the Lung 2012					
3360	ConScenD	The ConScenD dataset consists of over 340 scenarios extracted from the naturalistic highway dataset highD. This scenarios can be used to test for the introduction of Level 3 Automated Lane Keeping Systems according to the UNECE R157 ALKS Regulation.	https://paperswithcode.com/dataset/conscend	17/03/2021						
3361	LDC2020T02		https://paperswithcode.com/dataset/ldc2020t02		LDC2020T02 AMR					
3362	KoDF	"The Korean DeepFake Detection Dataset (KoDF) is a large-scale collection of synthesized and real videos focused on Korean subjects, used for the task of deepfake detection.
The dataset consists of 62,166 real videos and 175,776 fake videos from 403 subjects. The fake videos are created using 6 different methods: FaceSwap, DeepFaceLab, FSGAN, FOMM, ATFHP and Wav2Lip."	https://paperswithcode.com/dataset/kodf	18/03/2021	Korean DeepFake Detection Dataset					
3363	HDA Facial Tattoo and Painting Database	The Hochschule Darmstadt (HDA) facial tattoo and paintings database contains 500 pairs of facial images of individuals with and without facial tattoos or paintings. The database was collected from multiple online sources.	https://paperswithcode.com/dataset/hda-facial-tattoo-and-painting-database	17/03/2021						
3364	Gowalla	Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010.	https://paperswithcode.com/dataset/gowalla	21/08/2011	Gowalla					
3365	DODa	"Darija Open Dataset (DODa) is an open-source project for the Moroccan dialect. With more than 10,000 entries DODa is arguably the largest open-source collaborative project for Darija-English translation built for Natural Language Processing purposes. In fact, besides semantic categorization, DODa also adopts a syntactic one, presents words under different spellings, offers verb-to-noun and masculine-to-feminine correspondences, contains the conjugation of hundreds of verbs in different tenses, and many other subsets to help researchers better understand and study Moroccan dialect. 
Source: Moroccan Dialect -Darija- Open Dataset
Image source: Moroccan Dialect -Darija- Open Dataset"	https://paperswithcode.com/dataset/doda	28/02/2021	Darija Open Dataset					
3366	LeT-Mi	"Levantine Twitter dataset for Misogynistic language (LeT-Mi) is an Arabic Levantine Twitter dataset for misogynistic language to be the first benchmark dataset for Arabic misogyny.
⚠️ Note: To be made publicly available on Github
Source: Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language
Image source: Let-Mi: An Arabic Levantine Twitter Dataset for Misogynistic Language"	https://paperswithcode.com/dataset/let-mi	18/03/2021	Levantine Twitter dataset for Misogynistic language					
3367	SVT	"The Street View Text (SVT) dataset was harvested from Google Street View. Image text in this data exhibits high variability and often has low resolution. In dealing with outdoor street level imagery, we note two characteristics. (1) Image text often comes from business signage and (2) business names are easily available through geographic business searches. These factors make the SVT set uniquely suited for word spotting in the wild: given a street view image, the goal is to identify words from nearby businesses.
Note: the dataset has undergone revision since the time it was evaluated in this publication. Please consult the ICCV2011 paper for most up-to-date results.
Source: Street View Text Dataset
Image source: Street View Text Dataset"	https://paperswithcode.com/dataset/svt		Street View Text Dataset					
3368	RETWEET	"RETWEET is a dataset of tweets and overall predominant sentiment of their replies.
SUMMARY
WHAT: Message-level Polarity Classification.
GOAL: To predict the predominant sentiment among (potential) first-order replies to a given tweet.
IDEA: Mitigate the problem of lacking labeled training data wi treating the unsupervised nature of the problem as a supervised learning case.
APPROACH:

Train a tweet classifier. 
Automatically label the replies using the classifier trained in the first part.
Choose a final label representing the general predominant sentiment of the replies of every tweet.

DATA COLLECTION
To download all of the replies to a tweet, the Search API should be used. However, the Search API is limited to 75000 requests per hour, which causes the mining and downloading process to be slow.
Furthermore, using the Twitter API, there is no possibility of downloading absolute random data. Therefore, we try to make the procedure as random as possible by utilizing two different strategies for data downloading and using them in an intermixed manner.


Our first strategy is based on a sample of English tweets obtained by filtering the Twitter stream via a list of cultural keywords. This list consists of 147 words that are deemed to play a ""pivotal role in discussions of culture and society"", covering diverse words such as aesthetics, environment, feminism, power, tourism, or youth. We extracted all tweets in 2019 that have a minimum of 20 first-order replies in the dataset. The data come with an obvious caveat: Both the source tweet as well as all the replies must contain at least one word from the list of keywords. Therewith, it is highly unlikely that the list of replies for any given source is exhaustive, i.e. there might be many more first-order replies to the source tweet that are not in the dataset.


As our second approach, we use the GetOldTweets3 library to download all the replies corresponding to every tweet. We define few restrictions to add randomization to the process. Firstly, every tweet and also every reply should contain at least 20 strings. This is due to the fact that our automatic tweet classifier, explsined in the paper, is optimized based on the message-level classification paradigm. Therefore, it operates optimal when the input contains at least a sufficient number of words. The second constraint is that every tweet should contain at least 20 first-order replies. In order to increase randomness, in this strategy, instead of referencing to a list of keywords, we manually choose some keywords, which are most likely to include long discussions, such as Coronavirus and football or the ones, which are most likely to include strong opinions such as birthday, war, or racism in order to account for the easy-to-guess examples. 


MANUAL ANNOTATIONS FOR THE RETWEET (TEST GOLD DATASET)
5,015 tweets with their corresponding replies, collected as a combination of the two different collection strategies, were given to three different students. Each of them had to read all the replies corresponding to every tweet, without observing the original tweet in order to avoid having a prior knowledge, and decide on ONE final sentiment for the replies. The assigned sentiment can only be one of the positive, negative, or neutral labels.
Considering the fact that this is a really challenging task for the machine, to prevent human mistakes, we correlated the results of the three annotators and only chose the tweets, in which all of the annotators had the same opinion on the labels, as the final gold standard test data. Therefore, we finally, ended up with a test set consisting of 1,519 human labeled tweets, with the labels being the sentiment of the replies of a tweet and not the tweet itself. 
DATASET CONTENTS
1. Training raw dataset: 34,953 unique tweets in total and individual automatic labels for all of their corresponding replies (1,519,504 total replies). Including,

./RETWEET_data/train_reply_labels_set1.txt
./RETWEET_data/train_reply_labels_set2.txt

2. Training autamtically-labeled dataset: 34,953 unique tweets and ONE final automatic label (chosen based on the algorithm 1 of our paper) for every tweet. Including,

./RETWEET_data/train_final_label.txt

3. Gold standard test dataset (RETWEET): 1,519 unique tweets with their manual labels for replies. ONE final label, which states the predominant overall polarity of all its replies, is assigned to every tweet. Including,

./RETWEET_data/test_gold.txt

NOTES


Please note that by downloading the Twitter data you agree to abide by the Twitter terms of service, and in particular you agree not to redistribute the data and to delete tweets that are marked deleted in the future.


The ""neutral"" label in the annotations stands for objective or neutral.


The distribution consists of a set of Twitter unique tweet IDs with annotations (overall polarity of replies). As for data privacy, the texts of the tweets and replies are not distributed. But as all the utilized resources in this dataset are taken from public tweets, having the tweet unique IDs, you can download the tweet and its replies.
You can use the Semeval Twitter data downloading script to obtain the corresponding tweets:  
https://github.com/seirasto/twitter_download/


The dataset URL:
https://kaggle.com/soroosharasteh/retweet/ 


LICENSE
The accompanying dataset is released under a Creative Commons Attribution 4.0 International License.
SOURCE CODE
The official source code of the paper: https://github.com/starasteh/retweet
In case you use this dataset, please cite the original paper:
S. Tayebi Arasteh, M. Monajem, V. Christlein, P. Heinrich, A. Nicolaou, H.N. Boldaji, M. Lotfinia,  S. Evert. ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"". Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC), Laguna Hills, CA, USA, January 2021.
BibTex
@inproceedings{RETWEET,
  title = ""How Will Your Tweet Be Received? Predicting the Sentiment Polarity of Tweet Replies"",
  author = ""Tayebi Arasteh, Soroosh and Monajem, Mehrpad and Christlein, Vincent and
  Heinrich, Philipp and Nicolaou, Anguelos and Naderi Boldaji, Hamidreza and Lotfinia, Mahshad and Evert, Stefan"",
  booktitle = ""Proceedings of the 2021 IEEE 15th International Conference on Semantic Computing (ICSC)"",
  address = ""Laguna Hills, CA, USA"",
  pages = ""370-373"",
  doi = ""10.1109/ICSC50631.2021.00068"",
  url = ""https://ieeexplore.ieee.org/document/9364527/"",
  month = ""01"",       
  year = ""2021""
  }


Dataset DOI: 10.34740/kaggle/ds/736988
Paper: https://ieeexplore.ieee.org/document/9364527
Paper DOI: 10.1109/ICSC50631.2021.00068

CONTACT
E-mail: soroosh.arasteh@fau.de
DATA FORMAT FOR ALL THE FILES
label TAB id

where, ""label"" can be positive, neutral or negative, corresponding to the overall message-level polarity of the replies of the tweet and ""id"" corresponds to the Twitter unique ID for the tweets."	https://paperswithcode.com/dataset/retweet	21/04/2021						
3369	TRANCE	TRANCE extends CLEVR by asking a uniform question, i.e. what is the transformation between two given images, to test the ability of transformation reasoning. TRANCE includes three levels of settings, i.e. Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Detailed information can be found in https://hongxin2019.github.io/TVR.	https://paperswithcode.com/dataset/trance	26/11/2020	Transformation Driven Visual Reasoning					
3370	Sewer-ML	"Sewer-ML is a sewer defect dataset. It contains 1.3 million images, from 75,618 videos collected from three Danish water utility companies over nine years. All videos have been annotated by licensed sewer inspectors following the Danish sewer inspection standard, Fotomanualen. This leads to consistent and reliable annotations, and a total of 17 annotated defect classes.
Source: Sewer-ML
Image Source: Sewer-ML"	https://paperswithcode.com/dataset/sewer-ml	19/03/2021						
3371	HW-NAS-Bench	HW-NAS-Bench is a dataset for HardWare-aware Neural Architecture Search (HW-NAS). It is the first dataset for HW-NAS research aiming to democratize HW-NAS research to non-hardware experts and facilitate a unified benchmark for HW-NAS to make HW-NAS research more reproducible and accessible, covering two SOTA NAS search spaces including NAS-Bench-201 and FBNet	https://paperswithcode.com/dataset/hw-nas-bench	19/03/2021						
3372	MMKG	"MMKG is a collection of three knowledge graphs for link prediction and entity matching research. Contrary to other knowledge graph datasets, these knowledge graphs contain both numerical features and images for all entities as well as entity alignments between pairs of KGs. While MMKG is intended to perform relational reasoning across different entities and images, previous resources are intended to perform visual reasoning within the same image.
The three knowledge graphs augmented with numerical features and images are called FB15k, YAGO15k, and DBPEDIA15k."	https://paperswithcode.com/dataset/mmkg	13/03/2019						
3373	UBI-Fights	UBI-Fights - Concerning a specific anomaly detection and still providing a wide diversity in fighting scenarios, the UBI-Fights dataset is a unique new large-scale dataset of 80 hours of video fully annotated at the frame level. Consisting of 1000 videos, where 216 videos contain a fight event, and 784 are normal daily life situations. All unnecessary video segments (e.g., video introductions, news, etc.) that could disturb the learning process were removed.	https://paperswithcode.com/dataset/ubi-fights	03/01/2021	Abnormal Event Detection Dataset					
3374	SKAB	SKAB is designed for evaluating algorithms for anomaly detection. The benchmark currently includes 30+ datasets plus Python modules for algorithms’ evaluation. Each dataset represents a multivariate time series collected from the sensors installed on the testbed. All instances are labeled for evaluating the results of solving outlier detection and changepoint detection problems.	https://paperswithcode.com/dataset/skab	28/11/2020	Skoltech Anomaly Benchmark					
3375	DF20	"Danish Fungi 2020 (DF20) is a fine-grained dataset and benchmark. The dataset, constructed from observations submitted to the Danish Fungal Atlas, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints. 
The dataset has 1,604 different classes, with 248,466 training images and 27,608 test images.
Image Source: Danish Fungi 2020 - Not Just Another Image Recognition Dataset"	https://paperswithcode.com/dataset/df20	18/03/2021	Danish Fungi 2020					
3376	DF20 - Mini	"Danish Fungi 2020 (DF20) is a novel fine-grained dataset and benchmark. The dataset, constructed from observations submitted to the Danish Fungal Atlas, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints.
Source: Danish Fungi 2020 - Not Just Another Image Recognition Dataset
Image source: Danish Fungi 2020 - Not Just Another Image Recognition Dataset"	https://paperswithcode.com/dataset/df20-mini	18/03/2021	Danish Fungi 2020 - Mini					
3377	MasakhaNER	MasakhaNER is a collection of Named Entity Recognition (NER) datasets for 10 different African languages. The languages forming this dataset are: Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, and Yorùbá.	https://paperswithcode.com/dataset/masakhaner	22/03/2021						
3378	3D Vehicle Tracking Simulation Dataset	"To collect the 3D Vehicle Tracking Simulation Dataset, a driving simulation is used to obtain accurate 3D bounding box annotations at no cost of human efforts. The data collection and annotation pipeline extend the previous works like VIPER and FSV, especially in terms of linking identities across frames. The simulation is based on Grand Theft Auto V, a modern game that simulates a functioning city and its surroundings in a photo-realistic three-dimensional world. Note that the pipeline is real-time, providing the potential of largescale data collection, while VIPER requires expensive offline processings.
Source: Monocular Quasi-Dense 3D Object Tracking
Image source: Monocular Quasi-Dense 3D Object Tracking"	https://paperswithcode.com/dataset/3d-vehicle-tracking-simulation-dataset	12/03/2021						
3379	UIT-ViCTSD	UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection) is a dataset for constructive and toxic speech detection in Vietnamese. It consists of 10,000 human-annotated comments.	https://paperswithcode.com/dataset/uit-victsd	18/03/2021	UIT Vietnamese Constructive and Toxic Speech Detection					
3380	ParaCrawl	"ParaCrawl v.7.1 is a parallel dataset with 41 language pairs primarily aligned with English (39 out of 41) and mined using the parallel-data-crawling tool Bitextor which includes downloading documents, preprocessing and normalization, aligning documents and segments, and filtering noisy data via Bicleaner. ParaCrawl focuses on European languages, but also includes 9 lower-resource, non-European language pairs in v7.1.
Source: Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"	https://paperswithcode.com/dataset/paracrawl	01/07/2020						
3381	mC4	"mC4 is a multilingual variant of the C4 dataset called mC4. mC4 comprises natural text in 101 languages drawn from the public Common Crawl web scrape. 
Source: mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"	https://paperswithcode.com/dataset/mc4	22/10/2020						
3382	Penson et al.'s dataset derived from the MSK-IMPACT dataset	The dataset is derived from the MSK-IMPACT dataset designed and published by Zehir using the code published by Penson et al.. The derivation process is described in Development of Genome-Derived Tumor Type Prediction to Inform Clinical Cancer Care.	https://paperswithcode.com/dataset/penson-et-al-s-dataset-derived-from-the-msk							
3383	Cross-Linguistic Polysemies	Data from: Using network approaches to enhance the analysis of cross-linguistic polysemies	https://paperswithcode.com/dataset/cross-linguistic-polysemies	01/03/2013	Data from: Using network approaches to enhance the analysis of cross-linguistic polysemies					
3384	PRW	PRW is a large-scale dataset for end-to-end pedestrian detection and person recognition in raw video frames. PRW is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities.	https://paperswithcode.com/dataset/prw	09/04/2016	Person Re-identification in the Wild					
3385	TICaM	TICaM is a Time-of-flight In-car Cabin Monitoring dataset for vehicle interior monitoring using a single wide-angle depth camera. This dataset addresses the deficiencies of other available in-car cabin datasets in terms of the ambit of labeled classes, recorded scenarios and provided annotations; all at the same time. It consists of an exhaustive list of actions performed while driving and multi-modal labeled images (depth, RGB and IR), with complete annotations for 2D and 3D object detection, instance and semantic segmentation as well as activity annotations for RGB frames. Additional to real recordings, it also contains a synthetic dataset of in-car cabin images with same multi-modality of images and annotations, providing a unique and extremely beneficial combination of synthetic and real data for effectively training cabin monitoring systems and evaluating domain adaptation approaches.	https://paperswithcode.com/dataset/ticam	22/03/2021	Time-of-flight In-car Cabin Monitoring					
3386	F-SIOL-310	"F-SIOL-310 is a robotic dataset and benchmark for Few-Shot Incremental Object Learning, which is used to test incremental learning capabilities for robotic vision from a few examples. 
A robot was used to actively capture household objects on a table. The dataset is specifically designed for FSIL with only a small set of training images and a larger set of test images per object category captured by the robot using its own camera and it considers various other robot vision challenges as well, such as different object sizes,
object transparency and a clear distinction between objects in the train and test sets. It contains images of 310 objects from 22 categories."	https://paperswithcode.com/dataset/f-siol-310	23/03/2021	Few-Shot Incremental Object Learning					
3387	PESMOD	The PESMOD (PExels Small Moving Object Detection) dataset consists of high resolution aerial images in which moving objects are labelled manually. It was created from videos selected from the Pexels website. The aim of this dataset is to provide a different and challenging dataset for moving object detection methods evaluation. Each moving object is labelled for each frame with PASCAL VOC format in a XML file. The dataset consists of 8 different video sequences.	https://paperswithcode.com/dataset/pesmod	21/03/2021	PExels Small Moving Object Detection					
3388	SwissDial	SwissDial is an annotated parallel corpus of spoken Swiss German across 8 major dialects, plus a Standard German reference. It contains parallel spoken data for 8 different regions: Aargau (AG), Bern (BE), Basel (BS), Graubunden (GR), Luzern (LU), St. Gallen (SG), Wallis (VS) and Zurich (ZH).	https://paperswithcode.com/dataset/swissdial	21/03/2021						
3389	Alsat-2B	Alsat-2B is a remote sensing dataset of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. The dataset has been created from 13 images captured by the Alsat-2B Earth observation satellite, where the image cover 13 different cities.	https://paperswithcode.com/dataset/alsat-2b	21/03/2021						
3390	Benchmark for Neural Paraphrase Detection	"This is a benchmark for neural paraphrase detection, to differentiate between original and machine-generated content.
Training:
1,474,230 aligned paragraphs (98,282 original, 1,375,948 paraphrased with 3 models and 5 hyperparameter configurations each 98,282) extracted from 4,012 (English) Wikipedia articles.
Testing:
```
BERT-large (cased): 
    arXiv             - Original - 20,966;     Paraphrased - 20,966; 
    Theses          - Original - 5,226;      Paraphrased - 5,226;
    Wikipedia      - Original - 39,241;     Paraphrased - 39,241;
RoBERTa-large (cased): 
    arXiv             - Original - 20,966;     Paraphrased - 20,966; 
    Theses          - Original - 5,226;      Paraphrased - 5,226;
    Wikipedia      - Original - 39,241;     Paraphrased - 39,241;
Longformer-large (uncased): 
    arXiv             - Original - 20,966;     Paraphrased - 20,966; 
    Theses          - Original - 5,226;      Paraphrased - 5,226;
    Wikipedia      - Original - 39,241;     Paraphrased - 39,241;
```"	https://paperswithcode.com/dataset/benchmark-for-neural-paraphrase-detection	23/03/2021						
3391	Ascent KB	This dataset contains 8.9M commonsense assertions extracted by the Ascent pipeline developed at the Max Planck Institute for Informatics. The focus of this dataset is on everyday concepts such as elephant, car, laptop, etc. The current version of Ascent KB (v1.0.0) is approximately 19 times larger than ConceptNet (note that, in this comparison, non-commonsense knowledge in ConceptNet such as lexical relations is excluded).	https://paperswithcode.com/dataset/ascentkb	02/11/2020						
3392	AHP	"The AHP dataset consists of 56,599 images in total which are collected from several large-scale instance segmentation and detection datasets, including COCO, VOC (w/ SBD), LIP, Objects365 and OpenImages. Each image is annotated with a pixel-level segmentation mask of a single integrated human.
The dataset is initially proposed to solve the task of human de-occlusion.
Data Splits

Train: Totally 56,302 images with annotations of integrated humans.
Valid: Totally 297 images of synthesized occlusion cases.
Test: Totally 56 images of artificial occlusion cases."	https://paperswithcode.com/dataset/ahp	22/03/2021	Amodal Human Perception					
3393	VCAS-Motion	"Video class agnostic segmentation (VCAS) is the task of segmenting objects without regards to its semantics combining appearance, motion and geometry from monocular video sequences. The main motivation behind this is to account for unknown objects in the scene and to act as a redundant signal along with the segmentation of known classes for better safety as shown in the following Figure.
This VCAS benchmark is built from KITTI-MOTS and Cityscapes-VPS."	https://paperswithcode.com/dataset/vcas-motion	19/03/2021	Video Class Agnostic Segmentation Benchmark					
3394	Tatoeba Translation Challenge	"The Tatoeba Translation Challenge is a benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages.
The Tatoeba translation challenge includes shuffled training data taken from OPUS, an open collection of parallel corpora, and test data from Tatoeba, a crowd-sourced collection of user-provided translations in a large number of languages. 
The current release includes over 500GB of compressed data for 2,961 language pairs covering 555 languages. The data sets are released per language pair with the following structure (using deu-eng as an example):
data/deu-eng/
data/deu-eng/train.src.gz
data/deu-eng/train.trg.gz
data/deu-eng/train.id.gz
data/deu-eng/dev.id
data/deu-eng/dev.src
data/deu-eng/dev.trg
data/deu-eng/test.src
data/deu-eng/test.trg
data/deu-eng/test.id"	https://paperswithcode.com/dataset/tatoeba-translation-challenge	13/10/2020						
3395	trek05-1		https://paperswithcode.com/dataset/drebin		trek05-1					
3396	Drebin		https://paperswithcode.com/dataset/drebin-1		Drebin					
3397	TREC-05		https://paperswithcode.com/dataset/trec-05		TREC 2005 Spam Public Corpora					
3398	MISAW	"The MISAW data set is composed of 27 sequences of micro-surgical anastomosis on artificial blood vessels performed by 3 surgeons and 3 engineering students. The dataset contained video, kinematic, and procedural descriptions synchronized at 30Hz. The procedural descriptions contained phases, steps, and activities performed by the participants.
Source: MIcro-Surgical Anastomose Workflow recognition challenge report"	https://paperswithcode.com/dataset/misaw	24/03/2021	MIcro-Surgical Anastomose Workflow recognition on training sessions					
3399	Falling Objects		https://paperswithcode.com/dataset/falling-objects	01/12/2020						
3400	SNDZoo	The softwarised network data zoo (SNDZoo) is an open collection of software networking data sets aiming to streamline and ease machine learning research in the software networking domain. Most of the published data sets focus on, but are not limited to, the performance of virtualised network functions (VNFs). The data is collected using fully automated NFV benchmarking frameworks, such as tng-bench, developed by us or third party solutions like Gym. The collection of the presented data sets follows the general VNF benchmarking methodology described in.	https://paperswithcode.com/dataset/sndzoo	21/10/2019	The Softwarised Network Data Zoo					
3401	TbD-3D		https://paperswithcode.com/dataset/tbd-3d	25/11/2019						
3402	TbD		https://paperswithcode.com/dataset/tbd	09/05/2019						
3403	Us Vs. Them	$\textit{Us vs. Them}$ dataset, consisting of 6861 Reddit comments annotated for populist attitudes and the first large-scale computational models of this phenomenon. It covers the relationship between populist mindsets and social groups, as well as a range of emotions typically associated with these.	https://paperswithcode.com/dataset/us-vs-them	28/01/2021	Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions					
3404	TAS500	TAS500 is a semantic segmentation dataset for autonomous driving in unstructured environments. TAS500 offers fine-grained vegetation and terrain classes to learn drivable surfaces and natural obstacles in outdoor scenes effectively.	https://paperswithcode.com/dataset/tas500	24/03/2021						
3405	CSFCube	CSFCube is an expert annotated test collection to evaluate models trained to perform faceted Query by Example. This test collection consists of a diverse set of 50 query documents, drawn from computational linguistics and machine learning venues	https://paperswithcode.com/dataset/csfcube	24/03/2021						
3406	Finnish Paraphrase Corpus	Finnish Paraphrase Corpus is a fully manually annotated paraphrase corpus for Finnish containing 53,572 paraphrase pairs harvested from alternative subtitles and news headings. Out of all paraphrase pairs in the corpus 98% are manually classified to be paraphrases at least in their given context, if not in all contexts.	https://paperswithcode.com/dataset/finnish-paraphrase-corpus	24/03/2021						
3407	Rainbow	Rainbow is multi-task benchmark for common-sense reasoning that uses different existing QA datasets: aNLI, Cosmos QA, HellaSWAG. Physical IQa, Social IQa, WinoGrande.	https://paperswithcode.com/dataset/rainbow	24/03/2021						
3408	Re-TACRED	"The Re-TACRED dataset is a significantly improved version of the TACRED dataset for relation extraction. Using new crowd-sourced labels, Re-TACRED prunes poorly annotated sentences and addresses TACRED relation definition ambiguity, ultimately correcting 23.9% of TACRED labels. This dataset contains over 91 thousand sentences spread across 40 relations. Dataset presented at AAAI 2021.
Paper (arXiv): https://arxiv.org/abs/2104.08398"	https://paperswithcode.com/dataset/re-tacred	16/04/2021	Revised-TACRED					
3409	ThreeDWorld Transport Challenge	"ThreeDWorld Transport Challenge is a visually-guided and physics-driven task-and-motion planning benchmark. In this challenge, an embodied agent equipped with two 9-DOF articulated arms is spawned randomly in a simulated physical home environment. The agent is required to find a small set of objects scattered around the house, pick them up, and transport them to a desired final location. Several containers are positioned around the house that can be used as tools to assist with transporting objects efficiently. To complete the task, an embodied agent must plan a sequence of actions to change the state of a large number of objects in the face of realistic physical constraints. 
This benchmark challenge has been built using the ThreeDWorld simulation: a virtual 3D environment where all objects respond to physics, and where can be controlled using fully physics-driven navigation and interaction API."	https://paperswithcode.com/dataset/threedworld-transport-challenge	25/03/2021						
3410	USB	The Universal-Scale object detection Benchmark (USB) is a benchmark for object detection that has variations in object scales and image domains by incorporating COCO with the recently proposed Waymo Open Dataset and Manga109-s dataset. To enable fair comparison, USB establishes different protocols by defining multiple thresholds for training epochs and evaluation image resolutions.	https://paperswithcode.com/dataset/usb	25/03/2021	Universal-Scale Object Detection Benchmark					
3411	StyleKQC	StyleKQC is a style-variant paraphrase  corpus for korean questions and commands. It was built with a corpus construction scheme that simultaneously considers the core content and style of directives, namely intent and formality, for the Korean language. Utilizing manually generated natural language queries on six daily topics, the corpus was expanded to formal and informal sentences by human rewriting and transferring.	https://paperswithcode.com/dataset/stylekqc	24/03/2021						
3412	ECtHR	ECtHR is a dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. This dataset comprises 11k ECtHR cases and can be viewed as an enriched version of the ECtHR dataset of Chalkidis et al. (2019), which did not provide ground truth for alleged article violations (articles discussed) and rationales. It is released with silver rationales obtained from references in court decisions, and gold rationales provided by ECHR-experienced lawyers	https://paperswithcode.com/dataset/ecthr	24/03/2021	European Court of Human Rights Cases					
3413	BookingDataChallenge	The dataset contains anonymised hotel checkins. The dataset contains train and test parts, the in the test part city of the last checkin is masked.  The goal is to predict this masked checkin.	https://paperswithcode.com/dataset/bookingdatachallenge		Booking.com Data Challenge					
3414	MULTIMODAL HUMOR		https://paperswithcode.com/dataset/multimodal-humor							
3415	Multimodal Humor Dataset	A great number of situational comedies (sitcoms) are being regularly made and the task of adding laughter tracks to these is a critical task. Providing an ability to be able to predict whether something will be humorous to the audience is also crucial. In this project, we aim to automate this task. Towards doing so, we annotate an existing sitcom (Big Bang Theory') and use the laughter cues present to obtain a manual annotation for this show. We provide detailed analysis for the dataset design and further evaluate various state of the art baselines for solving this task. We observe that existing LSTM and BERT based networks on the text alone do not perform as well as joint text and video or only video-based networks. Moreover, it is challenging to ascertain that the words attended to while predicting laughter are indeed humorous. Our dataset and analysis provided through this paper is a valuable resource towards solving this interesting semantic and practical task. As an additional contribution, we have developed a novel model for solving this task that is a multi-modal self-attention based model that outperforms currently prevalent models for solving this task.	https://paperswithcode.com/dataset/multimodal-humor-dataset	06/01/2021	Multimodal Humor Dataset: Predicting Laughter Tracks for Sitcoms					
3416	UJIIndoorLoc	The UJIIndoorLoc is a Multi-Building Multi-Floor indoor localization database to test Indoor Positioning System that rely on WLAN/WiFi fingerprint.	https://paperswithcode.com/dataset/ujiindoorloc							
3417	RC-49	"RC-49 is a benchmark dataset for generating images conditional on a continuous scalar variable. It is made by rendering 49 3-D chair models from ShapeNet individually. Each chair model is rendered at 899 yaw angles from $0.1^{\circ}$ to $89.9^{\circ}$ with a stepsize of $0.1^{\circ}$. This dataset contains 44,051 RGB images of size $64\times64$ with corresponding yaw angles as labels. 
Note that in CcGAN, angles are used for training if their last digits are odd. Thus, there are 450 angles in the training set. Moreover, for these 450 training angles, only 25 images for each angle are used for the training."	https://paperswithcode.com/dataset/rc-49	15/11/2020						
3418	Cell-200	Cell-200 is a a dataset of synthetic fluorescence microscopy images with cell populations generated by SIMCEP. The Cell-200 dataset consists of 200,000 $64\times 64$ grayscale images. The number of cells per image ranges from 1 to 200 and there are 1,000 images for each cell count. However, only a subset of Cell-200 with only odd cell counts and 10 images per count (1,000 training images in total) is used for the GAN training.	https://paperswithcode.com/dataset/cell-200							
3419	RealSRSet	20 real low-resolution images selected from existing datasets or downloaded from internet	https://paperswithcode.com/dataset/realsrset	25/03/2021						
3420	L3CubeMahaSent	"L3CubeMahaSent  is a large publicly available Marathi Sentiment Analysis dataset. It consists of marathi tweets which are manually labelled.
This dataset contains a total of 18,378 tweets which are classified into three classes - Positive (1), Negative (-1) and Neutral (0). All tweets are present in their original form, without any preprocessing.
Out of these, 15,864 tweets are considered for splitting them into train, test and validation datasets. This has been done to avoid class imbalance in the dataset.
The remaining 2,514 tweets are also provided in a separate sheet."	https://paperswithcode.com/dataset/l3cubemahasent	21/03/2021						
3421	ACRE	"Abstract Causal REasoning (ACRE) is a dataset for the systematic evaluation of current vision systems in causal induction, i.e., identifying unobservable mechanisms that lead to the observable relations among variables.
Each split of the dataset is structured as follows:
config/
    train.json
    val.json
    test.json
images/
    ACRE_train_00*.png
    ACRE_val_00*.png
    ACRE_test_00*.png
scenes/
    ACRE_train_00*.json
    ACRE_val_00*.json
    ACRE_test_00*.json
Each image file in the images folder has a corresponding scene description file in scenes with the same name (except for the extension).
Each ACRE problem is named after ACRE_{train/val/test}_{6_digit_problem_idx}_{2_digit_panel_idx}"	https://paperswithcode.com/dataset/acre	26/03/2021	Abstract Causal REasoning					
3422	Identifying Machine-Paraphrased Plagiarism	"This dataset is used to train and evaluate models for the detection of machine-paraphrased text.
The training set consists of 200,767 paragraphs (98,282 original, 102,485 paraphrased) extracted from 8,024 Wikipedia (English) articles (4,012 original, 4,012 paraphrased using the SpinBot API).
The test set is divided into 3 subsets: one created from preprints of research papers on arXiv, one from graduation theses, and one from Wikipedia articles. Additionally, different marchine-paraphrasing methods were used.
Test sets:
```
SpinBot: 
    arXiv         - Original - 20,966;    Spun - 20,867
    Theses        - Original - 5,226;        Spun - 3,463
    Wikipedia    - Original - 39,241;    Spun - 40,729
SpinnerChief-4W: 
    arXiv         - Original - 20,966;    Spun - 21,671
    Theses        - Original - 2,379;        Spun - 2,941
    Wikipedia    - Original - 39,241;    Spun - 39,618
SpinnerChief-2W: 
    arXiv         - Original - 20,966;    Spun - 21,719
    Theses        - Original - 2,379;        Spun - 2,941
    Wikipedia    - Original - 39,241;    Spun - 39,697
```"	https://paperswithcode.com/dataset/identifying-machine-paraphrased-plagiarism	22/03/2021						
3423	RAVDESS	"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7,356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.
Paper: The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English
Source:"	https://paperswithcode.com/dataset/ravdess	31/12/2020	Ryerson Audio-Visual Database of Emotional Speech and Song					
3424	CaSiNo	CaSiNo is a dataset of 1030 negotiation dialogues in English. To create the dataset, two participates take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. This design keeps the task tractable, while still facilitating linguistically rich and personal conversations.	https://paperswithcode.com/dataset/casino	29/03/2021						
3425	SUTD-TrafficQA	SUTD-TrafficQA (Singapore University of Technology and Design - Traffic Question Answering) is a dataset which takes the form of video QA based on 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, the dataset proposes 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events.	https://paperswithcode.com/dataset/trafficqa	29/03/2021						
3426	LaboroTVSpeech	LaboroTVSpeech is a large-scale Japanese speech corpus built from broadcast TV recordings and their subtitles. It contains over 2,000 hours of speech.	https://paperswithcode.com/dataset/laborotvspeech	26/03/2021						
3427	OFDIW	"OnFocus Detection In the Wild (OFDIW) is an onfocus detection dataset. It consists of 20,623 images in unconstrained capture conditions (thus called ""in the wild'') and contains individuals with diverse emotions, ages, facial characteristics, and rich interactions with surrounding objects and background scenes. The images are collected from the LFW dataset and the Oxford-IIIT Pet dataset. Onfocus detection aims at identifying whether the focus of the individual captured by a camera is on the camera or not."	https://paperswithcode.com/dataset/ofdiw	29/03/2021	OnFocus Detection In the Wild					
3428	P-OCT	"The entire dataset consists of 61 different subjects, for each of which 12 radial OCT B-scans are collected at the Ophthalmology Department of Shanghai General Hospital by using DRI OCT-1 Atlantis (Topcon Corporation, Tokyo, Japan). The image size is 1024 × 992 pixels, corresponding to a field of view of 20.48 mm × 7.94 mm. For each subject, 2 radial OCT B-scans were randomly selected to ensure mutual exclusion. Two graders annotated these images manually through ITK-SNAP software into the optic disc and nine retinal layers under the supervision of a glaucoma specialist.
For more details, please refer to our paper."	https://paperswithcode.com/dataset/p-oct		Peripapillary OCT Images					
3429	N-MNIST	"Brief Description
The Neuromorphic-MNIST (N-MNIST) dataset is a spiking version of the original frame-based MNIST dataset. It consists of the same 60 000 training and 10 000 testing samples as the original MNIST dataset, and is captured at the same visual scale as the original MNIST dataset (28x28 pixels). The N-MNIST dataset was captured by mounting the ATIS sensor on a motorized pan-tilt unit and having the sensor move while it views MNIST examples on an LCD monitor as shown in this video. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset.
Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.  “Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades"", Frontiers in Neuroscience, vol.9, no.437, Oct. 2015"	https://paperswithcode.com/dataset/n-mnist		Neuromorphic-MNIST					
3430	SBCoseg	The SBCoseg dataset includes 889 groups of images and each group consists of 18 images with a common object, leading to 16002 images in total. The whole dataset is divided into five subsets: with ECFB, with TR, with MH, with SD, and Normal (normal data). The five subsets contain 193, 251, 82, 83, and 280 image groups, respectively. Each original image is in JPG format with a pixel size of 360 ×360, and each ground-truth image is in PNG format.	https://paperswithcode.com/dataset/sbcoseg	03/12/2020	SBCoseg Dataset					
3431	LReID	"LReID is a benchmark for lifelong person reidentification. It has been built using existing datasets, and it consists of two subsets: LReID-Seen and LReID-Unseen.
LReID-Seen contains 40,459 training images of the 2,500 identities selected from the following datasets: CUHK03, Market-1501, MSMT17 V2, DukeMTMC-ReID, CUHK-SYSU ReID. This is used to test a model's performance on seen domains.
LReID-Unseen contains 9,854 images from 3,594 identities from the following datasets: VIPeR, PRID, GRID, i-LIDS, CUHK01, CUHK02, SenseReID. This subset is used to test the model's generalisation capabilities to unseen domains."	https://paperswithcode.com/dataset/lreid	23/03/2021						
3432	SenseReID	SenseReID is a person re-identification dataset for evaluating ReID models. It is captured from real surveillance cameras and the person bounding boxes are obtained from state-of-the-art detection algorithm. The dataset contains 1,717 identities in total.	https://paperswithcode.com/dataset/sensereid	01/07/2017						
3433	DIP-IMU	Dataset consisting of IMU measurements and corresponding SMPL poses. Participants were wearing 17 IMU sensors and reference SMPL poses were obtained by running the SIP optimization with all 17 sensors.	https://paperswithcode.com/dataset/dip-imu	10/10/2018						
3434	Twitter Abusive Context	This dataset for abusive content detection in Twitter consists of two sets of annotations for the same set of tweets, one where the human annotators had access to the tweet's content and one where they didn't know the context.	https://paperswithcode.com/dataset/twitter-abusive-context	27/03/2021						
3435	TCR-pMHC	"10x Genomics dataset of sequenced TCRs barcoded by a panel of pMHCs (arranged on a dextramer)
Source: A new way of exploring immunity: linking highly 
multiplexed antigen recognition to immune repertoire 
and phenotype"	https://paperswithcode.com/dataset/10x-tcr-pmhc	09/05/2019	10x Genomics T cell receptor peptide-MHC pairs					
3436	TCR-CMV	"Adaptive Biotechnologies' dataset of sequenced T cell repertoires labelled by patient age, HLA type, and CMV serostatus
Source: Immunosequencing identifies signatures of cytomegalovirus exposure history and HLA-mediated effects on the T-cell repertoire"	https://paperswithcode.com/dataset/tcr-cmv	27/02/2017	T cell repertoires labelled by CMV serostatus					
3437	GLUCOSE	"GLUCOSE is a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement.
Source: GLUCOSE: GeneraLized and COntextualized Story Explanations
Image source: GLUCOSE: GeneraLized and COntextualized Story Explanations"	https://paperswithcode.com/dataset/glucose	16/09/2020						
3438	ArtDL	ArtDL is a novel painting data set for iconography classification composed of images collected from online sources. Most of the paintings are from the Renaissance period and depict scenes or characters of Christian art. The data set is annotated with classes representing specific characters belonging to the Iconclass classification system.	https://paperswithcode.com/dataset/artdl	06/10/2020	ArtDL					
3439	Paris6k	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/paris6k							
3440	INSTRE	"INSTRE is a benchmark for INSTance-level visual object REtrieval and REcognition (INSTRE). INSTRE has the following major properties: (1) balanced data scale,
(2) more diverse intraclass instance variations, (3) cluttered and less contextual backgrounds, (4) object localization annotation for each image, (5) well-manipulated double-labelled images for measuring multiple object (within one image) case.
The whole dataset is split into three disjoint subsets INSTRE-S1 (for single object case 1), INSTRE-S2 (for single object case 2) and INSTRE-M (for multiple object case). INSTRE-S1 and INSTRE-S2 are collected for measuring single object case, both of which have 100 object classes. INSTRE-S1 contains 11011 images and INSTRE-S2 contains 12059 images."	https://paperswithcode.com/dataset/instre	02/02/2015						
3441	TrackML challenge Accuracy phase dataset	"The dataset comprises multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN. The goal of the tracking machine learning challenge is to group the recorded measurements or hit for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hit, their ground truth counterpart and their association to particles, and the initial parameters of those particles. The test dataset contains only the recorded hits.
The dataset was used for the Accuracy Phase of the Tracking Machine Learning challenge on Kaggle.
See more details in the home page url."	https://paperswithcode.com/dataset/trackml	14/04/2019	Tracking Machine Learning Challenge					
3442	fGn Traffic Traces	fGn series used in the article to develop the simulations.	https://paperswithcode.com/dataset/fgn-traffic-traces	02/03/2021						
3443	PIE	"PIE is a new dataset for studying pedestrian behavior in traffic. PIE contains over 6 hours of footage recorded in typical traffic scenes with on-board camera. It also provides accurate vehicle information from OBD sensor (vehicle speed, heading direction and GPS coordinates) synchronized with video footage.
Rich spatial and behavioral annotations are available for pedestrians and vehicles that potentially interact with the ego-vehicle as well as for the relevant elements of infrastructure (traffic lights, signs and zebra crossings).
There are over 300K labeled video frames with 1842 pedestrian samples making this the largest publicly available dataset for studying pedestrian behavior in traffic."	https://paperswithcode.com/dataset/pie	01/10/2019	Pedestrian Intention Estimation					
3444	HEV-I	Honda Egocentric View-Intersection Dataset (HEV-I) is introduced to enable research on traffic participants interaction modelling, future object localization, as well as learning driver action in challenging driving scenarios. The dataset includes 230 video clips of real human driving in different intersections from the San Francisco Bay Area, collected using an instrumented vehicle equipped with different sensors including cameras, GPS/IMU, and vehicle states signals.	https://paperswithcode.com/dataset/hev-i	19/09/2018	Honda Egocentric View-Intersection Dataset					
3445	Dry Bean Dataset	"Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.
Source: UCI Machine Learning Repository"	https://paperswithcode.com/dataset/dry-bean-dataset	01/07/2020						
3446	3D AffordanceNet	3D AffordanceNet is a dataset of 23k shapes for visual affordance. It consists of 56,307 well-defined affordance information annotations for 22,949 shapes covering 18 affordance classes and 23 semantic object categories.	https://paperswithcode.com/dataset/3d-affordancenet	30/03/2021						
3447	RUSS Dataset	RUSS (Rapid Universal Support Service) is a dataset that consists of a collection of 741 real-world step-by-step natural language instructions (raw and annotated) from the open web, and for each: its corresponding webpage DOM, ground-truth ThingTalk, and ground-truth actions.	https://paperswithcode.com/dataset/russ-dataset	30/03/2021						
3448	AGQA	"Action Genome Question Answering (AGQA) is a benchmark for compositional spatio-temporal reasoning. AGQA contains 192M unbalanced question answer pairs for 9.6K videos. It also contains a balanced subset of 3.9M question answer pairs, 3 orders of magnitude larger than existing benchmarks, that minimizes bias by balancing the answer distributions and types of question structures. 
AGQA introduces multiple training/test splits to test for various reasoning abilities, including generalization to novel compositions, to indirect references, and to more compositional steps."	https://paperswithcode.com/dataset/agqa	30/03/2021	Action Genome Question Answering					
3449	MSRB	MSRB is a benchmarking dataset for marine snow removal of underwater images. Marine snow is one of the main degradation sources of underwater images that are caused by small particles, e.g., organic matter and sand, between the underwater scene and photosensors. The dataset consists of large-scale pairs of ground-truth and degraded images to calculate objective qualities for marine snow removal and to train a deep neural network. We propose two marine snow removal tasks using the dataset and show the first benchmarking results of marine snow removal.	https://paperswithcode.com/dataset/msrb	26/03/2021	Marine Snow Removal Benchmarking					
3450	RoomR	"The task of Room Rearrangement consists on an agent exploring a room and recording objects' initial configurations. The agent is removed and the poses and states (e.g., open/closed) of some objects in the room are changed. The agent must restore the initial configurations of all objects in the room. 
RoomR includes 6,000 distinct rearrangement settings involving 72 different object types in 120 scenes."	https://paperswithcode.com/dataset/roomr	30/03/2021	Room Rearrangement					
3451	Libri-adhoc40	Libri-adhoc40 is a synchronized speech corpus which collects the replayed Librispeech data from loudspeakers by ad-hoc microphone arrays of 40 strongly synchronized distributed nodes in a real office environment. Besides, to provide the evaluation target for speech frontend processing and other applications, the authors also recorded the replayed speech in an anechoic chamber.	https://paperswithcode.com/dataset/libri-adhoc40	28/03/2021						
3452	Food2K	Food2K is a large food recognition dataset with 2,000 categories and over 1 million images. Compared with existing food recognition datasets, Food2K bypasses them in both categories and images by one order of magnitude, and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning.  Food2K can be further explored to benefit more food-relevant tasks including emerging and more complex ones (e.g., nutritional understanding of food), and the trained models on Food2K can be expected as backbones to improve the performance of more food-relevant tasks.	https://paperswithcode.com/dataset/food2k	30/03/2021						
3453	U.S. Broadband Coverage	The U.S. Broadband Coverage data set is a publicly available dataset that reports broadband coverage percentages at a zip code-level. The authors have used differential privacy to guarantee that the privacy of individual households is preserved. The data set also contains error ranges estimates, providing information on the expected error introduced by differential privacy per zip code.	https://paperswithcode.com/dataset/u-s-broadband-coverage	24/03/2021						
3454	Win-Fail Action Understanding	First  of  its  kind paired win-fail action understanding dataset with samples from  the  following  domains:  “General  Stunts,”  “Internet Wins-Fails,”  “Trick  Shots,”  &  “Party  Games.” The task is to identify successful and failed attempts at various activities. Unlike existing  action  recognition  datasets,  intra-class  variation is  high  making  the  task  challenging,  yet  feasible.	https://paperswithcode.com/dataset/win-fail-action-understanding	15/02/2021	Win-Fail Action Understanding					
3455	Multimodal PISA	Dataset for multimodal skills assessment focusing on assessing piano player’s skill level. Annotations include player's skills level, and song difficulty level. Bounding box annotations around pianists' hands are also provided.	https://paperswithcode.com/dataset/multimodal-pisa	13/01/2021	Multimodal Piano Skills Assessment					
3456	iNat2021	"iNat2021 is a large-scale image dataset collected and annotated by community scientists that contains over 2.7M images from 10k different species.
To make the dataset more accessible the authors have also created a ""mini"" training dataset with 50 examples per species for a total of 500K images. Each species has 10 validation images, for a total of 100k validation images. There are a total of 500,000 test images. In addition to its overall scale, the main distinguishing feature of iNat2021 is that it contains at least 152 images in the training set for each species."	https://paperswithcode.com/dataset/inat2021	30/03/2021	iNaturalist 2021					
3457	AMT Objects	"AMT Objects is a large dataset of object centric videos suitable for training and benchmarking models for generating 3D models of objects from a small number of photos of the objects. The dataset consists of multiple views of a large collection of object instances.
The dataset contains 7 object categories from the MS COCO classes: apple, sandwich, orange, donut, banana, carrot and hydrant. For each class, annotators were asked to collect a video by looking ‘around’ a class instance, resulting in a turntable video. The dataset contains 169-457 videos per class. For each class, the videos were randomly split into training and testing videos in an 8:1 ratio."	https://paperswithcode.com/dataset/amt-objects	30/03/2021						
3458	RepLab 2013	"RepLab 2013 dataset uses Twitter data in English and Spanish (more than 142,000 tweets). The balance between both languages depends on the availability of data for each of the entities included in the dataset. The corpus consists of a collection of tweets referring to a selected set of 61 entities from four domains: automotive, banking, universities and music/artists. The domain selection was done to offer a variety of scenarios for reputation studies.
Crawling was performed during the period from the 1st June 2012 till the 31st Dec 2012 using the entity’s canonical name as query. For each entity, at least 2,200 tweets are collected: at least 700 tweets at the beginning of the timeline are used as training set, and at least 1,500 last tweets are reserved for the test set. The corpus also comprises additional background tweets for each entity (up to 50,000 tweets, with a large variability across entities). This distribution was set in this way to obtain a temporal separation (ideally of several months) between the training and test data.
Note that the final amount of available tweets in these sets may be lower, since some posts may have been deleted by the users: in order to respect Twitter’s terms of service, we do not provide the contents of the tweets. The tweet identifiers can be used to retrieve the texts of the posts. We provide a download tool that is similarly to the mechanism used in the TREC Microblog Track in 2011 and 2012.
For more information, please refer to the RepLab 2013 Overview's paper."	https://paperswithcode.com/dataset/replab-2013							
3459	ASVspoof 2019		https://paperswithcode.com/dataset/asvspoof-2019	11/02/2021	Automatic Speaker Verification Spoofing And Countermeasures Challenge					
3460	ABCD	Action-Based Conversations Dataset (ABCD) is a goal-oriented dialogue fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. The dataset is proposed to study customer service dialogue systems in more realistic settings.	https://paperswithcode.com/dataset/abcd	01/04/2021	Action-Based Conversations Dataset					
3461	VidSitu	VidSitu is a dataset for the task of semantic role labeling in videos (VidSRL). It is a large-scale video understanding data source with 29K 10-second movie clips richly annotated with a verb and semantic-roles every 2 seconds. Entities are co-referenced across events within a movie clip and events are connected to each other via event-event relations. Clips in VidSitu are drawn from a large collection of movies (∼3K) and have been chosen to be both complex (∼4.2 unique verbs within a video) as well as diverse (∼200 verbs have more than 100 annotations each).	https://paperswithcode.com/dataset/vidsitu	02/04/2021						
3462	NaturalProofs	"The NaturalProofs Dataset is a large-scale dataset for studying mathematical reasoning in natural language. NaturalProofs consists of roughly 20,000 theorem statements and proofs, 12,500 definitions, and 1,000 additional pages (e.g. axioms, corollaries) derived from ProofWiki, an online compendium of mathematical proofs written by a community of contributors. 
Source: NaturalProofs"	https://paperswithcode.com/dataset/naturalproofs	24/03/2021						
3463	Mirrored-Human	Mirrored-Human is a dataset for 3D pose estimation from a single view. It covers a large variety of human subjects, poses and backgrounds. The images are collected from the internet and consists of people in front of mirrors, were both the person and the reflected image are visible. Actions cover dancing, fitness, mirror installation, swing practice	https://paperswithcode.com/dataset/mirrored-human	01/04/2021						
3464	Omiverse Object dataset	Omiverse Object is a large-scale synthetic dataset of 60,000 images including both transparent and opaque objects in different scenes. It is used for depth completion of transparent objects from a single RGB-D view.	https://paperswithcode.com/dataset/omiverse-object-dataset	01/04/2021						
3465	Auto-KWS	"Auto-KWS is a dataset for customized keyword spotting, the task of detecting spoken keywords. The dataset closely resembles real world scenarios, as each recorder is assigned with an unique wake-up word and can choose their recording environment and familiar dialect freely.
All data is recorded by near-field mobile phones, (located in front of the speakers at around 0.2m distance). Each sample is recorded in single channel, 16-bit streams at a 16kHz sampling rate. There are 4 datasets: training dataset, practice dataset, feedback dataset, and private dataset. Training dataset, recorded from around 100 recorders, is used for participants to develop Auto-KWS solutions. Practice dataset contains 5 speakers data, each with 5 enrollment audio data and seveal test audio. Practice dataset together with the downloadable docker provides an example of how platform would call the participants' code. Both Training and practice dataset can be downloaded for local debugging. The feedback dataset and private dataset have the same format of practice dataset and are used for final evaluation and thus will be hiden from participants."	https://paperswithcode.com/dataset/auto-kws	31/03/2021						
3466	UAV123		https://paperswithcode.com/dataset/uav123							
3467	LemgoRL	LemgoRL is an open-source benchmark tool for traffic signal control designed to train reinforcement learning agents in a highly realistic simulation scenario with the aim to reduce Sim2Real gap. In addition to the realistic simulation model, LemgoRL encompasses a traffic signal logic unit that ensures compliance with all regulatory and safety requirements. LemgoRL offers the same interface as the well-known OpenAI gym toolkit to enable easy deployment in existing research work.	https://paperswithcode.com/dataset/lemgorl	30/03/2021						
3468	Chinese Treebank		https://paperswithcode.com/dataset/chinese-treebank							
3469	ShARe/CLEF 2014: Task 2 Disorders		https://paperswithcode.com/dataset/share-clef-2014-task-2-disorders	01/11/2013						
3470	THYME-2016		https://paperswithcode.com/dataset/thyme-2016	01/06/2016						
3471	LSARS	"In an active e-commerce environment, customers process a large number of reviews when deciding on whether to buy a product or not. Abstractive Multi-Review Summarization aims to assist users to efficiently consume the reviews that are the most relevant to them. We propose the first large-scale abstractive multi-review summarization dataset that leverages more than 17.9 billion raw reviews and uses novel aspect-alignment techniques based on aspect annotations. Furthermore, we demonstrate that one can generate higher-quality review summaries by using a novel aspect-alignment-based model. Results from both automatic and human evaluation show that the proposed dataset plus the innovative aspect-alignment model can generate high-quality and trustful review summaries.
Paper: Large Scale Abstractive Multi-Review Summarization (LSARS) via Aspect Alignment"	https://paperswithcode.com/dataset/lsars	07/04/2021	Large Scale Abstractive multi-Review Summarization					
3472	speechocean762	speechocean762 is an open-source speech corpus designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, word-level and phoneme-level. This corpus is allowed to be used freely for commercial and non-commercial purposes. To avoid subjective bias, each expert scores independently under the same metric	https://paperswithcode.com/dataset/speechocean762	03/04/2021						
3473	Time Series Prediction Benchmarks		https://paperswithcode.com/dataset/time-series-prediction-benchmarks		Time Series Prediction Benchmarks					
3474	Timers and Such	"Timers and Such is an open source dataset of spoken English commands for common voice control use cases involving numbers. The dataset has four intents, corresponding to four common offline voice assistant uses: SetTimer, SetAlarm, SimpleMath, and UnitConversion. The semantic label for each utterance is a dictionary with the intent and a number of slots. 
All recordings were converted from their original formats to single-channel 16,000 Hz .wav files."	https://paperswithcode.com/dataset/timers-and-such	04/04/2021						
3475	BS-RSCD	BS-RSCD is a dataset for rolling shutter correction and deblurring (RSCD). The dataset includes both ego-motion and object-motion in dynamic scenes. Real distorted and blurry videos with corresponding ground truth are recorded simultaneously via a beam-splitter-based acquisition system.	https://paperswithcode.com/dataset/bs-rscd	04/04/2021						
3476	SPGISpeech	"SPGISpeech (pronounced “speegie-speech”) is a large-scale transcription dataset, freely available for academic research. SPGISpeech is a collection of 5,000 hours of professionally-transcribed financial audio. Contrary to previous transcription datasets, SPGISpeech contains global english accents, strongly varying audio quality as well as both spontaneous and presentation style speech. The transcripts have each been cross-checked by multiple professional editors for high accuracy and are fully formatted including sentence structure and capitalization.
SPGISpeech consists of 5,000 hours of recorded company earnings calls and associated manual transcription text. The original calls were split based on silences into slices ranging from 5 to 15 seconds to allow easy training of a speech recognition system. The format of each WAV file is single channel, 16kHz, 16 bit audio.
Transcription text represents the output of several stages of manual post-processing. As such, the text contains polished English orthography following a detailed style guide, including proper casing, punctuation, and denormalized non-standard words such as numbers or acronyms, making SPGISpeech suited for training fully formatted end-to-end models.
In general, the transcriptions aim at professional utility rather than linguistic fidelity, and the correspondence between verbatim speech and finalized text is therefore not exact, resulting in the occasional purposeful omission of meeting operator instructions or certain verbal pleasantries."	https://paperswithcode.com/dataset/spgispeech	05/04/2021						
3477	PF-PASCAL		https://paperswithcode.com/dataset/pf-pascal	21/03/2017						
3478	PF-WILLOW		https://paperswithcode.com/dataset/pf-willow	21/03/2017						
3479	DogWhistle	Cant (also known as doublespeak, cryptolect, argot, anti-language or secret language) is important for understanding advertising, comedies and dog-whistle politics. DogWhistle is a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective.	https://paperswithcode.com/dataset/dogwhistle	06/04/2021						
3480	Summarizing Source Code using a Neural Attention Model	Presents a new dataset of code snippets with short descriptions, created using data gathered from Stackoverflow, a popular programming help website. Since access is open and unrestricted, the content is inherently noisy (ungrammatical, non-parsable, lacking content).	https://paperswithcode.com/dataset/summarizing-source-code-using-a-neural	01/08/2016						
3481	iPer	"iPer is a new dataset, with diverse styles of clothes in videos, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. There are 30 subjects of different conditions of shape, height, and gender. Each subject wears different clothes and performs an A-pose video and a video with random actions. There are 103 clothes in total. The whole dataset contains 206 video sequences with 241,564 frames.
Source: Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis"	https://paperswithcode.com/dataset/iper	18/11/2020	Impersonator					
3482	EasyCall	"EasyCall is a new dysarthric speech command dataset in Italian. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure. The corpus aims at providing a resource for the development of ASR-based assistive technologies for patients with dysarthria. In particular, it may be exploited to develop a voice-controlled contact application for commercial smartphones, aiming at improving dysarthric patients' ability to communicate with their family and caregivers. Before recording the dataset, participants were administered a survey to evaluate which commands are more likely to be employed by dysarthric individuals in a voice-controlled contact application. In addition, the dataset includes a list of non-commands (i.e., words near/inside commands or phonetically close to commands) that can be leveraged to build a more robust command recognition system.
Source: EasyCall corpus: a dysarthric speech dataset"	https://paperswithcode.com/dataset/easycall	06/04/2021						
3483	UFO Cherry Tree Point Clouds	"UFO Cherry Tree Point Clouds consists of a collection of 82 scanned Upright Fruiting Offshoot (UFO) cherry tree point clouds. 
Paper: Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning
Image source: Semantics-guided Skeletonization of Sweet Cherry Trees for Robotic Pruning"	https://paperswithcode.com/dataset/ufo-cherry-tree-point-clouds							
3484	PATS	"PATS dataset consists of a diverse and large amount of aligned pose, audio and transcripts. With this dataset, we hope to provide a benchmark that would help develop technologies for virtual agents which generate natural and relevant gestures.
Webpage
Scripts"	https://paperswithcode.com/dataset/pats	24/07/2020	Pose Audio Transcript Style					
3485	OCD	OCD (Out-of-Context Dataset) is a synthetic dataset with fine-grained control over scene context. The images are generated using a 3D simulation engine in the VirtualHome environment, which allows to control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment.	https://paperswithcode.com/dataset/ocd	06/04/2021	Out-of-Context Dataset					
3486	VGG-SS	VGG-SS (VGG Sound Source) is a benchmark for evaluating sound source localisation in videos. The dataset consists on a new set of annotations for the recently-introduced VGG-Sound dataset, where the sound sources visible in each video clip are explicitly marked with bounding box annotations. This dataset is 20 times larger than analogous existing ones, contains 5K videos spanning over 200 categories, and, differently from Flickr SoundNet, is video-based.	https://paperswithcode.com/dataset/vgg-ss	06/04/2021	VGG-Sound Source					
3487	RadarScenes	"RadarScenes is a real-world radar point cloud dataset for automotive applications.
It consists of measurements and point-wise annotations from more than four hours of driving collected by four series radar sensors mounted on one test vehicle. Individual detections of dynamic objects were manually grouped to clusters and labeled afterwards. The purpose of this data set is to enable the development of novel (machine learning-based) radar perception algorithms with the focus on moving road users. Images of the recorded sequences were captured using a documentary camera."	https://paperswithcode.com/dataset/radarscenes	06/04/2021						
3488	BiasCorp	BiasCorp is a dataset for racism detection containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube.	https://paperswithcode.com/dataset/biascorp	06/04/2021						
3489	GovReport	"GovReport is a dataset for long document summarization, with significantly longer documents and summaries. It consists of reports written by government research agencies including Congressional Research Service and U.S. Government Accountability Office.
Compared with other long document summarization datasets, government report dataset has longer summaries and documents and requires reading in more context to cover salient words to be summarized."	https://paperswithcode.com/dataset/govreport	05/04/2021						
3490	PartialSpoof	PartialSpoof is a dataset of partially-spoofed data to evaluate detection of partially-spoofed speech data. It has been built based on the ASVspoof 2019 LA database since the latter covers 17 types of spoofed data produced by advanced speech synthesizers, voice converters, and hybrids. The authors used the same set of bona fide data from the ASVspoof 2019 LA database but created partially spoofed audio from the ASVspoof 2019 LA data.	https://paperswithcode.com/dataset/partialspoof	06/04/2021						
3491	Movies and tropes, March 2020	"This dataset is a hash that uses as key the normalized movie name (for instance, {\sf TheAvengers} and as value an array of all the tropes used in that specific movie, as reported by TVTropes.org users.
Note: JSON scraped from tvtropes.org, containing the list of all movies and tropes used in them.
Image source: Tropes in films: an initial analysis"	https://paperswithcode.com/dataset/movies-and-tropes-march-2020	07/06/2020						
3492	Casual Conversations	"Casual Conversations dataset is designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions.
Casual Conversations is composed of over 45,000 videos (3,011 participants) and intended to be used for assessing the performance of already trained models in computer vision and audio applications for the purposes permitted in the data user agreement. The videos feature paid individuals who agreed to participate in the project and explicitly provided age and gender labels themselves. The videos were recorded in the U.S. with a diverse set of adults in various age, gender and apparent skin tone groups. A group of trained annotators labeled the participants’ apparent skin tone using the Fitzpatrick scale in addition to annotations of videos recorded in low ambient lighting conditions.
Source: Casual Conversations Dataset
Image source: Towards measuring fairness in AI: the Casual Conversations dataset"	https://paperswithcode.com/dataset/casual-conversations-dataset	06/04/2021						
3493	CalMS21	"The Caltech Mouse Social Interactions (CalMS21) dataset is a multi-agent dataset from behavioral neuroscience. The dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. The CalMS21 dataset is part of the Multi-Agent Behavior Challenge 2021.
To help accelerate behavioral studies, the CalMS21 dataset provides a benchmark to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabelled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of the dataset is to be able to classify behaviors accurately using both labelled and unlabelled tracking data, as well as being able to generalize to new annotators and behaviors."	https://paperswithcode.com/dataset/calms21	06/04/2021	Caltech Mouse Social Interactions					
3494	HumAID	"Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. 
HumAID is a large-scale dataset for crisis informatics research with ~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19 disaster events that happened between 2016 and 2019. The annotations in the provided datasets consists of following humanitarian categories. The dataset consists only english tweets and it is the largest dataset for crisis informatics so far.
Humanitarian categories:
* Caution and advice
* Displaced people and evacuations
* Don't know can't judge
* Infrastructure and utility damage
* Injured or dead people
* Missing or found people
* Not humanitarian
* Other relevant information
* Requests or urgent needs
* Rescue volunteering or donation effort
* Sympathy and support"	https://paperswithcode.com/dataset/humaid	07/04/2021	Human-Annotated Disaster Incidents Data					
3495	PlasticineLab	PasticineLab is a differentiable physics benchmark, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents.	https://paperswithcode.com/dataset/plasticinelab	07/04/2021						
3496	DFUC2021	The Diabetic Foot Ulcers dataset (DFUC2021) is a dataset for analysis of pathology, focusing on infection and ischaemia. The final release of DFUC2021 consists of 15,683 DFU patches, with 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The ground truth labels are four classes, i.e. control, infection, ischaemia and both conditions.	https://paperswithcode.com/dataset/dfuc2021	07/04/2021	Diabetic Foot Ulcers 2021					
3497	UAV-Human	UAV-Human is a large dataset for human behavior understanding with UAVs. It contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. The dataset was collected by a flying UAV in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and UAV flying attitudes. This dataset can be used for UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition.	https://paperswithcode.com/dataset/uav-human	02/04/2021						
3498	Criteo live traffic data	"Content of this dataset
This dataset includes following files:
README.md
criteo_attribution_dataset.tsv.gz: the dataset itself (623M compressed)
Experiments.ipynb: ipython notebook with code and utilities to reproduce the results in the paper. Can also be used as a starting point for further research on this data. It requires python 3.* and standard scientific libraries such as pandas, numpy and sklearn.
Data description
This dataset represents a sample of 30 days of Criteo live traffic data. Each line corresponds to one impression (a banner) that was displayed to a user. For each banner we have detailed information about the context, if it was clicked, if it led to a conversion and if it led to a conversion that was attributed to Criteo or not. Data has been sub-sampled and anonymized so as not to disclose proprietary elements.
Here is a detailed description of the fields (they are tab-separated in the file):
timestamp: timestamp of the impression (starting from 0 for the first impression). The dataset is sorted according to timestamp.
uid a unique user identifier
campaign a unique identifier for the campaign
conversion 1 if there was a conversion in the 30 days after the impression (independently of whether this impression was last click or not)
conversion_timestamp the timestamp of the conversion or -1 if no conversion was observed
conversion_id a unique identifier for each conversion (so that timelines can be reconstructed if needed). -1 if there was no conversion
attribution 1 if the conversion was attributed to Criteo, 0 otherwise
click 1 if the impression was clicked, 0 otherwise
click_pos the position of the click before a conversion (0 for first-click)
click_nb number of clicks. More than 1 if there was several clicks before a conversion
cost the price paid by Criteo for this display (disclaimer: not the real price, only a transformed version of it)
cpo the cost-per-order in case of attributed conversion (disclaimer: not the real price, only a transformed version of it)
time_since_last_click the time since the last click (in s) for the given impression
cat[1-9] contextual features associated to the display. Can be used to learn the click/conversion models. We do not disclose the meaning of these features but it is not relevant for this study. Each column is a categorical variable. In the experiments, they are mapped to a fixed dimensionality space using the Hashing Trick (see paper for reference).
Key figures
2,4Gb uncompressed
16.5M impressions
45K conversions
700 campaigns
Tasks
This dataset can be used in a large scope of applications related to Real-Time-Bidding, including but not limited to:
Attribution modeling: rule based, model based, etc…
Conversion modeling in display advertising: the data includes cost and value used for computing Utility metrics.
Offline metrics for real-time bidding"	https://paperswithcode.com/dataset/criteo-live-traffic-data	20/07/2017	Sample of 30 days of Criteo live traffic data					
3499	BSTC	"BSTC (Baidu Speech Translation Corpus) is a large-scale dataset for automatic simultaneous interpretation. BSTC version 1.0 contains 50 hours of real speeches, including three parts, the audio files, the transcripts, and the translations. The corpus can be used to build automatic simultaneous interpretation system.
The corpus is collected from the Chinese mandarin talks and reports, including science, technology, culture, economy, etc.,. The utterances in talks and reports are carefully transcribed into Chinese text, and further translated into English text. The sentence boundary is determined by the English text instead of the Chinese text which is analogous to the previous related corpus (TED and Translation Augmented LibriSpeech Corpus).
The corpus is divided into training/develop/test datasets. In each dataset, there are three types of files:
1. Acoustic signal files, which are named as baidu_XX.wav, where XX is the identical code. All signal files are encoded in Waveform Audio File Format (WAVE) from a mono recording, with a sample rate of 16K Hz, and a bit resolution of 16bits (2 bytes).
2. Description files, encoded in JSON format for each utterance, including the corresponding description information for each acoustic signal file, such as translation, transcript, duration, offset and so on.
Source: BSTC"	https://paperswithcode.com/dataset/bstc	08/04/2021	Baidu Speech Translation Corpus					
3500	READ 2016	"This dataset arises from the READ project (Horizon 2020).
The dataset consists of a subset of documents from the Ratsprotokolle collection composed of minutes of the council meetings held from 1470 to 1805 (about 30.000 pages), which will be used in the READ project. This dataset is written in Early Modern German. The number of writers is unknown. Handwriting in this collection is complex enough to challenge the HTR software.
The training dataset is composed of 400 pages; most of the pages consist of a single block with many difficulties for line detection and extraction. The ground-truth in this set is in PAGE format and it is provided annotated at line level in the PAGE files.
The previous dataset is the same that is located at https://zenodo.org/record/218236#.WnLhaCHhBGF
The new file includes the test set corresponding to the HTR competition held at ICFHR 2016
Toselli, A.H., Romero, V., Villegas, M., Vidal, E., & Sánchez, J.A. (2018). HTR Dataset ICFHR 2016 (Version 1.2.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.1297399"	https://paperswithcode.com/dataset/read-2016		HTR Dataset ICFHR 2016					
3501	RIMES	"The RIMES database (Reconnaissance et Indexation de données Manuscrites et de fac similÉS / Recognition and Indexing of handwritten documents and faxes) was created to evaluate automatic systems of recognition and indexing of handwritten letters. Of particular interest are cases such as those sent by postal mail or fax by individuals to companies or administrations.
The database was collected by asking volunteers to write handwritten letters in exchange of gift vouchers. Volunteer were given a fictional identity (same sex as the real one) and up to 5 scenarios. Each scenario has been chosen among 9 realistic following themes : change of personal information (address, bank account), information request, opening and closing (customer account), modification of contract or order, complaint (bad service quality…), payment difficulties (asking for a delay, tax exemption…), reminder letter, damage declaration with further circumstances and a destination (administrations or service providers (telephone, power, bank, insurances). The volunteers composed a letter with those pieces of information using their own words. The layout was free and it was only asked to use white paper and to write in a readable way with black ink.
The collect was a success with more than 1,300 people who have participated to the RIMES database creation by writing up to 5 mails. The RIMES database thus obtained contains 12,723 pages corresponding to 5605 mails of two to three pages."	https://paperswithcode.com/dataset/rimes		Reconnaissance & Indexation de données Manuscrites et de fac similÉS / Recognition & Indexing of handwritten documents & faxes					
3502	Twitter-MEL	Twitter-MEL is a multimodal entity linking (MEL) dataset built from Twitter. The dataset consists of tweets that had both text and images, with a total of 2.6M timeline tweets and 20k entities.	https://paperswithcode.com/dataset/twitter-mel	07/04/2021						
3503	PhoNER COVID19	PhoNER_COVID19 is a dataset for recognising COVID-19 related named entities in Vietnamese, consisting of 35K entities over 10K sentences. The authors defined 10 entity types with the aim of extracting key information related to COVID-19 patients, which are especially useful in downstream applications. In general, these entity types can be used in the context of not only the COVID-19 pandemic but also in other future epidemics.	https://paperswithcode.com/dataset/phoner-covid19	08/04/2021						
3504	CAMUS	"The goal of this project is to provide all the materials to the community to resolve the problem of echocardiographic image segmentation and volume estimation from 2D ultrasound sequences (both two and four-chamber views). To this aim, the following solutions were set-up
introduction of the largest publicly-available and fully-annotated dataset for 2D echocardiographic assessment (to our knowledge). The CAMUS dataset, containing 2D apical four-chamber and two-chamber view sequences acquired from 500 patients, is made available for download

deployement of a dedicated Girder online platform. This platform aims at assessing in a reproductible manner the performance of methods for the segmentation of cardiac structures (left ventricle endocardium and epicardium and left atrium borders) and the extraction of clinical indices (left ventricle volumes and ejection fraction).

The CAMUS online platform is now available and will be maintained and kept open as long as the data remains relevant for clinical research."	https://paperswithcode.com/dataset/camus		Cardiac Acquisitions for Multi-structure Ultrasound Segmentation					
3505	ORBIT	ORBIT is a real-world few-shot dataset and benchmark grounded in a real-world application of teachable object recognizers for people who are blind/low vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones, and the benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions.	https://paperswithcode.com/dataset/orbit	08/04/2021						
3506	DexYCB	"DexYCB is a dataset for capturing hand grasping of objects. It can be used three relevant tasks: 2D object and keypoint detection, 6D object pose estimation, and 3D hand pose estimation. 
The dataset was built using 20 objects from the YCB-Video dataset, and consists of multiple trials from 10 subjects. For each trial, there is a target object with 2 to 4 other objects placed on a table. The subject is asked to start from a relaxed pose, pick up the target object, and hold it in the air. Some subjects were asked to pretend to hand over the object to someone across from them. Each action is recorded for 3 seconds, repeating the trial 5 times for each target object, each time with a random set of accompanied objects and placement. In total there are 100 trials per subject, and 1,000 trials in total for all subjects.
Source: DexYCB: A Benchmark for Capturing Hand Grasping of Objects"	https://paperswithcode.com/dataset/dexycb	09/04/2021						
3507	FM2	"FoolMeTwice (FM2 for short) is a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using ""shortcuts"" compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players ""pay"" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks."	https://paperswithcode.com/dataset/fm2	10/04/2021	FoolMeTwice					
3508	ManyTypes4Py	ManyTypes4Py is a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files.	https://paperswithcode.com/dataset/manytypes4py	10/04/2021						
3509	EtymDB 2.0	A multilingual etymological database extracted from the Wiktionary (described in Methodological Aspects of Developing and Managing an Etymological Lexical Resource: Introducing EtymDB-2.0)	https://paperswithcode.com/dataset/etymdb-2-0	01/05/2020						
3510	ContraCAT	Current approaches to context-aware MT rely on a set of surface heuristics to translate pronouns, which break down when translations require real reasoning.  We create a new template test set ContraCAT to assess the ability of Machine Translation to handle the specific steps necessary for successful pronoun translation.	https://paperswithcode.com/dataset/contracat	06/12/2020	Contrastive Coreference Analytical Templates (for Machine Translation)					
3511	SynD	"SynD is a synthetic energy dataset with a focus on residential buildings. This dataset is the result of a custom simulation process that relies on power traces of household appliances. The output of simulations is the power consumption of 21 household appliances as well as the household-wide consumption (i.e. mains). Therefore, SynD's can be used for Non-Intrusive Load Monitoring, also referred to as Energy Disaggregation.
Source: A synthetic energy dataset for non-intrusive load monitoring in households"	https://paperswithcode.com/dataset/synd	11/03/2020	A Synthetic Energy Dataset for Non-Intrusive Load Monitoring in Households					
3512	Samanantar	Samanantar is the largest publicly available parallel corpora collection for Indic languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu. The corpus has 49.6M sentence pairs between English to Indian Languages.	https://paperswithcode.com/dataset/samanantar	12/04/2021						
3513	MindReader	MindReader is a novel dataset providing explicit user ratings over a knowledge graph within the movie domain. The latest stable version of the dataset contains 218,794 ratings from 2,316 users over 12,206 entities entities, and an associated knowledge graph consisting of 18,133 movie-related entities. The dataset is collected from an online movie recommendation game, MindReader, where users are pseudo-randomly asked to provide preferences for both movie- and non-movie entities (e.g., genres, actors, and directors). For each entity, users can either like it, dislike it, or state that they do not know it.	https://paperswithcode.com/dataset/mindreader	01/10/2020						
3514	WEC-Eng	WEC-eng is a cross-document event coreference resolution dataset extracted from English Wikipedia. Coreference links are not restricted within predefined topics. The training set includes 40,529 mentions distributed into 7,042 coreference clusters.	https://paperswithcode.com/dataset/wec-eng	11/04/2021						
3515	FreSaDa	FreSaDa is a French satire dataset for cross-domain satire detection, which is composed of 11,570 articles from the news domain. The dataset samples have been split into training, validation and test, such that the training publication sources are distinct from the validation and test publication sources. This gives rise to a cross-domain (cross-source) satire detection task.	https://paperswithcode.com/dataset/fresada	10/04/2021						
3516	L3DAS21	"L3DAS21 is a dataset for 3D audio signal processing. It consists of a 65 hours 3D audio corpus, accompanied with a Python API that facilitates the data usage and results submission stage. 
The LEDAS21 datasets contain multiple-source and multiple-perspective B-format Ambisonics audio recordings. The authors sampled the acoustic field of a large office room, placing two first-order Ambisonics microphones in the center of the room and moving a speaker reproducing the analytic signal in 252 fixed spatial positions. Relying on the collected Ambisonics impulse responses (IRs), the authors augmented existing clean monophonic datasets to obtain synthetic tridimensional sound sources by convolving the original sounds with our IRs. 
The dataset is divided in two main sections, respectively dedicated to the challenge tasks.
The first section is optimized for 3D Speech Enhancement and contains more than 30000 virtual 3D audio environments with a duration up to 10 seconds. In each sample, a spoken voice is always present alongside with other office-like background noises. As target data for this section the authors provide the clean monophonic voice signals.
The other sections, instead, is dedicated to the 3D Sound Event Localization and Detection task and contains 900 60-seconds-long audio files. Each data point contains a simulated 3D office audio environment in which up to 3 simultaneous acoustic events may be active at the same time. In this section, the samples are not forced to contain a spoken voice.  As target data for this section the authors provide a list of the onset and offset time stamps, the typology class, and the spatial coordinates of each individual sound event present in the data-points.
Source: L3DAS21"	https://paperswithcode.com/dataset/l3das21	12/04/2021						
3517	SI-Score	"SI-SCORE is a synthetic dataset for the analysis of robustness to object location, rotation and size. It consists of images that vary only for factors like object size and object location.
SI-SCORE was built by taking objects and backgrounds and systematically varying object size, location and rotation angle so that the effect of changing these factors on model performance can be studied."	https://paperswithcode.com/dataset/si-score-1	09/04/2021						
3518	RLU	"RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, we provide the datasets with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning.
In this suite of benchmarks, we try to focus on the following problems:
High dimensional action spaces, for example the locomotion humanoid domains, we have 56 dimensional actions.
High dimensional observations.
Partial observability, observations have egocentric vision.
Difficulty of exploration, using states of the art algorithms and imitation to generate data for difficult environments.
Real world challenges."	https://paperswithcode.com/dataset/caglar	24/06/2020	RL Unplugged					
3519	Multifog KITTI dataset	we propose the augmented KITTI dataset with fog for both camera and LiDAR sensors with different visibility ranges from 20 to 80 meters to best match realistic fog environment.	https://paperswithcode.com/dataset/multifog-kitti-dataset							
3520	OSIC Pulmonary Fibrosis Progression	"Imagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That’s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren’t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.
Current methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety—in addition to fibrosis-related symptoms—from the disease’s opaque path of progression.
Open Source Imaging Consortium (OSIC) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.
In this competition, you’ll predict a patient’s severity of decline in lung function based on a CT scan of their lungs. You’ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.
If successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments."	https://paperswithcode.com/dataset/osic-pulmonary-fibrosis-progression	13/04/2021	OSIC Pulmonary Fibrosis Progression: Predict lung function decline					
3521	QMSum	QMSum is a new human-annotated benchmark for query-based multi-domain meeting summarisation task, which consists of 1,808 query-summary pairs over 232 meetings in multiple domains.	https://paperswithcode.com/dataset/qmsum	13/04/2021						
3522	SVAMP	"A challenge set for elementary-level Math Word Problems (MWP). An MWP consists of a short Natural Language narrative that describes a state of the world and poses a question about some unknown quantities.
The examples in SVAMP test a model across different aspects of solving MWPs: 1) Is the model question sensitive? 2) Does the model have robust reasoning ability? 3) Is it invariant to structural alterations?"	https://paperswithcode.com/dataset/svamp	12/03/2021	Simple Variations on Arithmetic Math word Problems					
3523	SPARTQA	"SpartQA is a textual question answering benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state-of-the-art language models (LM).
SPARTQA is built on NLVR’s images containing more objects with richer spatial structures. SPARTQA’s stories are more natural, have more sentences, and richer in spatial relations in each sentence, and the questions require deeper reasoning and have four types: find relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine-grained analysis of models’ capabilities.
https://aclanthology.org/2021.naacl-main.364/"	https://paperswithcode.com/dataset/spartqa	12/04/2021	SPAtial Reasoning on Textual Question Answering					
3524	StylePTB	StylePTB is a fine-grained text style transfer benchmark. It consists of paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as compositions of multiple transfers which allow modelling of fine-grained stylistic changes as building blocks for more complex, high-level transfers.	https://paperswithcode.com/dataset/styleptb	12/04/2021						
3525	NorDial	NorDial is the first step to creating a corpus of dialectal variation of written Norwegian. It consists of small corpus of tweets manually annotated as Bokmål, Nynorsk, any dialect, or a mix.	https://paperswithcode.com/dataset/nordial	11/04/2021						
3526	FixMyPose	"FixMyPose is a dataset for automated pose correction. It consists of descriptions to correct a ""current"" pose to look like a ""target"" pose, in English and Hindi. The collected descriptions have interesting linguistic properties such as egocentric relations to environment objects, analogous references, etc., requiring an understanding of spatial relations and commonsense knowledge about postures. 
Further, to avoid ML biases, the dataset maintains a balance across characters with diverse demographics, who perform a variety of movements in several interior environments (e.g., homes, offices). 
This dataset introduces the pose-correctional-captioning task and its reverse target-pose-retrieval task. During the correctional-captioning task, models must generate descriptions of how to move from the current to target pose image, whereas in the retrieval task, models should select the correct target pose given the initial pose and correctional description."	https://paperswithcode.com/dataset/fixmypose	04/04/2021						
3527	AcinoSet	AcinoSet is a dataset of free-running cheetahs in the wild that contains 119,490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7,588 human-annotated frames. The authors utilized markerless animal pose estimation with DeepLabCut to provide 2D keypoints (in the 119K frames). It also includes 3D trajectories, human-checked 3D ground truth, and an interactive tool to inspect the data.	https://paperswithcode.com/dataset/acinoset	24/03/2021						
3528	Vietnamese intent detection and slot filling	This is a dataset for intent detection and slot filling for the Vietnamese language. The dataset consists of 5,871 gold annotated utterances with 28 intent labels and 82 slot types.	https://paperswithcode.com/dataset/vietnamese-intent-detection-and-slot-filling	05/04/2021						
3529	XFORMAL	XFORMAL is a multilingual formal style transfer benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian.	https://paperswithcode.com/dataset/xformal	08/04/2021						
3530	SSN	SSN (short for Semantic Scholar Network) is a scientific papers summarization dataset which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph.	https://paperswithcode.com/dataset/ssn	07/04/2021	Semantic Scholar Network					
3531	Global Wheat	Global WHEAT Dataset is the first large-scale dataset for wheat head detection from field optical images. It included a very large range of cultivars from differents continents. Wheat is a staple crop grown all over the world and consequently interest in wheat phenotyping spans the globe. Therefore, it is important that models developed for wheat phenotyping, such as wheat head detection networks, generalize between different growing environments around the world.	https://paperswithcode.com/dataset/global-wheat	25/04/2020	Global Wheat Head Dataset 2020					
3532	Brain-Score	"The Brain-Score platform aims to yield strong computational models of the ventral stream. We enable researchers to quickly get a sense of how their model scores against standardized brain benchmarks on multiple dimensions and facilitate comparisons to other state-of-the-art models. At the same time, new brain data can quickly be tested against a wide range of models to determine how well existing models explain the data.
Brain-Score is organized by the Brain-Score team in collaboration with researchers and labs worldwide. We are working towards an easy-to-use platform where a model can easily be submitted to yield its scores on a range of brain benchmarks and new benchmarks can be incorporated to challenge the models.
This quantified approach lets us keep track of how close our models are to the brain on a range of experiments (data) using different evaluation techniques (metrics). For more details, please refer to the technical paper and the perspective paper."	https://paperswithcode.com/dataset/brain-score	05/09/2018						
3533	ACDC Scribbles	"We release expert-made scribble annotations for the medical ACDC dataset 1. The released data must be considered as extending the original ACDC dataset.
The ACDC dataset contains cardiac MRI images, paired with hand-made segmentation masks. It is possible to use the segmentation masks provided in the ACDC dataset to evaluate the performance of methods trained using only scribble supervision. 
References: 
1 Bernard, Olivier, et al. ""Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?."" IEEE transactions on medical imaging 37.11 (2018): 2514-2525."	https://paperswithcode.com/dataset/acdc-scribbles	02/07/2020						
3534	Synthetic COVID-19 CXR Dataset	"A public open dataset of synthetic chest X-ray images of COVID-19.
The dataset consists of 21,295 synthetic COVID-19 chest X-ray images. Images are generated using an unsupervised domain adaptation approach by leveraging class conditioning and adversarial training from source datasets RSNA Kaggle Dataset and COVID-19 Image Data Collection. 
Implementation of the algorithm is available here."	https://paperswithcode.com/dataset/synthetic-covid-19-cxr-dataset	20/10/2020						
3535	Twitter Stance Election 2020	"The data set contains 2500 manually-stance-labeled tweets, 1250 for each candidate (Joe Biden and Donald Trump). These tweets were sampled from the unlabeled set that our research team collected English tweets related to the 2020 US Presidential election. Through the Twitter Streaming API, the authors collected data using election-related hashtags and keywords. Between January 2020 and September 2020, over 5 million tweets were collected, not including quotes and retweets.
Paper: Knowledge Enhanced Masked Language Model for Stance Detection"	https://paperswithcode.com/dataset/twitter-stance-election-2020	26/05/2021						
3536	A2D Sentences	"The Actor-Action Dataset (A2D) by Xu et al. [29] serves as the largest video dataset for the general actor and action segmentation task. It contains 3,782 videos from YouTube with pixel-level labeled actors and their actions. The dataset includes eight different actions, while a total of seven actor classes are considered to perform those actions. We follow [29], who split the dataset into 3,036 training videos and 746 testing videos. 
As we are interested in pixel-level actor and action segmentation from sentences, we augment the videos in A2D with natural language descriptions about what each actor is doing in the videos.  Following the guidelines set forth
in 12, we ask our annotators for a discriminative referring expression of each actor instance if multiple objects are considered in a video. The annotation process resulted in a total of 6,656 sentences, including 811 different nouns, 225 verbs and 189 adjectives. Our sentences enrich the actor and action pairs from the A2D dataset with finer granularities. For example, the actor adult in A2D may be annotated with man, woman, person and player in our sentences, while action rolling may also refer to flipping, sliding, moving and running when describing different actors in different scenarios. Our sentences contain on average more words than the ReferIt dataset 12 (7.3 vs 4.7), even when we leave out prepositions, articles and linking verbs (4.5 vs 3.6). This makes sense as our sentences contain a variety of verbs while existing referring expression datasets mostly ignore verbs."	https://paperswithcode.com/dataset/a2d-sentences	20/03/2018	Sentences for the Actor-Action Dataset (A2D)					
3537	NewsCLIPpings	NewsCLIPpings is a dataset for detecting mismatched images and captions. Different to previous misinformation datasets, in NewsCLIPpings both the images and captions are unmanipulated, but some of them are mismatched.	https://paperswithcode.com/dataset/newsclippings	13/04/2021						
3538	Countix-AV	"Countix-AV is a dataset for repetitive action counting by sight and sound created by repurposing the Countix dataset.
It is created by selecting 19 categories from Countix for which the repetitive action has a clear sound, such as clapping, playing tennis, etc. The dataset contains 1,863 videos, with 987, 311 and 565 for training, validation and testing.
The authors maintained the original count annotations from Countix and kept the same split (i.e. training, validation, or testing) for each video."	https://paperswithcode.com/dataset/countix-av	24/03/2021						
3539	Referring Expressions for DAVIS 2016 & 2017	"Our task is to localize and provide a pixel-level mask of an object on all video frames given a language referring expression obtained either by looking at the first frame only or the full video. To validate our approach we employ two popular video object segmentation datasets, DAVIS16 [38] and DAVIS17 [42]. These two datasets introduce various challenges, containing videos with single or multiple salient objects, crowded scenes, similar looking instances, occlusions, camera view changes, fast motion, etc.
DAVIS16 [38] consists of 30 training and 20 test videos of diverse object categories with all frames annotated with pixel-level accuracy. Note that in this dataset only a single object is annotated per video. For the multiple object video segmentation task we consider DAVIS17. Compared to DAVIS16, this is a more challenging dataset, with multiple objects annotated per video and more complex scenes with more distractors, occlusions, smaller objects, and fine structures. Overall, DAVIS17 consists of a training set with 60 videos, and a validation/test-dev/test-challenge set with 30 sequences each. 
As our goal is to segment objects in videos using language specifications, we augment all objects annotated with mask labels in DAVIS16 and DAVIS17 with non-ambiguous referring expressions. We follow the work of [34] and ask the annotator to provide a language description of the object, which has a mask annotation, by looking only at the
first frame of the video. Then another annotator is given the first frame and the corresponding description, and asked to identify the referred object. If the annotator is unable to correctly identify the object, the description is corrected to remove ambiguity and to specify the object uniquely. We have collected two referring expressions per target
object annotated by non-computer vision experts (Annotator 1, 2).
However, by looking only at the 1st frame, the obtained referring expressions may potentially be invalid for an entire video. (We actually quantified that only∼ 15% of the
collected descriptions become invalid over time and it does not affect strongly segmentation results as temporal consistency step helps to disambiguate some of such cases, see the supp. material for details.) Besides, in many applications, such as video editing or video-based advertisement, the user has access to a full video. Providing a language
query which is valid for all frames might decrease the editing time and result in more coherent predictions. Thus, on DAVIS17 we asked the workers to provide a description of the object by looking at the full video. We have collected one expression of the full video type per target object. Future work may choose to use either setting.
The average length for the first frame/full video expressions is 5.5/6.3 words. For DAVIS17 first frame annotations we notice that descriptions given by Annotator 1 are longer than the ones by Annotator 2 (6.4 vs. 4.6 words). We evaluate the effect of description length on the grounding performance in §5. Besides, the expressions relevant to a full video mention verbs more often than the first frame descriptions (44% vs. 25%). This is intuitive, as referring to an object which changes its appearance and position over time may require mentioning its actions. Adjectives are present in over 50% for all annotations. Most of them refer to colors (over 70%), shapes and sizes (7%) and spatial/ordering words (6% first frame vs. 13% full video expressions). The full video expressions also have a higher number of adverbs and prepositions, and overall are more complex than the ones provided for the first frame.
Overall augmented DAVIS16/17 contains ∼ 1.2k referring expressions for more than 400 objects on 150 videos with ∼ 10k frames. We believe the collected data will be
of interest to segmentation as well as vision and language communities, providing an opportunity to explore language as alternative input for video object segmentation."	https://paperswithcode.com/dataset/referring-expressions-for-davis-2016-2017	21/03/2018						
3540	IIIT-ILST	IIIT-ILST is a dataset and benchmark for scene text recognition for three Indic scripts - Devanagari, Telugu and Malayalam. IIIT-ILST contains nearly 1000 real images per each script which are annotated for scene text bounding boxes and transcriptions.	https://paperswithcode.com/dataset/iiit-ilst	09/04/2021						
3541	A2Dre	We obtain A2Dre by selecting only instances that were labeled as non-trivial, which are 433 REs from 190 videos. We do not use the trivial cases as the analysis of such examples is not relevant, as referents can be described by using the category alone. Each annotator was presented with a RE, a video in which the target object was marked by a bounding box, and a set of questions paraphrasing our categories. A2Dre was annotated by 3 authors of the paper. Our final set of category annotations used for analysis was derived by means of majority voting: for each nontrivial RE, we kept all category labels which were assigned to the RE by at least two annotators.	https://paperswithcode.com/dataset/a2d-referring-expressions	01/10/2020	Subset of A2D Sentences which are not trivial					
3542	A2Dre+	"A2Dre is a subset from the A2D test set including $433$~\textit{non-trivial} REs. Due to its highly unbalanced distribution across the $7$~semantic categories we select the $4$~major categories \textsl{appearance, location, motion and static}. The four categories have in common that in most cases, for a given referent, a RE can be provided that expresses a certain category, and one that does not. We use these categories to augment A2Dre with additional REs, which vary according to the presence or absence of each of them. Specifically, based on our categorization of the original REs, for each RE~$re$ and category~$C$, we produce an additional RE~$re'$ by modifying $re$ slightly such that it does (or does not) express~$C$.  For example, for the last RE in  Figure~\ref{fig:a2d-images}, i.e. \emph{girl in yellow dress standing near the woman}, which could be categorized as \textit{appearance}, \textit{location}, no \textit{motion} and \textit{static}, we produce new REs for each category: \emph{girl standing near the woman} (no \textit{appearance}), \emph{girl in yellow dress standing} (no \textit{location}), \emph{girl in yellow dress walking} (\textit{motion}) and \emph{girl in yellow dress near the woman} (no \textit{static}). 
We do not apply this procedure for \textsl{category}, since it is expressed in almost all REs, and its removal may be difficult in many cases.  We name this extended dataset as A2Dre+."	https://paperswithcode.com/dataset/a2dre	01/10/2020	Extension of A2D sentences where trivial cases where filtered					
3543	RGB-D-D	RGB-D-D is a large-scale dataset for depth map super-resolution (SR). It consists of real-world paired low-resolution (LR) and high-resolution (HR) depth maps. The paired LR and HR depth maps are captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes.	https://paperswithcode.com/dataset/rgb-d-d	13/04/2021						
3544	WikiEvents	WikiEvents is a document-level event extraction benchmark dataset which includes complete event and coreference annotation.	https://paperswithcode.com/dataset/wikievents	13/04/2021						
3545	RaindropsOnWindshield	RaindropsOnWindshield is a dataset for training and assessing vision algorithms' performance for different tasks of image artifacts detection on either camera lens or windshield. The dataset contains 8190 images, of which 3390 contain raindrops. Images are annotated with the binary mask representing areas with raindrops.	https://paperswithcode.com/dataset/raindropsonwindshield	11/04/2021						
3546	How2Sign	"One of the factors that have hindered progress in the areas of sign language recognition, translation, and production is the absence of large annotated datasets. 
Towards this end, we introduce How2Sign, a multimodal and multiview continuous American Sign Language (ASL) dataset, consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio
enabling detailed 3D pose estimation."	https://paperswithcode.com/dataset/how2sign	18/08/2020	A Large-scale Multimodal Dataset for Continuous American Sign Language					
3547	TNL2K	"Tracking by Natural Language (TNL2K) is constructed for the evaluation of tracking by natural language specification. TNL2K features:


Large-scale: 2,000 sequences, contains 1,244,340 frames, 663 words, 1300 / 700 for the train / testing respectively 


High-quality: Manual annotation with careful inspection in each frame 


Multi-modal: Providing visual and language annotation for each sequence 


Adversarial-samples: Randomly adding adversarial samples for research on adversarial attack and defence 


Significant-appearance-variation:  Containing videos with cloth/face change for pedestrian 


Heterogeneous: Containing RGB, thermal, Cartoon,  Synthetic data 


Multiple-baseline: Tracking-by-BBox, Tracking-by-Language, Tracking-by-Joint-BBox-Language


Source: Towards More Flexible and Accurate Object Tracking with Natural Language:  Algorithms and Benchmark"	https://paperswithcode.com/dataset/tnl2k	31/03/2021	Tracking by natural language					
3548	ElBa	"ElBa is composed of procedurally-generated realistic renderings, where we vary in a continuous way element
shapes and colors and their distribution, to generate 30K texture images with different local symmetry, stationarity, and density of (3M) localized texels, whose attributes are thus known by construction.
Download"	https://paperswithcode.com/dataset/elba	29/08/2019	ElBa: Element Based Textures Dataset					
3549	MS^2	MS^2 (Multi-Document Summarization of Medical Studies) is a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain.	https://paperswithcode.com/dataset/ms-2	13/04/2021	Multi-Document Summarization of Medical Studies					
3550	CarFusion	We provide manual annotations of 14 semantic keypoints for 100,000 car instances (sedan, suv, bus, and truck) from 53,000 images captured from 18 moving cameras at Multiple intersections in Pittsburgh, PA. Please fill the google form to get a email with the download links:	https://paperswithcode.com/dataset/vehicle-pose-estimation	01/06/2018						
3551	Subjective Discourse	This is a discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents.  The dataset consists of witness testimonials in U.S. congressional hearings.	https://paperswithcode.com/dataset/subjective-discourse	09/04/2021						
3552	WMT19 Metrics Task	"This shared task will examine automatic evaluation metrics for machine translation. The goals of the shared metrics task are:
To achieve the strongest correlation with human judgement of translation quality;
To illustrate the suitability of an automatic evaluation metric as a surrogate for human evaluation;
To address problems associated with comparison with a single reference translation;
To move automatic evaluation beyond system-level ranking to finer-grained sentence-level ranking.
All datasets for this task are available here."	https://paperswithcode.com/dataset/wmt19-metrics-task	01/08/2019						
3553	ML-CB	"In this paper, we develop a new privacy enhancing tool: ML-CB—a means of using distinguishable pictorial information combined with underlying website source code to produce accurate and robust machine learning classifiers able to discern fingerprinting (i.e., surreptitious tracking) from non-fingerprinting canvas-based actions.
The data introduced in the paper is collected by scraping roughly half a million websites using a custom Google Chrome extension storing information related to the canvas.
Source: ML-CB: Machine Learning Canvas Block"	https://paperswithcode.com/dataset/ml-cb	12/04/2021	ML-CB: Machine Learning Canvas Block					
3554	Eedi Dataset	"The Eedi dataset contains from two school years (September 2018 to May 2020) of students’ answers to mathematics questions from Eedi, a leading educational platform which millions of students interact with daily around the globe. Eedi offers diagnostic questions to students from primary to high school (roughly between 7 and 18 years old). Each diagnostic question is a multiple-choice question with 4 possible answer choices, exactly one of which is correct. Currently, the platform mainly focuses on mathematics questions.
The data is split for different tasks: 1 & 2: Answer prediction, 3: Predict question quality, and 4: Recommend questions.
The total number of answer records for these tasks training sets exceeds 17 million, making it one of the largest educational datasets to date. We also provide extensive metadata on questions, students and answers."	https://paperswithcode.com/dataset/eedi-dataset	08/04/2021						
3555	KolektorSDD2	"KolektorSDD2 is a surface-defect detection dataset with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem.
The dataset consists of:

356 images with visible defects
2979 images without any defect
image sizes of approximately 230 x 630 pixels
train set with 246 positive and 2085 negative images
test set with 110 positive and 894 negative images
several different types of defects (scratches, minor spots, surface imperfections, etc.)"	https://paperswithcode.com/dataset/kolektorsdd2	13/04/2021	Kolektor Surface-Defect Dataset 2					
3556	Quasimodo	"Quasimodo is commonsense knowledge base that focuses on salient properties of objects. We provide several subsets:

Positive statements only
Positive statements top 10%


Negated statements only
Occupations
Positive statements
Negative statements


Animals
Positive statements
Negative statements


Culture
Positive statements
Negative statements


ConceptNet-mapped statements

Image source: https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/commonsense/quasimodo"	https://paperswithcode.com/dataset/quasimodo	27/05/2019						
3557	HO-3D	A hand-object interaction dataset with 3D pose annotations of hand and object. The dataset contains 66,034 training images and 11,524 test images from a total of 68 sequences. The sequences are captured in multi-camera and single-camera setups and contain 10 different subjects manipulating 10 different objects from YCB dataset. The annotations are automatically obtained using an optimization algorithm. The hand pose annotations for the test set are withheld and the accuracy of the algorithms on the test set can be evaluated with standard metrics using the CodaLab challenge submission(see project page). The object pose annotations for the test and train set are provided along with the dataset.	https://paperswithcode.com/dataset/ho-3d	02/07/2019						
3558	DogFaceNet	A dog face dataset for dog face verification and recognition/identification.	https://paperswithcode.com/dataset/dogfacenet	28/08/2019						
3559	Retailrocket	The dataset consists of three files: a file with behaviour data (events.csv), a file with item properties (itemproperties.сsv) and a file, which describes category tree (categorytree.сsv). The data has been collected from a real-world ecommerce website. It is raw data, i.e. without any content transformations, however, all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback.	https://paperswithcode.com/dataset/retailrocket							
3560	Ulm-TSST	Ulm-TSST is a dataset continuous emotion (valence and arousal) prediction and `physiological-emotion' prediction. It consists of a multimodal richly annotated dataset of self-reported, and external dimensional ratings of emotion and mental well-being. After a brief period of preparation the subjects are asked to give an oral presentation, within a job-interview setting.  Ulm-TSST includes biological recordings, such as Electrocardiogram (ECG),  Electrodermal Activity (EDA), Respiration, and Heart Rate (BPM) as well as continuous arousal and valence annotations. With 105 participants (69.5% female) aged between 18 and 39 years, a total of 10 hours were accumulated.	https://paperswithcode.com/dataset/ulm-tsst	14/04/2021	Ulm-Trier Social Stress Dataset					
3561	OmniFlow	OmniFlow is a synthetic omnidirectional human optical flow dataset. Based on a rendering engine the authors created a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. The dataset consists of 23,653 image pairs and corresponding forward and backward optical flow.	https://paperswithcode.com/dataset/omniflow	16/04/2021						
3562	hERG	hERG is a large-scale biophysics federated molecular dataset related to cardiac toxicity. It consists of 10,572 compounds, with an average of 29.39 nodes and 94.09 edges in each graph.	https://paperswithcode.com/dataset/herg	14/04/2021						
3563	RTC	"RTC is a benchmark corpus of social media comments sampled over three years. The corpus consists of 36.36m unlabelled comments for adaptation and evaluation on an upstream masked language modelling task as well as 0.9m labelled comments for finetuning and evaluation on a downstream document classification task. 
The Reddit Time Corpus (RTC) covers three years between March 2017 and February 2020 and is split into 36 evenly-sized monthly subsets based on comment timestamps. RTC is sampled from the Pushshift Reddit dataset."	https://paperswithcode.com/dataset/rtc	16/04/2021	Reddit Time Corpus					
3564	Follicular-Segmentation	"The Follicular-Segmentation dataset consists of 6900 cropped typical image patches of 1024x1024 pixels containing: follicular areas, colloid areas, and the other blank background areas.
Image source: https://github.com/bupt-ai-cz/Hybrid-Model-Enabling-Highly-Efficient-Follicular-Segmentation"	https://paperswithcode.com/dataset/follicular-segmentation	28/04/2021						
3565	Semantic Textual Similarity (2012 - 2016)	"Semantic Textual Similarity (2012 - 2016) involves a set of semantic textual similarity datasets that were part of previous shared tasks (2012-2016):
STS12 -  Semeval-2012 task 6: A pilot on semantic textual similarity
STS13 - SEM 2013 shared task: Semantic Textual Similarity
STS14 - SemEval-2014 task 10: Multilingual semantic textual similarity
STS15 - SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability
STS16 - SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation"	https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016		STS					
3566	JUSThink Dialogue and Actions Corpus	"The information contained in JUSThink Dialogue and Actions Corpus dataset includes dialogue transcripts, event logs, and test responses of children aged 9 through 12, as they participate in a robot-mediated human-human collaborative learning activity named JUSThink, where children in teams of two solve a problem on graphs together.
The dataset consists of three parts:

transcripts: anonymised dialogue transcripts for 10 teams of two children
logs: anonymised event logs for 39 teams of two children
test responses: pre-test and post-test responses for 39 teams, and the key i.e. the correct response"	https://paperswithcode.com/dataset/justhink-dialogue-and-actions-corpus	09/04/2021						
3567	MediaSpeech	"MediaSpeech is a media speech dataset (you might have guessed this) built with the purpose of testing Automated Speech Recognition (ASR) systems performance. The dataset consists of short speech segments automatically extracted from media videos available on YouTube and manually transcribed, with some pre- and post-processing. The dataset contains 10 hours of speech for each language provided. This release contains audio datasets in French, Arabic, Turkish and Spanish, and is a part of a larger private dataset.
Source: MediaSpeech: Multilanguage ASR Benchmark and Dataset"	https://paperswithcode.com/dataset/mediaspeech	30/03/2021						
3568	NISQA Speech Quality Corpus	"The NISQA Corpus includes more than 14,000 speech samples with simulated (e.g. codecs, packet-loss, background noise) and live (e.g. mobile phone, Zoom, Skype, WhatsApp) conditions. Each file is labelled with subjective ratings of the overall quality and the quality dimensions Noisiness, Coloration, Discontinuity, and Loudness. In total, it contains more than 97,000 human ratings for each of the dimensions and the overall MOS.
The NISQA Speech Quality Corpus contains two training, two validation and four test datasets:      

NISQA_TRAIN_SIM and NISQA_VAL_SIM: contains simulated distortions with speech samples from four different datasets. Divided into a training and a validation set.
NISQA_TRAIN_LIVE and NISQA_VAL_LIVE: contains live phone and Skype recordings with Librivox audiobook samples. Divided into training and validation set.
NISQA_TEST_LIVETALK: contains recordings of real phone and VoIP calls.
NISQA_TEST_FOR: contains live and simulated conditions with speech samples from the forensic speech dataset.
NISQA_TEST_NSC: contains live and simulated conditions with speech samples from the NSC dataset.
NISQA_TEST_P501: contains live and simulated conditions with speech samples from ITU-T Rec. P.501.

The datasets are provided under the original terms of the used source speech and noise samples. Please see the individual readme and license files in each of the dataset folders within the NISQA_Corpus.zip for more details about the datasets and the licenses. Generally, all of the files in this corpus can be used for non-commercial research purposes and some of the datasets can be also be used for commercial purposes."	https://paperswithcode.com/dataset/nisqa-speech-quality-corpus	19/04/2021						
3569	IBM Debater Mention Detection Benchmark	"This dataset contains general and named entities annotations on both clean written text and on noisy speech data. 
It includes 1000 sentences from Wikipedia and 1000 sentences of speech data that appear in two forms: (1) transcribed manually, and (2) the output of an ASR engine. 
Each of the datasets includes a total of around 6500 mentions linked to there DBPedia pages."	https://paperswithcode.com/dataset/ibm-debater-mention-detection-benchmark	23/01/2018						
3570	HopeEDI	Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff’s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness.	https://paperswithcode.com/dataset/hopeedi	01/12/2020	HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion					
3571	Apolloscape Trajectory	Our trajectory dataset consists of camera-based images, LiDAR scanned point clouds, and manually annotated trajectories. It is collected under various lighting conditions and traffic densities in Beijing, China. More specifically, it contains highly complicated traffic flows mixed with vehicles, riders, and pedestrians.	https://paperswithcode.com/dataset/apolloscape-trajectory	06/11/2018						
3572	Apolloscape Inpainting	The Inpainting dataset consists of synchronized Labeled image and LiDAR scanned point clouds. It's captured by HESAI Pandora All-in-One Sensing Kit. It is collected under various lighting conditions and traffic densities in Beijing, China.	https://paperswithcode.com/dataset/apolloscape-inpainting	17/07/2020						
3573	GooAQ	GooAQ is a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to the collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections.	https://paperswithcode.com/dataset/gooaq	18/04/2021						
3574	DiS-ReX	DiS-ReX is a multilingual dataset for distantly supervised (DS) relation extraction (RE). The dataset has over 1.5 million instances, spanning 4 languages (English, Spanish, German and French). The dataset has 36 positive relation types + 1 no relation (NA) class.	https://paperswithcode.com/dataset/dis-rex	17/04/2021						
3575	Concadia	Concadia is a publicly available Wikipedia-based corpus, which consists of 96,918 images with corresponding English-language descriptions, captions, and surrounding context.	https://paperswithcode.com/dataset/concadia	16/04/2021						
3576	XLEnt	XLEnt consists of parallel entity mentions in 120 languages aligned with English. These entity pairs were constructed by performing named entity recognition (NER) and typing on English sentences from mined sentence pairs. These extracted English entity labels and types were projected to the non-English sentences through word alignment. Word alignment was performed by combining three alignment signals ((1) word co-occurence alignment with FastAlign (2) semantic alignment using LASER embeddings, and (3) phonetic alignment via transliteration) into a unified word-alignment model. This lexical/semantic/phonetic alignment approach yielded more than 160 million aligned entity pairs in 120 languages paired with English. Recognizing that each English is often aligned to mulitple entities in different target languages, we can join on English entities to obtain aligned entity pairs that directly pair two non-English entities (e.g., Arabic-French)	https://paperswithcode.com/dataset/xlent	17/04/2021						
3577	NFCorpus	NFCorpus is a full-text English retrieval data set for Medical Information Retrieval. It contains a total of 3,244 natural language queries (written in non-technical English, harvested from the NutritionFacts.org site) with 169,756 automatically extracted relevance judgments for 9,964 medical documents (written in a complex terminology-heavy language), mostly from PubMed.	https://paperswithcode.com/dataset/nfcorpus							
3578	CQADupStack	CQADupStack is a benchmark dataset for community question-answering research. It contains threads from twelve StackExchange subforums, annotated with duplicate question information. Pre-defined training and test splits are provided, both for retrieval and classification experiments, to ensure maximum comparability between different studies using the set. Furthermore, it comes with a script to manipulate the data in various ways.	https://paperswithcode.com/dataset/cqadupstack							
3579	SciFact	SciFact is a dataset of 1.4K expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and rationales.	https://paperswithcode.com/dataset/scifact	30/04/2020						
3580	Co/FeMn bilayers	Co/FeMn bilayers measured.	https://paperswithcode.com/dataset/co-femn-bilayers		Co/FeMn bilayers					
3581	BoostCLIR	"BoostCLIR is a bilingual (Japanese-English) corpus of patent abstracts, extracted from the MAREC patent data, and the data from the NTCIR PatentMT workshop collections, accompanied with relevance judgements for the task of patent prior-art search.
Important: The English side of the corpus contains patent IDs as well as the text of the abstracts. The Japanese side only contains patent IDs because of NTCIR copyright restrictions. The Japanese patent abstracts can be extracted from full text Japanese patent documents, which are available from the organizers of the NTCIR workshop."	https://paperswithcode.com/dataset/boostclir	01/10/2013						
3582	ConferenceVideoSegmentationDataset	"This is a video and image segmentation dataset for human head and shoulders, relevant for creating elegant media for videoconferencing and virtual reality applications. The source
data includes ten online conference-style green screen videos. The authors extracted 3600 frames from the videos and generated the ground truth masks for each character in the video, and then applied virtual background to the frames to generate the training/testing sets."	https://paperswithcode.com/dataset/conferencevideosegmentationdataset	20/04/2021						
3583	DeCOCO	DeCOCO is a bilingual (English-German) corpus of image descriptions, where the English part is extracted from the COCO dataset, and the German part are translations by a native German speaker.	https://paperswithcode.com/dataset/decoco	15/01/2016						
3584	HumanMT	"HumanMT is a collection of human ratings and corrections of machine translations. It consists of two parts: The first part contains five-point and pairwise sentence-level ratings, the second part contains error markings and corrections. Details are described in the following.
I. Sentence-level ratings
This is a collection of five-point and pairwise ratings for 1000 German-English machine translations of TED talks (IWSLT 2014). The ratings were collected with the purpose of assessing machine translation quality rating reliability and learnability to improve a neural machine translation model with human reinforcement (see publications).
II. Error markings and corrections
This is a collection of word-level error markings and post-edits/corrections for 3120 English-German machine translated sentences of 30 selected TED talks (IWSLT 2017). Each sentence received either a correction or a marking of errors from human annotators. This data was collected with the purpose of comparing annotation cost and quality, and potential for downstream machine translation improvements between annotation modes (see publications).
Source: HumanMT: Human Machine Translation Ratings"	https://paperswithcode.com/dataset/humanmt	27/05/2018						
3585	MVP	MVP is a multi-view partial point cloud dataset (MVP) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model.	https://paperswithcode.com/dataset/mvp	20/04/2021	Multi-View Partial point cloud					
3586	MetaCLIR	This data adds textual meta-infomation data to two existing corpora for cross language information retrieval: BoostCLIR, and the Large Scale CLIR Dataset (wiki-clir).	https://paperswithcode.com/dataset/metaclir	30/10/2020						
3587	WiTA	WiTA (Writing in The Air) is a dataset for the challenging writing in the air (WiTA) task -- an elaborate task bridging vision and NLP. The dataset consists of five sub-datasets in two languages (Korean and English) and amounts to 209,926 video instances from 122 participants. Finger movement for WiTA is captured with RGB cameras to ensure wide accessibility and cost-efficiency.	https://paperswithcode.com/dataset/wita	19/04/2021	Writing in The Air					
3588	Large-Scale CLIR Dataset	"The Large-Scale CLIR Dataset is a retrieval dataset built for Cross-Language Information Retrieval (CLIR).
The dataset is derived from Wikipedia and contains more 2.8 million English single-sentence queries with relevant documents from 25 other selected languages."	https://paperswithcode.com/dataset/large-scale-clir-dataset	01/06/2018						
3589	NLmaps	"There are two versions of the NLmaps corpus. NLmaps (v1) and its extension NLmaps v2. Both versions of the NLmaps corpus consist of questions about geographical facts that can be answered with the OpenStreetMap (OSM) database (available under the Open Database Licence). The questions are in English and have a corresponding Machine Readable Language (MRL) parse. Gold answers can be obtained by executing the gold parses against the OSM database using the NLmaps backend, which is based on the Overpass-API (available under the Affero GPL v3).
Source: NLmaps"	https://paperswithcode.com/dataset/nlmaps	01/12/2016						
3590	SciGen	"SciGen is a challenge dataset for the task of reasoning-aware data-to-text generation consisting of tables from scientific articles and their corresponding descriptions.  The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is therefore the first dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, i.e., tables from scientific articles. SciGen opens new avenues for future research in reasoning-aware text generation and evaluation.
The dataset consists of 1.3K pairs of tables with their descriptions, with an average of 53 cells in each table."	https://paperswithcode.com/dataset/scigen	16/04/2021						
3591	PatTR	"PatTR is a sentence-parallel corpus extracted from the MAREC patent collection. The current version contains more than 22 million German-English and 18 million French-English parallel sentences collected from all patent text sections as well as 5 million German-French sentence pairs from patent titles, abstracts and claims.
Source: PatTR"	https://paperswithcode.com/dataset/pattr	01/04/2012	Patent Translation Resource					
3592	WikiCaps	"WikiCaps is a large-scale multilingual but non-parallel data set for multimodal machine translation and retrieval. The image-caption data was extracted from Wikimedia Commons and is thus a representative of the collection of largely available non-descriptive image-caption pairs in the web. The current version of the dataset contains 3,816,940 images with 3,825,132 English captions and additional 1,000 image-caption pairs in German, French, and Russian together with their English counterparts.
Source: WikiCaps"	https://paperswithcode.com/dataset/wikicaps	01/03/2018						
3593	WikiCLIR	"WikiCLIR is a large-scale (German-English) retrieval data set for Cross-Language Information Retrieval (CLIR). It contains a total of 245,294 German single-sentence queries with 3,200,393 automatically extracted relevance judgments for 1,226,741 English Wikipedia articles as documents. Queries are well-formed natural language sentences that allow large-scale training of (translation-based) ranking models.
Source:"	https://paperswithcode.com/dataset/wikiclir	01/06/2014						
3594	CPM-Synt-1	CPM-Synt-1 is a dataset consisting of 5555 images with synthesis - makeup images with pattern segmentation mask	https://paperswithcode.com/dataset/cpm-synt-1	05/04/2021	Color-Pattern-Makeup Synthesis 1					
3595	CPM-Synt-2	CPM-Synt-2 is a dataset consisting of 1625 images with synthesis - triplets: makeup, non-makeup, ground-truth.	https://paperswithcode.com/dataset/cpm-synt-2	05/04/2021	Color-Pattern-Makeup Synthesis 2					
3596	Stickers	"Stickers is a dataset consisting of 577 high-quality sticker images with alpha channel.
Image source: https://github.com/VinAIResearch/CPM"	https://paperswithcode.com/dataset/stickers	05/04/2021						
3597	CPM-Real	"CPM-Real is a dataset consisting of 3895 images representing real - makeup styles.
Image source: https://github.com/VinAIResearch/CPM"	https://paperswithcode.com/dataset/cpm-real	05/04/2021						
3598	BTAD	The BTAD ( beanTech Anomaly Detection) dataset is a real-world industrial anomaly dataset. The dataset contains a total of 2830 real-world images of 3 industrial products showcasing body and surface defects.	https://paperswithcode.com/dataset/btad	20/04/2021	beanTech Anomaly Detection					
3599	COPA-HR	"The COPA-HR dataset (Choice of plausible alternatives in Croatian) is a translation of the English COPA dataset by following the XCOPA dataset translation methodology. The dataset consists of 1000 premises (My body cast a shadow over the grass), each given a question (What is the cause?), and two choices (The sun was rising; The grass was cut), with a label encoding which of the choices is more plausible given the annotator or translator (The sun was rising).
Source: Choice of plausible alternatives dataset in Croatian COPA-HR"	https://paperswithcode.com/dataset/copa-hr	19/04/2021						
3600	CASP13 MQA	CASP13 MQA is a dataset that contains predicted models for CASP13 targets and their scores.	https://paperswithcode.com/dataset/casp13							
3601	ACID	"ACID consists of thousands of aerial drone videos of different coastline and nature scenes on YouTube. Structure-from-motion is used to get camera poses.
Image source: https://arxiv.org/pdf/2012.09855v2.pdf"	https://paperswithcode.com/dataset/acid	17/12/2020	Aerial Coastline Imagery Dataset					
3602	KazakhTTS	KazakhTTS is an open-source speech synthesis dataset for Kazakh, a low-resource language spoken by over 13 million people worldwide. The dataset consists of about 91 hours of transcribed audio recordings spoken by two professional speakers (female and male). It is the first publicly available large-scale dataset developed to promote Kazakh text-to-speech (TTS) applications in both academia and industry.	https://paperswithcode.com/dataset/kazakhtts	17/04/2021						
3603	italki NLI	"A large, crowd-sourced dataset for the Native Language Identification (NLI) task. People learning English as a second language write practice Notebooks which can be used to classify the author's native language using word choice, spelling mistakes and other language features.
The dataset has:

11 languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telagu, Turkish)
111,917 documents"	https://paperswithcode.com/dataset/italki-nli	10/12/2018	italki NLI					
3604	Toronto NeuroFace Dataset	"Toronto NeuroFace Dataset: A New Dataset for Facial Motion Analysis in Individuals with Neurological Disorders
Toronto NeuroFace Dataset is a public dataset with videos of oro-facial gestures performed by individuals with oro-facial impairment due to neurological disorders, such as amyotrophic lateral sclerosis (ALS) and stroke. Perceptual clinical scores from trained clinicians are provided as metadata. Manual annotation of facial landmarks is also provided for a subset of over 3300 frames."	https://paperswithcode.com/dataset/toronto-neuroface-dataset	01/04/2021						
3605	GE852	GE852 is a dataset of 852 game engine repositories mined from GitHub in two languages, namely Java and C++. The dataset contains metadata of all the mined repositories including commits, pull requests, issues and so on. This dataset can lays the foundation for empirical investigation in the area of game engines.	https://paperswithcode.com/dataset/ge852	11/05/2019						
3606	Newer College	"The Newer College Dataset is a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College, Oxford. The dataset includes data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Additionally, the authors used a tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing ∼290 million points). 
Using the map the authors inferred centimeter-accurate 6 Degree of Freedom (DoF) ground truth for the position of the device for each LiDAR scan to enable better evaluation of LiDAR and vision localisation, mapping and reconstruction systems. The dataset combines both built environments, open spaces and vegetated areas so as to test localization and mapping systems such as vision-based navigation, visual and LiDAR SLAM, 3D LIDAR reconstruction and appearance-based place recognition."	https://paperswithcode.com/dataset/newer-college	12/03/2020						
3607	VideoSet	"VideoSet is a large-scale compressed video quality dataset based on just-noticeable-difference (JND) measurement.
The dataset consists of 220 5-second sequences in four resolutions (i.e., 1920×1080, 1280×720, 960×540 and 640×360). Each of the 880 video clips is encoded using the H.264 codec with QP=1,⋯,51 and measure the first three JND points with 30+ subjects. The dataset is called the ""VideoSet"", which is an acronym for ""Video Subject Evaluation Test (SET)""."	https://paperswithcode.com/dataset/videoset	15/01/2017						
3608	Cable TV News	Cable TV news is a data set of nearly 24/7 video, audio, and text captions from three U.S. cable TV networks (CNN, FOX, and MSNBC) from January 2010 to July 2019. Using machine learning tools, the authors detect faces in 244,038 hours of video, label each face's presented gender, identify prominent public figures, and align text captions to audio.	https://paperswithcode.com/dataset/cable-tv-news	14/10/2020						
3609	AISHELL-3	AISHELL-3 is a large-scale and high-fidelity multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-to-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers and total 88035 utterances. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Accordingly, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. The  word & tone transcription accuracy rate is above 98%, through professional speech annotation and strict quality inspection for tone and prosody.	https://paperswithcode.com/dataset/aishell-3	22/10/2020						
3610	Election2020	Election2020 is a Twitter dataset on the 2020 US presidential elections. To facilitate the understanding of political discourse and try to empower the Computational Social Science research community, the authors decided to publicly release this massive-scale, longitudinal dataset of U.S. politics- and election-related tweets. This multilingual dataset encompasses hundreds of millions of tweets and tracks all salient U.S. politics trends, actors, and events between 2019 and 2020. It predates and spans the whole period of Republican and Democratic primaries, with real-time tracking of all presidential contenders of both sides of the isle. After that, it focuses on presidential and vice-presidential candidates. The dataset release is curated, documented and will be constantly updated on a weekly-basis, until the November 3, 2020 election and beyond.	https://paperswithcode.com/dataset/election2020	01/10/2020						
3611	Wikipedia Citations	Wikipedia Citations is a comprehensive dataset of citations extracted from Wikipedia. A total of 29.3M citations were extracted from 6.1M English Wikipedia articles as of May 2020, and classified as being to books, journal articles or Web contents. We were thus able to extract 4.0M citations to scholarly publications with known identifiers -- including DOI, PMC, PMID, and ISBN -- and further equip an extra 261K citations with DOIs from Crossref. As a result, we find that 6.7% of Wikipedia articles cite at least one journal article with an associated DOI, and that Wikipedia cites just 2% of all articles with a DOI currently indexed in the Web of Science.	https://paperswithcode.com/dataset/wikipedia-citations	14/07/2020						
3612	OMD	"The Oxford Multimotion Dataset (OMD) provides a number of multimotion estimation problems of varying complexity. It includes both complex problems that challenge existing algorithms as well as a number of simpler problems to support development. These include observations from both static and dynamic sensors, a varying number of moving bodies, and a variety of different 3D motions. It also provides a number of experiments designed to isolate specific challenges of the multimotion problem, including rotation about the optical axis and occlusion.
In total, the Oxford Multimotion Dataset contains over 110 minutes of multimotion data consisting of stereo and RGB-D camera images, IMU data, and Vicon ground-truth trajectories. The dataset culminates in a complex toy car segment representative of many challenging real-world scenarios."	https://paperswithcode.com/dataset/omd	13/06/2019	Oxford Multimotion Dataset					
3613	Cry Wolf	"Cry Wolf is a dataset for cyber security analysis tasks. It is an open-access dataset of 73 true and false Intrusion Detection System (IDS) alarms derived from real-world examples of ""impossible travel"" scenarios."	https://paperswithcode.com/dataset/cry-wolf	24/02/2020						
3614	IITM-Bandersnatch	"IITM-Bandersnatch is a dataset to evaluate traffic analysis techniques. The dataset comprises of data points of the form {encrypted traces, ground truth choices}. To collect each data point, we asked the viewer to watch Bandersnatch from the beginning and note down the choices made by them. At the same time, we collected the encrypted network traffic.
As of now, our dataset contains information corresponding to 100 viewers who volunteered for this study."	https://paperswithcode.com/dataset/iitm-bandersnatch	15/03/2019						
3615	Hate Counter	This dataset is built from Twitter and contains 1290 hate tweet and counterspeech reply pairs. After the annotation process, the dataset consists of 558 unique hate tweets from 548 user and 1290 counterspeech replies from 1239 users.	https://paperswithcode.com/dataset/hate-counter	06/12/2018						
3616	Alexa Domains	"This dataset is composed of the URLs of the top 1 million websites.
The domains are ranked using the Alexa traffic ranking which
is determined using a combination of the browsing behavior
of users on the website, the number of unique visitors, and the number of pageviews. In more detail, unique visitors are the
number of unique users who visit a website on a given day,
and pageviews are the total number of user URL requests for
the website. However, multiple requests for the same website
on the same day are counted as a single pageview. The website
with the highest combination of unique visitors and pageviews
is ranked the highest"	https://paperswithcode.com/dataset/gagan-bhatia	14/11/2019						
3617	ChestX-Det	ChestX-Det is a chest X-Ray dataset with instance-level annotations (boxes and masks). ChestX-Det is a subset of the public dataset NIH ChestX-ray14. It contains ~3500 images of 13 common disease categories labeled by three board-certified radiologists.	https://paperswithcode.com/dataset/chestx-det	21/04/2021						
3618	MAI	MAI is a dataset for multi-scene recognition in single aerial images. It consists of 3,923 labelled large-scale images from Google Earth imagery that covers the United States, Germany, and France. The size of each image is 512 ×512, and spatial resolutions vary from 0.3 m/pixel to 0.6 m/pixel. After capturing aerial images, multiple scene-level labels were manually assigned to each image from in total 24 scene categories, including apron, baseball, beach, commercial, farmland, woodland, parking lot, port, residential, river, storage tanks, sea, bridge, lake, park, roundabout, soccer field, stadium, train station, works, golf course, runway, sparse shrub, and tennis court	https://paperswithcode.com/dataset/mai	22/04/2021	Multi-scene Aerial Image					
3619	Event-Human3.6m	Event-Human3.6m is a challenging dataset for event-based human pose estimation by simulating events from the RGB Human3.6m dataset. It is built by converting the RGB recordings of Human3.6m into events and synchronising raw joints ground-truth with events frames through interpolation.	https://paperswithcode.com/dataset/event-human3-6m	21/04/2021						
3620	MLDS	MLDS is a collection of thousands of trained neural networks labelled with the data used to train them. MLDS allows meta weight-space analysis across thousands of networks trained with identical or similar training data.	https://paperswithcode.com/dataset/mlds	21/04/2021	Machine Learning Datasets					
3621	CaseHOLD	"CaseHOLD (Case Holdings On Legal Decisions) is a law dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). The citing context from the judicial decision serves as the prompt for the question. The answer choices are holding statements derived from citations following text in a legal decision. There are five answer choices for each citing text. The correct answer is the holding statement that corresponds to the citing text. The four incorrect answers are other holding statements.
To read more about the dataset, please see our paper or our blogpost."	https://paperswithcode.com/dataset/casehold	18/04/2021	Case Holdings On Legal Decisions					
3622	Hateful Users on Twitter	This is a Twitter dataset of 100,386 users along with up to 200 tweets from their timelines with a random-walk-based crawler on the retweet graph, with a subsample of 4,972 which is manually annotated as hateful or not through crowdsourcing. The dataset can be used to examine the difference between user activity patterns, the content disseminated between hateful and normal users, and network centrality measurements in the sampled graph.	https://paperswithcode.com/dataset/hateful-users-on-twitter	14/01/2018						
3623	Alibaba Cluster Trace	"Alibaba Cluster Trace captures detailed statistics for the co-located workloads of long-running and batch jobs over a course of 24 hours. The trace consists of three parts: (1) statistics of the studied homogeneous cluster of 1,313 machines, including each machine’s hardware configuration, and the runtime {CPU, Memory, Disk} resource usage for a duration of 12 hours (the 2nd half of the 24-hour period); (2) long-running job workloads, including a trace of all container deployment requests and actions, and a resource usage trace for 12 hours; (3) co-located batch job workloads, including a trace of all batch job requests and actions, and a trace of per-instance resource usage over 24 hours.
It also has a second version of traces cluster-trace-v2018 that includes about 4,000 machines in a period of 8 days. Besides having larger scaler than trace-v2017, this piece trace also contains the DAG information of the production batch workloads."	https://paperswithcode.com/dataset/alibaba-cluster-trace	08/08/2018						
3624	Continuous Defect Prediction	Continuous Defect Prediction (CDP) is a dataset of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data mined from software repositories. The dataset embraces 1,265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset the authors used TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories.	https://paperswithcode.com/dataset/continuous-defect-prediction	22/06/2017						
3625	BB-MAS	"BB-MAS is a behavioural biometrics dataset. It consists of data collected from 117 subjects for typing (both fixed and free text), gait (walking, upstairs and downstairs) and touch on Desktop, Tablet and Phone. The dataset consists a total of about: 3.5 million keystroke events; 57.1 million data-points for accelerometer and gyroscope each; 1.7 million data-points for swipes; and enables future research to explore previously unexplored directions in inter-device and inter-modality biometrics. 
Image Source: SU-AIS BB-MAS (SYRACUSE UNIVERSITY AND ASSURED INFORMATION SECURITY - BEHAVIORAL BIOMETRICS MULTI-DEVICE AND MULTI-ACTIVITY DATA FROM SAME USERS) DATASET"	https://paperswithcode.com/dataset/bb-mas	19/12/2019	Behavioural Biometrics Multi-device and multi-Activity data from Same users					
3626	UK-DALE	UK-DALE is an open-access dataset from the UK recording Domestic Appliance-Level Electricity to conduct research on disaggregation algorithms, with data describing not just the aggregate demand per building but also the `ground truth' demand of individual appliances. It was built at a sample rate of 16 kHz for the whole-house and at 1/6 Hz for individual appliances. This is the first open access UK dataset at this temporal resolution. It wAS recorded from five houses, one of which was recorded for 655 days.	https://paperswithcode.com/dataset/uk-dale	19/03/2015						
3627	RAE	The Rainforest Automation Energy (RAE) dataset was create to help smart grid researchers test their algorithms which make use of smart meter data. This initial release of RAE contains 1Hz data (mains and sub-meters) from two residential houses. In addition to power data, environmental and sensor data from the house's thermostat is included. Sub-meter data from one of the houses includes heat pump and rental suite captures which is of interest to power utilities.	https://paperswithcode.com/dataset/rae	12/02/2018	Rainforest Automation Energy					
3628	Dataset of Dockerfiles	This dataset of approximately 178,000 unique Dockerfiles collected from GitHub to facilitate sophisticated semantics-aware static analysis of Dockerfiles. To enhance the usability of this data, the authors use five representations for working with, mining from, and analyzing these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and the final representation, created by three levels of nested parsing and abstraction, makes tasks such as mining and static checking tractable.	https://paperswithcode.com/dataset/dataset-of-dockerfiles	28/03/2020						
3629	LIDDI	LInked Drug-Drug Interactions (LIDDI) is a public nanopublication-based RDF dataset with trusty URIs that encompasses some of the most cited prediction methods and sources to provide researchers a resource for leveraging the work of others into their prediction methods. As one of the main issues to overcome the usage of external resources is their mappings between drug names and identifiers used, the dataset also provides the set of mappings the authors curated to be able to compare the multiple sources aggregated in the dataset.	https://paperswithcode.com/dataset/liddi	20/07/2015	LInked Drug-Drug Interactions					
3630	UofTPed50	"UofTPed50 is an object detection and tracking dataset which uses GPS to ground truth the position and velocity of a pedestrian.
It can be used for benchmarking the positional accuracy of 3D pedestrian detection. It contains accurate positioning information by attaching a GPS system to the pedestrian itself. This dataset consists of 50 sequences of varying distance, pedestrian trajectory, and ego-vehicle trajectory. Each sequence contains one pedestrian. The scenarios are broken into four groups."	https://paperswithcode.com/dataset/uoftped50	21/05/2019						
3631	Credibility Factors 2020	"This dataset focuses on 50 articles about climate science, which were annotated completely by 49 students, 26 Upwork workers, 3 science and 3 journalism experts.
Before participation, crowd raters filled out a demographic survey followed by committing to an Annotator Code of Conduct of performing their duties in as accurate and diligent a manner as possible provided in their informed consent. Once eligible, they received reading and rating tasks that included their credibility perception per article on a 5-point Likert scale, ranging from very low (1) to very high (5). Instructions to fill out the seven question survey across a 7-10 day period (estimated at 10 hours) were provided in a handbook with a recommended limit of 10-15 minutes per article.
Articles chosen were written in English and represented a range of liberal to conservative positions or attitudes towards climate problems. To gather articles, the team used the Buzzsumo social media research tool in late 2018 to find the most popular articles over the previous year with the keywords of “climate change,” “global warming,” “environment,” and “pollution.” Among the top results, our team selected a set of articles with varying amounts of scientific reference."	https://paperswithcode.com/dataset/credibility-factors-2020	21/08/2020						
3632	Acticipate	Acticipate is a publicly available dataset with recordings of human body-motion and eye-gaze, acquired in an experimental scenario with an actor interacting with three subjects. It contains synchronised and labelled video+gaze and body motion in a dyadic scenario of interaction.	https://paperswithcode.com/dataset/acticipate	10/08/2018						
3633	Human Optical Flow	A synthetic data of videos of human action sequences and the corresponding optical flow.	https://paperswithcode.com/dataset/coma-1	14/06/2018	Human Optical Flow dataset					
3634	Overruling	"The Overruling dataset is a law dataset corresponding to the task of determining when a sentence is overruling a prior decision. This is a binary classification task, where positive examples are overruling sentences and negative examples are non-overruling sentences extracted from legal opinions. In law, an overruling sentence is a statement that nullifies a previous case decision as a precedent, by a constitutionally valid statute or a decision by the same or higher ranking court which establishes a different rule on the point of law involved. The Overruling dataset consists of 2,400 sentences.
To read more about the dataset, please see our paper or our blogpost."	https://paperswithcode.com/dataset/overruling	18/04/2021						
3635	Terms of Service	The Terms of Service dataset is a law dataset corresponding to the task of identifying whether contractual terms are potentially unfair. This is a binary classification task, where positive examples are potentially unfair contractual terms (clauses) from the terms of service in consumer contracts. Article 3 of the Directive 93/13 on Unfair Terms in Consumer Contracts defines an unfair contractual term as follows. A contractual term is unfair if: (1) it has not been individually negotiated; and (2) contrary to the requirement of good faith, it causes a significant imbalance in the parties rights and obligations, to the detriment of the consumer. The Terms of Service dataset consists of 9,414 examples.	https://paperswithcode.com/dataset/terms-of-service	03/05/2018						
3636	WebQuestionsSP	"The WebQuestionsSP dataset is released as part of our ACL-2016 paper “The Value of Semantic Parse Labeling for Knowledge Base Question Answering” [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013]. The WebQuestionsSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and “partial” annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer. This release also includes an evaluation script and the output of the STAGG semantic parsing system when trained using the full semantic parses. More detail can be found in the document and labeling instructions included in this release, as well as the paper.
Source: WebQuestions Semantic Parses Dataset"	https://paperswithcode.com/dataset/webquestionssp	01/08/2016	WebQuestions Semantic Parses Dataset					
3637	NBA SportVU	The NBA SportVU dataset contains player and ball trajectories for 631 games from the 2015-2016 NBA season. The raw tracking data is in the JSON format, and each moment includes information about the identities of the players on the court, the identities of the teams, the period, the game clock, and the shot clock.	https://paperswithcode.com/dataset/nba-sportvu							
3638	TCGA		https://paperswithcode.com/dataset/tcga		The Cancer Genome Atlas					
3639	robo-vln	"The Robo-VLN dataset is a continuous control formulation of the VLN-CE dataset by Krantz et al ported over from Room-to-Room (R2R) dataset created by Anderson et al. The details regarding converting discrete VLN dataset into continuous control formulation can be found in our paper. 
| Dataset   | Path to extract               | Size      |
|-------------- |----------------------------   |-------    |
| robo_vln_v1.zip     | data/datasets/robo_vln_v1           | 76.9 MB   |
Robo-VLN Dataset
The dataset robo_vln_v1 contains the train, val_seen, and val_unseen splits. 

train: 7739 episodes
val_seen: 570 episodes
val_unseen: 1224 episodes

Format of {split}.json.gz
{
    'episodes' = [
        {
            'episode_id': 4991,
            'trajectory_id': 3279,
            'scene_id': 'mp3d/JeFG25nYj2p/JeFG25nYj2p.glb',
            'instruction': {
                'instruction_text': 'Walk past the striped area rug...',
                'instruction_tokens': [2384, 1589, 2202, 2118, 133, 1856, 9]
            },
            'start_position': [10.257800102233887, 0.09358400106430054, -2.379739999771118],
            'start_rotation': [0, 0.3332950713608026, 0, 0.9428225683587541],
            'goals': [
                {
                    'position': [3.360340118408203, 0.09358400106430054, 3.07817006111145], 
                    'radius': 3.0
                }
            ],
            'reference_path': [
                [10.257800102233887, 0.09358400106430054, -2.379739999771118], 
                [9.434900283813477, 0.09358400106430054, -1.3061100244522095]
                ...
                [3.360340118408203, 0.09358400106430054, 3.07817006111145],
            ],
            'info': {'geodesic_distance': 9.65537166595459},
        },
        ...
    ],
    'instruction_vocab': [
        'word_list': [..., 'orchids', 'order', 'orient', ...],
        'word2idx_dict': {
            ...,
            'orchids': 1505,
            'order': 1506,
            'orient': 1507,
            ...
        },
        'itos': [..., 'orchids', 'order', 'orient', ...],
        'stoi': {
            ...,
            'orchids': 1505,
            'order': 1506,
            'orient': 1507,
            ...
        },
        'num_vocab': 2504,
        'UNK_INDEX': 1,
        'PAD_INDEX': 0,
    ]
}
* Format of {split}_gt.json.gz
{
    '4991': {
        'actions': [
          ...
          [-0.999969482421875, 1.0],
          [-0.9999847412109375, 0.15731772780418396],
          ...
          ],
        'forward_steps': 325,
        'locations': [
            [10.257800102233887, 0.09358400106430054, -2.379739999771118],
            [10.257800102233887, 0.09358400106430054, -2.379739999771118],
            ...
            [-12.644463539123535, 0.1518409252166748, 4.2241311073303220]
        ]
    }
    ...
}"	https://paperswithcode.com/dataset/robo-vln	21/04/2021	Robotics Vision-and-Language Navigation					
3640	2devs	2devs is a publicly available dataset of fine-grained untangled code changes collected by recording the development sessions of two developers over the course of four months, and the corresponding manual clustering.	https://paperswithcode.com/dataset/2devs	24/02/2015						
3641	ToN_IoT	"The TON_IoT datasets are new generations of Internet of Things (IoT) and Industrial IoT (IIoT) datasets for evaluating the fidelity and efficiency of different cybersecurity applications based on Artificial Intelligence (AI). The datasets have been called ‘ToN_IoT’ as they include heterogeneous data sources collected from Telemetry datasets of IoT and IIoT sensors, Operating systems datasets of Windows 7 and 10 as well as Ubuntu 14 and 18 TLS and Network traffic datasets. The datasets were collected from a realistic and large-scale network designed at the IoT Lab of the UNSW Canberra Cyber, the School of Engineering and Information technology (SEIT), UNSW Canberra @ the Australian Defence Force Academy (ADFA). 
The datasets were gathered in a parallel processing to collect several normal and cyber-attack events from IoT networks. A new testbed was developed at the IoT lab to connect many virtual machine, physical systems, hacking platforms, cloud and fog platforms, IoT and IIoT sensors to mimic the complexity and scalability of industrial IoT and Industry 4.0 networks.
Different hacking techniques, such as DoS, DDoS and ransomware against, were launched against web applications, IoT gateways and computer systems across the IIoT network."	https://paperswithcode.com/dataset/ton-iot	04/10/2020						
3642	APND	"APND (Arm Point Nav Dataset) is a dataset for the generalizable object manipulation task called ARMPOINTNAV, which consists on moving an object in the scene from a source location to a target location.
The dataset consists of 30 kitchen scenes in AI2-THOR that include more than 150 object categories (69 interactable object categories) with a variety of shapes, sizes and textures."	https://paperswithcode.com/dataset/apnd	22/04/2021	Arm Point Nav Dataset					
3643	Mid-level perceptual musical features	"This dataset contains annotations for 5000 music files on the following music properties:

Melodiousness
Articulation
Rhythmic stability
Rhythmic complexity
Dissonance
Tonal stability
Modality

The annotations were given by musicians and collected through a crowd-sourcing platform (Toloka)."	https://paperswithcode.com/dataset/mid-level-perceptual-musical-features	13/06/2018						
3644	Warblr	"Warblr is a dataset for the acoustic detection of birds. The dataset comes from a UK bird-sound crowdsourcing research spinout called Warblr. From this initiative the authors collected over 10,000 ten-second smartphone audio recordings from around the UK. The audio totals around 28 hours duration.
The audio covers a wide distribution of UK locations and environments, and includes weather noise, traffic noise, human speech and even human bird imitations. It is directly representative of the data that is collected from a mobile crowdsourcing initiative. Annotations of the Warblr dataset are performed by a network of volunteers."	https://paperswithcode.com/dataset/warblr	11/08/2016						
3645	JoCAD	JoCAD is a dataset for anomaly detection in citation networks.	https://paperswithcode.com/dataset/jocad	28/05/2020						
3646	Hand Poses	This is a dataset for benchmarking in-hand manipulation on different robot platforms.	https://paperswithcode.com/dataset/hand-poses	09/01/2020						
3647	CD18		https://paperswithcode.com/dataset/cd18	09/07/2020	Cellphone Dataset with 18 Features					
3648	BEIR	"BEIR (Benchmarking IR) is an heterogeneous benchmark containing different information retrieval (IR) tasks. Through BEIR, it is possible to systematically study the zero-shot generalization capabilities of multiple neural retrieval approaches.
The benchmark contains a total of 9 information retrieval tasks (Fact Checking, Citation Prediction, Duplicate Question Retrieval, Argument Retrieval, News Retrieval, Question Answering, Tweet Retrieval, Biomedical IR, Entity Retrieval) from 17 different datasets:

MS MARCO
TREC-COVID
NFCorpus
BioASQ
Natural Questions
HotpotQA
FiQA-2018
Signal-1M
TREC-News
ArguAna
Touche 2020
CQADupStack
Quora Question Pairs
DBPedia
SciDocs
FEVER
Climate-FEVER
SciFact"	https://paperswithcode.com/dataset/beir	17/04/2021	Benchmarking IR					
3649	KUMC	The KUMC dataset for polyp detection and classification was collected from the University of Kansas Medical Center. It contains 80 colonoscopy video sequences which are manually labeled with bounding boxes as well as the polyp classes for the entire dataset.	https://paperswithcode.com/dataset/kumc	22/04/2021						
3650	SumeCzech-NER	SumeCzech-NER contains named entity annotations of SumeCzech 1.0, a Czech news-based summarization dataset.	https://paperswithcode.com/dataset/sumeczech-ner	21/04/2021						
3651	FIQA	"The growing maturity of Natural Language Processing (NLP) techniques and resources is drastically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The financial domain, with its dependency on the interpretation of multiple unstructured and structured data sources and with its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques. This challenge, FIQA, focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain. 
Task 1: Aspect-based financial sentiment analysis
Task 2: Opinion-based QA over financial data
Source: FIQA"	https://paperswithcode.com/dataset/fiqa-1		Financial Opinion Mining and Question Answering					
3652	Signal-1M Related Tweets	"Signal-1M Related Tweets consists of A TREC-like data collection to evaluate approaches for the task of related-tweet retrieval for news articles.
Learn more about the data collection process here."	https://paperswithcode.com/dataset/signal-1m-related-tweets							
3653	MTST	"The Mobile Turkish Scene Text (MTST 200) dataset consists of 200 indoor and outdoor Turkish scene text images.
The images were collected with mobile phones and downsized to 576 × 1024 (portrait) or 1024 × 576 (landscape) pixels. The text lines are horizontal or near horizontal, some with slight in– and out-of-plane rotations."	https://paperswithcode.com/dataset/mtst	17/08/2016	Mobile Turkish Scene Text					
3654	LHQ	A dataset of 90,000 high-resolution nature landscape images, crawled from Unsplash and Flickr and preprocessed with Mask R-CNN and Inception V3.	https://paperswithcode.com/dataset/lhq	14/04/2021	Landscapes High-Quality					
3655	SemEval2017	DOI: 10.18653/v1/S17-2091	https://paperswithcode.com/dataset/semeval2017	15/08/2017						
3656	Inspec	"Paper: Improved automatic keyword extraction given more linguistic knowledge
Doi: 10.3115/1119355.1119383"	https://paperswithcode.com/dataset/inspec	11/07/2003						
3657	Essays	J. W. Pennebaker and L. A. King, “Linguistic styles: Language use as an individual difference,” J. Pers. Soc. Psychol., vol. 77, no. 6, pp. 1296–1312, Dec. 1999, doi: 10.1037/0022-3514.77.6.1296.	https://paperswithcode.com/dataset/essays		Stream-of-consciousness Essays					
3658	MIT Indoor Scenes	"Context
This is the Original data provided by MIT .
Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g., bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information.
Content
The database contains 67 Indoor categories, and a total of 15620 images. The number of images varies across categories, but there are at least 100 images per category. All images are in jpg format. The images provided here are for research purposes only.
Acknowledgements
Thanks to MIT
Thanks to Aude Oliva for helping to create the database of indoor scenes.
Funding for this research was provided by NSF Career award (IIS 0747120)"	https://paperswithcode.com/dataset/mit-indoors-scenes							
3659	Intel Image Classification	"Context
This is image data of Natural Scenes around the world.
Content
This Data contains around 25k images of size 150x150 distributed under 6 categories.
{'buildings' -> 0,
'forest' -> 1,
'glacier' -> 2,
'mountain' -> 3,
'sea' -> 4,
'street' -> 5 }
The Train, Test and Prediction data is separated in each zip files. There are around 14k images in Train, 3k in Test and 7k in Prediction.
This data was initially published on https://datahack.analyticsvidhya.com by Intel to host a Image classification Challenge.
Acknowledgements
Thanks to https://datahack.analyticsvidhya.com for the challenge and Intel for the Data
Photo by Jan Böttinger on Unsplash
Inspiration
Want to build powerful Neural network that can classify these images with more accuracy."	https://paperswithcode.com/dataset/intel-image-classification							
3660	UPFD	"For benchmarking, please refer to its variant UPFD-POL and UPFD-GOS.
The dataset has been integrated with Pytorch Geometric (PyG) and Deep Graph Library (DGL). You can load the dataset after installing the latest versions of PyG or DGL. 
The UPFD dataset includes two sets of tree-structured graphs curated for evaluating binary graph classification, graph anomaly detection, and fake/real news detection tasks. The dataset is dumped in the form of Pytorch-Geometric dataset object. You can easily load the data and run various GNN models using PyG.
The dataset includes fake&real news propagation (retweet) networks on Twitter built according to fact-check information from Politifact and Gossipcop.
The news retweet graphs were originally extracted by FakeNewsNet.
Each graph is a hierarchical tree-structured graph where the root node represents the news; the leaf nodes are Twitter users who retweeted the root news.
A user node has an edge to the news node if he/she retweeted the news tweet. Two user nodes have an edge if one user retweeted the news tweet from the other user. 
We crawled near 20 million historical tweets from users who participated in fake news propagation in FakeNewsNet to generate node features in the dataset.
We incorporate four node feature types in the dataset, the 768-dimensional bert and 300-dimensional spacy features 
are encoded using pretrained BERT and spaCy word2vec, respectively.
The 10-dimensional profile feature is obtained from a Twitter account's profile.
You can refer to profile_feature.py for profile feature extraction.
The 310-dimensional content feature is composed of a 300-dimensional user comment word2vec (spaCy) embedding plus a 10-dimensional profile feature.
The dataset statistics is shown below:
| Data  | #Graphs  | #Fake News| #Total Nodes  | #Total Edges  | #Avg. Nodes per Graph  |
|-------|--------|--------|--------|--------|--------|
| Politifact | 314   |   157    |  41,054  | 40,740 |  131 |
| Gossipcop |  5464  |   2732   |  314,262  | 308,798  |  58  |
Please refer to the paper for more details about the UPFD dataset.
Due to the Twitter policy, we could not release the crawled user's historical tweets publicly.
To get the corresponding Twitter user information, you can refer to the news lists under \data in our github repo
and map the news id to FakeNewsNet.
Then, you can crawl the user information by following the instruction on FakeNewsNet.
In the UPFD project, we use Tweepy and Twitter Developer API to get the user information."	https://paperswithcode.com/dataset/upfd	25/04/2021	User Preference-aware Fake News Detection					
3661	InfographicVQA	InfographicVQA is a dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills.	https://paperswithcode.com/dataset/infographicvqa	26/04/2021						
3662	LDV	LDV is a dataset for video enhancement. It contains 240 videos with diverse categories of content, different kinds of motion and various frame-rates.	https://paperswithcode.com/dataset/ldv	21/04/2021	Large-scale Diverse Video					
3663	Earnings-21	Earnings-21, a 39-hour corpus of earnings calls containing entity-dense speech from nine different financial sectors. This corpus is intended to benchmark ASR (Automatic Speech Recognition) systems in the wild with special attention towards named entity recognition.	https://paperswithcode.com/dataset/earnings-21	22/04/2021						
3664	Comparative Question Completion	"Comparative Question Completion is a dataset to evaluate what do large Language Models learn.
The dataset includes short questions in natural language that make comparisons between entity pairs, for example, “is a cockroach or beetle more dangerous?”
The questions are in three subject domains: animals, cities and NBA players.
In each sentence, one of the compared entities in the sentence has been 'masked' (replaced with a [MASK] symbol). For example, for the question above the masked sentence is: “is a [MASK] or beetle more dangerous?” The dataset presents the task of automatically recovering the masked entity name, and provides the original entity for evaluation purposes. In addition to the original masked entity text (e.g., 'cockroach'), it details the respective Wikidata entity ID, (e.g., 'Q18123008')."	https://paperswithcode.com/dataset/comparative-question-completion	05/04/2021						
3665	ECTF	ECTF is a dataset for Twitter fake news detection in the Covid-19 domain.	https://paperswithcode.com/dataset/ectf	12/04/2021	Early COVID-19 Twitter Fake news					
3666	Extended UCF Crime	The Extended UCF Crime extends the UCF Crime data set that consists of 13 anomaly classes. The extension adds two different anomaly classes to the data set, which are ”molotov bomb” and ”protest” classes. It also adds 33 videos to the fighting class. In total, the extension adds 216 videos to the training set, 17 videos to the test set.	https://paperswithcode.com/dataset/extended-ucf-crime	14/04/2021						
3667	EntailmentBank	EntailmentBank is a dataset that contains multistep entailment trees. At each node in the tree (typically) two or more facts compose together to produce a new conclusion. Given a hypothesis (question + answer), three increasingly difficult explanation tasks are defined: generate a valid entailment tree given (a) all relevant sentences (the leaves of the gold entailment tree) (b) all relevant and some irrelevant sentences (c) a corpus.	https://paperswithcode.com/dataset/entailmentbank	17/04/2021						
3668	AM2iCo	"AM2iCo is a wide-coverage and carefully designed cross-lingual and multilingual evaluation set. It aims to assess the ability of state-of-the-art representation models to reason over cross-lingual lexical-level concept alignment in context for 14 language pairs. 
English (EN), German (DE), Russian (RU), Japanese (JA), Korean (KO), Mandarin Chinese(ZH), Arabic(AR), Indonesian(IN), Finnish(FI), Turkish(TR), Basque(EU), Georgian(KA), Urdu(UR), Bengali(BN), Kazakh(KK)."	https://paperswithcode.com/dataset/am2ico	17/04/2021	Adversarial and Multilingual Meaning in Context					
3669	TED-talks	"In order to create the TED-talks dataset,  3,035 YouTube videos were downloaded using the ""TED talks"" query. From these initial candidates, videos in which the upper part of the person is visible for at least 64 frames, and the height of the person bounding box was at least 384 pixels were selected. Static videos were manually filtered out and videos in which a person is doing something other than presenting.
Image source: https://arxiv.org/pdf/2104.11280v1.pdf"	https://paperswithcode.com/dataset/ted-talks	22/04/2021						
3670	ExampleStack	"This is a dataset of code snippets in StackOverflow that have been used in Github repositories by extending and adapting them. The dataset links SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. 
The authors qualitatively inspected 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, an automated adaptation analysis technique on top of GumTree is built to classify the entire dataset into these types."	https://paperswithcode.com/dataset/examplestack	28/05/2019						
3671	MGif	"MGif is a dataset of videos containing movements of different cartoon animals. Each video is a moving gif file. The dataset consists of 1000 videos. The dataset
is particularly challenging because of the high appearance variation and motion diversity. 
Source:"	https://paperswithcode.com/dataset/mgif	20/12/2018						
3672	Android Common Libraries	This dataset was constructed from an analysis of about 1.5 million apps from Google Play to identify a set of common libraries, to facilitate Android app analysis. It contains 1,113 libraries supporting common functionalities and 240 libraries for advertisement.	https://paperswithcode.com/dataset/android-common-libraries	20/11/2015						
3673	Workflow Trace Archive	The Workflow Trace Archive (WTA) is an open-access archive of workflow traces from diverse computing infrastructures. The WTA includes >48 million workflows captured from >10 computing infrastructures, representing a broad diversity of trace domains and characteristics.	https://paperswithcode.com/dataset/workflow-trace-archive	11/07/2019						
3674	Dex-Net 2.0	Dex-Net 2.0 is a dataset associating 6.7 million point clouds and analytic grasp quality metrics with parallel-jaw grasps planned using robust quasi-static GWS analysis on a dataset of 1,500 3D object models.	https://paperswithcode.com/dataset/dex-net-2-0	27/03/2017	Dexterity Network 2.0					
3675	Italian disinformation	This is a large-scale dataset of tweets associated to thousands of news articles published on Italian disinformation websites in the context of 2019 European elections.	https://paperswithcode.com/dataset/italian-disinformation	28/10/2019						
3676	MUSAN	MUSAN is a corpus of music, speech and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises.	https://paperswithcode.com/dataset/musan	28/10/2015						
3677	BLEBeacon	The BLEBeacon dataset is a collection of Bluetooth Low Energy (BLE) advertisement packets/traces generated from BLE beacons carried by people following their daily routine inside a university building for a whole month. A network of Raspberry Pi 3 (RPi)-based edge devices were deployed inside a multi-floor facility continuously gathering BLE advertisement packets and storing them in a cloud-based environment. The focus is on presenting a real-life realization of a location-aware sensing infrastructure, that can provide insights for smart sensing platforms, crowd-based applications, building management, and user-localization frameworks.	https://paperswithcode.com/dataset/blebeacon	24/02/2018						
3678	AdobeIndoorNav	AdobeIndoorNav is a dataset collected in real-world to facilitate the research in DRL based visual navigation. The dataset includes 3D reconstruction for real-world scenes as well as densely captured real 2D images from the scenes. It provides high-quality visual inputs with real-world scene complexity to the robot at dense grid locations.	https://paperswithcode.com/dataset/adobeindoornav	24/02/2018						
3679	nicolingua-0003-west-african-radio-corpus	This dataset contains 17,090 audio clips of length 30 seconds sampled from archives collected from 6 Guinean radio stations. The broadcasts consist of news and various radio shows in languages including French, Guerze, Koniaka, Kissi, Kono, Maninka, Mano, Pular, Susu, and Toma. Some radio shows include phone calls, background and foreground music, and various noise types. We collected this dataset for the purpose of unsupervised speech representation learning. A validation set of 300 tagged audio clips is also included.	https://paperswithcode.com/dataset/nicolingua-0003-west-african-radio-corpus		West African Radio Corpus					
3680	nicolingua-0004-west-african-va-asr-corpus	"This dataset contains 10,083 recorded utterances in French, Maninka, Pular and Susu from 49 speakers (16 female and 33 male) ranging from 5 to 76 years old on a variety of devices.
Please see our paper for more details on this dataset. Additional resources can be found in the following git repository: https://github.com/mdoumbouya/nicolingua"	https://paperswithcode.com/dataset/west-african-virtual-assistant-speech		West African Virtual Assistant Speech Recognition Corpus					
3681	GermanQuAD	GermanQuAD is a Question Answering (QA) dataset of 13,722 extractive question/answer pairs in German.	https://paperswithcode.com/dataset/germanquad	26/04/2021						
3682	GermanDPR	GermanDPR is a dataset for passage retrieval in German. GermanDPR comprises 8,245 question/answer pairs in the training set, 1,030 pairs in the development set, and 1,025 pairs in the test set. For each pair, there are one positive context and three hard negative contexts.	https://paperswithcode.com/dataset/germandpr	26/04/2021						
3683	LoED	LoED (LoRaWAN at the Edge Dataset) is a dataset from nine LoRaWAN gateways collected in an urban environment. The dataset contains raw payload information, along with other metadata from the gateway. The dataset contains packet header information and all physical layer properties reported by gateways such as the CRC, RSSI, SNR and spreading factor. Files are provided to analyse the data and get aggregated statistics	https://paperswithcode.com/dataset/loed	27/10/2020						
3684	BuGL	BuGL is a large-scale cross-language dataset for bug localization in code. BuGL constitutes of more than 10,000 bug reports drawn from open-source projects written in four programming languages, namely C, C++, Java, and Python. The dataset consists of information which includes Bug Reports and Pull-Requests. BuGL aims to unfold new research opportunities in the area of bug localization.	https://paperswithcode.com/dataset/bugl	19/04/2020						
3685	VOICES	"The VOICES corpus is a dataset to promote speech and signal processing research of speech recorded by far-field microphones in noisy room conditions. 
For this corpus, audio was recorded in furnished rooms with background noise played in conjunction with foreground speech selected from the LibriSpeech corpus. Multiple sessions were recorded in each room to accommodate for all foreground speech-background noise combinations. Audio was recorded using twelve microphones placed throughout the room, resulting in 120 hours of audio per microphone."	https://paperswithcode.com/dataset/voices	15/05/2018	Voices Obscured In Complex Environmental Settings					
3686	AndroZoo	"AndroZoo is a growing collection of Android apps collected from several sources, including the official Google Play app market and a growing collection of various metadata of those collected apps aiming at facilitating the Android-relevant research works. 
It currently contains 15,097,876 different APKs, each of which has been (or will be) analysed by tens of different AntiVirus products to know which applications are detected as Malware. Each app has over 20 types of metadata such as VirusTotal reports."	https://paperswithcode.com/dataset/androzoo	15/09/2017						
3687	BIRD (Big Impulse Response Dataset)	BIRD (Big Impulse Response Dataset) is an open dataset that consists of 100,000 multichannel room impulse responses (RIRs) generated from simulations using the Image Method, making it the largest multichannel open dataset currently available. These RIRs can be used to perform efficient online data augmentation for scenarios that involve two microphones and multiple sound sources.	https://paperswithcode.com/dataset/bird-big-impulse-response-dataset	19/10/2020						
3688	BCSD	"The dataset consists of images of 158 filled out bank checks containing various complex backgrounds, and handwritten text and signatures in the respective fields, along with both pixel-level and patch-level segmentation masks for the signatures on the checks. Please visit the dataset homepage for more details.
If you use this dataset in your work, please include the following citation:
Khan, Muhammad Saif Ullah. “A Novel Segmentation Dataset for Signatures on Bank Checks.” ArXiv:2104.12203 [Cs], Apr. 2021. arXiv.org, http://arxiv.org/abs/2104.12203.
Acknowledgements
1 P. Dansena, S. Bag, and R. Pal, “Differentiating Pen Inks in Hand-written Bank Cheques Using Multi-Layer Perceptron”, Proc. of 7th International Conference on Pattern recognition and Machine Intelligence, Kolkata, India, December 2017."	https://paperswithcode.com/dataset/bcsd	25/04/2021	Bank Check Segmentation Dataset					
3689	AbuseAnalyzer Dataset	"The dataset contains 7,601 Gab posts classified on three different aspects: abuse presence or not, abuse severity and abuse target.
The Binary label distribution is as follows:
Abusive Posts: 4,120
Non-Abusive Posts: 3481
The Abuse Severity label distribution is as follows:
Biased Attitude: 1830
Act of Bias and Discrimination: 1807
*Violence and Genocide: 483
The Abuse Target label distribution is as follows:
Individual (Second-Person):  389
Individual (Third-Person): 1330
*Group: 2401"	https://paperswithcode.com/dataset/abuseanalyzer-dataset	30/09/2020						
3690	MoGaze	"MoGaze is a dataset of full-body motion for everyday manipulation tasks, which includes 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze. The motion data was captured using a traditional motion capture system based on reflective markers. The eye-gaze was captured using a wearable pupil-tracking device.
The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed."	https://paperswithcode.com/dataset/mogaze	23/11/2020						
3691	Software Heritage Graph Dataset	Software Heritage is the largest existing public archive of software source code and accompanying development history. It spans more than five billion unique source code files and one billion unique commits , coming from more than 80 million software projects. These software artifacts were retrieved from major collaborative development platforms (e.g., GitHub, GitLab) and package repositories (e.g., PyPI, Debian, NPM), and stored in a uniform representation linking together source code files, directories, commits, and full snapshots of version control systems (VCS) repositories as observed by Software Heritage during periodic crawls. This dataset is unique in terms of accessibility and scale, and allows to explore a number of research questions on the long tail of public software development, instead of solely focusing on ''most starred'' repositories as it often happens.	https://paperswithcode.com/dataset/software-heritage-graph-dataset	16/11/2020						
3692	MM-COVID	MM-COVID is a dataset for fake news detection related to COVID-19. This dataset provides the multilingual fake news and the relevant social context. It contains 3,981 pieces of fake news content and 7,192 trustworthy information from English, Spanish, Portuguese, Hindi, French and Italian, 6 different languages.	https://paperswithcode.com/dataset/mm-covid	08/11/2020	Multilingual and Multidimensional COVID-19 Fake News Data Repository					
3693	Vent	"The Vent dataset is a large annotated dataset of text, emotions, and social connections. It comprises more than 33 millions of posts by nearly a million of users together with their social connections. Each post has an associated emotion. There are 705 different emotions, organized in 63 ""emotion categories"", forming a two-level taxonomy of affects."	https://paperswithcode.com/dataset/vent	15/01/2019						
3694	VOICe	"VOICe is a dataset for the development and evaluation of domain adaptation methods for sound event detection. VOICe consists of mixtures with three different sound events (""baby crying"", ""glass breaking"", and ""gunshot""), which are over-imposed over three different categories of acoustic scenes: vehicle, outdoors, and indoors. Moreover, the mixtures are also offered without any background noise.
VOICe consists of 1,449 different mixtures of three different sound events:

1,242 mixtures with background noise of three different categories of acoustic scenes (""vehicle"","" outdoors"", and ""indoors""), mixed under 2 SNR values (-3, -9 dB), that is 207 mixtures x 3 acoustic scenes x 2 SNRs = 1,242
207 mixtures without any background noise."	https://paperswithcode.com/dataset/voice	25/11/2019						
3695	FieldSAFE	"The FieldSAFE dataset is a multi-modal dataset for obstacle detection in agriculture. It comprises 2 hours of raw sensor data from a tractor-mounted sensor system in a grass mowing scenario in Denmark, October 2016.
Ground truth information on object location and class labels for both static and moving obstacles is available as timestamped global (geographic) coordinates."	https://paperswithcode.com/dataset/fieldsafe	11/09/2017						
3696	SocNav1	SocNav1 is a dataset for social navigation conventions. The aims of SocNav1 are two-fold: a) enabling comparison of the algorithms that robots use to assess the convenience of their presence in a particular position when navigating; b) providing a sufficient amount of data so that modern machine learning algorithms such as deep neural networks can be used. Because of the structured nature of the data, SocNav1 is particularly well-suited to be used to benchmark non-Euclidean machine learning algorithms such as Graph Neural Networks	https://paperswithcode.com/dataset/socnav1	14/01/2020						
3697	Public Git Archive	The Public Git Archive is a dataset of 182,014 top-bookmarked Git repositories from GitHub totalling 6 TB. The dataset provides the source code of the projects, the related metadata, and development history.	https://paperswithcode.com/dataset/public-git-archive	20/03/2018						
3698	AIR-Act2Act	AIR-Act2Act is a human-human interaction dataset for teaching non-verbal social behaviors to robots. It is different from other datasets because elderly people have participated in as performers. The authors recruited 100 elderly people and two college students to perform 10 interactions in an indoor environment. The entire dataset has 5,000 interaction samples, each of which contains depth maps, body indexes and 3D skeletal data that are captured with three Microsoft Kinect v2 cameras. In addition, the dataset also contains the joint angles of a humanoid NAO robot which are converted from the human behavior that robots need to learn.	https://paperswithcode.com/dataset/air-act2act	04/09/2020						
3699	Nighttime Driving	"Nighttime Driving is a dataset of road scenes consisting of 35,000 images ranging from daytime to twilight time and to nighttime. 
Image source: http://people.ee.ethz.ch/~daid/NightDriving/#"	https://paperswithcode.com/dataset/nighttime-driving	05/10/2018						
3700	Dark Zurich	"Dark Zurich is an image dataset containing a total of 8779 images captured at nighttime, twilight, and daytime, along with the respective GPS coordinates of the camera for each image. These GPS annotations are used to construct cross-time-of-day correspondences, i.e., to match each nighttime or twilight image to its daytime counterpart.
These attributes allow the usage of Dark Zurich as a dataset to build models and systems that perform:
1) domain adaptation (unsupervised, weakly supervised or semi-supervised), e.g. for semantic segmentation or object detection,
2) image translation / style transfer to different times of day,
3) robust image matching / visual localization across diverse domains, and
4) other visual perception tasks that are central for autonomous vehicles and other robotic applications."	https://paperswithcode.com/dataset/dark-zurich	17/01/2019						
3701	DX7 Timbre Dataset	"This is a dataset of 22.5 hours of synthesized audio using the open-source learnfm clone of the DX7 FM synthesizer, based upon 31K presets from Bobby Blue. These represent ""natural'' synthesis sounds---i.e.presets devised by humans.
The authors generated 4-second samples playing midi note 69 (A440) with a note-on duration of 3 seconds. For each preset, the authors varied only the velocity, from 1--127, and perceptually normalized the level of each sound. Sounds that were completely identical were removed from the dataset. DX7 FM synthesis is good for this purpose because it doesn't have a noise oscillator. Thus, for a particular preset, there is a timbral variation as the velocity increases. 8K presets had only one unique sound. The median was 51 unique sound per preset, mean 41.9, stddev 27.4."	https://paperswithcode.com/dataset/dx7-timbre-dataset	27/04/2021						
3702	CSRC	CSRC is a collection of data for Children Speech Recognition. The data for this challenge is divided into 3 datasets, referred to as A (Adult speech training set), C1 (Children speech training set) and C2 (Children conversation training set). All dataset combined amount to 400 hours of Mandarin speech data.	https://paperswithcode.com/dataset/csrc	13/11/2020	Children Speech Recognition Challenge					
3703	NELA-GT-2018	NELA-GT-2018 is a dataset for the study of misinformation that consists of 713k articles collected between 02/2018-11/2018. These articles are collected directly from 194 news and media outlets including mainstream, hyper-partisan, and conspiracy sources. It includes ground truth ratings of the sources from 8 different assessment sites covering multiple dimensions of veracity, including reliability, bias, transparency, adherence to journalistic standards, and consumer trust.	https://paperswithcode.com/dataset/nela-gt-2018	02/04/2019						
3704	NELA-GT-2019	NELA-GT-2019 is an updated version of the NELA-GT-2018 dataset. NELA-GT-2019 contains 1.12M news articles from 260 sources collected between January 1st 2019 and December 31st 2019. Just as with NELA-GT-2018, these sources come from a wide range of mainstream news sources and alternative news sources. Included with the dataset are source-level ground truth labels from 7 different assessment sites covering multiple dimensions of veracity.	https://paperswithcode.com/dataset/nela-gt-2019	18/03/2020						
3705	NELA-GT-2020	NELA-GT-2020 is an updated version of the NELA-GT-2019 dataset. NELA-GT-2020 contains nearly 1.8M news articles from 519 sources collected between January 1st, 2020 and December 31st, 2020. Just as with NELA-GT-2018 and NELA-GT-2019, these sources come from a wide range of mainstream news sources and alternative news sources. Included in the dataset are source-level ground truth labels from Media Bias/Fact Check (MBFC) covering multiple dimensions of veracity. Additionally, new in the 2020 dataset are the Tweets embedded in the collected news articles, adding an extra layer of information to the data.	https://paperswithcode.com/dataset/nela-gt-2020	08/02/2021						
3706	WhatsApp, Doc?	"This is a large-scale dataset collected from WhatsApp public groups. It has been created from 178 public groups containing around 45K users and 454K messages.
This dataset allows researchers to ask questions like (i) Are WhatsApp groups a broadcast, multicast or unicast medium? (ii) How interactive are users, and how do these interactions emerge over time? (iii) What geographical span do WhatsApp groups have, and how does geographical placement impact interaction dynamics? (iv) What role does multimedia content play in WhatsApp groups, and how do users form interaction around multimedia content? (v) What is the potential of WhatsApp data in answering further social
science questions, particularly in relation to bias and representability?"	https://paperswithcode.com/dataset/whatsapp-doc	04/04/2018						
3707	THÖR	THÖR is a dataset with human motion trajectory and eye gaze data collected in an indoor environment with accurate ground truth for position, head orientation, gaze direction, social grouping, obstacles map and goal coordinates. THOR also contains sensor data collected by a 3D lidar and involves a mobile robot navigating the space.	https://paperswithcode.com/dataset/thor	11/12/2019						
3708	EGAD	The Evolved Grasping Analysis Dataset (EGAD) comprises over 2000 generated objects aimed at training and evaluating robotic visual grasp detection algorithms. The objects in EGAD are geometrically diverse, filling a space ranging from simple to complex shapes and from easy to difficult to grasp, compared to other datasets for robotic grasping, which may be limited in size or contain only a small number of object classes.	https://paperswithcode.com/dataset/egad	03/03/2020	Evolved Grasping Analysis Dataset					
3709	Weibo-COV	Weibo-COV is a large-scale COVID-19 social media dataset from Weibo, covering more than 30 million posts from 1 November 2019 to 30 April 2020. Moreover, the field information of the dataset is very rich, including basic posts information, interactive information, location information and retweet network.	https://paperswithcode.com/dataset/weibo-cov	19/05/2020						
3710	COUGHVID	The COUGHVID dataset provides over 20,000 crowdsourced cough recordings representing a wide range of subject ages, genders, geographic locations, and COVID-19 statuses. First, the dataset was filtered using an open-sourced cough detection algorithm. Second, experienced pulmonologists labeled more than 2,000 recordings to diagnose medical abnormalities present in the coughs, thereby contributing one of the largest expert-labeled cough datasets in existence that can be used for a plethora of cough audio classification tasks.	https://paperswithcode.com/dataset/coughvid	24/09/2020						
3711	JVS	JVS is a Japanese multi-speaker voice corpus which contains voice data of 100 speakers in three styles (normal, whisper, and falsetto). The corpus contains 30 hours of voice data including 22 hours of parallel normal voices.	https://paperswithcode.com/dataset/jvs	17/08/2019						
3712	FacebookVideoLive18	FacebookVideosLive18 dataset includes 1,000,000 Facebook live videos with their metadata (title, source, length, creation time, description, etc.), broadcasters locations and  viewers locations. We are using a set of synchronised scripts that allow to have a global view of the real time streaming system every 3 minutes. We believe that our dataset is the first that tracks the locations and behaviors of live viewers. We expect FacebookVideosLive18 to support various trending research areas such as cloud computing, multimedia data allocation, multi-cloud allocation, edge computing, edge caching and transcoding, data analytics, etc.	https://paperswithcode.com/dataset/facebookvideolive18	24/03/2020						
3713	Pushshift Telegram	The Pushshift Telegram dataset is made up of over 27.8K channels and 317M messages from 2.2M unique users. The Pushshift Telegram dataset can help researchers from a variety of disciplines interested in studying online social movements, protests, political extremism, and disinformation.	https://paperswithcode.com/dataset/pushshift-telegram	23/01/2020						
3714	FTR-18	FTR-18 is a multilingual rumour dataset on football transfer news. Transfer rumours are continuously published by sports media. They can both harm the image of player or a club or increase the player's market value. The proposed dataset includes transfer articles written in English, Spanish and Portuguese. It also comprises Twitter reactions related to the transfer rumours. FTR-18 is suited for rumour classification tasks and allows the research on the linguistic patterns used in sports journalism.	https://paperswithcode.com/dataset/ftr-18	30/11/2018						
3715	RWCP-SSD-Onomatopoeia	RWCP-SSD-Onomatopoeia is a dataset consisting of 155,568 onomatopoeic words paired with audio samples for environmental sound synthesis.	https://paperswithcode.com/dataset/rwcp-ssd-onomatopoeia	09/07/2020						
3716	HuGaDB	HuGaDB is human gait data collection for analysis and activity recognition consisting of continues recordings of combined activities, such as walking, running, taking stairs up and down, sitting down, and so on; and the data recorded are segmented and annotated. Data were collected from a body sensor network consisting of six wearable inertial sensors (accelerometer and gyroscope) located on the right and left thighs, shins, and feet. Additionally, two electromyography sensors were used on the quadriceps (front thigh) to measure muscle activity. This database can be used not only for activity recognition but also for studying how activities are performed and how the parts of the legs move relative to each other. Therefore, the data can be used (a) to perform health-care-related studies, such as in walking rehabilitation or Parkinson's disease recognition, (b) in virtual reality and gaming for simulating humanoid motion, or (c) for humanoid robotics to model humanoid walking.	https://paperswithcode.com/dataset/hugadb	11/07/2017						
3717	GitHub Repository Deduplication	This is a dataset of 10.6 million GitHub projects that are copies of others, and link each record with the project's ultimate parent. The ultimate parents were derived from a ranking along six metrics. The related projects were calculated as the connected components of an 18.2 million node and 12 million edge denoised graph created by directing edges to ultimate parents. The graph was created by filtering out more than 30 hand-picked and 2.3 million pattern-matched clumping projects. Projects that introduced unwanted clumping were identified by repeatedly visualizing shortest path distances between unrelated important projects.	https://paperswithcode.com/dataset/github-repository-deduplication	15/06/2020						
3718	PersianQA	"PersianQA: a dataset for Persian Question Answering
Persian Question Answering (PersianQA) Dataset is a reading comprehension
dataset on Persian Wikipedia. The crowd-sourced
the dataset consists of more than 9,000 entries. Each entry can be either an
impossible-to-answer or a question with one or more answers spanning in the
passage (the context) from which the questioner proposed the question.
Much like the SQuAD2.0 dataset, the impossible or unanswerable questions can be
utilized to create a system which ""knows that it doesn't know the answer"".
Moreover, the dataset has 900 test data available. On top of that, the very
first models trained on the dataset, Transformers, are available online.
All the crowd workers of the dataset are native Persian speakers. Also, it worth
mentioning that the contexts are collected from all categories of the Wiki
(Historical, Religious, Geography, Science, etc)."	https://paperswithcode.com/dataset/persianqa	29/04/2021	Persian Question Answering Dataset					
3719	CRIC Cervix	he Center for Recognition and Inspection of Cells (CRIC) platform enables the creation of CRIC Cervix collection, currently with 400 images (1,376 x 1,020 pixels) curated from conventional Pap smears, with manual classification of 11,534 cells.	https://paperswithcode.com/dataset/cric-cervix		Center for Recognition and Inspection of Cells (CRIC) Cervix collection					
3720	VISION	"The dataset contains more than 35000 images and 600 videos captured using 35 different portable devices of 11 major brands. In addition to the original acquisitions, images were shared through Facebook and WhatsApp whereas videos were shared through YouTube and WhatsApp platforms.
The dataset was introduced in the following paper: Shullani, Dasara, et al. ""VISION: a video and image dataset for source identification."" EURASIP Journal on Information Security 2017.1 (2017): 1-16, https://doi.org/10.1186/s13635-017-0067-2."	https://paperswithcode.com/dataset/vision	03/10/2017	VISION dataset					
3721	EVA	"The dataset contains 7000 videos: native, altered and exchanged through social platforms. The altered contents include manipulations with FFmpeg, AVIdemux, Kdenlive and Adobe Premiere. The social platforms used to exchange the native and altered videos are Facebook, Tiktok, Youtube and Weibo. 
A detailed description of the dataset is available in the journal paper by Yang, Pengpeng, et al. ""Efficient Video Integrity Analysis Through Container Characterization."" IEEE Journal of Selected Topics in Signal Processing 14.5 (2020): 947-954, 10.1109/JSTSP.2020.3008088."	https://paperswithcode.com/dataset/eva	26/01/2021	EVA-7K dataset					
3722	Light-Field Material	This is a 4D light-field dataset of materials. The dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total.	https://paperswithcode.com/dataset/light-field-material	24/08/2016						
3723	CASCONet	CASCONet is a a collection of data about the CAS Conference (CASCON) for the past 25 years including information about papers, technology showcase demos, workshops, and keynote presentations.	https://paperswithcode.com/dataset/casconet	28/06/2017						
3724	VeReMi	The Vehicular Reference Misbehavior (VeReMi) dataset, is a dataset for the evaluation of misbehavior detection mechanisms for VANETs (vehicular networks). This dataset consists of message logs of on-board units, including a labelled ground truth, generated from a simulation environment. The dataset includes malicious messages intended to trigger incorrect application behavior, which is what misbehavior detection mechanisms aim to prevent. The initial dataset contains a number of simple attacks: the idea of this dataset release is not just to provide a baseline for the comparison of detection mechanisms, but also to serve as a starting point for more complex attacks.	https://paperswithcode.com/dataset/veremi	18/04/2018						
3725	EDNA-Covid	EDNA-Covid is a multilingual, large-scale dataset of coronavirus-related tweets collected since January 25, 2020. EDNA-Covid includes, at time of this publication, over 600M tweets from around the world in over 10 languages.	https://paperswithcode.com/dataset/edna-covid	06/10/2020						
3726	YT-UGC	YT-UGC is a large scale UGC (User Generated Content) dataset (1,500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). This dataset can be used to study video compression and quality assessment.	https://paperswithcode.com/dataset/yt-ugc	13/04/2019	YouTube UGC					
3727	putEMG	putEMG and putEMG-Force datasets are databases of surface electromyographic activity recorded from forearm. Datasets allows for development of algorithms for gesture recognition and grasp force recognition. Experiment was conducted on 44 participants, with two repetitions separated by, minimum of one week. The dataset includes 7 active gestures (like hand flexion, extension, etc.) + idle and a set of trials with isometric contractions. sEMG was recorded using a 24-electrode matrix.	https://paperswithcode.com/dataset/putemg	22/08/2019						
3728	DeepMIMO	DeepMIMO is a generic dataset for mmWave/massive MIMO channels. The DeepMIMO dataset generation framework has two important features. First, the DeepMIMO channels are constructed based on accurate ray-tracing data obtained from Remcom Wireless InSite. The DeepMIMO channels, therefore, capture the dependence on the environment geometry/materials and transmitter/receiver locations, which is essential for several machine learning applications. Second, the DeepMIMO dataset is generic/parameterized as the researcher can adjust a set of system and channel parameters to tailor the generated DeepMIMO dataset for the target machine learning application. The DeepMIMO dataset can then be completely defined by the (i) the adopted ray-tracing scenario and (ii) the set of parameters, which enables the accurate definition and reproduction of the dataset.	https://paperswithcode.com/dataset/deepmimo	18/02/2019						
3729	US-Accidents	This is a countrywide traffic accident dataset, which covers 49 states of the United States. The data is continuously being collected from February 2016, using several data providers, including two APIs which provide streaming traffic event data. These APIs broadcast traffic events captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 4.2 million accident records in this dataset.	https://paperswithcode.com/dataset/us-accidents	12/06/2019						
3730	JSESM Publications		https://paperswithcode.com/dataset/jsesm-publications	24/07/2018						
3731	COOLL	Controlled On/Off Loads Library (COOLL) is a dataset of high-sampled electrical current and voltage measurements representing individual appliances consumption. The measurements were taken in June 2016 in the PRISME laboratory of the University of Orléans, France. The appliances are mainly controllable appliances (i.e. we can precisely control their turn-on/off time instants). 42 appliances of 12 types were measured at a 100 kHz sampling frequency.	https://paperswithcode.com/dataset/cooll	17/11/2016	Controlled On/Off Loads Library					
3732	Pushshift Reddit	Pushshift makes available all the submissions and comments posted on Reddit between June 2005 and April 2019. The dataset consists of 651,778,198 submissions and 5,601,331,385 comments posted on 2,888,885 subreddits.	https://paperswithcode.com/dataset/pushshift-reddit	23/01/2020						
3733	HoaxItaly	HoaxItaly consists of over 1 million tweets shared during 2019 and containing links to thousands of news articles published on two classes of Italian outlets: (1) disinformation websites, i.e. outlets which have been repeatedly flagged by journalists and fact-checkers for producing low-credibility content such as false news, hoaxes, click-bait, misleading and hyper-partisan stories; (2) fact-checking websites which notably debunk and verify online news and claims. The dataset includes title and body for approximately 37k news articles.	https://paperswithcode.com/dataset/hoaxitaly	29/01/2020						
3734	GED	GED is a dataset on the economic activity of mainland China, which measures the volume of establishments at a 0.01 latitude by 0.01 longitude scale. Specifically, the dataset captures the geographically based opening and closing of approximately 25.5 million firms that registered in mainland China over the period 2005-2015. The characteristics of fine granularity and long-term observability give the GED a high application value.	https://paperswithcode.com/dataset/ged	31/10/2020	Gridded Establishment Dataset					
3735	Interactive Gibson Environment	"Interactive Gibson is a comprehensive benchmark for training and evaluating Interactive Navigation: robot navigation strategies where physical interaction with objects is allowed and even encouraged to accomplish a task. The benchmark has two main components:

The Interactive Gibson Environment,  which simulates high fidelity visuals of indoor scenes, and high fidelity physical dynamics of the robot and common objects found in these scenes.
set of Interactive Navigation metrics which allows one to study the interplay between navigation and physical simulation."	https://paperswithcode.com/dataset/interactive-gibson-environment	30/10/2019						
3736	TAU Spatial Sound Events 2019	"TAU Spatial Sound Events 2019 consists of 2 datasets: Ambisonic (FOA) and Microphone Array (MIC), of identical sound scenes with the only difference in the format of the audio. The FOA dataset provides four-channel First-Order Ambisonic recordings while the MIC dataset provides four-channel directional microphone recordings from a tetrahedral array configuration. Both formats are extracted from the same microphone array.
Both the datasets, consists of a development and evaluation set. The development set consists of 400 one-minute long recordings sampled at 48000 Hz, divided into four cross-validation splits of 100 recordings each. The evaluation set consists of 100 one-minute long recordings. These recordings were synthesized using spatial room impulse response (IRs) collected from five indoor environments, at 504 unique combinations of azimuth-elevation-distance."	https://paperswithcode.com/dataset/tau-spatial-sound-events-2019	21/05/2019						
3737	BugHunter	The BugHunter dataset is an automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information.	https://paperswithcode.com/dataset/bughunter	17/06/2020						
3738	Natural Hazards Twitter Dataset	Natural Hazards is a natural disaster dataset with sentiment labels, which contains nearly 50,00 Twitter data about different natural disasters in the United States (e.g., a tornado in 2011, a hurricane named Sandy in 2012, a series of floods in 2013, a hurricane named Matthew in 2016, a blizzard in 2016, a hurricane named Harvey in 2017, a hurricane named Michael in 2018, a series of wildfires in 2018, and a hurricane named Dorian in 2019).	https://paperswithcode.com/dataset/natural-hazards-twitter-dataset	28/05/2020						
3739	20-MAD	20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores.	https://paperswithcode.com/dataset/20-mad	31/03/2020	20-MAD: Mozilla Apache Dataset					
3740	Dataset of Video Game Development Problems	This is a grounded dataset describing software-engineering problems in video-game development extracted from postmortems. The dataset was created using an iterative method through which the authors manually coded more than 200 postmortems spanning 20 years (1998 to 2018) and extracted 1,035 problems related to software engineering while maintaining traceability links to the postmortems. The problems were grouped in 20 different types. This dataset is useful to understand the problems faced by developers during video-game development, providing researchers and practitioners a starting point to study video-game development in the context of software engineering.	https://paperswithcode.com/dataset/dataset-of-video-game-development-problems	02/01/2020						
3741	iBugMask		https://paperswithcode.com/dataset/ibugmask	04/02/2021						
3742	A Dataset of State-Censored Tweets	This is a dataset of 583,437 tweets by 155,715 users that were censored between 2012-2020 July. It also contains 4,301 accounts that were censored in their entirety. Additionally, another set of tweets is related, consisting of 22,083,759 supplemental tweets made up of all tweets by users with at least one censored tweet as well as instances of other users retweeting the censored user.	https://paperswithcode.com/dataset/a-dataset-of-state-censored-tweets	15/01/2021						
3743	Enterprise-Driven Open Source Software	This is a dataset of open source software developed mainly by enterprises rather than volunteers. This can be used to address known generalizability concerns, and, also, to perform research on open source business software development. Based on the premise that an enterprise's employees are likely to contribute to a project developed by their organization using the email account provided by it, we mine domain names associated with enterprises from open data sources as well as through white- and blacklisting, and use them through three heuristics to identify 17,264 enterprise GitHub projects. We provide these as a dataset detailing their provenance and properties. A manual evaluation of a dataset sample shows an identification accuracy of 89%.	https://paperswithcode.com/dataset/enterprise-driven-open-source-software	21/04/2020						
3744	Test Scene Dataset for Physically Based Rendering	"This is a comprehensive test database of scenes that treat different light setups in conjunction with diverse materials. It delivers a comprehensive foundation for evaluating existing and newly developed rendering techniques.
The source files are provided in the Blender file format for easy editing and additional exports to the Mitsuba XML format are included."	https://paperswithcode.com/dataset/test-scene-dataset-for-physically-based	30/08/2020						
3745	QA-SRL Bank 2.0	QA-SRL Bank 2.0 is a large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations. The corpus consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that was shown to have high precision and good recall at modest cost.	https://paperswithcode.com/dataset/qa-srl-bank-2-0	14/05/2018						
3746	HomebrewedDB	HomebrewedDB is a dataset for 6D pose estimation mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. It also consists of a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter.	https://paperswithcode.com/dataset/homebreweddb	05/04/2019						
3747	DIHARD II	The DIHARD II development and evaluation sets draw from a diverse set of sources exhibiting wide variation in recording equipment, recording environment, ambient noise, number of speakers, and speaker demographics. The development set includes reference diarization and speech segmentation and may be used for any purpose including system development or training.	https://paperswithcode.com/dataset/dihard-ii	18/06/2019						
3748	SPHERE	"The dataset for the SPHERE challenge consists on a multimodal activity recognition dataset consisting of accelerometer, RGB-D and environmental data. Accelerometer is samplled at 20 Hz and given in its raw format. Raw video is not given in order to preserve anonymity of the participants. Instead, extracted features that relate to the centre of mass and bounding box of the identified persons are provided. Environmental data consists of Passive Infra-Red (PIR) sensors, and these is given in raw format.
Twenty (posture/ambulation) activities labels are annotated in the dataset."	https://paperswithcode.com/dataset/sphere	17/03/2016						
3749	ANIMAL	10 classes with 50, 000 training and 5, 000 testing images. Please note that, in ANIMAL10N, noisy labels were injected naturally by human mistakes, where its noise rate was estimated at 8%.	https://paperswithcode.com/dataset/animal		ANIMAL-10N					
3750	Ghera	Ghera is a repository of Android app vulnerabilities.	https://paperswithcode.com/dataset/ghera	08/08/2017						
3751	ManySStuBs4J	"The ManySStuBs4J corpus is a collection of simple fixes to Java bugs, designed for evaluating program repair techniques. We collect all bug-fixing changes using the SZZ heuristic, and then filter these to obtain a data set of small bug fix changes.
These are single statement fixes, classified where possible into one of 16 syntactic templates which we call SStuBs.
The dataset contains simple statement bugs mined from open-source Java projects hosted in GitHub.
There are two variants of the dataset. One mined from the 100 Java Maven Projects and one mined from the top 1000 Java Projects.
The dataest contains 153,652 single statement bugfix changes mined from 1,000 popular open-source Java projects, annotated by whether they match any of a set of 16 bug templates, inspired by state-of-the-art program repair techniques."	https://paperswithcode.com/dataset/manysstubs4j	30/05/2019						
3752	DACT	DACT contains two subsets of annotated car trajectories data. The dataset contains 50 trajectories which cover about 13 hours of driving data. In DACT, we manually specified significant driving patterns by using an interactive framework. A significant driving pattern can be anything like a turn, speed-up, slow-down, etc. The annotation process consists of a crowd-sourcing task followed by comprehensive aggregation phases. The aggregation is done by two different strategies: Strict and Easy. For the first one, we used some strict constraints to aggregate crowd-sourcing results, while we used flexible constraints to generate the second subset of DACT.	https://paperswithcode.com/dataset/dact	16/05/2017	Dataset of Annotated Car Trajectories					
3753	OMG-Emotion	"The One-Minute Gradual-Emotional Behavior dataset (OMG-Emotion) dataset is composed of Youtube videos which are around a minute in length and are annotated taking into consideration a continuous emotional behavior. The videos were selected using a crawler technique that uses specific keywords based on long-term emotional behaviors such as ""monologues"", ""auditions"", ""dialogues"" and ""emotional scenes"".
It contains 567 emotion videos with an average length of 1 minute, collected from a variety of Youtube channels. The videos were separated into clips based on utterances, and each utterance was annotated by at least five independent subjects using the Amazon Mechanical Turk tool."	https://paperswithcode.com/dataset/omg-emotion	14/03/2018	One-Minute Gradual-Emotional Behavior					
3754	JVS-MuSiC	"JVS-MuSiC is a Japanese multispeaker singing-voice corpus called ""JVS-MuSiC"" with the aim to analyze and synthesize a variety of voices. The corpus consists of 100 singers' recordings of the same song, Katatsumuri, which is a Japanese children's song. It also includes another song that is different for each singer."	https://paperswithcode.com/dataset/jvs-music	20/01/2020						
3755	SUMMIT	SUMMIT is a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physical and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control, planning, and end-to-end learning.	https://paperswithcode.com/dataset/summit	11/11/2019						
3756	KinGaitWild	"To study kinship verification from gait, we collected the dataset KinGaitWild consisting of several videos from youtube. Most of the videos were taken under uncontrolled conditions in terms of background, camera motion, luminance and viewpoints. The KinGaitWild dataset contains 105 videos of celebrities and their relatives. The average time duration of each video is around 10 seconds. The database includes 60 pairs of Father-Son (FS) relationships. These pairs are equally split into 5 groups. We focus in this study on the Father-Son relationships.
The database collection was done as follows. First, we used the YouTube Data API to search for videos showing celebrities walking in the wild. To avoid biases, we selected
the pairs of celebrities so that the videos are not originated from the same source nor environment. For each video, we labeled the position of each specified person by a bounding box (bbox). These bboxes are used to estimate the human pose for silhouette-based approaches and to crop the region of interest for both the video-based and deep-learning-based approaches."	https://paperswithcode.com/dataset/kingaitwild	28/10/2020	Kinship verification from gait dataset					
3757	FINO-Net	FINO-Net is a multimodal (RGB, depth and audio) dataset, containing 229 real-world manipulation data of 5 different manipulation types recorded with a Baxter robot.	https://paperswithcode.com/dataset/fino-net	11/11/2020						
3758	DSSN	DSSN is a spatiotemporal dataset of 0.7 million data points of continuous location data logged at an interval of every 2 minutes by mobile phones of 46 subjects. The total number of data points reported in this dataset are 6,59,268. The total number of subjects using the application to record data are 74, however with cleaning based on quality checks. The number was reduced to 46. The data recorded varies in accuracy with an average accuracy of 36.0 meters.	https://paperswithcode.com/dataset/dssn	10/11/2016	DAIICT Spatio-Temporal Network					
3759	TRANCOS		https://paperswithcode.com/dataset/trancos		TRaffic ANd COngestionS					
3760	CoronaVis	CoronaVis is a dataset of tweets related to coronavirus.	https://paperswithcode.com/dataset/coronavis	29/04/2020						
3761	DroidBugs	DroidBugs is a benchmark for Automated Program Repair (APR) of Android applications.	https://paperswithcode.com/dataset/droidbugs	19/09/2018						
3762	Multi-Codec DASH	This is a multi-codec DASH dataset comprising AVC, HEVC, VP9, and AV1 in order to enable interoperability testing and streaming experiments for the efficient usage of these codecs under various conditions.	https://paperswithcode.com/dataset/multi-codec-dash	19/03/2018						
3763	UCR Time Series Classification Archive	"The UCR Time Series Archive - introduced in 2002,
has become an important resource in the time series data mining
community, with at least one thousand published papers making
use of at least one data set from the archive. The original
incarnation of the archive had sixteen data sets but since that
time, it has gone through periodic expansions. The last expansion
took place in the summer of 2015 when the archive grew from
45 to 85 data sets. This paper introduces and will focus on the
new data expansion from 85 to 128 data sets. Beyond expanding
this valuable resource, this paper offers pragmatic advice to
anyone who may wish to evaluate a new algorithm on the archive.
Finally, this paper makes a novel and yet actionable claim: of the
hundreds of papers that show an improvement over the standard
baseline (1-nearest neighbor classification), a large fraction may
be misattributing the reasons for their improvement. Moreover,
they may have been able to achieve the same improvement with
a much simpler modification, requiring just a single line of code."	https://paperswithcode.com/dataset/ucr-time-series-classification-archive	17/10/2018	UCR Time Series Classification Archive					
3764	Near-Collision	"Near-Collision is a large-scale dataset of 13,658 egocentric
video snippets of humans navigating in indoor hallways. In
order to obtain ground truth annotations of human pose, the
videos are provided with the corresponding 3D point cloud
from LIDAR."	https://paperswithcode.com/dataset/near-collision	21/03/2019						
3765	Apiza Corpus	The Apiza Corpus is a WoZ-like (Wizard of Oz) set of dialogues between 30 programmers and a simulated virtual assistant. This corpus can be used to study or train a virtual assistant for software engineering.	https://paperswithcode.com/dataset/apiza-corpus	27/01/2020						
3766	YoutubeGraph-Dyn	YoutubeGraph-Dyn is an evolving graph dataset generated from YouTube real-world interactions. It can be used to study temporal evolution on graphs. YoutubeGraph-Dyn provides intra-day time granularity (with 416 snapshots taken every 6 hours for a period of 104 days), multi-modal relationships that capture different aspects of the data, multiple attributes including timestamped, non-timestamped, word embeddings, and integers.	https://paperswithcode.com/dataset/youtubegraph-dyn	04/07/2019						
3767	HoMG	HoMG is a holoscopic 3D micro-gesture dataset captured with a holoscopic 3D camera. HoMG database recorded the image sequence of 3 conventional gestures from 40 participants under different settings and conditions. For the purpose of H3D micro-gesture recognition, HoMG has a video subset of 960 videos and a still image subset with 30,635 images.	https://paperswithcode.com/dataset/homg	19/01/2018						
3768	Wildtrack	"Wildtrack is a large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated
frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than
300 individuals."	https://paperswithcode.com/dataset/wildtrack	01/06/2018						
3769	pixraw10P	face image datasets	https://paperswithcode.com/dataset/pixraw10p	29/01/2016	pixraw10P					
3770	warpPIE10P	face dataset	https://paperswithcode.com/dataset/warppie10p	20/11/2020	warpPIE10P					
3771	iris	"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus""."	https://paperswithcode.com/dataset/iris-1	20/11/2020	iris					
3772	UPFD-POL	"The PolitiFact variant of the UPFD dataset for benchmarking.
Please refer to the UPFD dataset for more details of the data."	https://paperswithcode.com/dataset/upfd-pol	25/04/2021	User Preference-aware Fake News Detection					
3773	UPFD-GOS	"The Gossipcop variant of the UPFD dataset for benchmarking.
Please refer to the UPFD dataset for more details of the data."	https://paperswithcode.com/dataset/upfd-gos	25/04/2021	User Preference-aware Fake News Detection					
3774	australian	"Data Set Information:
This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.
This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values."	https://paperswithcode.com/dataset/australian	20/11/2020	Statlog (Australian Credit Approval) Data Set					
3775	BA		https://paperswithcode.com/dataset/ba	20/11/2020	Binary Alphabet					
3776	Dataset of Rendered Chess Game State Images	This dataset contains 4,888 synthetic images of chess game states that occurred in games played by Magnus Carlsen. The images were rendered in Blender at different angles and lighting conditions.	https://paperswithcode.com/dataset/dataset-of-rendered-chess-game-state-images	30/04/2021						
3777	POT-210	Planar object tracking is an actively studied problem in vision-based robotic applications. While several benchmarks have been constructed for evaluating state-of-theart algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking.	https://paperswithcode.com/dataset/pot-210	23/03/2017	Planar Object Tracking in the Wild: A Benchmark					
3778	Risk-Aware Planning Dataset	Risk-Aware Planning is a dataset that contains the overhead images and their semantic segmentation captured by a drone from the CityEnviron environment in AirSim simulator.	https://paperswithcode.com/dataset/risk-aware-planning-dataset	25/03/2020						
3779	Fire Drill Anti-Pattern Dataset	"Fire Drill Anti-Pattern Dataset is a collection of nine real-world software projects for detection of the fire drill anti-pattern with ground truth, issue-tracking data, source code density, models and code. The data is supposed to aid the detection of the presence of the Fire Drill anti-pattern. It includes data, ground truth, code, and notebooks. The data supports two distinct methods of detecting the AP: a) through issue-tracking data, and b) through the underlying source code. Therefore, this package includes the following:
Fire Drill in issue-tracking data:

Ground truth for whether and how strong each project exhibits the Fire Drill AP, on a scale from [0,10]. This was determined by two individual raters, who also reached a consensus.
Coefficients for indicators for the first method, per project.
Detailed issue-tracing data for each project: what occurred and when.
Time logs for each project.

Fire Drill in source-code data:

Three technical reports that document the developed method of how to translate a description into a detectable pattern, and to use the pattern to detect the presence and to score it (similar to the rating). Also includes a report for how activities were assigned to individual commits.
Source code density data (metrics) for each commit in each of the nine projects as a separate dataset.
Code: a snapshot of the repository that holds all code, models, notebooks, and pre-computed results, for utmost reproducibility (the code is written in R)."	https://paperswithcode.com/dataset/detection-of-the-fire-drill-anti-pattern-nine	03/05/2021						
3780	Mobility Flow	"This is a multiscale dynamic human mobility flow dataset across the United States, with data starting from January 1st, 2019. By analyzing millions of anonymous mobile phone users’ visit trajectories to various places provided by SafeGraph, the daily and weekly dynamic origin-to-destination (O-D) population flows are computed, aggregated, and inferred at three geographic scales: census tract, county, and state.
Such a high spatiotemporal resolution human mobility flow dataset at different geographic scales over time may help monitor epidemic spreading dynamics, inform public health policy, and deepen our understanding of human behavior changes under the unprecedented public health crisis."	https://paperswithcode.com/dataset/mobility-flow	27/08/2020						
3781	York Urban Line Segment Database	"The York Urban Line Segment Database is a compilation of 102 images (45 indoor, 57 outdoor) of urban environments consisting mostly of scenes from the campus of York University and downtown Toronto, Canada. The images are 640 x 480 in size and have been taken with a calibrated Panasonic Lumix DMC-LC80 digital camera.
Each image in the database has been hand-labelled to identify the set of line segments satisfying the “Manhattan assumption” (Coughlan & Yuille 2003), i.e., the set of line segments that conform to the 3D orthogonal frame of the urban environment.
These hand-labelled data have been used to identify the three Manhattan vanishing points in each image and from these to identify the Euler angles relating the camera frame to the Manhattan frame of the scene.
The database provides the original images, camera calibration parameters, ground truth line segments, and estimated Manhattan frame relative to the camera for each image.
Source: York Urban Line Segment Database"	https://paperswithcode.com/dataset/york-urban-line-segment-database							
3782	MuSe-CaR	"The MuSe-CAR database is a large, multimodal (video, audio, and text) dataset which has been gathered in-the-wild with the intention of further understanding Multimodal Sentiment Analysis in-the-wild, e.g., the emotional engagement that takes place during product reviews (i.e., automobile reviews) where a sentiment is linked to a topic or entity. 
The estimated age range of the professional, semi-professional (influncers), and casual reviewers is between the middle of 20s until the late 50s. Most are native English speakers from the UK or the US, while a small minority are non-native, yet fluent English speakers."	https://paperswithcode.com/dataset/muse-car	15/01/2021	Multimodal Sentiment Analysis in Car Reviews					
3783	OpenWPM Crawls	OpenWPM Crawls is a dataset of 103 online, mostly mainstream news websites. With the help of two experts, alongside data from the Media Ownership Monitor of the Reporters without Borders, we label these websites according to their partisanship (Left, Right, or Centre). We study and compare user tracking on these sites with different metrics: numbers of cookies, cookie synchronizations, device fingerprinting, and invisible pixel-based tracking. We find that Left and Centre websites serve more cookies than Right-leaning websites. However, through cookie synchronization, more user IDs are synchronized in Left websites than Right or Centre. Canvas fingerprinting is used similarly by Left and Right, and less by Centre. Invisible pixel-based tracking is 50% more intense in Centre-leaning websites than Right, and 25% more than Left. Desktop versions of news websites deliver more cookies than their mobile counterparts. A handful of third-parties are tracking users in most websites in this study.	https://paperswithcode.com/dataset/openwpm-crawls	06/02/2021						
3784	MRPB 1.0	MRPB 1.0 is a mobile robot local planning benchmark. The benchmark facilitates both motion planning researchers who want to compare the performance of a new local planner relative to many other state-of-the-art approaches as well as end users in the mobile robotics industry who want to select a local planner that performs best on some problems of interest.	https://paperswithcode.com/dataset/mrpb-1-0	01/11/2020						
3785	Algonauts 2021	"The Algonauts dataset provides human brain responses to a set of 1,102 3-s long video clips of everyday events. The brain responses are measured with functional magnetic resonance imaging (fMRI). fMRI is a widely used brain imaging technique with high spatial resolution that measures blood flow changes associated with neural responses. 
Splits:
Training: 
The training set consists of 1,000 video clips and the associated brain responses. The brain responses are provided here in two tracks corresponding to two independent tracks in the Algonauts challenge. In the first track, brain responses provided are from a set of specific regions of interest (ROIs) known to play a key role in visual perception. These ROIs start in early and mid-level visual cortex (V1, V2, V3, and V4) and extend into higher-level cortex that responds preferentially to all objects or particular categories (Body- EBA; Face - FFA, STS; Object - LOC; Scene - PPA).  In the second track, brain responses provided are from selected voxels across the whole brain showing reliable responses to videos.  
Test:
The test set consists of 102 short videos. The associated brain responses will be released at a later date.
For further details see here."	https://paperswithcode.com/dataset/algonauts-2021	28/04/2021	How the Human Brain Makes Sense of a World in Motion					
3786	BLM-17m	BLM-17m is a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of the George Floy incident. The dataset was labelled by monitoring most trending news topics from global and local newspapers.	https://paperswithcode.com/dataset/blm-17m	04/05/2021						
3787	Healthline	Healthline is a nutrition related dataset for multi-document summarization, using scientific studies.	https://paperswithcode.com/dataset/healthline	22/03/2021						
3788	LoLi-Phone	LoLi-Phone is a large-scale low-light image and video dataset for Low-light image enhancement (LLIE). The images and videos are taken by different mobile phones' cameras under diverse illumination conditions.	https://paperswithcode.com/dataset/loli-phone	21/04/2021						
3789	NAVVS	"NAVVS is a volumetric dataset of naturalistic actions whose captured sound and visual appearance yield an open-access resource for immersive and interactive research within an artificial 3D audio-visual environment, such as VR/AR/XR with six degree-of-freedom (6DoF) interaction. It includes a variety of short volumetric sounding actions. It provides a valuable resource for multimodal research and testing under realistic conditions. The dataset includes ten different actions designed with both semantic and acoustic diversity. For each action, four 2-seconds takes are available to provide a total of forty audio-visual clips. 
The scenes were captured at the Centre for Vision, Speech & Signal Processing (CVSSP) of the University of Surrey (UK) with the aid of multiple cameras and multiple microphones.
Along with the final clips' volumetric textured instances and the audio stereo mix, additional data is provided. This includes: the separated microphones' audio channels, raw images from the 16 UHD cameras, binary masks, camera calibration data, coarse visual hull reconstruction, and volumetric stereo refinement."	https://paperswithcode.com/dataset/navvs	03/05/2021	Naturalistic audio-visual volumetric sequences					
3790	FSVOD-500	"FSVOD-500 is a large-scale video dataset comprising of 500 classes with class-balanced videos in each category for few-shot learning. FSVOD-500 is the first benchmark specially designed for few-shot video object detection for evaluating the performance of a given model on novel classes.
Source: Few-Shot Video Object Detection
Image source: Few-Shot Video Object Detection"	https://paperswithcode.com/dataset/fsvod-500	30/04/2021						
3791	Tracking the Trackers	Tracking the Trackers is a large-scale analysis of third-party trackers on the World Wide Web. We extract third-party embeddings from more than 3.5 billion web pages of the CommonCrawl 2012 corpus, and aggregate those to a dataset containing more than 140 million third-party embeddings in over 41 million domains.	https://paperswithcode.com/dataset/tracking-the-trackers	29/07/2016						
3792	Dataset for Mid-Price Forecasting of Limit Order Book Data	This is a benchmark dataset for mid-price forecasting of limit order book data. It is a dataset of high-frequency limit order markets for mid-price prediction. The authors extracted normalized data representations of time series data for five stocks from the NASDAQ Nordic stock market for a time period of ten consecutive days, leading to a dataset of ~4,000,000 time series samples in total. A day-based anchored cross-validation experimental protocol is also provided that can be used as a benchmark for comparing the performance of state-of-the-art methodologies.	https://paperswithcode.com/dataset/dataset-for-mid-price-forecasting-of-limit	09/05/2017						
3793	Analytic Provenance	"Analytic provenance is a data repository that can be used to study human analysis activity, thought processes, and software interaction with visual analysis tools during exploratory data analysis. It was collected during a series of user studies involving exploratory data analysis scenario with textual and cyber security data. Interactions logs, think-alouds, videos and all coded data in this study are available online for research purposes. 
Analysis sessions are segmented in multiple sub-task steps based on user think-alouds, video and audios captured during the studies. These analytic provenance datasets can be used for research involving tools and techniques for analyzing interaction logs and analysis history."	https://paperswithcode.com/dataset/analytic-provenance	16/01/2018						
3794	CSI	CSI is a criminal conversational dataset for speaker identification built from the CSI television show. The authors collected transcripts of 39 episodes and video/audio of 4 episodes. Each episode involves on average more than 30 speakers. Utterances last on average 3 to 4 seconds. There are around 45 to 50 distinct scenes/conversations per episode.	https://paperswithcode.com/dataset/csi	21/09/2020						
3795	Coronavirus-themed Mobile Malware	This is a dataset for coronavirus-themed malware for Android devices. It is a daily growing COVID-19 themed mobile app dataset, which contains 4,322 COVID-19 themed apk samples (2,500 unique apps) and 611 potential malware samples (370 unique malicious apps) by the time of mid-November, 2020.	https://paperswithcode.com/dataset/coronavirus-themed-mobile-malware	29/05/2020						
3796	Dataset of Grouped Commit Author IDs after Identity Resolution	This Dataset contains the IDs of 5,427,024 commit authors who have created commits in git version control system, and have more than 1 ID in git. It is a compressed CSV file (separated by ; ) with 14,861,538 author IDs, where the first column is the group ID, which is same as the first (randomly selected) author ID of the group, and the second column is the author ID that is part of the group. If an author was found to have 2 different IDs: I1, I2, then it is recorded in the file in 2 separate lines, with the lines being I1;I1 and I1;I2, i.e. the first column is the group identifier, which is one of the IDs in a group, and the second column contains the different author IDs in separate lines. This data set contains email addresses for various Git author's, but the '@' within the email address has been replaced with a '#'.	https://paperswithcode.com/dataset/dataset-of-grouped-commit-author-ids-after	27/03/2020						
3797	Sketchy		https://paperswithcode.com/dataset/sketchy	26/09/2019						
3798	DR-VCTK	This dataset is a new variant of the voice cloning toolkit (VCTK) dataset: device-recorded VCTK (DR-VCTK), where the high-quality speech signals recorded in a semi-anechoic chamber using professional audio devices are played back and re-recorded in office environments using relatively inexpensive consumer devices.	https://paperswithcode.com/dataset/dr-vctk	10/11/2019	Device Recorded VCTK					
3799	Shelf&Tote Benchmark Dataset		https://paperswithcode.com/dataset/shelf-tote	29/09/2016	MIT-Princeton Amazon Picking Challenge 2016 Shelf&Tote Benchmark Dataset					
3800	IoT Inspector	IoT Inspector is a large dataset of labeled network traffic from smart home devices from within real-world home networks. It is used to conduct data-driven smart home research. An open source tool with the same name has been used to collect data from 44,956 smart home devices across 13 categories and 53 vendors.	https://paperswithcode.com/dataset/iot-inspector	21/09/2019						
3801	Shelf&Tote Training Dataset		https://paperswithcode.com/dataset/shelf-tote-training-dataset	29/09/2016	MIT-Princeton Amazon Picking Challenge 2016 Shelf&Tote Training Dataset					
3802	BugSwarm	BugSwarm is a dataset of reproducible faults and fixes to perform experimental evaluation of approaches to software quality. The BugSwarm toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers.	https://paperswithcode.com/dataset/bugswarm	22/07/2019						
3803	Peer to Peer Hate	Peer to Peer Hate is a comprehensive hate speech dataset capturing various types of hate. It has been built from 27,330 hate speech tweets.	https://paperswithcode.com/dataset/peer-to-peer-hate	12/04/2018						
3804	Dense Forest Trail	Dense Forest Trail is an UAV dataset collected from a variety of simulated environment in Unreal Engine.	https://paperswithcode.com/dataset/dense-forest-trail	10/06/2018						
3805	MengeROS	MengeROS is an open-source crowd simulation tool for robot navigation that integrates Menge with ROS. It extends Menge to introduce one or more robot agents into a crowd of pedestrians. Each robot agent is controlled by external ROS-compatible controllers. MengeROS has been used to simulate crowds with up to 1000 pedestrians and 20 robots.	https://paperswithcode.com/dataset/mengeros	26/01/2018						
3806	Dizi	Dizi is a dataset of music style of the Northern school and the Southern School. Characteristics include melody and playing techniques of the two different music styles are deconstructed.	https://paperswithcode.com/dataset/dizi	10/11/2020						
3807	P3	A set of patterns used in psychophysical research to evaluate the ability of saliency algorithms to find targets distinct from distractors in orientation, color and size. Each image is a 7x7 grid and contains a single target. All images are 1024x1024px and have corresponding ground truth masks for the target and distractors.	https://paperswithcode.com/dataset/p3	09/09/2019	Psychophysical Patterns Dataset					
3808	O3	"A set of realistic odd-one-out stimuli gathered ""in the wild"". Each image in the Odd-One-Out (O3) dataset depicts a scene with multiple objects similar to each other in appearance (distractors) and a singleton (target) distinct in one or more feature dimensions (e.g. color, shape, size). All images are resized so that the larger dimension is 1024px. Targets represent approx. 400 common object types such as flowers, sweets, chicken eggs, leaves, tiles and birds. Pixelwise masks are provided for targets and distractors. Annotations are generated using CVAT."	https://paperswithcode.com/dataset/o3	09/09/2019	Odd-One-Out Dataset					
3809	KvasirCapsule-SEG	"The dataset contains a Video capsule endoscopy dataset for polyp segmentation. 
The dataset can be downloaded from here:
https://www.kaggle.com/debeshjha1/kvasircapsuleseg
https://www.dropbox.com/home/KvasirCapsule-SEG
The detail about the dataset can be found from 
https://arxiv.org/pdf/2104.11138.pdf"	https://paperswithcode.com/dataset/kvasircapsule-seg	22/04/2021						
3810	Kvasir-Sessile dataset	"The Kvasir-SEG dataset includes 196 polyps smaller than 10 mm classified as Paris class 1 sessile or Paris class IIa. We have selected it with the help of expert gastroenterologists. We have released this dataset separately as a subset of Kvasir-SEG. We call this subset Kvasir-Sessile.
The dataset is publicly available. It can be downloaded from here:
https://drive.google.com/drive/folders/1OjsStQh6yuKz0bG6OA3BzmIiXDZILg7V?usp=sharing
If you use this dataset, please cite our paper,
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9314114
/https://arxiv.org/pdf/1911.07069.pdf
if you use our dataset,"	https://paperswithcode.com/dataset/kvasir-sessile-dataset	16/11/2019	Sessile polyps from Kvasir-SEG					
3811	Kvasir-Capsule	Kvasir-Capsule dataset is the largest publicly released VCE dataset. In total, the dataset contains 47,238 labeled images and 117 videos, where it captures anatomical landmarks and pathological and normal findings. The results is more than 4,741,621 images and video frames altogether.	https://paperswithcode.com/dataset/kvasir-capsule							
3812	Hyper-Kvasir Dataset	HyperKvasir dataset contains 110,079 images and 374 videos where it captures anatomical landmarks and pathological and normal findings. A total of around 1 million images and video frames altogether.	https://paperswithcode.com/dataset/hyper-kvasir-dataset							
3813	pathbased	"pathbased is a 3-cluster data set. The data set consists of a circular cluster with an opening near the bottom and two Gaussian distributed clusters inside. Each cluster contains 100 data points.
Source: Robust path-based spectral clustering"	https://paperswithcode.com/dataset/pathbased							
3814	Gun Detection Dataset	This is a gun detection dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification collected from a few different sources.	https://paperswithcode.com/dataset/gun-detection-dataset	03/05/2021						
3815	RADDet	RADDet is a radar dataset that contains radar data in the form of Range-Azimuth-Doppler tensors along with the bounding boxes on the tensor for dynamic road users, category labels, and 2D bounding boxes on the Cartesian Bird-Eye-View range map. It is used to train and evaluate methods for object detection using automotive radars.	https://paperswithcode.com/dataset/raddet	02/05/2021	Range-Azimuth-Doppler based Radar Dataset					
3816	RoadAnomaly21	RoadAnomaly21 is a dataset for anomaly segmentation, the task of identify the image regions containing objects that have never been seen during training. It consists of an evaluation dataset of 100 images with pixel-level annotations. Each image contains at least one anomalous object, e.g. animals or unknown vehicles. The anomalies can appear anywhere in the image and widely differ in size, covering from 0.5% to 40% of the image	https://paperswithcode.com/dataset/roadanomaly21	30/04/2021						
3817	MARS Map	MARS Map is a set of three dataset collected to evaluate the performance of mapping algorithms within a room and between rooms.	https://paperswithcode.com/dataset/mars-map	23/07/2020						
3818	FFT-75	The FFT-75 dataset contains randomly sampled, potentially overlapping file fragments from 75 popular file types. It is a diverse and balanced dataset which is labeled with class IDs and is ready for training supervised machine learning models. We distinguish 6 different scenarios with different granularity and provide variants with 512 and 4096-byte blocks. In each case, we sampled a balanced dataset and split the data as follows: 80% for training, 10% for testing and 10% for validation.	https://paperswithcode.com/dataset/fft-75	16/08/2019						
3819	Windows PE Malware	This is a dataset for the task of PE-type malware in the Windows operating system. The different samples in the dataset are classified into 8 main malware families: Trojan, Backdoor, Downloader, Worms, Spyware Adware, Dropper, Virus.	https://paperswithcode.com/dataset/windows-pe-malware	06/05/2019						
3820	Bonn RGB-D Dynamic	Bonn RGB-D Dynamic is a dataset for RGB-D SLAM, containing highly dynamic sequences. We provide 24 dynamic sequences, where people perform different tasks, such as manipulating boxes or playing with balloons, plus 2 static sequences. For each scene we provide the ground truth pose of the sensor, recorded with an Optitrack Prime 13 motion capture system. The sequences are in the same format as the TUM RGB-D Dataset, so that the same evaluation tools can be used. Furthermore, we provide a ground truth 3D point cloud of the static environment recorded using a Leica BLK360 terrestrial laser scanner.	https://paperswithcode.com/dataset/bonn-rgb-d-dynamic	06/05/2019						
3821	VMRD	VMRD is a multi-object grasp dataset. It has been collected and labeled using hundreds of objects coming from 31 categories. There are totally 5,185 images including 17,688 object instances and 51,530 manipulation relationships.	https://paperswithcode.com/dataset/vmrd	30/08/2018	Visual Manipulation Relationship Dataset					
3822	TSP/HCP Benchmark set	This is a benchmark set for Traveling salesman problem (TSP) with characteristics that are different from the existing benchmark sets. In particular, it focuses on small instances which prove to be challenging for one or more state-of-the-art TSP algorithms. These instances are based on difficult instances of Hamiltonian cycle problem (HCP). This includes instances from literature, specially modified randomly generated instances, and instances arising from the conversion of other difficult problems to HCP.	https://paperswithcode.com/dataset/tsp-hcp-benchmark-set	25/06/2018						
3823	Logic Bombs	This is a set of small programs with logic bombs. The logic bomb can be triggered when certain conditions are met. Any dynamic testing tools (especially symbolic execution) can employ the dataset to benchmark their capabilities.	https://paperswithcode.com/dataset/logic-bombs	25/05/2018						
3824	CapriDB	CapriDB is a 3D object database for robotics.	https://paperswithcode.com/dataset/capridb	17/10/2016						
3825	Xamarin Q&A	Xamarin Q&A consists of two datasets of questions and answers for studying the development of cross-platform mobile applications using the Xamarin framework. The two datasets were created by mining two Q&A sites: Xamarin Forum and Stack Overflow. The datasets have 85,908 questions mined from the Xamarin Forum and 44,434 from Stack Overflow.	https://paperswithcode.com/dataset/xamarin-q-a	27/12/2017						
3826	3DSSG	3DSSG provides 3D semantic scene graphs for 3RScan. A semantic scene graph is defined by a set of tuples between nodes and edges where nodes represent specific 3D object instances in a 3D scan. Nodes are defined by its semantics, a hierarchy of classes as well as a set of attributes that describe the visual and physical appearance of the object instance and their affordances. The edges in our graphs are the semantic relationships (predicates) between the nodes such as standing on, hanging on, more comfortable than or same material.	https://paperswithcode.com/dataset/3dssg	08/04/2020						
3827	AudioCaps	"AudioCaps is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the AudioSet dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed).
Source: Audio Retrieval with Natural Language Queries
Image source: https://audiocaps.github.io/"	https://paperswithcode.com/dataset/audiocaps	01/06/2019						
3828	ToolNet	The dataset is organized as follows. We have 8 different goals and 10 different world instances for both the domains, home and factory. Each domain has 8 directories corresponding to the goals possible for the domain. These goals itself, contain directories for the 10 different world instances. Each goal for each world instance in a particular domain thus has a number of different human demonstrations, and these are saved in the form of a .datapoint file for each plan.	https://paperswithcode.com/dataset/toolnet	09/06/2020						
3829	CollATe	The CollATe dataset is large dataset consisting of two types of collusive entities on YouTube – videos submitted to gain collusive likes and comment requests, and channels submitted to gain collusive subscriptions.	https://paperswithcode.com/dataset/collate	13/05/2020						
3830	Semantic Trails	Semantic Trails Datasets (STDs) are two different datasets of semantically annotated trails created starting from check-ins performed on the Foursquare social network.	https://paperswithcode.com/dataset/semantic-trails	30/12/2019						
3831	360 EM	Data set of 360-degree equirectangular videos, gaze recordings, eye movement (EM) ground-truth and an automatic EM classification algorithm.	https://paperswithcode.com/dataset/360-em	15/03/2019						
3832	Innovation and Revenue	This is a dataset that catalogs 2.6 million patents granted between 2005 and 2017.	https://paperswithcode.com/dataset/innovation-and-revenue	21/04/2020						
3833	Pull Request Descriptions	This is a dataset of over 333K Pull Requests, used for automatic pull request description generation.	https://paperswithcode.com/dataset/pull-request-descriptions	16/09/2019						
3834	Reddit Norm Violations	This is a dataset of over 40K Reddit comments removed by moderators according to the specific type of macro norm being violated.	https://paperswithcode.com/dataset/reddit-norm-violations	07/04/2019						
3835	DBLP Temporal	DBLP Temporal is a dataset for temporal entity resolution, based on author profiles extracted from the Digital Bibliography and Library Project (DBLP).	https://paperswithcode.com/dataset/dblp-temporal	20/06/2018						
3836	Rediscovery Datasets	We present three defect rediscovery datasets mined from Bugzilla. The datasets capture data for three groups of open source software projects: Apache, Eclipse, and KDE. The datasets contain information about approximately 914 thousands of defect reports over a period of 18 years (1999-2017) to capture the inter-relationships among duplicate defects.	https://paperswithcode.com/dataset/rediscovery-datasets	18/03/2017						
3837	FLOBOT Perception	This dataset was collected with FLOBOT - an advanced autonomous floor scrubber - includes data from four different sensors for environment perception, as well as the robot pose in the world reference frame. Specifically, FLOBOT relies on a 3D lidar and a RGB-D camera for human detection and tracking, and a second RGB-D and a stereo camera for dirt and object detection. Data collection was performed in four public places (three of them are released in this dataset), two in Italy and two in France, in FOLBOT working mode with the corresponding testing procedures for final project validation	https://paperswithcode.com/dataset/flobot-perception	24/02/2020						
3838	Collision Avoidance Challenge dataset	"The Collision Avoidance Challenge dataset is the official dataset used during the ESA's Kelvins competition for ""Collision Avoidance Challenge"". The dataset is a collection of Conjunction Data Messages (CDMs) received by ESA from 2015 to 2019. The CDMs have been anonymised for distribution. The initial raw data, as well as the labels that were kept private during the competition, are also released.
ESA thanks the US Space Surveillance Network for the provision of surveillance data supporting safe operations of ESA’s spacecraft. In addition, we are grateful to the agreement which allows to publicly release the current dataset.
The dataset is represented as a table, where each row corresponds to a single CDM, and each CDM contains 103 recorded characteristics/features. There are thus 103 columns, which are described in the competition pages. The dataset is made of several unique collision/close approach events, which are identified in the event_id column. In turn, each collision event is made of several CDMs recorded over time. Therefore, a single collision event can be thought of as a times series of CDMs. From these CDMs, for every collision event, we are interested in predicting the final risk which is computed in the last CDM of the time series (i.e. the risk value in the last row of each collision event).
For a detailed description on the challenge and this dataset, visit https://kelvins.esa.int/collision-avoidance-challenge/data/.
The paper describing the competition setup and result can be found at https://arxiv.org/pdf/2008.03069.pdf."	https://paperswithcode.com/dataset/collision-avoidance-challenge-dataset	25/01/2021						
3839	OREBA	The OREBA dataset aims to provide a comprehensive multi-sensor recording of communal intake occasions for researchers interested in automatic detection of intake gestures. Two scenarios are included, with 100 participants for a discrete dish and 102 participants for a shared dish, totalling 9069 intake gestures. Available sensor data consists of synchronized frontal video and IMU with accelerometer and gyroscope for both hands.	https://paperswithcode.com/dataset/oreba	31/07/2020	Objectively Recognizing Eating Behavior and Associated Intake					
3840	IWSLT2015	"The IWSLT 2015 Evaluation Campaign featured three tracks: automatic speech recognition (ASR), spoken language translation (SLT), and machine translation (MT). For ASR we offered two tasks, on English and German, while
for SLT and MT a number of tasks were proposed, involving English, German, French, Chinese, Czech, Thai, and Vietnamese. All tracks involved the transcription or translation of TED talks, either made available by the official TED website or by other TEDx events. A notable change with respect to previous evaluations was the use of unsegmented speech in the SLT track in order to better fit a real application scenario."	https://paperswithcode.com/dataset/iwslt2015		IWSLT2015					
3841	MIMII DUE	"This dataset is a sound dataset for malfunctioning industrial machine investigation and inspection with domain shifts due to changes in operational and environmental conditions (MIMII DUE). The dataset consists of normal and abnormal operating sounds of five different types of industrial machines, i.e., fans, gearboxes, pumps, slide rails, and valves. The data for each machine type includes six subsets called ""sections'', and each section roughly corresponds to a single product. Each section consists of data from two domains, called the source domain and the target domain, with different conditions such as operating speed and environmental noise. This dataset is a subset of the dataset for DCASE 2021 Challenge Task 2, so the dataset is entirely the same as data included in the development dataset and additional training dataset."	https://paperswithcode.com/dataset/mimii-due	06/05/2021						
3842	DroneCrowd	DroneCrowd is a benchmark for object detection, tracking and counting algorithms in drone-captured videos. It is a drone-captured large scale dataset formed by 112 video clips with 33,600 HD frames in various scenarios. Notably, it has annotations for 20,800 people trajectories with 4.8 million heads and several video-level attributes.	https://paperswithcode.com/dataset/dronecrowd	06/05/2021						
3843	Content4All	"Content4All is a collection of six open research datasets aimed at automatic sign language translation research.
Sign language interpretation footage was captured by the broadcast partners SWISSTXT and VRT. Raw footage was anonymized and processed to extract 2D and 3D human
body pose information. From the roughly 190 hours of processed data, three base (RAW) datasets were released, namely 1) SWISSTXT-RAW-NEWS, 2) SWISSTXT-RAW-WEATHER and 3) VRT-RAW. Each dataset contains sign language interpretations, corresponding spoken language subtitles, and extracted 2D/3D human pose information.
A subset from each base dataset was selected and manually annotated to align spoken language subtitles and sign language interpretations. The subset selection was
done to resemble the benchmark Phoenix 2014T dataset. Our aim is for these three new annotated public datasets, namely 4) SWISSTXT-NEWS, 5) SWISSTXT-WEATHER
and 6) VRT-NEWS to become benchmarks and underpin future research as the field moves closer to translation and production on larger domains of discourse."	https://paperswithcode.com/dataset/content4all	05/05/2021						
3844	MIAP	"MIAP is a dataset created by obtaining a new set of annotations on a subset of the Open Images dataset, containing bounding boxes and attributes for all of the people visible in those images, as the original Open Images dataset annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image.
The MIAP dataset focuses on enabling ML Fairness research. It provides additional annotations for 100,000 (70k from training and 30k from validation/test) images that contain at least one person bounding box in the original annotations.
These additional annotations provide exhaustive bounding boxes for all people in an image. Person boxes are further annotated with attribute labels for fairness research. Annotated attributes include the human perceived gender presentation (predominantly feminine, predominantly masculine, and unknown) and perceived age range (young, middle, older, and unknown) of the localized person. This procedure adds nearly 100,000 new boxes that were not annotated under the original labeling pipeline.
Annotations on the exhaustive set enable research into the fairness properties of models trained on partial annotations and the pipelines that produce these annotations."	https://paperswithcode.com/dataset/miap	05/05/2021	More Inclusive Annotations for People					
3845	MQTT-IoT-IDS2020	"Message Queuing Telemetry Transport (MQTT) protocol is one of the most used standards used in Internet of Things (IoT) machine to machine communication. The increase in the number of available IoT devices and used protocols reinforce the need for new and robust Intrusion Detection Systems (IDS). However, building IoT IDS requires the availability of datasets to process, train and evaluate these models. 
MQTT-IoT-IDS2020 is the first dataset to simulate an MQTT-based network. The dataset is generated using a simulated MQTT network architecture. The network comprises twelve sensors, a broker, a simulated camera, and an attacker. Five scenarios are recorded: (1) normal operation, (2) aggressive scan, (3) UDP scan, (4) Sparta SSH brute-force, and (5) MQTT brute-force attack.  The raw pcap files are saved, then features are extracted. Three abstraction levels of features are extracted from the raw pcap files: (a) packet features, (b) Unidirectional flow features and (c) Bidirectional flow features. The csv feature files in the dataset are suited for Machine Learning (ML) usage. Also, the raw pcap files are suitable for the deeper analysis of MQTT IoT networks communication and the associated attacks."	https://paperswithcode.com/dataset/mqtt-iot-ids2020	16/11/2020						
3846	Backstabber’s Knife Collection	Backstabber’s Knife Collection is a dataset of 174 malicious software packages that were used in real-world attacks on open source software supply chains, and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed.	https://paperswithcode.com/dataset/backstabbers-knife-collection	19/05/2020						
3847	Ukiyo-e Faces	The ukiyo-e faces dataset comprises of 5209 images of faces from ukiyo-e prints. The images are 1024x1024 pixels in jpeg format and have been aligned using the procedure used for the FFHQ dataset	https://paperswithcode.com/dataset/ukiyo-e-faces	11/10/2020						
3848	Data Loss repository	This is a benchmark of data loss bugs for android apps. It is a public benchmark of 110 data loss faults in Android apps that we systematically collected to facilitate research and experimentation with these problems. The benchmark is available on GitLab and includes the faulty apps, the fixed apps (when available), the test cases to automatically reproduce the problems, and additional information that may help researchers in their tasks.	https://paperswithcode.com/dataset/data-loss-repository	27/05/2019						
3849	RePack	RePack is a dataset to study the detection of repackaged Android apps.	https://paperswithcode.com/dataset/repack	20/11/2018						
3850	PFN-VT	PFN-VT is a dataset for the estimation of tactile properties from vision, such as slipperiness or roughness. The dataset is collected with a webcam and uSkin tactile sensor mounted on the end-effector of a Sawyer robot, which strokes the surfaces of 25 different materials.	https://paperswithcode.com/dataset/pfn-vt	09/03/2018	PFN Visuo-Tactile Dataset					
3851	Twitter Abusive Behavior	80k tweets annotated concerning Inappropriate Speech (more particularly in matters of Abusive and Hateful speech) as well as Normal and Spam.	https://paperswithcode.com/dataset/twitter-abusive-behavior	01/02/2018						
3852	CinemAirSim	CinemAirSim is an extension of the well-known drone simulator, AirSim, with a cinematic camera as well as extended its API to control all of its parameters in real time, including various filming lenses and common cinematographic properties.	https://paperswithcode.com/dataset/cinemairsim	17/03/2020						
3853	ICDCN2019	This is a dataset consisting of complete network traces comprising benign and malicious traffic, which is feature-rich and publicly available.	https://paperswithcode.com/dataset/icdcn2019	23/10/2018						
3854	Mal-Activity	"This is a dataset of Internet malicious activity (mal-activity in short). It contains more than 51 million mal-activity reports involving 662K unique IP addresses covering the period form January 2007 to June 2017. Leveraging the Wayback Machine, antivirus (AV) tool reports and several additional public datasets (e.g., BGP Route Views and Internet registries) the data is enriched with historical meta-information including geo-locations (countries), autonomous system (AS) numbers and types of mal-activity. 
An initially labelled dataset of approx 1.57 million mal-activities (obtained from public blacklists) is used to train a machine learning classifier to classify the remaining unlabeled dataset of approx 44 million mal-activities obtained through additional sources."	https://paperswithcode.com/dataset/mal-activity	24/04/2019						
3855	α-Satellite	This is a collection of datasets related to Covid-19. It consists of large scale and real-time pandemic related data from multiple sources, including disease related data from official public health organizations and digital media, demographic data, mobility data, and user generated data from social media (i.e., Reddit).	https://paperswithcode.com/dataset/a-satellite	27/03/2020						
3856	VAST Absorption	VAST Absorption is a dataset of spatial binaural features annotated with acoustic properties such as the 3D source position and the walls’ absorption coefficients.	https://paperswithcode.com/dataset/vast-absorption	20/03/2017						
3857	On the Origins of Memes by Means of Fringe Web Communities	"This dataset was collected with research funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie Grant Agreement No 691025.
The dataset consists of all the URLs and phashes for images from Twitter, Reddit, 4chan's /pol/, and Gab posted between July 2016 and end of July 2017."	https://paperswithcode.com/dataset/on-the-origins-of-memes-by-means-of-fringe	31/05/2018						
3858	Bimanual Actions Dataset	The Bimanual Actions Dataset is a collection of 540 RGB-D videos, showing subjects perform bimanual actions in a kitchen or workshop context. The main purpose for its compilation is to research bimanual human behaviour in order to eventually improve the capabilities of humanoid robots.	https://paperswithcode.com/dataset/bimanual-actions-dataset	12/09/2019						
3859	ColosseumRL	"ColosseumRL is a framework for research in reinforcement learning in n-player games.
ColosseumRL contains a number of multiagent free-for-all games. Currently, we have Tron, Blokus, and 3 and 4-player tic-tac-toe. In the future, we will be adding Chinese checkers and other similar games. Tron is a fully-observable multiagent free-for-all turn-based snake variant where players try to survive the longest without crashing into walls or each other. Blokus is a fully-observable multiagent free-for-all turn-based game in which players place pieces on a board to claim space and strategically block opponents from placing their own pieces."	https://paperswithcode.com/dataset/colosseumrl	10/12/2019						
3860	Online Cryptocurrency-topic diffusion on Twitter, Telegram, and Discord	"This Dataset is described in Charting the Landscape of Online Cryptocurrency Manipulation. IEEE Access (2020), a study that aims to map and assess the extent of cryptocurrency manipulations within and across the online ecosystems of Twitter, Telegram, and Discord. Starting from tweets mentioning cryptocurrencies, we leveraged and followed invite URLs from platform to platform, building the invite-link network, in order to study the invite link diffusion process.
Please, refer to the paper below for more details.
Nizzoli, L., Tardelli, S., Avvenuti, M., Cresci, S., Tesconi, M. & Ferrara, E. (2020). Charting the Landscape of Online Cryptocurrency Manipulation. IEEE Access (2020).
This dataset is composed of: 
~16M tweet ids shared between March and May 2019, mentioning at least one of the 3,822 cryptocurrencies (cashtags) provided by the CryptoCompare public API;
~13k nodes of the invite-link network, i.e., the information about the Telegram/Discord channels and Twitter users involved in the cryptocurrency discussion (e.g., id, name, audience, invite URL);
~62k edges of the invite-link network, i.e., the information about the flow of invites (e.g., source id, target id, weight)."	https://paperswithcode.com/dataset/online-cryptocurrency-topic-diffusion-on	28/01/2020						
3861	3D-Printing-Data	This is a dataset for anomalies detection in 3D printing.	https://paperswithcode.com/dataset/3d-printing-data	19/04/2020						
3862	Grasp Affordance	This is a dataset for visual grasp affordance prediction that promotes more robust and heterogeneous robotic grasping methods. The dataset contains different attributes from 30 different objects. Each object instance is related not only to the semantic descriptions, but also the physical features describing visual attributes, locations and different grasping regions related to a variety of actions.	https://paperswithcode.com/dataset/grasp-affordance	24/06/2019						
3863	DeformingThings4D	"DeformingThings4D is a synthetic dataset containing 1,972 animation sequences spanning 31 categories of humanoids and animals. It provides 200 animations for humanoids and 1772 animations for animals. 
Use case of the dataset
The dataset is designed to tackle the following tasks using data-driven approaches:


Scene flow estimation


Non-rigid tracking/registration


Shape and motion completion


Learning riggings from observation


Generic non-rigid reconstruction


Source: DeformingThings4D dataset
Image source: DeformingThings4D dataset"	https://paperswithcode.com/dataset/deformingthings4d	05/05/2021						
3864	Visual Servoing	"Dataset for visual servoing (VS) and camera pose estimation. 
The images were obtained by a manipulator robot with an eye-in-hand camera in different poses. 
The labels represent the camera pose. 
It is possible to obtain the absolute pose of the camera without any pre-processing of the dataset, as well as the relative pose between images through matrix transformations. 
One may also use the dataset to get the camera's 6DoF speeds so that the visual servo control between two images can be performed."	https://paperswithcode.com/dataset/visual-servoing	13/10/2020						
3865	DeepStab	DeepStab is a dataset for online video stabilization consisting of synchronized steady/unsteady video pairs collected via a well designed hand-held hardware.	https://paperswithcode.com/dataset/deepstab	22/02/2018						
3866	CxC	"Crisscrossed Captions (CxC) contains 247,315 human-labeled annotations including positive and negative associations between image pairs, caption pairs and image-caption pairs.
Image source: Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO"	https://paperswithcode.com/dataset/cxc	30/04/2020	Crisscrossed Captions					
3867	AdversarialQA	"We have created three new Reading Comprehension datasets constructed using an adversarial model-in-the-loop.
We use three different models; BiDAF (Seo et al., 2016), BERTLarge (Devlin et al., 2018), and RoBERTaLarge (Liu et al., 2019) in the annotation loop and construct three datasets; D(BiDAF), D(BERT), and D(RoBERTa), each with 10,000 training examples, 1,000 validation, and 1,000 test examples.
The adversarial human annotation paradigm ensures that these datasets consist of questions that current state-of-the-art models (at least the ones used as adversaries in the annotation loop) find challenging. The three AdversarialQA round 1 datasets provide a training and evaluation resource for such methods."	https://paperswithcode.com/dataset/adversarialqa	02/02/2020						
3868	3DCSR dataset	Cross-source point cloud dataset for registration task. It includes point clouds from structure from motion (SFM), Kinect, Lidar.	https://paperswithcode.com/dataset/3dcsr-dataset	03/03/2021	3D cross-source point cloud registration dataset					
3869	DTU	DTU MVS 2014 is a multi-view stereo dataset, which is an order of magnitude larger in number of scenes and with a significant increase in diversity. Specifically, it contains 80 scenes of large variability. Each scene consists of 49 or 64 accurate camera positions and reference structured light scans, all acquired by a 6-axis industrial robot.	https://paperswithcode.com/dataset/dtu	01/06/2014	DTU MVS dataset - 2014					
3870	Tanks and Temples	"We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.
Paper: Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene
reconstruction. In ACM Transactions on Graphics (TOG), 2017."	https://paperswithcode.com/dataset/tanks-and-temples	30/07/2017						
3871	IAPR TC-12	The image collection of the IAPR TC-12 Benchmark consists of 20,000 still natural images taken from locations around the world and comprising an assorted cross-section of still natural images. This includes pictures of different sports and actions, photographs of people, animals, cities, landscapes, and many other aspects of contemporary life. Each image is associated with a text caption in up to three different languages (English, German and Spanish).	https://paperswithcode.com/dataset/iapr-tc-12	24/05/2006	IAPR TC-12 Benchmark					
3872	QASPER	QASPER is a dataset for question answering on scientific research papers. It consists of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers.	https://paperswithcode.com/dataset/qasper	07/05/2021						
3873	im2latex-100k	"A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~100k formulas and images splitted into train, validation and test sets. Formulas were parsed from LaTeX sources provided here: http://www.cs.cornell.edu/projects/kddcup/datasets.html(originally from  arXiv)
Each image is a PNG image of fixed size. Formula is in black and rest of the image is transparent.
For related tools (eg. tokenizer) check out this repository: https://github.com/Miffyli/im2latex-dataset
For pre-made evaluation scripts and built im2latex system check this repository: https://github.com/harvardnlp/im2markup
Newlines used in formulas_im2latex.lst are UNIX-style newlines (\n). Reading file with other type of newlines results to slightly wrong amount of lines (104563 instead of 103558), and thus breaks the structure used by this dataset. Python 3.x reads files using newlines of the running system by default, and to avoid this file must be opened with newlines=""\n"" (eg. open(""formulas_im2latex.lst"", newline=""\n"")).
Source: https://zenodo.org/record/56198#.YJjuCGZKgox
Image source: https://arxiv.org/pdf/1609.04938v2.pdf"	https://paperswithcode.com/dataset/im2latex-100k	16/09/2016						
3874	RLBench	RLBench is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning.	https://paperswithcode.com/dataset/rlbench	26/09/2019						
3875	MULTEXT-East	"The MULTEXT-East resources are a multilingual dataset for language engineering research and development. It consists of the (1) MULTEXT-East morphosyntactic specifications, defining categories (parts-of-speech), their morphosyntactic features (attributes and values), and the compact MSD tagset representations; (2) morphosyntactic lexica, (3) the annotated parallel ""1984"" corpus; and (4) some comparable text and speech corpora. The specifications are available for the following macrolanguages, languages and language varieties: Albanian, Bulgarian, Chechen, Czech, Damaskini, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbo-Croatian, Slovak, Slovene, Torlak, and Ukrainian, while the other resources are available for a subset of these languages."	https://paperswithcode.com/dataset/multext-east	31/03/2020						
3876	SketchyCOCO	"SketchyCOCO dataset consists of two parts:
Object-level data
Object-level data contains $20198(train18869+val1329)$ triplets of {foreground sketch, foreground image, foreground edge map} examples covering 14 classes, $27683(train22171+val5512)$ pairs of {background sketch, background image} examples covering 3 classes.
Scene-level data
Scene-level data contains $14081(train 11265 + val 2816)$ pairs of {foreground image&background sketch, scene image} examples, $14081(train 11265 + val 2816)$ pairs of {scene sketch, scene image} examples and the segmentation ground truth for $14081(train 11265 + val 2816)$ scene sketches. Some val scene images come from the train images of the COCO-Stuff dataset for increasing the number of the val images of the SketchyCOCO dataset.
Source: https://github.com/sysu-imsl/SketchyCOCO
Image source: https://arxiv.org/pdf/2003.02683v5.pdf"	https://paperswithcode.com/dataset/sketchycoco	05/03/2020						
3877	Scribble	"Scribble is a new outline dataset consisting of 200 images (150 train, 50 test) for each of 10 classes – basketball, chicken, cookie, cupcake, moon, orange, soccer, strawberry, watermelon and pineapple. All the images have a white background and were collected using search keywords on popular search engines. In each image, we obtain rough
outlines for the image. We find the largest blob in the image after thresholding it into a black and white image. We fill the interior holes of the largest blob and obtain a smooth outline using the SavitzkyGolay filter.
Source: https://arxiv.org/pdf/1909.11081v2.pdf"	https://paperswithcode.com/dataset/scribble	24/09/2019						
3878	Milling Data Set	Experiments on a metal milling machine for different speeds, feeds, and depth of cut. Records the wear of the milling insert, VB. The data set was provided by the BEST lab at UC Berkeley.	https://paperswithcode.com/dataset/milling-data-set	01/01/2007	UC Berkeley Milling Data Set					
3879	DeepTrash		https://paperswithcode.com/dataset/deeptrash	05/05/2021						
3880	Wiki-Reliability	"Wiki-Reliability is the first dataset of English Wikipedia articles annotated with a wide set of content reliability issues. Templates are tags used by expert Wikipedia editors to indicate content issues, such as the presence of ""non-neutral point of view"" or ""contradictory articles"", and serve as a strong signal for detecting reliability issues in a revision. We select the 10 most popular reliability-related templates on Wikipedia, and propose an effective method to label almost 1M samples of Wikipedia article revisions as positive or negative with respect to each template. Each positive/negative example in the dataset comes with the full article text and 20 features from the revision's metadata. We provide an overview of the possible downstream tasks enabled by such data, and show that Wiki-Reliability can be used to train large-scale models for content reliability prediction."	https://paperswithcode.com/dataset/wiki-reliability	10/05/2021						
3881	ExpMRC	ExpMRC is a benchmark for the Explainability evaluation of Machine Reading Comprehension. ExpMRC contains four subsets of popular MRC datasets with additionally annotated evidences, including SQuAD, CMRC 2018, RACE+ (similar to RACE), and C3, covering span-extraction and multiple-choice questions MRC tasks in both English and Chinese.	https://paperswithcode.com/dataset/expmrc	10/05/2021						
3882	DiagSet	DiagSet is a histopathological dataset for prostate cancer detection. The proposed dataset consists of over 2.6 million tissue patches extracted from 430 fully annotated scans, 4675 scans with assigned binary diagnosis, and 46 scans with diagnosis given independently by a group of histopathologists.	https://paperswithcode.com/dataset/diagset	09/05/2021						
3883	gComm	gComm is a step towards developing a robust platform to foster research in grounded language acquisition in a more challenging and realistic setting. It comprises a 2-D grid environment with a set of agents (a stationary speaker and a mobile listener connected via a communication channel) exposed to a continuous array of tasks in a partially observable setting. The key to solving these tasks lies in agents developing linguistic abilities and utilizing them for efficiently exploring the environment. The speaker and listener have access to information provided in different modalities, i.e. the speaker's input is a natural language instruction that contains the target and task specifications and the listener's input is its grid-view. Each must rely on the other to complete the assigned task, however, the only way they can achieve the same, is to develop and use some form of communication. gComm provides several tools for studying different forms of communication and assessing their generalization.	https://paperswithcode.com/dataset/gcomm	09/05/2021						
3884	e-ViL	"e-ViL is a benchmark for explainable vision-language tasks. e-ViL spans across three datasets of human-written NLEs (natural language explanations), and provides a unified evaluation framework that is designed to be re-usable for future works.
This benchmark uses the following datasets: e-SNLI-VE, VCR, VQA-X."	https://paperswithcode.com/dataset/e-vil	08/05/2021						
3885	e-SNLI-VE	e-SNLI-VE is a large VL (vision-language) dataset with NLEs (natural language explanations) with over 430k instances for which the explanations rely on the image content. It has been built by merging the explanations from e-SNLI and the image-sentence pairs from SNLI-VE.	https://paperswithcode.com/dataset/e-snli-ve	08/05/2021						
3886	HLGD	"The Headline Grouping dataset is a binary classification dataset on pairs of news headline.
For each pair of headline, the binary label indicates whether the two headlines are part of the same group (and describe the same underlying event), or whether they are in distinct groups.
The dataset contains a total of 20k annotated headline pairs, further split in a train, validation and test portions."	https://paperswithcode.com/dataset/hlgd	12/05/2021	Headline Grouping Dataset					
3887	CRUW	CRUW is a dataset for the radar object detection (ROD) task, which aims to classify and localize the objects in 3D purely from radar's radio frequency (RF) images. The CRUW dataset has a systematic annotation and evaluation system, which involves camera RGB images and radar RF images, collected in various driving scenarios.	https://paperswithcode.com/dataset/cruw	11/05/2021						
3888	ImageNet-to-Bing		https://paperswithcode.com/dataset/imagenet-to-bing		ImageNet-to-Bing					
3889	FoodSeg103	"FoodSeg103 is a new food image dataset containing 7,118 images. Images are annotated with 104 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. It's provided as a large-scale benchmark for food image segmentation.
Major Challenges:

High intra-variance of the same food ingredient with different cooking methods
Long-tail distribution
Complicated contexts

Image source: https://arxiv.org/pdf/2105.05409v1.pdf"	https://paperswithcode.com/dataset/foodseg103	12/05/2021						
3890	Kleister NDA	Kleister NDA is a dataset for Key Information Extraction (KIE). The dataset contains a mix of scanned and born-digital long formal English-language documents. For this datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features.  The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract.	https://paperswithcode.com/dataset/kleister-nda	12/05/2021						
3891	LabPics	"LabPics Chemistry Dataset
LabPics Medical Dataset
Dataset for computer vision for autonomous chemistry labs and medical labs.
Experimental chemistry consists largely of the handling of materials in vessels.  Whether it involves moving and mixing liquids, dissolving or precipitating solids, or extraction and distillation, these manipulations almost always consist of handling materials within transparent containers and depend heavily on visual recognition.  For chemists in the lab, it is crucial not only to be able to identify the vessel and the fill level of the material inside it but also to be able to accurately identify the region and phase boundaries of each individual material phase as well as its type (liquid, solid, foam, suspension, powder, etc.).
Format
The dataset contains annotated images for both material and vessels in chemistry labs, medical labs, and any area where liquids and solids are handled within vessels. There are two levels of annotation for each image. One annotation set for vessels and the second for the material phases inside these vessels. Vessels are defined as any container that can carry materials such as Jars, Erlenmayers, Tubes, Funnels, syringes, IV bags, and any other labware or glassware that can contain or carry materials. Material phases are any material contained within or on the vessel. For example, for two-phase separating liquids, each liquid phase is annotated as one instance. If there is foam above the liquid or a chunk of solid inside the liquid, the foam, liquid, and solid will be annotated as different phases. In addition, vessel parts like cork, labels, and valves are annotated as instances. For each instance, there is a list of all the classes it belongs to, and a list of its property. For vessels, the instance classes are the vessel type (Cup, jar, Separatory-funnel…) and the vessel properties (Transparent, Opaque…). For materials, the classes are the material types ( Liquid, solid, suspension, foam, powder…) and their properties (Scattered, On vessel surface…), and for parts, the part type (cork/label). In addition, the relations between instances are annotated. This includes which materials instances are inside which vessels, which vessels are linked to each other or are inside each other (for vessels inside other vessels), and which material phase is immersed inside another material phase. In addition to instance segmentation maps, the dataset also includes semantic segmentation maps that give for each pixel in the image all the classes to which it belongs. In other words, for each class (Liquid, Solid, Vessel, Foam), there is a map of all the regions in the image belonging to this class. Note that every pixel and every instance can have several classes. In addition, instances often overlap, like in the case of material inside the vessel, vessel inside the vessel, and material phase immerse inside other material (like solid inside liquid).
File sources and copyright
Creating this dataset was impossible without a community of chemists who take and share high-quality photos of their experiments. Most of the images of this dataset were shared by these sources. Copyright for all images belongs to their contributors. The dataset is shared for academic purposes only. For any other use of the images, inquire with the image source. The source of every image is mention in the source.txt file located next to the image file. Specific copyright for each image also appears in the same file. If no copyright is mentioned, contact the image source for inquiries. See Image sources and copyright file in main folder and source.txt in image folder for detail.
""This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government"	https://paperswithcode.com/dataset/labpics	04/05/2021	LabPics Dataset for computer vision for autonomous chemistry labs and medical labs					
3892	TextOCR	"TextOCR is a dataset to benchmark text recognition on arbitrary shaped scene-text. TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning.
Dataset statistics:

28,134 natural images from TextVQA
903,069 annotated scene-text words
32 words per image on average"	https://paperswithcode.com/dataset/textocr	12/05/2021						
3893	PINN	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/pinn							
3894	Scientific statement classification dataset from arXMLiv 08.2018	"This resource contains 10.5 million paragraphs with associated statement labels, realized as one paragraph per file, one sentence per line. Each file is placed in a subdirectory named after its annotated class. The statements were extracted from author-annotated environments, where we only selected the first paragraph,immediately following the heading. Headings include both structural sections (e.g. Introduction), as well as scholarly statement annotations, (e.g. Definition, Proof, Remark).
The annotated statement dataset is derived from arXMLiv, a machine-readable HTML5 representation of the arXiv corpus of scientific articles.
Examples
Definition with math lexemes (main data, single sentence, linebreaks for readability):
a directed quantum turing automaton is a quadruple
  italic_T RELOP_equals OPEN_( caligraphic_H PUNCT_, caligraphic_K PUNCT_, caligraphic_L PUNCT_, italic_tau CLOSE_) PUNCT_,
where
  caligraphic_H caligraphic_K and caligraphic_L
are finite dimensional hilbert spaces over the complex field blackboard_C and
  italic_tau METARELOP_colon caligraphic_H MULOP_tensor_product caligraphic_K ARROW_rightarrow
    caligraphic_H MULOP_tensor_product caligraphic_L
is an isometry in fdhilb
source: definition/1e4a1aea317bbf363c5314fb25eaf72c8a350a1007bb8aafc542e188405b93d5.txt
Same definition without math lexemes (nomath data, single sentence, linebreaks for readability):
a directed quantum turing automaton is a quadruple
  where and are finite dimensional hilbert spaces over the complex field and
  is an isometry in fdhilb
nomath source: definition/35b170bae4259a5c430846116142d4e4a45097e52daf818b78ea378d94d14a21.txt"	https://paperswithcode.com/dataset/scientific-statement-classification-dataset	29/08/2019						
3895	arXMLiv:08.2018	"This is a second public release of the arXMLiv dataset generated by the KWARC research group. It contains 1,232,186 HTML5 scientific documents from the arXiv.org preprint archive, converted from their respective TeX sources. A 13% increase in available articles over the 08.2017 release.
The dataset is segmented in 3 different subsets, each corresponding to a severity level of the LaTeXML software responsible for the HTML5 conversion.
derivative word embeddings and a token model are available separately here"	https://paperswithcode.com/dataset/arxmliv-08-2018	31/08/2018						
3896	Extreme Countix-AV	"214 videos under various extreme sight conditions for audiovisual repetition counting


7 vision challenges: camera viewpoint changes, cluttered background, low illumination, fast motion, disappearing activity, scale variation, low resolution"	https://paperswithcode.com/dataset/extreme-countix-av	24/03/2021						
3897	Ninapro DB5	"The 5th Ninapro database includes 10 intact subjects recorded with two Thalmic Myo (https://www.myo.com/) armbands.
The database can be used to test the Myo armbands separately as well.
The 5th Ninapro database is thoroughly described in the paper: ""Stefano Pizzolato, Luca Tagliapietra, Matteo Cognolato, Monica Reggiani, Henning Müller, Manfredo Atzori, Comparison of six electromyography acquisition setups on hand movement classification tasks, PLOS One, 2017""
Acquisition Protocol
The subjects have to repeat several movements represented by movies that are shown on the screen of a laptop.
The experiment is divided in three exercises:

Basic movements of the fingers
Isometric, isotonic hand configurations and basic wrist movements
Grasping and functional movements

During the acquisition, the subjects were asked to repeat the movements with the right hand. Each movement repetition lasted 5 seconds and was followed by 3 seconds of rest.
The protocol includes 6 repetitions of 52 different movements (plus rest) performed by 10 intact subjects. The movements were selected from the hand taxonomy as well as from hand robotics literature.
Acquisition Setup
The muscular activity is gathered using 2 Thalmic Myo armbands. The database can be used to test the Myo armbands separately as well.
The subjects in this database wore two Myo armbands one next to the other, including 16 active single–differential wireless electrodes. The top Myo armband is placed closed to the elbow with the first sensor placed on the radio humeral joint, as in the standard Ninapro configuration for the equally spaced electrodes; the second Myo armband is placed just after the first, nearer to the hand, tilted of 22.5 degrees. This configuration provides an extended uniform muscle mapping at an extremely affordable cost. The Myo sensors do not require the arm to be shaved and after few minutes the armband tighten very firmly to the arm of the subject.
The sEMG signals are sampled at a rate of 200 Hz."	https://paperswithcode.com/dataset/ninapro-db5	30/09/2017	Ninapro DB5 - 10 Intact Subjects - Double Myo Armband					
3898	Copel-AMR	"This dataset contains 12,500 meter images acquired in the field by the employees of the Energy Company of Paraná (Copel), which directly serves more than 4 million consuming units, across 395 cities and 1,113 locations (i.e., districts, villages and settlements), located in the Brazilian state of Paraná.
Copel-AMR is composed of images captured in unconstrained scenarios, which typically include blur (due to camera motion), dirt, scale variations, in-plane and out-of-plane rotations, reflections, shadows, and occlusions. In 2,500 images (i.e., 20% of the dataset), it is not even possible to perform the meter reading due to occlusions or faulty meters. 
The images have a resolution of 480×640 or 640×480 pixels, depending on the orientation in which they were taken. Considering that the meter is operational and that there are no occlusions, these resolutions are enough for the meter reading to be legible.
The dataset was randomly split as follows: 5,000 images for training, 5,000 images for testing and 2,500 images for validation, following the split protocol (i.e., 40%/40%/20%) used in the UFPR-AMR dataset. For reproducibility purposes, the subsets generated are explicitly available along with the Copel-AMR dataset.
For each image in our dataset, we manually labeled the meter reading, the position (x, y) of each of the four corners of the counter, and a bounding box (x, y, w, h) for each digit. Corner annotations – which can be converted to a bounding box – enable the counter to be rectified, while bounding boxes enable the training of object detectors.
Source: Towards Image-based Automatic Meter Reading in Unconstrained Scenarios: A Robust and Efficient Approach"	https://paperswithcode.com/dataset/copel-amr-dataset	12/05/2021						
3899	BRUSH	"The BRUSH dataset (BRown University Stylus Handwriting) contains 27,649 online handwriting samples from a total of 170 writers. Every sequence is labeled with intended characters such that dataset users can identify to which character a point in a sequence corresponds. The dataset was introduced in the paper ""Generating Handwriting via Decoupled Style Descriptors"" by Atsunobu Kotani, Stefanie Tellex, James Tompkin from Brown University, presented at European Conference on Computer Vision (ECCV) 2020.
Terms of Use
The BRUSH dataset may only be used for non-commercial research purposes. Anyone wanting to use it for other purposes should contact Prof. James Tompkin. If you publish materials based on this database, we request that you please include a reference to our paper.
@inproceedings{kotani2020generating,
  title={Generating Handwriting via Decoupled Style Descriptors},
  author={Kotani, Atsunobu and Tellex, Stefanie and Tompkin, James},
  booktitle={European Conference on Computer Vision},
  pages={764--780},
  year={2020},
  organization={Springer}
}"	https://paperswithcode.com/dataset/brush	26/08/2020	Brown University Stylus Handwriting					
3900	UFPR-ALPR	"This dataset includes 4,500 fully annotated images (over 30,000 LP characters) from 150 vehicles in real-world scenarios where both the vehicle and the camera (inside another vehicle) are moving.
The images were acquired with three different cameras and are available in the Portable Network Graphics (PNG) format with a size of 1,920 × 1,080 pixels. The cameras used were: GoPro Hero4 Silver, Huawei P9 Lite, and iPhone 7 Plus.
We collected 1,500 images with each camera, divided as follows:
- 900 of cars with gray LP;
- 300 of cars with red LP;
- 300 of motorcycles with gray LP.

The dataset is split as follows: 40% for training, 40% for testing and 20% for validation. Every image has the following annotations available in a text file: the camera in which the image was taken, the vehicle’s position and information such as type (car or motorcycle), manufacturer, model and year; the identification and position of the license plate, as well as the position of its characters. 
Source: A Robust Real-Time Automatic License Plate Recognition Based on the YOLO Detector"	https://paperswithcode.com/dataset/ufpr-alpr	15/10/2018						
3901	SSIG-SegPlate	"This dataset aims at evaluating the License Plate Character Segmentation (LPCS) problem. The experimental results of the paper Benchmark for License Plate Character Segmentation were obtained using a dataset providing 101 on-track vehicles captured during the day. The video was recorded using a static camera in early 2015.
The images of the dataset were acquired with a digital camera in Full-HD and are available in the Portable Network Graphics (PNG) format with 1920×1080 pixels each. The average size of each file is 4.08 Megabytes (a total of 8.60 Gigabytes for the entire dataset). In addition, since there are some approaches that track the car to utilize redundant information to improve the recognition results, we decided to make a dataset with multiples frames per car. In this dataset, there are, on average, 19.80 image frames per vehicle (with a standard deviation of 4.14).
Source: Benchmark for License Plate Character Segmentation"	https://paperswithcode.com/dataset/ssig-segplate	11/07/2016						
3902	UFPR-ADMR-v1	"This dataset contains 2,000 dial meter images obtained on-site by employees of the Energy Company of Paraná (Copel), which serves more than 4 million consuming units in the Brazilian state of Paraná. The images were acquired with many different cameras and are available in the JPG format with 320×640 or 640×320 pixels (depending on the camera orientation). 
The dataset is split into three sets: training (1200 images), validation (400 images), and testing (400 images). Every image has the following annotations available in a .txt file: the counter’s corners (x1, y1), (x2, y2), (x3, y3), (x4, y4). The corners can be used to rectify the counter patch and represent, respectively, the top-left, top-right, bottom-right, and bottom-left corners. For each dial, the current position (x, y, w, h) and the corresponding reading (pointed values and final reading). All counters of the dataset (regardless of meter type) have 4 or 5 dials; thus, 9,097 dials were manually annotated.
Source: Deep Learning for Image-based Automatic Dial Meter Reading: Dataset and Baselines"	https://paperswithcode.com/dataset/ufpr-admr-v1-dataset	28/09/2020						
3903	LFM-BeyMS	"This dataset is based on the LFM-1b [ and the Cultural LFM-1b 2 datasets. LFM-BeyMS includes equally-sized groups of both, beyond-mainstream and mainstream music listeners and thus, can be used for studying the characteristics of beyond-mainstream music listeners for recommendation experiments. For more details, we refer to our publication.
LFM-BeyMS contains
* 4,148 users
* 1,084,922 tracks
* 110,898 artists
* 16,687,363 listening events"	https://paperswithcode.com/dataset/lfm-beyms	05/05/2020						
3904	GUITAR-FX-DIST: A Dataset of Processed Guitar Recordings for Music Research - (Mono Continuous)	"GUITAR-FX-DIST is a dataset of electric guitar recordings processed with overdrive, distortion, and fuzz audio effects. It was developed for research in guitar effects detection, classification, and parameters estimation. The dataset is also useful for research on automatic music transcription, intelligent music production, signal processing, or effects modelling. It contains both unprocessed and processed recordings.
The dataset is split into 4 sub-datasets: Mono Continuous, Mono Discrete, Poly Continuous, Poly Discrete
Authors:
Marco Comunità - Centre for Digital Music, Queen Mary University of London
Reference:
If you make use of GUITAR-FX-DIST, please cite the following publication:
@article{comunita2020guitar,
  title={Guitar Effects Recognition and Parameter Estimation with Convolutional Neural Networks},
  author={Comunit{\`a}, Marco and Stowell, Dan and Reiss, Joshua D},
  journal={arXiv preprint arXiv:2012.03216},
  year={2020}
}"	https://paperswithcode.com/dataset/guitar-fx-dist-mono-continuous	06/12/2020						
3905	GUITAR-FX-DIST: A Dataset of Processed Guitar Recordings for Music Research - (Mono Discrete)	"GUITAR-FX-DIST is a dataset of electric guitar recordings processed with overdrive, distortion and fuzz audio effects. It was developed for research in guitar effects detection, classification and parameters estimation. The dataset is also useful for research on automatic music transcription, intelligent music production, signal processing or effects modelling. It contains both unprocessed and processed recordings.
The dataset is split into 4 sub-datasets: Mono Continuous, Mono Discrete, Poly Continuous, Poly Discrete
Authors:
Marco Comunità - Centre for Digital Music, Queen Mary University of London
Reference:
If you make use of GUITAR-FX-DIST, please cite the following publication:
@article{comunita2020guitar,
  title={Guitar Effects Recognition and Parameter Estimation with Convolutional Neural Networks},
  author={Comunit{\`a}, Marco and Stowell, Dan and Reiss, Joshua D},
  journal={arXiv preprint arXiv:2012.03216},
  year={2020}
}"	https://paperswithcode.com/dataset/guitar-fx-dist-a-dataset-of-processed-guitar	06/12/2020						
3906	GUITAR-FX-DIST: A Dataset of Processed Guitar Recordings for Music Research - (Poly Discrete)	"GUITAR-FX-DIST is a dataset of electric guitar recordings processed with overdrive, distortion and fuzz audio effects. It was developed for research in guitar effects detection, classification and parameters estimation. The dataset is also useful for research on automatic music transcription, intelligent music production, signal processing or effects modelling. It contains both unprocessed and processed recordings.
The dataset is split into 4 sub-datasets: Mono Continuous, Mono Discrete, Poly Continuous, Poly Discrete
Authors:
Marco Comunità - Centre for Digital Music, Queen Mary University of London
Reference:
If you make use of GUITAR-FX-DIST, please cite the following publication:
@article{comunita2020guitar,
  title={Guitar Effects Recognition and Parameter Estimation with Convolutional Neural Networks},
  author={Comunit{\`a}, Marco and Stowell, Dan and Reiss, Joshua D},
  journal={arXiv preprint arXiv:2012.03216},
  year={2020}
}"	https://paperswithcode.com/dataset/guitar-fx-dist-a-dataset-of-processed-guitar-1	06/12/2020						
3907	GUITAR-FX-DIST: A Dataset of Processed Guitar Recordings for Music Research - (Poly Continuous)	"GUITAR-FX-DIST is a dataset of electric guitar recordings processed with overdrive, distortion and fuzz audio effects. It was developed for research in guitar effects detection, classification and parameters estimation. The dataset is also useful for research on automatic music transcription, intelligent music production, signal processing or effects modelling. It contains both unprocessed and processed recordings.
The dataset is split into 4 sub-datasets: Mono Continuous, Mono Discrete, Poly Continuous, Poly Discrete
Authors:
Marco Comunità - Centre for Digital Music, Queen Mary University of London
Reference:
If you make use of GUITAR-FX-DIST, please cite the following publication:
@article{comunita2020guitar,
  title={Guitar Effects Recognition and Parameter Estimation with Convolutional Neural Networks},
  author={Comunit{\`a}, Marco and Stowell, Dan and Reiss, Joshua D},
  journal={arXiv preprint arXiv:2012.03216},
  year={2020}
}"	https://paperswithcode.com/dataset/guitar-fx-dist-a-dataset-of-processed-guitar-2	06/12/2020						
3908	METAR	Weather reports of 57 stations in the east coast.	https://paperswithcode.com/dataset/metar		Meteorological Terminal Aviation Routine Dataset					
3909	Netzschleuder	"This is a catalogue and repository of network datasets with the aim of aiding scientific research.
This website is meant to be browsed both by humans and machines alike, and can also be accessed via a convenient JSON API, or via the graph-tool library. The network datasets themselves are available in several machine-readable formats, in particular gt, GraphML, GML and CSV.
The upstream origin of each dataset is meant to be as transparent as possible. Each dataset contains its own publicly available extraction and parsing script, accessible via a git repository, which also includes the entire code for this website, released as Free Software under the AGPLv3.
Users are encouraged to inspect the entire pipeline from original upstream data publication, downloading, parsing and format conversion.
Users are also welcome to report problems or omissions with the datasets, as well as suggest new ones, either by opening an issue, or simply by forking the git repository and proposing a merge request."	https://paperswithcode.com/dataset/netzschleuder	01/07/2020	network catalogue, repository and centrifuge					
3910	Darpa OpTC	"Operationally Transparent Cyber (OpTC) was a technology transition pilot study funded under Boston Fusion Corp.'s Cyber APT Scenarios for Enterprise Systems (CASES) project. Its primary objective was to determine if DARPA Transparent Computing (TC) program technologies could scale without loss of detection performance to address cyber defense capability gaps identified in USTRANSCOM's Joint Deployment Distribution Enterprise (JDDE) solicitation for the government fiscal years 2019-2023. Boston Fusion along with two performers from the TC program (Five Directions providing endpoint telemetry (TA1) and BAE providing analysis over the data (TA2)) worked to scale their systems from two machines to one thousand machines. The OpTC team conducted scaling and detection tests in the fall of 2019. A third performer (Provatek), not originally associated with the TC program, acted as a red team and test coordinator. This data set represents a subset of that activity.
The OpTC system architecture is based on one used in TC program evaluations. Kafka, an open-source stream-processing server, is used to pass information among system components. Each Windows 10 endpoint is equipped with an endpoint sensor that monitors host events, packs them into JSON records, and sends them to Kafka. As these records flow into Kafka, a translation server aggregates them into new data records in a format called eCAR that are then pushed back to Kafka. As the translation server pushes eCAR records to Kafka, a data analytics component integrates them into a graph data structure for analysis and visualization.
OpTC took TC system components that worked well on two hosts in TC program tests and scaled them up to work with one thousand hosts. This scaled-up system was evaluated over two weeks in a highly instrumented environment, and the data in this collection contains approximately a terabyte of data in compressed JSON compatible format from that evaluation. The evaluation started with a period of benign record generation, followed by the injection of malware by a red team. Benign traffic ran continuously during red team activity. Due to constraints in collection data space during the evaluation, data from five hundred hosts were collected rather than from the full set of one-thousand hosts."	https://paperswithcode.com/dataset/darpa-optc	17/06/2020	Darpa Operationally Transparent Cyber (OpTC) Dataset					
3911	Home Action Genome	"Home Action Genome is a large-scale multi-view video database of indoor daily activities. Every activity is captured by synchronized multi-view cameras, including an egocentric view.
There are 30 hours of vides with 70 classes of daily activities and 453 classes of atomic actions."	https://paperswithcode.com/dataset/home-action-genome	11/05/2021						
3912	OVIS	"OVIS is a new large scale benchmark dataset for video instance segmentation task. It is designed with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes. OVIS consists of:

296k high-quality instance masks
25 commonly seen semantic categories
901 videos with severe object occlusions
5,223 unique instances

If the description or image is from a different paper, please refer to it as follows:
Source: http://songbai.site/ovis/"	https://paperswithcode.com/dataset/ovis	02/02/2021	Occluded Video Instance Segmentation					
3913	SyntheticFur	"SyntheticFur is a dataset for neural rendering. Collecting and generating high quality fur images is an expensive and difficult process that requires content specialists to generate. By releasing this unique dataset with high quality lighting simulation via ray tracing, this can save time for researchers seeking to advance studies of fur rendering and simulation, without having to recreate this laborious process.
The dataset was used for neural rendering research at Google that takes advantage of rasterized image buffers and converts them into high quality raytraced fur renders. We believe that this dataset can contribute to the computer graphics and machine learning community to develop more advanced techniques with fur rendering.
It contains approximately 140,000 procedurally generated images and 15 simulations with Houdini. The images consist of fur groomed with different skin primitives and move with various motions in a predefined set of lighting environments."	https://paperswithcode.com/dataset/syntheticfur	13/05/2021						
3914	TabLeX	TabLeX is a large-scale benchmark dataset comprising table images generated from scientific articles. TabLeX consists of two subsets, one for table structure extraction and the other for table content extraction. Each table image is accompanied by its corresponding LATEX source code. To facilitate the development of robust table IE tools, TabLeX contains images in different aspect ratios and in a variety of fonts.	https://paperswithcode.com/dataset/tablex	12/05/2021						
3915	PeMSD7	PeMSD7 is traffic data in District 7 of California consisting of the traffic speed of 228 sensors while the period is from May to June in 2012 (only weekdays) with a time interval of 5 minutes. This dataset is popular for benchmark the traffic forecasting models.	https://paperswithcode.com/dataset/pemsd7							
3916	PeMSD4	The dataset refers to the traffic speed data in San Francisco Bay Area, containing 307 sensors on 29 roads. The time span of the dataset is January-February in 2018. It is a popular benchmark for traffic forecasting.	https://paperswithcode.com/dataset/pemsd4							
3917	PeMSD8	This dataset contains the traffic data in San Bernardino from July to August in 2016, with 170 detectors on 8 roads with a time interval of 5 minutes. This dataset is popular as a benchmark traffic forecasting dataset.	https://paperswithcode.com/dataset/pemsd8	20/01/2019						
3918	SaRoCo	SaRoCo is a dataset for detecting satire in Romanian news containing 55,608 news articles from multiple real and satirical news sources, of which 27,980 are regular and 27,628 satirical news reports. We provide the data in csv format, in three files following the train/validation/test splits.	https://paperswithcode.com/dataset/saroco	13/05/2021						
3919	CHAOS	"CHAOS challenge aims the segmentation of abdominal organs (liver, kidneys and spleen)  from CT and MRI data. ONsite section of the CHAOS was held in The IEEE International Symposium on Biomedical Imaging (ISBI) on April 11, 2019, Venice, ITALY.  Online submissions are still welcome!
\textbf{Challenge Description} 
Understanding prerequisites of complicated medical procedures plays an important role in the success of the operations. To enrich the level of understanding, physicians use advanced tools such as three-dimensional visualization and printing, which require extraction of the object(s) of interest from DICOM images. Accordingly, the precise segmentation of abdominal organs (i.e. liver, kidney(s) and spleen) has critical importance for several clinical procedures including but not limited to pre-evaluation of liver for living donor-based transplantation surgery or detailed analysis of abdominal organs to determine the vessels arising from and entering them for correct positioning of a graft prior to abdominal aortic surgery. This motivates ongoing research to achieve better segmentation results and overcoming countless challenges originating from both highly flexible anatomical properties of abdomen and limitations of modalities reflected to image characteristics. In this context, the proposed challenge has two separate but related aims:
1) Segmentation of liver from computed tomography (CT) data sets, which are acquired at portal phase after contrast agent injection for pre-evaluation of living donated liver transplantation donors.
2) Segmentation of four abdominal organs (i.e. liver, spleen, right and left kidneys) from magnetic resonance imaging (MRI) data sets acquired with two different sequences (T1-DUAL and T2-SPIR).
CHAOS tasks contain combination of these organs' segmentation.
\textbf{Tasks} 
There are five competition categories in which the participating teams can take place and submit their result(s):
1) Liver Segmentation (CT & MRI): This is also called ""cross-modality"" 1 and it is simply based on using a single system, which can segment liver from both CT and MRI. For instance, the training and test sets of a machine learning approach would have images from both modalities without explicitly feeding the model with corresponding information. A unique study about this is a reference below and this task is one of the most interesting tasks of the challenge. Keep in mind that the fusion of individual systems for different modalities (i.e. two models, one working on CT and the other on MRI ) would not be valid for this category. They can be evaluated as individual systems at Tasks 2 and 3. On the other hand, in this task, fusion of individual systems between MR sequences (i.e. two models, one working on T1-DUAL and the other on  T2-SPIR ) is allowed.
2) Liver Segmentation (CT only): This is mostly a regular task of liver segmentation from CT, (such as SLIVER07). This task is easier than SLIVER07 as it only contains healthy livers aligned in the same direction and patient position. However, the challenging part is the enhanced vascular structures (portal phase) due to the contrast injection.
3) Liver Segmentation (MRI only): Similar to ""Task 2"", this is also a regular task of liver segmentation from MRI. It includes two different pulse sequences: T1-DUAL and T2-SPIR. Moreover, T1-DUAL has two forms (in and out phase). The developed system should work on both sequences. In this task, the fusion of individual systems between MR sequences (i.e. two models, one working on T1-DUAL and the other on  T2-SPIR ) are allowed.
4) Segmentation of abdominal organs (CT & MRI): This task is extension of Task 1 to kidneys and spleen in MRI data. In this task, the interesting part is that CT datasets have only liver, but the MRI datasets have four annotated abdominal organs (liver, kidneys, spleen). Keep in mind that fusion of individual systems for different modalities (i.e. two models, one working on CT and the other on MRI ) would not be valid for this category. On the other hand, in this task, fusion of individual systems between MR sequences (i.e. two models, one working on T1-DUAL and the other on  T2-SPIR ) are allowed.
5) Segmentation of abdominal organs (MRI only): The same task given in ""Task 3"" but extended to four abdominal organs; liver, kidneys, spleen. In this task, ensemble or fusion of individual systems between MR sequences (i.e. two models, one working on T1-DUAL and the other on  T2-SPIR ) are allowed.  
1 Valindria, V. et al. (2018, March). Multi-modal learning from unpaired images: Application to multi-organ segmentation in CT and MRI. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 547-556). IEEE. https://doi.ieeecomputersociety.org/10.1109/WACV.2018.00066"	https://paperswithcode.com/dataset/chaos	11/04/2019	CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation					
3920	TrackML challenge Throughput phase dataset	"The dataset comprises multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN. The goal of the tracking machine learning challenge is to group the recorded measurements or hit for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hit, their ground truth counterpart and their association to particles, and the initial parameters of those particles. The test dataset contains only the recorded hits.
The dataset was used for the Throughput Phase of the Tracking Machine Learning challenge on Codalab.
See more details in the home page url."	https://paperswithcode.com/dataset/trackml-challenge-throughput-phase-dataset	03/05/2021	Tracking Machine Learning Challenge					
3921	Scroll Readability Dataset	Scroll Readability Dataset contains scroll interactions of 598 participants reading advanced and elementary texts from the OneStopEnglish corpus.	https://paperswithcode.com/dataset/scroll-readability-dataset	13/05/2021						
3922	AID	"AID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms.
The new dataset is made up of the following 30 aerial scene types: airport, bare land, baseball field, beach, bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the field of remote sensing image interpretation, and some samples of each class are shown in Fig.1. In all, the AID dataset has a number of 10000 images within 30 classes.
The images in AID are actually multi-source, as Google Earth images are from different remote imaging sensors. This brings more challenges for scene classification than the single source images like UC-Merced dataset. Moreover, all the sample images per each class in AID are carefully chosen from different countries and regions around the world, mainly in China, the United States, England, France, Italy, Japan, Germany, etc., and they are extracted at different time and seasons under different imaging conditions, which increases the intra-class diversities of the data."	https://paperswithcode.com/dataset/aid	18/08/2016	Aerial Image Dataset					
3923	GID	GID is large-scale land-cover dataset with Gaofen-2 (GF-2) satellite images. This new dataset, which is named as Gaofen Image Dataset (GID), has superiorities over the existing land-cover dataset because of its large coverage, wide distribution, and high spatial resolution. GID consists of two parts: a large-scale classification set and a fine land-cover classification set. The large-scale classification set contains 150 pixel-level annotated GF-2 images, and the fine classification set is composed of 30,000 multi-scale image patches coupled with 10 pixel-level annotated GF-2 images. The training and validation data with 15 categories is collected and re-labeled based on the training and validation images with 5 categories, respectively.	https://paperswithcode.com/dataset/gid	16/07/2018	Gaofen Image Dataset					
3924	WHU-RS19	WHU-RS19 is a set of satellite images exported from Google Earth, which provides high-resolution satellite images up to 0.5 m. Some samples of the database are displayed in the following picture. It contains 19 classes of meaningful scenes in high-resolution satellite imagery, including airport, beach, bridge, commercial, desert, farmland, footballfield, forest, industrial, meadow, mountain, park, parking, pond, port, railwaystation, residential, river, and viaduct. For each class, there are about 50 samples. It’s worth noticing that the image samples of the same class are collected from different regions in satellite images of different resolutions and then might have different scales, orientations and illuminations.	https://paperswithcode.com/dataset/whu-rs19	10/05/2010						
3925	SECOND	SECOND is a well-annotated semantic change detection dataset. To ensure data diversity, we firstly collect 4662 pairs of aerial images from several platforms and sensors. These pairs of images are distributed over the cities such as Hangzhou, Chengdu, and Shanghai. Each image has size 512 x 512 and is annotated at the pixel level. The annotation of SECOND is carried out by an expert group of earth vision applications, which guarantees high label accuracy. For the change category in the SECOND dataset, we focus on 6 main land-cover classes, i.e. , non-vegetated ground surface, tree, low vegetation, water, buildings and playgrounds , that are frequently involved in natural and man-made geographical changes. It is worth noticing that, in the new dataset, non-vegetated ground surface ( n.v.g. surface for short) mainly corresponds to impervious surface and bare land. In summary, these 6 selected land-cover categories result in 30 common change categories (including non-change ). Through the random selection of image pairs, the SECOND reflects real distributions of land-cover categories when changes occur.	https://paperswithcode.com/dataset/second	12/10/2020	SEmantic Change detectiON Dataset					
3926	Shellcode_IA32	"Shellcode_IA32 is a dataset containing 20 years of shellcodes from a variety of sources is the largest collection of shellcodes in assembly available to date.
This dataset consists of 3,200 examples of instructions in assembly language for IA-32 (the 32-bit version of the x86 Intel Architecture) from publicly available security exploits. We collected assembly programs used to generate shellcode from exploit-db and from shell-storm. We enriched the dataset by adding examples of assembly programs for the IA-32 architecture from popular tutorials and books. This allowed us to understand how different authors and assembly experts comment and, thus, how to deal with the ambiguity of natural language in this specific context. Our dataset consists of 10% of instructions collected from books and guidelines, and the rest from real shellcodes.
Our focus is on Linux, the most common OS for security-critical network services. Accordingly, we added assembly instructions written with Netwide Assembler (NASM) for Linux.
Each line of Shellcode_IA32 dataset represents a snippet - intent pair. The snippet is a line or a combination of multiple lines of assembly code, built by following the NASM syntax. The intent is a comment in the English language.
Further statistics on the dataset and a set of preliminary experiments performed with a neural machine translation (NMT) model are described in the following paper: Shellcode_IA32: A Dataset for Automatic Shellcode Generation."	https://paperswithcode.com/dataset/shellcode-ia32	27/04/2021	Shellcode_IA32					
3927	Analogy Test	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/analogy-test	11/05/2021						
3928	SILICONE Benchmark	The Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE (SILICONE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems specifically designed for spoken language. All datasets are in the English language and covers a  large variety of domains (e.g daily life, scripted scenarios, joint task completion, phone call conversations, and televsion dialogue). Some datasets additionally include emotion and/or sentiment labels.	https://paperswithcode.com/dataset/silicone-benchmark	23/09/2020	SILICONE					
3929	FKD	The football keyword dataset (FKD), as a new keyword spotting dataset in Persian, is collected with crowdsourcing. This dataset contains nearly 31000 samples in 18 classes.	https://paperswithcode.com/dataset/fkd	31/12/2020	Football Keywords Dataset					
3930	SDCNL (Suicide vs Depression Classification)	We develop a primary dataset based on our task of suicide or depression classification. This dataset is web-scraped from Reddit. We collect our data from subreddits using the Python Reddit API. We specifically scrape from two subreddits, r/SuicideWatch3 and r/Depression. The dataset contains 1,895 total posts. We utilize two fields from the scraped data: the original text of the post as our inputs, and the subreddit it belongs to as labels. Posts from r/SuicideWatch are labeled as suicidal, and posts from r/Depression are labeled as depressed. We make this dataset and the web-scraping script available in our code.	https://paperswithcode.com/dataset/sdcnl-suicide-vs-depression-classification	18/02/2021						
3931	Reddit C-SSRS	"The C-SSRS dataset contains 500 Reddit posts from the subreddit r/depression. These posts are labeled by psychologists on a five point scale according to guidelines established in the Columbia Suicide Severity Rating Scale, which progress according to
severity of depression. As this dataset is clinically verified and labeled, it is an adequate dataset to validate the label correction method, especially since it is from the same domain of mental health.
Source: Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction
Paper: Knowledge-aware Assessment of Severity of Suicide Risk for Early Intervention"	https://paperswithcode.com/dataset/reddit-c-ssrs							
3932	SHHS	The Sleep Heart Health Study (SHHS) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing. It tests whether sleep-related breathing is associated with an increased risk of coronary heart disease, stroke, all cause mortality, and hypertension.  In all, 6,441 men and women aged 40 years and older were enrolled between November 1, 1995 and January 31, 1998 to take part in SHHS Visit 1. During exam cycle 3 (January 2001- June 2003), a second polysomnogram (SHHS Visit 2) was obtained in 3,295 of the participants. CVD Outcomes data were monitored and adjudicated by parent cohorts between baseline and 2011. More than 130 manuscripts have been published investigating predictors and outcomes of sleep disorders.	https://paperswithcode.com/dataset/shhs		Sleep Heart Health Study					
3933	AraCOVID19-MFH	AraCOVID19-MFH is a manually annotated multi-label Arabic COVID-19 fake news and hate speech detection dataset. The dataset contains 10,828 Arabic tweets annotated with 10 different labels.	https://paperswithcode.com/dataset/aracovid19-mfh	07/05/2021	AraCOVID19-MFH: Arabic COVID-19 Multi-label Fake News and Hate Speech Detection Dataset					
3934	UAVVaste	"The UAVVaste dataset consists to date of 772 images and 3716 annotations. The main motivation for creation of the dataset was the lack of domain-specific data. The datasets that are widely used for object detection evaluation benchmarking. The dataset is made publicly available and is intended to be expanded.
| Date          | Images count  | Annotations count     |
|------------   |:------------: |:-----------------:    |
| 14.11.2020    |      772      |        3716           |"	https://paperswithcode.com/dataset/uavvaste	03/11/2020						
3935	AvaSym	"Global Symmetry Ground-truth for AVA dataset.
Release Date: 2016.
Users of this software are encouraged to cite the following article:
Elawady, Mohamed, Cécile Barat, Christophe Ducottet, and Philippe Colantoni. ""Global Bilateral Symmetry Detection Using Multiscale Mirror Histograms."" In International Conference on Advanced Concepts for Intelligent Vision Systems, pp. 14-24. Springer International Publishing, 2016.
Contents:
GT_AVA/AVA_GT.mat : list of image names and axis groundtruth (x1,y1,x2,y2).
DwnImgs.m : MATLAB code file to download images from DpChallenge website and to show how to use groundtruth, the m-code will create two directories ('Imgs', 'ImgsGT')."	https://paperswithcode.com/dataset/avasym	01/10/2016						
3936	ConvQuestions	ConvQuestions is the first realistic benchmark for conversational question answering over knowledge graphs. It contains 11,200 conversations which can be evaluated over Wikidata. They are compiled from the inputs of 70 Master crowdworkers on Amazon Mechanical Turk, with conversations from five domains: Books, Movies, Soccer, Music, and TV Series. The questions feature a variety of complex question phenomena like comparisons, aggregations, compositionality, and temporal reasoning. Answers are grounded in Wikidata entities to enable fair comparison across diverse methods. The data gathering setup was kept as natural as possible, with the annotators selecting entities of their choice from each of the five domains, and formulating the entire conversation in one session. All questions in a conversation are from the same Turker, who also provided gold answers to the questions. For suitability to knowledge graphs, questions were constrained to be objective or factoid in nature, but no other restrictive guidelines were set. A notable property of ConvQuestions is that several questions are not answerable by Wikidata alone (as of September 2019), but the required facts can, for example, be found in the open Web or in Wikipedia. For details, please refer to our CIKM 2019 full paper.	https://paperswithcode.com/dataset/convquestions	08/10/2019						
3937	Drinking Waste Classification	"About the Dataset:
4 classes of drinking waste: Aluminium Cans, Glass bottles, PET (plastic) bottles and HDPE (plastic) Milk bottles.
rawimgs - images of 4 classes of waste
YOLO_imgs - images of 4 classes of waste with corresponding txt file (annotations for YOLO framework)
labels.txt - labels of the classes
Story
This dataset was manually labelled and collected as a part of final year Individual Project at University College London. Pictures were taken with 12 MP phone camera. I created a real-time waste detection and identification system powered by YOLO framework. Use it as you like, if you could cite me in your work, would be much appreciated. Please reach out to me if this dataset actually helped you with your project.
Arkadiy Serezhkin - arkadiyhacks@gmail.com
Acknowledgements
The dataset used parts of manually collected dataset of Gary Thung and Mindy Yang. I would like to thank them for collecting their dataset as this is not a fun thing to do (from my own experience). You can find their repository here."	https://paperswithcode.com/dataset/drinking-waste-classification							
3938	R2VQ	"R2VQ is a dataset designed for testing competence-based comprehension of machines over a multimodal recipe collection, which contains text-video aligned recipes.
A total of 51,331 cooking events are annotated, which contain 19,201 explicit ingredients, 16,338 implicit ingredients, 12,316 explicit props, and 11,868 implicit props."	https://paperswithcode.com/dataset/r2vq	12/05/2021	Recipe-to-Video Questions					
3939	ionosphere	The original ionosphere dataset from UCI machine learning repository is a binary classification dataset with dimensionality 34. There is one attribute having values all zeros, which is discarded. So the total number of dimensions are 33. The ‘bad’ class is considered as outliers class and the ‘good’ class as inliers.	https://paperswithcode.com/dataset/ionosphere	15/12/2008						
3940	GeoLifeCLEF 2020	The GeoLifeCLEF 2020 dataset is a large-scale remote sensing dataset. More specifically, it consists of 1.9 million species observations from the community science platform iNaturalist, each of which is paired with high-resolution covariates (RGB-IR imagery, land cover, and altitude). The dataset is roughly evenly split between the US and France, and covers over 31k plant and animal species.	https://paperswithcode.com/dataset/geolifeclef-2020	08/04/2020						
3941	DBATES	"DBATES is a database of multimodal communication features extracted from debate speeches in the 2019 North American Universities Debate Championships (NAUDC).
Author's note: If you want to access the dataset for research purposes, please email the authors.
Image source: https://arxiv.org/pdf/2103.14189v1.pdf"	https://paperswithcode.com/dataset/dbates	26/03/2021	DataBase of Audio features, Text and visual Expressions in competitive debate Speeches					
3942	Boombox	"Boombox is a multi-modal dataset for visual reconstruction from acoustic vibrations. Involves dropping objects into a box and capturing resulting images and vibrations. Used for training ML systems that predict images from vibration.
Potential application domain: Computer Vision, Multimodal Perception, Vision and Sound, Sight from Sound, Robotics, Deep Learning, and Machine Learning."	https://paperswithcode.com/dataset/boombox	17/05/2021						
3943	ARC-100	"The ARC-100 dataset was collected as part of a prototype retail checkout system titled ARC (Automatic Retail Checkout). It consists of 31,000 $640\times480$ RGB images of 100 commonly found retail items in Lahore, Pakistan. Each retail item has 310 images captured at various logical orientations (on a black, matte finish conveyor belt) by a Logitech C310 webcam, under a wooden hood frame illuminated by LED strips (luminance set to approximately $70lx$). In the proposed setup, images were pre-processed and standardized before feeding into a Convolutional Neural Network for identification.
Links:

ARC paper
ARC-100 dataset"	https://paperswithcode.com/dataset/arc-100	07/04/2021						
3944	ImageNet-O	ImageNet-O consists of images from classes that are not found in the ImageNet-1k dataset. It is used to test the robustness of vision models to out-of-distribution samples. It's reported using the AUPR metric.	https://paperswithcode.com/dataset/imagenet-o							
3945	ImageNet-9	ImageNet-9 consists of images with different amounts of background and foreground signal, which you can use to measure the extent to which your models rely on image backgrounds. This dataset is helpful in testing the robustness of vision models with respect to their dependence on the backgrounds of images.	https://paperswithcode.com/dataset/imagenet-9							
3946	xSID	xSID, a new evaluation benchmark for cross-lingual (X) Slot and Intent Detection in 13 languages from 6 language families,  including a very low-resource dialect, covering Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), German (de), Indonesian (id), Italian (it), Japanese (ja), Kazakh (kk), Serbian (sr), Turkish (tr) and an Austro-Bavarian German dialect, South Tyrolean (de-st).	https://paperswithcode.com/dataset/xsid	15/05/2021	Cross-lingual Slot and Intent Detection					
3947	Few-NERD	Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER)).	https://paperswithcode.com/dataset/few-nerd	16/05/2021	Few-NERD					
3948	DL-HARD	"Deep Learning Hard  (DL-HARD) is an annotated dataset designed to more effectively evaluate neural ranking models on complex topics. It builds on TREC Deep Learning (DL) questions extensively annotated with query intent categories, answer types, wikified entities, topic categories, and result type metadata from a leading web search engine.
DL-HARD contains 50 queries from the official 2019/2020 evaluation benchmark, half of which are newly and independently assessed. Overall, DL-HARD is a new resource that promotes research on neural ranking methods by focusing on challenging and complex queries."	https://paperswithcode.com/dataset/dl-hard	17/05/2021	Deep Learning Hard					
3949	SciDuet	SciDuet is a dataset for training and benchmarking models for automating document-to-slides generation. It consists of pairs of papers and their corresponding slides decks from recent years' NLP and ML conferences (e.g., ACL). This dataset contains  1,088 papers and 10,034 slides.	https://paperswithcode.com/dataset/sciduet	08/05/2021						
3950	Flat Real World Simulink Models	"This dataset contains:
(1) Slforge Generated Simulink Models : Synthetic Simulink Models
(2) Source of Real World Simulink Models
The .txt file is a combined text file that contains all the real world Simulink models based on SLGPT's experimental setup."	https://paperswithcode.com/dataset/flat-real-world-simulink-models	16/05/2021						
3951	PhotoShape	The PhotoShape dataset consists of photorealistic, relightable, 3D shapes produced by the work proposed in the work of Park et al. (2021).	https://paperswithcode.com/dataset/photoshape	26/09/2018						
3952	Fruits 360	"Fruits 360 dataset: A dataset of images containing fruits and vegetables
Version: 2020.05.18.0
Content
The following fruits and are included: 
Apples (different varieties: Crimson Snow, Golden, Golden-Red, Granny Smith, Pink Lady, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red, Lady Finger), Beetroot Red, Blueberry, Cactus fruit, Cantaloupe (2 varieties), Carambula, Cauliflower, Cherry (different varieties, Rainier), Cherry Wax (Yellow, Red, Black), Chestnut, Clementine, Cocos, Corn (with husk), Cucumber (ripened), Dates, Eggplant, Fig, Ginger Root, Granadilla, Grape (Blue, Pink, White (different varieties)), Grapefruit (Pink, White), Guava, Hazelnut, Huckleberry, Kiwi, Kaki, Kohlrabi, Kumsquats, Lemon (normal, Meyer), Lime, Lychee, Mandarine, Mango (Green, Red), Mangostan, Maracuja, Melon Piel de Sapo, Mulberry, Nectarine (Regular, Flat), Nut (Forest, Pecan), Onion (Red, White), Orange, Papaya, Passion fruit, Peach (different varieties), Pepino, Pear (different varieties, Abate, Forelle, Kaiser, Monster, Red, Stone, Williams), Pepper (Red, Green, Orange, Yellow), Physalis (normal, with Husk), Pineapple (normal, Mini), Pitahaya Red, Plum (different varieties), Pomegranate, Pomelo Sweetie, Potato (Red, Sweet, White), Quince, Rambutan, Raspberry, Redcurrant, Salak, Strawberry (normal, Wedge), Tamarillo, Tangelo, Tomato (different varieties, Maroon, Cherry Red, Yellow, not ripened, Heart), Walnut, Watermelon.
Dataset properties
Total number of images: 90483.
Training set size: 67692 images (one fruit or vegetable per image).
Test set size: 22688 images (one fruit or vegetable per image).
Number of classes: 131 (fruits and vegetables).
Image size: 100x100 pixels.
Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg) or r2_image_index_100.jpg or r3_image_index_100.jpg. ""r"" stands for rotated fruit. ""r2"" means that the fruit was rotated around the 3rd axis. ""100"" comes from image size (100x100 pixels).
Different varieties of the same fruit (apple for instance) are stored as belonging to different classes.
How we made it
Fruits and vegetables were planted in the shaft of a low-speed motor (3 rpm) and a short movie of 20 seconds was recorded. 
A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.
Behind the fruits, we placed a white sheet of paper as background. 
However, due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm that extracts the fruit from the background. This algorithm is of flood fill type:  we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.
All marked pixels are considered as being background (which is then filled with white) and the rest of the pixels are considered as belonging to the object.
The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.
Pictures from the test-multiple_fruits folder were taken with a Nexus 5X phone.
Research papers
Horea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.
The paper introduces the dataset and implementation of a Neural Network trained to recognize the fruits in the dataset.
Alternate download
This dataset is also available for download from GitHub: Fruits-360 dataset
History
Fruits were filmed at the dates given below (YYYY.MM.DD):
2017.02.25 - Apple (golden).
2017.02.28 - Apple (red-yellow, red, golden2), Kiwi, Pear, Grapefruit, Lemon, Orange, Strawberry, Banana.
2017.03.05 - Apple (golden3, Braeburn, Granny Smith, red2).
2017.03.07 - Apple (red3).
2017.05.10 - Plum, Peach, Peach flat, Apricot, Nectarine, Pomegranate.
2017.05.27 - Avocado, Papaya, Grape, Cherrie.
2017.12.25 - Carambula, Cactus fruit, Granadilla, Kaki, Kumsquats, Passion fruit, Avocado ripe, Quince.
2017.12.28 - Clementine, Cocos, Mango, Lime, Lychee.
2017.12.31 - Apple Red Delicious, Pear Monster, Grape White.
2018.01.14 - Ananas, Grapefruit Pink, Mandarine, Pineapple, Tangelo.
2018.01.19 - Huckleberry, Raspberry.
2018.01.26 - Dates, Maracuja, Plum 2, Salak, Tamarillo.
2018.02.05 - Guava, Grape White 2, Lemon Meyer
2018.02.07 - Banana Red, Pepino, Pitahaya Red.
2018.02.08 - Pear Abate, Pear Williams.
2018.05.22 - Lemon rotated, Pomegranate rotated.
2018.05.24 - Cherry Rainier, Cherry 2, Strawberry Wedge.
2018.05.26 - Cantaloupe (2 varieties).
2018.05.31 - Melon Piel de Sapo.
2018.06.05 - Pineapple Mini, Physalis, Physalis with Husk, Rambutan.
2018.06.08 - Mulberry, Redcurrant.
2018.06.16 - Hazelnut, Walnut, Tomato, Cherry Red.
2018.06.17 - Cherry Wax (Yellow, Red, Black).
2018.08.19 - Apple Red Yellow 2, Grape Blue, Grape White 3-4, Peach 2, Plum 3, Tomato Maroon, Tomato 1-4 .
2018.12.20 - Nut Pecan, Pear Kaiser, Tomato Yellow.
2018.12.21 - Banana Lady Finger, Chesnut, Mangostan.
2018.12.22 - Pomelo Sweetie.
2019.04.21 - Apple Crimson Snow, Apple Pink Lady, Blueberry, Kohlrabi, Mango Red, Pear Red, Pepper (Red, Yellow, Green).
2019.06.18 - Beetroot Red, Corn, Ginger Root, Nectarine Flat, Nut Forest, Onion Red, Onion Red Peeled, Onion White, Potato Red, Potato Red Washed, Potato Sweet, Potato White.
2019.07.07 - Cauliflower, Eggplant, Pear Forelle, Pepper Orange, Tomato Heart.
2019.09.22 - Corn Husk, Cucumber Ripe, Fig, Pear 2, Pear Stone, Tomato not Ripened, Watermelon.
License
MIT License
Copyright (c) 2017-2021 Mihai Oltean
Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE."	https://paperswithcode.com/dataset/fruits-360-1	01/06/2017	A dataset of images containing fruits and vegetables					
3953	WMT 2021 Ge'ez-Amharic	"WMT 2021 Ge'ez-Amharic is a Ge'ez-Amharic  dataset prepared for NMT tasks of the 6th Workshop on NLP at Debre Berhan University, Ethiopia. The corpus has been collected from:

Ethiopian Orthodox Church old bible (from ethiopianorthodox.org), Anaphora, praise of St. Virgin Mary, praise of Lord Jesus and other Church's books.
Ge'ez teaching books,
Websites and other internet sources such as www.geez.org, www.debelo.org, 

The Dataset has about 15454 parallel Ge'ez and Amharic sentences for training, 1001 parallel sentences for testing and 1001 parallel sentences  for validation."	https://paperswithcode.com/dataset/wmt-2021-ge-ez-amharic	12/06/2017						
3954	PubMed Term, Abstract, Conclusion, Title Dataset	This dataset gathers three types of pairs: Title-to-Abstract (Training: 22,811/Development: 2095/Test: 2095), Abstract-to-Conclusion and Future work (Training: 22,811/Development: 2095/Test: 2095), Conclusion and Future work-to-Title (Training: 15,902/Development: 2095/Test: 2095) from PubMed. Each pair contains a pair of input and output as well as the corresponding terms(from original KB and link prediction results).	https://paperswithcode.com/dataset/pubmed-term-abstract-conclusion-title-dataset	20/05/2019						
3955	PubMed Paper Reading Dataset	This dataset gathers 14,857 entities, 133 relations, and entities corresponding tokenized text from PubMed. It contains 875,698 training pairs, 109,462 development pairs, and 109,462 test pairs.	https://paperswithcode.com/dataset/pubmed-paper-reading-dataset	20/05/2019						
3956	ReviewRobot Dataset	"ReviewRobot Dataset
Overview
This repository contains data for paper ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. [Dataset]
Dataset
There are three folders: Raw_data, IE_result, and KGs. 
Raw_data folder
The Raw_data has two parts: Background Corpus and Paper-review Corpus. 
We create the Background Corpus by selecting machine learning related pappers from the Semantic Scholar Open Research Corpus. It contains papers with their titles and abstracts published from the year of 1965 to 2019 (included).
The Paper-review Corpus contains parsed paper pdfs and their corresponding reviews. The paper-review pairs of acl_2017 and iclr_2017 folders come from PeerRead dataset. We fetched the rest from OpenReview and NeruIPS. We parsed those pdfs using GROBID. In each folder, metadata.txt contains all human reviews, and the txt/ folder contains all processed papers.
IE_result folder
The IE_result folder contains information extraction results from SciIE. In each group, the *_json/ contains tokenized texts, and the *_output/ contains IE results of tokenized texts.
The Background_IE contains two folders from one group for all paper abstracts from 1965 to 2019.
The Paper-review_IE contains four folders from two groups. The first group: iclrnipsabs_json and iclrnipsabs_output contain IE results for abstracts of Paper-review Corpus. The second group: iclrnips_json and iclrnips_output contain IE results for rest of papers in Paper-review Corpus.
KGs
The KGs folder contains the knowledge graphs built on the IE_result. 
back_kg
The back_kg contains the background KGs built up to a certain year. For each year, there are three files. 
Take 2012 as an example:
*   2012.pkl contains the background knowledge graph up to (include) 2012. It contains a dictionary of 6 fields: num_doc is the number of papers up to that year, cluster2entity is a mapping from the entity to its mentions, entity2cluster is a mapping from the mention to its corresponding entity, cluster2type is a mapping from the entity to its type, entity refers to all mentions in current KG, and relations refers to all relations in current KG. 
*   2012_key.pkl contains the mappings from knowledge elements to paper ids. It has two fields: cluster is the mapping from an entity to its corresponding paper ids, and relation is the mapping from a relation to the corresponding paper ids.
*   2012_paper contains the mappings from paper id to its paper title.
idea_kg
The idea_kg folder contains idea KGs constructed from paper abstracts and conclusions. Each line is a paper in the venue and has the following fields: id for the paper id, abs_num for the number of abstract sentences, sent for all sentences related to  idea_kg, entity for all mentions in current KG, cluster2sent for the corresponding sentence ids for a specific entity, entity2num for the occurence of a specific mention, relation2num for the occurence of a specific relation, cluster2entity for a mapping from the entity to its mentions, entity2type conains a mapping from the mention to the type, relations for all relations in current KG, relation2sent for corresponding sentence ids for a specific relation, and entity2cluster for a mapping from the mention to its corresponding entity. 
related_kg
The related_kg contains related KGs constructed from related work for each venue. It is of the same structure as idea_kg.
contribute_kg
The contribute_kg contains contribute KGs constructed from paper contribution section (under introduction section) and experiment section. It contains a dictionary of 4 fields:  id for the paper id,  total for the number of entities covered in the contribution section, covered for the number of entities covered in the experiment section, sents related sentences that covered those entities from both sections.
future_kg
The future_kg contains future KGs constructed from future work for each venue. It is of the same structure as idea_kg.
Review-annotation
The Review-annotation folder contains human annotations for review category and paper-review sentence pairs. The review.txt contains annotation for review category including 236 sentences for ""SUMMARY"", 33 sentences for ""NOVELTY"", 174 sentences for ""SOUNDNESS_CORRECTNESS"", 16 sentences for ""MEANINGFUL_COMPARISON"", and 14 sentences for ""IMPACT"". The pair.txt   contains 2,535 review-paper pairs. For each pair, the first slot is the review sentence; the second slot is the paper sentence, the third slot is the label where 0 indicates two sentences are not related and 1 indicates they are related.
License
Creative Commons — Attribution 4.0 International — CC BY 4.0"	https://paperswithcode.com/dataset/reviewrobot-dataset	13/10/2020						
3957	FlyingThings3D	FlyingThings3D is a synthetic dataset for optical flow, disparity and  scene flow estimation. It consists of everyday objects flying along randomized 3D trajectories. We generated about 25,000 stereo frames with ground truth data. Instead of focusing on a particular task (like KITTI) or enforcing strict naturalism (like Sintel), we rely on randomness and a large pool of rendering assets to generate orders of magnitude more data than any existing option, without running a risk of repetition or saturation.	https://paperswithcode.com/dataset/flyingthings3d	07/12/2015						
3958	CitySaliency	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/citysaliency							
3959	ATD-12K	"ATD-12K is a large-scale animation triplet dataset, which comprises 12,000 triplets(train10k,test2k) by manually inspect and the test2k with rich annotations, including levels of difficulty, the Regions of Interest (RoIs) on movements, and tags on motion categories
The dataset collected from 30 series of movies(Is wild and modern) made by diversified producers, with a total duration of 25+ hours. total of 101 clips in two resolutions (i.e., 1920×1080, 1280×720)

Note that some triplet has subtitles, watermarking issues"	https://paperswithcode.com/dataset/atd-12k	06/04/2021						
3960	Project CodeNet	"Project CodeNet is a large-scale dataset with approximately 14 million code samples, each of which is an intended solution to one of 4000 coding problems. The code samples are written in over 50 programming languages (although the dominant languages are C++, C, Python, and Java) and they are annotated with a rich set of information, such as its code size, memory footprint, cpu run time, and status, which indicates acceptance or error types. The dataset is accompanied by a repository, where we provide a set of tools to aggregate codes samples based on user criteria and to transform code samples into token sequences, simplified parse trees and other code graphs. A detailed discussion of Project CodeNet is available in this paper.
The rich annotation of Project CodeNet enables research in code search, code completion, code-code translation, and a myriad of other use cases. We also extracted several benchmarks in Python, Java and C++ to drive innovation in deep learning and machine learning models in code classification and code similarity.
Citation
@inproceedings{puri2021codenet,
  author = {Ruchir Puri and David Kung and Geert Janssen and Wei Zhang and Giacomo Domeniconi and Vladmir Zolotov and Julian Dolby and Jie Chen and Mihir Choudhury and Lindsey Decker and Veronika Thost and Luca Buratti and Saurabh Pujar and Ulrich Finkler},
  title = {Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks},
  year = {2021},
 }
Source: Project CodeNet"	https://paperswithcode.com/dataset/project-codenet	25/05/2021						
3961	DanbooRegion	"DanbooRegion is a dataset consists of 5377 in-the-wild illustration downloaded from the Danbooru2018 and region segment map annotation pairs
samples are provided as at 1024px 8-bit RGB images, and region segment maps as int-32 index images."	https://paperswithcode.com/dataset/danbooregion	01/08/2020						
3962	Voice Navigation	Voice Navigation is a large-scale dataset of Chinese speech for slot filling, containing more than 830,000 samples.	https://paperswithcode.com/dataset/voice-navigation	10/05/2021						
3963	Active Terahertz	This is a public dataset for evaluating multi-object detection algorithms in active Terahertz imaging resolution 5 mm by 5 mm.	https://paperswithcode.com/dataset/active-terahertz	08/05/2021						
3964	BookSum	"BookSum is a collection of datasets for long-form narrative summarization. This dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of this dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures.
BookSum contains summaries for 142,753 paragraphs, 12,293 chapters and 436 books."	https://paperswithcode.com/dataset/booksum	18/05/2021						
3965	SPI dataset	The SPI dataset consists of force-controlled industrial robot data for training shadow program inversion (SPI) models.	https://paperswithcode.com/dataset/spi-dataset	26/03/2021						
3966	QAConv	"QAConv is a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations including business emails, panel discussions, and work channels. Unlike opendomain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge. In total, we collect 34,204 QA pairs, including span-based, free-form, and unanswerable questions, from 10,259 selected conversations with both human-written and machine-generated questions. We segment long conversations into chunks, and use a question generator and dialogue summarizer as auxiliary tools to collect multi-hop questions. The dataset has two testing scenarios, chunk mode and full mode, depending on whether the grounded chunk is provided or retrieved from a large conversational pool.
Source: QAConv: Question Answering on Informative Conversations
Image source: QAConv: Question Answering on Informative Conversations"	https://paperswithcode.com/dataset/qaconv	14/05/2021						
3967	Fetoscopy Placenta Data	"The fetoscopy placenta dataset is associated with our MICCAI2020 publication titled “Deep Placental Vessel Segmentation for Fetoscopic Mosaicking”. The dataset contains 483 frames with ground-truth vessel segmentation annotations taken from six different in vivo fetoscopic procedure videos. The dataset also includes six unannotated in vivo continuous fetoscopic video clips (950 frames) with predicted vessel segmentation maps obtained from the leave-one-out cross-validation of our method.
For ground-truth vessel annotation, we selected the non-occluded (no fetus or tool presence) frames through a separate frame-level fetoscopic event identification approach Bano:IJCARS2020. We annotate a binary mask for vessel segmentation using the Pixel Annotation Tool."	https://paperswithcode.com/dataset/fetoscopy-placenta-data	08/07/2020						
3968	Fusion-DHL	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/fusion-dhl	18/05/2021						
3969	seeds	"The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.
The data set can be used for the tasks of classification and cluster analysis."	https://paperswithcode.com/dataset/seeds	30/04/2010						
3970	97 synthetic datasets	"97 synthetic datasets consists of 97 datasets (as illustrated in the figure) and can be used to test graph-based clustering algorithms. 
https://github.com/deric/clustering-benchmark"	https://paperswithcode.com/dataset/97-synthetic-datasets	01/01/2019						
3971	PPR10K	"PPR10K is a dataset for portrait photo retouching (PPR), which aims to enhance the visual quality of a collection of flat-looking portrait photos. The Portrait Photo Retouching dataset (PPR10K) is a large-scale and diverse dataset that contains:

11,161 high-quality raw portrait photos (resolutions from 4K to 8K) in 1,681 groups;
3 versions of manual retouched targets of all photos given by 3 expert retouchers;
full resolution human-region masks of all photos."	https://paperswithcode.com/dataset/ppr10k	19/05/2021	Portrait Photo Retouching dataset					
3972	SoftAttributes	The dataset consists of sets of movie titles, with each set annotated with a single English soft attribute (subjective descriptive property, such as 'confusing' or 'romantic') and a reference movie. For each set, a crowd worker has placed the movies into three sets: more, equally, and less than the reference movie. There are 5,991 such sets, from which one can infer approximately 250,000 pairwise preferences over movies for the 60 distinct soft attributes studied.	https://paperswithcode.com/dataset/softattributes	19/05/2021	SoftAttributes: Relative movie attribute dataset for soft attributes					
3973	Ali-CCP	This data set is provided by Alimama	https://paperswithcode.com/dataset/ali-ccp		Alibaba Click and Conversion Prediction					
3974	Essay-BR	"This repository contains essays written by high school Brazilian students. These essays were graded by humans professionals following the criteria of the ENEM exam.
Source: https://github.com/rafaelanchieta/essay"	https://paperswithcode.com/dataset/essay-br	19/05/2021						
3975	OpenMEVA	"OpenMEVA is a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples.
Source: OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics
Image source: OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics"	https://paperswithcode.com/dataset/openmeva	19/05/2021						
3976	RITEyes	Deep neural networks for video based eye tracking have demonstrated resilience to noisy environments, stray reflections and low resolution. However, to train these networks, a large number of manually annotated images are required. To alleviate the cumbersome process of manual labeling, computer graphics rendering is employed to automatically generate a large corpus of annotated eye images under various conditions. In this work, we introduce RIT-Eyes, a novel synthetic eye image generation platform which improves upon previous work by adding features such as retinal retro-reflection, realistic blinks, an active deformable iris and an aspherical cornea. We add various external influences which potentially degrade eye tracking such as corrective eye-wear with varying refractive indices. To demonstrate the utility of RIT-Eyes, we generate and publicly share a large dataset of images with a variety of eye poses and viewing conditions.	https://paperswithcode.com/dataset/riteyes	05/06/2020						
3977	NAVER LABS Localization Datasets	"The NAVER LABS localization datasets are 5 new indoor datasets for visual localization in challenging real-world environments. They were captured in a large shopping mall and a large metro station in Seoul, South Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser scanners. In order to obtain accurate ground truth camera poses, we used a robust LiDAR SLAM which provides initial poses that are then refined using a novel structure-from-motion based optimization.
The datasets are provided in the kapture format and contain about 130k images as well as 6DoF camera poses for training and validation. We also provide sparse Lidar-based depth maps for the training images. The poses of the test set are withheld to not bias the benchmark."	https://paperswithcode.com/dataset/naver-labs-localization-datasets	19/05/2021						
3978	MIT-Adobe FiveK	"The MIT-Adobe FiveK dataset consists of 5,000 photographs taken with SLR cameras by a set of different photographers. They are all in RAW format; that is, all the information recorded by the camera sensor is preserved. We made sure that these photographs cover a broad range of scenes, subjects, and lighting conditions. We then hired five photography students in an art school to adjust the tone of the photos. Each of them retouched all the 5,000 photos using a software dedicated to photo adjustment (Adobe Lightroom) on which they were extensively trained. We asked the retouchers to achieve visually pleasing renditions, akin to a postcard. The retouchers were compensated for their work.
This dataset was collected for our project on learning photographic adjustments. When using images from this dataset, please cite this dataset using the following BibTeX:
@inproceedings{fivek,
    author = ""Vladimir Bychkovsky and Sylvain Paris and Eric Chan and Fr{\'e}do Durand"",
    title = ""Learning Photographic Global Tonal Adjustment with a Database of Input / Output Image Pairs"",
    booktitle = ""The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition"",
    year = ""2011""
}
Source: https://data.csail.mit.edu/graphics/fivek/
Image source: https://data.csail.mit.edu/graphics/fivek/"	https://paperswithcode.com/dataset/mit-adobe-fivek							
3979	behavioral observation data entry apps	In this repository, we provide the set-up files and output files of 5 behavioral observation data entry applications. These applications allow observers to collect animal behavior data on a handheld computer (phone/tablet).	https://paperswithcode.com/dataset/behavioral-observation-data-entry-apps	12/03/2021						
3980	GazeCapture	"From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost $2.5M$ frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10 - 15fps) on a modern mobile device. Our model achieves a prediction error of 1.7cm and 2.5cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.3cm and 2.1cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results.
Image source: Eye Tracking for Everyone"	https://paperswithcode.com/dataset/gazecapture		Eye Tracking for Everyone					
3981	Gaze360	"Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting.
Image source: Gaze360: Physically Unconstrained Gaze Estimation in the Wild"	https://paperswithcode.com/dataset/gaze360	22/10/2019	Physically Unconstrained Gaze Estimation in the Wild					
3982	Rare Diseases Mentions in MIMIC-III	"Data annotation
The 1,073 full rare disease mention annotations (from 312 MIMIC-III discharge summaries) are in full_set_RD_ann_MIMIC_III_disch.csv.
The data split:
* the first 400 rows are used for validation, validation_set_RD_ann_MIMIC_III_disch.csv, and
* the last 673 rows are used for testing, test_set_RD_ann_MIMIC_III_disch.csv.
The 198 rare disease mention annotations (from 145 MIMIC-III radiology reports) are in test_set_RD_ann_MIMIC_III_rad.csv. To note that radiology reports were only used for testing and not for validation.
To note: a row can only be consider a true phenotype of the patient only when the value of the column gold mention-to-ORDO label is 1.
Data sampling and annotation procedure


(i) Randomly sampled 500 discharge summaries (and 1000 radiology reports) from MIMIC-III


(ii) 312 of the 500 discharge summaries (and 145 of the 1000 radiology reports) have at least one positive UMLS mention linked to ORDO, as identified by SemEHR; there are altogether 1073 (and 198 in radiology reports) UMLS/ORDO mentions.


(iii) 3 medical informatics researchers (staff or PhD students) annotated the 1,073 mentions (and 2 medical informatics researchers annotated the 198 mentions in radiology reports), regarding whether they are the correct patient phenotypes matched to UMLS and ORDO. Contradictions in the annotations were then resolved by another research staff having biomedical background.


Data dictionary
| Column   Name                                | Description                                                                                                                                                                                                   |
|----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ROW_ID                                       | Identifier unique to each row, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                     |
| SUBJECT_ID                                | Identifier unique to a patient, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                                                                              |
| HADM_ID                                      | Identifier unique to a patient hospital stay, see https://mimic.physionet.org/mimictables/noteevents/                                                                                                                                                                                                              |
| document structure name                    | The document structure name of the mention. The document structure name is identified by   SemEHR  (only for discharge summaries).                                                                                                          |
| document structure offset in full document | The start and ending offsets of the document structure texts (or template) in the whole discharge summary. The document structure is parsed by SemEHR with regular expressions  (only for discharge summaries).                            |
| mention                                      | The mention identified by SemEHR.                                                                                                                                                                          |
| mention offset in document structure       | The start and ending offsets of the mention in the document structure (only for discharge summaries).                                                                                                                                      |
| mention offset in full document            | The start and ending offsets of the mention in the whole discharge summary. They can be calculated by document structure offset in full document and mention offset in document structure.                                                                                     |
| UMLS with desc                               | The UMLS identified by SemEHR, corresponding to the mention.                                                                                                                                                |
| ORDO with desc                               | The ORDO matched to the UMLS, using the linkage in the ORDO ontology, see https://www.ebi.ac.uk/ols/ontologies/ordo/terms?iri=http%3A%2F%2Fwww.orpha.net%2FORDO%2FOrphanet_3325 as an example.          |
| gold mention-to-UMLS label                 | Whether the mention-UMLS pair indicate a correct phenotype of the patient (i.e. a positive mention that correctly matches to the UMLS concept), 1 if correct, 0 if not.                                 |
| gold UMLS-to-ORDO label                    | Whether the matching is correct from the UMLS concept to the ORDO concept, 1 if correct, 0 if not.                                                                                                          |
| gold mention-to-ORDO label                 | Whether the mention-ORDO triple indicates a correct phenotype of the patient, 1 if correct, 0 if not. This column is 1 if both the mention-to-UMLS label and the UMLS-to-ORDO label are 1, otherwise 0. |
Note:
* These manual annotations are by no means to be perfect. There are hypothetical mentions which were difficult for the annotators to make a decision. Also, they are based on the output of SemEHR, which does not have 100% recall, so the annotations may not cover all rare diseases mentions from the sampled discharge summaries.
* In row 323 from the full set or the validation set, the mention nph is not in the document structure (due to error in mention extraction), thus the gold mention-to-UMLS label is -1."	https://paperswithcode.com/dataset/rare-disease-annotation-from-mimic-iii	05/05/2021	Rare disease mention annotations from a sample of MIMIC-III clinical notes					
3983	APPS	"The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and evaluating the correctness of solutions. The problems range in difficulty from introductory to collegiate competition level and measure coding ability as well as problem-solving. 
The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,836 test cases for checking solutions and 232,444 ground-truth solutions written by humans. Problems can be complicated, as the average length of a problem is 293.2 words. The data are split evenly into training and test sets, with 5,000 problems each. In the test set, every problem has multiple test cases, and the average number of test cases is 21.2. Each test case is specifically designed for the corresponding problem, enabling us to rigorously evaluate program functionality.
Source: Measuring Coding Challenge Competence With APPS
Image source: Measuring Coding Challenge Competence With APPS"	https://paperswithcode.com/dataset/apps	20/05/2021	Automated Programming Progress Standard					
3984	BigCQ	"BigCQ is a dataset of Competency Question templates paired with SPARQL-OWL query templates. These represent templates of ontology requirements formalizations which are then translated into SPARQL-OWL query language used to query T-Box level of ontologies. Thus, such a dataset can be used in various scenarios regarding ontology authoring:

Provide a large scale dataset for automatization of CQ involving tasks (automatic extraction of Glossary of Terms from requirements, automatic translation of CQs into queries to check how mature given ontology is).
Allow to understand better the relation between human-language and ontology constructs.
Make Competency Question driven ontology authoring more popular, since, although CQs are suggested in many ontology design methodologies, there is very limited set of CQs made publicly available.
Provide guidelines on how CQs can be constructed to target given modelling styles."	https://paperswithcode.com/dataset/bigcq	20/05/2021						
3985	KLUE	"Korean Language Understanding Evaluation (KLUE) benchmark is a series of datasets to evaluate natural language understanding capability of Korean language models. KLUE consists of 8 diverse and representative tasks, which are accessible to anyone without any restrictions. With ethical considerations in mind, we deliberately design annotation guidelines to obtain unambiguous annotations for all datasets. Furthermore, we build an evaluation system and carefully choose evaluations metrics for every task, thus establishing fair comparison across Korean language models.
KLUE benchmark is composed of 8 tasks:

Topic Classification (TC)
Sentence Textual Similarity (STS)
Natural Language Inference (NLI)
Named Entity Recognition (NER)
Relation Extraction (RE)
(Part-Of-Speech) + Dependency Parsing (DP)
Machine Reading Comprehension (MRC)
Dialogue State Tracking (DST)"	https://paperswithcode.com/dataset/klue	20/05/2021	Korean Language Understanding Evaluation					
3986	GBSG2	"The German Breast Cancer Study Group (GBSG2) dataset studies the effects of hormone treatment on recurrence-free survival time. 
The event of interest is the recurrence of cancer time. 
This data frame contains the observations of 686 women:

horTh: hormonal therapy, a factor at two levels (yes and no).
age: age of the patients in years.
menostat: menopausal status, a factor at two levels pre (premenopausal) and post (postmenopausal).
tsize: tumor size (in mm).
tgrade: tumor grade, a ordered factor at levels I < II < III.
pnodes: number of positive nodes.
progrec: progesterone receptor (in fmol).

estrec: estrogen receptor (in fmol).


time: recurrence free survival time (in days).

cens: censoring indicator (0- censored, 1- event).

References

W. Sauerbrei and P. Royston (1999). Building multivariable prognostic and diagnostic models: transformation of the predictors by using fractional polynomials. Journal of the Royal Statistics Society Series A, Volume 162(1), 71–94
M. Schumacher, G. Basert, H. Bojar, K. Huebner, M. Olschewski, W. Sauerbrei, C. Schmoor, C. Beyerle, R.L.A. Neumann and H.F. Rauschecker for the German Breast Cancer Study Group (1994), Randomized 2 × 2 trial evaluating hormonal treatment and the duration of chemotherapy in node- positive breast cancer patients. Journal of Clinical Oncology, 12, 2086–2093

Source: Lifelines"	https://paperswithcode.com/dataset/gbsg2		German Breast Cancer Study Group 2					
3987	PBC	"Primary sclerosing cholangitis is an autoimmune disease leading to destruction of the small bile ducts in the liver. Progression is slow but inexhortable, eventually leading to cirrhosis and liver decompensation. The condition has been recognised since at least 1851 and was named ""primary biliary cirrhosis"" in 1949. Because cirrhosis is a feature only of advanced disease, a change of its name to ""primary biliary cholangitis"" was proposed by patient advocacy groups in 2014.
This data is from the Mayo Clinic trial in PBC conducted between 1974 and 1984. A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval, met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine. The first 312 cases in the data set participated in the randomized trial and contain largely complete data. The additional 112 cases did not participate in the clinical trial, but consented to have basic measurements recorded and to be followed for survival. Six of those cases were lost to follow-up shortly after diagnosis, so the data here are on an additional 106 cases as well as the 312 randomized participants.

age: in years
albumin:   serum albumin (g/dl)
alk.phos: alkaline phosphotase (U/liter)
ascites: presence of ascites
ast: aspartate aminotransferase, once called SGOT (U/ml)
bili: serum bilirunbin (mg/dl)
chol: serum cholesterol (mg/dl)
copper: urine copper (ug/day)
edema: 0 no edema, 0.5 untreated or successfully treated, 1 edema despite diuretic therapy
hepato: presence of hepatomegaly or enlarged liver
id: case number
platelet: platelet count
protime:  standardised blood clotting time
sex: m/f
spiders: blood vessel malformations in the skin
stage: histologic stage of disease (needs biopsy)
status: status at endpoint, 0/1/2 for censored, transplant, dead
time: number of days between registration and the earlier of death, transplantion, or study analysis in July, 1986
trt: 1/2/NA for D-penicillmain, placebo, not randomised
trig: triglycerides (mg/dl)

Source: R"	https://paperswithcode.com/dataset/pbc		Mayo Clinic Primary Biliary Cholangitis data					
3988	Toyota Smarthome dataset	Toyota Smarthome Trimmed has been designed for the activity classification task of 31 activities. The videos were clipped per activity, resulting in a total of 16,115 short RGB+D video samples.  activities were performed in a natural manner. As a result, the dataset poses a unique combination of challenges: high intra-class variation, high-class imbalance, and activities with similar motion and high duration variance. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome Trimmed from other datasets for activity classification.	https://paperswithcode.com/dataset/toyota-smarthome-dataset	01/10/2019	Toyota Smarthome Trimmed					
3989	VPCD	"VPCD contains multi-modal annotations (face, body and voice) for all primary and secondary characters from a range of diverse TV-shows and movies. It is used for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features.
It consists of more than 30,000 face and body tracks of 300+ characters, from over 23 hours of video."	https://paperswithcode.com/dataset/vpcd	20/05/2021	Video Person-Clustering					
3990	MNIST Large Scale dataset	"The MNIST Large Scale dataset is based on the classic MNIST dataset, but contains large scale variations up to a factor of 16. The motivation behind creating this dataset was to enable testing the ability of different algorithms to learn in the presence of large scale variability and specifically the ability to generalise to new scales not present in the training set over wide scale ranges.
The dataset contains training data for each one of the relative size factors 1, 2 and 4 relative to the original MNIST dataset and testing data for relative scaling factors between 1/2 and 8, with a ratio of $\sqrt4{2}$ between adjacent scales."	https://paperswithcode.com/dataset/mnist-large-scale-dataset	05/06/2020						
3991	NewsTSC	NewsTSC is a dataset for target-dependent sentiment classification (TSC), to investigate TSC in news articles, a much less researched domain, despite the importance of news as an essential information source in individual and societal decision making.	https://paperswithcode.com/dataset/newstsc	20/05/2021						
3992	DeepCAD	DeepCAD is a CAD dataset consisting of 179,133 models and their CAD construction sequences. It can be used to train generative models of 3D shapes.	https://paperswithcode.com/dataset/deepcad	20/05/2021						
3993	UIT-ViWikiQA	The UIT-ViWikiQA is a dataset for evaluating sentence extraction-based machine reading comprehension in the Vietnamese language. The UIT-ViWikiQA dataset is converted from the UIT-ViQuAD dataset, consisting of 23,074 question-answers based on 5,109 passages of 174 Vietnamese articles from Wikipedia.	https://paperswithcode.com/dataset/uit-viwikiqa	19/05/2021						
3994	ZuBuD	The goal of the ZuBuD Image Database is to share image data sets with researcheres around the world. To facilitate this, we have created this site, which contains over 1005 images about Zurich city building. The detail information about the database can be found on our Technical Report:TR-​260.	https://paperswithcode.com/dataset/zubud-1	01/04/2003	Zurich Buildings Database					
3995	A Billion Ways to Grasp	Robot grasping is often formulated as a learning problem. With the increasing speed and quality of physics simulations, generating large-scale grasping data sets that feed learning algorithms is becoming more and more popular. An often overlooked question is how to generate the grasps that make up these data sets. In this paper, we review, classify, and compare different grasp sampling strategies. Our evaluation is based on a fine-grained discretization of SE(3) and uses physics-based simulation to evaluate the quality and robustness of the corresponding parallel-jaw grasps. Specifically, we consider more than 1 billion grasps for each of the 21 objects from the YCB data set. This dense data set lets us evaluate existing sampling schemes w.r.t. their bias and efficiency. Our experiments show that some popular sampling schemes contain significant bias and do not cover all possible ways an object can be grasped.	https://paperswithcode.com/dataset/a-billion-ways-to-grasp	11/12/2019						
3996	The RBO Dataset of Articulated Objects and Interactions	The RBO dataset of articulated objects and interactions is a collection of 358 RGB-D video sequences (67:18 minutes) of humans manipulating 14 articulated objects under varying conditions (light, perspective, background, interaction). All sequences are annotated with ground truth of the poses of the rigid parts and the kinematic state of the articulated object (joint states) obtained with a motion capture system. We also provide complete kinematic models of these objects (kinematic structure and three-dimensional textured shape models). In 78 sequences the contact wrenches during the manipulation are also provided.	https://paperswithcode.com/dataset/the-rbo-dataset-of-articulated-objects-and	17/06/2018						
3997	Clarkson Fingerprint Generator	Clarkson Fingerprint Generator consists of a dataset of 50K synthetically generated fingerprints.	https://paperswithcode.com/dataset/clarkson-fingerprint-generator	21/05/2021						
3998	ReactionGIF	ReactionGIF is an affective dataset of 30K tweets which can be used for tasks like induced sentiment prediction and multilabel classification of induced emotions.	https://paperswithcode.com/dataset/reactiongif	20/05/2021						
3999	scb_name_length_data_Sweden_Stockholm_2019	"Appendix A in this paper contains a real-world name length data for the whole of Sweden as well as Stockholm Municipality (Swedish: Stockholms kommun) as of 31 December 2019. It excludes names that either belong to people with protected identities or are suspiciously incorrect due to errors in petition.  But these excluded numbers are low and should not matter for statistical purposes. 
The data are in the forms first name || last name (fl) and first name || maiden name || last name (fml). The name lengths are counted straight off with no spaces between different parts of the name. 
It could be useful for security and privacy research, e.g., to evaluate message expansion of different padding schemes.
Source: Swedish government agency SCB (Statistiska centralbyrån) https://www.scb.se/."	https://paperswithcode.com/dataset/scb-name-length-data-sweden-stockholm-2019	21/05/2021	SCB's Name Length Data in Sweden and Stockholm					
4000	DIBCO and H_DIBCO	"In H-DIBCO 2018, the general objective is to record recent advances in document image binarization using established evaluation performance measures. The benchmarking dataset that will be used in the contest will augment the existing dataset of the DIBCO series containing handwritten document images that are representative of the potential problems which are challenging in the binarization process.
Read more from the competition page."	https://paperswithcode.com/dataset/dibco-and-h-dibco		Document Image Binarization COmpetition (DIBCO)					
4001	EPISURG	"EPISURG is a clinical dataset of $T_1$-weighted magnetic resonance images (MRI) from 430 epileptic patients who underwent resective brain surgery at the National Hospital of Neurology and Neurosurgery (Queen Square, London, United Kingdom) between 1990 and 2018.
The NIfTI files are anonymised and the images have been defaced to further protect the patients' identity.
The dataset comprises 430 postoperative MRI. The corresponding preoperative MRI is present for 269 subjects.
Three human raters segmented the resection cavity on partially overlapping subsets of EPISURG:

Rater 1: 133 subjects (researcher in neuroimaging)
Rater 2: 34 subjects (clinical research fellow)
Rater 3: 33 subjects (neurologist)

Acknowledgements
If you use this dataset for your research please cite the following publications:
Pérez-García F., Rodionov R., Alim-Marvasti A., Sparks R., Duncan J.S., Ourselin S. (2020) Simulation of Brain Resection for Cavity Segmentation Using Self-supervised and Semi-supervised Learning. In: Martel A.L. et al. (eds) Medical Image Computing and Computer Assisted Intervention – MICCAI 2020. Lecture Notes in Computer Science, vol 12263. Springer, Cham. https://doi.org/10.1007/978-3-030-59716-0_12
Pérez-García F., Rodionov R., Alim-Marvasti A., Sparks R., Duncan J.S., Ourselin S. EPISURG: MRI dataset for quantitative analysis of resective neurosurgery for refractory epilepsy. University College London (2020). DOI 10.5522/04/9996158.v1
Graphical user interface (GUI)
The 3D Slicer extension EPISURG may be used to visualise the dataset.
Data use agreement
The EPISURG data are distributed to the greater scientific community under the following terms:

You will not attempt to establish the identity or to make contact with any of the included subjects.
You will acknowledge the use of EPISURG data and data derived from EPISURG data when publicly presenting any results or algorithms that benefitted from their use. Papers, book chapters, books, posters, oral presentations, and all other printed and digital presentations of results derived from EPISURG data should cite the publications listed above.
You will not further disclose these data beyond the uses outlined in this agreement and understand that redistribution of data in any manner is prohibited.
You will require anyone on your team who uses these data, or anyone with whom you share these data to comply with this data use agreement."	https://paperswithcode.com/dataset/episurg		EPISURG: a dataset of postoperative MRI for quantitative analysis of resection neurosurgery for refractory epilepsy					
4002	SICAPv2	"SICAPv2 is a database containing prostate histology whole slide images with both annotations of global Gleason scores and path-level Gleason grades. 
Data associated with the paper:
Silva-Rodríguez, J., Colomer, A., Sales, M. A., Molina, R., & Naranjo, V. (2020). Going deeper through the Gleason scoring scale : An automatic end-to-end system for histology prostate grading and cribriform pattern detection. Computer Methods and Programs in Biomedicine, 195. https://doi.org/10.1016/j.cmpb.2020.105637"	https://paperswithcode.com/dataset/sicapv2	21/05/2021						
4003	FineAction	FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. FineAction introduces new opportunities and challenges for temporal action localization, thanks to its distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes.	https://paperswithcode.com/dataset/fineaction	24/05/2021						
4004	VANiLLa	VANiLLa is a dataset for Question Answering over Knowledge Graphs (KGQA) offering answers in natural language sentences. The answer sentences in this dataset are syntactically and semantically closer to the question than to the triple fact. The dataset consists of over 100k simple questions adapted from the CSQA and SimpleQuestionsWikidata datasets and generated using a semi-automatic framework.	https://paperswithcode.com/dataset/vanilla	24/05/2021						
4005	TabStructDB	"In ICDAR-17, a Page-Object Detection (POD) competition was organized where the task was to identify page objects in documents which includes tables, figures and equations in document. The dataset was composed of 2417 images in total, where 1600 images were used for training, while the rest of the 817 images were used for testing. We are introducing a new table structure recognition dataset, TabStructDB, where we labeled each tabular region present in the ICDAR-17 POD dataset with table structure information comprising of the row and column information.
Source: DeepTabStR: Deep Learning based Table Structure Recognition"	https://paperswithcode.com/dataset/tabstructdb							
4006	TEP	"The original paper presented a model of the industrial chemical process named Tennessee Eastman Process and a model-based TEP simulator for data generation. The most widely used benchmark consists of 22 datasets, 21 of which (Fault 1–21) contain faults and 1 (Fault 0) is fault-free. It is available in repository. All datasets have training (500 samples) and testing (960 samples) parts: training part has healthy state observations, testing part begins right after training, and contains faults which appear after 8 h since the training part. Each dataset has 52 features or observation variables with a 3 min sampling rate for most of all.
Source: Unsupervised Offline Changepoint Detection Ensembles"	https://paperswithcode.com/dataset/tep	01/01/1993	Tennessee Eastman Process					
4007	FB122		https://paperswithcode.com/dataset/fb122	01/11/2016	Freebase-122					
4008	TaxiBJ	"TaxiBJ consists of trajectory data from taxicab GPS data and meteorology data in Beijing from four time intervals: 1st Jul. 2013 - 30th Otc. 2013, 1st Mar. 2014 - 30th Jun. 2014, 1st Mar. 2015 - 30th Jun. 2015, 1st Nov. 2015 - 10th Apr. 2016. 
Source: https://arxiv.org/pdf/1701.02543.pdf"	https://paperswithcode.com/dataset/taxibj	01/10/2016						
4009	VidHOI	"VidHOI is a video-based human-object interaction detection benchmark. VidHOI is based on VidOR which is densely annotated with all humans and predefined objects showing up in each frame. VidOR is also more challenging as the videos are non-volunteering user-generated and thus jittery at times.
Image source: https://xdshang.github.io/docs/vidor.html"	https://paperswithcode.com/dataset/vidhoi	25/05/2021						
4010	PIC	The Person In Context (PIC) dataset is a dataset for human-centric relation segmentation (HRS), which contains 17,122 high-resolution images and densely annotated entity segmentation and relations, including 141 object categories, 23 relation categories and 25 semantic human parts.	https://paperswithcode.com/dataset/pic	24/05/2021	Person In Context 2021					
4011	JobStack	JobStack is a new corpus for de-identification of personal data in job vacancies on Stackoverflow.  De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data.	https://paperswithcode.com/dataset/jobstack	24/05/2021						
4012	WikiBioCTE	WikiBioCTE is a dataset for controllable text edition based on the existing dataset WikiBio (originally created for table-to-text generation).  In the task of controllable text edition the input is a long text, a question, and a target answer, and the output is a minimally modified text, so that it fits the target answer. This task is very important in many situations, such as changing some conditions, consequences, or properties in a legal document, or changing some key information of an event in a news text.	https://paperswithcode.com/dataset/wikibiocte	23/05/2021						
4013	BDD-X	"Berkeley Deep Drive-X (eXplanation) is a dataset is composed of over 77 hours of driving within 6,970 videos. The videos are taken in diverse driving conditions, e.g. day/night, highway/city/countryside, summer/winter etc. On average 40 seconds long, each video contains around 3-4 actions, e.g. speeding up, slowing down, turning right etc., all of which are annotated with a description and an explanation. Our dataset contains over 26K activities in over 8.4M frames.
Image source: https://github.com/JinkyuKimUCB/BDD-X-dataset"	https://paperswithcode.com/dataset/bdd-x		Berkeley Deep Drive-X (eXplanation)					
4014	"Dataset for: ""It is just a flu: Assessing the Effect of Watch History on YouTube's Pseudoscientific Video Recommendations"""	"The dataset consists of three files: the metadata, comments, and captions of the ground-truth dataset videos collected and manually reviewed in this paper.

Video Metadata:
""groundtruth_videos.json"": Contains the metadata of our manually reviewed ground-truth dataset videos. The ground-truth dataset includes 1,197 science, 1,325 pseudoscience, and 3,212 irrelevant videos. More specifically, it includes the metadata of videos related to the following pseudoscientific topics:
COVID-19: (607 science, 368 pseudoscience, 721 irrelevant videos)
Anti-vaccination (363 science, 394 pseudoscience, and 1,060 irrelevant videos)
Anti-mask (65 science, 188 pseudoscience, and 724 irrelevant videos)
Flat Earth (162 science, 375 pseudoscience, and 707 irrelevant videos)

Note, that 600 of the videos in this dataset include the ""annotation.manual_review_label"" attribute, which is the label assigned by the first author of this paper to evaluate the performance of the crowdsourced annotation process.

Video Metadata Description:
""search_term"": The search terms used to search YouTube and retrieve these videos during our data collection. It can be one of the following search terms: 'covid-19', 'coronavirus', 'anti-vaccination', 'anti-vaxx', 'anti-mask', or 'flat earth'.
""annotation.annotations"": The list of the three annotations assigned to each video by our crowdsourced annotators.
""annotation.label"": The annotation label assigned to the video based on the majority agreement of the crowdsourced annotators.
""annotation.manual_review_label"": The label assigned by the first author of this paper to evaluate the performance of the crowdsourced annotation process.
""isSeed"": 0 if the video is a seed video of our data collection, 1 if it is a recommended video of a seed video.

""relatedVideos"": The recommended videos of the given video as returned by the YouTube Data API.


Video Comments: 


""groundtruth_videos_comments_ids.json"": Includes the identifiers of the comments of our ground-truth videos.


Video Transcripts:

""groundtruth_videos_transcripts.json"": Includes the captions of our ground-truth videos.
If you use this dataset in any publication, of any form and kind, please cite using this data."	https://paperswithcode.com/dataset/dataset-for-it-is-just-a-flu-assessing-the	22/10/2020						
4015	Enron Email Dataset	This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.	https://paperswithcode.com/dataset/enron-email-dataset		Enron Email Dataset					
4016	DCASE 2019 Mobile	"TAU Urban Acoustic Scenes 2019 Mobile development dataset consists of 10-seconds audio segments from 10 acoustic scenes:
Airport
Indoor shopping mall
Metro station
Pedestrian street
Public square
Street with medium level of traffic
Travelling by a tram
Travelling by a bus
Travelling by an underground metro
Urban park

Recordings were made with three devices that captured audio simultaneously. Each acoustic scene has 1440 segments (240 minutes of audio) recorded with device A (main device) and 108 segments of parallel audio (18 minutes) each recorded with devices B and C. The dataset contains in total 46 hours of audio.
DCASE website
Source: Zenodo 
Image Source: Acoustic Scene Classification"	https://paperswithcode.com/dataset/dcase-2019-mobile	25/07/2018	TAU Urban Acoustic Scenes 2019 Mobile					
4017	MacaquePose	MacaquePose is an animal pose estimation dataset containing pictures of macaque monkeys and manually labeled annotations on them.	https://paperswithcode.com/dataset/macaquepose		MacaquePose					
4018	Vinegar Fly	Vinegar Fly is a pose estimation dataset for fruit flies.	https://paperswithcode.com/dataset/vinegar-fly		Vinegar Fly					
4019	Desert Locust	Desert Locus is a animal pose estimation dataset for desert locuses.	https://paperswithcode.com/dataset/desert-locust		Desert Locust					
4020	Grévy’s Zebra	Grévy’s Zebra is an animal pose estimation dataset for zebras.	https://paperswithcode.com/dataset/grevys-zebra							
4021	RHD	Rendered Hand Pose (RHD) is a dataset for hand pose estimation. It provides segmentation maps with 33 classes: three for each finger, palm, person, and background. The 3D kinematic model of the hand provides 21 keypoints per hand: 4 keypoints per finger and one keypoint close to the wrist.	https://paperswithcode.com/dataset/rhd	03/05/2017	Rendered Hand Pose					
4022	TransNAS-Bench-101	TransNAS-Bench-101 is a Neural Architecture Search (NAS) benchmark dataset containing network performance across seven tasks, covering classification, regression, pixel-level prediction, and self-supervised tasks. This diversity provides opportunities to transfer NAS methods among tasks and allows for more complex transfer schemes to evolve. We explore two fundamentally different types of search space: cell-level search space and macro-level search space. With 7,352 backbones evaluated on seven tasks, 51,464 trained models with detailed training information are provided. With TransNAS-Bench-101, we hope to encourage the advent of exceptional NAS algorithms that raise cross-task search efficiency and generalizability to the next level.	https://paperswithcode.com/dataset/transnas-bench-101	25/05/2021						
4023	EarthNet2021	Satellite images are snapshots of the Earth surface. We propose to forecast them. We frame Earth surface forecasting as the task of predicting satellite imagery conditioned on future weather. EarthNet2021 is a large dataset suitable for training deep neural networks on the task. It contains Sentinel~2 satellite imagery at $20$~m resolution, matching topography and mesoscale ($1.28$~km) meteorological variables packaged into $32000$ samples. Additionally we frame EarthNet2021 as a challenge allowing for model intercomparison. Resulting forecasts will greatly improve ($>\times50$) over the spatial resolution found in numerical models. This allows localized impacts from extreme weather to be predicted, thus supporting downstream applications such as crop yield prediction, forest health assessments or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech.	https://paperswithcode.com/dataset/earthnet2021	16/04/2021	EarthNet2021: Earth Surface Forecasting					
4024	DaN+	DaN+ is a new multi-domain corpus and annotation guidelines for Danish nested named entities (NEs) and lexical normalization to support research on cross-lingual cross-domain learning for a less-resourced language.	https://paperswithcode.com/dataset/dan	24/05/2021						
4025	BAM!	"The Behance Artistic Media dataset (BAM!) is a large-scale dataset of contemporary artwork from Behance, a website containing millions of portfolios from professional and commercial artists. We annotate Behance imagery with rich attribute labels for content, emotions, and artistic media. We believe our Behance Artistic Media dataset will be a good starting point for researchers wishing to study artistic imagery and relevant problems.
The dataset consists of:

Automatically-labeled binary attribute scores for over 2.5 million images across 20 attributes each
393,000 crowdsourced binary attribute labels for individual images
Short image descriptions/captions for 74,000 images from the crowd"	https://paperswithcode.com/dataset/bam	27/04/2017	Behance Artistic Media					
4026	XGLUE	XGLUE is an evaluation benchmark XGLUE,which is composed of 11 tasks that span 19 languages. For each task, the training data is only available in English. This means that to succeed at XGLUE, a model must have a strong zero-shot cross-lingual transfer capability to learn from the English data of a specific task and transfer what it learned to other languages. Comparing to its concurrent work XTREME, XGLUE has two characteristics: First, it includes cross-lingual NLU and cross-lingual NLG tasks at the same time; Second, besides including 5 existing cross-lingual tasks (i.e. NER, POS, MLQA, PAWS-X and XNLI), XGLUE selects 6 new tasks from Bing scenarios as well, including News Classification (NC), Query-Ad Matching (QADSM), Web Page Ranking (WPR), QA Matching (QAM), Question Generation (QG) and News Title Generation (NTG). Such diversities of languages, tasks and task origin provide a comprehensive benchmark for quantifying the quality of a pre-trained model on cross-lingual natural language understanding and generation.	https://paperswithcode.com/dataset/xglue	03/04/2020						
4027	CAMO++	CAMO++ is a dataset for camouflaged object segmentation. This dataset increases the number of images with hierarchical pixel-wise ground-truths. The authors also provide a benchmark suite for the task of camouflaged instance segmentation.	https://paperswithcode.com/dataset/camo-1	31/03/2021						
4028	GLGE	GLGE is a general language generation evaluation benchmark which is composed of 8 language generation tasks, including Abstractive Text Summarization (CNN/DailyMail, Gigaword, XSUM, MSNews), Answer-aware Question Generation (SQuAD 1.1, MSQG), Conversational Question Answering (CoQA), and Personalizing Dialogue (Personachat).	https://paperswithcode.com/dataset/glge	24/11/2020	General Language Generation Evaluation					
4029	LandCover.ai	"The LandCover.ai (Land Cover from Aerial Imagery) dataset is a dataset for automatic mapping of buildings, woodlands, water and roads from aerial images. 
Dataset features

land cover from Poland, Central Europe
three spectral bands - RGB
33 orthophotos with 25 cm per pixel resolution (~9000x9500 px)
8 orthophotos with 50 cm per pixel resolution (~4200x4700 px)
total area of 216.27 sq. km

Dataset format

rasters are three-channel GeoTiffs with EPSG:2180 spatial reference system
masks are single-channel GeoTiffs with EPSG:2180 spatial reference system"	https://paperswithcode.com/dataset/landcover-ai	05/05/2020	Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery					
4030	CURE-OR	CURE-OR is a large-scale, controlled, and multi-platform object recognition dataset denoted as Challenging Unreal and Real Environments for Object Recognition. In this dataset, there are 1,000,000 images of 100 objects with varying size, color, and texture that are positioned in five different orientations and captured using five devices including a webcam, a DSLR, and three smartphone cameras in real-world (real) and studio (unreal) environments. The controlled challenging conditions include underexposure, overexposure, blur, contrast, dirty lens, image noise, resizing, and loss of color information.	https://paperswithcode.com/dataset/cure-or	18/10/2018	Challenging Unreal and Real Environments for Object Recognition					
4031	USM-SED	USM-SED is a dataset for polyphonic sound event detection in urban sound monitoring use-cases. Based on isolated sounds taken from the FSD50k dataset, 20,000 polyphonic soundscapes are synthesized with sounds being randomly positioned in the stereo panorama using different loudness levels.	https://paperswithcode.com/dataset/usm-sed	06/05/2021						
4032	CEREC	CEREC is a large scale corpus for entity resolution in email conversations. The corpus consists of 6001 email threads from the Enron Email Corpus containing 36,448 email messages and 60,383 entity coreference chains. The annotation is carried out as a two-step process with minimal manual effort.	https://paperswithcode.com/dataset/cerec	21/05/2021	Corpus for Entity Resolution in Email Conversations					
4033	GOO	GOO (Gaze-on-Objects) is a dataset for gaze object prediction, where the goal is to predict a bounding box for a person's gazed-at object. GOO is composed of a large set of synthetic images (GOO Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment.	https://paperswithcode.com/dataset/goo	22/05/2021	Gaze on Objects					
4034	SimJEB	Simulated Jet Engine Bracket Dataset (SimJEB)  is a public collection of crowdsourced mechanical brackets and high-fidelity structural simulations designed specifically for surrogate modeling. SimJEB models are more complex, diverse, and realistic than the synthetically generated datasets commonly used in parametric surrogate model evaluation. In contrast to existing engineering shape collections, SimJEB's models are all designed for the same engineering function and thus have consistent structural loads and support conditions. The models in SimJEB were collected from the original submissions to the GrabCAD Jet Engine Bracket Challenge: an open engineering design competition with over 700 hand-designed CAD entries from 320 designers representing 56 countries. Each model has been cleaned, categorized, meshed, and simulated with finite element analysis according to the original competition specifications. The result is a collection of diverse, high-quality and application-focused designs for advancing geometric deep learning and engineering surrogate models.	https://paperswithcode.com/dataset/simjeb	07/05/2021	Simulated Jet Engine Bracket					
4035	POINTREC	POINTREC is a test collection for point of interest (POI) recommendation, comprising of (i) a set of information needs, (ii) a dataset of POIs, and (iii) graded relevance assessments for information need and POI pairs.	https://paperswithcode.com/dataset/pointrec	19/05/2021						
4036	SkyCam	SkyCam dataset is a collection of sky images from a variety of locations with diverse topological characteristics (Swiss Jura, Plateau and Pre-Alps regions), from both single and stereo camera settings coupled with a high-accuracy pyranometers. The dataset was collected with a high frequency with a data sample every 10 seconds. 13 images with different exposures times are generated along with a post-processed HDR images and a solar radiance values for each of the cameras and locations. We hope that SkyCam dataset will enable researchers to tackle the problem of short-term local camera-based solar radiance prediction.	https://paperswithcode.com/dataset/skycam	06/05/2021						
4037	CASIA-Face-Africa	CASIA-Face-Africa is a face image database which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc.	https://paperswithcode.com/dataset/casia-face-africa	08/05/2021						
4038	Robotic Pushing	"The Robotic Pushing Dataset  is a dataset for video prediction for real-world interactive agents which consists of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a ""visual imagination"" of different futures based on different courses of action."	https://paperswithcode.com/dataset/robotic-pushing	23/05/2016						
4039	ParaQA	ParaQA is a question answering (QA) dataset with multiple paraphrased responses for single-turn conversation over knowledge graphs (KG). The dataset was created using a semi-automated framework for generating diverse paraphrasing of the answers using techniques such as back-translation. The existing datasets for conversational question answering over KGs (single-turn/multi-turn) focus on question paraphrasing and provide only up to one answer verbalization. However, ParaQA contains 5000 question-answer pairs with a minimum of two and a maximum of eight unique paraphrased responses for each question.	https://paperswithcode.com/dataset/paraqa	13/03/2021						
4040	EDT	"The EDT dataset is designed for corporate event detection and text-based stock prediction (trading strategy) benchmark.


Corporate Event Detection
It includes 9721​ news articles with token-level event labels. Including 11 event types:
Acquisitions, Clinical Trials, Guidance Changes, New Contracts, Stock Repurchases, Stock Split, Reverse Stock Split/ADS Ratio Change, Regular Dividend, Special Dividend, Dividend Cut, Dividend Increase


Text-Based Stock Prediction Benchmark
It includes 303893​ first-hand news articles from high-quality sources. Each news article is assigned a minute-level timestamp and comprehensive stock price labels.


Please see this Github Link and paper for more details."	https://paperswithcode.com/dataset/edt	26/05/2021						
4041	ARD-16	"We create ARD-16 (Ati Realworld Dataset), a
first of its kind real-world paired correspondence dataset, by applying our dataset generation method on 16-beam VLP-16 Puck LiDAR scans on a slow-moving Unmanned Ground Vehicle. We obtain ground truth poses by using fine resolution brute force scan matching, similar to Google's Cartographer. It was captured in outdoor environment at Robert Bosch centre, IISc
with no moving objects during static run and several moving objects (1 car, 1 2-wheeler, few pedestrians) during dynamic run. It consists of 1.5k scans/run and we collected 10 dynamic and 5 static runs. This gives about 14k LiDAR scan pairs
for training, validation and testing."	https://paperswithcode.com/dataset/ard-16-dataset	26/05/2021	Ati Real-world Dataset					
4042	CARLA-64	We create 64-beam LiDAR dataset with settings similar to Velodyne VLP-64 LiDAR on the CARLA simulator. It contains no moving objects during static run and several moving objects (cars, 2-wheelers, pedestrians) during dynamic runs. It consists of 16 dynamic runs and 8 static runs. This gives about 32k LiDAR scan pairs for training, validation ad tesing.	https://paperswithcode.com/dataset/carla-64	26/05/2021						
4043	NEMO-Corpus	"Named Entity (NER) annotations of the Hebrew Treebank (Haaretz newspaper) corpus, including: morpheme and token level NER labels, nested mentions, and more.
We publish the NEMO corpus in the TACL paper ""Neural Modeling for Named Entities and Morphology (NEMO^2)"" 1, where we use it in extensive experiments and analyses, showing the importance of morphological boundaries for neural modeling of NER in morphologically rich languages. Code for these models and experiments can be found in the NEMO code repo.
Main features:

Morpheme, token-single and token-multi sequence labels. Morpheme labels provide exact boundaries, token-multi provide partial sub-word morphological but no exact boundaries, token-single provides only token-level information. 
All annotations are in BIOSE format (B=Begin, I=Inside, O=Outside, S=Singleton, E=End).
Widely-used OntoNotes entity category set: GPE (geo-political entity), PER (person), LOC (location), ORG (organization), FAC (facility), EVE (event), WOA (work-of-art), ANG (language), DUC (product).
NEMO includes NER annotations for the two major versions of the Hebrew Treebank, UD (Universal Dependency) and SPMRL. These can be aligned to the other morphosyntactic information layers of the treebank using bclm
We provide nested mentions. Only the first, widest, layer is used in the NEMO^2 paper. We invite you to take on this challenge!
Guidelines used for annotation are provided here.
Corpus was annotated by two native Hebrew speakers of academic education, and curated by the project manager. We provide the original annotations made by the annotators as well to promote work on learning with disagreements.
Annotation was performed using WebAnno (version 3.4.5)

Basic Corpus Statistics
|                              | train        | dev          | test          |
|------------------------------|           --:|           --:|            --:|
|  Sentences                   |  4,937       |  500         |  706          |
|  Tokens                      |  93,504      |  8,531       |  12,619       |
|  Morphemes                   |  127,031     |  11,301      |  16,828       |
|  All mentions                |  6,282       |  499         |  932          |
|  Type: Person         (PER)  |  2,128       |  193         |  267          |
|  Type: Organization   (ORG)  |  2,043       |  119         |  408          |
|  Type: Geo-Political  (GPE)  |  1,377       |  121         |  195          |
|  Type: Location       (LOC)  |  331         |  28          |  41           |
|  Type: Facility       (FAC)  |  163         |  12          |  11           |
|  Type: Work-of-Art    (WOA)  |  114         |  9           |  6            |
|  Type: Event          (EVE)  |  57          |  12          |  0            |
|  Type: Product        (DUC)  |  36          |  2           |  3            |
|  Type: Language       (ANG)  |  33          |  3           |  1            |
Evaluation
An evaluation script is provided in the NEMO code repo along with evaluation instructions.
Citation
@article{10.1162/tacl_a_00404,
    author = {Bareket, Dan and Tsarfaty, Reut},
    title = ""{Neural Modeling for Named Entities and Morphology (NEMO2)}"",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {909-928},
    year = {2021},
    month = {09},
    abstract = ""{Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically rich languages (MRLs) pose a challenge to this basic formulation, as the boundaries of named entities do not necessarily coincide with token boundaries, rather, they respect morphological boundaries. To address NER in MRLs we then need to answer two fundamental questions, namely, what are the basic units to be labeled, and how can these units be detected and classified in realistic settings (i.e., where no gold morphology is available). We empirically investigate these questions on a novel NER benchmark, with parallel token- level and morpheme-level NER annotations, which we develop for Modern Hebrew, a morphologically rich-and-ambiguous language. Our results show that explicitly modeling morphological boundaries leads to improved NER performance, and that a novel hybrid architecture, in which NER precedes and prunes morphological decomposition, greatly outperforms the standard pipeline, where morphological decomposition strictly precedes NER, setting a new performance bar for both Hebrew NER and Hebrew morphological decomposition tasks.}"",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00404},
    url = {https://doi.org/10.1162/tacl\_a\_00404},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00404/1962472/tacl\_a\_00404.pdf},
}"	https://paperswithcode.com/dataset/nemo-corpus	30/07/2020	NEMO Hebrew NER and Morphology Corpus					
4044	Custom FINNgers	A dataset with 3200 images (200 for each number quantity on each hand).	https://paperswithcode.com/dataset/custom-finngers	26/05/2021						
4045	Sentinel 2 manually extracted deep water spectra with high noise levels and sunglint	"This dataset includes 2.133.324 reflectance water spectra which were manually extracted by visual observation from 30 Sentinel 2 level 1C satellite images. The spectra were extracted from deep water areas with high noise levels and sunglint. The Sentinel 2 images depicted 2 tiles of the same orbit and were collected in 2016 (2 images), 2017 (19 images) and 2018 (9 images). The images contain 13 bands, 3 with 60 m spatial resolution, 4 with 10 m spatial resolution and 6 with 20 m spatial resolution. Before the spectra extraction, the bands with spatial resolution 10 and 20 m were resampled to 60 m and then the images were cropped in order to remove the land and depict optically homogenous sea regions. A figure depicting the location of the Sentinel 2 tiles (white polygons (1,2)) and the cropped tiles (red polygons (3,4)) is included in this folder. A figure depicting example scenes from which spectra were obtained through regions of interest (rois) is included as well.
The spectra are stored in .csv files. Each file is named after the name of the Sentinel 2 product which includes sensing and creation date as well the relative orbit number and tile code. The content of each file includes latitude and longitude coordinates (UTM/WGS84 projection) of each spectral signature as well as the reflectance values of the 13 Sentinel 2 bands.
This dataset was created for the purpose of the study described in https://www.tandfonline.com/doi/full/10.1080/01431161.2020.1714776"	https://paperswithcode.com/dataset/sentinel-2-manually-extracted-deep-water	29/01/2020	Sentinel 2 manually extracted deep water spectra with high noise levels and sunglint					
4046	RAW-C	Relatedness judgments of ambiguous English words, in experimentally controlled sentential contexts.	https://paperswithcode.com/dataset/raw-c	27/05/2021						
4047	TexRel	"Green family of datasets for emergent communications on relations.
By comparison with other relations datasets, TexRel provides rapid training and experimentation, whilst being sufficiently large to avoid overfitting in the context of emergent communications."	https://paperswithcode.com/dataset/texrel	26/05/2021						
4048	ShapeWorld	"ShapeWorld is a new evaluation methodology and framework for multimodal deep learning models, with a focus on formal-semantic style generalization capabilities. In this framework, artificial data is automatically generated according to predefined specifications. This controlled data generation makes it possible to introduce previously unseen instance configurations during evaluation, which consequently require the system to recombine learned concepts in novel ways.
Source: ShapeWorld - A new test methodology for multimodal language understanding
Image Source: ShapeWorld - A new test methodology for multimodal language understanding"	https://paperswithcode.com/dataset/shapeworld	14/04/2017						
4049	VideoLT	VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution.	https://paperswithcode.com/dataset/videolt	06/05/2021						
4050	CMWD	CMWD (Cloud Motion Wind Dataset) is the first cloud motion wind dataset for deep learning research. It contains 6388 adjacent grayscale image pairs for training and another 715 images pairs for testing.	https://paperswithcode.com/dataset/cmwd	03/10/2020	Cloud Motion Wind Dataset					
4051	TCLD	TCLD (Typhoon Center Location Dataset) is a brand new typhoon center location dataset for deep learning research. It contains 1809 grayscale images for training and another 319 images for testing.	https://paperswithcode.com/dataset/tcld	03/10/2020	Typhoon Center Location Dataset					
4052	SCMD2016	SCMD dataset is a brand new cloudage nowcasting dataset for deep learning research. It contains 20000 grayscale image sequences for training and another 3500 image sequences for testing. You can get the SCMD2016 dataset at any time but only for scientific research. At the same time, please cite our work when you use the SCMD dataset	https://paperswithcode.com/dataset/scmd2016	19/05/2019	Satellite Cloudage Map Dataset					
4053	EviLOG	"The dataset contains synthetic training, validation and test data for occupancy grid mapping from lidar point clouds. Additionally, real-world lidar point clouds from a test vehicle with the same lidar setup as the simulated lidar sensor is provided.
Point clouds are stored as PCD files and occupancy grid maps are stored as PNG images whereas one image channel describes evidence for a free and another one describes evidence for occupied cell state."	https://paperswithcode.com/dataset/evilog	25/02/2021	Evidential Lidar Occupancy Grid Mapping					
4054	DEAP	"The DEAP dataset consists of two parts:

The ratings from an online self-assessment where 120 one-minute extracts of music videos were each rated by 14-16 volunteers based on arousal, valence and dominance.
The participant ratings, physiological recordings and face video of an experiment where 32 volunteers watched a subset of 40 of the above music videos. EEG and physiological signals were recorded and each participant also rated the videos as above. For 22 participants frontal face video was also recorded."	https://paperswithcode.com/dataset/deap	01/01/2012						
4055	Viwiki-Spelling	We introduce a first Vietnamese Spelling Correction dataset containing manual labelling mistakes and corresponding correct words.	https://paperswithcode.com/dataset/viwiki-spelling	25/05/2021	Vietnamese Spelling Correction Dataset					
4056	RISEdb	"The RISE (Robust Indoor Localization in Complex Scenarios) dataset is meant to train and evaluate visual indoor place recognizers.
It contains more than 1 million geo-referenced images spread over 30 sequences, covering 5 heterogeneous buildings.
For each building we provide:
    - A high resolution 3D point cloud (1cm) that defines the localization reference frame and that was generated with a mobile laser scanner and an inertial system.
    - Several image sequences spread over time with accurate ground truth poses retrieved by the laser scanner. Each sequence contains both, stereo pairs and spherical images.
    - Geo-referenced smartphone data, retrieved from the standard sensors of such devices."	https://paperswithcode.com/dataset/risedb	10/01/2021	Robust Indoor Localization in Complex Scenarios (RISE) database					
4057	SynthDerm	SynthDerm is a synthetically generated dataset inspired by the real-world characteristics of melanoma skin lesions in dermatology settings. These characteristics include whether the lesion is asymmetrical, its border is irregular or jagged, is unevenly colored, has a diameter more than 0.25 inches, or is evolving in size, shape, or color over time. These qualities are usually referred to as ABCDE of melanoma. We generate SynthDerm algorithmically by varying several factors: skin tone, lesion shape, lesion size, lesion location (vertical and horizontal), and whether there are surgical markings present. We randomly assign one of the following to the lesion shape: round, asymmetrical, with jagged borders, or multi-colored (two different shades of colors overlaid with salt-and-pepper noise). For skin tone values, we simulate Fitzpatrick ratings. Fitzpatrick scale is a commonly used approach to classify the skin by its reaction to sunlight exposure modulated by the density of melanin pigments in the skin. This rating has six values, where 1 represents skin that always burns (lowest melanin) and 6 represents skin that never burns in sunlight (highest melanin). For our synthetic generation, we consider six base skin tones that similarly resemble different amounts of melanin. We also add a small amount of random noise to the base color to add further variety. Overall, SynthDerm includes more than 2,600 images of size 64x64.	https://paperswithcode.com/dataset/synthderm	31/05/2021						
4058	Paralex	Paralex learns from a collection of 18 million question-paraphrase pairs scraped from WikiAnswers.	https://paperswithcode.com/dataset/paralex	01/06/2013						
4059	DiaKG	"DiaKG is a high-quality Chinese dataset for Diabetes knowledge graph.
The dataset is derived from 41 diabetes guidelines and consensus, which are from authoritative Chinese journals including basic research, clinical research, drug usage, clinical cases, diagnosis and treatment methods, etc. The dataset covers the most extensive field of research content and hotspot in recent years. All the annotators have a medical background, and finally conduct a high-quality diabetes database which contains 22,050 entities and 6,890 relations in total. Based on this dataset, doctors, researchers, and enterprise developers can develop knowledge bases for clinical diagnosis, knowledge graphs, and auxiliary diagnostics to further explore the mysteries of diabetes."	https://paperswithcode.com/dataset/diakg	31/05/2021						
4060	MAOMaps	MAOMaps is a dataset for evaluation of Visual SLAM, RGB-D SLAM and Map Merging algorithms. It contains 40 samples with RGB and depth images, and ground truth trajectories and maps. These 40 samples are joined into 20 pairs of overlapping maps for map merging methods evaluation. The samples were collected using Matterport3D dataset and Habitat simulator.	https://paperswithcode.com/dataset/maomaps	31/05/2021						
4061	CTSpine1K	"CTSpine1K is a large-scale and comprehensive dataset for research in spinal image analysis. CTSpine1K is curated from the following four open sources, totalling 1,005 CT volumes (over 500,000 labeled slices and over 11,000 vertebrae) of diverse appearance variations.


COLONOG. This is a subset of the CT COLONOGRAPHY dataset related to a CT colonography trial12. 


HNSCC-3DCT-RT. This sub-dataset contains three dimensional (3D) high-resolution fan-beam CT scans collected during pre-treatment, mid-treatment, and post-treatment using a Siemens 16-slice CT scanner with the standard clinical protocol for head-and-neck squamous cell carcinoma (HNSCC) patients13. These images are in DICOM format.


MSD T10. This sub-dataset comes from the 10th Medical Segmentation Decathlon14. To attain more slices containing the spine, we select the task03_liver dataset consisting of 201 cases. These images are in Neuroimaging Informatics Technology Initiative (NIfTI) format (https://nifti.nimh.nih.gov/nifti-1).


COVID-19. This sub-dataset consists of non-enhanced chest CTs from 632 patients with COVID-19 infections. The images were acquired at the point of care in an outbreak setting from patients with Reverse Transcription Polymerase Chain Reaction(RT-PCR) confirmation for the presence of SARS-CoV-215. We pick 40 scans with the images stored in NIfTI format."	https://paperswithcode.com/dataset/ctspine1k	31/05/2021						
4062	Cleft	The Cleft dataset is a collection of ultrasound tongue imaging and audio data, gathered from children with cleft lip and palate by a research speech and language therapist working in a hospital environment.	https://paperswithcode.com/dataset/cleft	31/05/2021						
4063	GeoQA	"GeoQA is a dataset for automatic geometric problem solving containing 5,010 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems
Compared with another publicly available dataset GeoS, GeoQA is 25 times larger, in which the program annotations can provide a practical testbed for future research on explicit and explainable numerical reasoning."	https://paperswithcode.com/dataset/geoqa	30/05/2021	Geometric Question Answering					
4064	GeoS	GeoS is a dataset for automatic math problem solving. It is a dataset of SAT plane geometry questions where every question has a textual description in English accompanied by a diagram and multiple choices. Questions and answers are compiled from previous official SAT exams and practice exams offered by the College Board. We annotate ground-truth logical forms for all questions in the dataset.	https://paperswithcode.com/dataset/geos	01/09/2015						
4065	XL-BEL	XL-BEL is a benchmark for cross-lingual biomedical entity linking (XL-BEL). The benchmark spans 10 typologically diverse languages.	https://paperswithcode.com/dataset/xl-bel	30/05/2021						
4066	CoDesc	CoDesc is a large dataset of 4.2m Java source code and parallel data of their description from code search, and code summarization studies.	https://paperswithcode.com/dataset/codesc	29/05/2021						
4067	D-OCC	D-OCC is a large-scale dataset of 5,617 dialogues to enable fine-grained evaluation and analysis of various dialogue systems. It is used to study common grounding in dynamic environments.	https://paperswithcode.com/dataset/d-occ	29/05/2021	Dynamic-OneCommon Corpus					
4068	Neural Closure Models - Runs	The following are all the runs used to generate figures in the paper. Every experiment solves the corresponding high- and low-fidelity model to generate the training, validation, and prediction data.	https://paperswithcode.com/dataset/neural-closure-models-runs	27/12/2020						
4069	OTTers	"OTTers is a  dataset of human one-turn topic transitions. In this task, models must connect two topics in a cooperative and coherent manner, by generating a ""bridging"" utterance connecting the new topic tot he topic of the previous conversation turn."	https://paperswithcode.com/dataset/otters	28/05/2021						
4070	Instantiation Dataset	Instantiation is a dataset for the task of instantiation detection	https://paperswithcode.com/dataset/instantiation	05/08/2018						
4071	LIGHT-Quests	LIGHT-Quests is an extension of LIGHT, a large-scale crowd-sourced fantasy text-game, to generate a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both).	https://paperswithcode.com/dataset/light-quests	01/10/2020						
4072	UW-IS	UW-IS (UW Indoor Scenes) is a dataset for object recognition in indoor environments comprising scene images from two different environments, namely, a living room and a mock warehouse.	https://paperswithcode.com/dataset/uw-is	07/10/2020	UW Indoor Scenes					
4073	MT40K	The MT40K dataset for predicting malware threat intelligence is a collection of 40,000 triples generated from 27,354 unique entities and 34 relations. The corpus consists of approximately 1,100 de-identified plain text threat reports written between 2006-2021 and all CVE vulnerability descriptions created between 1990 to 2021. The annotated keyphrases were classified into entities derived from semantic categories defined in malware threat ontologies.	https://paperswithcode.com/dataset/mt40k	10/02/2021						
4074	MoleculeNet	MoleculeNet is a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance.	https://paperswithcode.com/dataset/moleculenet	02/03/2017						
4075	Data Collected with Package Delivery Quadcopter Drone	"This experiment was performed in order to empirically measure the energy use of small, electric Unmanned Aerial Vehicles (UAVs). We autonomously direct a DJI ® Matrice 100 (M100) drone to take off, carry a range of payload weights on a triangular flight pattern, and land. Between flights, we varied specified parameters through a set of discrete options, payload of 0 , 250 g and 500 g; altitude during cruise of 25 m, 50 m, 75 m and 100 m; and speed during cruise of 4 m/s, 6 m/s, 8 m/s, 10 m/s and 12 m/s.
We simultaneously collect data from a broad array of on-board sensors. The onboard sensors used to collect these data are


Wind sensor: FT Technologies FT205 UAV-mountable, pre-calibrated ultrasonic wind sensor with accuracy of $\pm$ 0.1 m/s and refresh rate of 10 Hz.;


Position: 3DM-GX5-45 GNSS/INS sensor pack. These sensors use a built-in Kalman filtering system to fuse the GPS and IMU data. The sensor has a maximum output rate of 10Hz with accuracy of $\pm$2 m RMS horizontal, $\pm$5 m RMS vertical.


Current and Voltage: Mauch Electronics PL-200 sensor. This sensor can record currents up to 200 A and voltages up to 33 V. Analogue readings from the sensor were converted into a digital format using an 8 channel 17 bit analogue-to-digital converter (ADC).


The number of flights performed varying operational parameters (payload, altitude, speed) was 196. In addition, 13 recordings were done to assess the drone’s ancillary power and hover conditions."	https://paperswithcode.com/dataset/data-collected-with-package-delivery	27/07/2020						
4076	Tc1 Mouse cerebellum atlas	"This mouse cerebellar atlas can be used for  mouse cerebellar morphometry.
We recommend to use Multi Atlas Segmentation and Morphometric analysis toolkit (MASMAT) for mouse brain MRI along with other mouse brain atlases in this repo.
Reference/citation:

If you're using the this mouse MRI cerebellar atlas in your paper, we ask you to please kindly cite the following papers:
Ma, D., Cardoso, M. J., Zuluaga, M. A., Modat, M., Powell, N. M., Wiseman, F. K., Cleary, J. O., Sinclair, B., Harrison, I. F., Siow, B., Popuri, K., Lee, S., Matsubara, J. A., Sarunic, M. V, Beg, M. F., Tybulewicz, V. L. J., Fisher, E. M. C., Lythgoe, M. F., & Ourselin, S. (2020). Substantially thinner internal granular layer and reduced molecular layer surface in the cerebellum of the Tc1 mouse model of Down Syndrome – a comprehensive morphometric analysis with active staining contrast-enhanced MRI. NeuroImage, 117271. https://doi.org/https://doi.org/10.1016/j.neuroimage.2020.117271
Ma, D., Cardoso, M. J., Zuluaga, M. A., Modat, M., Powell, N., Wiseman, F., Tybulewicz, V., Fisher, E., Lythgoe, M. F., & Ourselin, S. (2015). Grey Matter Sublayer Thickness Estimation in the Mouse Cerebellum. In Medical Image Computing and Computer Assisted Intervention 2015 (pp. 644–651). https://doi.org/10.1007/978-3-319-24574-4_77"	https://paperswithcode.com/dataset/tc1-mouse-cerebellum-atlas	08/01/2019	Tc1 Mouse cerebellum atlas with Purkinje layer segmentation					
4077	Multi-template MRI mouse brain atlas	"Mouse Brain MRI atlas (both in-vivo and ex-vivo) (repository relocated from the original webpage)
List of atlases


FVB_NCrl: Brain MRI atlas of the wild-type FVB_NCrl mouse strain (used as the background strain for the  rTg4510 which is a tauopathy model mice express a repressible form of human tau containing the P301L mutation that has been linked with familial frontotemporal dementia.)


NeAt: Brain MRI atlas of the whld-type C57BL/6J mouse strain. Atlas was created based on the original MRM NeAt mouse brain atlas (template images reoriented and bias-corrected, left/right structure label seperated, and 4th ventricle manual segmentation added).


Tc1 Cerebellum: TC1 mouse cerebellar cortical sublayer lobules.This mouse cerebellar atlas can be used for mouse cerebellar morphometry.


Citation


If you use the segmented brain structure, or use the atlas along with the automatic mouse brain MRI segmentation tools, we ask you to kindly cite the following papers:


Ma D, Cardoso MJ, Modat M, Powell N, Wells J, Holmes H, Wiseman F, Tybulewicz V, Fisher E, Lythgoe MF, Ourselin S. Automatic structural parcellation of mouse brain MRI using multi-atlas label fusion. PloS one. 2014 Jan 27;9(1):e86576.
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086576


Ma D, Holmes HE, Cardoso MJ, Modat M, Harrison IF, Powell NM, O'Callaghan J, Ismail O, Johnson RA, O’Neill MJ, Collins EC, Mirza F. Beg, Karteek Popuri, Mark F. Lythgoe, and Sebastien Ourselin Study the longitudinal in vivo and cross-sectional ex vivo brain volume difference for disease progression and treatment effect on mouse model of tauopathy using automated MRI structural parcellation. Frontiers in Neuroscience. 2019;13:11.
https://www.frontiersin.org/articles/10.3389/fnins.2019.00011


If you use the brain MR images of the FVB_NCrl mouse strain (the wildtype background of rTg4510), we ask you to kindly cite the following papers: 


Wells JA, O'Callaghan JM, Holmes HE, Powell NM, Johnson RA, Siow B, Torrealdea F, Ismail O, Walker-Samuel S, Golay X, Rega M. In vivo imaging of tau pathology using multi-parametric quantitative MRI. Neuroimage. 2015 May 1;111:369-78.
https://www.sciencedirect.com/science/article/pii/S105381191500124X


Holmes HE, Colgan N, Ismail O, Ma D, Powell NM, O'Callaghan JM, Harrison IF, Johnson RA, Murray TK, Ahmed Z, Heggenes M. Imaging the accumulation and suppression of tau pathology using multiparametric MRI. Neurobiology of aging. 2016 Mar 1;39:184-94.
https://www.sciencedirect.com/science/article/pii/S0197458015006053


Holmes HE, Powell NM, Ma D, Ismail O, Harrison IF, Wells JA, Colgan N, O'Callaghan JM, Johnson RA, Murray TK, Ahmed Z. Comparison of in vivo and ex vivo MRI for the detection of structural abnormalities in a mouse model of tauopathy. Frontiers in neuroinformatics. 2017 Mar 31;11:20.
https://www.frontiersin.org/articles/10.3389/fninf.2017.00020/full


If you're using the mouse MRI T2* Active Starining Cerebellar atlas, we ask you to please kindly cite the following papers:

Ma, D., Cardoso, M. J., Zuluaga, M. A., Modat, M., Powell, N. M., Wiseman, F. K., Cleary, J. O., Sinclair, B., Harrison, I. F., Siow, B., Popuri, K., Lee, S., Matsubara, J. A., Sarunic, M. V, Beg, M. F., Tybulewicz, V. L. J., Fisher, E. M. C., Lythgoe, M. F., & Ourselin, S. (2020). Substantially thinner internal granular layer and reduced molecular layer surface in the cerebellum of the Tc1 mouse model of Down Syndrome – a comprehensive morphometric analysis with active staining contrast-enhanced MRI. NeuroImage, 117271. https://doi.org/https://doi.org/10.1016/j.neuroimage.2020.117271
Ma, D., Cardoso, M. J., Zuluaga, M. A., Modat, M., Powell, N., Wiseman, F., Tybulewicz, V., Fisher, E., Lythgoe, M. F., & Ourselin, S. (2015). Grey Matter Sublayer Thickness Estimation in the Mouse Cerebellum. In Medical Image Computing and Computer Assisted Intervention 2015 (pp. 644–651). https://doi.org/10.1007/978-3-319-24574-4_77

Reference

For the original information of the NeAt atlas, please please refer to the website: http://brainatlas.mbi.ufl.edu/, and the following two reference papers:
Ma Yu, Smith David, Hof Patrick R, Foerster Bernd, Hamilton Scott, Blackband Stephen J, Yu Mei, Benveniste Helene In Vivo 3D Digital Atlas Database of the Adult C57BL/6J Mouse Brain by Magnetic Resonance Microscopy. Front. Neuroanat. 2, 1 (2008).
Ma Yu,  Hof P R,  Grant S C,  Blackband S J,  Bennett R,  Slatest L,  McGuigan M D,  Benveniste H A three-dimensional digital atlas database of the adult C57BL/6J mouse brain by magnetic resonance microscopy. Neuroscience 135, 1203–15 (2005).

Funding
The works in this repositories received multiple funding from EPSRC, UCL Leonard Wolfson Experimental Neurology center, Medical Research Council (MRC), the NIHR Biomedical Research Unit (Dementia) at UCL and the National Institute for Health Research University College London Hospitals Biomedical Research center, the UK Regenerative Medicine Platform Safety Hub, and the Kings College London and UCL Comprehensive Cancer Imaging center CRUK & EPSRC in association with the MRC and DoH (England), UCL Faculty of Engineering funding scheme, Alzheimer Society Reseasrch Program from Alzheimer Society Canada, NSERC, CIHR, MSFHR Canada， Eli Lilly and Company, Wellcome Trust, the Francis Crick Institute, Cancer Research UK, and University of Melbourne McKenzie Fellowship."	https://paperswithcode.com/dataset/multi-template-mri-mouse-brain-atlas	01/07/2016	Multi-template MRI mouse brain atlas for both in vivo and ex vivo analysis					
4078	ESD	ESD is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies.	https://paperswithcode.com/dataset/esd	31/05/2021	Emotional Speech Database					
4079	D3DFACS	"The D3DFACS dataset is a dynamic 3D facial expression data set based on the Facial Action Coding System. It contains Action Unit (AU) sequences from 10 people, with 519 sequences in total. The peak image of each expression sequence has been manually FACS coded by a certified expert.
Registered meshes in FLAME mesh topology are available under https://flame.is.tue.mpg.de/downloads"	https://paperswithcode.com/dataset/d3dfacs		Dynamic 3D Facial Action Coding System Database					
4080	H01	"The H01 dataset is a 1.4 petabyte rendering of a small sample of human brain tissue, released by a collaboration between the Lichtman Laboratory at Harvard University and Google. The H01 sample was imaged at 4nm-resolution by serial section electron microscopy, reconstructed and annotated by automated computational techniques, and analyzed for preliminary insights into the structure of the human cortex. 
The dataset comprises imaging data that covers roughly one cubic millimeter of brain tissue, and includes tens of thousands of reconstructed neurons, millions of neuron fragments, 130 million annotated synapses, 104 proofread cells, and many additional subcellular annotations and structures. H01 is thus far the largest sample of brain tissue imaged and reconstructed in this level of detail, in any species, and the first large-scale study of synaptic connectivity in the human cortex that spans multiple cell types across all layers of the cortex. The primary goals of this project are to produce a novel resource for studying the human brain and to improve and scale the underlying connectomics technologies.
The dataset can be browsed online using the Neuroglancer browser interface.
Source: A Browsable Petascale Reconstruction of the Human Cortex
Image Source: A Browsable Petascale Reconstruction of the Human Cortex
Paper: A connectomic study of a petascale fragment of human cerebral cortex"	https://paperswithcode.com/dataset/h01	31/05/2021						
4081	DialogSum	"DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics.
This work is accepted by ACL findings 2021. You may find the paper here: https://arxiv.org/pdf/2105.06762.pdf.
If you want to use our dataset, please cite our paper. 
Dialogue Data
We collect dialogue data for DialogSum from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2019), as well as an English speaking practice website. 
These datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel.
Most conversations take place between friends, colleagues, and between service providers and customers.
Compared with previous datasets, dialogues from DialogSum have distinct characteristics: 
* Under rich real-life scenarios, including more diverse task-oriented scenarios;
* Have clear communication patterns and intents, which is valuable to serve as summarization sources;
* Have a reasonable length, which comforts the purpose of automatic summarization.
Summaries
We ask annotators to summarize each dialogue based on the following criteria:
* Convey the most salient information;
* Be brief;
* Preserve important named entities within the conversation;
* Be written from an observer perspective;
* Be written in formal language.
Topics
In addition to summaries, we also ask annotators to write a short topic for each dialogue, which can be potentially useful for future work, e.g. generating summaries by leveraging topic information.
Image source: https://arxiv.org/pdf/2105.06762.pdf"	https://paperswithcode.com/dataset/dialogsum	14/05/2021						
4082	Classic ECN AQM Fall-Back	"Clickable heat-map visualizations of the experiments run to quantify the Classic ECN AQM problem and to evaluate the success of the Classic AQM Detection and Fall-back algorithm.
Clicking through gives access to whisker-plot summary results, more detailed clickable heat-maps and time-series plots of all the variables in each experiment run."	https://paperswithcode.com/dataset/classic-ecn-aqm-fall-back	19/04/2020						
4083	iMet Collection	A dataset for fine-grained art attribute recognition introduced in the 6th FGVC Workshop at CVPR 2019. It is a high-quality artwork image dataset with professional photographs of artworks from The Metropolitan Museum of Art and attribute labels curated or verified by experts.	https://paperswithcode.com/dataset/imet-collection							
4084	FED	The FED dataset is constructed by annotating a set of human-system and human-human conversations with eighteen fine-grained dialog qualities.	https://paperswithcode.com/dataset/fed	23/06/2020	Fine-grained Evaluation of Dialog					
4085	Com2Sense	Complementary Commonsense (Com2Sense) is a dataset for benchmarking commonsense reasoning ability of NLP models. This dataset contains 4k statement true/false sentence pairs. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, the dataset is designed along the dimensions of knowledge domains, reasoning scenarios and numeracy.	https://paperswithcode.com/dataset/com2sense	02/06/2021	Complementary Commonsense					
4086	Semi-iNat	"Semi-iNat is a challenging dataset for semi-supervised classification with a long-tailed distribution of classes, fine-grained categories, and domain shifts between labeled and unlabeled data. The data is obtained from iNaturalist, a community driven project aimed at collecting observations of biodiversity. 
The dataset comes with standard training, validation and test sets. The training set consists of:


labeled images from 810 species, where around 10% of the images are labeled.


unlabeled images contains unlabeled images from the same set of classes as the labeled images (in-class), plus the images from a different set of classes as the labeled set (out-of-class). The species are guaranteed to have species at the same phylum level in the labels set. This reflects a common scenario where a coarser taxonomic label of an image can be easily obtained."	https://paperswithcode.com/dataset/semi-inat	02/06/2021	Semi-Supervised iNaturalist					
4087	FacetSum	FacetSum is a faceted summarization dataset for scientific documents. FacetSum has been built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value.	https://paperswithcode.com/dataset/facetsum	31/05/2021						
4088	ClueWeb09	The ClueWeb09 dataset was created to support research on information retrieval and related human language technologies. It consists of about 1 billion web pages in ten languages that were collected in January and February 2009. The dataset is used by several tracks of the TREC conference.	https://paperswithcode.com/dataset/clueweb09							
4089	TRECDD	"The dataset used for TREC 2017 Dynamic Domain Track consists of two domains: Ebola and New York Times.
1.1 Ebola
The Ebola dataset is crawled by Juliana Friere (NYU, juliana dot freire at nyu dot edu), Kien Pham(NYU), Peter Landwehr (Giant Oak, peter dot landwehr at giantoak dot com) and Lewis McGibbney (JPL, Lewis dot J dot Mcgibbney at jpl dor nasa dot gov).
The Ebola dataset contains records related to the Ebola outbreak in Africa in 2014-2015. The original dataset includes tweets relating to the outbreak, web pages from sites hosted in the affected countries as well as PDF documents from websites such as World Health Organization, Financial Tracking Service and The World Bank. Such information resources are designed to provide information to citizens and aid workers on the ground.
1.2 New York Times
The New York Times dataset is published by Evan Sandhaus in 2008 under LDC Catalog No. LDC2008T19.
The New York Times dataset consists of articles published in New York Times from January 1, 1987 to June 19, 2007 with metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. Most articles are manually summarized and tagged by professional staffs. The original form of this dataset is in News Industry Text Format (NITF). This dataset can aid the research in Document Categorization, Information Retrieval, Entity Extraction and etc."	https://paperswithcode.com/dataset/trecdd		TREC Dynamic Domain					
4090	BugClassify	"Dataset of 5,591 labeled issue tickets. Originally created by Herzig et al. in : ""It’s Not a Bug, It’s a Feature: How Misclassification Impacts Bug Prediction"" (paper)"	https://paperswithcode.com/dataset/bugclassify							
4091	OntoGUM	OntoGUM is an OntoNotes-like coreference dataset converted from GUM, an English corpus covering 12 genres using deterministic rules.	https://paperswithcode.com/dataset/ontogum	02/06/2021						
4092	ConvoSumm	ConvoSumm is a suite of four datasets to evaluate a model’s performance on a broad spectrum of conversation data.	https://paperswithcode.com/dataset/convosumm	01/06/2021						
4093	EQA	"The EQA (Embodied Question Answering) dataset is a dataset of visual questions and answers grounded in House3D. For this dataset an agent is spawned at a random location in a 3D environment and asked a question (for e.g. ""What color is the car?""). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person vision, and then answer the question (""orange"")."	https://paperswithcode.com/dataset/eqa	30/11/2017	Embodied Question Answering					
4094	Everybody Dance Now	Everybody Dance Now is a dataset of videos that can be used for training and motion transfer. It contains long single-dancer videos that can be used to train and evaluate the model. All subjects have consented to allowing the data to be used for research purposes.	https://paperswithcode.com/dataset/everybody-dance-now	22/08/2018						
4095	PDE dataset	"Contains data of parametric PDEs

Burgers' equation
Darcy's flow
Navier-Stokes equation"	https://paperswithcode.com/dataset/pde-dataset		Parametric Partial Differential Equation dataset					
4096	TaL Corpus	"The Tongue and Lips (TaL) corpus is a multi-speaker corpus of ultrasound images of the tongue and video images of lips. This corpus contains synchronised imaging data of extraoral (lips) and intraoral (tongue) articulators from 82 native speakers of English.
The TaL corpus consists of two datasets:


TaL1 is a single-speaker dataset containing data of one professional voice talent, a male native speaker of English, over six recording sessions.


TaL80 is a multi-speaker dataset contains recording sessions of 81 native speakers of English without voice talent experience. Each speaker was recording over a single recording session.


Image source: https://ultrasuite.github.io/data/tal_corpus/"	https://paperswithcode.com/dataset/tal-corpus		The Tongue and Lips Corpus					
4097	MRS	MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks.	https://paperswithcode.com/dataset/mrs	03/06/2021	Multilingual Reply Suggestion					
4098	CCPM	"Introduction
CCPM is a large Chinese classical poetry matching dataset that can be used for poetry matching, understanding and translation.
The main task of this dataset is: given a description in modern Chinese, the model is supposed to select one line of Chinese classical poetry from four candidates that semantically match the given description most.
Size
It contains 27,218 instances in total, which are split into training (21,778), validation (2,720) and test (2,720) sets.
Format
Each instance is composed of translation (the description in modern Chinese, a string), choice (four candidate lines of Chinese classical poetry, a list) and answer (the index of the correct line, an integer between 0 and 3).
Source: https://github.com/THUNLP-AIPoet/CCPM"	https://paperswithcode.com/dataset/ccpm	03/06/2021	Chinese Classical Poetry Matching					
4099	The 'Call me sexist but' Dataset (CMSB)	"Tweets and items from psychological scales for sexism detection with counterfactual examples.
This dataset consists of three types of 'short-text' content:

social media posts (tweets)
psychological survey items, and
synthetic adversarial modifications of the former two categories.

The tweet data can be further divided into 3 separate datasets based on their source:
1.1 the hostile sexism dataset,
1.2 the benevolent sexism dataset, and
1.3 the callme sexism dataset.
1.1 and 1.2 are pre-existing datasets obtained from Waseem, Z., & Hovy, D. (2016) and Jha, A., & Mamidi, R. (2017) that we re-annotated (see our paper and data statement for further information). The rationale for including these dataset specifically is that they feature a variety of sexist expressions in real conversational (social media) settings. In particular, they feature expressions that range from overtly antagonizing the minority gender through negative stereotypes (1.1) to leveraging positive stereotypes to subtly dismiss it as less-capable and fragile (1.2).
The callme sexism dataset (1.3) was collected by us based on the presence of the phrase 'call me sexist but' in tweets. The rationale behind this choice of query was that several Twitter users opine potentially sexist comments and signal so using the presence of this phrase, which arguably serves as a disclaimer for sexist opinions.
The survey items (2) pertain to attitudinal surveys that are designed to measure sexist attitudes and gender bias in participants. We provide a detailed account of our selection procedure in our paper.
Finally, the adversarial examples are generated by crowdworkers from Amazon Mechanical Turk by making minimal changes to tweets and scale items, in order to change sexist examples to non-sexist ones. We hope that these examples will help us control for typical confounds in non-sexist data (e.g., topic, civility) and lead to datasets with fewer biases, and consequently allow us to train more robust machine learning models. We only asked to turn sexist examples into non-sexist ones, and not vice versa, for ethical reasons.
The dataset is annotated to capture cases where text is sexist because of its content (what the speaker believes) or its phrasing (the speaker's choice of words). We explain the rationale for this codebook in our paper."	https://paperswithcode.com/dataset/the-call-me-sexist-but-dataset-cmsb	06/02/2021						
4100	ambivalent sexism	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/ambivalent-sexism							
4101	Webly-Reference SR Dataset	"Webly-Reference SR dataset is a test dataset for evaluating Ref-SR methods. It has the following advantages:

Collected in a more realistic way: For every input image, its reference image is searched using Google Image.
More diverse than previous datasets."	https://paperswithcode.com/dataset/webly-reference-sr-dataset	03/06/2021						
4102	CPNet	CPNet dataset has a collection of 25 categories, 2,334 models based on ShapeNetCore, which includes 1,000+ correspondence sets with 104,861 points.	https://paperswithcode.com/dataset/cpnet	29/12/2019	CorresPondenceNet					
4103	FLORES-101	"The FLORES evaluation benchmark consists of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.
Paper: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"	https://paperswithcode.com/dataset/flores-101	06/06/2021						
4104	Enoch Oluwumi	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/enoch-oluwumi		Enoch Okikijesu Oluwumi					
4105	Visuomotor affordance learning (VAL) robot interaction dataset	"This data contains about 2500 trajectories (with images and actions) of a Sawyer robot interacting with various objects.
Examples from the dataset are shown in the adjacent video. We provide two versions of the VAL dataset - one with low-res images (1.4 GB) and one with high-res images (162 GB). The data quantity and format is the same between these two versions; the difference is only the image observation quality.
The smaller dataset, with 48x48x3 images which can be used for eg. offline RL, is available for direct download: https://drive.google.com/file/d/1UuWANkVtWLg4egIK2LB_YCKuF87rMQ1H/view?usp=sharing
The larger dataset, with 480x640x3 which might be preferred for eg. representation learning, is available at this Google drive folder:  https://drive.google.com/drive/folders/1kD9kyP7-RlIrSnuN7rpEASAGWp5qnNov?usp=sharing
To download the larger dataset, we suggest using https://rclone.org/
The data is sorted into several folders. There are a total of 300 files and 2500 trajectories.
- fixed_drawer - Human-controlled demonstration data opening and closing drawers. (~10%)
- fixed_pnp - Human-controlled demonstration data picking up objects. (~10%)
- fixed_pot - Human-controlled demonstration data interacting with a pot and a lid. (~10%)
- fixed_tray - Human-controlled demonstration data picking up objects and placing it in a tray. (~10%)
- general - Further human-controlled demonstration data collected with the most diversity and variation. (~40%)
- onpolicy_eval - Evaluation data collected by an RL policy. (~10%)
- onpolicy_expl - Exploration data collected by an RL policy. (~10%)"	https://paperswithcode.com/dataset/visuomotor-affordance-learning-val-robot	01/06/2021						
4106	XFUND	XFUND is a multilingual form understanding benchmark dataset that includes human-labeled forms with key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese).	https://paperswithcode.com/dataset/xfun	18/04/2021	A Multilingual Form Understanding Benchmark					
4107	NTIRE 2021 HDR	The NTIRE 2021 HDR was built for the first challenge on high-dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2021. The challenge aims at estimating a HDR image from one or multiple respective low-dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed by two tracks: In Track 1 only a single LDR image is provided as input, whereas in Track 2 three differently-exposed LDR images with inter-frame motion are available. In both tracks, the ultimate goal is to achieve the best objective HDR reconstruction in terms of PSNR with respect to a ground-truth image, evaluated both directly and with a canonical tone mapping operation.	https://paperswithcode.com/dataset/ntire-2021-hdr	02/06/2021						
4108	BAAI-VANJEE	BAAI-VANJEE is a dataset for benchmarking and training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion. The BAAI-VANJEE roadside dataset consists of LiDAR data and RGB images collected by VANJEE smart base station placed on the roadside about 4.5m high. This dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images, including 20% collected at the same time. It also contains 12 classes of objects, 74K 3D object annotations and 105K 2D object annotations.	https://paperswithcode.com/dataset/baai-vanjee	29/05/2021						
4109	PS5k	We introduce a new data set containing 5000 scientific papers and their slides crawled from conference proceeding websites such as aclweb and usenix.	https://paperswithcode.com/dataset/ps5k	25/08/2020	presentation-slide-pairs					
4110	SemEval-2013 Task 2	The SemEval-2013 Task 2 dataset contains data for two subtasks: A, an expression-level subtask, and B, a message-level subtask. Crowdsourcing was used to label a large Twitter training dataset along with additional test sets of Twitter and SMS messages for both subtasks.	https://paperswithcode.com/dataset/semeval-2013-task-2	14/12/2019						
4111	MLQuestions	MLQuestions is a domain-adaptation dataset for the machine learning domain containing 50K unaligned passages and 35K unaligned questions, and 3K aligned passage and question pairs.	https://paperswithcode.com/dataset/mlquestions	18/04/2021						
4112	iWildCam 2021	iWildCam 2021 is a dataset for counting the number of animals of each species that appear in sequences of images captured with camera traps. The training data and test data are from different cameras spread across the globe. The set of species seen in each camera overlap but are not identical. The challenge is to categorize species and count the number of individuals across image bursts.	https://paperswithcode.com/dataset/iwildcam-2021	07/05/2021						
4113	NeoRL	"NeoRL is a collection of environments and datasets for offline reinforcement learning with a special focus on real-world applications. The design follows real-world properties like the conservative of behavior policies, limited amounts of data, high-dimensional state and action spaces, and the highly stochastic nature of the environments.
The datasets include robotics, industrial control, finance trading and city management tasks with real-world properties, containing three-level sizes of dataset, three-level quality of data to mimic the dataset we will meet in offline RL scenarios. 
Users can use the dataset to evaluate offline RL algorithms with near real-world application nature."	https://paperswithcode.com/dataset/neorl	01/02/2021						
4114	ARC Ukiyo-e Faces	ARC Ukiyo-e Faces is a large-scale (>10k paintings, >20k faces) Ukiyo-e dataset with coherent semantic labels and geometric annotations through augmenting and organizing existing datasets with automatic detection.	https://paperswithcode.com/dataset/arc-ukiyo-e-faces	04/06/2021						
4115	IBims-1	"iBims-1 (independent Benchmark images and matched scans - version 1) is a new high-quality RGB-D dataset, especially designed for testing single-image depth estimation (SIDE) methods. A customized acquisition setup, composed of a digital single-lens reflex (DSLR) camera and a high-precision laser scanner was used to acquire high-resolution images and highly accurate depth maps of diverse indoors scenarios.
Compared to related RGB-D datasets, iBims-1 stands out due to a very low noise level, sharp depth transitions, no occlusions, and high depth ranges.
Our dataset consists of the following components:
Core dataset:

100 RGB-D image pairs of various indoor scenes in high- and low resolution
Masks for invalid, transparent and planar regions (tables, floors, walls)
Masks for distinct depth transitions
Camera calibration parameters

Auxiliary dataset:
- 56 different color and geometric augmentations for each image of the core dataset
- Additional hand-held images for testing MVS methods
- Images of printed patterns and photos posted on a wall to assess performance of textured planar surfaces
- Several RGB-D image sequences of static scenes with varying illumation
Source: Evaluation of CNN-based Single-Image Depth Estimation Methods
Image source: https://arxiv.org/pdf/1805.01328v1.pdf"	https://paperswithcode.com/dataset/ibims-1	03/05/2018	Independent benchmark images and matched scans v1					
4116	UIT-ViSFD	UIT-ViSFD is a Vietnamese Smartphone Feedback Dataset as a new benchmark corpus built based on strict annotation schemes for evaluating aspect-based sentiment analysis, consisting of 11,122 human-annotated comments for mobile e-commerce, which is freely available for research purposes.	https://paperswithcode.com/dataset/uit-visfd	31/05/2021	Vietnamese Aspect-Based Sentiment Analysis Dataset					
4117	ToyADMOS2	ToyADMOS2 is a dataset of miniature-machine operating sounds for anomalous sound detection under domain shift conditions.	https://paperswithcode.com/dataset/toyadmos2	04/06/2021						
4118	Colored MNIST	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/colored-mnist							
4119	Stanford Schema2QA Dataset	"Schema2QA is the first large question answering dataset over real-world Schema.org data. It covers 6 common domains: restaurants, hotels, people, movies, books, and music, based on crawled Schema.org metadata from 6 different websites (Yelp, Hyatt, LinkedIn, IMDb, Goodreads, and last.fm.). In total, there are over 2,000,000 examples for training, consisting of both augmented human paraphrase data and high-quality synthetic data generated by Genie. All questions are annotated with executable virtual assistant programming language ThingTalk.
Schema2QA includes challenging evaluation questions collected from crowd workers. Workers are prompted with only what the domain is and what properties are supported. Thus, the sentences are natural and diverse. They also contain entities unseen during training. The collected sentences are manually annotated with ThingTalk by the authors. In total there are over 5,000 examples for dev and test.
An example of an evaluation question and its ThingTalk annotation is shown below:
""What are the highest ranked burger joints in the 40 mile area around Asheville NC?""
sort(aggregateRating.ratingValue desc of @org.schema.Restaurant.Restaurant() 
  filter distance(geo, new Location(""asheville nc"" )) &lt;= 40 mi &amp;&amp; 
         servesCuisine =~ ""burger"")[1] ;"	https://paperswithcode.com/dataset/schema2qa	19/10/2020						
4120	CBC	The complete blood count (CBC) dataset contains 360 blood smear images along with their annotation files splitting into Training, Testing, and Validation sets. The training folder contains 300 images with annotations. The testing and validation folder both contain 60 images with annotations. We have done some modifications over the original dataset to prepare this CBC dataset where some of the image annotation files contain very low red blood cells (RBCs) than actual and one annotation file does not include any RBC at all although the cell smear image contains RBCs. So, we clear up all the fallacious files and split the dataset into three parts. Among the 360 smear images, 300 blood cell images with annotations are used as the training set first, and then the rest of the 60 images with annotations are used as the testing set. Due to the shortage of data, a subset of the training set is used to prepare the validation set which contains 60 images with annotations.	https://paperswithcode.com/dataset/complete-blood-count-cbc-dataset	17/07/2019	Complete Blood Count					
4121	Quo Vadis, Open Source?	"This is an complete set of the data we collected and analyzed in our study ""Quo Vadis, Open Source? The Limits of Open Source Growth"". Please see our GitHub repository for details and tool chain."	https://paperswithcode.com/dataset/quo-vadis-open-source	18/08/2020	Quo Vadis, Open Source? The Limits of Open Source Growth					
4122	X4K1000FPS	"Dataset of high-resolution (4096×2160), high-fps (1000fps) video frames with extreme motion.
X-TEST consists of 15 video clips with 33-length of 4K-1000fps frames. 
X-TRAIN consists of 4,408 clips from various types of 110 scenes. The clips are 65-length of 1000fps frames
Source: XVFI: eXtreme Video Frame Interpolation
Image Source: GitHub"	https://paperswithcode.com/dataset/x4k1000fps	30/03/2021						
4123	Webis-ConcluGen-21	Webis-ConcluGen-21 is a large-scale corpus of 136,996 samples of argumentative texts and their conclusions used for the task of generating informative conclusions.	https://paperswithcode.com/dataset/webis-conclugen-21	02/06/2021						
4124	CTFW	CTFW is a large annotated procedural text dataset in the cybersecurity domain (3154 documents). It is used to generate flow graphs from procedural texts.	https://paperswithcode.com/dataset/ctfw	29/05/2021						
4125	Herbarium 2021 Half–Earth	"The Herbarium Half-Earth dataset is a large and diverse dataset of herbarium specimens to date for automatic taxon recognition. The Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list.
This dataset has a long tail; there are a minimum of 3 images per species. However, some species can be represented by more than 100 images. This dataset only includes vascular land plants which include lycophytes, ferns, gymnosperms, and flowering plants. The extinct forms of lycophytes are the major component of coal deposits, ferns are indicators of ecosystem health, gymnosperms provide major habitats for animals, and flowering plants provide almost all of our crops, vegetables, and fruits."	https://paperswithcode.com/dataset/herbarium-2021-half-earth	28/05/2021						
4126	Dark Machines Anomaly Score	"This dataset is the outcome of a data challenge conducted as part of the Dark Machines Initiative and the Les Houches 2019 workshop on Physics at TeV colliders. The challenge aims at detecting signals of new physics at the LHC using unsupervised machine learning algorithms. 
It consists on a large benchmark dataset, consisting of >1 Billion simulated LHC events corresponding to 10 fb−1 of proton-proton collisions at a center-of-mass energy of 13 TeV."	https://paperswithcode.com/dataset/dark-machines-anomaly-score	28/05/2021						
4127	Onmiglot		https://paperswithcode.com/dataset/onmiglot							
4128	KNNIST		https://paperswithcode.com/dataset/knnist							
4129	KMNIST		https://paperswithcode.com/dataset/kmnist							
4130	notMNIST		https://paperswithcode.com/dataset/notmnist	08/09/2011						
4131	PROST	The PROST (Physical Reasoning about Objects Through Space and Time) dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting.	https://paperswithcode.com/dataset/prost	07/06/2021	Physical Reasoning about Objects Through Space and Time					
4132	COVID-Fact	COVID-Fact is a FEVER-like dataset of  claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence.	https://paperswithcode.com/dataset/covid-fact	07/06/2021						
4133	AppleScabLDs	"Dataset contains images with apple leaves infected by scab. The images are grouped in two folders: ""Healthy"" and ""Scab"". The collection of digital images were carried out in different locations of Latvia. Digital images with characteristic scab symptoms on leaves were collected by the Institute of Horticulture (LatHort) under project ""lzp-2019/1-0094 Application of deep learning and datamining for the study of plant-pathogen interaction: the case of apple and pear scab"" with a goal to create mobile application for apple scab detection using convolution neural networks. Devices: smartphone cameras (12 MP, 13 MP, 48 MP) and a digital compact camera (10 MP). The collection of images was carried out in field conditions, in orchards. The images were taken at three different stages of the day - in the morning (9:00-10:00), around noon (12:00-14:00), as well as in the evening (16:00-17:00) to provide a variety of natural light conditions. The images were also taken on both sunny days and overcast days to provide different types of light (soft light and hard light). The leaves were framed so that they occupied the image area as much as possible and were in the center of the image, and the focal point was on the object. The object may have had other leaves or fruits in the background. The same object was photographed from multiple viewpoints.
Source:  https://www.kaggle.com/projectlzp201910094/applescablds
Introduced by:  S. Kodors, G. Lacis, O. Sokolova, V. Zhukovs, I. Apeinans and T. Bartulsons. 2021. Apple Scab Detection using CNN and Transfer Learning. Agronomy Research, 19(2), 507–519. doi: 10.15159/AR.21.045"	https://paperswithcode.com/dataset/applescablds	22/04/2021						
4134	AppleScabFDs	"Dataset contains images with apples infected by scab. The images are grouped in two folders: ""Healthy"" and ""Scab"". The collection of digital images were carried out in different locations of Latvia. Digital images with characteristic scab symptoms on fruits were collected by the Institute of Horticulture (LatHort) under project ""lzp-2019/1-0094 Application of deep learning and datamining for the study of plant-pathogen interaction: the case of apple and pear scab"" with a goal to create mobile application for apple scab detection using convolution neural networks. Devices: smartphone cameras (12 MP, 13 MP, 48 MP) and a digital compact camera (10 MP). The collection of images was carried out in field conditions, in orchards. The images were taken at three different stages of the day - in the morning (9:00-10:00), around noon (12:00-14:00), as well as in the evening (16:00-17:00) to provide a variety of natural light conditions. The images were also taken on both sunny days and overcast days to provide different types of light (soft light and hard light). The leaves were framed so that they occupied the image area as much as possible and were in the center of the image, and the focal point was on the object. The object may have had other leaves or fruits in the background. The same object was photographed from multiple viewpoints.
Source:  https://www.kaggle.com/projectlzp201910094/applescablds
Introduced by:  S. Kodors, G. Lacis, O. Sokolova, V. Zhukovs, I. Apeinans and T. Bartulsons. 2021. Apple Scab Detection using CNN and Transfer Learning. Agronomy Research, 19(2), 507–519. doi: 10.15159/AR.21.045"	https://paperswithcode.com/dataset/applescabfds	22/04/2021						
4135	LSEC	The LSEC (Live Stream E-Commerce) dataset has two subsets: LSEC-Small and LSEC-Large. It is a dataset for studying E-commerce transactions in the context of live streams, where the streames are talking about products while interacting with their audience. The dataset consists of interaction information among streamers, users, and products.	https://paperswithcode.com/dataset/lsec	07/06/2021	Live Stream E-Commerce					
4136	BiToD	BiToD is a bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual ToD systems and cross-lingual transfer learning approaches.	https://paperswithcode.com/dataset/bitod	05/06/2021						
4137	CoSQA	CoSQA (Code Search and Question Answering) It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators.	https://paperswithcode.com/dataset/cosqa	27/05/2021	Code Search and Question Answering					
4138	TikTok Dataset	"We learn high fidelity human depths by leveraging a collection of social media dance videos scraped from the TikTok mobile social networking application. It is by far one of the most popular video sharing applications across generations, which include short videos (10-15 seconds) of diverse dance challenges as shown above. We manually find more than 300 dance videos that capture a single person performing dance moves from TikTok dance challenge compilations for each month, variety, type of dances, which are moderate movements that do not generate excessive motion blur.   For each video, we extract RGB images at 30 frame per second, resulting in more than 100K images. We segmented these images using Removebg application, and computed the UV coordinates from DensePose.  
Download TikTok Dataset:


Please use the dataset only for the research purpose.


The dataset can be viewed and downloaded from the Kaggle page. (you need to make an account in Kaggle to be able to download the data. It is free!)


The dataset can also be downloaded from here (42 GB). The dataset resolution is: (1080 x 604)


The original YouTube videos corresponding to each sequence and the dance name can be downloaded from here (2.6 GB)."	https://paperswithcode.com/dataset/tiktok-dataset	04/03/2021	Learning High Fidelity Depths of Dressed Humans  by Watching Social Media Dance Videos					
4139	Unsplash2K	Unsplash2K is high-resolution image dataset with 2K resolution. Unsplash2K dataset is crawled from unsplash. Unsplash2K dataset contains 498 high-resolution images and corresponding low-resolution images which are downsampled by bicubic downsamling for x2, x4, x8 scale. Unsplash2K contains diverse contents such as animals, architectures and flowers.	https://paperswithcode.com/dataset/unsplash2k	06/06/2021						
4140	DIPS-Plus	How and where proteins interface with one another can ultimately impact the proteins' functions along with a range of other biological processes. As such, precise computational methods for protein interface prediction (PIP) come highly sought after as they could yield significant advances in drug discovery and design as well as protein function analysis. However, the traditional benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a paltry 230 complexes for training, validating, and testing different machine learning algorithms. In this work, we expand on a dataset recently introduced for this task, the Database of Interacting Protein Structures (DIPS), to present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for geometric deep learning of protein interfaces. The previous version of DIPS contains only the Cartesian coordinates and types of the atoms comprising a given protein complex, whereas DIPS-Plus now includes a plethora of new residue-level features including protrusion indices, half-sphere amino acid compositions, and new profile hidden Markov model (HMM)-based sequence features for each amino acid, giving researchers a large, well-curated feature bank for training protein interface prediction methods.	https://paperswithcode.com/dataset/dips-plus	07/06/2021	The Enhanced Database of Interacting Protein Structures for Interface Prediction					
4141	Disfl-QA	"Disfl-QA is a targeted dataset for contextual disfluencies in an information seeking setting, namely question answering over Wikipedia passages. Disfl-QA builds upon the SQuAD-v2 dataset, where each question in the dev set is annotated to add a contextual disfluency using the paragraph as a source of distractors.
The final dataset consists of ~12k (disfluent question, answer) pairs. Over 90% of the disfluencies are corrections or restarts, making it a much harder test set for disfluency correction. Disfl-QA aims to fill a major gap between speech and NLP research community. We hope the dataset can serve as a benchmark dataset for testing robustness of models against disfluent inputs."	https://paperswithcode.com/dataset/disfl-qa	08/06/2021						
4142	TimeDial	"TimeDial presents a crowdsourced English challenge set, for temporal commonsense reasoning, formulated as a multiple choice cloze task with around 1.5k carefully curated dialogs. The dataset is derived from the DailyDialog, which is a multi-turn dialog corpus.
TimeDial dataset consists of 1,104 dialog instances with 2 correct and 2 incorrect options with the following statistics:"	https://paperswithcode.com/dataset/timedial	08/06/2021						
4143	Rel3D	"Understanding spatial relations (e.g., “laptop on table”) in visual input is important
for both humans and robots. Existing datasets are insufficient as they lack largescale, high-quality 3D ground truth information, which is critical for learning spatial
relations. In this paper, we fill this gap by constructing Rel3D: the first large-scale,
human-annotated dataset for grounding spatial relations in 3D. Rel3D enables
quantifying the effectiveness of 3D information in predicting spatial relations
on large-scale human data. Moreover, we propose minimally contrastive data
collection—a novel crowdsourcing method for reducing dataset bias. The 3D
scenes in our dataset come in minimally contrastive pairs: two scenes in a pair
are almost identical, but a spatial relation holds in one and fails in the other. We
empirically validate that minimally contrastive examples can diagnose issues with
current relation detection models as well as lead to sample-efficient training. Code
and data are available at https://github.com/princeton-vl/Rel3D."	https://paperswithcode.com/dataset/rel3d	03/12/2020						
4144	Topo-boundary	"Topo-boundary is a new benchmark dataset, named \textit{Topo-boundary}, for off-line topological road-boundary detection. The dataset contains 21,556 1000 X 1000-sized 4-channel aerial images. Each image is provided with 8 training labels for different sub-tasks.
Image source: https://github.com/TonyXuQAQ/Topo-boundary"	https://paperswithcode.com/dataset/topo-boundary	31/03/2021						
4145	Swords	Swords (Standford Word Substitution) is a benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. Swords is composed of context, target word, and substitute triples (c, w, w'), each of which has a score that indicates the appropriateness of the substitute.	https://paperswithcode.com/dataset/swords	08/06/2021	Stanford Word Substitution benchmark					
4146	SPANet $t\bar{t}$	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/spanet-t-bar-t	07/06/2021						
4147	Emol news articles and comments	The dataset provides News articles obtained from emol.cl including their content, title and all the comments it received in JSON format	https://paperswithcode.com/dataset/emol-news-articles-and-comments	07/06/2021						
4148	JFT-3B	JFT-3B is an internal Google dataset and a larger version of the JFT-300M dataset. It consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic pipeline. In other words, the data and associated labels are noisy.	https://paperswithcode.com/dataset/jft-3b	08/06/2021	JFT-3B					
4149	VOID	"The dataset was collected using the Intel RealSense D435i camera, which was configured to produce synchronized accelerometer and gyroscope measurements at 400 Hz, along with synchronized VGA-size (640 x 480) RGB and depth streams at 30 Hz. The depth frames are acquired using active stereo and is aligned to the RGB frame using the sensor factory calibration. All the measurements are timestamped.
The dataset contains 56 sequences in total, both indoor and outdoor with challenging motion. Typical scenes include classrooms, offices, stairwells, laboratories, and gardens. Of the 56 sequences, 48 sequences (approximately 47K frames) are designated for training and 8 sequences for testing, from which we sampled 800 frames to construct the testing set. Each sequence constains sparse depth maps at three density levels, 1500, 500 and 150 points, corresponding to 0.5%, 0.15% and 0.05% of VGA size."	https://paperswithcode.com/dataset/void	15/05/2019	Visual Odometry with Inertial and Depth					
4150	FastZIP Data	"Structure of code/data folders and how to use them
fastzip-code

Contains codebase to generate results in fastzip-results folder
Individual notebooks contain comments on their functionality/how to use them
FastZIP-Resample.ipynb (optional)
Resamples the collected sensor data to desired sampling rates (eliminates the effect of sampling rate instability/drift)
Input: fastzip-data/exp-X/raw
Output: fastzip-data/exp-X/adv and fastzip-data/exp-X/non-adv


FastZIP-Process.ipynb
Computes error rates for adversarial and benign devices in different configurations, generates binary fingerprints (see comments inside the notebook)
Input: fastzip-data/exp-X/adv and fastzip-data/exp-X/non-adv
Output:  fastzip-results/logs and fastzip-results/fps


FastZIP-Results.ipynb
Parses and caches results generated by FastZIP-Process.ipynb notebook to be used for plotting and data analysis
Input: fastzip-results/logs and fastzip-results/fps
Output: fastzip-results/cache


The system paths that are used by all notebooks are set in fastzip-code/const/globconst.py --> adjust them before running the code!
Uses folder fastzip-data as input
Uses folder fastzip-results as output
The above notebooks were run using Python 3.6.5 on Ubuntu 18.04.5 LTS bionic (x64)
The list of python packages with versions installed on the test machine is in fastzip-code/python3-packages.txt

fastzip-data

Contains sensor data collected from multiple devices in running/stationary cars in various conditions (e.g., in a city, etc.) from three experiments: exp-3, exp-4, and exp-5
Each exp-X folder has the same structure:
adv - sensor data collected when two cars drive one after another 
non-adv - sensor when two cars drive the same route but not one after another 
In adv and non-adv the sensor data is resampled (see description of FastZIP-Resample.ipynb )
raw - sensor data collected in the experiment before resampling or split into adv and non-adv



fastzip-min_entropy

Contains input and evaluation results for min-entropy estimation of our generated fingerprints using  NIST SP 800 90B tests
See README inside the folder for more detail

fastzip-results

Results in json or json.gz formats generated by FastZIP-Process.ipynb and FastZIP-Results.ipynb
See comments in these notebooks for the type of results stored in folders inside fastzip-results

fpake

Contains the implementation of the fPAKE protocol as well as input and output for its benchmarking
See README inside the folder for more detail"	https://paperswithcode.com/dataset/fastzip-data	09/06/2021	FastZIP Dataset and Code					
4151	TESTIMAGES	"A collection of photographic and synthetic images intended for analysis of image processing techniques and quality assessment of displays.
Image source: https://testimages.org/"	https://paperswithcode.com/dataset/testimages	22/09/2014						
4152	SEDE	SEDE is a dataset comprised of 12,023 complex and diverse SQL queries and their natural language titles and descriptions, written by real users of the Stack Exchange Data Explorer out of a natural interaction. These pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset. The goal of this dataset is to take a significant step towards evaluation of Text-to-SQL models in a real-world setting. Compared to other Text-to-SQL datasets, SEDE contains at least 10 times more SQL queries templates (queries after canonization and anonymization of values) than other datasets, and has the most diverse set of utterances and SQL queries (in terms of 3-grams) out of all single-domain datasets. SEDE introduces real-world challenges, such as under-specification, usage of parameters in queries, dates manipulation and more.	https://paperswithcode.com/dataset/sede	09/06/2021	Stack Exchange Data Explorer					
4153	A First Look Into Blockchain Overlays	"Click to add a brief desCcription of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/a-first-look-into-blockchain-overlays	07/04/2021						
4154	CoNaLa	"The CMU CoNaLa, the Code/Natural Language Challenge dataset is a joint project from the Carnegie Mellon University NeuLab and Strudel labs. Its purpose is for testing the generation of code snippets from natural language. The data comes from StackOverflow questions. There are 2379 training and 500 test examples that were manually annotated. Every example has a natural language intent and its corresponding python snippet.  In addition to the manually annotated dataset, there are also 598,237 mined intent-snippet pairs. These examples are similar to the hand-annotated ones except that they contain a probability if the pair is valid.
Source: CoNaLa dataset Homepage"	https://paperswithcode.com/dataset/conala	23/05/2018	CMU CoNaLa, the Code/Natural Language Challenge					
4155	SIPaKMeD	"a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/sipakmed		SIPaKMeD Pap Smear dataset					
4156	CoNaLa-Ext	"The CoNaLa Extended With Question Text is an extension to the original CoNaLa Dataset (Papers With Code Link) proposed in the NLP4Prog workshop paper ""Reading StackOverflow Encourages Cheating: Adding Question Text
Improves Extractive Code Generation"". The key additions are that every example now has the full question body from its respective StackOverflow Question.
IMPORTANT If you use this dataset, you MUST cite the original CoNaLa dataset paper.
Source: CoNaLa-Ext Homepage"	https://paperswithcode.com/dataset/conala-ext	08/06/2021	CoNaLa Extended With Question Text					
4157	VALUE	"VALUE is a Video-And-Language Understanding Evaluation benchmark to test models that are generalizable to diverse tasks, domains, and datasets. It is an assemblage of 11 VidL (video-and-language) datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. 
The datasets used for the VALUE benchmark are: TVQA, TVR, TVC, How2R, How2QA, VIOLIN, VLEP, YouCook2 (YC2C, YC2R), VATEX"	https://paperswithcode.com/dataset/value	08/06/2021	Video-And-Language Understanding Evaluation					
4158	HErlev		https://paperswithcode.com/dataset/herlev		HErlev Pap Smear Dataset					
4159	Itihasa	Itihasa is a large-scale corpus for Sanskrit to English translation containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata.	https://paperswithcode.com/dataset/itihasa	06/06/2021						
4160	5k_presetation_slides	We crawled 5000 paper, slide pairs from conference proceeding websites. (e.g. acl.org and usenix.org).	https://paperswithcode.com/dataset/5k-presetation-slides	06/06/2021	5000 presentation slide pairs					
4161	Notre-Dame Cathedral Fire	"Number of images: 1,657 images during or after the fire
If you use the dataset, please cite the following works:

Padilha, Rafael and Andaló, Fernanda A. and Rocha, Anderson. “Improving the chronological sorting of images through occlusion: A study on the Notre-Dame cathedral fire,” in 45th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020. 

Description of the event and data collection:
On April 15th, 2019, large parts of Notre-Dame Cathedral's structure and spire were devastated by a fire. People worldwide followed the tragic event through images and videos that were shared by the media and citizens.
From the generated imagery, we collected a total of 23,683 images posted on Twitter during and on the day after the fire. Even though most of them were related to the event, several were memes, cartoons, compositions and artwork, while some depicted the cathedral before the fire. As we focus on learning how the fire and appearance of the cathedral evolved during the event, we removed them, reducing our set to 5,206 relevant images. Among these, several examples were duplicates or near-duplicates of other images. Considering their little contribution to the training process, after their removal, we were left with 1,657 distinct images related to the event.
The cleaning process involved using methods such as local sensitive hashing for filtering near-duplicates, and semi-supervised approaches based on Optimum-path Forest theory to mine for relevant and non-relevant imagery of the event.
By analyzing the event's description, four main sub-events can be defined: spire on fire, spire collapsing, fire continues on roof, and fire extinguished. Each sub-event contains specific visual clues (e.g., the absence of the central spire) that can be leveraged to estimate the temporal position of an image. Each image in the dataset was manually labeled as being captured in one of these sub-events. We also consider an unknown category for images that do not contain any hint of the sub-event in which they were captured, such as zoom-ins of the cathedral's facades.
Besides that, each image was annotated with respect to the intercardinal direction of the cathedral’s facade being depicted in the image (north, northeast, east, southeast, south, southwest, west, northwest).
Image source:  Improving the chronological sorting of images through occlusion: A study on the Notre-Dame cathedral fire"	https://paperswithcode.com/dataset/notre-dame-cathedral-fire	21/05/2020						
4162	CLINC-Single-Domain-OOS	"A dataset with two separate domains, i.e., the  ""Banking''  domain and the ""Credit cards''  domain with both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents. Each domain in CLINC150 originally includes 15 intents. Each domain includes ten in-scope intents in this dataset, and the ID-OOS queries are built up based on five held-out in-scope intents.
Can be used to conduct intent detection with and without OOD-OOS and ID-OOS queries"	https://paperswithcode.com/dataset/clinc-single-domain-oos	08/06/2021						
4163	BANKING77-OOS	"A dataset with a single banking domain, includes both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents.  BANKING77 originally includes 77 intents. BANKING77-OOS includes 50 in-scope intents in this dataset, and the ID-OOS queries are built up based on 27 held-out in-scope intents.
Conduct intent detection with and without OOD-OOS and ID-OOS queries"	https://paperswithcode.com/dataset/banking77-oos	08/06/2021						
4164	ZeroWaste	ZeroWaste is a dataset for automatic waste detection and segmentation. This dataset contains over 1,800 fully segmented video frames collected from a real waste sorting plant along with waste material labels for training and evaluation of the segmentation methods, as well as over 6,000 unlabeled frames that can be further used for semi-supervised and self-supervised learning techniques. ZeroWaste also provides frames of the conveyor belt before and after the sorting process, comprising a novel setup that can be used for weakly-supervised segmentation.	https://paperswithcode.com/dataset/zerowaste	04/06/2021						
4165	ILDC	The ILDC dataset (Indian Legal Documents Corpus) is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. The dataset is used for Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case.	https://paperswithcode.com/dataset/ildc	28/05/2021	Indian Legal Documents Corpus					
4166	CiteWorth	CiteWorth is a a large, contextualized, rigorously cleaned labelled dataset for cite-worthiness detection built from a massive corpus of extracted plain-text scientific documents.	https://paperswithcode.com/dataset/citeworth	23/05/2021						
4167	IndiaPoliceEvents	IndiaPoliceEvents is a corpus of 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. This dataset is used for automated event extraction.	https://paperswithcode.com/dataset/indiapoliceevents	27/05/2021						
4168	Multilingual TOP	Multilingual TOP is a dataset for multilingual semantic parsing with human-written sentences as opposed to machine translated ones. The dataset sentences are in English, Italian and Japanese and it is based on the Facebook Task Oriented Parsing (TOP) dataset.	https://paperswithcode.com/dataset/multilingual-top	07/06/2021						
4169	MultiOpEd	MultiOpEd is a corpus of multi-perspective news editorials. It is an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MultiOpEd aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a single-sentence thesis statement summarizing the arguments presented.	https://paperswithcode.com/dataset/multioped	04/06/2021						
4170	S_B_D	"100,000 LR synthetic barcode datasets along with their corresponding bounding boxes ground truth masks.
100,000 UHR synthetic barcode datasets along with their corresponding bounding boxes ground truth masks."	https://paperswithcode.com/dataset/s-b-d	13/02/2021	Synthetic Barcode Dataset					
4171	EMOTyDA	EMOTyDA is a multimodal Emotion aware Dialogue Act dataset collected from open-sourced dialogue datasets.	https://paperswithcode.com/dataset/emotyda	01/07/2020	Emotion aware Dialogue Act					
4172	Rent3D++	"Rent3D++ is an extension of the Rent3D floorplans + photos dataset. 
The floorplans are annotated with room outline polygons, doors/windows as line segments, object-icons as axis-aligned bounding boxes, room-door-room connectivity graphs, and photo-room assignments. We have extracted rectified surface crops from architectural surfaces in photos, and these can drive interior texturing/material modeling tasks. This dataset can be used with our paper Plan2Scene to generate textured 3D mesh models of houses using floorplans and photos.
The complete list of improvements we did on the Rent3D dataset are as follows:

Fixed incorrectly categorized rooms and added wall outlines and categories from missing rooms.
Expanding the room category set {reception, bedroom, kitchen, bathroom, outdoor} by adding another 7 common room types: {closet, entrance, corridor, staircase, balcony, terrace, unknown}.
Generated room-door-room connectivity graphs for floorplans. 
Annotated all windows, doors, and other wall openings, and associated them with corresponding rooms.
Defined a new 60/20/20% (129/43/43 houses) training, validation, test split (cf. original 100/30/85 house split), giving more samples to training and validation.
Extract rectified surface crops from architectural surfaces seen in photos (floors, walls, ceilings).
Annotated axis-aligned bounding boxes for fixed object icons indicated on the test set floorplans."	https://paperswithcode.com/dataset/rent3d-1	09/06/2021						
4173	Date Estimation in the Wild	"~1M Flickr images from the XX century-aged from the 1910s to 1990s.
Dataset was introduced by Müller et al. and can be found  https://www.radar-service.eu/radar/en/dataset/tJzxrsYUkvPklBOw"	https://paperswithcode.com/dataset/date-estimation-in-the-wild							
4174	Dirty-MNIST	"DirtyMNIST is a concatenation of MNIST + AmbiguousMNIST, with 60k samples each in the training set. AmbiguousMNIST contains additional ambiguous digits with varying ambiguity. The AmbiguousMNIST test set contains 60k ambiguous samples as well.
Additional Guidance

DirtyMNIST is a concatenation of MNIST + AmbiguousMNIST, with 60k samples each in the training set.
The current AmbiguousMNIST contains 6k unique samples with 10 labels each. This multi-label dataset gets flattened to 60k samples. The assumption is that ambiguous samples have multiple ""valid"" labels as they are ambiguous. MNIST samples are intentionally undersampled (in comparison), which benefits AL acquisition functions that can select unambiguous samples.
Pick your initial training samples (for warm starting Active Learning) from the MNIST half of DirtyMNIST to avoid starting training with potentially very ambiguous samples, which might add a lot of variance to your experiments.
Make sure to pick your validation set from the MNIST half as well, for the same reason as above.
Make sure that your batch acquisition size is >= 10 (probably) given that there are 10 multi-labels per samples in Ambiguous-MNIST.
By default, Gaussian noise with stddev 0.05 is added to each sample to prevent acquisition functions (in Active Learning) from cheating by disgarding ""duplicates"".
If you want to split Ambiguous-MNIST into subsets (or Dirty-MNIST within the second ambiguous half), make sure to split by multiples of 10 to avoid splits within a flattened multi-label sample."	https://paperswithcode.com/dataset/dirty-mnist	23/02/2021						
4175	Symmetric Solids	"This is a pose estimation dataset, consisting of symmetric 3D shapes where multiple orientations are visually indistinguishable. The challenge is to predict all equivalent orientations when only one orientation is paired with each image during training (as is the scenario for most pose estimation datasets). In contrast to most pose estimation datasets, the full set of equivalent orientations is available for evaluation.
There are eight shapes total, each rendered from 50,000 viewpoints distributed uniformly at random over the full space of 3D rotations. Five of the shapes are featureless -- tetrahedron, cube, icosahedron, cone, and cylinder. Of those, the three Platonic solids (tetrahedron, cube, icosahedron) are annotated with their 12, 24, and 60 discrete symmetries, respectively. The cone and cylinder are annotated with their continuous symmetries discretized at 1 degree intervals. These symmetries are provided for evaluation; the intended supervision is only a single rotation with each image.
The remaining three shapes are marked with a distinguishing feature. There is a tetrahedron with one red-colored face, a cylinder with an off-center dot, and a sphere with an X capped by a dot. Whether or not the distinguishing feature is visible, the space of possible orientations is reduced. We do not provide the set of equivalent rotations for these shapes.
Each example contains of

the 224x224 RGB image
a shape index so that the dataset may be filtered by shape.
The indices correspond to:

0 = tetrahedron
1 = cube
2 = icosahedron
3 = cone
4 = cylinder
5 = marked tetrahedron
6 = marked cylinder
7 = marked sphere
- the rotation used in the rendering process, represented as a 3x3 rotation matrix

the set of known equivalent rotations under symmetry, for evaluation.
In the case of the three marked shapes, this is only the rendering rotation."	https://paperswithcode.com/dataset/symmetric-solids	10/06/2021						
4176	Evidence-based Factual Error Correction	Intermediate annotations from the FEVER dataset that describe original facts extracted from Wikipedia and the mutations that were applied, yielding the claims in FEVER.	https://paperswithcode.com/dataset/evidence-based-factual-error-correction	31/12/2020						
4177	TI1K Dataset	"Thumb Index 1000 (TI1K) is a dataset of 1000 hand images with the hand bounding box, and thumb and index fingertip positions. The dataset includes the natural movement of the thumb and index fingers making it suitable for mixed reality (MR) applications.
The dataset contains images only with the thumb and index fingers of both hands of resolution 640x480. All the annotations of the training and test images are in the ""label.txt"" file in the Annotation folder."	https://paperswithcode.com/dataset/ti1k-dataset	16/03/2020	Thumb Index 1000 Hand & Fingertip Detection Dataset					
4178	Bus Trajectory Dataset	This dataset contains the bus trajectory dataset collected by 6 volunteers who were asked to travel across the sub-urban city of Durgapur, India, on intra-city buses (route name: 54 Feet). During the travel, the volunteers captured sensor logs through an Android application installed on COTS smartphones.	https://paperswithcode.com/dataset/bus-trajectory-dataset	24/05/2021						
4179	MARS-DL	"MARS dataset processed with our re-Detect and Link (DL) module.
More information: https://github.com/jackie840129/CF-AAN"	https://paperswithcode.com/dataset/mars-dl	22/05/2021						
4180	DukeMTMC-VideoReID-DL	DukeMTMC-VideoReID-DL processed with our re-Detect and Link (DL) module.	https://paperswithcode.com/dataset/dukemtmc-videoreid-dl	22/05/2021						
4181	PHASE	PHASE is a dataset of physically-grounded abstract social events, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans.	https://paperswithcode.com/dataset/phase	02/03/2021	PHysically-grounded Abstract Social Events					
4182	AGENT	Inspired by cognitive development studies on intuitive psychology, we present a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology.	https://paperswithcode.com/dataset/agent	24/02/2021						
4183	World War III - International signed relations network		https://paperswithcode.com/dataset/world-war-iii-international-signed-relations	26/05/2021						
4184	TerraIncognita	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/terraincognita							
4185	TCIA Brain-Tumor-Progression	"This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy.  Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). 
All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images).  The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent.  All of the series are co-registered with the T1+C images.  The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.
Data Citation
Schmainda KM, Prah M (2018). Data from Brain-Tumor-Progression. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2018.15quzvnb"	https://paperswithcode.com/dataset/tcia-brain-tumor-progression							
4186	Ruddit	"Ruddit is a dataset of English language Reddit comments that has fine-grained, real-valued scores for offensive language detection between -1 (maximally supportive) and 1 (maximally offensive).
The dataset was annotated using Best--Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales."	https://paperswithcode.com/dataset/ruddit	10/06/2021						
4187	HuRDL	The Human-Robot Dialogue Learning (HuRDL) Corpus is a dataset about asking questions in situated task-based interactions. It is a dialogue corpus collected in an online interactive virtual environment in which human participants play the role of a robot performing a collaborative tool-organization task.	https://paperswithcode.com/dataset/hurdl	11/06/2021	Human-Robot Dialogue Learning Corpus					
4188	HPO-B	HPO-B is a benchmark for assessing the performance of HPO (Hyperparameter optimization) algorithms.	https://paperswithcode.com/dataset/hpo-b	11/06/2021						
4189	DUO	DUO is a dataset for Underwater object detection for robot picking. The dataset contains a collection of diverse underwater images with more rational annotations.	https://paperswithcode.com/dataset/duo	10/06/2021	Detecting Underwater Objects					
4190	SciCo	SciCo is an expert-annotated dataset for hierarchical CDCR (cross-document coreference resolution) for concepts in scientific papers, with the goal of jointly inferring coreference clusters and hierarchy between them.	https://paperswithcode.com/dataset/scico	18/04/2021	Scientific Concept Induction Corpus					
4191	Python Programming Puzzles (P3)	"Python Programming Puzzles (P3) is an open-source dataset where each puzzle is defined by a short Python program , and the goal is to find an input which makes output ""True"". The puzzles are objective in that each one is specified entirely by the source code of its verifier, so evaluating  is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding.
The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping.
Source: Programming Puzzles"	https://paperswithcode.com/dataset/python-programming-puzzles-p3	10/06/2021						
4192	2021 Hotel-ID	2021 Hotel-ID is a dataset for hotel recognition to help raise awareness of human trafficking and generate novel approaches. The dataset consists of hotel room images that have been crowd-sourced and uploaded through the TraffickCam mobile application.	https://paperswithcode.com/dataset/2021-hotel-id	10/06/2021						
4193	FEVEROUS	FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) is a fact verification dataset which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict.	https://paperswithcode.com/dataset/feverous	10/06/2021	Fact Extraction and VERification Over Unstructured and Structured information					
4194	FetReg	Fetoscopic Placental Vessel Segmentation and Registration (FetReg)  is a large-scale multi-centre dataset for the development of generalized and robust semantic segmentation and video mosaicking algorithms for the fetal environment with a focus on creating drift-free mosaics from long duration fetoscopy videos.	https://paperswithcode.com/dataset/fetreg	10/06/2021						
4195	WNUT 2016 NER		https://paperswithcode.com/dataset/wnut-2016-ner	01/12/2016	WNUT 2016 Twitter Named Entity Recognition					
4196	Replication Data for: Online Learning with Optimism and Delay	"The model forecasts for the sub-seasonal forecasting application considered in the Online Learning under Optimism and Delay paper experiments. This dataset consists of a single ZIP archive (919MB) that contains 1) a ""models"" folder that contains, for each model the forecasts for the Precip. 3-4w, Precip. 5-6w, Temp. 3-4w, Temp. 5-6w tasks on the western United States geography, and 2) a ""data"" folder that contains supporting geographic data. The data should be used to reproduce the PoolD experiments in https://github.com/geflaspohler/poold as described in the README. (2021-06-10)"	https://paperswithcode.com/dataset/replication-data-for-online-learning-with	13/06/2021						
4197	https://github.com/facebookresearch/InvarianceUnitTests	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/https-github-com-facebookresearch							
4198	Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People's Republic of China	"This data is for the Mis2-KDD 2021 under review paper: Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People’s Republic of China
We present our dataset that focuses on propaganda techniques in Mandarin based on a state-linked information operations dataset from the PRC released by Twitter in July 2019. The dataset consists of multi-label propaganda techniques of the sampled tweets.
In total, we have 9,950 labeled tweets with 21 different propaganda techniques. The tweets are the state-linked information operations dataset from the PRC released by Twitter."	https://paperswithcode.com/dataset/dataset-of-propaganda-techniques-of-the-state	14/06/2021	Dataset of Propaganda Techniques of the State-Sponsored Information Operation of the People's Republic of China					
4199	GitHub-Python	Repair AST parse (syntax) errors in Python code	https://paperswithcode.com/dataset/github-python	11/06/2021						
4200	Artificial signal data for signal alignment testing	"This is a set of signals-pairs, univariate and multivariate, that can be used to test alignment algorithms.
Signals are morphologically different.
Signal data is synchronized, but the provided timestamp is shifted with small time-jumps."	https://paperswithcode.com/dataset/artificial-signal-data-for-signal-alignment	07/06/2021						
4201	BBBC005	"Since robust foreground/background separation and segmentation of cellular objects (i.e.,identification of which pixels below to which objects) strongly depends on image quality, focus artifacts are detrimental to data quality. This image set provides examples of in- and out-of-focus synthetic images, which can be used for validation of focus metrics.
Image source: https://bbbc.broadinstitute.org/BBBC005"	https://paperswithcode.com/dataset/bbbc005							
4202	BBBC039	"This image set is part of a high-throughput chemical screen on U2OS cells, with examples of 200 bioactive compounds. The effect of the treatments was originally imaged using the Cell Painting assay (fluorescence microscopy). This data set only includes the DNA channel of a single field of view per compound. These images present a variety of nuclear phenotypes, representative of high-throughput chemical perturbations. The main use of this data set is the study of segmentation algorithms that can separate individual nucleus instances in an accurate way, regardless of their shape and cell density. The collection has around 23,000 single nuclei manually annotated to establish a ground truth collection for segmentation evaluation.
This data set has a total of 200 fields of view of nuclei captured with fluorescence microscopy using the Hoechst stain. These images are a sample of the larger BBBC022 chemical screen. The images are stored as TIFF files with 520x696 pixels at 16 bits.
Source: https://bbbc.broadinstitute.org/BBBC039
Image source: https://bbbc.broadinstitute.org/BBBC039"	https://paperswithcode.com/dataset/bbbc039							
4203	TNBC	"Inolves an annotated a large number of cells, including normal epithelial and myoepithelial breast cells (localized in ducts and lobules), invasive carcinomatous cells, fibroblasts, endothelial cells, adipocytes, macrophages and inflammatory cells (lymphocytes and plasmocytes). In total, our data set consists of 50 images with a total of 4022 annotated cells, the maximum number of cells in one sample is 293 and the minimum number of cells in one sample is 5, with an average of 80 cells per sample and a high standard deviation of 58. The annotation was performed by three experts: an expert pathologist and two trained research fellows. Each sample was annotated by one of the annotators, checked by another one and in case of disagreement, a consensus was established by discussion among the 3 experts.
Source: https://zenodo.org/record/1175282#.YMisCTZKgow
Image source:  https://zenodo.org/record/1175282#.YMisCTZKgow"	https://paperswithcode.com/dataset/tnbc							
4204	alpha-matte MFIF dataset	"A large-scale training dataset suffering from the defocus spread effect (DSE) is synthesized by applying an $\alpha$-matte boundary defocus model to the VOC 2012 dataset.
Motivation: Due to the lack of large-scale datasets of multi-focus images, several data generation methods based on public natural image datasets have been adopted in many deep learning (DL)-based multi-focus image fusion algorithms. However, the DSE is neglected in all the abovementioned datasets. This unrealistic training data may limit the performance of these algorithms.
Application: For training DL-based multi-focus image fusion algorithms."	https://paperswithcode.com/dataset/alpha-matte-mfif-dataset	21/09/2020	alpha-matte multi-focus image fusion dataset					
4205	Dataset of Context information for Zero Interaction Security	We release both the processed data and evaluation results from our own experiments, and the underlying raw data that can be used for future experiments and schemes in the domain of Zero-Interaction Security. Find more details in the dataset description on Zenodo.	https://paperswithcode.com/dataset/dataset-of-context-information-for-zero	11/01/2019						
4206	GitTables	GitTables is a corpus of currently 1.7M relational tables extracted from CSV files in GitHub. Table columns in GitTables have been annotated with more than 2K different semantic types from Schema.org and DBpedia. The column annotations consist of semantic types, hierarchical relations, range types and descriptions.	https://paperswithcode.com/dataset/gittables	14/06/2021						
4207	PartialSpoof_v1	"All existing databases of spoofed speech contain attack data that is spoofed in its entirety. In practice, it is entirely plausible that successful attacks can be mounted with utterances that are only partially spoofed. By definition, partially-spoofed utterances contain a mix of both spoofed and bona fide segments, which will likely degrade the performance of countermeasures trained with entirely spoofed utterances. This hypothesis raises the obvious question: ‘Can we detect partially spoofed audio?’ This paper introduces a new database of partially-spoofed data, named PartialSpoof, to help address this question. This new database enables to investigate and compare the performance of countermeasures on both utterance- and segmental- level labels. 
Source: https://zenodo.org/record/4817532#.YMi9-jZKgox"	https://paperswithcode.com/dataset/partialspoof-v1	06/04/2021						
4208	SurfaceGrid	The SurfaceGrid dataset contains nearly a million 512x512 images for use in training neural networks on shape-fron-surface contour task.	https://paperswithcode.com/dataset/surfacegrid	14/06/2021						
4209	WNUT 2020	The training and development dataset for our task was taken from previous work on wet lab corpus (Kulkarni et al., 2018) that consists of from the 623 protocols. We excluded the eight duplicate protocols from this dataset and then re-annotated the 615 unique protocols in BRAT (Stenetorp et al., 2012).	https://paperswithcode.com/dataset/wnut-20-task-1-extracting-entities-and	27/10/2020	WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols					
4210	selfie2anime	"The selfie dataset contains 46,836 selfie images annotated with 36 different attributes. We only use photos of females as training data and test data. The size of the training dataset is 3400, and that of the test dataset is 100, with the image size of 256 x 256. For the anime dataset, we have firstly retrieved 69,926 animation character images from Anime-Planet1. Among those images, 27,023 face images are extracted by using an anime-face detector2. After selecting only female character images and removing monochrome images manually, we have collected two datasets of female anime face images, with the sizes of 3400 and 100 for training and test data respectively, which is the same numbers as the selfie dataset. Finally, all anime face images are resized to 256 x 256 by applying a CNN-based image super-resolution algorithm.
Source: U-GAT-IT: UNSUPERVISED GENERATIVE ATTENTIONAL NETWORKS WITH ADAPTIVE LAYER-INSTANCE NORMALIZATION FOR IMAGE-TO-IMAGE TRANSLATION
."	https://paperswithcode.com/dataset/selfie2anime	25/07/2019						
4211	WNUT-2020 Task 2	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/wnut-2020-task-2	16/10/2020	WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets					
4212	DIR-LAB COPDgene	Inspiratory and exipratory breath-hold CT image pairs acquired from the National Heart Lung Blood Institute COPDgene study archive.	https://paperswithcode.com/dataset/dir-lab-copdgene		The Deformable Image Registration Laboratory					
4213	Children's Song Dataset	"Children's Song Dataset is open source dataset for singing voice research. This dataset contains 50 Korean and 50 English songs sung by one Korean female professional pop singer. Each song is recorded in two separate keys resulting in a total of 200 audio recordings. Each audio recording is paired with a MIDI transcription and lyrics annotations in both grapheme-level and phoneme-level.
Dataset Structure
The entire data splits into Korean and English and each language splits into 'wav', 'mid', 'lyric', 'txt' and 'csv' folders. Each song has the identical file name for each format. Each format represents following information. Additional information like original song name, tempo and time signature for each song can be found in 'metadata.json'.

'wav': Vocal recordings in 44.1kHz 16bit wav format
'mid': Score information in MIDI format
'lyric':  Lyric information in grapheme-level
'txt': Lyric information in syllable and phoneme-level
'csv': Note onsets and offsets and syllable timings in comma-separated value (CSV) format"	https://paperswithcode.com/dataset/children-s-song-dataset							
4214	TCC	The largest and most realistic dataset available for TCC. It consists of 600 real-world videos recorded with a high-resolution mobile phone camera shooting 1824 x 1368 sized pictures. The length of these videos ranges from 3 to 17 frames (7.3 on average, the median is 7.0 and mode is 8.5). Ground truth information is present only for the last frame in each video (i.e., the shot frame), and was collected using a gray surface calibration target.	https://paperswithcode.com/dataset/tcc							
4215	USPTO-50k	Subset and preprocessed version of Chemical reactions from US patents (1976-Sep2016) by Daniel Lowe.	https://paperswithcode.com/dataset/uspto-50k							
4216	Eduge	"Eduge news classification dataset provided by Bolorsoft LLC. Used to train the Eduge.mn production news classifier
75K news articles in 9 categories: урлаг соёл, эдийн засаг, эрүүл мэнд, хууль, улс төр, спорт, технологи, боловсрол and байгал орчин
Download train/test data via:
train
test"	https://paperswithcode.com/dataset/eduge	19/03/2019	Eduge news classification dataset					
4217	Learning to Autofocus	This dataset contains 510 focal stacks (49 different focal distances) from in-the-wild scenes with calculated depth from SFM. This dataset was designed for research on Autofocus but can be used for any research which is interested in focal stacks, defocus cues, or depth signals (particularly for interest in close depth).	https://paperswithcode.com/dataset/learning-to-autofocus	26/04/2020						
4218	IT Job Prediction Dataset	"Briefly describe the dataset. Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

If the description or image is from a different paper, please refer to it as follows:
Source: title
Image Source: title"	https://paperswithcode.com/dataset/it-job-prediction-dataset							
4219	CMeEE	Chinese Medical Named Entity Recognition, a dataset first released in CHIP20204, is used for CMeEE task. Given a pre-defined schema, the task is to identify and extract entities from the given sentence and classify them into nine categories: disease, clinical manifestations, drugs, medical equipment, medical procedures, body, medical examinations, microorganisms, and department.	https://paperswithcode.com/dataset/cmeee	15/06/2021	Chinese Medical Named Entity Recognition Dataset					
4220	CMeIE	Chinese Medical Information Extraction, a dataset that is also released in CHIP2020, is used for CMeIE task. The task is aimed at identifying both entities and relations in a sentence following the schema constraints. There are 53 relations defined in the dataset, including 10 synonymous sub-relationships and 43 other sub-relationships.	https://paperswithcode.com/dataset/cmeie	15/06/2021	Chinese Medical Information Extraction Dataset					
4221	EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/eev-a-large-scale-dataset-for-studying-evoked							
4222	CHIP-STS	"CHIP Semantic Textual Similarity, a dataset for sentence similarity in the non-i.i.d.
(non-independent and identically distributed) setting, is used for the CHIP-STS task. Specifically, the
task aims to transfer learning between disease types on Chinese disease questions and answer data.
Given question pairs related to 5 different diseases (The disease types in the training and testing set
are different), the task intends to determine whether the semantics of the two sentences are similar."	https://paperswithcode.com/dataset/chip-sts	15/06/2021	Semantic Textual Similarity Dataset					
4223	CHIP-CDN	"CHIP Clinical Diagnosis Normalization, a dataset that aims to standardize the terms
from the final diagnoses of Chinese electronic medical records, is used for the CHIP-CDN task.
Given the original phrase, the task is required to normalize it to standard terminology based on the
International Classification of Diseases (ICD-10) standard for Beijing Clinical Edition v601."	https://paperswithcode.com/dataset/chip-cdn	15/06/2021	Clinical Diagnosis Normalization Dataset					
4224	CHIP-CTC	"CHIP Clinical Trial Classification, a dataset aimed at classifying clinical trials eligibility criteria, which are fundamental guidelines of clinical trials defined to identify whether a subject
meets a clinical trial or not, is used for the CHIP-CTC task. All text data are collected from the
website of the Chinese Clinical Trial Registry (ChiCTR)
, and a total of 44 categories are defined.
The task is like text classification; although it is not a new task, studies and corpus for the Chinese
clinical trial criterion are still limited, and we hope to promote future researches for social benefits."	https://paperswithcode.com/dataset/chip-ctc							
4225	KUAKE-QIC	"KUAKE Query Intent Classification, a dataset for intent classification, is used for the
KUAKE-QIC task. Given the queries of search engines, the task requires to classify each of them into
one of 11 medical intent categories defined in KUAKE-QIC, including diagnosis, etiology analysis,
treatment plan, medical advice, test result analysis, disease description, consequence prediction,
precautions, intended effects, treatment fees, and others."	https://paperswithcode.com/dataset/kuake-qic	15/06/2021	Query Intent Classification Dataset					
4226	KUAKE-QTR	"KUAKE Query Title Relevance, a dataset used to estimate the relevance of the title
of a query document, is used for the KUAKE-QTR task. Given a query (e.g., “Symptoms of vitamin
B deficiency”), the task aims to find the relevant title (e.g., “The main manifestations of vitamin B
deficiency”)."	https://paperswithcode.com/dataset/kuake-qtr	15/06/2021	Query-Title Relevance Dataset					
4227	KUAKE-QQR	"KUAKE Query-Query Relevance, a dataset used to evaluate the relevance of the
content expressed in two queries, is used for the KUAKE-QQR task. Similar to KUAKE-QTR, the
task aims to estimate query-query relevance, which is an essential and challenging task in real-world
search engines."	https://paperswithcode.com/dataset/kuake-qqr	15/06/2021	Query-Query Relevance Dataset					
4228	Oxford Road Boundaries	"The Oxford Road Boundaries is a dataset designed for training and testing machine-learning-based road-boundary detection and inference approaches. 
The authors have hand-annotated two of the 10 km-long forays from the Oxford Robotcar Dataset and generated from other forays several thousand further examples with semi-annotated road-boundary masks. To boost the number of training samples in this way, the authors used a vision-based localiser to project labels from the annotated datasets to other traversals at different times and weather conditions. 
As a result, the dataset consists of 62,605 labelled samples, of which 47,639 samples are curated. Each of these samples contains both raw and classified masks for left and right lenses. Our data contains images from a diverse set of scenarios such as straight roads, parked cars, junctions, etc."	https://paperswithcode.com/dataset/oxford-road-boundaries	16/06/2021						
4229	JRDB-Act	"JRDB-Act is an extension of the JRDB dataset to create a large-scale multi-modal dataset for spatio-temporal action, social group and activity detection. 
JRDB-Act has been densely annotated with atomic actions, comprises over 2.8M action labels, constituting a large-scale spatio-temporal action detection dataset. Each human bounding box is labelled with one pose-based action label and multiple (optional) interaction-based action labels. Moreover JRDB-Act comes with social group identification annotations conducive to the task of grouping individuals based on their interactions in the scene to infer their social activities (common activities in each social group)."	https://paperswithcode.com/dataset/jrdb-act	16/06/2021						
4230	RyanSpeech	RyanSpeech is a speech corpus for research on automated text-to-speech (TTS) systems. This dataset contains textual materials from real-world conversational settings. These materials contain over 10 hours of a professional male voice actor's speech recorded at 44.1 kHz.	https://paperswithcode.com/dataset/ryanspeech	15/06/2021						
4231	LARC	"LARC is a dataset built from ARC (Abstraction and Reasoning Corpus). ARC is a set of tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI.
LARC or Language-annotated ARC, is a collection of natural language descriptions by a group of human participants, unfamiliar both with ARC and with each other, who instruct each other on how to solve ARC tasks. LARC contains successful instructions for 88% of the ARC tasks."	https://paperswithcode.com/dataset/larc	15/06/2021	Language-annotated Abstraction and Reasoning					
4232	Physion	Physion is a visual and physical prediction benchmark to measure the performance of machine learning models on making predictions about commonplace real world physical events. In realistically simulating a wide variety of physical phenomena -- rigid and soft-body collisions, stable multi-object configurations, rolling and sliding, projectile motion -- this dataset presents a more comprehensive challenge than existing benchmarks. Moreover, the dataset also contains human responses for the stimuli so that model predictions can be directly compared to human judgments.	https://paperswithcode.com/dataset/physion	15/06/2021						
4233	EuroCrops	"EuroCrops is a dataset for automatic vegetation classification from multi-spectral and multi-temporal satellite data, annotated with official LIPS reporting data from countries of the European Union, curated by the Technical University of Munich and GAF AG. The project is managed by the DLR Space Administration and funded by BMWI (Federal Ministry for Economic Affairs and Energy). This dataset is publicly available for research causes with the idea in mind to assist in the subsidy control of agricultural self-declarations.
Source: https://www.eurocrops.tum.de/"	https://paperswithcode.com/dataset/eurocrops	14/06/2021						
4234	GigaSpeech	GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training.	https://paperswithcode.com/dataset/gigaspeech	13/06/2021						
4235	Imgur5K	Imgur5k is a large-scale handwritten in-the-wild dataset, containing challenging real world handwritten samples from nearly 5K writers. It consists of ~135K handwritten English words from 5K different images. As opposed to existing dataests for OCR which have limited variability in their images, the images in Imgur5K contain a diverse set of styles.	https://paperswithcode.com/dataset/imgur5k	15/06/2021						
4236	DisKnE	"DisKnE is a benchmark for Disease Knowledge Evaluation built from MedNLI and MEDIQA-NLI. This benchmark is constructed to specifically test the medical reasoning capabilities of ML models, such as mapping symptoms to diseases.
The dataset was built by annotating each positive MedNLI example with the types of medical reasoning that are needed. Negative examples were created by corrupting these positive examples in an adversarial way. Furthermore, the training-test splits are defined per disease, ensuring that no knowledge about test diseases can be learned from the training data."	https://paperswithcode.com/dataset/diskne	14/06/2021	Disease Knowledge Evaluation					
4237	PATTERN	"PATTERN is a node classification tasks generated with Stochastic Block Models, which is widely used to model communities in social networks by modulating the intra- and extra-communities connections, thereby controlling the difficulty of the task. PATTERN tests the fundamental graph task of recognizing specific predetermined subgraphs.
Source: Benchmarking Graph Neural Networks"	https://paperswithcode.com/dataset/pattern	02/03/2020						
4238	CLUSTER	"CLUSTER is a node classification tasks generated with Stochastic Block Models, which is widely used to model communities in social networks by modulating the intra- and extra-communities connections, thereby controlling the difficulty of the task. CLUSTER aims at identifying community clusters in a semi-supervised setting.
Source: Benchmarking Graph Neural Networks"	https://paperswithcode.com/dataset/cluster	02/03/2020						
4239	CSL	"CSL is a synthetic dataset introduced in Murphy et al. (2019) to test the expressivity of GNNs. In particular, graphs are isomorphic if they have the same degree and the task is to classify non-isomorphic graphs.
Source: Benchmarking Graph Neural Networks"	https://paperswithcode.com/dataset/csl	02/03/2020						
4240	Large-scale Anomaly Detection	Large-scale Anomaly Detection (LAD) is a database to benchmark anomaly detection in video sequences, which is featured in two aspects. 1) It contains 2000 video sequences including normal and abnormal video clips with 14 anomaly categories including crash, fire, violence, etc. with large scene varieties, making it the largest anomaly analysis database to date. 2) It provides the annotation data, including video-level labels (abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal video frame) to facilitate anomaly detection.	https://paperswithcode.com/dataset/large-scale-anomaly-detection	16/06/2021						
4241	Counting Probe	"Probing cross-modal capabilities of Vision & Language models with a counting task.

binary classification
following a FOIL setup (as introduced by Shekhar et al. 2017: https://www.aclweb.org/anthology/P17-1024/)"	https://paperswithcode.com/dataset/counting-probe	22/12/2020	Counting Probe based on Visual7W					
4242	BestRev	Survey instrument, analysis code, and anonymized responses for the paper on review practices in SE.	https://paperswithcode.com/dataset/bestrev	02/09/2020	Understanding Peer Review of Software Engineering Papers					
4243	COVID-19 Case Surveillance Public Use Data	This case surveillance public use dataset has 12 elements for all COVID-19 cases shared with CDC and includes demographics, any exposure history, disease severity indicators and outcomes, presence of any underlying medical conditions and risk behaviors, and no geographic data.	https://paperswithcode.com/dataset/covid-19-case-surveillance-public-use-data	15/05/2020						
4244	BABEL	BABEL is a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction --  sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc.	https://paperswithcode.com/dataset/babel-1	17/06/2021						
4245	IFCNet	The full IFCNet dataset currently consists of 19,000 CAD models distributed over 65 classes according to the taxonomy of the Industry Foundation Classes (IFC) standard. The IFC standard provides an open data exchange format for projects in the Architecture, Engineering and Construction (AEC) domain. Due to high imbalances with respect to the number of objects in each class, a subset of 8,000 objects from 20 classes is selected to form the IFCNetCore dataset, providing a more balanced distribution. Apart from the geometric information of the CAD model, most objects also have semantic information in the form of key-value pairs, enums or lists, which are relevant to different stages of the construction process.	https://paperswithcode.com/dataset/ifcnet	17/06/2021						
4246	FIN	A dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 words) were randomly selected for manual annotation, based on the four NE types provided in the CoNLL-2003 dataset: LOCATION (LOC), ORGANISATION (ORG), PERSON (PER), and MISCELLANEOUS (MISC).	https://paperswithcode.com/dataset/fin	01/12/2015						
4247	WebVid	"WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content.
Currently 2.5M pairs are available for download, the full 10M is coming soon.
https://github.com/m-bain/webvid-dataset"	https://paperswithcode.com/dataset/webvid	01/04/2021						
4248	Kinships	The Kinships dataset describes relationships between members of the Australian tribe Alyawarra and consists of 10,686 triples. It contains 104 entities representing members of the tribe and 26 relationship types that represent kinship terms such as Adiadya or Umbaidya.	https://paperswithcode.com/dataset/kinships							
4249	CI-MNIST	CI-MNIST (Correlated and Imbalanced MNIST) is a variant of MNIST dataset with introduced different types of correlations between attributes, dataset features, and an artificial eligibility criterion. For an input image $x$, the label $y \in \{1, 0\}$ indicates eligibility or ineligibility, respectively, given that $x$ is even or odd. The dataset defines the background colors as the protected or sensitive attribute $s \in \{0, 1\}$, where blue denotes the unprivileged group and red denotes the privileged group. The dataset was designed in order to evaluate bias-mitigation approaches in challenging setups and be capable of controlling different dataset configurations.	https://paperswithcode.com/dataset/ci-mnist	07/06/2021	Correlated and Imbalanced MNIST					
4250	Box2D	Continuous control tasks in the Box2D simulator.	https://paperswithcode.com/dataset/box2d							
4251	JSRT	"The standard digital image database with and without chest lung nodules (JSRT database) was created(*1) by the Japanese Society of Radiological Technology (JSRT) in cooperation with the Japanese Radiological Society (JRS) in 1998. Since then, the JSRT database has been used by a number of researchers in the world for various research purposes such as image processing, image compression, evaluation of image display, computer-aided diagnosis (CAD), picture archiving and communication system (PACS), and for training and testing.
・Useful for ROC analysis (154 nodule and 93 non-nodule images)
・   High resolution (2048 x 2048 matrix size, 0.175mm pixel size)
・   Wide density range (12bit, 4096 gray scale)
・   Universal image format (no header, big-endian raw data)
・   Useful for diagnostic training and testing
・   Additional information:
    patient age, gender, diagnosis (malignant or benign), X and Y coordinates of nodule, simple diagram of nodule location, degree of subtlety in visual detection of nodules
Source: http://db.jsrt.or.jp/eng.php"	https://paperswithcode.com/dataset/jsrt		Japanese Society of Radiological Technology Database					
4252	Sports10	"Games dataset containing 100,000 Gameplay Images of 175 Video Games across 10 Sports Genres - AMERICAN FOOTBALL, BASKETBALL, BIKE RACING, CAR RACING, FIGHTING, HOCKEY, SOCCER, TABLE TENNIS, TENNIS. 


Hand-curated images to remove menu/transition frames and only include gameplay sequences.


Games are divided into three visual styling categories: 
        RETRO (arcade-style, 1990s and earlier)
        MODERN (roughly 2000s)
        PHOTOREAL (roughly late 2010s)."	https://paperswithcode.com/dataset/sports10	18/06/2021						
4253	Solar-Power	"Solar Power Data for Integration Studies
NREL's Solar Power Data for Integration Studies are synthetic solar photovoltaic (PV) power plant data points for the United States representing the year 2006.
The data are intended for use by energy professionals—such as transmission planners, utility planners, project developers, and university researchers—who perform solar integration studies and need to estimate power production from hypothetical solar plants.
Data Methodologies
The Solar Power Data for Integration Studies consist of 1 year (2006) of 5-minute solar power and hourly day-ahead forecasts for approximately 6,000 simulated PV plants. Solar power plant locations were determined based on the capacity expansion plan for high-penetration renewables in Phase 2 of the Western Wind and Solar Integration Study and the Eastern Renewable Generation Integration Study.
NREL generated the 5-minute data set using the Sub-Hour Irradiance Algorithm. The day-ahead solar forecast data for locations in the western United States were generated by 3TIER based on numerical weather predication simulations for Phase 1 of the Western Wind and Solar Integration Study. NREL generated the day-ahead solar forecast data in eastern U.S. locations using the Weather Research and Forecasting model.
The data are for specific years and should not be assumed to be representative of typical radiation levels for a site. These data should not generally be used for site-specific project development work.
Naming Convention
The naming convention of the state-wise solar power data (.csv files) from the Solar Integration Studies is as follows.
Data Type_Latitude_Longitude_Weather Year_PV Type_CapacityMW_Time Interval _Min.csv
Data Type
Actual: Real power output
DA: Day ahead forecast
HA4: 4 hour ahead forecast
Weather Year: The PV data is based on the particular year's known weather condition.
PV Type
UPV: Utility scale PV
DPV: Distributed PV
Note: The practical difference between UPV and DPV is in the configurations (UPV has single axis tracking while DPV is fixed tilt equaling to latitude) and the smoothing (both are run through a low-pass filter, the DPV will have more of the high frequency variability smoothed out).
Capacity: Installed capacity in MW
Time Interval: PV generation data reading interval in minutes.
Contact
Yingchen Zhang
Manager, Sensing, Measurement, and Forecasting Group
Yingchen.Zhang@nrel.gov
303-384-7090"	https://paperswithcode.com/dataset/solar-power		Solar Power Data for Integration Studies (Alabama)					
4254	ZhihuRec	"ZhihuRec dataset is collected from a knowledge-sharing platform (Zhihu), which is composed of around 100M interactions collected within 10 days, 798K users, 165K questions, 554K answers, 240K authors, 70K topics, and more than 501K user query logs. There are also descriptions of users, answers, questions, authors, and topics, which are anonymous. To the best of our knowledge, this is the largest real-world interaction dataset for personalized recommendation.
Source: https://github.com/THUIR/ZhihuRec-Dataset
Image source: https://arxiv.org/pdf/2106.06467v1.pdf"	https://paperswithcode.com/dataset/zhihurec	11/06/2021						
4255	Personal Events in Dialogue Corpus	The PEDC is a corpus of 14 episodes of This American Life podcast transcripts that have been annotated for events. The corpus contains excerpts from these episodes (listed in Tabe 1) that are dialogue. The granularity of annotation in this corpus is the token; each token is either annotated as an event, or a nonevent. For more information please download the corpus, and see the annotation guide for more specifics on how we define event, and the README for how the annotations are encoded. Also, much more information regarding the corpus, and its use is in the Automatic extraction of personal events from dialogue paper.	https://paperswithcode.com/dataset/personal-events-in-dialogue-corpus	01/07/2020						
4256	MeshRIR	"MeshRIR is a dataset of acoustic room impulse responses (RIRs) at finely meshed grid points. Two subdatasets are currently available: one consists of IRs in a 3D cuboidal region from a single source, and the other consists of IRs in a 2D square region from an array of 32 sources. This dataset is suitable for evaluating sound field analysis and synthesis methods.
See the link below for the details.

https://sh01k.github.io/MeshRIR"	https://paperswithcode.com/dataset/meshrir	21/06/2021						
4257	ONCE	"ONCE (One millioN sCenEs) is a dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than other 3D autonomous driving datasets available like nuScenes and Waymo, and it is collected across a range of different areas, periods and weather conditions. 
Consists of:


1 Million LiDAR frames, 7 Million camera images


200 km² driving regions, 144 driving hours


15k fully annotated scenes with 5 classes (Car, Bus, Truck, Pedestrian, Cyclist)


Diverse environments (day/night, sunny/rainy, urban/suburban areas)"	https://paperswithcode.com/dataset/once	21/06/2021	One Million Scenes					
4258	SODA10M	SODA10M is a large-scale object detection benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data. SODA10M contains 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes.	https://paperswithcode.com/dataset/soda10m	21/06/2021						
4259	Calliar	Calliar is a dataset for Arabic calligraphy. The dataset consists of 2500 json files that contain strokes manually annotated for Arabic calligraphy.	https://paperswithcode.com/dataset/calliar	20/06/2021						
4260	HICRD	HICRD (Heron Island Coral Reef Dataset) is a large-scale real underwater image dataset for underwater image restoration. There are 2000 reference restored images and 6003 original underwater images in the unpaired training set.	https://paperswithcode.com/dataset/hicrd	20/06/2021	Heron Island Coral Reef Dataset					
4261	GOLOS	"Golos is a Russian speech dataset suitable for speech research. The dataset mainly consists of recorded audio files manually annotated on the crowd-sourcing platform. The total duration of the audio is about 1240 hours.
Dataset structure
| Domain         | Train files | Train hours  | Test files | Test hours |
|----------------|------------|--------|-------|------|
| Crowd          | 979 796    | 1 095  | 9 994 | 11.2 |
| Farfield       | 124 003    |   132.4| 1 916 |  1.4 |
| Total          | 1 103 799  | 1 227.4|11 910 | 12.6 |
Audio files in opus format
| Archive          | Size       |  Link               |
|------------------|------------|---------------------|
| golos_opus.tar   | 20.5 GB    | https://sc.link/JpD |
Audio files in wav format
| Archives          | Size       |  Links              |
|-------------------|------------|---------------------|
| train_farfield.tar| 15.4 GB    | https://sc.link/1Z3 |
| train_crowd0.tar  | 11 GB      | https://sc.link/Lrg |
| train_crowd1.tar  | 14 GB      | https://sc.link/MvQ |
| train_crowd2.tar  | 13.2 GB    | https://sc.link/NwL |
| train_crowd3.tar  | 11.6 GB    | https://sc.link/Oxg |
| train_crowd4.tar  | 15.8 GB    | https://sc.link/Pyz |
| train_crowd5.tar  | 13.1 GB    | https://sc.link/Qz7 |
| train_crowd6.tar  | 15.7 GB    | https://sc.link/RAL |
| train_crowd7.tar  | 12.7 GB    | https://sc.link/VG5 |
| train_crowd8.tar  | 12.2 GB    | https://sc.link/WJW |
| train_crowd9.tar  | 8.08 GB    | https://sc.link/XKk |
| test.tar          | 1.3 GB     | https://sc.link/Kqr |
Evaluation
Percents of Word Error Rate for different test sets
| Decoder \ Test set    | Crowd test  | Farfield test    | MCV<sup>1</sup> dev | MCV<sup>1</sup> test |
|-------------------------------------|-----------|----------|-----------|----------|
| Greedy decoder                      | 4.389 %   | 14.949 % | 9.314 %   | 11.278 % |
| Beam Search with Common Crawl LM    | 4.709 %   | 12.503 % | 6.341 %   | 7.976 % |
| Beam Search with Golos train set LM | 3.548 %   | 12.384 % |  -        | -       |
| Beam Search with Common Crawl and Golos LM | 3.318 %   | 11.488 % | 6.4 %     | 8.06 %   |"	https://paperswithcode.com/dataset/golos	18/06/2021						
4262	JerichoWorld	"JerichoWorld  is a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives -- or text-adventure games -- are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects -- each with their own unique descriptions -- providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. 
JerichoWorld provides 24,198 mappings between rich natural language observations and: (1) knowledge graphs that reflect the world state in the form of a map; (2) natural language actions that are guaranteed to cause a change in that particular world state. The training data is collected across 27 games in multiple genres and contains a further 7,836 heldout instances over 9 additional games in the test set."	https://paperswithcode.com/dataset/jerichoworld	17/06/2021						
4263	X-Fact	X-FACT is a large publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models.	https://paperswithcode.com/dataset/x-fact	17/06/2021						
4264	DocNLI	DocNLI is a large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets.	https://paperswithcode.com/dataset/docnli	17/06/2021						
4265	EMOVIE	EMOVIE is a Mandarin emotion speech dataset including 9,724 samples with audio files and its emotion human-labeled annotation.	https://paperswithcode.com/dataset/emovie	17/06/2021						
4266	DISC21	DISC21 is a benchmark for large-scale image similarity detection. This benchmark is used for the Image Similarity Challenge at NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a modified copy of any image in a reference corpus of size 1~million. The benchmark features a variety of image transformations such as automated transformations, hand-crafted image edits and machine-learning based manipulations. This mimics real-life cases appearing in social media, for example for integrity-related problems dealing with misinformation and objectionable content. The strength of the image manipulations, and therefore the difficulty of the benchmark, is calibrated according to the performance of a set of baseline approaches. Both the query and reference set contain a majority of ``distractor'' images that do not match, which corresponds to a real-life needle-in-haystack setting, and the evaluation metric reflects that.	https://paperswithcode.com/dataset/disc21	17/06/2021	Dataset for ISC 2021					
4267	Hi-Phy	Hi-Phy is a benchmark for physical reasoning that allows researchers to test individual physical reasoning capabilities. Inspired by how humans acquire these capabilities, the benchmark proposes a general hierarchy of physical reasoning capabilities with increasing complexity. this benchmark tests capabilities according to this hierarchy through generated physical reasoning tasks in the video game Angry Birds.	https://paperswithcode.com/dataset/hi-phy	17/06/2021						
4268	VAW	"VAW is a large scale visual attributes dataset with explicitly labelled positive and negative attributes. 
Details: 

620 Unique Attributes including color, shape, texture, posture and many others
2,260 Unique Objects observed in the wild
72,274 Images from the Visual Genome Dataset
4 different evaluation metrics for measuring multi-faceted performance metrics"	https://paperswithcode.com/dataset/vaw	17/06/2021	Visual Attributes in the Wild					
4269	IMFW	Indian Masked faces in the wild Database is collected into three sets:(i) Indian Celebrity, (ii) Instagram and (iii) Indian Crowd. The Indian Celebrity contains 40 Indian celebrities with 435 images, including Bollywood actors/actresses, television stars, sports personalities, and politicians. The Instagram set contains 377 images of 40 subjects downloaded from Instagram. We collected masked and non-masked images of Indian people with a public profile. The Indian Crowd set is collected from the common people who volunteered to contribute to the dataset. This set contains 120 subjects with 562 images. All the Images are collected in both constrained and unconstrained environments with variation in pose, illumination, background and masks worn by the people.	https://paperswithcode.com/dataset/imfw	17/06/2021	Indian Masked Faces In The Wild					
4270	Fishnet Open Images	Fishnet Open Images Database is a large dataset of EM imagery for fish detection and fine-grained categorisation onboard commercial fishing vessels. The dataset consists of 86,029 images containing 34 object classes, making it the largest and most diverse public dataset of fisheries EM imagery to-date. It includes many of the characteristic challenges of EM data: visual similarity between species, skewed class distributions, harsh weather conditions, and chaotic crew activity.	https://paperswithcode.com/dataset/fishnet-open-images	16/06/2021						
4271	Synthetic COVID-19 Chest X-ray	The Synthetic COVID-19 Chest X-ray Dataset consists of 21,295 synthetic COVID-19 chest X-ray images to be used for computer-aided diagnosis. These images, generated via an unsupervised domain adaptation approach, are of high quality.	https://paperswithcode.com/dataset/synthetic-covid-19-chest-x-ray	17/06/2021						
4272	Dataset for methane combustion	The dataset contains 578,731 structures for methane combustion and their energies and forces under MN15/6-31G** level.	https://paperswithcode.com/dataset/dataset-for-methane-combustion	11/11/2020						
4273	CICIDS2017	"Intrusion Detection Evaluation Dataset (CIC-IDS2017)
Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of reliable test and validation datasets, anomaly-based intrusion detection approaches are suffering from consistent and accurate performance evolutions.
Our evaluations of the existing eleven datasets since 1998 show that most are out of date and unreliable. Some of these datasets suffer from the lack of traffic diversity and volumes, some do not cover the variety of known attacks, while others anonymize packet payload data, which cannot reflect the current trends. Some are also lacking feature set and metadata.
CICIDS2017 dataset contains benign and the most up-to-date common attacks, which resembles the true real-world data (PCAPs). It also includes the results of the network traffic analysis using CICFlowMeter with labeled flows based on the time stamp, source, and destination IPs, source and destination ports, protocols and attack (CSV files). Also available is the extracted features definition. 
Generating realistic background traffic was our top priority in building this dataset. We have used our proposed B-Profile system (Sharafaldin, et al. 2016) to profile the abstract behavior of human interactions and generates naturalistic benign background traffic. For this dataset, we built the abstract behaviour of 25 users based on the HTTP, HTTPS, FTP, SSH, and email protocols.
The data capturing period started at 9 a.m., Monday, July 3, 2017 and ended at 5 p.m. on Friday July 7, 2017, for a total of 5 days. Monday is the normal day and only includes the benign traffic. The implemented attacks include Brute Force FTP, Brute Force SSH, DoS, Heartbleed, Web Attack, Infiltration, Botnet and DDoS. They have been executed both morning and afternoon on Tuesday, Wednesday, Thursday and Friday.
In our recent dataset evaluation framework (Gharib et al., 2016), we have identified eleven criteria that are necessary for building a reliable benchmark dataset. None of the previous IDS datasets could cover all of the 11 criteria. In the following, we briefly outline these criteria:
Complete Network configuration: A complete network topology includes Modem, Firewall, Switches, Routers, and presence of a variety of operating systems such as Windows, Ubuntu and Mac OS X.
Complete Traffic: By having a user profiling agent and 12 different machines in Victim-Network and real attacks from the Attack-Network.
Labelled Dataset: Section 4 and Table 2 show the benign and attack labels for each day. Also, the details of the attack timing will be published on the dataset document.
Complete Interaction: As Figure 1 shows, we covered both within and between internal LAN by having two different networks and Internet communication as well.
Complete Capture: Because we used the mirror port, such as tapping system, all traffics have been captured and recorded on the storage server.
Available Protocols: Provided the presence of all common available protocols, such as HTTP, HTTPS, FTP, SSH and email protocols.
Attack Diversity: Included the most common attacks based on the 2016 McAfee report, such as Web based, Brute force, DoS, DDoS, Infiltration, Heart-bleed, Bot and Scan covered in this dataset.
Heterogeneity: Captured the network traffic from the main Switch and memory dump and system calls from all victim machines, during the attacks execution.
Feature Set: Extracted more than 80 network flow features from the generated network traffic using CICFlowMeter and delivered the network flow dataset as a CSV file. See our PCAP analyzer and CSV generator.
MetaData: Completely explained the dataset which includes the time, attacks, flows and labels in the published paper.
The full research paper outlining the details of the dataset and its underlying principles:
Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, “Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization”, 4th International Conference on Information Systems Security and Privacy (ICISSP), Purtogal, January 2018
Day, Date, Description, Size (GB)
Monday, Normal Activity, 11.0G
Tuesday, attacks + Normal Activity, 11G
Wednesday, attacks + Normal Activity, 13G
Thursday, attacks + Normal Activity, 7.8G
Friday, attacks + Normal Activity, 8.3G
Victim and attacker networks information
Firewall: 205.174.165.80, 172.16.0.1
DNS+ DC Server: 192.168.10.3
Outsiders (Attackers network)
Kali: 205.174.165.73
Win: 205.174.165.69, 70, 71
Insiders (Victim network)
Web server 16 Public: 192.168.10.50, 205.174.165.68
Ubuntu server 12 Public: 192.168.10.51, 205.174.165.66
Ubuntu 14.4, 32B: 192.168.10.19
Ubuntu 14.4, 64B: 192.168.10.17
Ubuntu 16.4, 32B: 192.168.10.16
Ubuntu 16.4, 64B: 192.168.10.12
Win 7 Pro, 64B: 192.168.10.9
Win 8.1, 64B: 192.168.10.5
Win Vista, 64B: 192.168.10.8
Win 10, pro 32B: 192.168.10.14
Win 10, 64B: 192.168.10.15
MAC: 192.168.10.25
Monday, July 3, 2017
Benign (Normal human activities)
Tuesday, July 4, 2017
Brute Force
FTP-Patator (9:20 – 10:20 a.m.)
SSH-Patator (14:00 – 15:00 p.m.)
Attacker: Kali, 205.174.165.73
Victim: WebServer Ubuntu, 205.174.165.68 (Local IP: 192.168.10.50)
NAT Process on Firewall:
Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50
Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73
Wednesday, July 5, 2017
DoS / DDoS
DoS slowloris (9:47 – 10:10 a.m.)
DoS Slowhttptest (10:14 – 10:35 a.m.)
DoS Hulk (10:43 – 11 a.m.)
DoS GoldenEye (11:10 – 11:23 a.m.)
Attacker: Kali, 205.174.165.73
Victim: WebServer Ubuntu, 205.174.165.68 (Local IP192.168.10.50)
NAT Process on Firewall:
Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50
Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73
Heartbleed Port 444 (15:12 - 15:32)
Attacker: Kali, 205.174.165.73
Victim: Ubuntu12, 205.174.165.66 (Local IP192.168.10.51)
NAT Process on Firewall:
Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.11 -> 192.168.10.51
Reply: 192.168.10.51 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73
Thursday, July 6, 2017
Morning
Web Attack – Brute Force (9:20 – 10 a.m.)
Web Attack – XSS (10:15 – 10:35 a.m.)
Web Attack – Sql Injection (10:40 – 10:42 a.m.)
Attacker: Kali, 205.174.165.73
Victim: WebServer Ubuntu, 205.174.165.68 (Local IP192.168.10.50)
NAT Process on Firewall:
Attack: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.10 -> 192.168.10.50
Reply: 192.168.10.50 -> 172.16.0.1 -> 205.174.165.80 -> 205.174.165.73
Afternoon
Infiltration – Dropbox download
Meta exploit Win Vista (14:19 and 14:20-14:21 p.m.) and (14:33 -14:35)
Attacker: Kali, 205.174.165.73
Victim: Windows Vista, 192.168.10.8
Infiltration – Cool disk – MAC (14:53 p.m. – 15:00 p.m.)
Attacker: Kali, 205.174.165.73
Victim: MAC, 192.168.10.25
Infiltration – Dropbox download
Win Vista (15:04 – 15:45 p.m.)
First Step:
Attacker: Kali, 205.174.165.73
Victim: Windows Vista, 192.168.10.8
Second Step (Portscan + Nmap):
Attacker:Vista, 192.168.10.8
Victim: All other clients
Friday, July 7, 2017
Morning
Botnet ARES (10:02 a.m. – 11:02 a.m.)
Attacker: Kali, 205.174.165.73
Victims: Win 10, 192.168.10.15 + Win 7, 192.168.10.9 + Win 10, 192.168.10.14 + Win 8, 192.168.10.5 + Vista, 192.168.10.8
Afternoon
Port Scan:
Firewall Rule on (13:55 – 13:57, 13:58 – 14:00, 14:01 – 14:04, 14:05 – 14:07, 14:08 - 14:10, 14:11 – 14:13, 14:14 – 14:16, 14:17 – 14:19, 14:20 – 14:21, 14:22 – 14:24, 14:33 – 14:33, 14:35 - 14:35)
Firewall rules off (sS 14:51-14:53, sT 14:54-14:56, sF 14:57-14:59, sX 15:00-15:02, sN 15:03-15:05, sP 15:06-15:07, sV 15:08-15:10, sU 15:11-15:12, sO 15:13-15:15, sA 15:16-15:18, sW 15:19-15:21, sR 15:22-15:24, sL 15:25-15:25, sI 15:26-15:27, b 15:28-15:29)
Attacker: Kali, 205.174.165.73
Victim: Ubuntu16, 205.174.165.68 (Local IP: 192.168.10.50)
NAT Process on Firewall:
Attacker: 205.174.165.73 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.1
Afternoon
DDoS LOIT (15:56 – 16:16)
Attackers: Three Win 8.1, 205.174.165.69 - 71
Victim: Ubuntu16, 205.174.165.68 (Local IP: 192.168.10.50)
NAT Process on Firewall:
Attackers: 205.174.165.69, 70, 71 -> 205.174.165.80 (IP Valid Firewall) -> 172.16.0.1
License
The CICIDS2017 dataset consists of labeled network flows, including full packet payloads in pcap format, the corresponding profiles and the labeled flows (GeneratedLabelledFlows.zip) and CSV files for machine and deep learning purpose (MachineLearningCSV.zip) are publicly available for researchers. If you are using our dataset, you should cite our related paper which outlining the details of the dataset and its underlying principles:
Iman Sharafaldin, Arash Habibi Lashkari, and Ali A. Ghorbani, “Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization”, 4th International Conference on Information Systems Security and Privacy (ICISSP), Portugal, January 2018"	https://paperswithcode.com/dataset/cicids2017		Intrusion Detection Evaluation Dataset (CIC-IDS2017)					
4274	zbMATH Open dataset 2021	zbMATH Open contains over 4 million bibliographic entries with reviews or abstracts drawn from more than 3.000 journals and book series and more than 190.000 books.	https://paperswithcode.com/dataset/zbmath-open-dataset-2021	08/06/2021						
4275	Photozilla	Photozilla is a large-scale dataset which includes over 990k images belonging to 10 different photographic styles. The dataset can be used to train classification models to automatically classify the images into the relevant style.	https://paperswithcode.com/dataset/photozilla	21/06/2021						
4276	Text-to-3D House Model	The dataset contains 2,000 houses, 13,478 rooms and 873 (some rooms have same textures so this number is smaller than the total number of rooms.) texture images with corresponding natural language descriptions. These descriptions are firstly generated from some pre-defined templates and then refined by human workers. The average length of the description is 173.73 and there are 193 unique words. In our experiments, we use 1,600 pairs for training while 400 for testing in the building layout generation. For texture synthesis, we use 503 data for training and 370 data for testing.	https://paperswithcode.com/dataset/text-to-3d-house-model	01/03/2020	Text--to--3D House Model					
4277	EchoCP	"EchoCP is an echocardiography dataset in cTTE targeting PFO (Patent foramen ovale) diagnosis. EchoCP consists of 30 patients with both rest and Valsalva maneuver videos which covers various PFO grades.
Patent foramen ovale (PFO) is a potential separation between the septum, primum and septum secundum located in the anterosuperior portion of the atrial septum. PFO is one of the main factors causing cryptogenic stroke which is the fifth leading cause of death in the United States. For PFO diagnosis, contrast transthoracic echocardiography (cTTE) is preferred as being a more robust method compared with others."	https://paperswithcode.com/dataset/echocp	18/05/2021						
4278	WikiPII	WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction.	https://paperswithcode.com/dataset/wikipii	19/05/2021						
4279	NVGaze	Quality, diversity, and size of training dataset are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (two million images at 1280x960), and a real-world dataset collected with 35 subjects (2.5 million images at 640x480). Using our datasets, we train a neural network for gaze estimation, achieving 2.06 (+/- 0.44) degrees of accuracy across a wide 30 x 40 degrees field of view on real subjects excluded from training and 0.5 degrees best-case accuracy (across the same field of view) when explicitly trained for one real subject. We also train a variant of our network to perform pupil estimation, showing higher robustness than previous methods. Our network requires fewer convolutional layers than previous networks, achieving sub-millisecond latency.	https://paperswithcode.com/dataset/nvgaze	04/05/2019	NVGaze: An Anatomically-Informed Dataset for Low-Latency, Near-Eye Gaze Estimation					
4280	UA-GEC	UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language	https://paperswithcode.com/dataset/ua-gec	31/03/2021	UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language					
4281	InFashAI	"AI algorithms, and in particular Machine Learning (ML) algorithms, learn from data tasks that have been traditionally done by humans such as: image classification, facial recognition, linguistic translation etc. To have a good generalization capability, AI algorithms must learn from sufficiently representative data, which is unfortunately not often the case. This results in a hyper-specialization of AI and its inability to perform well on new data whose distribution is too far from the one of the training set. It raises ethical questions which will undoubtedly have direct or indirect consequences on society. However, and despite biases they can entail, AI technologies are revolutionizing virtually every industry, and are forcing players in those industries to reinvent their businesses.
Like many industries, the fashion industy is being hit hard by exponential advances in AI. AI technologies are now able to describe a dress model, extract its attributes (color, style, type of sleeve, etc.), predict future fashion trends and even offer personalized clothing styles. Several publicly large datasets of fashion images made it possible.
However, the lack of diversity in available datasets is palpable. For example, images of African fashion are almost absent from these datasets. It raises problems in terms of ability to generalize algorithms trained on those datasets to African styles for instance, and therefore it limits the adoption of AI technologies within the African fashion industry.
Therefore, for an inclusive AI in the field of fashion, and to ensure that African fashion can benefit from the potentials of AI, Ai4Innov initiated the Inclusive Fashion AI project (InFashAI) which aims to create datasets that are much more representative of the diversity that exists in the world of fashion. We will first focus on building up a large volume of data on African fashion. This dataset will be progressively open source and, we hope, will be the backbone for AI tools adapted to African fashion."	https://paperswithcode.com/dataset/infashai	23/06/2021	Inclusive Fashion AI					
4282	PNPB dataset	"The dataset consists of a total of 20 videos, each of which is 5.5 minutes long in duration. The videos are captured at a resolution of 1024x1024 and at 30 frames per second. Each video contains only one pig performing the Novel Object Recognition task.
It contains annotations for the following tasks:
Action Recognition: Time intervals for object investigations made by the pig were manually annotated.
Keypoint Detection: The base of the tail, the tip of the nose, the crown of the head, and a bounding box for the whole pig were annotated for 668 frames."	https://paperswithcode.com/dataset/pnpb-dataset	23/06/2021	Pig Novelty Preference Behavior dataset					
4283	Place Pulse 2.0	"Place Pulse is a crowdsourcing effort that aims to map which areas of a city are perceived as safer, livelier, wealthier, more active, beautiful and friendly. By asking users to select images from a pair, Place Pulse collected more than 1.5 million reports that evaluate more than 100,000 images from 56 cities.
Source: https://figshare.com/articles/dataset/Place_Pulse/11859993
Image source: https://www.media.mit.edu/projects/place-pulse-1/overview/"	https://paperswithcode.com/dataset/place-pulse-2-0	05/08/2016						
4284	Russian Event2Mind	"The work provides a comprehensive overview of the corpus for the Russian language for the commonsense inference task. Namely, we construct event phrases, which cover a wide range of everyday situations with labelled intents and reactions of the event main participant and emotions of other people involved. 
Example:

The main contribution of the paper is a text corpus suitable for event2mind training in Russian which consists of two parts:
1. 6,756 event phrases covering a diverse range of everyday events and situations in Russian,
2. a subset of 23,409 event phrases from English corpus translated via Google
translator."	https://paperswithcode.com/dataset/russian-event2mind	03/02/2020						
4285	Taiga Corpus	"Taiga is a corpus, where text sources and their meta-information are collected according to popular ML tasks.
Each text in corpus is represented in plain text and with morphological and syntactic annotation (UDPipe, homonymy resolved automatically) + has metainformation - date, theme, authorship, text difficulcy…etc (depending on source)
By now, about 5 billions of words are 77% literary texts (33 literary magazines), 19% of naive poetry, 2% of news (4 popular sites) and 2% of other (popular science, culture mags, social networks, amateur poems and prose), with documentation available.
Segments Info
| Genres           | Tokens, millions | %   |
|------------------|------------------|-----|
| News             | 92               | 1.5 |
| Literary Texts   | 4605             | 76  |
| Special datasets | 2.5              | 0.5 |
| Social media     | 80               | 1.5 |
| Subtitles        | 101              | 1.5 |
| Poems            | 1130             | 19  |
Annotation Example 
(CONLL-u):
```
newdoc
newpar
sent_id = 1
2003Armeniya.xml 1
text = В советский период времени число ИТ- специалистов в Армении составляло около десяти тысяч.
sent_id = 1
1   В   в   ADP _   _   3   case    3:case  _
2   советский   советский   ADJ _   Animacy=Inan|Case=Acc|Degree=Pos|Gender=Masc|Number=Sing    3   amod    3:amod  _
3   период  период  NOUN    _   Animacy=Inan|Case=Acc|Gender=Masc|Number=Sing   11  obl 11:obl  _
4   времени время   NOUN    _   Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing   3   nmod    3:nmod  _
5   число   число   NOUN    _   Animacy=Inan|Case=Acc|Gender=Neut|Number=Sing   11  obj 11:obj  _
6   ИТ  ит  PROPN   _   Animacy=Inan|Case=Nom|Gender=Neut|Number=Sing   8   compound    8:compound  SpaceAfter=No
7   -   -   PUNCT   _   _   6   punct   6:punct _
8   специалистов    специалист  NOUN    _   Animacy=Anim|Case=Gen|Gender=Masc|Number=Plur   5   nmod    5:nmod  _
9   в   в   ADP _   _   10  case    10:case _
10  Армении армения PROPN   _   Animacy=Inan|Case=Loc|Gender=Fem|Number=Sing    5   nmod    5:nmod  _
11  составляло  составлять  VERB    _   Aspect=Imp|Gender=Neut|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act   0   root    0:root  _
12  около   около   ADP _   _   14  case    14:case _
13  десяти  десять  NUM _   Case=Gen    14  nummod  14:nummod   _
14  тысяч   тысяча  NOUN    _   Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur    11  nsubj   11:nsubj    SpaceAfter=No
15  .   .   PUNCT   _   _   14  punct   14:punct    _
```"	https://paperswithcode.com/dataset/taiga-corpus	01/07/2017	An open-source corpus for machine learning.					
4286	Morph Call	"Morph Call is a suite of 46 probing tasks for four Indo-European languages that fall under different morphology: Russian, French, English, and German. The tasks are designed to explore the morphosyntactic content of multilingual transformers which is a less studied aspect at the moment.
The tasks are divided into four groups:

Morphosyntactic Features: probe the encoder for the occurrence of the morphosyntactic properties.
Masked Token: analogous to Morphosyntactic Features with the exception that the target word is replaced with a tokenizer-specific mask token.
Morphosyntactic Values: is a group of k-way classification tasks for each feature where k is the number of values that the feature can take.
Perturbations: tasks test the encoder sensitivity to syntactic and inflectional sentence perturbations.

Probing Methods

Supervised probing involves training a Logistic Regression classifier to predict a property. The performance is used as a proxy to evaluate the model knowledge.
Neuron-level Analysis [Durrani et al., 2020] allows retrieving a group of individual neurons that are most relevant to predict a linguistic property.
Contextual Correlation Analysis [Wu et al., 2020] is a representation-level similarity measure that allows identifying pairs of layers of similar behavior. 

Usage
We provide an example of the experiment on Masked Token task (Case, German).
```
bash
me@my-laptop:~$ python3 probe.py --help
INFO: Showing help with the command 'probe.py -- --help'.
NAME
    probe.py - configure the experiment AND perform probing
SYNOPSIS
    probe.py <flags>
DESCRIPTION
    configure the experiment AND perform probing
FLAGS
    --results_path=RESULTS_PATH
        Type: Optional[str]
        Default: None
        path to a folder to store the probing results and the model intermediate activations
    --model_architecture=MODEL_ARCHITECTURE
        Type: typ...
        Default: 'bert multilingual'
    --model_is_finetuned=MODEL_IS_FINETUNED
        Type: bool
        Default: False
        if to perform the experiment on the fine-tuned model
    --model_finetuned_path=MODEL_FINETUNED_PATH
        Type: Optional[str]
        Default: None
        (only if model_is_finetuned is True) path to store the fine-tuned model
    --model_finetuned_config_google_url=MODEL_FINETUNED_CONFIG_GOOGLE_URL
        Type: Optional[]
        Default: None
        (only if model_is_finetuned is True) the url of the fine-tuned model config if to be downloaded
    --model_finetuned_model_google_url=MODEL_FINETUNED_MODEL_GOOGLE_URL
        Type: Optional[]
        Default: None
        (only if model_is_finetuned is True) the url of the fine-tuned model weights if to be downloaded
    --model_is_random=MODEL_IS_RANDOM
        Type: bool
        Default: False
        if to perform the random initialization of the model
    --layers_to_probe=LAYERS_TO_PROBE
        Type: List
        Default: 'all'
        (either ""all"" or list w. possible numbers from 0 to 11) -- model layers to probe. e.g.: [1, 3, 11], or ""all""
    --train_n_sentences=TRAIN_N_SENTENCES
        Type: int
        Default: 1500
        number of sentences used to train the probing classifier
    --test_n_sentences=TEST_N_SENTENCES
        Type: int
        Default: 1000
        number of sentences used to evaluate the probing classifier
    --dev_n_sentences=DEV_N_SENTENCES
        Type: int
        Default: 0
        DEPRECATED
```"	https://paperswithcode.com/dataset/morph-call	26/04/2021						
4287	UDIS-D	UDIS-D is a large image dataset for image stitching or image registration. It contains different overlap rates, varying degrees of parallax, and variable scenes such as indoor, outdoor, night, dark, snow, and zooming.	https://paperswithcode.com/dataset/udis-d	24/06/2021	Unsupervised Deep Image Stitching Dataset					
4288	BCR dataset	Blender Cycles Ray-tracing (BCR) dataset contains 2449 high-quality images rendered from 1463 models. We render the images at a range of spp rates, including 1-8, 12, 16, 32, 64, 250, 1000, and 4000 spp. All the images are rendered at the resolution of 1080p. Each image contains not only the final rendered result but also the intermediate render layers, including albedo, normal, diffuse, glossy, and so on.	https://paperswithcode.com/dataset/bcr-dataset	24/06/2021	Blender Cycles Ray-tracing dataset					
4289	Gharbi Dataset	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/gharbi-dataset							
4290	RuCoS	"Russian reading comprehension with Commonsense reasoning (RuCoS) is a large-scale reading comprehension dataset that requires commonsense reasoning. RuCoS consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of RuCoS is to evaluate a machine`s ability of commonsense reasoning in reading comprehension.
Example
{'source': 'Lenta',
   'passage': {
          'text':
          'Мать двух мальчиков, брошенных отцом в московском аэропорту Шереметьево, забрала их. Об этом сообщили ТАСС в пресс-службе министерства образования и науки Хабаровского края. Сейчас младший ребенок посещает детский сад, а старший ходит в школу. В учебных заведениях с ними по необходимости работают штатные психологи. Также министерство социальной защиты населения рассматривает вопрос о бесплатном оздоровлении детей в летнее время. Через несколько дней после того, как Виктор Гаврилов бросил своих детей в аэропорту, он явился с повинной к следователям в городе Батайске Ростовской области.\n@context\nБросившего детей в Шереметьево отца задержали за насилие над женой\n@context\nРоссиянина заподозрили в истязании брошенных в Шереметьево детей\n@context\nОставивший двоих детей в Шереметьево россиянин сам пришел к следователям',
          'entities': [
              {'start': 60, 'end': 71, 'text': 'Шереметьево'},
              {'start': 102, 'end': 106, 'text': 'ТАСС'},
              {'start': 155, 'end': 172, 'text': 'Хабаровского края'},
              {'start': 470, 'end': 485, 'text': 'Виктор Гаврилов'},
              {'start': 563, 'end': 571, 'text': 'Батайске'},
              {'start': 572, 'end': 590, 'text': 'Ростовской области'},
              {'start': 620, 'end': 631, 'text': 'Шереметьево'},
              {'start': 725, 'end': 736, 'text': 'Шереметьево'},
              {'start': 778, 'end': 789, 'text': 'Шереметьево'}
          ]
      },
      'qas': [
          {
              'query': '26 января @placeholder бросил сыновей в возрасте пяти и семи лет в Шереметьево.',
              'answers': [
                  {'start': 470, 'end': 485, 'text': 'Виктор Гаврилов'}
              ],
              'idx': 0
          }
      ],
      'idx': 0
  }
How did we collect data?
All text examples were collected from open news sources, then automatically filtered with QA systems to prevent obvious questions to infiltrate the dataset. The texts were then filtered by IPM frequency of the contained words and, finally, manually reviewed."	https://paperswithcode.com/dataset/rucos	11/06/2020	Russian Reading Comprehension with Commonsense Reasoning					
4291	DaNetQA	"DaNetQA is a question answering dataset for yes/no questions. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings.
Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The text-pair classification setup is similar to existing natural language inference tasks.
By sampling questions from a distribution of information-seeking queries (rather than prompting annotators for text pairs), we observe significantly more challenging examples compared to existing NLI datasets.
Example
{
  ""text"": ""В период с 1969 по 1972 год по программе «Аполлон» было выполнено 6 полётов с посадкой на Луне. Всего на Луне высаживались 12 астронавтов США. Список космонавтов Список космонавтов — участников орбитальных космических полётов Список астронавтов США — участников орбитальных космических полётов Список космонавтов СССР и России — участников космических полётов Список женщин-космонавтов Список космонавтов, посещавших МКС Энциклопедия астронавтики."",
  ""question"": ""Был ли человек на луне?"",
  ""label"": true,
  ""idx"": 5
}
How did we collect data?
All text examples were collected in accordance with the methodology for collecting the original dataset. Answers to the questions were received with the help of assessors, and texts were also received automatically using ODQA systems on Wikipedia. Human assessment was carried out on Yandex.Toloka.
*Additionally, to increase number of samples and the distribution of yes/no answers, we added extra data in the same format (data were collected from Yandex.Toloka while generating MuSeRC dataset)."	https://paperswithcode.com/dataset/danetqa	29/10/2020	Yes/no Question Answering Dataset for the Russian					
4292	RWSD	"A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution. The schema takes its name from a well-known example by Terry Winograd.
The set would then be presented as a challenge for AI programs, along the lines of the Turing test. The strengths of the challenge are that it is clear-cut, in that the answer to each schema is a binary choice; vivid, in that it is obvious to non-experts that a program that fails to get the right answers clearly has serious gaps in its understanding; and difficult, in that it is far beyond the current state of the art.
Task Type
Logic and Reasoning, World knowledge. Binary Classification: true/false
Example
{
  ""text"": ""Кубок не помещается в коричневый чемодан, потому что он слишком большой.""
  ""label"": false,
  ""idx"": 5,
  ""target"": {
    ""span1_text"": ""чемодан"",
    ""span2_text"": ""он слишком большой"",
    ""span1_index"": 5,
    ""span2_index"": 8
  },
}
How did we collect data?
All text examples were collected manually translating and adapting original Winograd dataset for Russian. Human assessment was carried out on Yandex.Toloka."	https://paperswithcode.com/dataset/rwsd	29/10/2020	The Winograd Schema Challenge (Russian)					
4293	RUSSE	"WiC: The Word-in-Context Dataset A reliable benchmark for the evaluation of context-sensitive word embeddings.
Depending on its context, an ambiguous word can refer to multiple, potentially unrelated, meanings. Mainstream static word embeddings, such as Word2vec and GloVe, are unable to reflect this dynamic semantic nature. Contextualised word embeddings are an attempt at addressing this limitation by computing dynamic representations for words which can adapt based on context.
Russian SuperGLUE task borrows original data from the Russe project, Word Sense Induction and Disambiguation shared task (2018)
Task Type
Reading Comprehension. Binary Classification: true/false
Example
{
  ""idx"" : 8,
  ""word"" : ""дорожка"",
  ""sentence1"" : ""Бурые ковровые дорожки заглушали шаги"",
  ""sentence2"" : ""Приятели решили выпить на дорожку в местном баре"",
  ""start1"" : 15,
  ""end1"" : 23,
  ""start2"" : 26,
  ""end2"" : 34,
  ""label"" : false,
  ""gold_sense1"" : 1,
  ""gold_sense2"" : 2
}
How did we collect data?
All text examples were collected from Russe original dataset, already collected by Russian Semantic Evaluation at ACL SIGSLAV. Human assessment was carried out on Yandex.Toloka.
In version 2, we have manually collected in the same format testset."	https://paperswithcode.com/dataset/russe	15/03/2018	Russian Words in Context (based on RUSSE)					
4294	TERRa	"Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.
Task Type
RTE (Recognizing Textual Entailment) Sentence Pair Classification - Entailment - Not Entailment
Example
{
  ""premise"": ""Автор поста написал в комментарии, что прорвалась канализация."",
  ""hypothesis"": ""Автор поста написал про канализацию."",
  ""label"": ""entailment"",
  ""idx"": ""6062""
}
How did we collect data?
All text examples were collected from open news sources and literary magazines, then manually reviewed and supplemented by a human assessment on Yandex.Toloka"	https://paperswithcode.com/dataset/terra	29/10/2020	Textual Entailment Recognition for Russian					
4295	MuSeRC	"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.
Task Type
Binary classification by each answer. True/False
Example
{
        ""id"": 397,
        ""text"": ""(1) Мужская сборная команда Норвегии по биатлону в рамках этапа Кубка мира в немецком Оберхофе выиграла эстафетную гонку. (2) Вторыми стали французы, а бронзу получила немецкая команда. (3) Российские биатлонисты не смогли побороться даже за четвертое место, отстав от норвежцев более чем на две минуты. (4) Это худший результат сборной России в текущем сезоне. (5) Четвёртыми в Оберхофе стали австрийцы. (6) В составе сборной Норвегии на четвёртый этап вышел легендарный Уле-Эйнар Бьорндален. (7) Впрочем, Норвегия с самого начала гонки была в числе лидеров, успешно проведя все четыре этапа. (8) За сборную России в Оберхофе выступали Иван Черезов, Антон Шипулин, Евгений Устюгов и Максим Чудов. (9) Гонка не задалась уже с самого начала: если на стрельбе из положения лежа Черезов был точен, то из положения стоя он допустил несколько промахов, в результате чего ему пришлось бежать один дополнительный круг. (10) После этого отставание российской команды от соперников только увеличивалось. (11) Напомним, что днем ранее российские биатлонистки выиграли свою эстафету. (12) В составе сборной России выступали Анна Богалий-Титовец, Анна Булыгина, Ольга Медведцева и Светлана Слепцова. (13) Они опередили своих основных соперниц - немок - всего на 0,3 секунды."",
        ""questions"": [
            {
                ""question"": ""На сколько секунд женская команда опередила своих соперниц?"",
                ""answers"": [
                    {
                        ""text"": ""Всего на 0,3 секунды."",
                        ""label"": 1
                    },
                    {
                        ""text"": ""На 0,3 секунды."",
                        ""label"": 1
                    },
                    {
                        ""text"": ""На секунду."",
                        ""label"": 0
                    },
                    {
                        ""text"": ""На 0.5 секунд."",
                        ""label"": 0
                    }
                ],
                ""idx"": 0
            }]
    }
How did we collect data?
Our challenge dataset contains ∼6k questions for +800 paragraphs across 5 different domains:

elementary school texts
news
fiction stories
fairy tales
summary of series

First, we have collected all data from open sources and automatically preprocessed them, filtered only those paragraphs that corresponding to the following parameters: 1) paragraph length 2) number of NER entities 3) number of coreference relations. Afterwords we have check the correct splitting on sentences and numerate each of them.
Next, in Yandex.Toloka we have generated the crowdsource task to get from tolkers information: 1) generate questions 2) generate answers 3) check that to solve every question man need more than one sentence in the text.
Principles

We exclude any question that can be answered based on a single sentence from a paragraph.
Answers are not written in the full match form in the text.
Answers to the questions are independent from each other. Their number can distinguish."	https://paperswithcode.com/dataset/muserc	29/10/2020	Russian Multi-Sentence Reading Comprehension					
4296	PARus	"Choice of Plausible Alternatives for Russian language (PARus) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. Each question in PARus is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%.
Task Type
Evaluation of commonsense causal reasoning
Sentence Pair Classification: suitable - not suitable
Example
{
  ""premise"": ""Гости вечеринки прятались за диваном."",
  ""choice1"": ""Это была вечеринка-сюрприз."",
  ""choice2"":""Это был день рождения."",
  ""question"": ""cause"",
  ""label"": 0,
  ""idx"": 4
}
How did we collect data?
All text examples were collected from open news sources and literary magazines, then manually reviewed and supplemented by a human assessment on Yandex.Toloka"	https://paperswithcode.com/dataset/parus	29/10/2020	Choice of Plausible Alternatives for Russian language					
4297	RCB	"The Russian Commitment Bank is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment cancelling operator (question, modal, negation, antecedent of conditional).
Task Type
RTE (Recognizing Textual Entailment) Sentence Pair Classification - Entailment - Contradiction - Neutral
Example
{
          ""premise"": ""Сумма ущерба составила одну тысячу рублей. Уточняется, что на место происшествия выехала следственная группа, которая установила личность злоумышленника. Им оказался местный житель, ранее судимый за подобное правонарушение."",
          ""hypothesis"": ""Ранее местный житель совершал подобное правонарушение."",
          ""verb"": ""судить"",
          ""negation"": ""no_negation"",
          ""label"": ""entailment"",
          ""idx"": 269
          }
How did we collect data?
All text examples were collected from open news sources and literary magazines, then manually reviewed and supplemented by a human assessment on Yandex.Toloka."	https://paperswithcode.com/dataset/rcb	29/10/2020	Russian Commitment Bank					
4298	LiDiRus	"LiDiRus is a diagnostic dataset that covers a large volume of linguistic phenomena, while allowing you to evaluate information systems on a simple test of textual entailment recognition. See more details diagnostics.
Task Type
RTE (Recognizing Textual Entailment) Sentence Pair Classification - Entailment - Not Entailment
Example
{
     'sentence1': ""Кошка сидела на коврике."",
     'sentence2': ""Кошка не сидела на коврике."",
     'label': 'not_entailment',
     'knowledge': '',
     'lexical-semantics': '',
     'logic': 'Negation',
     'predicate-argument-structure': ''
    }
How did we collect data?
All text examples manually translated and adapted from English SuperGLUE Diagnostics"	https://paperswithcode.com/dataset/lidirus	29/10/2020	Linguistic Diagnostic for Russian					
4299	AGORA	"AGORA is a synthetic human dataset with high realism and accurate ground truth. It consists of around 14K training and 3K test images by rendering between 5 and 15 people per image using either image-based lighting or rendered 3D environments, taking care to make the images physically plausible and photoreal. In total, AGORA contains 173K individual person crops.
AGORA provides (1) SMPL/SMPL-X parameters and (2) segmentation masks for each subject in images."	https://paperswithcode.com/dataset/agora	29/04/2021						
4300	synthetic_dataset.h5	The synethetic dataset (10000 pairs of images and region, 2.95GB) is shared with the code (hdf5 dataset format).	https://paperswithcode.com/dataset/synthetic-dataset-h5							
4301	Fast Linking Numbers of Loopy Structures Dataset	"Copyright (C) 2021 Ante Qu &#97;&#110;&#116;&#101;&#113;&#117;&#64;&#99;&#115;&#46;&#115;&#116;&#97;&#110;&#102;&#111;&#114;&#100;&#46;&#101;&#100;&#117;.
This is the dataset for this paper:
Ante Qu and Doug L. James. 2021. Fast Linking Numbers for Topology Verification
of Loopy Structures. ACM Trans. Graph. 40, 4, Article 106 (August 2021),
19 pages. https://doi.org/10.1145/3450626.3459778 
In particular, these files correspond to all the closed-loop examples in
Table 1, with the exception that we do not release the chevron 3x3 stitch
pattern because it is not a collection of closed loops. Most files are in the
Binary Curve Collection (BCC) file format. The open-sourced fast linking
numbers curve verification tool released with the paper uses this file format.
The chainmail, rubber band, and woundball examples came originally from Houdini
OBJ files, and we have included their original files as well.
The reference certificate for each [name].bcc is in
reference_certificates/[name].txt. As described in the code release, the first
line indicates the number of curves in the model, and every subsequent line is
a triplet consisting of two curve indices (0-based) and the linking number
between the two curves. For example, ""5,6,-2"" indicates that curves 5 and 6
have a linking number of -2 between them. Any curve pairs not included in a
triplet have a linking number of zero. If the certificate file only has the
first row, then all its curves are unlinked.
We also included a script in scripts/dataset_referencegen.sh to generate these
certificates. Simply edit the ""VERIFYCURVESPATH="" line to point to your
""verifycurves"" executable.
Here is a list of the table entries and their corresponding BCC filenames:

Alien Sweater (Initial):              alien_sweater_init.bcc
Alien Sweater (Final):                alien_sweater_final.bcc
Sheep Sweater:                        sheep_sweater.bcc
Sweater:                              sweater.bcc
Glove:                                glove.bcc
Knit Tube (Initial):                  knittubeinit.bcc
Knit Tube (Final):                    knittubefinal.bcc
Chainmail (Initial):                  chainmail_init.bcc
Chainmail (Final):                    chainmail_final.bcc
Rubber Bands:                        rubber_bands_final.bcc
Double-Helix Ribbon λ=10, 200K Segs: double_helix_ribbon_lambda10_N200K.bcc
Double-Helix Ribbon λ=1K, 200K Segs: double_helix_ribbon_lambda1K_N200K.bcc
Double-Helix Ribbon λ=10, 20M Segs:  double_helix_ribbon_lambda10_N20M.bcc
Double-Helix Ribbon λ=1K, 20M Segs:  double_helix_ribbon_lambda1K_N20M.bcc
Thick Square Link, 500K Segs:        thicksquarelink_N500K.bcc
Thick Square Link, 4M Segs:          thicksquarelink_N4M.bcc
Torus λ=1M, 20M Segs:                torus_lambda1M_N20M.bcc
Torus λ=0,  40M Segs:                torus_lambda0_N40M.bcc
Woundball ν=1K,  1M Segs:            woundball_nu1K_N1M.bcc
Woundball ν=10K, 2M Segs:            woundball_nu10K_N2M.bcc

Note that some input files are B-Splines, some are uniform Catmull–Rom splines,
and some are polylines. This information is encoded in the BCC files.
knittubebroken.bcc: For the Knit Tube, knittubeinit.bcc comes from simulation
step 0 and knittubefinal.bcc comes from simulation step 13000, and both states
are unlinked and topologically valid. We also added knittubebroken.bcc, which
comes from simulation step 13080, when it has a linkage violation.
rubber_bands_init.bcc: For the Rubber Bands, we also added
rubber_bands_init.bcc with the initial state of the rubber bands.
Interestingly, this configuration has zero potentially linked loop pairs.
The knitted yarn models (1 through 7) are modified from Cem Yuksel's Yarn-level
Cloth Models located at http://www.cemyuksel.com/research/yarnmodels/.
Download the dataset zip here."	https://paperswithcode.com/dataset/fast-linking-numbers-of-loopy-structures	23/06/2021						
4302	AIT-QA	"AIT-QA is a dataset for Table Question Answering (Table-QA) which is specific to the airline industry. The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings of major airline companies for the fiscal years 2017-2019. It also contains annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms.
Different from the Table QA dataset, the tables in this dataset have more complex layouts."	https://paperswithcode.com/dataset/ait-qa	24/06/2021	Airline Industry Table QA					
4303	JetNet	JetNet is a particle cloud dataset, containing gluon, top quark, light quark jets saved in .csv format.	https://paperswithcode.com/dataset/jetnet	22/06/2021						
4304	GPLA-12	GPLA-12 is a new acoustic leakage dataset of gas pipelines involving 12 categories over 684 training/testing acoustic signals. The acoustic leakage signals were collected on the basis of an intact gas pipe system with external artificial leakages, and then preprocessed with structured tailoring which are turned into GPLA-12. GPLA-12 dedicates to serve as a feature learning dataset for time-series tasks and classifications.	https://paperswithcode.com/dataset/gpla-12	19/06/2021						
4305	XAI-Bench	XAI-Bench is a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets released offer a wide variety of parameters that can be configured to simulate real-world data.	https://paperswithcode.com/dataset/xai-bench	23/06/2021						
4306	riboflavin	"The dataset contains 71 samples with (normalized) expression data for 4,088 genes. The response variable is the riboflavin production rate in Bacilluss subtilis. It may be used to construct a graphical model.
Introduced by Buhlmann, P., Kalisch, M., and Meier, L. (2014). High-dimensional statistics
with a view toward applications in biology. Annual Review of Statistics and Its
Application, 1(1), 255–278"	https://paperswithcode.com/dataset/riboflavin							
4307	RuShiftEval	RuShiftEval is a manually annotated lexical semantic change dataset for Russian. Its novelty is ensured by a single set of target words annotated for their diachronic semantic shifts across three time periods, while the previous work either used only two time periods, or different sets of target words.	https://paperswithcode.com/dataset/rushifteval	15/06/2021						
4308	LIVE Livestream	LIVE Livestream is a database for Video Quality Assessment (VQA), specifically designed for live streaming VQA research. The dataset is called the Laboratory for Image and Video Engineering (LIVE) Live stream Database. The LIVE Livestream Database includes 315 videos of 45 contents impaired by 6 types of distortions.	https://paperswithcode.com/dataset/live-livestream	15/06/2021						
4309	X-CSQA	X-CSQA is a multilingual dataset for Commonsense reasoning research, based on CSQA.	https://paperswithcode.com/dataset/x-csqa	13/06/2021						
4310	DDPM	The Deception Detection and Physiological Monitoring (DDPM) dataset captures an interview scenario in which the interviewee attempts to deceive the interviewer on selected responses. The interviewee is recorded in RGB, near-infrared, and long-wave infrared, along with cardiac pulse, blood oxygenation, and audio. After collection, data were annotated for interviewer/interviewee, curated, ground-truthed, and organized into train/test parts for a set of canonical deception detection experiments. The dataset contains almost 13 hours of recordings of 70 subjects, and over 8 million visible-light, near-infrared, and thermal video frames, along with appropriate meta, audio, and pulse oximeter data.	https://paperswithcode.com/dataset/ddpm	11/06/2021	Deception Detection and Physiological Monitoring					
4311	rSoccer	rSoccer is an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments.	https://paperswithcode.com/dataset/rsoccer	15/06/2021						
4312	VideoMatting108	VideoMatting108 is a large-scale video matting and trimap generation dataset with 80 training and 28 validation foreground video clips with ground-truth alpha mattes.	https://paperswithcode.com/dataset/videomatting108	24/05/2021						
4313	Vāksañcayaḥ	This Sanskrit speech corpus has more than 78 hours of audio data and contains recordings of 45,953 sentences with a sampling rate of 22KHz. The content is mainly readings of texts spanning over various Śāstras of Saṃskṛtam literature and also includes contemporary stories, radio program, extempore discourse, etc.	https://paperswithcode.com/dataset/vaksancayah	02/06/2021	Sanskrit Speech Corpus by IIT Bombay					
4314	ADNI	"Alzheimer's Disease Neuroimaging Initiative (ADNI) is a multisite study that aims to improve clinical trials for the prevention and treatment of Alzheimer’s disease (AD).1 This cooperative study combines expertise and funding from the private and public sector to study subjects with AD, as well as those who may develop AD and controls with no signs of cognitive impairment.2 Researchers at 63 sites in the US and Canada track the progression of AD in the human brain with neuroimaging, biochemical, and genetic biological markers.2 This knowledge helps to find better clinical trials for the prevention and treatment of AD. ADNI has made a global impact,4
 firstly by developing a set of standardized protocols to allow the comparison of results from multiple centers,4 and secondly by its data-sharing policy which makes available all at the data without embargo to qualified researchers worldwide.5 To date, over 1000 scientific publications have used ADNI data.6 A number of other initiatives related to AD and other diseases have been designed and implemented using ADNI as a model.4 ADNI has been running since 2004 and is currently funded until 2021.7
Source: Wikipedia, https://en.wikipedia.org/wiki/Alzheimer%27s_Disease_Neuroimaging_Initiative"	https://paperswithcode.com/dataset/adni		Alzheimer's Disease NeuroImaging Initiative					
4315	Ambiguous-HOI	Ambiguous-HOI is a challenging dataset containing ambiguous human-object interaction images for HOI detection based on HICO-DET.	https://paperswithcode.com/dataset/ambiguous-hoi	17/04/2020						
4316	DeepBeam	"It contains 19 HDF5 files that represent a data collection campaign run on the NI mmWave Transceiver System with four SiBeam 60 GHz radio heads and on two Pi-Radio digital 60 GHz radios.
Please refer to the website deepbeam.net
Source: https://github.com/wineslab/deepbeam"	https://paperswithcode.com/dataset/deepbeam	28/12/2020						
4317	Amharic Error Corpus	Amharic Error Corpus is a manually annotated spelling error corpus for Amharic, lingua franca in Ethiopia. The corpus is designed to be used for the evaluation of spelling error detection and correction. The misspellings are tagged as non-word and real-word errors. In addition, the contextual information available in the corpus makes it useful in dealing with both types of spelling errors.	https://paperswithcode.com/dataset/amharic-error-corpus	25/06/2021						
4318	BarkNet 1.0	23,000 cropped images of tree bark, for 23 species of trees around Quebec City, Canada. The images were captured at a distance between 20-60 cm away from the trunk. Labels include: individual tree ID, its species, and its DBH (diameter at breast height). Pictures were taken with four different devices: Nexus 5, Samsung Galaxy S5, Samsung Galaxy S7, and a Panasonic Lumix DMC-TS5 camera. The dataset is sufficiently large to train a Deep network such as ResNet for species recognition.	https://paperswithcode.com/dataset/barknet-1-0	02/03/2018						
4319	Epilepsy seizure prediction	"The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds.
We divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time. So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}.
The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178
y contains the category of the 178-dimensional input vector. Specifically y in {1, 2, 3, 4, 5}:
5 - eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open
4 - eyes closed, means when they were recording the EEG signal the patient had their eyes closed
3 - Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area
2 - They recorder the EEG from the area where the tumor was located
1 - Recording of seizure activity
All subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure. Our motivation for creating this version of the data was to simplify access to the data via the creation of a .csv version of it. Although there are 5 classes most authors have done binary classification, namely class 1 (Epileptic seizure) against the rest."	https://paperswithcode.com/dataset/epilepsy-seizure-prediction							
4320	SymbolicData	"This dataset is a collection of input-label pairs where each input is in the form of a numerical dataset, itself a set of input and output pairs {(x, y)}, and the corresponding label is a string encoding the symbolic expression governing the relationship between variables in the numerical dataset.
Source: SymbolicGPT: A Generative Transformer Model for Symbolic Regression"	https://paperswithcode.com/dataset/symbolicdata	27/06/2021	A Tree-based Symbolic Dataset For Symbolic Regression					
4321	RadGraph	"RadGraph is a dataset of entities and relations in radiology reports based on our novel information extraction schema, consisting of 600 reports with 30K radiologist annotations and 221K reports with 10.5M automatically generated annotations.
We release a development dataset, which contains board-certified radiologist annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and a test dataset, which contains two independent sets of board-certified radiologist annotations for 100 radiology reports split equally across the MIMIC-CXR and CheXpert datasets. We also release an inference dataset, which contains automatically generated annotations for 220,763 MIMIC-CXR reports (around 6 million entities and 4 million relations) and 500 CheXpert reports (13,783 entities and 9,908 relations) with mappings to associated chest radiographs."	https://paperswithcode.com/dataset/radgraph	28/06/2021	RadGraph: Extracting Clinical Entities and Relations from Radiology Reports					
4322	CASIA-Iris-Complex	"Introduction
Iris is considered one of the most accurate and reliable biometric modality. Iris is more stable and distinctive compared with fingerprint, face, voice, etc, and difficult to be replicated for spoof attacks. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from laboratory to field application is still a challenging task. In practical applications, the iris recognition system must face various unpredictable iris image degraded. For example, recognition of low-quality iris images, non-cooperative iris images, long-range iris images, and moving iris images are all huge problems in iris recognition. We believe that the first step in solving these problems is to design and develop a database of iris images that includes all of these degraded.
Brief Descriptions and Statistics of the Database
CASIA-Iris-Complex contains 22,932 images from 292 Asian subjects. It includes two subsets: CASIA-Iris-CX1 and CASIA-Iris-CX2. All images were collected under NIR illumination and two eyes were captured simultaneously."	https://paperswithcode.com/dataset/casia-iris-complex	01/09/2020	CASIA-Iris-Complex					
4323	Extended YouTube Faces (E-YTF)	The proposed Extended-YouTube Faces (E-YTF) is an extension of the famous YouTube Faces (YTF) dataset and is specifically designed to further push the challenges of face recognition by addressing the problem of open-set face identification from heterogeneous data i.e. still images vs video.	https://paperswithcode.com/dataset/extended-youtube-faces-e-ytf	01/01/2018						
4324	Amazon-PQA	Amazon-PQA is a product question-answer dataset. The Amazon-PQA dataset includes questions and their answers that are published on Amazon website, along with the public product information and category (Amazon Browse Node name). It contains more than 8M questions from 1M+ products.	https://paperswithcode.com/dataset/amazon-pqa	19/05/2021						
4325	SSL	This is a dataset to benchmark real-time embedded object detection models for RoboCup SSL (Small Size League).	https://paperswithcode.com/dataset/ssl	28/06/2021	Small Size League					
4326	FilmStills	FilmStills is a dataset of stills taken from a variety of films and TV shows, each concatenated with a color-compressed (with a factor of 2.667) version of itself.	https://paperswithcode.com/dataset/filmstills	19/06/2021						
4327	LCO CR Dataset	"Cosmic rays in the LCO CR dataset are labeled accurately and consistently across many diverse observations from various instruments. To the best of our knowledge, this is the largest dataset of its kind. It consists of over 4,500 scientific images from Las Cumbres Observatory global telescope network's 23 instruments. Each sample in our dataset is a multi-extension FITS file, including three images, three corresponding CR masks, and three ignore masks. 
Usage: https://github.com/cy-xu/cosmic-conn
For technical questions regarding the Cosmic-CoNN LCO CR dataset, please contact cxu@ucsb.edu.
Licensing of images and data
All images and data derived from observations made at LCO facilities are made available for scientific or educational use, subject to the Creative Commons license BY-CC 2.0 and in the case of science data, subject to an additional proprietary period. This allows for all LCO data and images, which are not subject to a proprietary restriction, to be freely shared and redistributed on the condition that an appropriate attribution is made.
Any use of LCO images and data not for scientific or educational purposes, e.g. for commercial purposes, is not permitted unless through an explicit arrangement. Please contact us at image_use@lco.global.
Acknowledgments and Citations
Any scientific publication which results from the use of LCO facilities should include an acknowledgment of this resource:
""This work makes use of observations from the Las Cumbres Observatory global telescope network.""
Contact
If you would like to use our images for commercial purposes, or if further information or assistance is needed, please contact image_use@lco.global."	https://paperswithcode.com/dataset/lco-cr-dataset	28/06/2021	Las Cumbres Observatory Cosmic Ray Dataset					
4328	Message Content Rephrasing	We introduce a new task of rephrasing for amore natural virtual assistant. Currently, vir-tual assistants work in the paradigm of intent-slot tagging and the slot values are directlypassed as-is to the execution engine. However,this setup fails in some scenarios such as mes-saging when the query given by the user needsto be changed before repeating it or sending itto another user. For example, for queries like‘ask my wife if she can pick up the kids’ or ‘re-mind me to take my pills’, we need to rephrasethe content to ‘can you pick up the kids’ and‘take your pills’. In this paper, we study theproblem of rephrasing with messaging as ause case and release a dataset of 3000 pairs oforiginal query and rephrased query. We showthat BART, a pre-trained transformers-basedmasked language model with auto-regressivedecoding, is a strong baseline for the task, andshow improvements by adding a copy-pointerand copy loss to it. We analyze different trade-offs of BART-based and LSTM-based seq2seqmodels, and propose a distilled LSTM-basedseq2seq as the best practical model.	https://paperswithcode.com/dataset/message-content-rephrasing	03/11/2020						
4329	TAU-NIGENS Spatial Sound Events 2021	The TAU-NIGENS Spatial Sound Events 2021 dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a single direction-of-arrival (DoA) if static, a trajectory DoAs if moving, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the NIGENS general sound events database. These recordings serve as the development dataset for the DCASE 2021 Sound Event Localization and Detection Task of the DCASE 2021 Challenge.	https://paperswithcode.com/dataset/tau-nigens-spatial-sound-events-2021	28/02/2021	TAU-NIGENS Spatial Sound Events 2021					
4330	PAD	PAD (Purpose-driven Affordance Dataset) is a dataset for affordance detection, which refers to identifying the potential action possibilities of objects in an image, which is an important ability for robot perception and manipulation. The dataset consists of 4K images from 31 affordance and 72 object categories.	https://paperswithcode.com/dataset/pad	28/06/2021	Purpose-driven Affordance Dataset					
4331	XL-Sum	XL-Sum is a comprehensive and diverse dataset for abstractive summarization comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation.	https://paperswithcode.com/dataset/xl-sum	25/06/2021						
4332	TNCR Dataset	"We present TNCR, a new table dataset with varying image quality collected from free open source websites. TNCR dataset can be used for table detection in scanned document images and their classification into 5 different classes.
TNCR contains 9428 high-quality labeled images. In this paper, we have implemented state-of-the-art deep learning-based methods for table detection to create several strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone Network achieves the highest performance compared to other methods with a precision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset. We have made TNCR open source in the hope of encouraging more deep learning approaches to table detection, classification and structure recognition.
Image source: https://github.com/abdoelsayed2016/TNCR_Dataset"	https://paperswithcode.com/dataset/tncr-dataset	19/06/2021	Table Net Detection and Classification Dataset					
4333	HKR	"The database is written in Cyrillic and shares the same 33 characters. Besides these characters, the Kazakh alphabet also contains 9 additional specific characters. This dataset is a collection of forms. The sources of all the forms in the datasets were generated by LATEX which subsequently was filled out by persons with their handwriting. The database consists of more than 1400 filled forms. There are approximately 63000 sentences, more than 715699 symbols produced by approximately 200 diferent writers. We utilized three different datasets described as following:
Handwritten samples (Forms) of keywords in Kazakh and Russian (Areas, Cities , Village , etc.)
Handwritten Kazakh and Russian alphabet in cyrillic
Handwritten samples (Forms) of poems in Russian
Image source: https://github.com/abdoelsayed2016/HKR_Dataset"	https://paperswithcode.com/dataset/hkr	07/07/2020	Handwritten Kazakh and Russian (HKR) Database for Text Recognition					
4334	HT Docking	HT Docking is a dataset consisting of 200 million 3D complex structures and 2D structure scores across a consistent set of 13 million ``in-stock'' molecules over 15 receptors, or binding sites, across the SARS-CoV-2 proteome. It is used to study surrogate model accuracy for protein-ligand docking.	https://paperswithcode.com/dataset/ht-docking	13/06/2021						
4335	PointQA	PointQA is a set of datasets for Visual Question Datasets (VQA) that require a pointer to an object in the image to be answered correctly. The different datasets are: PointQA-Local, PointQA-LookTwice and PointQA-General.	https://paperswithcode.com/dataset/pointqa	27/11/2020						
4336	TinyFace	TinyFace is a large scale face recognition benchmark to facilitate the investigation of natively LRFR (Low Resolution Face Recognition) at large scales (large gallery population sizes) in deep learning. The TinyFace dataset consists of 5,139 labelled facial identities given by 169,403 native LR face images (average 20×16 pixels) designed for 1:N recognition test. All the LR faces in TinyFace are collected from public web data across a large variety of imaging scenarios, captured under uncontrolled viewing conditions in pose, illumination, occlusion and background.	https://paperswithcode.com/dataset/tinyface	21/11/2018						
4337	FB15K237-Refined	FB15K237-Refined is a refined version of FB15k237 by KGRefiner.	https://paperswithcode.com/dataset/fb15k237-refined	29/06/2021						
4338	WN18RR Refined	WN18RR Refined is a refined version of WN18RR by KGRefiner	https://paperswithcode.com/dataset/wn18rr-refined							
4339	CHORD	CHORD is the first chorus recognition dataset containing 627 songs for public use.	https://paperswithcode.com/dataset/chord	27/06/2021	CHOrus Recognition Dataset					
4340	TrajAir: A General Aviation Trajectory Dataset	"This dataset contains aircraft trajectories in an untowered terminal airspace collected over 8 months surrounding the Pittsburgh-Butler Regional Airport [ICAO:KBTP], a single runway GA airport, 10 miles North of the city of Pittsburgh, Pennsylvania. The trajectory data is recorded using an on-site setup that includes an ADS-B receiver. The trajectory data provided spans days from 18 Sept 2020 till 23 Apr 2021 and includes a total of 111 days of data discounting downtime, repairs, and bad weather days with no traffic. Data is collected starting at 1:00 AM local time to 11:00 PM local time. The dataset uses an Automatic Dependent Surveillance-Broadcast (ADS-B) receiver placed within the airport premises to capture the trajectory data. The receiver uses both the 1090 MHz and 978 MHz frequencies to listen to these broadcasts. The ADS-B uses satellite navigation to produce accurate location and timestamp for the targets which is recorded on-site using our custom setup. Weather data during the data collection time period is also included for environmental context. The weather data is obtained post-hoc using the METeorological Aerodrome Reports (METAR) strings generated by the Automated Weather Observing System (AWOS) system at KBTP. The raw METAR string is then appended to the raw trajectory data by matching the closest UTC timestamps.
We also provide processed data that filters, interpolates and transforms data from a global frame to an airport-centred inertial frame. The inertial frame is centred at one end of the runway with the x-axis along the runway. Trajectories are filtered with aircrafts under 6000 ft MSL and around a 5km radius around the airport origin. We also remove duplicates and interpolate data every second. The proceed files also contain wind-data; a crucial factor in decision-making; separated in components along and perpendicular to the runway direction."	https://paperswithcode.com/dataset/trajair	30/06/2021						
4341	PathQuestion	"Adopts two subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. Paths are extracted between two entities which span two hops (es → r1 → e1 → r2 → a, denoted by -2H) or three hops (es→ r1 → e1 →r2 → e2→ r3 → a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, paraphrasing templates and synonyms for relations are included by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched.
Source: An Interpretable Reasoning Network for Multi-Relation Question Answering"	https://paperswithcode.com/dataset/pathquestion	15/01/2018						
4342	PELD	"PELD is a text-based emotional dialog dataset with personality traits for speakers.
The dialogues in PELD are merged from the emotional dialogues in MELD and EmoryNLP , as well as the personality trait annotations from FriendsPersona.
The personality traits in PELD are adopted from the personality annotations in 711 different dialogues in FriendsPersona. Refer to the annotations, a role may exhibit different aspects of its personality in different dialogues.
We only keep the personality traits of the six main roles for confidence as their annotations are most frequent. For each of the main roles, we average their annotated personality traits in all the dialogues by $P_n = \frac{1}{K}\sum_{i=1}^K{P_i}$ for simplification, where $K$ is the number of annotations.
Source: https://github.com/preke/PELD"	https://paperswithcode.com/dataset/peld	30/06/2021						
4343	VoxLingua107	VoxLingua107 is a dataset for spoken language recognition of 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances.	https://paperswithcode.com/dataset/voxlingua107-1	25/11/2020						
4344	MultiSubs	"MultiSubs is a dataset of multilingual subtitles gathered from the OPUS OpenSubtitles dataset, which in turn was sourced from opensubtitles.org. We have supplemented some text fragments (visually salient nouns in this release) within the subtitles with web images, where the word sense of the fragment has been disambiguated using a cross-lingual approach. We have introduced a fill-in-the-blank task and a lexical translation task to demonstrate the utility of the dataset. Please refer to our paper for a more detailed description of the dataset and tasks. Multisubs will benefit research on visual grounding of words especially in the context of free-form sentence.
Josiah Wang, Pranava Madhyastha, Josiel Figueiredo, Chiraag Lala, Lucia Specia (2021). MultiSubs: A Large-scale Multimodal and Multilingual Dataset. CoRR, abs/2103.01910. Available at: [https://arxiv.org/abs/2103.01910]
(https://arxiv.org/abs/2103.01910)"	https://paperswithcode.com/dataset/multisubs	30/06/2021	MultiSubs: A Large-scale Multimodal and Multilingual Dataset					
4345	ISPRS Potsdam	"The data set contains 38 patches (of the same size), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic.
Source: ISPRS"	https://paperswithcode.com/dataset/isprs-potsdam		2D Semantic Labeling Contest - Potsdam					
4346	ISPRS Vaihingen	"The data set contains 33 patches (of different sizes), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic.
Source: ISPRS"	https://paperswithcode.com/dataset/isprs-vaihingen		2D Semantic Labeling - Vaihingen data					
4347	BrazilDam Dataset	BrazilDAM is a multi sensor and multitemporal dataset that consists of multispectral images of ore tailings dams throughout Brazil. Landsat 8 and Sentinel 2 satellites that capture multispectral images over the years 2016, 2017, 2018 and 2019 were used. The dataset contains samples collected in different regions, which increases the diversity and representativeness of the characteristics of the dams.	https://paperswithcode.com/dataset/brazildam-dataset	17/03/2020						
4348	Brazilian Coffee Scenes Dataset	This dataset is a composition of scenes taken by SPOT sensor in 2005 over four counties in the State of Minas Gerais, Brazil: Arceburgo, Guaranesia, Guaxupé and Monte Santo. It has many intraclass variance caused by different crop management techniques. Also, coffee is an evergreen culture and the South of Minas Gerais is a mountainous region, which means that this dataset includes scenes with different plant ages and/or with spectral distortions caused by shadows.	https://paperswithcode.com/dataset/brazilian-coffee-scenes-dataset							
4349	SinGAN-Seg-polyps	SinGAN-Seg-polyps is a synthetic dataset for polyp segmentation consisting of 10,000 synthetic polyps and masks.	https://paperswithcode.com/dataset/singan-seg-polyps	29/06/2021						
4350	XWINO	"XWINO is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual
commonsense reasoning capabilities. 
The datasets that comprise XWINO are:

Source: The original Winograd Schema Challenge (Levesque, 2012);
Source: Additional data from the SuperGLUE WSC benchmark (Wang et al., 2019);
Source: The Definite Pronoun Resolution dataset (Rahman and Ng, 2012) (accessed from https://github.com/Yre/wsc_naive);
Source: A collection of French Winograd Schemas (Amsili and Seminck, 2017);
Source: Japanese translation of Winograd Schema Challenge (柴田知秀 et al., 2015);
Source: Russian Winograd Schema Challenge (Shavrina et al., 2020);
Source: A collection of Winograd Schemas in Chinese;
Source: Winograd Schemas in Portuguese (Melo et al., 2019)."	https://paperswithcode.com/dataset/xwino	22/06/2021						
4351	NNE	NNE is a dataset for Nested Named Entity Recognition in English Newswire	https://paperswithcode.com/dataset/nne	04/06/2019						
4352	Antibody Watch	Antibody Watch is a dataset of text snippets extracted from over 2000 PubMed articles with annotations denoting specificity of antibodies.	https://paperswithcode.com/dataset/antibody-watch	05/08/2020						
4353	COVID-19 & Election	"These datasets were used in the paper 'Evaluation of Thematic Coherence in Microblogs' (ACL, 2021). The data is structured as follows: each file represents a cluster of tweets which contains the tweet IDs, the journalist annotations for quality evaluation and issue identification, as well as the metric evaluation scores. Note that a set of 50 clusters, equally split between COVID-19 and Election domains, is shared between the 3 annotators and thus contains 3 labels.
Each cluster of tweets is evaluated for its thematic coherence quality (3-point scale) and for its issue identification (Intruded, Chained or Random). For more information about the annotation scheme, please refer to the complete annotation guidelines (available at https://doi.org/10.6084/m9.figshare.14703471) or the paper.
Potential uses for these datasets are in the evaluation of thematic coherence, topic modelling and text summarisation fields."	https://paperswithcode.com/dataset/covid-19-election	30/06/2021						
4354	ZooScanNet	Plankton was sampled with various nets, from bottom or 500m depth to the surface, in many oceans of the world. Samples were imaged with a ZooScan. The full images were processed with ZooProcess which generated regions of interest (ROIs) around each individual object and a set of associated features measured on the object (see Gorsky et al 2010 for more information). The same objects were re-processed to compute features with the scikit-image toolbox (http://scikit-image.org). The 1,433,278 resulting objects were sorted by a limited number of operators, following a common taxonomic guide, into 93 taxa, using the web application EcoTaxa (http://ecotaxa.obs-vlfr.fr).	https://paperswithcode.com/dataset/zooscannet	03/07/2018	ZooScanNet: plankton images captured with the ZooScan					
4355	iMiGUE	"iMiGUE is a dataset for emotional artificial intelligence research: identity-free video dataset for Micro-Gesture Understanding and Emotion analysis (iMiGUE). Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity information, while the predominant researches of emotion analysis concern sensitive biometric data, like face and speech. Most importantly, iMiGUE focuses on micro-gestures, i.e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illustrative purposes. Furthermore, iMiGUE is designed to evaluate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separately (or isolatedly).
The authors collected 359 videos of post match press conferences of Grand Slam tournaments. This dataset contains 72 players from 28 countries and regions covering very continent which enables MGs analysis from diverse cultures. iMiGUE comprises 36 female and 36 male players whose ages are between 17 and 38."	https://paperswithcode.com/dataset/imigue	01/07/2021						
4356	MultiCite	MultiCite is a dataset of 12,653 citation contexts from over 1,200 computational linguistics papers used for Citation context analysis (CCA). MultiCite contains multi-sentence, multi-label citation contexts within full paper texts.	https://paperswithcode.com/dataset/multicite	01/07/2021						
4357	CityNet	CityNet is a multi-modal urban dataset containing data from 7 cities, each of which coming from 3 data sources, which can be used for urban computing and smart city research. The dataset consists of 3 types of raw data (city layout, taxi, meteorology) collected from 7 cities.	https://paperswithcode.com/dataset/citynet	30/06/2021						
4358	CrowdSpeech	CrowdSpeech is a publicly available large-scale dataset of crowdsourced audio transcriptions. It contains annotations for more than 20 hours of English speech from more than 1,000 crowd workers.	https://paperswithcode.com/dataset/crowdspeech	02/07/2021						
4359	Toloka Business ID Recognition	This dataset, commissioned by the Yandex Business Directory, contains 10,000 photos of organization information signs shot in the Russian Federation along with the INN (taxpayer ID) and OGRN (Primary State Registration Number) codes shown on these signs. Toloka was used for both capturing photos and recognizing INN and OGRN codes.	https://paperswithcode.com/dataset/toloka-business-id-recognition							
4360	pd4ml	"pd4ml is a collection of datasets from fundamental physics research -- including particle physics, astroparticle physics, and hadron- and nuclear physics -- for supervised machine learning studies. These datasets, containing hadronic top quarks, cosmic-ray induced air showers, phase transitions in hadronic matter, and generator-level histories, are made public to simplify future work on cross-disciplinary machine learning and transfer learning in fundamental physics.
It currently consists on 5 datasets:

Top Tagging Landscape (Classification) 
Train/val/test: 1.2M/400k/400k 
Structure: Four vectors 
Dimension: 200 particles, 4 features/particle


Smart Backgrounds (Classification)   
Train/val/test: 157k/39k/84k
Structure: Decay Graph
Dimension: 100 particles, 9 features/particle


Spinodal or Not (Classification)   
Train/val/test: 16.3k/4k/8.7k
Structure: 2D Histogram
Dimension: 20x20 histogram of pion spectra


EoS (Classification)  
Train/val/test: 121k/25k/54k
Structure: 2D Histogram
Dimension: 24x24 histogram of pion spectra


Air Showers (Regression)   
Train/val/test: 56k/30k/14k
Structure: 81 1D Traces
Dimension: 81 stations, 80 signal bins + timing"	https://paperswithcode.com/dataset/pd4ml	01/07/2021	Physics Data for Machine Learning					
4361	Toloka WaterMeters	This datase, contains 1244 images of hot and cold water meters as well as their readings and coordinates of the displays showing those readings. Each image contains exactly one water meter. The archive also includes the pictures of the results of segmentation with the masks and collages. Toloka was used for photo capturing, segmentation, and recognizing the readings.	https://paperswithcode.com/dataset/toloka-watermeters							
4362	RuADReCT	Created as part of the Social Media Mining for Health Applications (#SMM4H '20) shared tasks, this dataset consists of 9515 tweets describing health issues. Each tweet is labeled for whether it contains information about an adverse side effect that occurred when taking a drug. The dataset was a joint effort with the UPenn HLP Center and the Chemoinformatics and Molecular Modeling Research Laboratory at Kazan Federal University.	https://paperswithcode.com/dataset/ruadrect		The Russian Adverse Drug Reaction Corpus  of Tweets					
4363	LRWC	This dataset contains the opinions of Russian native speakers about the relationship between a generic term (hypernym) and a specific instance of it (hyponym). Assembled by Dmitry Ustalov in 2017. A set of 300 most frequent nouns was extracted from the Russian National Corpus. Then each method or resource (including RuThes and RuWordNet) produced at most five hypernyms, if possible. This resulted in 10,600 unique non-empty subsumption pairs, which were annotated by seven different performers whose mother tongue is Russian and were at least 20 years old as of February 1, 2017. As a result, 4,576 out of 10,600 pairs were annotated as positive while the remaining 6,024 were annotated as negative. Interestingly, the performers were more confident in the negative answers than in the positive ones.	https://paperswithcode.com/dataset/lrwc		Lexical Relations from the Wisdom of the Crowd					
4364	Human-Annotated Sense-Disambiguated Word Contexts for Russian	This dataset contains human-annotated sense identifiers for 2562 contexts of 20 words used in the RUSSE'2018 shared task on Word Sense Induction and Disambiguation for the Russian language. Assembled by Dmitry Ustalov in 2017. In particular, 80 pre-annotated contexts were used for training the human annotators, and 2562 contexts were annotated by humans such that each context was annotated by 9 different annotators. After the annotation, every context was additionally inspected (“curated”) by the organizers of the shared task.	https://paperswithcode.com/dataset/human-annotated-sense-disambiguated-word							
4365	ScanBank	ScanBank is a benchmark dataset for figure extraction from scanned electronic theses and dissertations containing 10 thousand scanned page images, manually labeled by humans as to the presence of the 3.3 thousand figures or tables found therein.	https://paperswithcode.com/dataset/scanbank	23/06/2021						
4366	Florence 3D actions dataset	The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.	https://paperswithcode.com/dataset/florence-3d-actions-dataset							
4367	Delaunay triangulation	"Delaunay triangulation dataset for 5, 10, 15, 20 points.
Both random and sorted datasets are included.
If you have any trouble to use this dataset, contact hunnino10@gmail.com"	https://paperswithcode.com/dataset/delaunay-triangulation	05/07/2021						
4368	ExBAN	The ExBAN dataset: a corpus of NL explanations generated by crowd-sourced participants presented with the task of explaining simple Bayesian Network (BN) graphical representations. These explanations, in a separate collection effort, are rated for clarity and informativeness.	https://paperswithcode.com/dataset/exban	15/03/2021	ExBAN Corpus (Explanations for BAyesian Networks)					
4369	ObMan-Ego	The ObMan-Ego is a large-scale synthetic hand dataset with egocentric scenes in which the simulated hands are provided by ObMan. The dataset is used for a hand segmentation task and its sim-to-real adaptation benchmark. Training, validation, and testing sets contain 150, 000, 6, 500, and 6, 500 images, respectively.	https://paperswithcode.com/dataset/obman-ego	06/07/2021						
4370	CPTC-2018	"Intrusion alert dataset captured through the Collegiate Penetration Testing Competition (CPTC) 2018. Contains alerts from 6 student teams. For details, see ""A Cybersecurity Dataset Derived from the National Collegiate Penetration Testing Competition"" by Nathan Munaiah et al."	https://paperswithcode.com/dataset/cptc-2018	28/11/2018						
4371	SURREALvols	Added information about the subject's body height and volumes of 14 individual body parts.	https://paperswithcode.com/dataset/surrealvols	05/07/2021						
4372	Fingerprint Dataset	"This dataset includes all music sources, background noises and impulse-reponses (IR) samples and conversation speech  that have been used in the work ""Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning"" ICASSP 2021 (https://arxiv.org/abs/2010.11910)."	https://paperswithcode.com/dataset/fingerprint-dataset	07/07/2021	Neural Audio Fingerprint Dataset					
4373	Steel Tube Dataset	8 kinds of weld defects	https://paperswithcode.com/dataset/steel-tube-dataset		Steel Tube Weld Defect Detection Dataset					
4374	Geography of Open Source Software	This dataset reports counts of active GitHub contributors (activity: 2019/2020) geolocated in early 2021. Counts are aggregated at the country level and at various regional scales. Besides countries, we report data on the EU NUTS2 level, for Brazilian, Russian, Chinese, Japanese, Indian, and US-American subnational geographies. We used a pipeline approach, attempting to infer location first from GitHub profile of a developer, then from linked Twitter accounts, then from email suffixes (country level only). Our data reports the count of developers identified by each stage of the pipeline, in case for instance one prefers to only use the GitHub account information.	https://paperswithcode.com/dataset/geography-of-open-source-software	07/07/2021						
4375	PDE solutions	"In this folder, you will find solutions of the following partial differential equations:
- Burgers
- Kortweg-de-Vries
-Newell-Whitehead
- Kuramoto-Sivashinsky
You will find more info about how these were generated in the supplementary material of the paper: https://arxiv.org/abs/2106.11936"	https://paperswithcode.com/dataset/pde-solutions							
4376	PDEs	"In this dataset, you will find solutions of the following partial differential equations: - Burgers - Kortweg-de-Vries -Newell-Whitehead - Kuramoto-Sivashinsky
You will find more info about how these were generated in the supplementary material of the paper: https://arxiv.org/abs/2106.11936"	https://paperswithcode.com/dataset/pdes	02/05/2021	Some PDE solutions					
4377	IowaRain	IowaRain is a dataset of rainfall events for the state of Iowa (2016-2019) acquired from the National Weather Service Next Generation Weather Radar (NEXRAD) system and processed by a quantitative precipitation estimation system. The dataset presented in this study could be used for better disaster monitoring, response and recovery by paving the way for both predictive and prescriptive modeling	https://paperswithcode.com/dataset/iowarain	07/07/2021						
4378	Kosp2e	Kosp2e (read as `kospi'), is a corpus that allows Korean speech to be translated into English text in an end-to-end manner	https://paperswithcode.com/dataset/kosp2e	06/07/2021						
4379	HumanoidRobotPose	The HumanoidRobotPose dataset is a dataset for real-time pose estimation of humanoid robots.	https://paperswithcode.com/dataset/humanoidrobotpose	06/07/2021						
4380	FaVIQ	"FaVIQ (Fact Verification from Information-seeking Questions) is a challenging and realistic fact verification dataset that reflects confusions raised by real users. We use the ambiguity in information-seeking questions and their disambiguation, and automatically convert them to true and false claims. These claims are natural, and require a complete understanding of the evidence for verification. FaVIQ serves as a challenging benchmark for natural language understanding, and improves performance in professional fact checking.
Source: https://faviq.github.io"	https://paperswithcode.com/dataset/faviq	05/07/2021	Fact Verification from Information-seeking Questions					
4381	OPA	"Object-Placement-Assessment (OPA) is a task consisting on verifying whether a composite image is plausible in terms of the object placement. The foreground object should be placed at a reasonable location on the background considering location, size, occlusion, semantics, and etc.
OPA is a synthesised dataset for Object Placement Assessment based on COCO dataset. The authors selected unoccluded objects from multiple categories as our candidate foreground objects. The foreground objects are pasted on their compatible background images with random sizes and locations to form composite images, which are sent to human annotators for rationality labeling."	https://paperswithcode.com/dataset/opa	05/07/2021	Object Placement Assessment					
4382	DPPIN	DPPIN is a collection of dynamic networks, which consists of twelve generated dynamic protein-protein interaction networks of yeast cells, stored in twelve folders.	https://paperswithcode.com/dataset/dppin	05/07/2021						
4383	MineRL BASALT	MineRL BASALT is an RL competition on solving human-judged tasks. The tasks in this competition do not have a pre-defined reward function: the goal is to produce trajectories that are judged by real humans to be effective at solving a given task.	https://paperswithcode.com/dataset/minerl-basalt	05/07/2021						
4384	SBU-WSD-Corpus	SBU-WSD-Corpus is a corpus for Persian Word Sense Disambiguation (WSD). It is manually annotated with senses from the Persian WordNet (FarsNet) sense inventory. SBU-WSD-Corpus consists of 19 Persian documents in different domains such as Sports, Science, Arts, etc. It includes 5892 content words of Persian running text and 3371 manually sense annotated words (2073 nouns, 566 verbs, 610 adjectives, and 122 adverbs).	https://paperswithcode.com/dataset/sbu-wsd-corpus	04/07/2021						
4385	VinDr-RibCXR	VinDr-RibCXR is a benchmark dataset for automatic segmentation and labeling of individual ribs from chest X-ray (CXR) scans. The VinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations provided by human experts.	https://paperswithcode.com/dataset/vindr-ribcxr	03/07/2021						
4386	Disaster	"Disaster is a dataset that contains images collected from various sources for three different disasters: fire, water and land. Besides this, it also contains images for various damaged infrastructure due to natural or man made calamities and damaged human due to war or accidents.
There are 13,720 manually annotated images in this dataset, each image is annotated by three individuals. The authors are also providing discriminating image class information annotated manually with bounding box for a set of 200 test images. Images are collected from different news portals, social media, and standard datasets made available by other researchers."	https://paperswithcode.com/dataset/disaster	02/07/2021						
4387	Google Landmarks	The Google Landmarks dataset contains 1,060,709 images from 12,894 landmarks, and 111,036 additional query images. The images in the dataset are captured at various locations in the world, and each image is associated with a GPS coordinate. This dataset is used to train and evaluate large-scale image retrieval models.	https://paperswithcode.com/dataset/google-landmarks	19/12/2016						
4388	KiTS19	"The 2021 Kidney and Kidney Tumor Segmentation challenge (abbreviated KiTS21) is a competition in which teams compete to develop the best system for automatic semantic segmentation of renal tumors and surrounding anatomy.
The 2021 Kidney and Kidney Tumor Segmentation Challenge
The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 Challenge"	https://paperswithcode.com/dataset/kits19	02/12/2019	The 2019 Kidney and Kidney Tumor Segmentation Challenge					
4389	UrbanScene3D	"UrbanScene3D is a large scale urban scene dataset associated with a handy simulator based on Unreal Engine 4 and AirSim, which consists of both man-made and real-world reconstruction scenes in different scales, referred to as UrbanScene3D. The manually made scene models have compact structures, which are carefully constructed/designed by professional modelers according to the images and maps of target areas. In contrast, UrbanScene3D also offers dense, detailed scene models reconstructed by aerial images through multi-view stereo (MVS) techniques. These scenes have realistic textures and meticulous structures. The release also includes the originally captured aerial images that have been used to reconstruct the 3D scene models, as well as a set of 4K video sequences that would facilitate designing algorithms, such SLAM and MVS.
Source: https://vcc.tech/UrbanScene3D/
Image source: https://vcc.tech/UrbanScene3D/"	https://paperswithcode.com/dataset/urbanscene3d	09/07/2021						
4390	ChangeSim	ChangeSim is a dataset aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations.	https://paperswithcode.com/dataset/changesim	09/03/2021	ChangeSim					
4391	ISO17	"Description
The molecules were randomly drawn from the largest set of isomers in the QM9 dataset 1 which consists of molecules with a fixed composition of atoms (C7O2H10) arranged in different chemically valid structures. It is an extension of the ismoer MD data used in 2.
The database was generated from molecular dynamics simulations using the Fritz-Haber Institute ab initio simulation package (FHI-aims)3. The simulations were carried out using the standard quantum chemistry computational method density functional theory (DFT) in the generalized gradient approximation (GGA) with the Perdew-Burke-Ernzerhof (PBE) functional4 and the Tkatchenko-Scheffler (TS) van der Waals correction method 5.
The database consist of 129 molecules each containing 5,000 conformational geometries, energies and forces with a resolution of 1 femtosecond in the molecular dynamics trajectories.
Format
The data is stored in ASE sqlite format with the total energy in eV under the key total energy and the atomic_forces under the key atomic_forces in eV/Ang.
The following Python snippet iterates over the first 10 entries of the dataset located at path_to_db:
```python
from ase.db import connect
with connect(path_to_db) as conn:
   for row in conn.select(limit=10):
       print(row.toatoms())
       print(row['total_energy'])
       print(row.data['atomic_forces'])
```
Partitions
The data is partitioned as used in the SchNet paper 6:
reference.db - 80% of steps of 80% of MD trajectories
reference_eq.db - equilibrium conformations of those molecules
test_within.db - remaining 20% unseen steps of reference trajectories
test_other.db - remaining 20% unseen MD trajectories
test_eq.db - equilibrium conformations of test trajectories
In the paper, we split the reference data (reference.db) into 400k training examples and 4k validation examples. The indices are given in the files train_ids.txt and validation_idx.txt, respectively.
Benchmarks
Model   Energy (within) [eV]    Force (within) [eV/A]   Energy (other) [eV] Force (other) [eV/A]
SchNet 6  0.016   0.043   0.104   0.095
Download
Available here: data/iso17.tar.gz (799.7 MB)
How to cite
When using this dataset, please make sure to cite the following papers:
K.T. Schütt, P.-J. Kindermans, H.E. Sauceda, S. Chmiela, A. Tkatchenko, K.-R. Müller. SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in Neural Information Processing System. 2017.
K.T. Schütt, F. Arbabzadah, S. Chmiela, K.R. Müller, A. Tkatchenko. Quantum-chemical insights from deep tensor neural networks. Nature Communications, 8, 13890. 2017.
R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014.
References
1 R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld. Quantum chemistry structures
and properties of 134 kilo molecules. Scientific Data, 1, 2014.
2 Schütt, K. T., Arbabzadah, F., Chmiela, S., Müller, K. R., & Tkatchenko, A. (2017). Quantum-chemical insights from deep tensor neural networks. Nature Communications, 8, 13890.
3 Blum, V.; Gehrke, R.; Hanke, F.; Havu, P.; Havu, V.; Ren, X.; Reuter, K.; Scheffler, M. Ab Initio Molecular Simulations with Numeric Atom-Centered Orbitals. Comput. Phys. Commun. 2009, 180 (11), 2175–2196.
4 Perdew, J. P.; Burke, K.; Ernzerhof, M. Generalized Gradient Approximation Made Simple. Phys. Rev. Lett. 1996, 77 (18), 3865–3868.
5 Tkatchenko, A.; Scheffler, M. Accurate Molecular Van Der Waals Interactions from Ground-State Electron Density and Free-Atom Reference Data. Phys. Rev. Lett. 2009, 102 (7), 73005.
6 Schütt, K. T., Kindermans, P. J., Sauceda, H. E., Chmiela, S., Tkatchenko, A., & Müller, K. R. SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in Neural Information Processing System (accepted). 2017."	https://paperswithcode.com/dataset/iso17		ISO17 - MD Trajectories of C7O2H10 with total energies and atomic forces					
4392	MCMD	A large-scale dataset in multi-programming languages and with rich information.	https://paperswithcode.com/dataset/mcmd	12/07/2021	Multi-programming-language Commit Message Dataset					
4393	BCOPA-CE	"We provide the BCOPA-CE test set, which has balanced token distribution in the correct and wrong alternatives and increases the difficulty of being aware of cause and effect.
construction

for each premise of the 500 samples in COPA-test set, we generate one event manually which is a plausible answer to the opposite question type of the original sample.
obtain 500 triplets of <premise, cause, effect>
construct 1000 samples by giving two different questions (cause or effect) to each triplet."	https://paperswithcode.com/dataset/bcopa-ce	05/07/2021	A Balanced COPA Test Set with cause-effect as alternatives					
4394	Multiple Testing and Variable Selection along Least Angle Regression's path	"Data used in paper entitled ""Multiple Testing and Variable Selection along Least Angle Regression's path"".
Zenodo file with the code of the paper arXiv:1906.12072"	https://paperswithcode.com/dataset/zenodo	28/06/2019						
4395	HumanEval	"This is an evaluation harness for the HumanEval problem solving dataset described in the paper ""Evaluating Large Language Models Trained on Code"". It used to measure functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some
comparable to simple software interview questions.
Source: Evaluating Large Language Models Trained on Code
Image Source: Evaluating Large Language Models Trained on Code"	https://paperswithcode.com/dataset/humaneval	07/07/2021						
4396	Unbalance Classification Using Vibration Data	"This dataset contains vibration data recorded on a rotating drive train. This drive train consists of an electronically commutated DC motor and a shaft driven by it, which passes through a roller bearing. With the help of a 3D-printed holder, unbalances with different weights and different radii were attached to the shaft. Besides the strength of the unbalances, the rotation speed of the motor was also varied. This dataset can be used to develop and test algorithms for the automatic detection of unbalances on drive trains. Datasets for 4 differently sized unbalances and for the unbalance-free case were recorded. The vibration data was recorded at a sampling rate of 4096 values per second. Datasets for development (ID ""D[0-4]"") as well as for evaluation (ID ""E[0-4]"") are available for each unbalance strength. The rotation speed was varied between approx. 630 and 2330 RPM in the development datasets and between approx. 1060 and 1900 RPM in the evaluation datasets. For each measurement of the development dataset there are approx. 107min of continuous measurement data available, for each measurement of the evaluation dataset 28min."	https://paperswithcode.com/dataset/unbalance-classification-using-vibration-data	31/03/2020	Vibration Measurements on a Rotating Shaft at Different Unbalance Strengths					
4397	HYPE	"HYPE Dataset - Version 1.0.0
REFERENCE PAPER
Morassi Sasso, A., Datta, S., Jeitler, M., Steckhan, N., Kessler, C. S., Michalsen, A., Arnrich, B., & Böttinger, E. (2020). HYPE: Predicting Blood Pressure from Photoplethysmograms in a Hypertensive Population. In M. Michalowski & R. Moskovitch (Eds.), Artificial Intelligence in Medicine. AIME 2020. Lecture Notes in Computer Science, volume 12299 (pp. 325–335). Springer, Cham. https://doi.org/10.1007/978-3-030-59137-3_29.
CONTENT

Sensor (PPG & Blood Pressure) and clinical data from 9 hypertensive subjects in two experiments (stress test and 24 hours)
PPG data from Empatica E4: photoplethysmography (PPG) data.
Spacelabs (SL 90217): blood pressure data.
Demographics and self-reported data during 24 hours (exercise, medication, etc.).

AVAILABILITY

Available to the scientific community through a data agreement. 
The requester must be affiliated to a research institution."	https://paperswithcode.com/dataset/hype	10/02/2021	PPG and Blood Pressure from a Hypertensive Population					
4398	Lakh Pianoroll Dataset	"The Lakh Pianoroll Dataset (LPD) is a collection of 174,154
multitrack pianorolls derived from the
Lakh MIDI Dataset (LMD).
Getting the dataset
We provide multiple subsets and versions of the dataset (see
here). The dataset is available here.
Using LPD
The multitrack pianorolls in LPD are stored in a special format for efficient
I/O and to save space. We recommend to load the data with
Pypianoroll (The dataset is created
using Pypianoroll v0.3.0.). See here
to learn how the data is stored and how to load the data properly.
License
Lakh Pianoroll Dataset is a derivative of
Lakh MIDI Dataset by
Colin Raffel, used under
CC BY 4.0.
Lakh Pianoroll Dataset is licensed under
CC BY 4.0 by
Hao-Wen Dong and
Wen-Yi Hsiao.
Please cite the following papers if you use Lakh Pianoroll Dataset in a
published work.


Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang,
  ""MuseGAN: Multi-track Sequential Generative Adversarial Networks for
  Symbolic Music Generation and Accompaniment,""
  in Proceedings of the 32nd AAAI Conference on Artificial Intelligence
  (AAAI), 2018.


Colin Raffel,
  ""Learning-Based Methods for Comparing Sequences, with Applications to
  Audio-to-MIDI Alignment and Matching,""
  PhD Thesis, 2016.


Related projects

MuseGAN
LeadSheetGAN"	https://paperswithcode.com/dataset/lakh-pianoroll-dataset	19/09/2017						
4399	Common Crawl	The Common Crawl corpus contains petabytes of data collected over 12 years of web crawling. The corpus contains raw web page data, metadata extracts and text extracts. Common Crawl data is stored on Amazon Web Services’ Public Data Sets and on multiple academic cloud platforms across the world.	https://paperswithcode.com/dataset/common-crawl							
4400	CADSketchNet	"CADSketchNet is an annotated collection of sketches of 3D CAD models.
Dataset-A has 58,696 computer-generated sketches of the 3D CAD models across 68 categories of MCB.
Dataset-B has 801 hand-drawn sketches of the 3D CAD models across 42 categories of ESB"	https://paperswithcode.com/dataset/cadsketchnet	13/07/2021						
4401	AIP Environment	"AI Playground (AIP) is an open-source, Unreal Engine-based tool for generating and labeling virtual image data. With AIP, it is trivial to capture the same image under different conditions (e.g., fidelity, lighting, etc.) and with different ground truths (e.g., depth or surface normal values). AIP is easily extendable and can be used with or without code.
Source: AI Playground: Unreal Engine-based Data Ablation Tool for Deep Learning"	https://paperswithcode.com/dataset/aip-environment	13/07/2020						
4402	Voice Conversion Challenge 2018	"Voice conversion (VC) is a technique to transform a speaker identity included in a source speech waveform into a different one while preserving linguistic information of the source speech waveform. The Voice Conversion Challenge (VCC) 2016 was launched in 2016 at Interspeech 2016. The objective of the 2016 challenge was to better understand different VC techniques built on a freely-available common dataset to look at a common goal, and to share views about unsolved problems and challenges faced by the current VC techniques. The VCC 2016 focused on the most basic VC task, that is, the construction of VC models that automatically transform the voice identity of a source speaker into that of a target speaker using a parallel clean training database where source and target speakers read out the same set of utterances in a professional recording studio. 17 research groups had participated in the 2016 challenge. The challenge was successful and it established new standard evaluation methodology and protocols for bench-marking the performance of VC systems. The second edition of VCC was launched in 2018, the VCC 2018. In this second edition, three aspects of the challenge were revised. First, the amount of speech data used for the construction of participant's VC systems was reduced to half. This is based on feedback from participants in the previous challenge and this is also essential for practical applications. Second, a more challenging task refereed to a Spoke task in addition to a similar task to the 1st edition was introduced, which we call a Hub task. In the Spoke task, participants need to build their VC systems using a non-parallel database in which source and target speakers read out different sets of utterances. Both parallel and non-parallel voice conversion systems are evaluated via the same large-scale crowdsourcing listening test. Third, bridging the gap between the ASV and VC communities was also attempted. Since new VC systems developed for the VCC 2018 may be strong candidates for enhancing the ASVspoof 2015 database, spoofing performance of the VC systems based on anti-spoofing scores was assessed.
Description from: https://datashare.ed.ac.uk/handle/10283/3061"	https://paperswithcode.com/dataset/voice-conversion-challenge-2018							
4403	SECBENCH	"Dataset of 676 security vulnerabilities patches. In 2017, we mined the commits messages of 238 projects using regular expressions for each vulnerability (cf. Patterns). In 2020, we classified vulnerabilities using the CWE taxonomy. Some vulnerabilities contain the score and severity information (CVEs).
Source: https://tqrg.github.io/secbench/"	https://paperswithcode.com/dataset/secbench							
4404	Sims4Action	"The Sims4Action Dataset: a videogame-based dataset for Synthetic→Real domain adaptation for human activity recognition.


Goal : Exploring the concept of constructing training examples for Activities of Daily Living (ADL) recognition by playing life simulation video games.  

 Sims4Action  dataset is created with the commercial game THE SIMS 4   by executing actions-of-interest within the game in a ""top-down"" manner. It features ten hours of video material of eight diverse characters and multiple environments.  Ten actions are selected to have a direct correspondence to categories covered in the real-life dataset Toyota Smarthome 2 to enable the research of Synthetic→Real transfer in action recognition.
Two benchmarks : Gaming→Gaming (training and evaluation on Sims4Action)  and Gaming→Real (training on Sims4Action, evaluation on the real Toyota Smarthome data 2).
Main challenge: Gaming→Real domain adaptation
  While ADL recognition on gaming data is interesting from a theoretical perspective, the key challenge arises from transferring knowledge learned from simulated data to real-world applications. Sims4Action specifically provides a benchmark for this scenario since it describes a Gaming→Real challenge, which evaluates models on real videos derived from the existing Toyota Smarthome dataset . 

References
1 Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games.
Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon Reiß, Rainer Stiefelhagen,
In International Conference on Intelligent Robots and Systems (IROS), 2021
(* denotes equal contribution.)
2 Toyota smarthome: Real-world activities of daily living.
Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, Gianpiero Francesca,
In International Conference on Computer Vision (ICCV), 2019."	https://paperswithcode.com/dataset/sims4action	12/07/2021						
4405	GLIB: image dataset	"data/images:
data/images/Base : 132 screenshots of game1 & game2 with UI display issues from 466 test reports.
data/images/Code : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method.
data/images/Normal: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene.
data/images/Rule(F) : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method.
data/images/Rule(R) : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method.
data/images/testDataSet : 192 screenshots with UI display issues from 466 test reports(exclude game1 & game2).
data/data_csv:
data/data_csv/Base : dataset for baseline method.
data/data_csv/Code : dataset for our Code Augmentation method.
data/data_csv/Rule(F) : dataset for our Rule(F) Augmentation method.
data/data_csv/Rule(R) : dataset for our Rule(R) Augmentation method.
data/data_csv/Code_plus_Rule(F) : dataset for our Code&Rule(F) Augmentation method.
data/data_csv/Code_plus_Rule(R) : dataset for our Code&Rule(R) Augmentation method.
data/data_csv/testDataSet : test dataset(normal image and real glitch images from 466 test reports)."	https://paperswithcode.com/dataset/glib-image-dataset	19/06/2021						
4406	Narvik Road Dataset	"DIT4BEARs Internship Project (at UiT-The Arctic University of Norway) Dataset
The dataset contains data of 5 months including weather conditions, friction coefficient, distance traveled, wind speed, surface temperature, air temperature, etc.
This dataset was provided by DIT4BEARs for the Smart Road Internship Project at UiT-The Arctic University of Norway
It can be used for weather forecasting, road friction forecasting, smart road proposition, minimization of accident rate in Barren Euro-Arctic regions."	https://paperswithcode.com/dataset/narvik-road-dataset	14/07/2021	DIT4BEARs Smart Road Dataset					
4407	PackIt	The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. This ability is referred to as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. PackIt is a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space.	https://paperswithcode.com/dataset/packit	21/07/2020						
4408	EasyCom	"The Easy Communications (EasyCom) dataset is a world-first dataset designed to help mitigate the cocktail party effect from an augmented-reality (AR) -motivated multi-sensor egocentric world view. The dataset contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head and face bounding boxes and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.
Source: EasyCom"	https://paperswithcode.com/dataset/easycom	09/07/2021						
4409	SportSett	This resource is designed to allow for research into Natural Language Generation.  In particular, with neural data-to-text approaches although it is not limited to these.	https://paperswithcode.com/dataset/sportsett	07/09/2020						
4410	NucMM	NucMM is a dataset for segmenting 3D cell nuclei from microscopy image volumes that pushes the task forward to the sub-cubic millimeter scale. It consists of two fully annotated volumes: one electron microscopy (EM) volume containing nearly the entire zebrafish brain with around 170,000 nuclei; and one micro-CT (uCT) volume containing part of a mouse visual cortex with about 7,000 nuclei.	https://paperswithcode.com/dataset/nucmm	13/07/2021						
4411	AxonEM	The AxonEM dataset consists of two 30x30x30 um^3 EM image volumes from the human and mouse cortex, respectively. It is used for 3D axon instance segmentation of brain cortical regions. The authors proofread over 18,000 axon instances to provide dense 3D axon instance segmentation, enabling large-scale evaluation of axon reconstruction methods. In addition, the authors also densely annotate nine ground truth subvolumes for training, per each data volume.	https://paperswithcode.com/dataset/axonem	12/07/2021						
4412	MSJudge	This is a challenging dataset from real courtrooms to predict the legal judgment in a reasonably encyclopedic manner by leveraging the genuine input of the case -- plaintiff's claims and court debate data, from which the case's facts are automatically recognized by comprehensively understanding the multi-role dialogues of the court debate, and then learnt to discriminate the claims so as to reach the final judgment through multi-task learning.	https://paperswithcode.com/dataset/msjudge	12/07/2021						
4413	SynPick	"SynPick is a synthetic dataset for dynamic scene understanding in bin-picking scenarios. In contrast to existing datasets, this dataset is both situated in a realistic industrial application domain -- inspired by the well-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with authentic picking actions as chosen by our picking heuristic developed for the ARC 2017. The dataset is compatible with the popular BOP dataset format.
The dataset consists of 21 Synthetic videos with 503,232 with diverse lightning and 3 different views of each video."	https://paperswithcode.com/dataset/synpick	10/07/2021						
4414	Cylinder in Crossflow	"Cylinder in Crossflow is a synthetic dataset that involves unsteady laminar flow past a cylinder that generates vortex
shedding pattern known as a von Kármán vortex street. The governing equations for
this system are the incompressible Navier-Stokes equations. The cylinder
has a diameter of 1 and the free stream velocity is 1. The kinematic viscosity $\nu$ is
varied such that the Reynolds number is between 100 and 400. Symmetry boundary conditions are applied at the top and bottom edges of the domain and an open pressure boundary condition is applied at the outlet. Solutions are generated on the
unstructured mesh of 6384 quad elements."	https://paperswithcode.com/dataset/cylinder-in-crossflow	11/06/2020						
4415	Color-connectivity	"Synthetic graph classification datasets with the task of recognizing the connectivity of same-colored nodes in 4 graphs of varying topology.

The four Color-connectivity datasets were created by taking a graph and randomly coloring half of its nodes one color, e.g., red, and the other nodes blue, such that the red nodes either form a single connected island or two disjoint islands.
  The binary classification task is then distinguishing between these two cases.
  The node colorings were sampled by running two red-coloring random walks starting from two random nodes.
For the underlying graph topology we used: 1) 16x16 2D grid, 2) 32x32 2D grid, 3) Euroroad road network (Šubelj et al. 2011), and 4) Minnesota road network.
We sampled a balanced set of 15,000 coloring examples for each graph, except for Minnesota network for which we generated 6,000 examples due to memory constraints.
The Color-connectivity task requires combination of local and long-range graph information processing to which most existing message-passing Graph Neural Networks (GNNs) do not scale.
  These datasets can serve as a common-sense validation for new and more powerful GNN methods.
  These testbed datasets can still be improved, as the node features are minimal (only a binary color) and recognition of particular topological patterns (e.g., rings or other subgraphs) is not needed to solve the task."	https://paperswithcode.com/dataset/color-connectivity	15/07/2021						
4416	MovieGraphBenchmark	"The dataset contains entities from IMDB, TheMovieDB and TheTVDB with goldstandard matches between the sources. Due to the licensing of IMDB we provide a script to build the IMDB part of the dataset yourself.
The dataset contains a variety of entity types to match: persons, movies, series, episodes and companies."	https://paperswithcode.com/dataset/moviegraphbenchmark	15/01/2021						
4417	OpenEA Benchmark	"1.0 Version of OpenEA benchmark datasets. Please use the updated 2.0 version, that has been subsequently released.
Introduced by ""A Benchmarking Study of Embedding-based Entity Alignment for Knowledge Graphs by Sun et. al, VLDB 2020""
Contains entities from DBpedia, YAGO and Wikidata."	https://paperswithcode.com/dataset/openea-benchmark							
4418	FewCLUE	Chinese Few-shot Learning Evaluation Benchmark (FewCLUE) is a comprehensive small sample evaluation benchmark in Chinese. It includes nine tasks, ranging from single-sentence and sentence-pair classification tasks to machine reading comprehension tasks.	https://paperswithcode.com/dataset/fewclue	15/07/2021						
4419	ZS-F-VQA	"The ZS-F-VQA dataset  is a new split of the F-VQA dataset for zero-shot problem.
Firstly we obtain the original train/test split of F-VQA dataset and combine them together to filter out the triples whose answers appear in top-500 according to its occurrence frequency.
Next, we randomly divide this set of answers into new training split (a.k.a. seen) $\mathcal{A}_s$ and testing split (a.k.a. unseen) $\mathcal{A}_u$ at the ratio of 1:1. 
With reference to F-VQA standard dataset, the division process is repeated 5 times. 
For each $(i,q,a)$ triplet in original F-VQA dataset, it is divided into training set if $a \in \mathcal{A}_s$. Else it is divided into testing set.
The overlap of answer instance between training and testing set in F-VQA are $2565$ compared to $0$ in ZS-F-VQA."	https://paperswithcode.com/dataset/zs-f-vqa	12/07/2021						
4420	IMC PhotoTourism	"Dataset provided by the Image Matching Workshop
https://www.cs.ubc.ca/research/image-matching-challenge/current/"	https://paperswithcode.com/dataset/imc-phototourism		Image Matching Challenge Phototourism					
4421	ValidData	"This dataset contains a total of 11 variables. These are:
1. vectorprice: The value in local currency of the product
2. Exchange: The official exchange rate between USD and the local currency when data was extracted.
3. Usprice: The price of the product in USD
4. vectorsold: The number of items sold by the vendor when data was extracted.
5. vectorproduct: The name of the product sold by the vendor
6. country: The name of the country where the product was sold.
7. vectorquestions: The number of questions that the vendor received when data was extracted
8. goodfeedback: the number of positive feedbacks that the vendor received when data was extracted
9. neutralfeedback: the number of neutral feedback (neither positive nor negative)
10. badfeedback: the number of negative feedback.
11. Trust: Just the ratio between goodfeedback divided by goodfeedback + neutralfeedback + badfeedback"	https://paperswithcode.com/dataset/validdata	01/11/2019						
4422	AIM-500	"AIM-500 is the first natural image matting test set, contains 500 high-resolution real-world natural images from three types of images (salient opaque foregrounds, salient transparent/meticulous foregrounds, non-salient foregrounds), and multiple categories. The amount of each category is shown in the following table.
| Portrait | Animal | Transparent | Plant | Furniture | Toy | Fruit |
| :----:| :----: |  :----: |  :----: |  :----: |  :----: |  :----: | 
| 100 | 200 | 34 | 75 | 45 | 36 | 10 |"	https://paperswithcode.com/dataset/aim-500	15/07/2021	Automatic Image Matting-500					
4423	BrnoCompSpeed	"The dataset contains 21 full-HD videos, each around 1 hr long, captured at six different locations. Vehicles in the videos (20 865 instances in total) are annotated with the precise speed measurements from optical gates using LiDAR and verified with several reference GPS tracks. The dataset is available for download and it contains the videos and metadata (calibration, lengths of features in image, annotations, and so on) for future comparison and evaluation.
This dataset was published with paper SOCHOR Jakub et al. Comprehensive Data Set for Automatic Single Camera Visual Speed Measurement, IEEE T-ITS."	https://paperswithcode.com/dataset/brnocompspeed	05/08/2018						
4424	VESUS	"The Varied Emotion in Syntactically Uniform Speech (VESUS) repository is a lexically controlled database collected by the NSA lab. Here, actors read a semantically neutral script of words, phrases, and sentences with different emotional inflections. VESUS contains 252 distinct phrases, each read by 10 actors in 5 emotional states (neutral, angry, happy, sad, fearful).
Source: https://engineering.jhu.edu/nsa/vesus/"	https://paperswithcode.com/dataset/vesus	16/09/2019	Varied Emotion in Syntactically Uniform Speech					
4425	Wasserstein Distances, Geodesics and Barycenters of Merge Trees	"This repository contains all the ensemble datasets (along with their meta-data) used in the manuscript ""Wasserstein Distances, Geodesics and Barycenters of Merge Trees""."	https://paperswithcode.com/dataset/wasserstein-distances-geodesics-and	16/07/2021						
4426	Shifts	The Shifts Dataset is a dataset for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation.	https://paperswithcode.com/dataset/shifts	15/07/2021						
4427	PASTIS	"PASTIS is a benchmark dataset for panoptic and semantic segmentation of agricultural parcels from satellite image time series. It is composed of 2433 one square kilometer-patches in the French metropolitan territory for which sequences of satellite observations are assembled into a four-dimensional spatio-temporal tensor. The dataset contains both semantic and instance annotations, assigning to each pixel a semantic label and an instance id. There is an official 5 fold split provided in the dataset's metadata.
Image source: https://github.com/VSainteuf/pastis-benchmark"	https://paperswithcode.com/dataset/pastis	16/07/2021	Panoptic Segmentation of satellite image TImes Series					
4428	Hockey Fight Detection Dataset	Whereas the action recognition community has focused mostly on detecting simple actions like clapping, walking or jogging, the detection of fights or in general aggressive behaviors has been comparatively less studied. Such capability may be extremely useful in some video surveillance scenarios like in prisons, psychiatric or elderly centers or even in camera phones. After an analysis of previous approaches we test the well-known Bag-of-Words framework used for action recognition in the specific problem of fight detection, along with two of the best action descriptors currently available: STIP and MoSIFT. For the purpose of evaluation and to foster research on violence detection in video we introduce a new video database containing 1000 sequences divided in two groups: fights and non-fights. Experiments on this database and another one with fights from action movies show that fights can be detected with near 90% accuracy.	https://paperswithcode.com/dataset/hockey-fight-detection-dataset	14/07/2011						
4429	Giantsteps	Giantsteps is a dataset that includes songs in major and minor scales for all pitch classes, i.e., a 24-way classification task.	https://paperswithcode.com/dataset/giantsteps							
4430	Emomusic	"1000 songs has been selected from Free Music Archive (FMA). The excerpts which were annotated are available in the same package song ids 1 to 1000. Some redundancies were identified, which reduced the dataset down to 744 songs. The dataset is split between the development set (619 songs) and the evaluation set (125 songs). The extracted 45 seconds excerpts are all re-encoded to have the same sampling frequency, i.e, 44100Hz. 
Source: https://cvml.unige.ch/databases/emoMusic/"	https://paperswithcode.com/dataset/emomusic		Emotion in Music Database					
4431	MTASS	MTASS is an open-source dataset in which mixtures contain three types of audio signals.	https://paperswithcode.com/dataset/mtass	14/07/2021						
4432	BNLP-Resources	Datasets for Bangla Natural Language Processing tasks.	https://paperswithcode.com/dataset/bnlp-resources							
4433	Hindi MSR-VTT	This dataset is the Hindi version of standard English MSR-VTT dataset.	https://paperswithcode.com/dataset/hindi-msr-vtt		Hindi Microsoft reseacrh video to text					
4434	CADNET	We introduce the CADNET dataset, which is an annotated collection of 3,317 3D Engineering models over 43 categories. Owing to the availability of large annotated datasets and also enough computational power in the form of GPUs, many deep learning-based solutions for object classification have been proposed of late, especially in the domain of images and graphical models. Nevertheless, very few solutions have been proposed for the task of functional classification of CAD models. Hence, for this research, CAD models have been collected from Engineering Shape Benchmark (ESB), National Design Repository (NDR), and augmented with newer models created using a modeling software to form a dataset - ‘CADNET’.	https://paperswithcode.com/dataset/cadnet	21/01/2021						
4435	BinKit	"BinKit is a binary code similarity analysis (BCSA) benchmark. BinKit provides scripts for building a cross-compiling environment, as well as the compiled dataset. The original dataset includes 1,352 distinct combinations of compiler options of 8 architectures, 5 optimization levels, and 13 compilers.
For more details, please check: https://github.com/SoftSec-KAIST/BinKit"	https://paperswithcode.com/dataset/binkit	21/11/2020	BinKit					
4436	TFix's Code Patches Data	"The dataset contains more than 100k code patch pairs extracted from open source projects on GitHub. Each pair comes with the erroneous and the fixed version of the corresponding code snippet. Instead of the whole file, the code snippets are extracted to focus on the problematic region (error line + other lines around it). For each sample, the repository name, the commit id, and the file names are provided so that one can access the complete files in case of interest.
The dataset only has JavaScript programs and the error are detected by the popular static code analyzer ESLint. The dataset can be used in the fields of: program repair, code generation, bug finding, transfer learning and many more fields related to machine learning for code"	https://paperswithcode.com/dataset/tfix-s-code-patch-data	18/07/2021						
4437	CVEfixes	"CVEfixes is a comprehensive vulnerability dataset that is automatically collected and curated from Common Vulnerabilities and Exposures (CVE) records in the public U.S. National Vulnerability Database (NVD). The goal is to support data-driven security research based on source code and source code metrics related to fixes for CVEs in the NVD by providing detailed information at different interlinked levels of abstraction, such as the commit-, file-, and method level, as well as the repository- and CVE level.
At the initial release, the dataset covers all published CVEs up to 9 June 2021. All open-source projects that were reported in CVE records in the NVD in this time frame and had publicly available git repositories were fetched and considered for the construction of this vulnerability dataset. The dataset is organized as a relational database and covers 5495 vulnerability fixing commits in 1754 open source projects for a total of 5365 CVEs in 180 different Common Weakness Enumeration (CWE) types. The dataset includes the source code before and after fixing of 18249 files, and 50322 functions."	https://paperswithcode.com/dataset/cvefixes	19/07/2021						
4438	MultiBench	MultiBench, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers evaluation methodology to study (1) generalization, (2) time and space complexity, and (3) modality robustness.	https://paperswithcode.com/dataset/multibench	15/07/2021						
4439	BEHAVIOR	"BEHAVIOR is a benchmark with the 100 household activities that represent a new challenge for embodied AI solutions.
BEHAVIOR is a challenge in simulation where embodied agents make continuous full-body control decisions based on sensor information. Agents need to navigate and manipulate the simulated environment with the goal of acomplishing 100 household activities. BEHAVIOR tests the ability to perceive the environment, plan, and execute complex long-horizon activities that involve multiple objects, rooms, and state transitions, all with the reproducibility, safety and observability offered by a realistic physics simulation.
Description from: BEHAVIOR Challenge @ ICCV 2021"	https://paperswithcode.com/dataset/behavior							
4440	OG RGB+D	"OG RGB+D is a new gait recognition database called OG RGB+D database, which breaks through the limitation of other gait databases and includes multimodal gait data of various occlusions (self-occlusion, active occlusion, and passive occlusion) by a multiple synchronous Azure Kinect DK sensors data acquisition system (multi-Kinect SDAS) that can be also applied in security situations. Because Azure Kinect DK can simultaneously collect multimodal data to support different types of gait recognition algorithms, especially enables to effectively obtain camera-centric multi-person 3D poses, and multi view is better to deal with occlusion than single-view. In particular, the OG RGB+D database provides accurate silhouettes and the optimized human 3D joints data (OJ) by fusing data collected by multi-Kinects which are more accurate in human pose representation under occlusion.
Description from: A Benchmark for Gait Recognition under Occlusion Collected by Multi-Kinect SDAS"	https://paperswithcode.com/dataset/og-rgb-d	19/07/2021						
4441	Wikidata-14M	Wikidata-14M is a recommender system dataset for recommending items to Wikidata editors. It consists of 220,000 editors responsible for 14 million interactions with 4 million items.	https://paperswithcode.com/dataset/wikidata-14m	13/07/2021						
4442	CARLE	CARLE is a life-like cellular automata simulator and reinforcement learning environment. CARLE is flexible, capable of simulating any of the 262,144 different rules defining Life-like cellular automaton universes. CARLE is also fast and can simulate automata universes at a rate of tens of thousands of steps per second through a combination of vectorization and GPU acceleration. Finally, CARLE is simple. Compared to high-fidelity physics simulators and video games designed for human players, CARLE's two-dimensional grid world offers a discrete, deterministic, and atomic universal playground, despite its complexity.	https://paperswithcode.com/dataset/carle	13/07/2021	Cellular Automata Reinforcement Learning Environment					
4443	Forms Dataset	The Forms Dataset is a dataset for document structure extraction comprising of 5K forms.	https://paperswithcode.com/dataset/forms-dataset	09/07/2021						
4444	Undecided Voters in US Presidential Elections	"This data contains the election polls for the 2004, 2008, 2012, and 2016 US presidential election by state including data on undecided voter proportions.
See https://github.com/bonStats/undecided-voters-us-pres-elections#readme for data description."	https://paperswithcode.com/dataset/undecided-voters-in-us-presidential-elections	02/09/2018						
4445	UHCSDB	"DeCost, Hecht, Francis, Webler, Picard, and Holm.
UHCSDB (Ultrahigh Carbon Steel micrograph DataBase): tools for exploring large heterogeneous microstructure datasets.
accepted for publication in IMMI 2017 doi: 10.1007/s40192-017-0097-0
ABSTRACT: 
We present a new microstructure dataset consisting of ultrahigh carbon steel (UHCS) micrographs taken over a range of length scales under systematically varied heat treatments. Using the UHCS dataset as a case study, we develop a set of visualization tools for interacting with and exploring large microstructure and metadata datasets. Based on generic microstructure representations adapted from the field of computer vision, these tools enable image-based microstructure retrieval, as well as spatial maps of both microstructure and related metadata, such as processing conditions or properties measurements. We provide the microstructure image data, processing metadata, and source code for these microstructure exploration tools. The UHCS dataset is intended as a community resource for development and evaluation of microstructure data science techniques and for creation of microstructure data science teaching modules."	https://paperswithcode.com/dataset/uhcsdb	17/04/2017	Ultrahigh Carbon Steel micrograph DataBase					
4446	AADB2021	We present a data set from a first-principles study of amino-methylated and acetylated (capped) dipeptides of the 20 proteinogenic amino acids – including alternative possible side chain protonation states and their interactions with selected divalent cations (Ca$^{2+}$, Mg$^{2+}$ and Ba$^{2+}$. The data covers 21,909 stationary points on the respective potential-energy surfaces in a wide relative energy range of up to 4 eV (390 kJ/mol). Relevant properties of interest, like partial charges, were derived for the conformers.	https://paperswithcode.com/dataset/aadb2021		Cation-coordinated conformers of 20 proteinogenic amino acids with different protonation states					
4447	AADB2021Ontology	This onotology is populated with the data from AADB2021 (https://dx.doi.org/10.17172/NOMAD/2021.02.10-1). Details can be found in the related article on arXiv.org: https://arxiv.org/abs/2107.08855	https://paperswithcode.com/dataset/aadb2021ontology		Ontology representation for a data set of cation-coordinated conformers of 20 proteinogenic amino acids					
4448	Global Wheat Head 2021	"Global WHEAT Dataset 2021 is the extentions of the Global Wheat Dataset 2020. It is the first large-scale dataset for wheat head detection from field optical images. It included a very large range of cultivars from differents continents. Wheat is a staple crop grown all over the world and consequently interest in wheat phenotyping spans the globe. Therefore, it is important that models developed for wheat phenotyping, such as wheat head detection networks, generalize between different growing environments around the world.
Dataset and official splits can be download here"	https://paperswithcode.com/dataset/global-wheat-head-2021	17/05/2021	Global Wheat Head Dataset 2021					
4449	WikiGraphs	"WikiGraphs is a dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. 
WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark with a subgraph from the Freebase knowledge graph. This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets."	https://paperswithcode.com/dataset/wikigraphs	20/07/2021						
4450	GR712RC LEON3 Power Model Data	"Dataset Files
The official dataset files are hosted at https://dx.doi.org/10.21227/1y7r-am78. 
Generating the models from the LEON3 sample data
The data for this paper is generated using a custom open-source methodology called REPPS. In order to replicate the results, first you must follow all the steps in https://github.com/TSL-UOB/TP-REPPS in order to install and configure all the scripts and supporting programs. Afterwards you can proceed with executing the following commands to generate the various models from the LEON3 data.
DISCLAIMER - If you have any issues please don't hesitate to contact via email.
Generate models trained on BEEBS and validated on the use_case_core application
ASIC Only Model
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_use_case_split.data -p 6 -e 4 -d 2 -o 2 -s 20210421_leon3_beebs_ucc_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools.data
Bottom-Up Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_use_case_split.data -p 6 -l 9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24 -m 1 -n 16 -c 1 -g -i 50 -d 2 -o 2 -s 20210425_leon3_beebs_ucc_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools.data
Top-Down Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_use_case_split.data -p 6 -l 9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24 -m 2 -n 1 -c 1 -g -i 50 -d 2 -o 2 -s 20210425_leon3_beebs_ucc_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools
Validate the previous models on BEEBS as well (no need to redo all the event selection, just use same events)
ASIC Only Model
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 4 -d 2 -o 2 -s 20210421_leon3_beebs_beebs_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools.data
Bottom-Up Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 24 -d 2 -o 2 -s 20210421_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools.data
Top-Down Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 9,10,12,13,14,15,16,18,19,20,22,23 -d 2 -o 2 -s 20210421_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools.data
Visualise the data
Generate model per-sample breakdown files for the 1st run of the use_case_opt application
ASIC Only Model
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_onlyusecaseopt_split.data -p 6 -e 4 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210421_leon3_beebs_uco_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools_1r.data
Bottom-Up Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_onlyusecaseopt_split.data -p 6 -e 24 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210427_leon3_beebs_uco_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools_1r.data
Top-Down Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_use_case_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_onlyusecaseopt_split.data -p 6 -e 9,10,12,13,14,15,16,18,19,20,22,23 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210427_leon3_beebs_uco_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools_1r.data
Generate model per-sample breakdown files for the 1st run of the BEEBS benchmarks
ASIC Only Model
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 4 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210423_leon3_beebs_beebs_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools_1r.data
Bottom-Up Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 24 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210427_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools_1r.data
Top-Down Search
./octave_makemodel.sh -r /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain.data -t /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain_1run.data -b /PATH/TO/ESL_paper_data/split/LEON3_BEEBS_BEEBS_split.data -p 6 -e 9,10,12,13,14,15,16,18,19,20,22,23 -d 2 -o 6 -s /PATH/TO/ESL_paper_data/20210427_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools_1r.data
Plot the model per-sample breakdwon data using MODELDATA_plot.py
Plot the use_case_opt 1st run per-sample physical measurements and model errors
./MODELDATA_plot.py -p 1 -x ""Samples[#]"" -t 10 -y ""Power[W]"" -b /PATH/TO/ESL_paper_data/data/LEON3_use_case_opt_finegrain_1run.data -l ""Sensor Data"" -i /PATH/TO/ESL_paper_data/20210421_leon3_beebs_uco_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools_1r.data -a 'ASIC Data Only' -i /PATH/TO/ESL_paper_data/20210427_leon3_beebs_uco_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools_1r.data -a ""Bottom-Up Search"" -i /PATH/TO/ESL_paper_data/20210427_leon3_beebs_uco_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools_1r.data -a ""Top-Down Search""
Plot the BEEBS 1st run per-sample physical measurements and model errors
./MODELDATA_plot.py -p 1 -x ""Samples[#]"" -t 10 -y ""Power[W]"" -b /PATH/TO/ESL_paper_data/data/LEON3_BEEBS_finegrain_1run_physicaldata.data -l ""Sensor Data"" -i /PATH/TO/ESL_paper_data/20210423_leon3_beebs_beebs_pwr_fngr_nocyc_nocth_asicdata_avgrelerr_nfolds_ools_1r.data -a 'ASIC Data Only' -i /PATH/TO/ESL_paper_data/20210427_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_botup_avgrelerr_nfolds_ools_1r.data -a ""Bottom-Up Search"" -i /PATH/TO/ESL_paper_data/20210427_leon3_beebs_beebs_pwr_fngr_allev_nocyc_nocth_topdown_avgrelerr_nfolds_ools_1r.data -a ""Top-Down Search"""	https://paperswithcode.com/dataset/leon3-sample-data	26/05/2021						
4451	S2Looking	S2Looking is a building change detection dataset that contains large-scale side-looking satellite images captured at varying off-nadir angles. The S2Looking dataset consists of 5,000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel) of rural areas throughout the world and more than 65,920 annotated change instances. We provide two label maps to separately indicate the newly built and demolished building regions for each sample in the dataset. We establish a benchmark task based on this dataset, i.e., identifying the pixel-level building changes in the bi-temporal images.	https://paperswithcode.com/dataset/s2looking	20/07/2021						
4452	QVHighlights	The Query-based Video Highlights (QVHighlights) dataset is a dataset for detecting customized moments and highlights from videos given natural language (NL). It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips.	https://paperswithcode.com/dataset/qvhighlights	20/07/2021	Query-based Video Highlights					
4453	GenWiki	"GenWiki is a large-scale dataset for knowledge graph-to-text (G2T) and text-to-knowledge graph (T2G) conversion. It is introduced in the paper ""GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation"" by Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang at COLING 2020."	https://paperswithcode.com/dataset/genwiki	01/12/2020						
4454	Iranis	"The Iranis Dataset is a Large-scale dataset of Farsi license plate characters containing a large-scale dataset with more than 83,000 images of Farsi numbers and letters collected from real-world license plate images captured by various cameras.
Image source: https://github.com/alitourani/Iranis-dataset"	https://paperswithcode.com/dataset/iranis	01/01/2021						
4455	Geometry3K	"A new large-scale geometry problem-solving dataset
- 3,002 multi-choice geometry problems 
- dense annotations in formal language for the diagrams and text
- 27,213 annotated diagram logic forms (literals)
- 6,293 annotated text logic forms (literals)"	https://paperswithcode.com/dataset/geometry3k	10/05/2021						
4456	TLFM dataset	TLFM dataset structured in sequences of at least nine timesteps. The dataset includes 9696 images of both brightfield and green fluorescent protein channels at a resolution of 256 × 256. Dataset for multi-domain (BF and GFP) microscopy image sequence generation.	https://paperswithcode.com/dataset/tlfm-dataset	15/06/2021	TLFM dataset for microscopy image sequence generation					
4457	RailSem19	"RailSem19 offers 8500 unique images taken from a the ego-perspective of a rail vehicle (trains and trams). Extensive semantic annotations are provided, both geometry-based (rail-relevant polygons, all rails as polylines) and dense label maps with many Cityscapes-compatible road labels. Many frames show areas of intersection between road and rail vehicles (railway crossings, trams driving on city streets). RailSem19 is usefull for rail applications and road applications alike.
Image credit: https://wilddash.cc/railsem19"	https://paperswithcode.com/dataset/railsem19	16/06/2019	RailSem19: A Dataset for Semantic Rail Scene Understanding					
4458	MSLR WEB30K	"The datasets are machine learning data, in which queries and urls are represented by IDs. The datasets consist of feature vectors extracted from query-url pairs along with relevance judgment labels:
(1) The relevance judgments are obtained from a retired labeling set of a commercial web search engine (Microsoft Bing), which take 5 values from 0 (irrelevant) to 4 (perfectly relevant).
(2) The features are basically extracted by us, and are those widely used in the research community.
In the data files, each row corresponds to a query-url pair. The first column is relevance label of the pair, the second column is query id, and the following columns are features. The larger value the relevance label has, the more relevant the query-url pair is. A query-url pair is represented by a 136-dimensional feature vector."	https://paperswithcode.com/dataset/mslr-web30k	09/06/2013	Microsoft Learning to Rank Datasets-30k					
4459	Istella LETOR	The Istella LETOR full dataset is composed of 33,018 queries and 220 features representing each query-document pair. It consists of 10,454,629 examples labeled with relevance judgments ranging from 0 (irrelevant) to 4 (perfectly relevant). The average number of per-query examples is 316. It has been splitted in train and test sets according to a 80%-20% scheme.	https://paperswithcode.com/dataset/istella-letor	01/12/2016	Istella Learning to Rank					
4460	DQN Replay Dataset	"The DQN Replay Dataset was collected as follows:
We first train a DQN agent, on all 60 Atari 2600 games
with sticky actions enabled for 200 million frames (standard protocol) and save all of the experience tuples
of (observation, action, reward, next observation) (approximately 50 million)
encountered during training.
This logged DQN data can be found in the public GCP bucket
gs://atari-replay-datasets which can be downloaded using gsutil.
To install gsutil, follow the instructions here.
After installing gsutil, run the command to copy the entire dataset:
gsutil -m cp -R gs://atari-replay-datasets/dqn
To run the dataset only for a specific Atari 2600 game (e.g., replace GAME_NAME
by Pong to download the logged DQN replay datasets for the game of Pong),
run the command:
gsutil -m cp -R gs://atari-replay-datasets/dqn/[GAME_NAME]
This data can be generated by running the online agents using
batch_rl/baselines/train.py for 200 million frames
(standard protocol). Note that the dataset consists of approximately 50 million
experience tuples due to frame skipping (i.e., repeating a selected action for
k consecutive frames) of 4. The stickiness parameter is set to 0.25, i.e.,
there is 25% chance at every time step that the environment will execute the
agent's previous action again, instead of the agent's new action."	https://paperswithcode.com/dataset/dqn-replay-dataset	10/07/2019						
4461	JS Fake Chorales	A MIDI dataset of 500 4-part chorales generated by the KS_Chorus algorithm, annotated with results from hundreds of listening test participants, with 150 further unannotated chorales.	https://paperswithcode.com/dataset/js-fake-chorales	21/07/2021	JS Fake Chorales					
4462	QC-Science	"QC-Science contains 47832 question-answer pairs belonging to the science domain tagged with labels of the form subject - chapter - topic. The dataset was collected with the help of a leading e-learning platform. The dataset consists of 40895 samples for training, 2153 samples for validation and 4784 samples for testing.
Description adopted from: https://arxiv.org/pdf/2107.10649v1.pdf
Image source: https://arxiv.org/pdf/2107.10649v1.pdf"	https://paperswithcode.com/dataset/qc-science	03/07/2021						
4463	OntoNotes 4.0	OntoNotes Release 4.0 contains the content of earlier releases -- OntoNotes Release 1.0 LDC2007T21, OntoNotes Release 2.0 LDC2008T04 and OntoNotes Release 3.0 LDC2009T24 -- and adds newswire, broadcast news, broadcast conversation and web data in English and Chinese and newswire data in Arabic. This cumulative publication consists of 2.4 million words as follows: 300k words of Arabic newswire 250k words of Chinese newswire, 250k words of Chinese broadcast news, 150k words of Chinese broadcast conversation and 150k words of Chinese web text and 600k words of English newswire, 200k word of English broadcast news, 200k words of English broadcast conversation and 300k words of English web text.	https://paperswithcode.com/dataset/ontonotes-4-0	15/02/2011	OntoNotes Release 4.0					
4464	Postal addresses	"The Multinational Structured Address Dataset is a collection of addresses of 61 different countries. The addresses can either be ""complete"" (all the usual address components) or ""incomplete"" (missing some usual address components)."	https://paperswithcode.com/dataset/postal-addresses		Multinational Structured Address Dataset					
4465	MyFood Dataset	MyFood Dataset is an image database for segmenting images of Brazilian foods. Composed of 9 classes: rice, beans, boiled egg, fried egg, pasta, salad, roasted meat, apple and chicken breast. With an average of 125 images per class and a total of 1250 images, with a ratio of 60-20-20 for the training, validation and testing sets, respectively.	https://paperswithcode.com/dataset/myfood-dataset	20/09/2020						
4466	MD17	Energies and forces for molecular dynamics trajectories of eight organic molecules.	https://paperswithcode.com/dataset/md17	05/05/2017	Molecular Dynamics 17					
4467	Spectrum Challange 2 Dataset	"The dataset is approved for public release, distribution unlimited.
The dataset is contained in two files - scrimmage4_link_dataset.pickle and scrimmage5_link_dataset.pickle
The pickle files are stored as list of tuples, each list corresponding to a single link, and containing two elements. Each element a length equal to the number of frames in that link - it varies between link to link.
The first tuple is contains the paramenters -
1. Signal to Noise Ratio ('snr') - 1 element
2. The Modulation and Coding Scheme ('mcs') - 1 element
3. The center frequency of the link ('centerFreq') - 1 element
4. The bandwidth of the link ('bandwidth') - 1 element
5. The Power Spectral Density ('psd') - 16 elements
Thus the total width of each element of the first tuple for a link is 20.
The second tuple contains the success of transmission ('rxSuccess'). If it is 1, there is no frame error, if it is 0, there is a frame error.
Here are the links to the dataset files mentioned in the code (one pickle file for each scrimmage):
Scrimmage 4 (547.5 MB) Mirror
Scrimmage 5 (979.7 MB) Mirror
A larger dataset containing complete information about each match is also available. Please refer to SC2_Dataset_Documentation.pdf for more details regarding the structure of the full dataset. SC2_Dataset_Technical_Design_Report.pdf contains more information about the dataset acquisition process.
Here is the link to the full dataset (separate sqlite files for each match):
Full Dataset (135.517 GB) Mirror (Needs Access Request)
Please use the following citation to refer to the dataset:
A. S. M. M. Jameel, A. P. Mohamed, X. Zhang and A. El Gamal, ""Deep Learning for Frame Error Prediction using a DARPA Spectrum Collaboration Challenge (SC2) Dataset,"" in IEEE Networking Letters, doi: 10.1109/LNET.2021.3096813."	https://paperswithcode.com/dataset/spectrum-challange-2-dataset	14/07/2021						
4468	AP	"This is a paraphrasing dataset created using the adversarial paradigm. A task was designed called the Adversarial Paraphrasing Task (APT) whose objective was to write sentences that mean the same as a given sentence but have as different syntactical and lexical properties as possible.
As shown in the paper, this dataset can be used to measure the performance of paraphrase identifier models and train them. This dataset and the task associated with it (APT) can also be used to challenge neural networks to generate better adversarial paraphrases (the work has done this for T5-base), which will in turn help create better paraphrase identifiers."	https://paperswithcode.com/dataset/ap	14/06/2021	Adversarial Paraphrase					
4469	Vehicle-Rear	Vehicle-Rear is a novel dataset for vehicle identification that contains more than three hours of high-resolution videos, with accurate information about the make, model, color and year of nearly 3,000 vehicles, in addition to the position and identification of their license plates.	https://paperswithcode.com/dataset/vehicle-rear	13/11/2019						
4470	TIP 2018	The first large demoire dataset. The dataset contains 135,000 image pairs, each containing an image contaminated with moire patterns and its corresponding uncontaminated reference image.	https://paperswithcode.com/dataset/tip-2018	08/05/2018						
4471	Action-Camera Parking	"The Action-Camera Parking Dataset contains 293 images captured at a roughly 10-meter height using a GoPro Hero 6 camera. It can be used for training machine learning models that perform image-based parking space occupancy classification.
Image credit: https://github.com/martin-marek/parking-space-occupancy"	https://paperswithcode.com/dataset/action-camera-parking	26/07/2021						
4472	ICDAR 2021 Competition on Historical Map Segmentation	"Revision: v1.0.0-full-20210527a
DOI: 10.5281/zenodo.4817662
Authors: J. Chazalon, E. Carlinet, Y. Chen, J. Perret, C. Mallet, B. Duménieu and T. Géraud
Official competition website: https://icdar21-mapseg.github.io/

This is the dataset of the ICDAR 2021 Competition on Historical Map Segmentation (“MapSeg”).
This competition ran from November 2020 to April 2021.
Motivation
This competition aims as encouraging research in the digitization of historical maps. In order to be usable in historical studies, information contained in such images need to be extracted. The general pipeline involves multiples stages; we list some essential ones here:

segment map content: locate the area of the image which contains map content;
extract map object from different layers: detect objects like roads, buildings, building blocks, rivers, etc. to create geometric data;
georeference the map: by detecting objects at known geographic coordinate, compute the transformation to turn geometric objects into geographic ones (which can be overlaid on current maps).

Tasks
The tasks we propose simulate the three essential digitization steps we just mentioned.
Task 1: “Detect Building Blocks”
This task is the flagship of this competition.
Given a fragment of map sheet image focused on map content, you need to detect the building blocks.
Building blocks are symbolized by a thick line.
They do not overlap between themselves, but many other elements can perturb their detection:

special buildings (hatched areas) can be included in building blocks (sometimes they cover the building block completely);
text can be overlaid on lines;
those maps contain many lines which need to be filtered out (internal building structures, railways, rivers, gardens…).

Expected output for this task is a binary mask indicating for each pixel whether it belongs to a building block or not.
Evaluation tools also tolerate a label map in TIFF format, where each pixel is labelled with the identifier of the shape it belongs to, using an INT16.
To extract shapes from the binary mask, 4-connectivity is used (hence the background has 8-connectivity).
Task 2: “Segment Map Area”
This tasks is the equivalent of text area detection for OCR: given the image of a complete map sheet, you need to segment the area which contains map content.
This area is usually well separated from the other elements (title, legend, scale…) by several frames but sometimes map contents exceeds the frame for some large objects.
While most of the area is delineated by straight lines, some objects were drawn outside the frame on several sheets.
We decided to segment each of those regions as closely as possible.
Expected output for this task is a binary mask indicating for each pixel whether it belongs to the map area or not.
Task 3: “Locate Graticule Lines Intersections”
This task is essential to the georeferencing of the map: graticule lines are lines which indicate the North/South/East/West coordinates relative to the reference point. Their intersection points are very useful to provide key points for the registration of the map image.
Given the image of a complete map sheet, you need to locate the intersection points of such lines.
These lines usually cover the map content from left to right or from top to bottom but beware:

due to document aging paper sheets are not flat anymore and lines are not straight;
lines may be in diagonal for some areas;
lines can be overlaid with many other objects.

Expected output for this task is a list of coordinates (in image referential, i.e. 0,0 at top left, x-axis pointing to the right and y-axis pointing downward)."	https://paperswithcode.com/dataset/icdar-2021-competition-on-historical-map	27/05/2021						
4473	MIT-BIH Arrhythmia Database	"The MIT-BIH Arrhythmia Database contains 48 half-hour excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979. Twenty-three recordings were chosen at random from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed population of inpatients (about 60%) and outpatients (about 40%) at Boston's Beth Israel Hospital; the remaining 25 recordings were selected from the same set to include less common but clinically significant arrhythmias that would not be well-represented in a small random sample.
The recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range. Two or more cardiologists independently annotated each record; disagreements were resolved to obtain the computer-readable reference annotations for each beat (approximately 110,000 annotations in all) included with the database.
This directory contains the entire MIT-BIH Arrhythmia Database. About half (25 of 48 complete records, and reference annotation files for all 48 records) of this database has been freely available here since PhysioNet's inception in September 1999. The 23 remaining signal files, which had been available only on the MIT-BIH Arrhythmia Database CD-ROM, were posted here in February 2005.
Much more information about this database may be found in the (MIT-BIH Arrhythmia Database Directory)[https://archive.physionet.org/physiobank/database/html/mitdbdir/mitdbdir.htm]."	https://paperswithcode.com/dataset/mit-bih-arrhythmia-database	28/05/2027						
4474	C# EditCompletion	We scraped the 53 most popular C# repositories from GitHub and extracted all commits since the beginning of the project’s history. From each commit, we extracted edits in C# files along with the edits in their surrounding context.	https://paperswithcode.com/dataset/c-editcompletion	27/05/2020						
4475	UDD	"UDD is an underwater open-sea farm object detection dataset. UDD consists of 3 categories (seacucumber, seaurchin, and scallop) with 2,227 images. It's the first dataset collected in a real open-sea farm for underwater robot picking.
Source: https://github.com/chongweiliu/UDD_Official
Image source: https://github.com/chongweiliu/UDD_Official"	https://paperswithcode.com/dataset/udd-official	03/03/2020						
4476	News Articles Dataset with Summary	"This dataset is the news articles scraped from New York Times, CNN, Business Insider and Breitbart. The original dataset published in Kaggle did not provide any human summaries, it only offered the title of the article, while this could be used as the summary, it is not ideal as the headline title was too short. We generated the label manually by adding the human summary for the available articles. We also added another column called theme to the dataset, this column would state the genre of the news articles.
The dataset is ideal for summarization as the provided news articles are long and will consume lots of time to read it. Therefore, it is ideal to generate automatic summarization for the articles in the dataset. The dataset consists of 50,001 rows of data."	https://paperswithcode.com/dataset/news-articles-dataset-with-summary	22/07/2021						
4477	Perla Dataset	"This dataset contains the results of a depression screening experiment using two instruments: The PHQ-9 depression screening questionnaire and the chabot Perla. 
The dataset was used to compare the results of these two methods to assess the presence of depression in the population. The sample consist of Spanish speaking participants who responded to both PHQ-9 and Perla's questions."	https://paperswithcode.com/dataset/perla-dataset	28/08/2020	Perla Depression Screening Dataset					
4478	MSU Video Super Resolution Benchmark	"This is a dataset for a video super-resolution task. The dataset contains the most complex content for the restoration task: faces, text, QR-codes, car numbers, unpatterned textures, small details. Videos include different types of motion and different types of degradation: bicubic interpolation (BI) and Gaussian blurring and downsampling (BD). The resolution of all input video sequences is 480x320.
Source: https://videoprocessing.ai/benchmarks/video-super-resolution.html
Image Source: https://videoprocessing.ai/benchmarks/video-super-resolution.html"	https://paperswithcode.com/dataset/msu-vsr-benchmark							
4479	SaRNet	SaRNet is a single class dataset consisting of tiles of satellite imagery labeled with potential 'targets'. Labelers were instructed to draw boxes around anything they suspect may a paraglider wing, missing in a remote area of Nevada. Volunteers were shown examples of similar objects already in the environment for comparison.	https://paperswithcode.com/dataset/sarnet	26/07/2021						
4480	H3DS	H3DS a high-resolution 3D full head textured scans and 360º images dataset collected with a structured light scanner, consisting of 23 3D full-head scans containing images, masks and camera poses. The 3D geometry has been captured using a structured light scanner, which leads to precise ground truth geometries.	https://paperswithcode.com/dataset/h3ds	26/07/2021						
4481	Navigation Turing Test	"Replay data from human players and AI agents navigating in a 3D game environment.
Introduced in ""Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation"" [ICML 2021] to learn how to evaluate humanlike behavior in agents."	https://paperswithcode.com/dataset/navigation-turing-test	20/05/2021						
4482	GIGO revisited: ML publications' approaches to training data	A random sample of 200 machine learning publications, systematically analyzed by a team of labelers, who asked up to 15 questions about how the publication discusses its training data. More documentation in data/README.md.	https://paperswithcode.com/dataset/gigo-revisited-ml-publications-approaches-to	05/07/2021						
4483	RaidaR	"RaidaR is a  rich annotated image dataset of rainy street scenes. RaidaR consists of 58,542 real rainy images containing several rain-induced artifacts: fog, droplets, road reflections, etc. 5,000/3,658 images were carefully semantic/instance segmentated, respectively.
Source: https://raidar-dataset.com/"	https://paperswithcode.com/dataset/raidar	09/04/2021	RaidaR: A Rich Annotated Image Dataset of Rainy Street Scenes					
4484	COVIDEmo	A dataset of tweets that reference the COVID-19 pandemic with emotion labels.	https://paperswithcode.com/dataset/covidemo	23/07/2021						
4485	CLIP	We created a dataset of clinical action items annotated over MIMIC-III. This dataset, which we call CLIP, is annotated by physicians and covers 718 discharge summaries, representing 107,494 sentences. Annotations were collected as character-level spans to discharge summaries after applying surrogate generation to fill in the anonymized templates from MIMIC-III text with faked data. We release these spans, their aggregation into sentence-level labels, and the sentence tokenizer used to aggregate the spans and label sentences. We also release the surrogate data generator, and the document IDs used for training, validation, and test splits, to enable reproduction. The spans are annotated with 0 or more labels of 7 different types, representing the different actions that may need to be taken: Appointment, Lab, Procedure, Medication, Imaging, Patient Instructions, and Other. We encourage the community to use this dataset to develop methods for automatically extracting clinical action items from discharge summaries.	https://paperswithcode.com/dataset/clip	04/06/2021	CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes					
4486	Smoking Data of hospitalized Covid-19 patients	Data related to 1040 patients with Covid-19 admitted to hospitals in Iran have been collected. These patients were randomly selected from patients admitted to hospitals in Rasht, Tehran, and Bojnord. Of these 1040 patients, 375 of them are female, and 665 of them are male. Also, the age of these people is between 14 and 91 years, and the average age is about 54 years.	https://paperswithcode.com/dataset/smoking-data-of-hospitalized-covid-19	27/06/2021						
4487	A robot dataset of successful and failed placement executions	"The dataset contains the following data from successful and failed executions of the Toyota HSR robot placing a book on a shelf.

RGB images from the robot's head camera
Depth images from the robot's head camera
Rendered images of the robot's 3D model from the point of view of the robot's head camera
Force-torque readings from a wrist-mounted force-torque sensor
Joint efforts, velocities and positions
extrinsic and intrinsic camera calibration parameters
frame-level anomaly annotations

The anomalies that occur during execution include:

the manipulated book falling down
books on the shelf being disturbed significantly
camera occlusions
robot being disturbed by an external collision

The dataset is split into a train, validation and test set with the following number of trials:

Train: 48 successful trials
Validation: 6 successful trials
Test: 60 anomalous trials and 7 successful trials"	https://paperswithcode.com/dataset/book-placement	03/03/2021						
4488	Reasonable Crowd	The Reasonable Crowd dataset is a dataset to evaluate autonomous driving in a limited operating domain. The data consists of 92 traffic scenarios, with multiple ways of traversing each scenario. Multiple annotators expressed their preference between pairs of scenario traversals.	https://paperswithcode.com/dataset/reasonable-crowd	28/07/2021						
4489	BH-rPPG	BH-rPPG dataset (stands for Beihang University Remote PhotoPlethysmoGraphy) is a dataset consists of 3 lighting conditions with uneven distribution which collected in indoor environment. In order to evaluate the performance of deep learning based rPPG under different lighting conditions, we recruited twelve healthy subjects (11 males and 1 females) on campus, with a mean age of 32, SD of 2.5.	https://paperswithcode.com/dataset/bh-rppg	28/07/2021						
4490	CalCROP21	CalCROP21 is a georeferenced multi-spectral dataset of satellite Imagery and crop labels. It is a semantic segmentation benchmark dataset, for the diverse crops in the Central Valley region of California at 10m spatial resolution using a Google Earth Engine based robust image processing pipeline.	https://paperswithcode.com/dataset/calcrop21	26/07/2021						
4491	IRLCov19	IRLCov19 is a multilingual Twitter dataset related to Covid-19 collected in the period between February 2020 to July 2020 specifically for regional languages in India. It contains more than 13 million tweets.	https://paperswithcode.com/dataset/irlcov19	26/07/2021						
4492	SNARE	SNARE, short for ShapeNet Annotated with Referring Expressions, is a benchmark requires a model to choose which of two objects is being referenced by a natural language description.	https://paperswithcode.com/dataset/snare	26/07/2021						
4493	TinyVIRAT-v2	TinyVIRAT-v2 is a benchmark dataset for recognizing real-world low-resolution activities present in videos. The dataset is comprised of naturally occuring low-resolution actions. This is an extension of the TinyVIRAT dataset and consists of actions with multiple labels. The videos are extracted from security videos which makes them realistic and more challenging.	https://paperswithcode.com/dataset/tinyvirat-v2	24/07/2021						
4494	OLR 2021	The OLR 2021 dataset contains the data for the Oriental Language Recognition (OLR) 2021 Challenge, which intends to improve the performance of language recognition systems and speech recognition systems within multilingual scenarios.	https://paperswithcode.com/dataset/olr-2021	23/07/2021						
4495	Facebook Page-Page	"This webgraph is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site. This graph was collected through the Facebook Graph API in November 2017 and restricted to pages from 4 categories which are defined by Facebook. These categories are: politicians, governmental organizations, television shows and companies. The task related to this dataset is multi-class node classification for the 4 site categories.
This web graph is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site. This graph was collected through the Facebook Graph API in November 2017 and restricted to pages from 4 categories that are defined by Facebook. These categories are: politicians, governmental organizations, television shows, and companies. The task related to this dataset is multi-class node classification for the 4 site categories."	https://paperswithcode.com/dataset/facebook-page-page	28/09/2019	Facebook Page-Page					
4496	Wiki Squirrel	The data was collected from the English Wikipedia (December 2018). These datasets represent page-page networks on specific topics (chameleons, crocodiles and squirrels). Nodes represent articles and edges are mutual links between them. The edges csv files contain the edges - nodes are indexed from 0. The features json files contain the features of articles - each key is a page id, and node features are given as lists. The presence of a feature in the feature list means that an informative noun appeared in the text of the Wikipedia article. The target csv contains the node identifiers and the average monthly traffic between October 2017 and November 2018 for each page. For each page-page network we listed the number of nodes an edges with some other descriptive statistics.	https://paperswithcode.com/dataset/wiki-squirrel	28/09/2019	Wikipedia Squirrel					
4497	BMELD	BMELD is a bilingual (English-Chinese) dialogue corpus for Neural chat translation.	https://paperswithcode.com/dataset/bmeld	23/07/2021						
4498	Open Buildings	"Building footprints are useful for a range of important applications, from population estimation, urban planning and humanitarian response, to environmental and climate science. This large-scale open dataset contains the outlines of buildings derived from high-resolution satellite imagery in order to support these types of uses. The project being based in Ghana, the current focus is on the continent of Africa.
Source: Google AI
Image credit: Google AI"	https://paperswithcode.com/dataset/open-buildings	26/07/2021						
4499	TERRA-REF	"The ARPA-E funded TERRA-REF project is generating open-access reference datasets for the study of plant sensing, genomics, and phenomics. Sensor data were generated by a field scanner sensing platform that captures color, thermal, hyperspectral, and active flourescence imagery as well as three dimensional structure and associated environmental measurements. This dataset is provided alongside data collected using traditional field methods in order to support calibration and validation of algorithms used to extract plot level phenotypes from these datasets.
Data were collected at the University of Arizona Maricopa Agricultural Center in Maricopa, Arizona. 
This site hosts a large field scanner with fifteen sensors, many of which are capable of capturing mm-scale images and point clouds at daily to weekly intervals.
These data are intended to be re-used, and are accessible as a combination of files and databases linked by spatial, temporal, and genomic information. In addition to providing open access data, the entire computational pipeline is open source, and we enable users to access high-performance computing environments.
The study has evaluated a sorghum diversity panel, biparental cross populations, and elite lines and hybrids from structured sorghum breeding populations. 
In addition, a durum wheat diversity panel was grown and evaluated over three winter seasons.
The initial release includes derived data from from two seasons in which the sorghum diversity panel was evaluated.
Future releases will include data from additional seasons and locations.
The TERRA-REF reference dataset can be used to characterize phenotype-to-genotype associations, on a genomic scale, that will enable knowledge-driven breeding and the development of higher-yielding cultivars of sorghum and wheat. 
The data is also being used to develop new algorithms for machine learning, image analysis, genomics, and optical sensor engineering."	https://paperswithcode.com/dataset/terra-ref	19/08/2020	TERRA-REF, An open reference data set from high resolution genomics, phenomics, and imaging sensors					
4500	Deezer User Networks	The data was collected from the music streaming service Deezer (November 2017). These datasets represent friendship networks of users from 3 European countries. Nodes represent the users and edges are the mutual friendships. We reindexed the nodes in order to achieve a certain level of anonimity. The csv files contain the edges -- nodes are indexed from 0. The json files contain the genre preferences of users -- each key is a user id, the genres loved are given as lists. Genre notations are consistent across users. In each dataset users could like 84 distinct genres. Liked genre lists were compiled based on the liked song lists. The countries included are Romania, Croatia and Hungary. For each dataset we listed the number of nodes an edges.	https://paperswithcode.com/dataset/deezer-user-networks	12/02/2018						
4501	Facebook Pages	We collected data about Facebook pages (November 2017). These datasets represent blue verified Facebook page networks of different categories. Nodes represent the pages and edges are mutual likes among them. We reindexed the nodes in order to achieve a certain level of anonimity. The csv files contain the edges -- nodes are indexed from 0. We included 8 different distinct types of pages. These are listed below. For each dataset we listed the number of nodes an edges.	https://paperswithcode.com/dataset/facebook-pages	12/02/2018						
4502	DBP2.0 zh-en	"The DBP2.0 dataset can be downloaded from the figshare repository. It has three entity alignment settings, i.e., ZH-EN, JA-EN and FR-EN. Each setting has the following files:
ent_links: reference entity alignment;
rel_triples_1: relation triples in the ZH or JA or FR KG, list of triples like (h \t r \t t);
rel_triples_2: relation triples in the EN KG;
splits/train_links: training data for entity alignment, list of pairs like (e1 \t e2);
splits/valid_links: validation data for entity alignment;
splits/test_links: test data for entity alignment;
splits/train_unlinked_ent1: training data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/train_unlinked_ent2: training data for dangling entity detection, list of dangling entities in the EN KG;
splits/valid_unlinked_ent1: validation data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/valid_unlinked_ent2: validation data for dangling entity detection, list of dangling entities in the EN KG;
splits/test_unlinked_ent1: test data for dangling entity detection, list of dangling entities in the ZH or JA or FR KG;
splits/test_unlinked_ent2: test data for dangling entity detection, list of dangling entities in the EN KG;
More information see: https://github.com/nju-websoft/OpenEA/tree/master/dbp2.0"	https://paperswithcode.com/dataset/dbp2-0-zh-en	04/06/2021						
4503	EmailSum	"Email Thread Summarization (EmailSum) is a dataset which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. It was developed to spur research in thread summarization.
Source: EMAILSUM: Abstractive Email Thread Summarization
Image source: EMAILSUM: Abstractive Email Thread Summarization"	https://paperswithcode.com/dataset/emailsum	30/07/2021	Email Thread Summarization					
4504	SyDog	"SyDog is a synthetic dataset of dogs containing ground truth pose and bounding box coordinates which was generated using the game engine, Unity.
Source: SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation
Image source: SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation"	https://paperswithcode.com/dataset/sydog	31/07/2021	A Synthetic Dog Dataset					
4505	mTVR	"mTVR is a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations.
Source: MTVR: Multilingual Moment Retrieval in Videos
Image source: MTVR: Multilingual Moment Retrieval in Videos"	https://paperswithcode.com/dataset/mtvr	30/07/2021						
4506	Chest ImaGenome	"Chest ImaGenome is a dataset with a scene graph data structure to describe 242,072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, the following are provided: i) 1256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from 500 unique patients.
Description from: Chest ImaGenome Dataset for Clinical Reasoning
Image source: Chest ImaGenome Dataset for Clinical Reasoning"	https://paperswithcode.com/dataset/chest-imagenome	31/07/2021						
4507	ManiSkill	"ManiSkill is a large-scale learning-from-demonstrations benchmark for articulated object manipulation with visual input (point cloud and image). ManiSkill supports object-level variations by utilizing a rich and diverse set of articulated objects, and each task is carefully designed for learning manipulations on a single category of objects. ManiSkill is equipped with high-quality demonstrations to facilitate learning-from-demonstrations approaches and perform evaluations on common baseline algorithms. ManiSkill can encourage the robot learning community to explore more on learning generalizable object manipulation skills.
Source: ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills
Image source: ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills"	https://paperswithcode.com/dataset/maniskill	30/07/2021						
4508	DadaGP	"DadaGP is a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back.
Description from: DadaGP: A Dataset of Tokenized GuitarPro Songs for Sequence Models
Image source: https://arxiv.org/pdf/2107.14653v1.pdf"	https://paperswithcode.com/dataset/dadagp	30/07/2021						
4509	OpenForensics	"OpenForensics is a large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation. With its rich annotations, the OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection.
Source: OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild
Image source: https://arxiv.org/pdf/2107.14480v1.pdf"	https://paperswithcode.com/dataset/openforensics	30/07/2021						
4510	HR-Crime	HR-Crime is a subset of the UCF-Crime dataset suitable for human-related anomaly detection tasks.	https://paperswithcode.com/dataset/hr-crime	31/07/2021	Human-Related Crime					
4511	USC	"The Uzbek speech corpus (USC) comprises 958 different speakers with a total of 105 hours of transcribed audio recordings. This is the first open-source Uzbek speech corpus dedicated to the ASR task.
Source: USC: An Open-Source Uzbek Speech Corpus and Initial Speech Recognition Experiments"	https://paperswithcode.com/dataset/usc	30/07/2021	Uzbek Speech Corpus					
4512	SoundingEarth	SoundingEarth consists of co-located aerial imagery and audio samples all around the world.	https://paperswithcode.com/dataset/soundingearth	02/08/2021						
4513	OLGA	"The OLGA dataset contains artist similarities from AllMusic, together with content features from AcousticBrainz. With 17,673 artists, this is the largest academic artist similarity dataset that includes content-based features to date. 
Source: Artist Similarity with Graph Neural Networks"	https://paperswithcode.com/dataset/olga	30/07/2021						
4514	TMED	"TMED is a clinically-motivated benchmark dataset for computer vision and machine learning from limited labeled data.
Two overall goals inspired this dataset:
1) We wish to improve timely diagnosis and treatment of aortic stenosis (AS), a common degenerative cardiac valve condition. AS is a particularly important condition where automation holds substantial promise.  Automated screening for AS can increase referral and treatment rates for patients with this life threatening condition.
2) We wish to provide an authentic assessment of semi-supervised learning (SSL) methods to the computer vision and ML research community. Especially in medical contexts, labels are often difficult and expensive to acquire. SSL is promising way to combine a small labeled set (images plus expert annotations) with a large, easy-to-acquire unlabeled set (images only). However, most existing benchmark datasets don't represent the challenges of truly uncurated unlabeled sets in a medical context. We hope our data release catalyzes work on methods for effective multi-task SSL.
The dataset is available for academic use to any researcher who applies for access on our website and agrees to a standard data use agreement (do not share the data with non-approved users, no commercial use, no attempt to reidentify patients, etc.).
Dataset contents
The TMED dataset contains imagery from 2773 patients and supervised labels for two classification tasks from a small subset of 260 patients (because labels are difficult to acquire). All data is de-identified and approved for release by our IRB. Imagery comes from transthoracic echocardiograms acquired in the course of routine care consistent with American Society of Echocardiography (ASE) guidelines, all obtained from 2015-2020 at Tufts Medical Center.
When gathering echocardiogram imagery for each patient, a sonographer manipulates a handheld transducer over the patient’s chest, manually choosing different acquisition angles in order to fully assess the heart’s complex anatomy. This imaging process results in multiple cineloop video clips of the heart depicting various anatomical views. We extract one still image from each available video clip, so each patient study is represented in our dataset as multiple images (typically ~100). Each image is preprocessed to a grayscale 64x64 PNG.
Two kinds of labels are available for the labeled subset of patients:

View labels (PLAX/PSAX/Other), indicating which standard anatomical view is shown by the image. Each image in our fully-labeled set is annotated.
Diagnostic labels (no AS, mild/moderate AS, severe AS), indicating the severity of disease. Each patient in our fully-labeled set is annotated.

For more information, see our website and our published paper at MLHC '21"	https://paperswithcode.com/dataset/tmed	30/07/2021	Tufts Medical Echocardiogram Dataset					
4515	DUC 2006	"There is currently much interest and activity aimed at building powerful multi-purpose information systems. The agencies involved include DARPA, ARDA and NIST. Their programmes, for example DARPA's TIDES (Translingual Information Detection Extraction and Summarization) programme, ARDA's Advanced Question & Answering Program and NIST's TREC (Text Retrieval Conferences) programme cover a range of subprogrammes. These focus on different tasks requiring their own evaluation designs.
Within TIDES and among other researchers interested in document understanding, a group grew up which has been focusing on summarization and the evaluation of summarization systems. Part of the initial evaluation for TIDES called for a workshop to be held in the fall of 2000 to explore different ways of summarizing a common set of documents. Additionally a road mapping effort was started in March of 2000 to lay plans for a long-term evaluation effort in summarization.
Out of the initial workshop and the roadmapping effort has grown a continuing evaluation in the area of text summarization called the Document Understanding Conferences (DUC). Sponsored by the Advanced Research and Development Activity (ARDA), the conference series is run by the National Institute of Standards and Technology (NIST) to further progress in summarization and enable researchers to participate in large-scale experiments."	https://paperswithcode.com/dataset/duc-2006		Document Understanding Conferences					
4516	DUC 2007	"There is currently much interest and activity aimed at building powerful multi-purpose information systems. The agencies involved include DARPA, ARDA and NIST. Their programmes, for example DARPA's TIDES (Translingual Information Detection Extraction and Summarization) programme, ARDA's Advanced Question & Answering Program and NIST's TREC (Text Retrieval Conferences) programme cover a range of subprogrammes. These focus on different tasks requiring their own evaluation designs.
Within TIDES and among other researchers interested in document understanding, a group grew up which has been focusing on summarization and the evaluation of summarization systems. Part of the initial evaluation for TIDES called for a workshop to be held in the fall of 2000 to explore different ways of summarizing a common set of documents. Additionally a road mapping effort was started in March of 2000 to lay plans for a long-term evaluation effort in summarization.
Out of the initial workshop and the roadmapping effort has grown a continuing evaluation in the area of text summarization called the Document Understanding Conferences (DUC). Sponsored by the Advanced Research and Development Activity (ARDA), the conference series is run by the National Institute of Standards and Technology (NIST) to further progress in summarization and enable researchers to participate in large-scale experiments."	https://paperswithcode.com/dataset/duc-2007		Document Understanding Conferences					
4517	raw gaze data	"Data was collected from Tobii Fusion screen-based Eye Tracker. This study collected drivers’ gaze data by letting participants watch dashcam captured videos of driving scenes in the lab setting. Original vidoes are downloaded from https://github.com/Cogito2012/CarCrashDataset. Each video lasts 5 seconds and the frequency of the videos is 10 Hz.
Full data and description can be found: https://github.com/yuli1102/eye_tracker_data"	https://paperswithcode.com/dataset/raw-gaze-data	03/08/2021						
4518	EXTREME CLASSIFICATION	"The objective in extreme multi-label classification is to learn feature architectures and classifiers that can automatically tag a data point with the most relevant subset of labels from an extremely large label set. This repository provides resources that can be used for evaluating the performance of extreme multi-label algorithms including datasets, code, and metrics.
For more details please visit the link http://manikvarma.org/downloads/XC/XMLRepository.html"	https://paperswithcode.com/dataset/extreme-classification	01/05/2013	Extreme Multi-label Classification					
4519	ImageNet-VidVRD	ImageNet-VidVRD dataset contains 1,000 videos selected from ILVSRC2016-VID dataset based on whether the video contains clear visual relations. It is split into 800 training set and 200 test set, and covers common subject/objects of 35 categories and predicates of 132 categories. Ten people contributed to labeling the dataset, which includes object trajectory labeling and relation labeling. Since the ILVSRC2016-VID dataset has the object trajectory annotation for 30 categories already, we supplemented the annotations by labeling the remaining 5 categories. In order to save the labor of relation labeling, we labeled typical segments of the videos in the training set and the whole of the videos in the test set.	https://paperswithcode.com/dataset/imagenet-vidvrd	23/10/2017						
4520	VidOR	VidOR (Video Object Relation) dataset contains 10,000 videos (98.6 hours) from YFCC100M collection together with a large amount of fine-grained annotations for relation understanding. In particular, 80 categories of objects are annotated with bounding-box trajectory to indicate their spatio-temporal location in the videos; and 50 categories of relation predicates are annotated among all pairs of annotated objects with starting and ending frame index. This results in around 50,000 object and 380,000 relation instances annotated. To use the dataset for model development, the dataset is split into 7,000 videos for training, 835 videos for validation, and 2,165 videos for testing.	https://paperswithcode.com/dataset/vidor	27/11/2019						
4521	Euro-PVI	The Euro-PVI dataset contains trajectories of pedestrians and bicyclists, with dense interactions with the ego-vehicle. The dataset is collected in Brussels and Leuven, Belgium. The goal of this dataset is to address the challenge of future trajectory prediction in urban environments with dense pedestrian (bicyclist) - vehicle interactions.	https://paperswithcode.com/dataset/euro-pvi	22/06/2021						
4522	Knot128	"Knot128 is a dataset to test knot untangling algorithms, i.e., highly-tangled configurations that can be difficult to smooth out into a canonical knot embedding. Knot128 is comprised of knots from 128 different isotopy classes; for each class, a tangled embedding, and a canonical embedding are provided. 
Source: http://www.cs.cmu.edu/~kmcrane/Projects/RepulsiveCurves/index.html
Image source: http://www.cs.cmu.edu/~kmcrane/Projects/RepulsiveCurves/index.html"	https://paperswithcode.com/dataset/knot128	14/06/2020						
4523	Trefoil100	"Trefoil100 is a dataset to test knot untangling algorithms, i.e., highly-tangled configurations that can be difficult to smooth out into a canonical knot embedding. Trefoil100 contains 100 tangled embeddings of the trefoil knot.
Source: http://www.cs.cmu.edu/~kmcrane/Projects/RepulsiveCurves/index.html
Image source: http://www.cs.cmu.edu/~kmcrane/Projects/RepulsiveCurves/index.html"	https://paperswithcode.com/dataset/trefoil100	14/06/2020						
4524	DONeRF: Evaluation Dataset	"This is the dataset for the CGF 2021 paper ""DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks"".
Please note the original creators of the individual 3D scenes themselves (individual license files can be found in the individual .zip archives in the dataset):
Bulldozer by ""Heinzelnisse"" (CC-BY-NC): https://www.blendswap.com/blend/11490
Forest by Robin Tran (CC-BY-SA 3.0): https://cloud.blender.org/p/gallery/5fbd186ec57d586577c57417
Classroom by Christophe Seux (CC-0): https://download.blender.org/demo/test/classroom.zip
San Miguel by Guillermo M. Leal Llaguno (CC-BY 3.0): https://casual-effects.com/g3d/data10/index.html#
Pavillon by Hamza Cheggour / ""eMirage"" (CC-BY): https://download.blender.org/demo/test/pabellon_barcelona_v1.scene_.zip
Barbershop by Blender Animation Studio (CC-BY): https://svn.blender.org/svnroot/bf-blender/trunk/lib/benchmarks/cycles/barbershop_interior/"	https://paperswithcode.com/dataset/donerf-evaluation-dataset	17/07/2021						
4525	CF-mMIMO data - measurement at USC	"This repo contains open-source channel measurement data for research and development purposes. 
Copyright Thomas Choi, University of Southern California. The data may be used for non-commercial purposes only. Redistribution prohibited. If you use this data for results presented in research papers, please cite as follows: Data were obtained from [Choi2021Using], whose data are available at [WiDeS_Choi2021Using].
[Choi2021Using] T. Choi et al., ""Using a drone sounder to measure channels for cell-free massive MIMO systems,"" arXiv preprint arXiv:2106.15276, 2021.
[WiDeS_Choi2021Using] T. Choi et al., “Open-Source Cell-Free Massive MIMO Channel Data 2020”. URL: https://wides.usc.edu/research_matlab.html"	https://paperswithcode.com/dataset/cf-mmimo-data-measurement-at-usc	23/05/2021						
4526	Bizarre Pose Dataset	"Human keypoint dataset of anime/manga-style character illustrations.  Extension of the AnimeDrawingsDataset, with additional features:


all 17 COCO-compliant human keypoints


character bounding boxes


2000 additional samples (4000 total) from Danbooru with difficult tags


Useful for pose estimation of illustrated characters, which allows downstream tasks such as pose-guided reference drawing retrieval (e.g. Hermit Purple)."	https://paperswithcode.com/dataset/bizarre-pose-dataset	04/08/2021	Bizarre Pose Dataset of Illustrated Characters					
4527	Anime Drawings Dataset	A dataset for 2D pose estimation of anime/manga images.	https://paperswithcode.com/dataset/anime-drawings-dataset							
4528	EMOPIA	EMOPIA (pronounced ‘yee-mò-pi-uh’) dataset is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators.	https://paperswithcode.com/dataset/emopia	03/08/2021	A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation					
4529	ailabs1k7	https://github.com/YatingMusic/compound-word-transformer	https://paperswithcode.com/dataset/ailabs1k7		AIlabs.tw Pop1K7					
4530	I2L-140K	"Introduced by Singh, Sumeet S.. “Teaching Machines to Code: Neural Markup Generation with Visual Attention.” ArXiv abs/1802.05415 (2018): n. pag.
A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~140k formulas and images splitted into train, validation and test sets. Superset of im2latex-100K dataset."	https://paperswithcode.com/dataset/i2l-140k	15/02/2018						
4531	Im2latex-90k	"Introduced by Singh, Sumeet S.. “Teaching Machines to Code: Neural Markup Generation with Visual Attention.” ArXiv abs/1802.05415 (2018): n. pag.
Sanitized version of im2latex-100K dataset (erroneous samples were removed). A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~90k formulas and images split into train, validation and test sets. Also see I2L-140K which is a superset of this dataset."	https://paperswithcode.com/dataset/im2latex-90k	15/02/2018						
4532	Interactive Media Experience Click Dataset	The dataset contains summary statistics and engagement metrics captured from users in a live, 'in-the-wild' study of an interactive TV show.	https://paperswithcode.com/dataset/interactive-media-experience-click-dataset	04/08/2021						
4533	UAV-based multispectral vineyards	"UAS-based Multispectral othomosaics of vineyards from central Portugal

2 distinct vineyards
Multispectral and HD orthomosaics"	https://paperswithcode.com/dataset/uav-based-multispectral-vineyards							
4534	Multispectral  and HD vineyard orthomosaics from central Portugal	"Multispectral  and HD vineyard orthomosaics from central Portugal

Mulstispectral and HD orthomosaics 
2 distinct vineyards 
ground-truth masks for row detection"	https://paperswithcode.com/dataset/multispectral-and-hd-vineyard-orthomosaics	02/08/2021						
4535	AGAR	The Annotated Germs for Automated Recognition (AGAR) dataset is an image database of microbial colonies cultured on an agar plate. It contains 18000 photos of five different microorganisms, taken under diverse lighting conditions with two different cameras. All images are classified into countable, uncountable, and empty, with the former being labeled by microbiologists with colony location and 5 species identification (336 442 colonies).	https://paperswithcode.com/dataset/agar	03/08/2021	Annotated Germs for Automated Recognition					
4536	Kinetics 400	"The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands.
Source: https://arxiv.org/abs/1705.06950
Image source: https://arxiv.org/abs/1705.06950"	https://paperswithcode.com/dataset/kinetics-400-1	19/05/2017						
4537	VideoRemoval4K	We provide video sequences with annotated object masks for video inpainting. The resolution is 3840 x 2160.	https://paperswithcode.com/dataset/videoremoval4k	04/08/2021						
4538	I-RAVEN	To fix the defacts of RAVEN dataset, we generate an alternative answer set for each RPM question in RAVEN, forming an improved dataset named Impartial-RAVEN (I-RAVEN for short).	https://paperswithcode.com/dataset/i-raven		Impartial-RAVEN					
4539	MedLEA	The MedLEA package provides morphological and structural features of 471 medicinal plant leaves and 1099 leaf images of 31 species and 29-45 images per species.	https://paperswithcode.com/dataset/medlea	15/06/2021	Medicinal Leaves					
4540	Swedish Leaf Dataset	"A dataset of images containing leaves from 15 tree classes. 
Image source: https://www.cvl.isy.liu.se/en/research/datasets/swedish-leaf/"	https://paperswithcode.com/dataset/swedish-leaf-dataset							
4541	Well-being Dataset	The dataset is a private dataset collected for automatic analysis of psychological distress. It contains self-reported distress labels provided by human volunteers. The dataset consists of 30-min interview recordings of participants.	https://paperswithcode.com/dataset/well-being-dataset	01/10/2020	Cambridge Well-being Dataset for Psychological Distress Analysis					
4542	XA Bin-Picking	"XA Bin-Picking is a point-cloud dataset comprising both simulated and real-world scenes with three industrial parts.  Synthesized scenes consists of 1000 training samples. The test samples are real scenes and the ground
truth instance labels are made manually. There are 20 to
30 identical types of parts randomly piled up in a scene.
Each scene contains about 60,000 boundary points. Each
point in the scene has instance annotations. The parts are
texture-less and have no discernible color. Both of training samples and test sam-
ples only contain the boundary points of parts.
Source: A Convolutional Neural Network for Point Cloud Instance Segmentation in Cluttered Scene Trained by Synthetic Data Without Color
Image Source: A Convolutional Neural Network for Point Cloud Instance Segmentation in Cluttered Scene Trained by Synthetic Data Without Color"	https://paperswithcode.com/dataset/xa-dateset	05/03/2020						
4543	German Credit Dataset	"Two datasets are provided. the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file ""german.data"". 
For algorithms that need numerical attributes, Strathclyde University produced the file ""german.data-numeric"". This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables. Several attributes that are ordered categorical (such as attribute 17) have been coded as integer. This was the form used by StatLog. 
This dataset requires use of a cost matrix:
|   | Good | Bad |
|---|---|---|
| Good | 0 | 1 |
| Bad | 5 | 0 |
The rows represent the actual classification and the columns the predicted classification. 
It is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1)."	https://paperswithcode.com/dataset/german-credit-dataset	17/11/1994						
4544	Monkey V1 dataset	"This dataset is used for neural co-training. 
mtl_monkey_dataset: was used for our MTL-Monkey model and involves neural responses that were predicted by a single-task trained model on real monkey V1.
mtl_oracle_dataset: was used for our MTL-Oracle model and involves neural responses that were predicted by our image classification oracle.
mtl_shuffled_dataset: was used for our MTL-Shuffled model and is the result of shuffling the mtl_monkey_dataset across images."	https://paperswithcode.com/dataset/monkey-v1-dataset	29/07/2021						
4545	S&P 500 Intraday Data	"Technical Information
Dates range from 2017-09-11 to 2018-02-16 and the time interval is 1 minute.
This is a MultiIndex CSV file, to load in pandas use:
dataset = pd.read_csv('dataset.csv', index_col=0, header=[0, 1]).sort_index(axis=1)
Stocks that entered or exited the Index during the dataset time range are omitted.
Collection & Processing
These are the scripts used for collecting the data, and also utilities to clean & scale the dataset & convert it to a numpy array:
https://github.com/nickdl/alpha"	https://paperswithcode.com/dataset/s-p-500-intraday-data	19/10/2020	S&P 500 Index Intraday Data with 1min Interval					
4546	Crello	"Crello dataset consists of design templates obtained from online design service, crello.com. The dataset contains designs for various display formats, such as social media posts, banner ads, blog headers, or printed posters, all in a vector format. In dataset construction, design templates and associated resources (e.g., linked images) from crello.com were first downloaded. After the initial data acquisition, the data structure was inspected and identified useful vector graphic information in each template. Next, mal-formed templates or those having more
than 50 elements were eliminated, resulting in 23,182 templates. The data was paritioned to 18,768 / 2,315 / 2,278 examples for train, validation, and test splits.
Source: https://arxiv.org/pdf/2108.01249v1.pdf
Image source: https://arxiv.org/pdf/2108.01249v1.pdf"	https://paperswithcode.com/dataset/crello	28/08/2021	Crello dataset					
4547	World Mortality Dataset	"The World Mortality Dataset contains weekly, monthly, or quarterly all-cause mortality data from 103 countries and territories. It contains country-level data on all-cause mortality in 2015–2021 collected from various sources.
Source: Tracking excess mortality across countries during the COVID-19 pandemic with the World Mortality Dataset
Image source: https://github.com/akarlinsky/world_mortality"	https://paperswithcode.com/dataset/world-mortality-dataset	30/06/2021						
4548	HiRID	"HiRID is a freely accessible critical care dataset containing data relating to almost 34 thousand patient admissions to the Department of Intensive Care Medicine of the Bern University Hospital, Switzerland (ICU), an interdisciplinary 60-bed unit admitting >6,500 patients per year. The ICU offers the full range of modern interdisciplinary intensive care medicine for adult patients. The dataset was developed in cooperation between the Swiss Federal Institute of Technology (ETH) Zürich, Switzerland and the ICU.
The dataset contains de-identified demographic information and a total of 681 routinely collected physiological variables, diagnostic test results and treatment parameters from almost 34 thousand admissions during the period from January 2008 to June 2016. Data is stored with a uniquely high time resolution of one entry every two minutes."	https://paperswithcode.com/dataset/hirid	18/02/2021						
4549	Q-Pain	"Q-Pain, a dataset for assessing bias in medical QA in the context of pain management, one of the most challenging forms of clinical decision-making. 
Source: Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management"	https://paperswithcode.com/dataset/q-pain	03/08/2021						
4550	SROIE	"Consists of a dataset with 1000 whole scanned receipt images and annotations for the competition on scanned receipts OCR and key information extraction (SROIE).
Image source: https://arxiv.org/pdf/2103.10213.pdf"	https://paperswithcode.com/dataset/sroie	18/03/2021						
4551	EPHOIE	"EPHOIE is a fully-annotated dataset which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. 
Source: Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution
Image source: https://github.com/HCIILAB/EPHOIE"	https://paperswithcode.com/dataset/ephoie	24/01/2021						
4552	28 Ghz wireless channel dataset	Our dataset which consists of multiple indoor and outdoor experiments for up to 30 m gNB-UE link. In each experiment, we fixed the location of the gNB and move the UE with an increment of roughly one degrees. The table above specifies the direction of user movement with respect to gNB-UE link, distance resolution, and the number of user locations for which we conduct channel measurements. Outdoor 30 m data also contains blockage between 3.9 m to 4.8 m. At each location, we scan the transmission beam and collect data for each beam. By doing so, we can get the full OFDM channels for different locations along the moving trajectory with all the beam angles. Moreover, we use 240 kHz subcarrier spacing, which is consistent with the 5G NR numerology at FR2, so the data we collect will be a true reflection of what a 5G UE will see.	https://paperswithcode.com/dataset/28-ghz-wireless-channel-dataset	12/01/2021						
4553	Mechanical MNIST Crack Path	The Mechanical MNIST Crack Path dataset contains Finite Element simulation results from phase-field models of quasi-static brittle fracture in heterogeneous material domains subjected to prescribed loading and boundary conditions. For all samples, the material domain is a square with a side length of $1$. There is an initial crack of fixed length ($0.25$) on the left edge of each domain. The bottom edge of the domain is fixed in $x$ (horizontal) and $y$ (vertical), the right edge of the domain is fixed in $x$ and free in $y$, and the left edge is free in both $x$ and $y$. The top edge is free in $x$, and in $y$ it is displaced such that, at each step, the displacement increases linearly from zero at the top right corner to the maximum displacement on the top left corner. Maximum displacement starts at $0.0$ and increases to $0.02$ by increments of $0.0001$ ($200$ simulation steps in total). The heterogeneous material distribution is obtained by adding rigid circular inclusions to the domain using the Fashion MNIST bitmaps as the reference location for the center of the inclusions. Specifically, each center point location is generated randomly inside a square region defined by the corresponding Fashion MNIST pixel when the pixel has an intensity value higher than $10$. In addition, a minimum center-to-center distance limit of $0.0525$ is applied while generating these center points for each sample. The values of Young’s Modulus $(E)$, Fracture Toughness $(G_f)$, and Failure Strength $(f_t)$ near each inclusion are increased with respect to the background domain by a variable rigidity ratio $r$. The background value for $E$ is $210000$, the background value for $G_f$ is $2.7$, and the background value for $f_t$ is $2445.42$. The rigidity ratio throughout the domain depends on position with respect to all inclusion centers such that the closer a point is to the inclusion center the higher the rigidity ratio will be. We note that the full algorithm for constructing the heterogeneous material property distribution is included in the simulations scripts shared on GitHub. The following information is included in our dataset: (1) A rigidity ratio array to capture heterogeneous material distribution reported over a uniform $64\times64$ grid, (2) the damage field at the final level of applied displacement reported over a uniform $256\times256$ grid, and (3) the force-displacement curves for each simulation. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/saeedmhz/phase-field).	https://paperswithcode.com/dataset/mechanical-mnist-crack-path	24/07/2021						
4554	Mechanical MNIST	"Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \times 28$ pixels), the material domain is a $28 \times 28$ unit square. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset).
The paper ""Mechanical MNIST: A benchmark dataset for mechanical metamodels"" can be found at https://doi.org/10.1016/j.eml.2020.100659. All code necessary to reproduce the metamodels demonstrated in the manuscript is available on GitHub (https://github.com/elejeune11/Mechanical-MNIST). For questions, please contact Emma Lejeune (elejeune@bu.edu)."	https://paperswithcode.com/dataset/mechanical-mnist							
4555	CIRR	"Composed Image Retrieval (or, Image Retreival conditioned on Language Feedback) is a relatively new retrieval task, where an input query consists of an image and short textual description of how to modify the image. 
For humans, the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details.
We identify a major challenge of this task as the inherent ambiguity in knowing what information is important (typically one object of interest in the scene) and what can be ignored (e.g., the background and other irrelevant objects).
We release the first dataset of open-domain, real-life images with human-generated modification sentences, which support research on one-shot composed image retrieval, dialogue systems, fine-grained visiolinguistic reasoning, and more."	https://paperswithcode.com/dataset/cirr	09/08/2021	Compose Image Retrieval on Real-life images					
4556	CWRU Bearing Dataset	Data was collected for normal bearings, single-point drive end and fan end defects.  Data was collected at 12,000 samples/second and at 48,000 samples/second for drive end bearing experiments.  All fan end bearing data was collected at 12,000 samples/second.	https://paperswithcode.com/dataset/cwru-bearing-dataset							
4557	KanHope	KanHope is a code mixed hope speech dataset for equality, diversity, and inclusion in Kannada, an under-resourced Dravidian language. The dataset consists of 6,176 user-generated comments in code mixed Kannada crawled from YouTube and manually labelled as bearing hope speech or not-hope speech.	https://paperswithcode.com/dataset/kanhope	10/08/2021	Kannada Hope speech dataset					
4558	BEOID	The BEOID dataset includes object interactions ranging from preparing a coffee to operating a weight lifting machine and opening a door. The dataset is recorded at six locations: kitchen, workspace, laser printer, corridor with a locked door, cardiac gym, and weight-lifting machine. For the first four locations, sequences from five different operators were recorded (two sequences per operator), and from three operators for the last two locations (three sequences per operator). The wearable gaze tracker hardware (ASL Mobile Eye XG) was used to record the dataset. Synchronized wide-lens video data with calibrated 2D gaze fixations are available. Moreover, we release 3D information using a pre-built cloud point map and PTAM tracking. Three-dimensional information of the image and the gaze fixations are included.	https://paperswithcode.com/dataset/beoid		Bristol Egocentric Object Interactions Dataset					
4559	BIDCD	"Bosch Industrial Depth Completion Dataset (BIDCD) is an RGBD dataset for of static table-top scenes with industrial objects. 
The data was collected with a RealSense depth-camera mounted on a robotic arm, i.e. from multiple Points-of-View (POV), approximately 60 for each scene. 
We generated depth ground truth with a customized pipeline for removing erroneous depth values, and applied Multi-View geometry to fuse the cleaned depth frames and fill-in missing information. 
The fused scene mesh was back-projected to each POV, and finally a bi-lateral filter was applied to reduce the remaining holes.
For each scene we provide RGB, raw Depth, Ground-Truth Depth. 
We also provide a corresponding file-system with 3D information: our fused meshes, camera poses, and camera parameters.
A simpler dataset with a Single-Item (SI) in each scene is also provided, using fewer POV, approximately 4 for each scene."	https://paperswithcode.com/dataset/bidcd	10/08/2021	Bosch Industrial Depth Completion Dataset					
4560	VR traffic traces	"The dataset contains traffic traces collected from 3 different VR applications.
Researchers can use this dataset to replicate the behavior of real VR traffic directly in their studies, e.g., their simulations.
Further information can be found in the repository."	https://paperswithcode.com/dataset/vr-traffic-traces	10/08/2021						
4561	The Boston Housing Dataset	This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 cases.	https://paperswithcode.com/dataset/the-boston-housing-dataset							
4562	DeliData	"DeliData is the first publicly available dataset containing collaborative conversations on solving a cognitive task, consisting of 500 group dialogues and 14k utterances.
Source: DeliData: A dataset for deliberation in multi-party problem solving
Image source: https://www.delibot.xyz/#"	https://paperswithcode.com/dataset/delidata	11/08/2021						
4563	IPAC	"IPAC (Icelandic Parallel Abstracts Corpus ) is a new Icelandic-English parallel corpus, composed of abstracts from student theses and dissertations. The texts were collected from the Skemman repository which keeps records of all theses, dissertations and final projects from students at Icelandic universities. The corpus was aligned based on sentence-level BLEU scores, in both translation directions, from NMT models using Bleualign. The result is a corpus of 64k sentence pairs from over 6 thousand parallel abstracts.
Source: Icelandic Parallel Abstracts Corpus"	https://paperswithcode.com/dataset/ipac	11/08/2021	Icelandic Parallel Abstracts Corpus					
4564	FakeAVCeleb	"FakeAVCeleb is a novel Audio-Video Deepfake dataset that not only contains deepfake videos but respective synthesized cloned audios as well. 
Image source: https://arxiv.org/pdf/2108.05080v1.pdf"	https://paperswithcode.com/dataset/fakeavceleb	11/08/2021						
4565	MuSiQue-Ans	MuSiQue-Ans is a new multihop QA dataset with ~25K 2-4 hop questions using seed questions from 5 existing single-hop datasets.	https://paperswithcode.com/dataset/musique-ans	02/08/2021						
4566	Bambara Language Dataset	A Bambara dialectal dataset dedicated for Sentiment Analysis, available freely for Natural Language Processing research purposes	https://paperswithcode.com/dataset/bambara-language-dataset	05/08/2021						
4567	TrUMAn	Trope Understanding in Movies and Animations (TrUMAn) is a dataset intending to evaluate and develop learning systems beyond visual signals.	https://paperswithcode.com/dataset/truman	10/08/2021	Trope Understanding in Movies and Animations					
4568	FoodLogoDet-1500	FoodLogoDet-1500 is a new large-scale publicly available food logo dataset, which has 1,500 categories, about 100,000 images and about 150,000 manually annotated food logo objects.	https://paperswithcode.com/dataset/foodlogodet-1500	10/08/2021						
4569	COMPARE	COMPARE is a taxonomy and a dataset of comparison discussions in peer reviews of research papers in the domain of experimental deep learning.	https://paperswithcode.com/dataset/compare	09/08/2021						
4570	CirCor DigiScope	"CirCor DigiScope is currently the largest pediatric heart sound dataset. A total of 5282 recordings have been collected from the four main auscultation locations of 1568 patients, in the process 215780 heart sounds have been manually annotated. Each cardiac murmur has been manually annotated by an expert annotator according to its timing, shape, pitch, grading and quality.
Source: https://arxiv.org/pdf/2108.00813v1.pdf"	https://paperswithcode.com/dataset/circor-digiscope	02/08/2021						
4571	STF_dense_fog	"We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are:
- We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions.
- The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects.
- In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods.
- Please check out our paper for more information."	https://paperswithcode.com/dataset/stf	24/02/2019	SeeingThroughFog					
4572	OpenStreetMap Multi-Sensor Scene Classification	"A high-resolution multi-sensor remote sensing scene classification dataset, appropriate for training and evaluating image classification models in the remote sensing domain.
The dataset consists of 8400 overhead scenes, each covered by Airbus Pléiades, Airbus SPOT, and USDA NAIP imagery. The scenes are classified into 12 OpenStreetMap categories:

man_made=bridge
man_made=breakwater
building=farm
power=substation
leisure=stadium
leisure=golf_course
waterway=dam
landuse=quarry
landuse=farmland
landuse=forest
natural=water
natural=bare_rock"	https://paperswithcode.com/dataset/openstreetmap-multi-sensor-scene	25/09/2019						
4573	ICFG-PEDES	"One large-scale database for Text-to-Image Person Re-identification, i.e., Text-based Person Retrieval. 
Compared with existing databases, ICFG-PEDES has three key advantages. First, its textual descriptions are identity-centric and fine-grained. Second, the images included in ICFG-PEDES are more challenging, containing more appearance variability due to the presence of complex backgrounds and variable illumination. Third, the scale of ICFG-PEDES is larger."	https://paperswithcode.com/dataset/icfg-pedes	01/07/2021	Identity-Centric and Fine-Grained Person Description Dataset					
4574	Two-probe macaque monkey auditory LFP	"Dataset accompanying paper Klein, N., Siegle, J.H., Teichert, T., Kass, R.E. (2021) ""Cross-population coupling of neural activity based on Gaussian process current source densities"". 
Auditory local field potential (LFP) recordings and evoked multi-unit activity (MUA) from two 24-electrode linear probes (V-Probes from Plexon) inserted in primary auditory cortex of a macaque monkey. The probes were arranged parallel to the iso-frequency bands in primary auditory cortex (A1), and had similar tonal response fields with preferred frequencies close to 1000 Hz. The first probe (which we call the lateral probe) was located centrally in A1, while the second probe (which we call the medial probe) was located more medially and closer to the boundary of A1 with the medio-lateral belt. The medial probe had lower response threshold, shorter MUA latencies, and overall stronger current sinks and sources than the lateral probe. The spacing between electrodes on each probe was 100 microns so that the probe spanned 2,300 microns. The treatment of the animals was in accordance with the guidelines set by the U.S. Department of Health and Human Services (NIH) for the care and use of laboratory animals, and all methods were approved by the Institutional Animal Care and Use Committee at the University of Pittsburgh."	https://paperswithcode.com/dataset/two-probe-macaque-monkey-auditory-lfp	20/04/2021						
4575	Neuropixels single-mouse LFP data	"Single-mouse Neuropixels recordings (spikes and LFPs) in NWB format. Dataset used in the paper ""Cross-population coupling of neural activity based on Gaussian process current source densities"" by Klein, N., Siegle, J.H., Teichert, T., and Kass, R.E. (preprint: https://arxiv.org/abs/2104.10070). 
The data is part of the Allen Brain Observatory Neuropixels dataset (©2019 Allen Institute for Brain Science, available from https://portal.brain-map.org/explore/circuits/visual-coding-neuropixels). Six Neuropixels probes were simultaneously inserted through visual cortex, hippocampus, thalamus, and midbrain. On each probe, LFP data was recorded from up to 374 electrode locations in a checkerboard layout spanning two spatial dimensions: four columns spaced 16 microns apart, with 20 micron spacing between rows. LFP data was acquired at 2500 Hz after applying a 1000 Hz low-pass filter. Boundaries between regions were manually identified based on decreases in unit density as well as physiological signatures (such as elevated theta-band activity in the hippocampus). LFP electrodes without region labels were not included in the analysis. Spike trains and downsampled LFP data for this mouse (subject ID: 730760270; session ID: 755434585) can be accessed via the AllenSDK or via the DANDI Archive https://dandiarchive.org/dandiset/000021/draft. The original LFP data used for this analysis is available as part of the Allen Brain Observatory AWS Public Data Set https://registry.opendata.aws/allen-brain-observatory/."	https://paperswithcode.com/dataset/neuropixels-single-mouse-lfp-data	20/04/2021						
4576	SPACE	SPACE is a simulator for physical Interactions and causal learning in 3D environments. The SPACE simulator is used to generate the SPACE dataset, a synthetic video dataset in a 3D environment, to systematically evaluate physics-based models on a range of physical causal reasoning tasks. Inspired by daily object interactions, the SPACE dataset comprises videos depicting three types of physical events: containment, stability and contact.	https://paperswithcode.com/dataset/space	13/08/2021						
4577	HatemojiCheck	HatemojiCheck is a test suite for detecting emoji-based hate of 3,930 test cases covering seven functionalities of emoji-based hate and six identities.	https://paperswithcode.com/dataset/hatemojicheck	12/08/2021						
4578	WikiScenes	"The WikiScenes dataset consists of paired images and language descriptions capturing world landmarks and cultural sites, with associated 3D models and camera poses. WikiScenes is derived from the massive public catalog of freely-licensed crowdsourced data in the Wikimedia Commons project, which contains a large variety of images with captions and other metadata. 
The dataset contains two forms of textual descriptions for each image: (1) Captions associated with images, describing the image using free-form language, and (2) The WikiCategory hierarchy obtained according to the hierarchy of WikiCategories associated with each image (see the examples in the image below). Overall, WikiScenes contains approximately 63K images with textual descriptions."	https://paperswithcode.com/dataset/wikiscenes	12/08/2021						
4579	ACS PUMS	"ACS PUMS stands for American Community Survey (ACS) Public Use Microdata Sample (PUMS) and has been used to construct several tabular datasets for studying fairness in machine learning:


ACSIncome: to predict whether an individual’s income is above $50,000.


ACSPublicCoverage: to predict whether an individual is covered by public health insurance.


ACSMobility: to predict whether an individual had the same residential address one year ago.


ACSEmployment: to predict whether an individual is employed.


ACSTravelTime: predict whether an individual has a commute to work that is longer than 20 minutes."	https://paperswithcode.com/dataset/acs-pums	10/08/2021						
4580	Computer Vision Values Dataset	This is a corpus of about 500 computer vision datasets, from which the authors sampled 114 dataset publications across different vision tasks and coded for themes through both structured and qualitative content analysis. This work most closely pairs with the following research question: How do dataset developers in CV and NLP research, describe and motivate the decisions that go into their creation?	https://paperswithcode.com/dataset/computer-vision-values-dataset	09/08/2021						
4581	PADv2	"With complex scenes and rich annotations, the PADv2 dataset can be used as a test bed to benchmark affordance detection methods and may also facilitate downstream vision tasks, such as scene understanding, action recognition, and robot manipulation. 
It contains 30k diverse images covering 39 affordance categories as well as 103 object categories from different scenes."	https://paperswithcode.com/dataset/padv2	08/08/2021	Purpose-driven Affordance Dataset v2					
4582	CDR	CDR is a reflection removal dataset that is categorized, diverse, and real-world. The dataset is constructed using diverse glass types under various environments to ensure diversity.	https://paperswithcode.com/dataset/cdr	07/08/2021						
4583	Screen2Words	Screen2Words is a large-scale screen summarization dataset annotated by human workers. The dataset contains more than 112k language summarization across 22k unique UI screens. This dataset can be used for Mobile User Interface Summarization, which is a task where a model generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen.	https://paperswithcode.com/dataset/screen2words	07/08/2021						
4584	Stereo Waterdrop	Steredo Waterdrop is a real-world dataset for research on stereo waterdrop removal. The dataset contains 837 stereo image pairs captured from 129 indoor and outdoor scenes with various waterdrops, disparities, and illumination conditions. We use the ZED 2 stereo camera for data collection.	https://paperswithcode.com/dataset/stereo-waterdrop	07/08/2021						
4585	KTH-TIPS2	"The KTH-TIPS (Textures under varying Illumination, Pose and Scale) image database was created to extend the CUReT database in two directions, by providing variations in scale as well as pose and illumination, and by imaging other samples of a subset of its materials in different settings.
The KTH-TIPS2 databases took this a step further by imaging 4 different samples of 11 materials, each under varying pose, illumination and scale.
For more information about the databases, please view the documentation. The databases may be downloaded here."	https://paperswithcode.com/dataset/kth-tips2	09/06/2006						
4586	VitaminC	"The VitaminC dataset contains more than 450,000 claim-evidence pairs for fact verification and factual consistent generation. Based on over 100,000 revisions to popular Wikipedia pages, and additional ""synthetic"" revisions."	https://paperswithcode.com/dataset/vitaminc	15/03/2021	VitaminC Robust Fact Verification					
4587	SWSR	The Sina Weibo Sexism Review (SWSR) dataset is a dataset to research online sexism in Chinese. The SWSR dataset provides labels at different levels of granularity including (i) sexism or non-sexism, (ii) sexism category and (iii) target type, which can be exploited, among others, for building computational methods to identify and investigate finer-grained gender-related abusive language.	https://paperswithcode.com/dataset/swsr	06/08/2021	Sina Weibo Sexism Review					
4588	iGibson 2.0	iGibson 2.0 is an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations.	https://paperswithcode.com/dataset/igibson-2-0	06/08/2021						
4589	gENder-IT	gENder-IT is an English-Italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the English source side and multiple gender alternative translations, where needed, on the Italian target side.	https://paperswithcode.com/dataset/gender-it	05/08/2021						
4590	WebFG-496	"WebFG-496 is a dataset for fine-grained recognition that contains 200 subcategories of the ""Bird"" (Web-bird), 100 subcategories of the Aircraft"" (Web-aircraft), and 196 subcategories of the ""Car"" (Web-car). It has a total number of 53339 web training images."	https://paperswithcode.com/dataset/webfg-496	05/08/2021						
4591	WDC-Dialogue	"WDC-Dialogue is a dataset built from the Chinese social media to train EVA. Specifically, conversations from various sources are gathered and a rigorous data cleaning pipeline is designed to enforce the quality of WDC-Dialogue. 
The dataset mainly focuses on three categories of textual interaction data, i.e., repost on social media, comment / reply on various online forums and online question and answer (Q&A) exchanges. Each round of these textual interactions yields a dialogue session via well-designed parsing rules."	https://paperswithcode.com/dataset/wdc-dialogue	03/08/2021						
4592	InferWiki	InferWiki is a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, the dataset includes various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation.	https://paperswithcode.com/dataset/inferwiki	03/08/2021						
4593	Marine Debris Turntable	Marine Debris Turntable is a dataset for sonar perception.	https://paperswithcode.com/dataset/marine-debris-turntable	02/08/2021						
4594	RareDis corpus	The RareDis corpus contains more than 5,000 rare diseases and almost 6,000 clinical manifestations are annotated. Moreover, the Inter Annotator Agreement evaluation shows a relatively high agreement (F1-measure equal to 83.5% under exact match criteria for the entities and equal to 81.3% for the relations). Based on these results, this corpus is of high quality, supposing a significant step for the field since there is a scarcity of available corpus annotated with rare diseases.	https://paperswithcode.com/dataset/raredis-corpus	02/08/2021						
4595	WikiChurches	"WikiChurches is a dataset for architectural style classification, consisting of 9,485 images of church buildings. Both images and style labels were sourced from Wikipedia. The dataset can serve as a benchmark for various research fields, as it combines numerous real-world challenges: fine-grained distinctions between classes based on subtle visual features, a comparatively small sample size, a highly imbalanced class distribution, a high variance of viewpoints, and a hierarchical organization of labels, where only some images are labeled at the most precise level.
In addition, we provide 631 bounding box annotations of characteristic visual features for 139 churches from four major categories. These annotations can, for example, be useful for research on fine-grained classification, where additional expert knowledge about distinctive object parts is often available."	https://paperswithcode.com/dataset/wikichurches	17/08/2021						
4596	HVIS Dataset	We propose a new benchmark called Human Video Instance Segmentation (HVIS), which focuses on complex real-world scenarios with sufficient human instance masks and identities. Our dataset contains 805 videos with 1447 detailedly annotated human instances. It also includes various overlapping scenes, which integrates into the most challenging video dataset related to humans.	https://paperswithcode.com/dataset/hvis	16/08/2021	Human Video Instance Segmentation Dataset					
4597	Raw data for NMR-POISE	"The NMR-POISE paper can be found at: Anal. Chem. 2021, 93 (31), 10735–10739 (DOI: 10.1021/acs.analchem.1c01767).
The majority of one- and multi-dimensional NMR experiments, indispensable to chemists in many areas of research, are often run with generic or ""compromise"" parameter values that are not optimised. This is particularly problematic when robust, automated acquisition on a variety of samples is desired. Here we present a Python package, NMR-POISE (Parameter Optimisation by Iterative Spectral Evaluation), with full integration into Bruker’s TopSpin software, that utilises feedback control for on-the-fly, sample-tailored optimisation of NMR experiments. POISE provides a highly extensible and user-friendly framework which allows its core optimisation algorithms to be implemented in a wide variety of scenarios.
The data attached herein provide examples of optimisation procedures where POISE can be used to great effect. The raw NMR data is attached here, together with all of the scripts used for processing and plotting this data (which can be used to directly regenerate the figures in the manuscript).
The raw NMR data is in the ""datasets"" directory, and the processing scripts in the ""figures"" directory. The scripts can be run as long as this directory structure is maintained, but require v0.4.1 of the ""penguins"" Python package: this can be installed using the command ""pip install penguins=0.4.1"" (without quotes). Please refer to the Supporting Information of the POISE paper for more details, including a full description of the individual datasets."	https://paperswithcode.com/dataset/raw-data-for-nmr-poise	16/08/2021						
4598	MobIE	"MobIE is a German-language dataset which is human-annotated with 20 coarse- and fine-grained entity types and entity linking information for geographically linkable entities. The dataset consists of 3,232 social media texts and traffic reports with 91K tokens, and contains 20.5K annotated entities, 13.1K of which are linked to a knowledge base. A subset of the dataset is human-annotated with seven mobility-related, n-ary relation types, while the remaining documents are annotated using a weakly-supervised labeling approach implemented with the Snorkel framework.
The dataset can be used for NER (Named entity recognition), EL (entity linking) and RE (relation extraction), and thus can be used for joint and multi-task learning of these fundamental information extraction tasks."	https://paperswithcode.com/dataset/mobie	16/08/2021						
4599	Who’s Waldo	Who's Waldo is a dataset of 270K image–caption pairs, depicting interactions of people, that is automatically mined from Wikimedia Commons. It is a benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an image.	https://paperswithcode.com/dataset/whos-waldo	16/08/2021						
4600	TUM-VIE	TUM-VIE is an event camera dataset for developing 3D perception and navigation algorithms. It contains handheld and head-mounted sequences in indoor and outdoor environments with rapid motion during sports and high dynamic range. TUM-VIE includes challenging sequences where state-of-the art VIO fails or results in large drift. Hence, it can help to push the boundary on event-based visual-inertial algorithms.	https://paperswithcode.com/dataset/tum-vie	16/08/2021	TUM Stereo Visual-Inertial Event Dataset					
4601	AutoChart	AutoChart is a dataset for chart-to-text generation, a task that consists on generating analytical descriptions of visual plots.	https://paperswithcode.com/dataset/autochart	16/08/2021						
4602	DAHLIA	"DAHLIA dataset 1 is devoted to human activity recognition, which is a major issue for adapting smart-home services such as user assistance.
DAHLIA has been realized in Mobile Mii Platform by CEA LIST, and has been partly supported by ITEA 3 Emospaces Project (https://itea3.org/project/emospaces.html)
Videos were recorded in realistic conditions, with 3 Kinect v2 sensors located as they would be in a real context. The long-range activities were performed in an unconstrained way (participants received only few instructions), and in a continuous (untrimmed) sequence, resulting in long videos (40 min in average per subject). Contrary to previously published databases, in which labeled actions are very short and have low-semantic level, this new database focuses on high-level semantic activities such as « Preparing lunch » or « House Working ».
1 G. Vaquette, A. Orcesi, L. Lucat and C. Achard, ""The DAily Home LIfe Activity Dataset: A High Semantic Activity Dataset for Online Recognition,"" 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), 2017, pp. 497-504, doi: 10.1109/FG.2017.67."	https://paperswithcode.com/dataset/dahlia-daily-human-life-activity	03/06/2017	DAily Human Life Activity					
4603	PIDray	PIDray is a large-scale dataset which covers various cases in real-world scenarios for prohibited item detection, especially for deliberately hidden items. The dataset contains 12 categories of prohibited items in 47, 677 X-ray images with high-quality annotated segmentation masks and bounding boxes.	https://paperswithcode.com/dataset/pidray	16/08/2021						
4604	CallOptionBSM	"This dataset collects 88,077 numerical samples of call options on Shanghai Stock Exchange from 2015-02 to 2020-07.  After the pre-processing, 83,427 samples remain in the data set. This data set records only original quotation of call options on Shanghai
Stock Exchange, and does not include derivative indicators published by stock brokerage firms."	https://paperswithcode.com/dataset/calloptionbsm	16/08/2021						
4605	"Dataset to ""Easing the Conscience with OPC UA: An Internet-Wide Study on Insecure Deployments"""	"This is the dataset to ""Easing the Conscience with OPC UA: An Internet-Wide Study on Insecure Deployments"" [In ACM Internet Measurement Conference (IMC ’20)]. It contains our weekly scanning results between 2020-02-09 and 2020-08-31 complied using our zgrab2 extensions, i.e, it contains an Internet-wide view on OPC UA deployments and their security configurations. To compile the dataset, we anonymized the output of zgrab2, i.e., we removed host and network identifiers from that dataset. More precisely, we mapped all IP addresses, fully qualified hostnames, and autonomous system IDs to numbers as well as removed certificates containing any identifiers. See the README file for more information. Using this dataset we showed that 93% of Internet-facing OPC UA deployments have problematic security configurations, e.g., missing access control (on 24% of hosts), disabled security functionality (24%), or use of deprecated cryptographic primitives (25%). Furthermore, we discover several hundred devices in multiple autonomous systems sharing the same security certificate, opening the door for impersonation attacks. Overall, with the analysis of this dataset we underpinned that secure protocols, in general, are no guarantee for secure deployments if they need to be configured correctly following regularly updated guidelines that account for basic primitives losing their security promises."	https://paperswithcode.com/dataset/dataset-to-easing-the-conscience-with-opc-ua	26/10/2020						
4606	MHMD	MHMD (Modern Historical Movies Dataset) is a dataset for old image colorization, built from historical movies. It consists of 1,353,166 images and 42 labels of eras, nationalities, and garment types for automatic colorization from 147 historical movies or TV series made in modern time.	https://paperswithcode.com/dataset/mhmd	14/08/2021	Modern Historical Movies Dataset					
4607	DensePASS	DensePASS - a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the Pinhole-to-Panoramic transfer and accompanied with pinhole camera training examples obtained from Cityscapes. DensePASS covers both, labelled- and unlabelled 360-degree images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source domain (i.e. pinhole) data.	https://paperswithcode.com/dataset/densepass	13/08/2021						
4608	STN PLAD	"STN PLAD is a high-resolution and real-world image dataset of multiple high-voltage power line components. It has 2,409 annotated objects divided into five classes: transmission tower, insulator, spacer, tower plate, and Stockbridge damper, which vary in size (resolution), orientation, illumination, angulation, and background.
Properties

Image size: 5472×3078 or 5472×3648
Total images: 133
Total instances: 2409
Average instances per image: 18.1
Nº of object classes (different assets): 5
Other stats: 


Abstract
Many power line companies are using UAVs to perform their inspection processes instead of putting their workers at risk by making them climb high voltage power line towers, for instance. A crucial task for the inspection is to detect and classify assets in the power transmission lines. However, public data related to power line assets are scarce, preventing a faster evolution of this area. This work proposes the Power Line Assets Dataset, containing high-resolution and real-world images of multiple high-voltage power line components. It has 2,409 annotated objects divided into five classes: transmission tower, insulator, spacer, tower plate, and Stockbridge damper, which vary in size (resolution), orientation, illumination, angulation, and background. This work also presents an evaluation with popular deep object detection methods, showing considerable room for improvement.
Baseline results

mAP: 89.2%

| Assets             | Average Precision |
|--------------------|-----------------|
| Transmission tower |       0.900       |
| Insulator          |       0.894       |
| Spacer             |       0.856       |
| Tower plate        |       0.971       |
| Stockbridge damper |       0.838       |
| mean           |     0.892     |"	https://paperswithcode.com/dataset/plad	18/08/2021	STN Power Line Assets Dataset					
4609	DocBank-TB	This dataset consisting 500 set of caption, table and coresponding paper page, processed from DocBank.	https://paperswithcode.com/dataset/docbank-tb	18/08/2021	DocBank-Table					
4610	Twitter Sentiment Analysis	This is an entity-level Twitter Sentiment Analysis dataset. For each message, the task is to judge the sentiment of the entire sentence towards a given entity. For example, A outperforms B is positive for entity A but negative for entity B. The dataset contains ~70K labeled training messages and 1K labeled validation messages. It is available online for free on Kaggle.	https://paperswithcode.com/dataset/twitter-sentiment-analysis		Entity-Level Twitter Sentiment Analysis Dataset					
4611	NASA C-MAPSS	Engine degradation simulation was carried out using C-MAPSS. Four different were sets simulated under different combinations of operational conditions and fault modes. Records several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.	https://paperswithcode.com/dataset/nasa-c-mapss	01/01/2008	Turbofan Engine Degradation Simulation Data Set					
4612	HiFiMask	HiFiMask is a large-scale High-Fidelity Mask dataset, namely CASIA-SURF HiFiMask (briefly HiFiMask). It contains a total amount of 54,600 videos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of sensors.	https://paperswithcode.com/dataset/hifimask	13/04/2021	CASIA-SURF HiFiMask					
4613	WebFace260M	"WebFace260M is a million-scale face benchmark, which is constructed for the research community towards closing the data gap behind the industry.
It consists of:
- Noisy 4M identities and 260M faces
- High-quality training data with 42M images of 2M identities by using automatic cleaning
- A test set with rich attributes and a time-constrained evaluation protocol"	https://paperswithcode.com/dataset/webface260m	06/03/2021						
4614	DeepFake MNIST+	DeepFake MNIST+ is a deepfake facial animation dataset. The dataset is generated by a SOTA image animation generator. It includes 10,000 facial animation videos in ten different actions, which can spoof the recent liveness detectors.	https://paperswithcode.com/dataset/deepfake-mnist	18/08/2021						
4615	mvor	"Multi-View Operating Room (MVOR) dataset consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR during real clinical interventions. Each multi-view frame consists of three color and three depth images. The MVOR dataset was sampled from four days of recording in an interventional room at the University Hospital of Strasbourg during procedures such as vertebroplasty and lung biopsy. There are in total 4699 bounding boxes, 2926 2D keypoint annotations, and 1061 3D keypoint annotations.
Source: https://github.com/CAMMA-public/mvor
Image Source: https://github.com/CAMMA-public/mvor"	https://paperswithcode.com/dataset/mvor-1	24/08/2018	Multi-View Operating Room					
4616	ConvRef	"ConvRef is a conversational QA benchmark with reformulations. 
It consists of around 11k natural conversations with about 205k reformulations.
ConvRef builds upon the conversational KG-QA benchmark ConvQuestions.
Questions come from five different domains: books, movies, music, TV series and soccer and answers are Wikidata entities.
 We used conversation sessions in ConvQuestions as input to our user study. Study participants interacted with a baseline QA system, that was trained using the available paraphrases in ConvQuestions as proxies for reformulations. Users were shown follow-up questions in a given conversation interactively, one after the other, along with the answer coming from the baseline QA system. For wrong answers, the user was prompted to reformulate the question up to four times if needed. In this way, users were able to pose reformulations based on previous wrong answers and the conversation history."	https://paperswithcode.com/dataset/convref	11/05/2021						
4617	ISAdetect dataset	"This repository holds two datasets: one with both the original binaries and the code sections extracted from them (“full dataset”), and one with only the code sections (“only code sections”). The code sections were extracted by carving out sections of the binary that were marked as executable. The binaries were scraped from Debian repositories.
There are also two CSV files available, one with full binaries and one with only code sections, which include the 293 features extracted from about 3000 binaries per architecture. These features can be used to train classifiers.
The dataset consists of thousands of binaries for the following 23 architectures: alpha, amd64, arm64, armel, armhf, hppa, i386, ia64, m68k, mips, mips64el, mipsel, powerpc, powerpcspe, powerpc64, powerpc64el, riscv, s390, s390x, sh4, sparc, sparc64 and x32.
There are 98 500 binary files, about 27 gigabytes (uncompressed) of binary files and about 15 gigabytes (uncompressed) of only code sections from those binary files.
Both datasets hold the binaries in directories named by the architecture. The files inside the folders are named as MD5 hashes of the original binary files, and a hash file ending with “.code” contains only the concatenation of all code sections of the original binary file. Each architecture folder also holds a JSON file named after the architecture, e.g. amd64 holds amd64.json. The structure of the JSON file is as follows (described in a JSON Schema-like notation)
This work is based on work by John Clemens, 2015, “Automatic classification of object code using machine learning” and De Nicolao, Pietro et al., 2018, “ELISA: ELiciting ISA of Raw Binaries for Fine-Grained Code and Data Separation”
This dataset is released as part of the following papers:
Sami Kairajärvi, Andrei Costin, and Timo Hämäläinen. 2020. ISAdetect: Usable automated detection of ISA (CPU architecture and endianness) for executable binary files and object code. In Tenth ACM Conference on Data and Application Security and Privacy (CODASPY’20), March 16–18, 2020, New Orleans, LA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3374664.3375742
Kairajärvi, Sami, Andrei Costin, and Timo Hämäläinen. ""Towards usable automated detection of CPU architecture and endianness for arbitrary binary files and object code sequences."" arXiv preprint arXiv:1908.05459 (2019).
Kairajärvi, Sami. ""Automatic identification of architecture and endianness using binary file contents."" (2019).
The code associated with this dataset can be found at https://github.com/kairis/isadetect
Changelog: version 6 - 29.3.2020
Add Weka models

version 5 - 17.1.2020
Clean up dataset

version 4 - 13.1.2020
Initial release"	https://paperswithcode.com/dataset/isadetect-dataset	20/01/2020	ISAdetect binary file and object code dataset					
4618	BBBC041	P. vivax (malaria) infected human blood smears with bounding box annotations. The data consists of two classes of uninfected cells (RBCs and leukocytes) and four classes of infected cells (gametocytes, rings, trophozoites, and schizonts).	https://paperswithcode.com/dataset/bbbc041	28/06/2012	P. vivax (malaria) infected human blood smears					
4619	SoMeSci	Knowledge about software used in scientific investigations is important for several reasons, for instance, to enable an understanding of provenance and methods involved in data handling. However, software is usually not formally cited, but rather mentioned informally within the scholarly description of the investigation, raising the need for automatic information extraction and disambiguation. Given the lack of reliable ground truth data, we present SoMeSci - Software Mentions in Science - a gold standard knowledge graph of software mentions in scientific articles. It contains high quality annotations (IRR: κ = .82) of 3756 software mentions in 1367 PubMed Central articles. Besides the plain mention of the software, we also provide relation labels for additional information, such as the version, the developer, a URL or citations. Moreover, we distinguish between different types, such as application, plugin or programming environment, as well as different types of mentions, such as usage or creation. To the best of our knowledge, SoMeSci is the most comprehensive corpus about software mentions in scientific articles, providing training samples for Named Entity Recognition, Relation Extraction, Entity Disambiguation, and Entity Linking. Finally, we sketch potential use cases and provide baseline results for the different tasks.	https://paperswithcode.com/dataset/somesci	20/08/2021	Software Mentions in Scientific Articles					
4620	MuDoCo_QueryRewrite	"<Task description: joint learning of coreference resolution and query rewrite>
Given an ongoing dialogue between a user and a dialogue assistant, for the user query, the model is required to predict both coreference links between the query and the dialogue context, and the self-contained rewritten user query that is independent to the dialogue context.
<Dataset>
The MuDoCo dataset is a public dataset that contains 7.5k task-oriented multi-turn dialogues across 6 domains (calling, messaging, music, news, reminders, weather). Each dialogue turn is annotated with coreference links (links field). Please refer to the paper of the MuDoCo dataset for more details. Upon on the MuDoCo dataset, we annotate the query rewrite for each utterance, including both user and system turns.  More details are provided in https://github.com/apple/ml-cread."	https://paperswithcode.com/dataset/mudoco-queryrewrite	20/05/2021	The MuDoCo dataset with Query Rewrite Annotations					
4621	VerbCL	"VerbCL is a dataset that consists of the citation graph of court opinions, which cite previously published court opinions in support of their arguments. In particular, it focuses on the verbatim quotes, i.e., where the text of the original opinion is directly reused.
VerbCL is derived from CourtListener and introduces the task of highlight extraction as a single-document summarization task based on the citation graph."	https://paperswithcode.com/dataset/verbcl	23/08/2021						
4622	BugRepo	BugRepo maintains a collection of bug reports that are publicly available for research purposes. Bug reports are a main data source for facilitating NLP-based research in software engineering. We categorize the datasets into the following research directions.	https://paperswithcode.com/dataset/bugrepo	12/07/2018	Bug Reports					
4623	THRED	"This is two-hop relation extraction dataset derived from WikiHop dataset 1. 
1  Johannes Welbl and Pontus Stenetorp and Sebastian Riedel. Constructing Datasets
for Multi-hop Reading Comprehension Across Documents. TACL, 2018."	https://paperswithcode.com/dataset/thred	21/08/2021	Two-Hop Relation Extraction Dataset					
4624	HiXray	HiXray is a High-quality X-ray security inspection image dataset, which contains 102,928 common prohibited items of 8 categories. It has been gathered from the real-world airport security inspection and annotated by professional security inspectors	https://paperswithcode.com/dataset/hixray	23/08/2021						
4625	Invisible Mobile Keyboard Dataset	Invisible Mobile Keyboard Dataset contains user initial, age, type of mobile devices, size of the screen, time taken for typing each phrase, and annotation of typed phrases with coordinate values of the typed position (x and y points). The collected dataset  is the first and only dataset for a novel IMK decoding task.	https://paperswithcode.com/dataset/invisible-mobile-keyboard-dataset	20/08/2021						
4626	MSDA	"5 domains: synthetic domain, document domain, street view domain, handwritten domain, and car license domain
over five million images"	https://paperswithcode.com/dataset/msda	24/08/2021	Multi-source domain adaptation dataset for text recognition					
4627	MAPS	MAPS – standing for MIDI Aligned Piano Sounds – is a database of MIDI-annotated piano recordings. MAPS has been designed in order to be released in the music information retrieval research community, especially for the development and the evaluation of algorithms for single-pitch or multipitch estimation and automatic transcription of music. It is composed by isolated notes, random-pitch chords, usual musical chords and pieces of music. The database provides a large amount of sounds obtained in various recording conditions.	https://paperswithcode.com/dataset/maps	01/01/2010	Midi Aligned Piano Dataset					
4628	LLVIP	"Visible-infrared Paired Dataset for Low-light Vision
30976  images (15488  pairs)
24 dark  scenes, 2 daytime scenes
Support for  image-to-image translation (visible to infrared, or infrared to visible), visible and infrared image fusion, low-light pedestrian detection, and infrared pedestrian detection"	https://paperswithcode.com/dataset/llvip	24/08/2021	A Visible-infrared Paired Dataset for Low-light Vision					
4629	VisEvent	"VisEvent (Visible-Event benchmark) is a dataset constructed for the evaluation of tracking by combing visible and event cameras. VisEvent is featured in: 
Large-scale: 820 video sequences (RGB video + Event flows), contains 371,128 frames, 500 / 320 for the train / testing respectively; 
High-quality Dense Annotation: Manual annotation with careful inspection in each frame; 
Multiple-baseline: Dual-modality SOTA trackers.
Image Source: VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows"	https://paperswithcode.com/dataset/visevent	11/08/2021						
4630	FLUE	FLUE is a French Language Understanding Evaluation benchmark. It consists of 5 tasks: Text Classification, Paraphrasing, Natural Language Inference, Constituency Parsing and Part-of-Speech Tagging, and Word Sense Disambiguation.	https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation	11/12/2019	French Language Understanding Evaluation					
4631	Images from camera traps in the Jura and Ain counties (France)	This dataset contains images taken from camera traps set up in the Jura and Ain counties in France. We use this dataset to illustrate the training of a deep learning algorithm with application to animal specie sidentification. See more here https://github.com/oliviergimenez/computo-deeplearning-occupany-lynx.	https://paperswithcode.com/dataset/images-from-camera-traps-in-the-jura-and-ain	25/08/2021						
4632	MAST	A new data consolidation called Multi-Attributed and Structured Text-to-face (MAST) dataset. The motivation is to have a large corpus of high-quality face images with fine-grained and attribute-focussed annotations. This has the benefits of the attribute oriented approach as well as the semantics in a textual description.	https://paperswithcode.com/dataset/mast	25/08/2021	Multi-Attributed Structured Text-to-face Dataset					
4633	FlickrStyle10K	FlickrStyle10K is collected and built on Flickr30K image caption dataset. The original FlickrStyle10K dataset has 10,000 pairs of images and stylized captions including humorous and romantic styles. However, only 7,000 pairs from the ofﬁcial training set are now publicly accessible. The dataset can be downloaded via https://zhegan27.github.io/Papers/FlickrStyle_v0.9.zip	https://paperswithcode.com/dataset/flickrstyle10k	01/07/2017						
4634	PEM Fuel Cell Dataset	This dataset are about Nafion 112 membrane standard tests and MEA activation tests of PEM fuel cell in various operation condition. Dataset include two general electrochemical analysis method, Polarization and Impedance curves. In this dataset, effect of different pressure of H2/O2 gas, different voltages and various humidity conditions in several steps are considered. Behavior of PEM fuel cell during distinct operation condition tests, activation procedure and different operation condition before and after activation analysis can be concluded from data. In Polarization curves, voltage and power density change as a function of flows of H2/O2 and relative humidity. Resistance of the used equivalent circuit of fuel cell can be calculated from Impedance data. Thus, experimental response of the cell is obvious in the presented data, which is useful in depth analysis, simulation and material performance investigation in PEM fuel cell researches.	https://paperswithcode.com/dataset/pem-fuel-cell-dataset	27/02/2020	Proton Exchange Membrane (PEM) Fuel Cell Dataset					
4635	OSLD	"Open Set Logo Detection Dataset (OSLD Dataset) is a dataset of eCommerce product images with associated brand logo images. It is released under creative commons (CC BY-NC 4.0) license to promote research in open set logo detection. The dataset can be used only for research purposes. The dataset contains:

20K eCommerce product images, with logo bounding box annotations
12.1K logo classes with 20.8K canonical logo images
Product image logo bounding box to canonical logo image match pair annotations

Image source: https://arxiv.org/pdf/1911.07440.pdf"	https://paperswithcode.com/dataset/osld	18/11/2019	Open Set Logo Detection Dataset					
4636	ESPADA	We present a new aerial image dataset, named ESPADA, intended for the training of deep neural networks for depth image estimation from a single aerial image. Given the difficulty of creating aerial image datasets containing image pairs of chromatic images related to their depth images, simulators such as AirSim have been proposed to generate synthetic images from photorealistic scenes. The latter enables the generation of thousands of images that can be used to train and evaluate neural models. However, we argue that synthetic photorealistic aerial image datasets can be improved by adding images generated from photogrammetric models imported into the simulator, thus enabling a less artificial generation of both chromatic and depth images. To assess the quality of these images, we compare the performance of 4 deep neural networks whose pre-trained models and code for re-training are publicly available. We also use ORB-SLAM, in its RGB-D version, to indirectly assess the estimated depth image. To accomplish this, chromatic images from 3 aerial videos and their depth images, estimated with the networks trained with ESPADA, are fed into ORB-SLAM. The estimated camera pose is compared against the trajectory retrieved from the GPS flight trajectory. Our results indicate that images generated from photogrammetric models improve the performance of depth estimation from a single aerial image.	https://paperswithcode.com/dataset/espada	04/08/2021	Extended Synthetic and Photogrammetric Aerial-Image Dataset					
4637	AP-10K	"AP-10K is the first large-scale benchmark for general animal pose estimation, to facilitate the research in animal pose estimation. AP-10K consists of 10,015 images collected and filtered from 23 animal families and 60 species following the taxonomic rank and high-quality keypoint annotations labeled and checked manually.
Source: AP-10K: A Benchmark for Animal Pose Estimation in the Wild
Image source: https://arxiv.org/pdf/2108.12617v1.pdf"	https://paperswithcode.com/dataset/ap-10k	28/08/2021						
4638	N15News	"N15News is a large-scale multimodal news dataset comprising 200K imagetext pairs and 15 categories, which exceeding the previous news dataset in both the number of categories and samples.
Image source: https://arxiv.org/pdf/2108.13327v1.pdf"	https://paperswithcode.com/dataset/n15news	30/08/2021						
4639	cats_vs_dogs	"A large set of images of cats and dogs. 
Homepage: https://www.microsoft.com/en-us/download/details.aspx?id=54765
Source code: tfds.image_classification.CatsVsDogs
Versions:
4.0.0 (default): New split API (https://tensorflow.org/datasets/splits)
Download size: 786.68 MiB
Source: https://www.tensorflow.org/datasets/catalog/cats_vs_dogs"	https://paperswithcode.com/dataset/cats-vs-dogs							
4640	GESTURES	"This is the dataset to support the paper:
Fernando Pérez-García et al., 2021, Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures.
The paper has been accepted for publication at the 24th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2021).
A preprint is available on arXiv: https://arxiv.org/abs/2106.12014
Contents:
1) A CSV file ""seizures.csv"" with the following fields:
- Subject: subject number
- Seizure: seizure number
- OnsetClonic: annotation marking the onset of the clonic phase
- GTCS: whether the seizure generalises
- Discard: whether one (Large, Small), none (No) or both (Yes) views were discarded for training.
2) A folder ""features_fpc_8_fps_15"" containing two folders per seizure.
The folders contain features extracted from all possible snippets from the small (S) and large (L) views. The snippets were 8 frames long and downsampled to 15 frames per second. The features are in "".pth"" format and can be loaded using PyTorch: https://pytorch.org/docs/stable/generated/torch.load.html
The last number of the file name indicates the frame index. For example, the file ""006_01_L_000015.pth"" corresponds to the features extracted from a snippet starting one second into the seizure video. Each file contains 512 numbers representing the deep features extracted from the corresponding snippet.
3) A description file, ""README.txt"""	https://paperswithcode.com/dataset/gestures	05/07/2021	Generalized Epileptic Seizure classification from video-Telemetry Using REcurrent convolutional neural networkS					
4641	ASR-GLUE	The ASR-GLUE benchmark is a collection of 6 different NLU (Natural Language Understanding) tasks for evaluating the performance of models under automatic speech recognition (ASR) error across 3 different levels of background noise and 6 speakers with various voice characteristics.	https://paperswithcode.com/dataset/asr-glue	30/08/2021						
4642	SHIFT15M	"SHIFT15M is a dataset that can be used to properly evaluate models in situations where the distribution of data changes between training and testing. 
The SHIFT15M dataset has several good properties: (i) Multiobjective. Each instance in the dataset has several numerical values that can be used as target variables. (ii) Large-scale. The SHIFT15M dataset consists of 15million fashion images. (iii) Coverage of types of dataset shifts. SHIFT15M contains multiple dataset shift problem settings (e.g., covariate shift or target shift). SHIFT15M also enables the performance evaluation of the model under various magnitudes of dataset shifts by switching the magnitude."	https://paperswithcode.com/dataset/shift15m	30/08/2021						
4643	VesselGraph	VesselGraph is a dataset of whole-brain vessel graphs based on specific imaging protocols. Specifically, vascular graphs are extracted using a refined graph extraction scheme leveraging the volume rendering engine Voreen and provided in an accessible and adaptable form through the OGB and PyTorch Geometric dataloaders.	https://paperswithcode.com/dataset/vesselgraph	30/08/2021						
4644	MEDIC	MEDIC is a large social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. It consists data from several data sources such as CrisisMMD, data from AIDR and Damage Multimodal Dataset (DMD).	https://paperswithcode.com/dataset/medic	29/08/2021						
4645	CrossedWires	"CrossedWires is a living dataset of models and hyperparameters that exposes semantic differences between two popular deep learning frameworks: PyTorch and Tensorflow. 
The CrossedWires dataset currently consists of models trained on CIFAR10 images using three different computer vision architectures: VGG16, ResNet50 and DenseNet121 across a large hyperparameter space. Using hyperparameter optimization, each of the three models was trained on 400 sets of hyperparameters suggested by the HyperSpace search algorithm. 
The CrossedWires dataset includes PyTorch and Tensforflow models with test accuracies as different as 0.681 on syntactically equivalent models and identical hyperparameter choices. The 340 GB dataset and benchmarks presented here include the performance statistics, training curves, and model weights for all 1200 hyperparameter choices, resulting in 2400 total models. The CrossedWires dataset provides an opportunity to study semantic differences between syntactically equivalent models across popular deep learning frameworks."	https://paperswithcode.com/dataset/crossedwires	29/08/2021						
4646	HeadlineCause	HeadlineCause is a dataset for detecting implicit causal relations between pairs of news headlines. The dataset includes over 5000 headline pairs from English news and over 9000 headline pairs from Russian news labeled through crowdsourcing. The pairs vary from totally unrelated or belonging to the same general topic to the ones including causation and refutation relations.	https://paperswithcode.com/dataset/headlinecause	28/08/2021						
4647	TIMo	TIMo (Time-of-Flight Indoor Monitoring) is a dataset of infrared and depth videos intended for the use in Anomaly Detection and Person Detection/People Counting. It features more than 1,500 sequences for anomaly detection, which sum up to more than 500,000 individual frames. For person detection the dataset contains more than than 240 sequences. The data was captured using a Microsoft Azure Kinect RGB-D camera. In addition, we provide annotations of anomalous frame ranges for use with anomaly detection and bounding boxes and segmentation masks for use with person detection. The data was captured in parts from a tilted view and a top-down perspective.	https://paperswithcode.com/dataset/timo	27/08/2021	TIMo (Time-of-Flight Indoor Monitoring)					
4648	Lyra	Lyra is a dataset for code generation that consists on Python code with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real usage projects. Each program is paired with both a Chinese comment and an English comment.	https://paperswithcode.com/dataset/lyra	27/08/2021						
4649	BSARD	The Belgian Statutory Article Retrieval Dataset (BSARD) is a French native corpus for studying statutory article retrieval. BSARD consists of more than 22,600 statutory articles from Belgian law and about 1,100 legal questions posed by Belgian citizens and labeled by experienced jurists with relevant articles from the corpus.	https://paperswithcode.com/dataset/bsard	26/08/2021	Belgian Statutory Article Retrieval Dataset					
4650	ReadingBank	ReadingBank is a benchmark dataset for reading order detection built with weak supervision from WORD documents, which contains 500K document images with a wide range of document types as well as the corresponding reading order information.	https://paperswithcode.com/dataset/readingbank	26/08/2021						
4651	WikiNLDB	"WikiNLDB is a novel dataset for training Natural Language Databases (NLDBs) which is generated by transforming structured data from Wikidata into natural language facts and queries.
Image source: https://arxiv.org/pdf/2106.01074v1.pdf"	https://paperswithcode.com/dataset/wikinldb	02/06/2021						
4652	UQ NIDS Datasets	A comprehensive dataset, merging all the aforementioned datasets. The newly published dataset represents the benefits of shared dataset feature sets, where the merging of multiple smaller ones is possible. This will eventually lead to a bigger and more universal NIDS datasets containing flows from multiple network setups and different attack settings. An additional label feature identifying the original dataset of each flow. This can be used to compare the same attack scenarios conducted over two or more different test-bed networks. The attack categories have been modified to combine all parent categories. Attacks named DoS attacks-Hulk, DoS attacks-SlowHTTPTest, DoS attacks-GoldenEye and DoS attacks-Slowloris have been renamed to the parent DoS category. Attacks named DDOS attack-LOIC-UDP, DDOS attack-HOIC and DDoS attacks-LOIC-HTTP have been renamed to DDoS. Attacks named FTP-BruteForce, SSH-Bruteforce, Brute Force -Web and Brute Force -XSS have been combined as a brute-force category. Finally, SQL Injection attacks have been included in the injection attacks category. The NF-UQ-NIDS dataset has a total of 11,994,893 records, out of which 9,208,048 (76.77%) are benign flows and 2,786,845 (23.23%) are attacks. The table below lists the distribution of the final attack categories.	https://paperswithcode.com/dataset/uq-nids-datasets	27/01/2021	Machine Learning-Based NIDS Datasets					
4653	UQ NetFlow NIDS v1	A comprehensive dataset, merging all the aforementioned datasets. The newly published dataset represents the benefits of shared dataset feature sets, where the merging of multiple smaller ones is possible. This will eventually lead to a bigger and more universal NIDS datasets containing flows from multiple network setups and different attack settings. An additional label feature identifying the original dataset of each flow. This can be used to compare the same attack scenarios conducted over two or more different test-bed networks. The attack categories have been modified to combine all parent categories. Attacks named DoS attacks-Hulk, DoS attacks-SlowHTTPTest, DoS attacks-GoldenEye and DoS attacks-Slowloris have been renamed to the parent DoS category. Attacks named DDOS attack-LOIC-UDP, DDOS attack-HOIC and DDoS attacks-LOIC-HTTP have been renamed to DDoS. Attacks named FTP-BruteForce, SSH-Bruteforce, Brute Force -Web and Brute Force -XSS have been combined as a brute-force category. Finally, SQL Injection attacks have been included in the injection attacks category. The NF-UQ-NIDS dataset has a total of 11,994,893 records, out of which 9,208,048 (76.77%) are benign flows and 2,786,845 (23.23%) are attacks. The table below lists the distribution of the final attack categories.	https://paperswithcode.com/dataset/uq-netflow-nids-v1	18/11/2020	Machine Learning-Based NIDS Datasets					
4654	AMFDS	"Arabic Multi Fonts Dataset
A multi-word multi-font Arabic word-image dataset. 
AMDS is a dataset of Arabic word images.
The dataset was generated using the TextImagesToolkit
https://github.com/msfasha/TextImagesToolkit.
The database of comprised of a number of binary files and text files.
The binary files stores all the image files in binary format.\
The text file include information about the image word and the location of that image in the binary file.
The binary file format is suitable for transferring images to the cloud, in addition to faster loading process which is suitable for large number of images.
More information about the dataset can be found at:
https://github.com/msfasha/Arabic-Multi-Fonts-Dataset/edit/main/README.md"	https://paperswithcode.com/dataset/amfds	04/09/2020	Arabic  Multi-Fonts  Dataset					
4655	VideoMatte240K	"VideoMatte240K consists of 484 high-resolution green screen videos and generate a total of 240,709 unique frames of alpha mattes and foregrounds with chroma-key software Adobe After Effects. The videos are purchased as stock footage or found as royalty-free materials online. 384 videos are at 4K resolution and 100 are in HD. The videos are split by 479 : 5 to form the train and validation sets. The dataset consists of a vast amount of human subjects, clothing, and poses that are helpful for training robust models.
Image source: https://arxiv.org/pdf/2012.07810v1.pdf"	https://paperswithcode.com/dataset/videomatte240k	14/12/2020						
4656	PhotoMatte85	"PhotoMatte85 contains 85 protrait images. The dataset is donated to us by a third-party commercial company. The footage are shot with professional studio lighting and the subjects are in standard portrait posing. We provide the alpha matte and foreground images extracted from the green screen photos. Due to license issue, we will not release the other 13K images used in training.
Source: https://grail.cs.washington.edu/projects/background-matting-v2/#/datasets
Image source: https://grail.cs.washington.edu/projects/background-matting-v2/#/datasets"	https://paperswithcode.com/dataset/photomatte85	14/12/2020						
4657	Phy-Q	"Phy-Q is a benchmark that requires an agent to reason about physical scenarios and take an action accordingly. Inspired by the physical knowledge acquired in infancy and the capabilities required for robots to operate in real-world environments, the authors identify 15 essential physical scenarios. For each scenario, a wide variety of distinct task templates are created, and all the task templates within the same scenario can be solved by using one specific physical rule. 
By having such a design, two distinct levels of generalization can be evaluated, namely the local generalization and the broad generalization.  The benchmark gives a Phy-Q (physical reasoning quotient) score that reflects the physical reasoning ability of the agents."	https://paperswithcode.com/dataset/phy-q	31/08/2021						
4658	UQ NIDS Datasets (FlowMeter Format)	CICFlowMeter format of the datasets are made up of 83 features.	https://paperswithcode.com/dataset/uq-nids-datasets-flowmeter-format	15/04/2021	Machine Learning-Based NIDS Datasets					
4659	CICIDS2018	"CICIDS2018 includes seven different attack scenarios: Brute-force, Heartbleed, Botnet, DoS, DDoS, Web attacks, and infiltration of the network from inside. The attacking infrastructure includes 50 machines and the victim organization has 5 departments and includes 420 machines and 30 servers. The dataset includes the captures network traffic and system logs of each machine, along with 80 features extracted from the captured traffic using CICFlowMeter-V3.
Source: https://www.unb.ca/cic/datasets/ids-2018.html"	https://paperswithcode.com/dataset/cicids2018		CSE-CIC-IDS2018 on AWS					
4660	PCC	"The Potsdam Commentary Corpus (PCC) is a corpus of 220 German newspaper commentaries (2.900 sentences, 44.000 tokens) taken from the online issues of the Märkische Allgemeine Zeitung (MAZ subcorpus) and Tagesspiegel (ProCon subcorpus) and is annotated with a range of different types of linguistic information.
The central subcorpus that we are making publicly available consists of 176 MAZ texts, which are annotated with

Sentence Syntax
Coreference
Discourse Structure (RST & PDTB)
Aboutness topics"	https://paperswithcode.com/dataset/pcc		Potsdam Commentary Corpus					
4661	MASC	"The Manually Annotated Sub-Corpus (MASC) consists of approximately 500,000 words of contemporary American English written and spoken data drawn from the Open American National Corpus (OANC).
All of MASC includes manually validated annotations for sentence boundaries, token, lemma and POS;
noun and verb chunks; named entities (person, location, organization, date); Penn Treebank syntax;
coreference; and discourse structure.
Additional manually produced or validated annotations have been produced by the MASC project
for portions of the sub-corpus, including full-text annotation for FrameNet frame elements
and a 100K+ sentence corpus with WordNet 3.1 sense tags, of which one-tenth are also annotated for
FrameNet frame elements.
Annotations of all or portions of the sub-corpus for a wide variety of other linguistic phenomena
have been contributed by other projects, including PropBank, TimeBank, Pittsburgh opinion, and several others.
Unlike most freely available corpora including a wide variety of linguistic annotations,
MASC contains a balanced selection of texts from a broad range of genres."	https://paperswithcode.com/dataset/masc		Manually Annotated Sub-Corpus					
4662	EVIL	To automatically generate Python and assembly programs used for security exploits, we curated a large dataset for feeding NMT techniques. A sample in the dataset consists of a snippet of code from these exploits and their corresponding description in the English language. We collected exploits from publicly available databases (exploitdb, shellstorm), public repositories (e.g., GitHub), and programming guidelines. In particular, we focused on exploits targeting Linux, the most common OS for security-critical network services, running on IA-32 (i.e., the 32-bit version of the x86 Intel Architecture). The dataset is stored in the folder EVIL/datasets and consists of two parts: i) Encoders: a Python dataset, which contains Python code used by exploits to encode the shellcode; ii) Decoders: an assembly dataset, which includes shellcode and decoders to revert the encoding.	https://paperswithcode.com/dataset/evil	01/09/2021						
4663	FinQA	"FinQA is a new large-scale dataset with Question-Answering pairs over Financial reports, written by financial experts. The dataset contains 8,281 financial QA
pairs, along with their numerical reasoning processes."	https://paperswithcode.com/dataset/finqa	01/09/2021						
4664	Common Objects in 3D	"Common Objects in 3D is a large-scale dataset with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects.
Source: Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction"	https://paperswithcode.com/dataset/common-objects-in-3d	01/09/2021						
4665	Creative Style Responses	Raw responses of ~10,000 people to a simple survey of creative habits. The numeric responses are an ordinal scale 1-5 for questions that ask about 2 contrasting creative habits/preferences along a given habit dimension. The endpoints of the scale are in the name of the column. The Discipline field is a 'check all that apply' question. These tags were mapped to 3 broad disciplines in the paper.  See 'CreativeStyle_Responses_Tagged_Cleaned.xlsx' for processed data where creative habits are assigned as tags.	https://paperswithcode.com/dataset/creativestyle-responses-csv	29/08/2021						
4666	Discipline Mapping	Mapping of detailed discipline tags to one of three broader disciplines (Arts, Science, Business)	https://paperswithcode.com/dataset/discipline-mapping-csv	29/08/2021						
4667	Gender Mapping	Mapping of free text gender entries to one of three genders: Male, Female, Non-Binary.	https://paperswithcode.com/dataset/gender-mapping-csv	29/08/2021						
4668	Shadow Accrual Maps	Large-scale shadows from buildings in a city play an important role in determining the environmental quality of public spaces. They can be both beneficial, such as for pedestrians during summer, and detrimental, by impacting vegetation and by blocking direct sunlight. Determining the effects of shadows requires the accumulation of shadows over time across different periods in a year. In our paper Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows over Time, we present a simple yet efficient class of approach that uses the properties of sun movement to track the changing position of shadows within a fixed time interval. This repository presents the computed shadow information for New York City, Chicago, Los Angeles, Boston and Washington DC.	https://paperswithcode.com/dataset/shadow-accrual-maps	09/07/2019						
4669	Vlogs	Given a  video and its transcript, which human actions are visible in the video?	https://paperswithcode.com/dataset/vlogs	10/06/2019						
4670	Creative Habit Tags	Survey responses where all creative habit ordinal responses were converted to Creative Habit Tags - these tags were used in the analysis to build a network of people linked if they share similar creative habit sets, or a network of creative habits linked if the co-occur in the similar sets of people.	https://paperswithcode.com/dataset/creativestyle-responses-tagged-cleaned-xlsx	29/08/2021						
4671	Business Matching	"This is a proprietary dataset from a large internet services company of ranked pairs of relevant and irrelevant businesses for different queries, for a total
of 17,069 pairs. How well a query matches a candidate is represented by 41 features."	https://paperswithcode.com/dataset/business-matching	12/06/2019						
4672	Wiki Talk Page Comments	This public dataset contains 127,820 comments from Wikipedia Talk Pages labeled with whether or not they are toxic	https://paperswithcode.com/dataset/wiki-talk-page-comments							
4673	W3C Experts	This is a subset of the TREC 2005 enterprise track data, and consists of 48 topics and 200 candidates per topic, with each candidate labeled as an expert or non-expert for the topic. The task is to rank the candidates based on their expertise on a topic, using a corpus of mailing lists from the World Wide Web Consortium (W3C). This is an application where the  unconstrained algorithm does better for the minority protected group.	https://paperswithcode.com/dataset/w3c-experts	20/06/2017						
4674	SMAC	The StarCraft Multi-Agent Challenge (SMAC) is a benchmark that provides elements of partial observability, challenging dynamics, and high-dimensional observation spaces. SMAC is built using the StarCraft II game engine, creating a testbed for research in cooperative MARL where each game unit is an independent RL agent.	https://paperswithcode.com/dataset/smac	11/02/2019	The StarCraft Multi-Agent Challenge					
4675	Source Code Tagger Training Set	"Ensemble Tagger Training and Testing Set
This data includes two files: The training set used to create the SCANL Ensemble tagger 1 and the ""unseen"" testing set that includes words from systems that are not available in the training set. These are derived from a prior dataset of Grammar Patterns; described in a different paper 2. Within each of these csv files, you'll find several columns. We explain these columns below:


Type (only in training set) - Type (or return type) of the identifier to which current word belongs.


Identifier - The full identifier from which the current word was split.


Grammar Pattern - The sequence of part-of-speech tags generated by splitting the identifier into words and annotating with part-of-spech tags.


Word - The current word; derived by splitting the corresponding identifier.


SWUM annotation - The annotation that the SWUM POS tagger applied to a given word.


POSSE annotation - The annotation that the POSSE POS tagger applied to a given word.


Stanford annotation - The annotation that the Stanford POS tagger applied to a given word.


Flair annotation - The annotation that the FLAIR POS tagger applied to a given word.


Position - The position of a given word within its original identifier. For example, given an identifier: GetXMLReaderHandler, Get is in position 1, XML is in position 2, Reader is in position 3 and Handler is in position 4.


Identifier size (max position) - The length, in words, of the identifier of which the word was originally part. 


Normalized position - We normalized the position metric described above such that the first word in the identifier is in position 1, all middle words are in position 2, and the last word is in position 3. For example, given an identifier: GetXMLReaderHandler, Get is in position 1, XML is in position 2, Reader is in position 2 and Handler is in position 3. The reason for this feature is to mitigate the sometimes-negative effect of very long identifiers 2.


Context - The dataset contains five categories of identifier name: function, parameter, attribute, declaration, and class. We provide the category to which the given identifier belongs as one of the features to allow the ensemble to learn patterns that are more pervasive for certain identifier types versus others. For example, function identifiers contain verbs at a higher rate than other types of identifiers 2.


Correct - The correct part-of-speech tag for the current word.


System - System in which the current word was found. 


Identifier Code - Each identifier has a unique number. Each word that has the same number is a part of the same identifier. For example, you can concatenate each word with a code of 0 to recreate the original identifier.


Context
The numbers under the context feature represent the following categories (number -> category):
1.  attribute
2.  class
3.  declaration
4.  function
5.  parameter
Best Features
We found 1 that the best features, of the features described above, were
1. SWUM
2. POSSE
3. Stanford
4. Normalized position
5. Context
Tagset
The tagset that we use is a subset of Penn treebank. Each of our annotations and an example can be found below. Further examples and definitions can be found in the paper 1
| Abbreviation | Expanded Form                           | Examples                                                        |
|--------------|-----------------------------------------|-----------------------------------------------------------------|
| N            | noun                                    | Disneyland, shoe, faucet, mother, bedroom                       |
| DT           | determiner                              | the, this, that, these, those, which                            |
| CJ           | conjunction                             | and, for, nor, but, or, yet, so                                 |
| P            | preposition                             | behind, in front of, at, under, beside, above, beneath, despite |
| NPL          | noun plural                             | streets, cities, cars, people, lists, items, elements.          |
| NM           | noun modifier (adjective)               | red, cold, hot, scary, beautiful, happy, faster, small          |
| NM           | noun modifier (noun-adjunct italicized) | employeeName, filePath, fontSize, userId              |
| V            | verb                                    | run, jump, drive, spin                                          |
| VM           | verb modifier (adverb)                  | very, loudly, seriously, impatiently, badly                     |
| PR           | pronoun                                 | she, he, her, him, it, we, us, they, them, I, me, you           |
| D            | digit                                   | 1, 2, 10, 4.12, 0xAF                                            |
| PRE          | preamble (e.g., Hungarian)              | Gimp, GLEW, GL, G, p_, m_, b_                                   |
Word of Caution
Flair and Stanford recognize a larger number of verb conjugations (e.g., VBZ, VBD) than the ensemble, Posse, and SWUM. We left these conjugations in just in case someone wants to use them. If you are uninterested in using these conjugations, you should normalized them to just V-- inline with our tagset.
Identifier Naming Structure Catalogue
We have put together a catalogue of identifier naming structures in source code. This catalogue explains a lot more about why this work is important, how we are using the ensemble tagger and why the tagset looks the way it does.
The actual tagger implementation
You can find the tagger that was trained using this data here: https://github.com/SCANL/ensemble_tagger
Please cite the paper!


C.  D.  Newman,  M.  J.  Decker,  R.  S.  AlSuhaibani,  A.  Peruma,  S.  Mohapatra,  T.  Vishoi, M. Zampieri, M. W. Mkaouer, T. J. Sheldon, and E. Hill,  ""An Ensemble Approach for Annotating Source Code Identifiers with Part-of-speech Tags,"" in IEEE Transactions on Software Engineering, doi: 10.1109/TSE.2021.3098242.


Christian D. Newman, Reem S. Alsuhaibani, Michael J. Decker, Anthony Peruma, Dishant Kaushik, Mohamed Wiem Mkaouer, Emily Hill,
On the generation, structure, and semantics of grammar patterns in source code identifiers, Journal of Systems and Software, 2020, 110740, ISSN 0164-1212, https://doi.org/10.1016/j.jss.2020.110740. (http://www.sciencedirect.com/science/article/pii/S0164121220301680) 


Interested in our research?
Check out https://scanl.org/"	https://paperswithcode.com/dataset/source-code-tagger-training-set	01/09/2021						
4676	VIVOS	"VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for Automatic Speech Recognition task.
The corpus was prepared by AILAB, a computer science lab of VNUHCM - University of Science, with Prof. Vu Hai Quan is the head of.
We publish this corpus in hope to attract more scientists to solve Vietnamese speech recognition problems. The corpus should only be used for academic purposes."	https://paperswithcode.com/dataset/vivos	01/12/2016	VIVOS Corpus					
4677	catbAbI QA-mode	"We aim to improve the bAbI benchmark as a means of developing intelligent dialogue agents. To this end, we propose concatenated-bAbI (catbAbI): an infinite sequence of bAbI stories. catbAbI is generated from the bAbI dataset and during training, a random sample/story from any task is drawn without replacement and concatenated to the ongoing story. The preprocessig for catbAbI addresses several issues: it removes the supporting facts, leaves the questions embedded in the story, inserts the correct answer after the question mark, and tokenises the full sample into a single sequence of words. As such, catbAbI is designed to be trained in an autoregressive way and analogous to closed-book question answering.
catbAbI models can be trained in two different ways: language modelling mode (LM-mode)
or question-answering mode (QA-mode). In LM-mode, the catbAbI models are trained like
autoregressive word-level language models. In QA-mode, the catbAbI models are only trained to
predict the tokens that are answers to questions—making it more similar to regular bAbI. QA-mode
is simply implemented by masking out losses on non-answer predictions. In both training modes,
the model performance is solely measured by its accuracy and perplexity when answering the
questions."	https://paperswithcode.com/dataset/catbabi	16/11/2020	concatenated-bAbI					
4678	catbAbI LM-mode	"We aim to improve the bAbI benchmark as a means of developing intelligent dialogue agents. To this end, we propose concatenated-bAbI (catbAbI): an infinite sequence of bAbI stories. catbAbI is generated from the bAbI dataset and during training, a random sample/story from any task is drawn without replacement and concatenated to the ongoing story. The preprocessig for catbAbI addresses several issues: it removes the supporting facts, leaves the questions embedded in the story, inserts the correct answer after the question mark, and tokenises the full sample into a single sequence of words. As such, catbAbI is designed to be trained in an autoregressive way and analogous to closed-book question answering.
catbAbI models can be trained in two different ways: language modelling mode (LM-mode) or question-answering mode (QA-mode). In LM-mode, the catbAbI models are trained like autoregressive word-level language models. In QA-mode, the catbAbI models are only trained to predict the tokens that are answers to questions—making it more similar to regular bAbI. QA-mode is simply implemented by masking out losses on non-answer predictions. In both training modes, the model performance is solely measured by its accuracy and perplexity when answering the questions."	https://paperswithcode.com/dataset/catbabi-lm-mode	16/11/2020	concatenated-bAbI					
4679	Security of Alerting Authorities in the WWW: Measuring Namespaces, DNSSEC, and Web PKI	"This data set includes all raw data (e.g., collected certificates) of the WWW 2021 paper ""Security of Alerting Authorities in the WWW: Measuring Namespaces, DNSSEC, and Web PKI"".

Current certificates in use by AA hosts.
CT-logged certificates used by AA hosts."	https://paperswithcode.com/dataset/secure-communications-of-alerting-authorities	24/08/2020						
4680	The Rise of Certificate Transparency and Its Implications on the Internet Ecosystem	"This includes all data from the ACM IMC 2018 paper ""The Rise of Certificate Transparency and Its Implications on the Internet Ecosystem""."	https://paperswithcode.com/dataset/the-rise-of-certificate-transparency-and-its	21/09/2018						
4681	SHAD3S	"We introduce the SHAD3S dataset, that for a given contour representation of a mesh, under a given illumination condition, provides the illumination masks on the object, a shadow mask on the ground, its diffuse and sketch renders.
Dataset creation code"	https://paperswithcode.com/dataset/shad3s	13/11/2020	SHAD3S Dataset					
4682	Exposure-Errors	A dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image.	https://paperswithcode.com/dataset/exposure-errors	25/03/2020						
4683	sRGB2XYZ Dataset	The sRGB2XYZ dataset contains ~1,200 pairs of camera-rendered sRGB and the corresponding scene-referred CIE XYZ images (971 training, 50 validation, and 244 testing images).	https://paperswithcode.com/dataset/srgb2xyz-dataset	23/06/2020						
4684	Raw2raw dataset	This dataset consists of an unpaired and paired set of images captured by two different smartphone cameras: Samsung Galaxy S9 and iPhone X. The unpaired set includes 196 images captured by each smartphone camera (total of 392). The paired set includes 115 pair of images used for testing. In addition to this paired set, we have another small set of 22 anchor paired images	https://paperswithcode.com/dataset/raw2raw-dataset	25/06/2021	Raw2raw dataset					
4685	Landscape Dataset	Landscape Dataset consists of landscape images collected from Flickr.	https://paperswithcode.com/dataset/landscape-dataset	23/11/2020	Landscape Dataset					
4686	Portrait Dataset	A portrait dataset of images collected from Flickr.	https://paperswithcode.com/dataset/portrait-dataset	23/11/2020	Portrait Dataset					
4687	DBFC Dataset	This dataset includes Direct Borohydride Fuel Cell (DBFC) impedance and polarization test in anode with Pd/C, Pt/C and Pd decorated Ni–Co/rGO catalysts. In fact, different concentration of Sodium Borohydride (SBH), applied voltages and various anode catalysts loading with explanation of experimental details of electrochemical analysis are considered in data. Voltage, power density and resistance of DBFC change as a function of weight percent of SBH (%), applied voltage and amount of anode catalyst loading that are evaluated by polarization and impedance curves with using appropriate equivalent circuit of fuel cell. Can be stated that interpretation of electrochemical behavior changes by the data of related cell is inevitable, which can be useful in simulation, power source investigation and depth analysis in DB fuel cell researches.	https://paperswithcode.com/dataset/single-dbfc-dataset	23/04/2020	Single Direct Borohydride Fuel Cell Dataset					
4688	CREAK	"A testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities with commonsense inferences.
Image source: https://arxiv.org/pdf/2109.01653v1.pdf"	https://paperswithcode.com/dataset/creak	03/09/2021						
4689	CameraFusion	"We present a novel approach to reference-based super-resolution (RefSR) with the focus on real-world dual-camera super-resolution (DCSR).
This dataset currently consists of 143 pairs of telephoto and wide-angle images in 4K resolution captured by smartphone dual-cameras.
See our paper for more details: Dual-Camera Super-Resolution with Aligned Attention Modules."	https://paperswithcode.com/dataset/camerafusion	03/09/2021						
4690	MSU Shot Boundary Detection Benchmark	This is a dataset for a shot boundary detection task. The dataset contains 2 existing datasets and 19 manually marked up open source videos with a total length of more than 1200 minutes and 10000 scene transitions. The dataset includes different types of videos with different resolutions from 360×288 to 1920×1080 in MP4 and MKV formats. Videos include samples in RGB scale or in grayscale with FPS from 23 to 60.	https://paperswithcode.com/dataset/msu-shot-boundary-detection-benchmark	13/01/2021						
4691	Real-world graphs for betweenness-centrality ranking estimation	"The ground truth betweenness-centralities for the real-world graphs are provided by AlGhamdi et al. (2017), which are computed by the parallel implementation of Brandes algorithm on a 96000-core supercomputer. The ground truth scores for the synthetic networks are provided by Fan et al. (2019) and are computed using the graph-tool (Peixoto, 2014) library.
The presented approach is compared to several baseline models. The performance of those models are adopted from the benchmark provided by Fan et al. (2019):
ABRA (Riondato & Upfal, 2018): Samples pairs of nodes until the desired accuracy is reached. Where the error tolerance λ was set to 0.01 and the probability δ was set to 0.1.
RK (Riondato & Kornaropoulos, 2014): The number of pairs of nodes is determined by the diameter of the network. Where the error tolerance and the probability were set similar to ABRA.
k-BC (Pfeffer & Carley, 2012): Does only k steps of Brandes algorithm (Brandes, 2001) which was set to 20% of the diameter of the network.
KADABRA (Borassi & Natale, 2019): Uses bidirectional BFS to sample the shortest paths. The variant where it computest the top-k% nodes with the highest betweenness-centrality was used. The error tolerance and probability were set to be the same as ABRA and RK.
Node2Vec (Grover & Leskovec, 2016): Uses a biased random walk to aggregate information from the neighbors. The vector representations of each node were then mapped with a trained MLP to ranking scores.
DrBC (Fan et al., 2019): Shallow graph convolutional network that outputs a ranking score for each node by propagating through the neighbors with a walk length of 5."	https://paperswithcode.com/dataset/real-world-graphs-for-betweenness-centrality							
4692	MOD	"MOD is a large-scale open-domain multimodal dialogue dataset incorporating abundant Internet memes into utterances. The dataset consists of ∼45K Chinese conversations with ∼606K utterances. Each conversation contains about 13 utterances with about 4 Internet memes on average and each utterance equipped with an Internet meme is annotated with the corresponding emotion.
Image source: https://arxiv.org/pdf/2109.01839v1.pdf"	https://paperswithcode.com/dataset/mod-1	04/09/2021	Meme incorporated Open-domain Dialogue					
4693	EVIL-Encoders	"This dataset contains samples to generate Python code for security exploits. In order to make the dataset representative of real exploits, it includes code snippets drawn from exploits from public databases. Differing from general-purpose Python code found in previous datasets, the Python code of real exploits entails low-level operations on byte data for obfuscation purposes (i.e., to encode shellcodes). Therefore, real exploits make extensive use of Python instructions for converting data between different encoders, for performing low-level arithmetic and logical operations, and for bit-level slicing, which cannot be found in the previous general-purpose Python datasets. 
In total, we built a dataset that consists of 1,114 original samples of exploit-tailored Python snippets and their corresponding intent in the English language. These samples include complex and nested instructions, as typical of Python programming. 
In order to perform more realistic training and for a fair evaluation, we left untouched the developers' original code snippets and did not decompose them. We provided English intents to describe nested instructions altogether. 
In order to bootstrap the training process for the NMT model, we include in our dataset both the original, exploit-oriented snippets and snippets from a previous general-purpose Python dataset. This enables the NMT model to generate code that can mix general-purpose and exploit-oriented instructions. Among the several datasets for Python code generation, we choose the Django dataset due to its large size. This corpus contains 14,426 unique pairs of Python statements from the Django Web application framework and their corresponding description in English.
Therefore, our final dataset contains 15,540 unique pairs of Python code snippets alongside their intents in natural language."	https://paperswithcode.com/dataset/evil-encoders	01/09/2021						
4694	EVIL-Decoders	"This is an assembly dataset built on top of Shellcode_IA32, a dataset for automatically generating assembly from natural language descriptions that consists of 3,200 assembly instructions, commented in the English language, which were collected from shellcodes for IA-32 and written for the Netwide Assembler (NASM) for Linux.
In order to make the data more representative of the code that we aim to generate (i.e., complete exploits, inclusive of decoders to be delivered in the shellcode), we enriched the dataset with further samples of assembly code, drawn from the exploits that we collected from public databases. Different from the previous dataset, the new one includes assembly code from real decoders used in actual exploits. The final dataset contains 3,715 unique pairs of assembly code snippets/English intents. 
To better support developers in the automatic generation of the assembly programs, we looked beyond a one-to-one mapping between natural language intents and their corresponding code. 
Therefore, the dataset includes 783 lines (~21% of the dataset) of multi-line snippets, i.e., intents that generate multiple lines of assembly code, separated by the newline character (\n). These multi-line snippets contain a number of different assembly instructions that can range between 2 and 5."	https://paperswithcode.com/dataset/evil-decoders	01/09/2021						
4695	Failure-Dataset-OpenStack	This failure dataset contains information on the events collected in the OpenStack cloud computing platform during three different campaigns of fault-injection experiments performed with three different workloads.	https://paperswithcode.com/dataset/failure-dataset-openstack	29/06/2021						
4696	SemEval-2021 Task 11: NLPContributionGraph	NLPContributionGraph was introduced as Task 11 at SemEval 2021 for the first time. The task is defined on a dataset of Natural Language Processing (NLP) scholarly articles with their contributions structured to be integrable within Knowledge Graph infrastructures such as the Open Research Knowledge Graph. The structured contribution annotations are provided as (1) Contribution sentences : a set of sentences about the contribution in the article; (2) Scientific terms and relations: a set of scientific terms and relational cue phrases extracted from the contribution sentences; and (3) Triples: semantic statements that pair scientific terms with a relation, modeled toward subject-predicate-object RDF statements for KG building. The Triples are organized under three (mandatory) or more of twelve total information units (viz., ResearchProblem, Approach, Model, Code, Dataset, ExperimentalSetup, Hyperparameters, Baselines, Results, Tasks, Experiments, and AblationAnalysis).	https://paperswithcode.com/dataset/semeval-2021-task-11-nlpcontributiongraph	16/08/2020						
4697	Chest-Xray8 (COVID-19)	"This dataset contains 1125 X-ray images of the studied individuals’ chests, including
125 images labeled as COVID-19, 500 images labeled as pneumonia, and 500 images labeled
as no findings."	https://paperswithcode.com/dataset/chest-xray8-covid-19							
4698	PlantVillage	The PlantVillage dataset consists of 54303 healthy and unhealthy leaf images divided into 38 categories by species and disease.	https://paperswithcode.com/dataset/plantvillage	25/11/2015						
4699	S-COCO	Synthetic COCO (S-COCO) is a synthetically created dataset for homography estimation learning. It was introduced by DeTone et al., where the source and target images are generated by duplicating the same COCO image. The source patch $I_S$ is generated by randomly cropping a source candidate at position $p$ with a size of 128 ×128 pixels. Then the patch’s corners are randomly perturbed vertically and horizontally by values within the range [−$\rho$,$\rho$] and the four correspondences define a homography $H_{ST}$ . The inverse of this homography $H_{TS} = (H_{ST} )^{-1}$ is applied to the target candidate and from the resulted warped image a target patch $I_T$ is cropped at the same location p. Both $I_S$ and $I_T$ are the input data with the homography $H_{ST}$ as ground truth.	https://paperswithcode.com/dataset/s-coco	13/06/2016	Synthetic COCO					
4700	PDS-COCO	Photometrically Distorted Synthetic COCO (PDS-COCO) dataset is a synthetically created dataset for homography estimation learning. The idea is exactly the same as in the Synthetic COCO (S-COCO) dataset with SSD-like image distortion added at the beginning of the whole procedure: the first step involves adjusting the brightness of the image using randomly picked value $\delta_b \in \mathcal{U}(-32, 32)$. Next, contrast, saturation and hue noise is applied with the following values: $\delta_c \in \mathcal{U}(0.5, 1.5)$, $\delta_s \in \mathcal{U}(0.5, 1.5)$ and $\delta_h \in \mathcal{U}(-18, 18)$. Finally, the color channels of the image are randomly swapped with a probability of $0.5$. Such a photometric distortion procedure is applied to the original image independently to create source and target candidates.	https://paperswithcode.com/dataset/pds-coco	20/04/2021	Photometrically Distorted Synthetic COCO					
4701	BioLeaflets	BioLeaflets is a biomedical dataset for Data2Text generation. It is a corpus of 1,336 package leaflets of medicines authorised in Europe, which were obtained by scraping the European Medicines Agency (EMA) website. Package leaflets are included in the packaging of medicinal products and contain information to help patients use the product safely and appropriately, under the guidance of their healthcare professional. Each document contains six sections: 1) What is the product and what is it used for 2) What you need to know before you take the product 3) product usage instructions 4) possible side effects, 5) product storage conditions 6) other information.	https://paperswithcode.com/dataset/bioleaflets	03/09/2021						
4702	CholecT50	"CholecT50 is a dataset of endoscopic videos of laparoscopic cholecystectomy surgery introduced to enable research on fine-grained action recognition in laparoscopic surgery.
It is annotated with triplet information in the form of \textlangle{instrument, verb, target}\textrangle{}.
The dataset is a collection of 50 videos consisting of 45 videos from the Cholec80 dataset  and 5 videos from an in-house dataset of the same surgical procedure.
It is an extension of CholecT40 with 10 additional videos and standardized classes."	https://paperswithcode.com/dataset/cholect50	14/03/2021	Cholecystectomy Action Triplet					
4703	WTW	"WTW (Wired Table in the Wild)  is a large-scale dataset which includes well-annotated structure parsing of multiple style tables in several scenes like the photo, scanning files, web pages.
WTW dataset has 10970 training samples and 3611 testing ones. The test images are divided into 7 challenging categories.
Dataset for trains and test contain images and labels. The label is in XML format, which has cell bbox and the structure label, includes start row, end row, start col, end col, and table id. In addition, the test set also contains separate file descripts sub-classification information for each image."	https://paperswithcode.com/dataset/wtw	06/09/2021	Wired Table in the Wild					
4704	PMPC	PMPC (Persona Match on Persona-Chat) is a dataset for Speaker Persona Detection (SPD) which aims to detect speaker personas based on the plain conversational text.	https://paperswithcode.com/dataset/pmpc	03/09/2021	Persona Match on Persona-Chat					
4705	MultiEURLEX	MultiEURLEX is a multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. The dataset covers 23 official EU languages from 7 language families.	https://paperswithcode.com/dataset/multieurlex	02/09/2021						
4706	M-PCCD	"The emerging MPEG point cloud codecs (V-PCC and G-PCC variants) are assessed, and best practices for rate allocation are investigated 1. For this purpose, three experiments are conducted. In the first experiment, a rigorous evaluation of the codecs is performed, adopting test conditions dictated by experts of the group on a carefully selected set of models, using both subjective and objective quality assessment methodologies. In the other two experiments, different rate allocation schemes for geometry-only and geometry-plus-color encoding are subjectively evaluated, in order to draw conclusions on the best-performing approaches in terms of perceived quality for a given bit rate.
In this webpage, we make publicly available quality scores associated with the stimuli under assessment for each experiment. For purposes of reproducibility, a content that is used while not being part of established point cloud repositories adopted by standardisation bodies, is re-distributed. Moreover, scripts are provided in order to generate the reference models and the rendering-related meta-data that are used in this study."	https://paperswithcode.com/dataset/m-pccd		MPEG Point Cloud Compression Dataset					
4707	FunKPoint	FunKPoint is a dataset for finding correspondences in visual data that has ground truth correspondences for 10 tasks and 20 object categories.	https://paperswithcode.com/dataset/funkpoint	02/09/2021						
4708	MFH	The MFH dataset is a multi-viewpoint fine-grained hand hygiene dataset. It contains 73,1147 samples in total, which are collected by 6 camera views in 6 different locations. All samples are split into 7 classes in total. MFH dataset is distinguished from existing datasets in three aspects: the large intra-class difference, the subtle inter-class difference, and the data mismatch in distribution between the training phase and the inference phase. This dataset thus provides a more realistic benchmark.	https://paperswithcode.com/dataset/mfh	07/09/2021						
4709	Hummingbird	Hummingbird is a dataset to examine stylistic lexical cues from human perception and BERT used to characterize their discrepancy. In HUMMINGBIRD crowd-workers relabeled benchmarking datasets for style classification tasks.	https://paperswithcode.com/dataset/hummingbird	06/09/2021						
4710	WhyAct	WhyAct is a dataset for identifying human action reasons in online videos, consisting of 1,077 visual actions manually annotated with their reasons.	https://paperswithcode.com/dataset/whyact	06/09/2021						
4711	Frustrated Legislators: Replication data and code	"Description: Replication data and code for
Aref, S., and Neal, Z.P., ""Identifying hidden coalitions in the US House of Representatives by optimally partitioning signed networks based on generalized balance"" (2021) Scientific Reports. http://dx.doi.org/10.1038/s41598-021-98139-w"	https://paperswithcode.com/dataset/frustrated-legislators-replication-data-and							
4712	TREx-2p	TREx-2p is a dataset to probe whether a pretrained LM possesses “indirect” 2-hop knowledge. It is a 2-hop variant of the T-REx dataset. It has been built by manually examining the 2-hop link existing in the knowledge graph of TREx-1p, and select eight 2- hop relation types that make sense to humans	https://paperswithcode.com/dataset/trex-2p	06/09/2021						
4713	Pano3D	Pano3D  is a new benchmark for depth estimation from spherical panoramas. Its goal is to drive progress for this task in a consistent and holistic manner.  The Pano3D 360 depth estimation benchmark provides a standard Matterport3D train and test split, as well as a secondary GibsonV2 partioning for testing and training as well. The latter is used for zero-shot cross dataset transfer performance assessment and decomposes it into 3 different splits, each one focusing on a specific generalization axis.	https://paperswithcode.com/dataset/pano3d	06/09/2021						
4714	VoicePrivacy 2020	VoicePrivacy 2020 is a dataset for developing anonymization solutions for speech technology. It is built from subsets of existing datasets such as: LibriSpeech, LibriTTS, VoxCeleb1, VoxCeleb2 and VCTK.	https://paperswithcode.com/dataset/voiceprivacy-2020	01/09/2021						
4715	ImageTBAD	"A dataset of A 3D Computed Tomography (CT) image dataset, ImageTBAD, for segmentation of Type-B Aortic Dissection is published. ImageTBAD contains 100 3D Computed Tomography (CT) images, which is of decent size compared with existing medical imaging datasets.
ImageTBAD contains a total of 100 3D CTA images gathered from Guangdong Peoples' Hospital Data from January 1,2013 to April 23, 2019. Images are acquired from a variety of scanners (GE Medical Systems, Siemens, Philips), resulting in large variance in voxel size, resolution and imaging quality. All the images are pre-operative TBAD CTA images whose top and bottom are around the neck and the brachiocephalic vessels, respectively, in the axial view. The segmentation labeling is performed by a team of two cardiovascular radiologists who have extensive experience with TBAD. The segmentation labeling of each patient is fulfilled by one radiologist and checked by the other. The segmentation"	https://paperswithcode.com/dataset/imagetbad	01/09/2021						
4716	WebQA	WebQA, is a new benchmark for multimodal multihop reasoning in which systems are presented with the same style of data as humans when searching the web: Snippets and Images. The system must then identify which information is relevant across modalities and combine it with reasoning to answer the query. Systems will be evaluated on both the correctness of their answers and their sources.	https://paperswithcode.com/dataset/webqa	01/09/2021						
4717	MiniF2F	MiniF2F is a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses.	https://paperswithcode.com/dataset/minif2f	31/08/2021						
4718	mMARCO	mMARCO is a multilingual version of the MS MARCO passage ranking dataset comprising 8 languages that was created using machine translation.	https://paperswithcode.com/dataset/mmarco	31/08/2021						
4719	AwA Pose	AwA Pose is a large scale animal keypoint dataset with ground truth annotations for keypoint detection of quadruped animals from images.	https://paperswithcode.com/dataset/awa-pose	31/08/2021						
4720	TREK-150	TREK-150 is a benchmark dataset for object tracking in First Person Vision (FPV) videos composed of 150 densely annotated video sequences.	https://paperswithcode.com/dataset/trek150	31/08/2021						
4721	OAK	OAK is a dataset for online continual object detection benchmark with an egocentric video dataset. OAK adopts the KrishnaCam videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes.	https://paperswithcode.com/dataset/oak	25/08/2021	Objects Around Krishna					
4722	Depth in the Wild	Depth in the Wild is a dataset for single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. It consists of images in the wild annotated with relative depth between pairs of random points.	https://paperswithcode.com/dataset/depth-in-the-wild	13/04/2016						
4723	Fashion-MMT	Fashion-MNT is large-scale bilingual product description dataset called Fashion-MMT, which contains over 114k noisy and 40k manually cleaned description translations with multiple product images.	https://paperswithcode.com/dataset/fashion-mmt	25/08/2021						
4724	ComSum	ComSum is a data set of 7 million commit messages for text summarization. When documenting commits, software code changes, both a message and its summary are posted. These messages are gathered and filtered to curate developers' work summarization data set.	https://paperswithcode.com/dataset/comsum	23/08/2021						
4725	Indiscapes2	Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with semantic layout annotations. Indiscapes2 contains documents from four different historical collections and is 150% larger than its predecessor, Indiscapes.	https://paperswithcode.com/dataset/indiscapes2	21/08/2021						
4726	BnB	BnB is a large-scale and diverse in-domain VLN (Vision and Language Navigation) dataset.	https://paperswithcode.com/dataset/bnb	20/08/2021						
4727	VIL-100	VIL-100 is a  video instance lane detection dataset, which contains 100 videos with in total 10,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quantitative performance evaluation.	https://paperswithcode.com/dataset/vil-100	19/08/2021						
4728	Automated Evolution of Feature Logging Statement Levels Using Git Histories and Degree of Interest	"Logging—used for system events and security breaches to more informational yet essential aspects of software features—is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model, transforms source code to revitalize feature log levels based on the ""interestingness"" of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with ~3 million lines of code and ~4K log statements. Our tool successfully analyzes 99.26% of logging statements, increases log level distributions by ~20%, identifies logs manually modified with a recall of ~80% and a level-direction match rate of ~87%, and increases the focus of logs in bug fix contexts ~83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels."	https://paperswithcode.com/dataset/automated-evolution-of-feature-logging	15/04/2021						
4729	VQA-CE	"This dataset provides a new split of VQA v2 (similarly to VQA-CP v2), which is built of questions that are hard to answer for biased models.
This dataset is designed to penalize biases, and encourage the learning of models that generalize well."	https://paperswithcode.com/dataset/vqa-ce	07/04/2021	VQA Counterexamples					
4730	CodeXGLUE	"CodeXGLUE is a benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison. CodeXGLUE stands for General Language Understanding Evaluation benchmark for CODE. It includes 14 datasets for 10 diversified code intelligence tasks covering the following scenarios:

code-code (clone detection, defect detection, cloze test, code completion, code repair, and code-to-code translation)
text-code (natural language code search, text-to-code generation)
code-text (code summarization)
text-text (documentation translation)

A brief summary of CodeXGLUE is provided in the figure, including tasks, datasets, language, sizes in various states, baseline systems, providers, and short definitions of each task. Datasets highlighted in BLUE are newly introduced.
Image source: https://github.com/microsoft/CodeXGLUE"	https://paperswithcode.com/dataset/codexglue	09/02/2021						
4731	Medical Wiki Paralell Corpus for Medical Text Simplification	A medical Wiki paralell corpus for medical text simplification.	https://paperswithcode.com/dataset/medical-wiki-paralell-corpus-for-medical-text	20/10/2020						
4732	LiDAR-MOS	"Tasks.
In moving object segmentation of point cloud sequences, one has to provide motion labels for each point of the test sequences 11-21. Therefore, the input to all evaluated methods is a list of coordinates of the three-dimensional points along with their remission, i.e., the strength of the reflected laser beam which depends on the properties of the surface that was hit. Each method should then output a label for each point of a scan, i.e., one full turn of the rotating LiDAR sensor. Here, we only distinguish between static and moving object classes.
Metric
To assess the labeling performance, we rely on the commonly applied Jaccard Index or intersection-over-union (mIoU) metric over moving parts of the environment. We map all moving-x classes of the original SemanticKITTI semantic segmentation benchmark to a single moving object class.
Citation
Citation. More information on the task and the metric, you can find in our publication related to the task:
@article{chen2021ral,
  title={{Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data}},
  author={X. Chen and S. Li and B. Mersch and L. Wiesmann and J. Gall and J. Behley and C. Stachniss},
  year={2021},
  journal={IEEE Robotics and Automation Letters(RA-L)},
  doi = {10.1109/LRA.2021.3093567}
}"	https://paperswithcode.com/dataset/lidar-mos	19/05/2021	LiDAR-based Moving Object Segmentation					
4733	Spider-Realistic	"Spider-Realistic dataset is used for evaluation in the paper ""Structure-Grounded Pretraining for Text-to-SQL"". The dataset is created based on the dev split of the Spider dataset (2020-06-07 version from https://yale-lily.github.io/spider). We manually modified the original questions to remove the explicit mention of column names while keeping the SQL queries unchanged to better evaluate the model's capability in aligning the NL utterance and the DB schema. For more details, please check our paper at https://arxiv.org/abs/2010.12773."	https://paperswithcode.com/dataset/spider-realistic	24/10/2020						
4734	Infologic sql queries	Sql queries	https://paperswithcode.com/dataset/infologic-sql-queries	09/08/2021						
4735	ProcGen	Procgen Benchmark includes 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills.	https://paperswithcode.com/dataset/procgen	03/12/2019						
4736	5DOF GB Interpolation	These are larger MATLAB .mat files required for reproducing plots from the sgbaird-5DOF/interp repository for grain boundary property interpolation. gitID-0055bee_uuID-475a2dfd_paper-data6.mat contains multiple trials of five degree-of-freedom interpolation model runs for various interpolation schemes. gpr46883_gitID-b473165_puuID-50ffdcf6_kim-rng11.mat contains a Gaussian Process Regression model trained on 46883 Fe simulation GBs. See Five degree-of-freedom property interpolation of arbitrary grain boundaries via Voronoi fundamental zone framework DOI: 10.1016/j.commatsci.2021.110756 for the peer-reviewed, published version of the paper.	https://paperswithcode.com/dataset/5dof-gb-interpolation	14/04/2021	Five Degree-of-Freedom Grain Boundary Interpolation					
4737	Self-stimulatory Behavior Dataset	"Autism Spectrum Disorders (ASD), often referred to as autism, are neurological disorders characterised by deficits in cognitive skills, social and communicative behaviours. A common way of diagnosing ASD is by studying behavioural cues expressed by the children.
We introduce a new publicly available dataset (SSBD) of children videos exhibiting self-stimulatory (‘stimming’) behaviours commonly used in autism diagnosis. These videos, posted by parents/caregivers on public domain websites, are collected and annotated for the stimming behaviours.
These videos are extremely challenging for automatic behaviour analysis as they are recorded in uncontrolled natural settings. The dataset contains 75 videos with an average duration of 90 seconds per video, grouped under three categories of stimming behaviours:

arm flapping,
head banging, and
spinning."	https://paperswithcode.com/dataset/self-stimulatory-behavior-dataset-1							
4738	2017 Robotic Instrument Segmentation Challenge	"Segmentation of robotic instruments is an important problem for robotic assisted minimially invasive surgery. It can be used for simple 2D applications such as overlay masking or 2D tracking but also for more complex 3D tasks such as pose estimation. In this challenge we invite applicants to participate in 3 different tasks: binary segmentation, multi-label segmentation and instrument recognition. Binary segmentation involves just separating the image into instruments and background, whereas multi-label segmentation requires the user to also recognize which parts of the instrument body correspond to the different articulated parts of a da Vinci robotic instrument. The final recogition task tests whether the user can recognize which segmentation corresponds to which da Vinci instrument type. 
To achieve this we are providing 8x 225-frame robotic surgical videos, captured at 2 Hz, where a trained team at Intuitive Surgical has manually labelled the different parts and types. The users are invited to test their algorithms on 8x 75-frame videos and 2x 300-frame videos which act as a test set.
Description from: Robotic Instrument Segmentation Sub-Challenge
Image source: https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/"	https://paperswithcode.com/dataset/2017-robotic-instrument-segmentation	18/02/2019						
4739	Hocalarim: Turkish Student Reviews	"We have constructed our dataset by five fields available on the website that
were found convenient for the study of student expectations and experience.
This includes out-of-five star ratings on easiness, understandability, recitation,
accessibility and helpfulness. Average rating was calculated based on these
given five fields. Overall sentiment of the review was determined based on the
average rating where any score higher than 3.5 (>=) was labeled as a positive
review, and anything lower than 2.5 (<) was labeled as a negative review. The
five main aspects students needed to rate was given below.
• Anlaşılırlık (Clarity): Are lectures by the professor clear and understandable?
• Ders Anlatımı (Recitation): Are the lecturing style and material usage of
the professor organized?
• Erişilebilirlik (Accessibility): Is the professor available for further support
outside of classes?
• Yardımseverlik (Helpfulness): How does the professor approach his/her
students?
• Kolaylık (Easiness): How easy are the assessments?
• Ortalama (Average): The final average score was calculated based on the
five different aspects"	https://paperswithcode.com/dataset/hocalarim-turkish-student-reviews	06/09/2021						
4740	COVID-19 Disinfo	With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that focuses on COVID-19,  combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual multilingual, and single task vs. multitask settings.	https://paperswithcode.com/dataset/covid-19-disinfo	05/09/2021	COVID-19 Disinformation Twitter Dataset					
4741	Waste Classification data	"PROBLEM
Waste management is a big problem in our country. Most of the wastes end up in landfills. This leads to many issues like:  Increase in landfills, Eutrophication, Consumption of toxic waste by animals, Leachate, Increase in toxins, Land, water and air pollution.
APPROACH
Studied white papers on waste management, Analysed the components of household waste, Segregated into two classes (Organic and recyclable), Automated the process by using IOT and machine learning, Reduce toxic waste ending in landfills
IMPLEMENTATION
Dataset is divided into train data (85%) and test data (15%)  
Training data - 22564 images  Test data - 2513 images"	https://paperswithcode.com/dataset/waste-classification-data	04/09/2021						
4742	MUC-4	A dataset for evaluate system's understanding of given passages.	https://paperswithcode.com/dataset/muc-4		Fourth Message Uunderstanding Conference					
4743	BiSECT	BiSECT is a dataset for sentence simplification, which is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. These were obtained by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language.	https://paperswithcode.com/dataset/bisect	10/09/2021						
4744	HPS Dataset	"HPS Dataset is a collection of 3D humans interacting with large 3D scenes (300-1000 $m^2$, up to 2500 $m^2$). 
The dataset contains images captured from a head-mounted camera coupled with the reference 3D pose and location of the person in a pre-scanned 3D scene. 7 people in 8 large scenes are captured performing activities such as exercising, reading, eating, lecturing, using a computer, making coffee, dancing. The dataset provides more than 300K synchronized RGB images coupled with the reference 3D pose and location.
The dataset can be used as a testbed for ego-centric tracking with scene constraints, to learn how humans interact and move within large scenes over long periods of time, and to learn how humans process visual input arriving at their eyes."	https://paperswithcode.com/dataset/hps-dataset	31/03/2021	Human POSEitioning System Dataset					
4745	ASTE-Data-V2	A benchmark dataset for the Aspect Sentiment Triplet Extraction, an updated version of ASTE-Data-V1.	https://paperswithcode.com/dataset/aste-data-v2	06/10/2020						
4746	DRKG	Drug Repurposing Knowledge Graph (DRKG) is a comprehensive biological knowledge graph relating genes, compounds, diseases, biological processes, side effects and symptoms. DRKG includes information from six existing databases including DrugBank, Hetionet, GNBR, String, IntAct and DGIdb, and data collected from recent publications particularly related to Covid19. It includes 97,238 entities belonging to 13 entity-types; and 5,874,261 triplets belonging to 107 edge-types. These 107 edge-types show a type of interaction between one of the 17 entity-type pairs (multiple types of interactions are possible between the same entity-pair), as depicted in the figure below. It also includes a bunch of notebooks about how to explore and analysis the DRKG using statistical methodologies or using machine learning methodologies such as knowledge graph embedding.	https://paperswithcode.com/dataset/drkg		Drug Repurposing Knowledge graph					
4747	CorruptionDataSet	"This original data set includes the following four sheets:
Sheet 1: Raw Data (the original data set)
Sheet 2: Variables (A list with the variables included in the study)
Sheet 3: Countries Scientific Relative Production
Sheet 4: Correlations"	https://paperswithcode.com/dataset/corruptiondataset	05/11/2015						
4748	TRIP	"Tiered Reasoning for Intuitive Physics (TRIP) is a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machines’ reasoning process. TRIP serves as a benchmark for physical commonsense reasoning that provides traces of reasoning for an end task of plausibility prediction. The dataset consists of human-authored stories describing sequences of concrete physical actions. Given two stories composed of individually plausible sentences and only differing by one sentence (i.e., Sentence 5), the proposed task is to determine which story is more plausible. To understand stories like these and make such a prediction, one must have knowledge of verb causality and precondition, and rules of intuitive physics.
Description from: Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding
Image source:  Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding"	https://paperswithcode.com/dataset/trip	10/09/2021	Tiered Reasoning for Intuitive Physics					
4749	Graphine	"The Graphine dataset contains 2,010,648 terminology definition pairs organized in 227 directed acyclic graphs. Each node in the graph is associated with a terminology and its definition. Terminologies are organized from coarse-grained ones to fine-grained ones in each graph.
Source: Graphine: A Dataset for Graph-aware Terminology Definition Generation
Image source: Graphine: A Dataset for Graph-aware Terminology Definition Generation"	https://paperswithcode.com/dataset/graphine	09/09/2021						
4750	MuCo-VQA	"MuCo-VQA consist of large-scale (3.7M) multilingual and code-mixed VQA datasets in multiple languages: Hindi (hi), Bengali (bn), Spanish (es), German (de), French (fr) and code-mixed language pairs: en-hi, en-bn, en-fr, en-de and en-es.
Image source: https://arxiv.org/pdf/2109.04653v1.pdf"	https://paperswithcode.com/dataset/muco-vqa	10/09/2021						
4751	LIVECell	"The LIVECell (Label-free In Vitro image Examples of Cells) dataset is a large-scale microscopic image dataset for instance-segmentation of individual cells in 2D cell cultures.
LIVECell consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy images with a total of 1,686,352 individual cells annotated from eight different cell types (average 313 cells per image). The  LIVECell images have predefined splits into training (3188), validation (539) and test (1512) sets. Each split is also further subdivided into each of the eight cell types. The training set also has splits of different sizes (2, 4, 5, 25, 50%) to allow dataset size experimentation."	https://paperswithcode.com/dataset/livecell	30/08/2021	Label-free In Vitro image Examples of Cells					
4752	Helix	See https://zenodo.org/record/5500215#.YUCgD51Kg2w	https://paperswithcode.com/dataset/helix	10/09/2021	Helix					
4753	Dataset of 3D Garments with Sewing Patterns	The Dataset contains more than 23500 3D garment models with their corresponding sewing patterns, each representing a unique garment design sampled from one of the 19 different categories. The dataset is suitable for training Deep Learning models to solve a variety of clothing-related tasks.	https://paperswithcode.com/dataset/dataset-of-3d-garments-with-sewing-patterns	12/09/2021						
4754	YorkTag	YorkTag provides pairs of sharp/blurred images containing fiducial markers and is proposed to train and qualitatively and quantitatively evaluate our model.	https://paperswithcode.com/dataset/yorktag	08/09/2021						
4755	GMEG-yahoo	Grammatical error correction dataset for text from Yahoo! Answers	https://paperswithcode.com/dataset/gmeg-yahoo	01/03/2019						
4756	GMEG-wiki	Grammatical error correction dataset for text from Wikipedia.	https://paperswithcode.com/dataset/gmeg-wiki	01/03/2019						
4757	SituatedQA	SituatedQA is an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. Answers to the same question may change depending on the extralinguistic contexts (when and where the question was asked).	https://paperswithcode.com/dataset/situatedqa	13/09/2021						
4758	E-Manual Corpus	E-Manual Corpus is a corpus of 307,957 E-manuals, used for pre-training models for Question Answering on e-manuals.	https://paperswithcode.com/dataset/e-manual-corpus	13/09/2021						
4759	CelebA-Dialog	"The CelebA-Dialog dataset has the following properties: 1) Facial images are annotated with rich fine-grained labels, which classify one attribute into multiple degrees according to its semantic meaning; 2) Accompanied with each image, there are captions describing the attributes and a user request sample.
Image source: https://arxiv.org/pdf/2109.04425v1.pdf"	https://paperswithcode.com/dataset/celeba-dialog	09/09/2021						
4760	MLFW	"The Masked LFW (MLFW), based on Cross-Age LFW (CALFW) database, is built using a simple but effective tool that generates masked faces from unmasked faces automatically.
Image source: https://arxiv.org/pdf/2109.05804v1.pdf"	https://paperswithcode.com/dataset/mlfw	13/09/2021	Masked LFW					
4761	VGaokao	VGaokao is a verification style reading comprehension dataset designed for native speakers' evaluation.	https://paperswithcode.com/dataset/vgaokao	11/09/2021						
4762	Implicit Hate	The Implicit Hate corpus is a dataset for hate speech detection with fine-grained labels for each message and its implication. This dataset contains 22,056 tweets from the most prominent extremist groups in the United States; 6,346 of these tweets contain implicit hate speech.	https://paperswithcode.com/dataset/implicit-hate	11/09/2021						
4763	ZESHEL	"ZESHEL is a zero-shot entity linking dataset, which places more emphasis on understanding the unstructured descriptions of entities to resolve the ambiguity of mentions on four unseen domains.
This dataset was constructed using Wikias from FANDOM."	https://paperswithcode.com/dataset/zeshel	18/06/2019						
4764	GD-VCR	"Geo-Diverse Visual Commonsense Reasoning (GD-VCR) is a new dataset to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense.
Image source: https://arxiv.org/pdf/2109.06860v1.pdf"	https://paperswithcode.com/dataset/gd-vcr	14/09/2021						
4765	Harm-C	Harm-C is a dataset for detecting harmful memes related to Covid-19.	https://paperswithcode.com/dataset/harm-c	11/09/2021						
4766	Commonsense-Dialogues	Commonsense-Dialogues is a crowdsourced dataset of ~11K dialogues grounded in social contexts involving utilization of commonsense. The social contexts used were sourced from the train split of the SocialIQA dataset, a multiple-choice question-answering based social commonsense reasoning benchmark.	https://paperswithcode.com/dataset/commonsense-dialogues	14/09/2021						
4767	BenchIE	BenchIE: a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English, Chinese and German. In contrast to existing OIE benchmarks, BenchIE takes into account informational equivalence of extractions: our gold standard consists of fact synsets, clusters in which we exhaustively list all surface forms of the same fact.	https://paperswithcode.com/dataset/benchie	14/09/2021						
4768	DMO	A large scale dataset to pre-train optical flow prediction network. The data are generated from the DAVIS videos using as-rigid-as-possible principle from Deep-matching and MaskRCNN. The dataset has shown better performance compared to the FlyingChairs dataset.	https://paperswithcode.com/dataset/dmo	05/12/2018						
4769	FlyingChairs	"The ""Flying Chairs"" are a synthetic dataset with optical flow ground truth. It consists of 22872 image pairs and corresponding flow fields. Images show renderings of 3D chair models moving in front of random backgrounds from Flickr. Motions of both the chairs and the background are purely planar."	https://paperswithcode.com/dataset/flyingchairs	26/04/2015						
4770	KVQA	It contains manually verified 183K question-answer pairs about more than 18K persons and 24K images. The questions in this dataset require multi-entity, multi-relation and multi-hop reasoning over KG to arrive at an answer. To enable visual named entity linking, it also provides a support set containing reference images of 69K persons harvested from Wikidata as part of the dataset.	https://paperswithcode.com/dataset/kvqa	17/07/2019	Knowledge-aware VQA					
4771	WADS	"Collected in the snow belt region of Michigan's Upper Peninsula, WADS is the first multi-modal dataset featuring dense point-wise labeled sequential LiDAR scans collected in severe winter weather.
Over 26 TB of multi modal data has been collected of which over 7 GB of LiDAR point clouds (3.6 billion points) have been labeled (semanticKITTI format).
This outdoor dataset introduces falling_snow and accumulated_snow along with all the semanticKITTI classes to further AV tasks like semantic and panoptic segmentation, object detection and tracking, and localization and mapping in conditions of moderate to severe snow."	https://paperswithcode.com/dataset/wads	15/09/2021	Winter  Adverse  Driving  dataSet					
4772	Nelson-Plosser	"US Macroeconomic dataset containing 14 time series of monthly observations. They have various lengths but all end in 1988. The variables: consumer price index, industrial production, nominal GNP, velocity, employment, interest rate, nominal wages, GNP deflator, money stock, real GNP, stock prices (S&P500), GNP per capita, real wages, unemployment.
The data is available in cleaned form in the R package 'tseries' 1.
First introduction contained data until 1970 2.
Rerefences:

Trapletti, Adrian, and Kurt Hornik. 2020. tseries: Time Series Analysis and Computational Finance. https://cran.r-project.org/package=tseries.
Nelson, Charles R., and Charles I. Plosser. 1982. “Trends and Random Walks in Macroeconmic Time Series.” Journal of Monetary Economics 10 (2): 139–162. doi: 10.1016/0304-3932(82)90012-5."	https://paperswithcode.com/dataset/nelson-plosser	01/01/1982	Nelson-Plosser US Macroeconomic Time Series					
4773	BioLAMA	BioLAMA is a benchmark comprised of 49K biomedical factual knowledge triples for probing biomedical Language Models. It is used to assess the capabilities of Language Models for being valid biomedical knowledge bases.	https://paperswithcode.com/dataset/biolama	15/09/2021						
4774	BLANCA	BLANCA (Benchmarks for LANguage models on Coding Artifacts) is a collection of benchmarks that assess code understanding based on tasks such as predicting the best answer to a question in a forum post, finding related forum posts, or predicting classes related in a hierarchy from class documentation.	https://paperswithcode.com/dataset/blanca	15/09/2021						
4775	ELITR ECA	"The ELITR ECA corpus is a multilingual corpus derived from publications of the European Court of Auditors. We use automatic translation together with Bleualign to identify parallel sentence pairs in all 506 translation directions. The result is a corpus comprising 264k document pairs and 41.9M sentence pairs.
Description from: The ELITR ECA Corpus"	https://paperswithcode.com/dataset/elitr-eca	15/09/2021						
4776	MindCraft	MindCraft is a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication.	https://paperswithcode.com/dataset/mindcraft	13/09/2021						
4777	M5Product	"The M5Product dataset is a large-scale multi-modal pre-training dataset with coarse and fine-grained annotations for E-products.
• 6 Million multi-modal samples, 5k properties with 24 Million values
• 5 modalities-image text table video audio
• 6 Million category annotations with 6k classes
• Wide data source (1 Million merchants provide)"	https://paperswithcode.com/dataset/m5product	09/09/2021						
4778	Roof-Image Dataset	"We created a building-image paired dataset that contains more than 3K samples using our roof modeling tools.
Image source: https://github.com/llorz/SGA21_roofOptimization/tree/main/RoofGraphDataset"	https://paperswithcode.com/dataset/roof-image-dataset	16/09/2021						
4779	AnlamVer	"In this paper, we present AnlamVer, which is a semantic model evaluation dataset for Turkish designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and relatedness. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the Turkish language.
(from the original abstract of the dataset paper)"	https://paperswithcode.com/dataset/anlamver	01/08/2018						
4780	EDGAR10-Q Dataset	This dataset is built from 10-Q documents (Quarterly Reports) of publicly listed companies on the SEC.	https://paperswithcode.com/dataset/edgar10-q-dataset	16/09/2021						
4781	ChFinAnn	"Ten years (2008-2018) ChFinAnn documents and human-summarized event knowledge bases to conduct the DS-based event labeling.
Five event types included: Equity Freeze (EF), Equity Repurchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value.
To ensure the labeling quality, the authors set constraints for matched document-record pairs.
There are 32, 040 documents in total, and this number is ten times larger than 2, 976 of DCFEE and about 53 times larger than 599 of ACE 2005.
These documents are divided into train, development, and test set with the proportion of 8 : 1 : 1 based on the time order.
This DS-generated data are pretty good, achieving high precision and acceptable recall.
In later experiments, the authors directly employ the automatically generated test set for evaluation due to its much broad coverage."	https://paperswithcode.com/dataset/chfinann	16/04/2019						
4782	TruthfulQA	"TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. The authors crafted questions that some humans would answer falsely due to a false belief or misconception.
Image source: https://arxiv.org/pdf/2109.07958v1.pdf"	https://paperswithcode.com/dataset/truthfulqa	08/09/2021						
4783	SWDE	"This dataset is a real-world web page collection used for research on the automatic extraction of structured data (e.g., attribute-value pairs of entities) from the Web. We hope it could serve as a useful benchmark for evaluating and comparing different methods for structured web data extraction.
Source: Official"	https://paperswithcode.com/dataset/swde		Structured Web Data Extraction					
4784	wikiHow-image	The dataset consists of 53,189 wikiHow articles across various categories of everyday tasks, 155,265 methods, and 772,294 steps with corresponding images.	https://paperswithcode.com/dataset/wikihow-image	12/04/2021						
4785	UESTC RGB-D	UESTC RGB-D Varying-view action database contains 40 categories of aerobic exercise. We utilized 2 Kinect V2 cameras in 8 fixed directions and 1 round direction to capture these actions with the data modalities of RGB video, 3D skeleton sequences and depth map sequences.	https://paperswithcode.com/dataset/uestc-rgb-d	23/05/2018	UESTC RGB-D Varying-view action database					
4786	NLB	Neural Latents is a benchmark for latent variable modeling of neural population activity. It consists of four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas.	https://paperswithcode.com/dataset/nlb	09/09/2021	Neural Latents Benchmark					
4787	HM3D	"Habitat-Matterport 3D (HM3D) is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces.
HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual fidelity. HM3D contains 112.5k m^2 of navigable space, which is 1.4 - 3.7x larger than other building-scale datasets such as MP3D and Gibson. When compared to existing photorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images rendered from HM3D have 20 - 85% higher visual fidelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91% fewer artifacts due to incomplete surface reconstruction."	https://paperswithcode.com/dataset/hm3d	16/09/2021	Habitat-Matterport 3D					
4788	Depth VIDIT	VIDIT is a reference evaluation benchmark and to push forward the development of illumination manipulation methods. Virtual datasets are not only an important step towards achieving real-image performance but have also proven capable of improving training even when real datasets are possible to acquire and available. VIDIT contains 300 virtual scenes used for training, where every scene is captured 40 times in total: from 8 equally-spaced azimuthal angles, each lit with 5 different illuminants.	https://paperswithcode.com/dataset/depth-vidit	01/06/2021	Virtual Image Dataset for Illumination Transfer					
4789	ADEFAN	This data set contains 50 low resolution (640 x 360) short videos containing a variety real life activities.	https://paperswithcode.com/dataset/adefan	14/04/2020						
4790	VR Curve on Surface Drawing Dataset	The datasets includes curves drawn on 3D surfaces (triangle meshes) in Virtual Reality. A total of 2,880 curves were created using two different techniques by 20 users on 6 meshes. For each curve, a 3D curve executed by the user is provided, the projected curve created on the mesh, and the ground truth target curve on the mesh. For collecting the data, two different task types were employed, which are described in the paper.	https://paperswithcode.com/dataset/vr-curve-on-surface-drawing-dataset	18/09/2020						
4791	Machine Learning Quantum Reaction Rate Constants	"Dataset of 1,517,419 quantum reaction rate constant products kQM(T)QR(T) computed from the transmission coefficient for model single and double barrier minimum energy paths. Here kQM(T) is the quantum reaction rate constant at temperature T and QR(T) is the reactant partition function computed with the rigid rotor and harmonic oscillator approximations.This dataset was created for Ref 1 where it was used to train and test a DNN to predict logkQM(T)QR(T).
See zenodo webpage for details and dataset
https://zenodo.org/record/5510392#.YUkbjWZKhOc"	https://paperswithcode.com/dataset/machine-learning-quantum-reaction-rate	16/07/2020	Evan Komp and Stephanie Valleau					
4792	ReaSCAN	ReaSCAN is a synthetic navigation task that requires models to reason about surroundings over syntactically difficult languages.	https://paperswithcode.com/dataset/reascan	18/09/2021	ReaSCAN: Compositional Reasoning in Language Grounding					
4793	CMU Motion Capture	"This dataset of motions is free for all uses.
Please don't crawl this database! Check out the FAQs.
This data is free for use in research projects.
You may include this data in commercially-sold products,
but you may not resell this data directly, even in converted form.
If you publish results obtained using this data, we would appreciate it
if you would send the citation to your published paper to jkh+mocap@cs.cmu.edu,
and also would add this text to your acknowledgments section:
The data used in this project was obtained from mocap.cs.cmu.edu.
The database was created with funding from NSF EIA-0196217.
Note: In this database, the same person may appear under more than one subject number.
Each subject, however, has its own calibrated skeleton.
Note: When browsing for motions, start with the higher numbered subjects first.
The lower numbers contain some of our earliest motion capture sessions, and may not be as high quality.
Note: The ""toe"" and ""hand"" joints in our motions tend to be noisy, and may require some smoothing.
The ""finger"" and ""thumb"" joints are added to the skeleton for editing convenience
- we do not actually capture these joints' motions and any such data should be ignored."	https://paperswithcode.com/dataset/cmu-motion-capture		CMU Graphics Lab Motion Capture Database					
4794	Berkeley MHAD	"Description
The Berkeley Multimodal Human Action Database (MHAD) contains 11 actions performed by 7 male and 5 female subjects in the range 23-30 years of age except for one elderly subject. All the subjects performed 5 repetitions of each action, yielding about 660 action sequences which correspond to about 82 minutes of total recording time. In addition, we have recorded a T-pose for each subject which can be used for the skeleton extraction; and the background data (with and without the chair used in some of the activities).  The specified set of actions comprises of the following: (1) actions with movement in both upper and lower extremities, e.g., jumping in place, jumping jacks, throwing, etc., (2) actions with high dynamics in upper extremities, e.g., waving hands, clapping hands, etc. and (3) actions with high dynamics in lower extremities, e.g., sit down, stand up. Prior to each recording, the subjects were given instructions on what action to perform; however no specific details were given on how the action should be executed (i.e., performance style or speed). The subjects have thus incorporated different styles in performing some of the actions (e.g., punching, throwing)."	https://paperswithcode.com/dataset/berkeley-mhad	07/03/2013	Berkeley Multimodal Human Action Database					
4795	Novel COVID-19 Chestxray Repository	"Authors of the Dataset:

Pratik Bhowal (B.E., Dept of Electronics and Instrumentation Engineering, Jadavpur University Kolkata, India) [LinkedIn], [Github]
Subhankar Sen (B.Tech, Dept of Computer Science Engineering, Manipal University Jaipur, India) [LinkedIn], [Github], [Google Scholar]
Jin Hee Yoon (faculty of the Dept. of Mathematics and Statistics at Sejong University, Seoul, South Korea) [LinkedIn], [Google Scholar]
Zong Woo Geem (faculty of College of IT Convergence at Gachon University, South Korea) [LinkedIn], [Google Scholar]
Ram Sarkar( Professor at Dept. of Computer Science Engineering, Jadavpur Univeristy Kolkata, India) [LinkedIn], [Google Scholar]

Overview
The authors have created a new dataset known as Novel COVID-19 Chestxray Repository by the fusion of publicly available chest-xray image repositories. In creating this combined dataset, three different datasets obtained from the Github and Kaggle databases,created by the authors of other research studies in this field, were utilized.In our study,frontal and lateral chest X-ray images are used since this view of radiography is widely used by radiologist in clinical diagnosis.In the following section, authors have summarized how this dataset is created.


COVID-19 Radiography Database: The first release of this dataset reports 219 COVID-19,1345 viral pneumonia and 1341 normal radiographic chest X-ray images. This dataset was created by a team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh in collaboration with medical doctors and specialists from Pakistan and Malaysia.This database is regularly updated with the emergence of new cases of COVID-19 patients worldwide.Related Paper:https://arxiv.org/abs/2003.13145


COVID-Chestxray set:Joseph Paul Cohen and Paul Morrison and Lan Dao have created a public image repository on Github  which consists both CT scans and digital chest x-rays.The data was collected mainly from retrospective cohorts of pediatric patients from Guangzhou Women and Children’s medical center.With the aid of metadata information provided along with the dataset,we were able to extract 521 COVID-19 positive,239 viral and bacterial pneumonias;which are of the following three broad categories:Middle East Respiratory Syndrome (MERS),Severe Acute Respiratory Syndrome (SARS), and Acute Respiratory Distress syndrome (ARDS);and 218 normal radiographic chest X-ray images of varying image resolutions. Related Paper: https://arxiv.org/abs/2006.11988


Actualmed COVID chestxray dataset:Actualmed-COVID-chestxray-dataset comprises of 12 COVID-19 positive and 80 normal radiographic chest x-ray images.


The combined dataset includes chest X-ray images of COVID-19,Pneumonia and Normal (healthy) classes, with a total of 752, 1584, and 1639 images respectively. Information about the  Novel  COVID-19  Chestxray  Database and its parent image repositories is provided in Table 1.
Table 1: Dataset Description
| Dataset| COVID-19 |Pneumonia | Normal |
| ------------- | ------------- | ------------- | -------------|
| COVID Chestxray set | 521 |239|218|
| COVID-19 Radiography Database(first release) | 219 |1345|1341|
| Actualmed COVID chestxray dataset| 12 |0|80|
| Total|752|1584|1639|
DATA ACCESS AND USE: Academic/Non-Commercial Use
Dataset License : Database: Open Database, Contents: Database Contents"	https://paperswithcode.com/dataset/novel-covid-19-chestxray-repository	09/09/2021	Novel COVID-19 Chestxray Repository					
4796	EmoCause	"EmoCause is a dataset of annotated emotion cause words in emotional situations from the EmpatheticDialogues valid and test set. The goal is to recognize emotion cause words in sentences by training only on sentence-level emotion labels without word-level labels (i.e., weakly-supervised emotion cause recognition). 
EmoCause is based on the fact that humans do not recognize the cause of emotions with supervised learning on word-level cause labels. Thus, we do not provide a training set.

Number of emotion categories: 32
Average number of cause words per utterance: 2.3
Total number of utterances: 4.6K (valid: 3.8K / test: 0.8K)"	https://paperswithcode.com/dataset/emocause	18/09/2021						
4797	CodeQA	"CodeQA is a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. 
Description from: CodeQA: A Question Answering Dataset for Source Code Comprehension"	https://paperswithcode.com/dataset/codeqa	17/09/2021						
4798	Draper VDisc Dataset	"Draper VDISC Dataset - Vulnerability Detection in Source Code
The dataset consists of the source code of 1.27 million functions mined from open source software, labeled by static analysis for potential vulnerabilities. For more details on the dataset and benchmark results, see https://arxiv.org/abs/1807.04320.
The data is provided in three HDF5 files corresponding to an 80:10:10 train/validate/test split, matching the splits used in our paper. The combined file size is roughly 1 GB. Each function's raw source code, starting from the function name, is stored as a variable-length UTF-8 string. Five binary 'vulnerability' labels are provided for each function, corresponding to the four most common CWEs in our data plus all others:

CWE-120 (3.7% of functions)
CWE-119 (1.9% of functions)
CWE-469 (0.95% of functions)
CWE-476 (0.21% of functions)
CWE-other (2.7% of functions)

Functions may have more than one detected CWE each.
Please cite our paper if you use this dataset in a publication: https://arxiv.org/abs/1807.04320"	https://paperswithcode.com/dataset/draper-vdisc-dataset	11/07/2018						
4799	VISUELLE	"VISUELLE is a repository build upon the data of a real fast fashion company, Nunalie, and is composed of 5577 new products and about 45M sales related to fashion seasons from 2016-2019. Each product in VISUELLE is equipped with multimodal information: its image, textual metadata, sales after the first release date, and three related Google Trends describing category, color and fabric popularity.
Download  <a href=""https://drive.google.com/file/d/11Bn2efKfO_PbtdqsSqj8U6y6YgBlRcP6/view?usp=sharing"">here</a>
Image source: https://arxiv.org/pdf/2109.09824v1.pdf"	https://paperswithcode.com/dataset/visuelle	20/09/2021						
4800	ARCA23K	ARCA23K is a dataset of labelled sound events created to investigate real-world label noise. It contains 23,727 audio clips originating from Freesound, and each clip belongs to one of 70 classes taken from the AudioSet ontology. The dataset was created using an entirely automated process with no manual verification of the data. For this reason, many clips are expected to be labelled incorrectly.	https://paperswithcode.com/dataset/arca23k	19/09/2021						
4801	EntityQuestions	"EntityQuestions is a dataset of simple, entity-rich questions based on facts from Wikidata (e.g., ""Where was Arve Furset born? "")."	https://paperswithcode.com/dataset/entityquestions	17/09/2021						
4802	OPV2V	OPV2V is a large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles.	https://paperswithcode.com/dataset/opv2v	16/09/2021						
4803	ObjectFolder	ObjectFolder is a dataset for multisensory object-centric perception, reasoning, and interaction. It consists of 100 virtualized objects. ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks.	https://paperswithcode.com/dataset/objectfolder	16/09/2021						
4804	Bentham	"Bentham manuscripts refers to a large set of documents that were written by the renowned English philosopher and reformer Jeremy Bentham (1748-1832). Volunteers of the Transcribe Bentham initiative transcribed this collection. Currently, >6 000 documents or > 25 000 pages have been transcribed using this public web platform.
For our experiments, we used the BenthamR0 dataset a part of the Bentham manuscripts."	https://paperswithcode.com/dataset/bentham	07/04/2014	Bentham project					
4805	Saint Gall	Saint Gall dataset contains handwritten historical manuscripts written in Latin that date back to the 9th century. It consists of 60 pages, 1 410 text lines and 11 597 words.	https://paperswithcode.com/dataset/saint-gall	16/09/2011						
4806	Konzil	Konzil dataset was created by specialists of the University of Greifswald. It contains manuscripts written in modern German. Train sample consists of 353 lines, validation - 29 lines and test - 87 lines.	https://paperswithcode.com/dataset/konzil	22/02/2016	Konzilsprotokolle_C					
4807	Schiller	Schiller contains handwritten texts written in modern German. Train sample consists of 244 lines, validation - 21 lines and test - 63 lines.	https://paperswithcode.com/dataset/shiller	08/01/2018	Shiller					
4808	Ricordi	Ricordi contains handwritten texts written in Italian. Train sample consists of 295 lines, validation - 19 lines and test - 69 lines.	https://paperswithcode.com/dataset/ricordi	08/01/2018						
4809	Patzig	Patzig contains handwritten texts written in modern German. Train sample consists of 485 lines, validation - 38 lines and test -118 lines.	https://paperswithcode.com/dataset/patzig	08/01/2018						
4810	Schwerin	Schwerin contains handwritten texts written in medieval German. Train sample consists of 793 lines, validation - 68 lines and test - 196 lines.	https://paperswithcode.com/dataset/schwerin	08/01/2018						
4811	WorldKG	The WorldKG knowledge graph is a comprehensive large-scale geospatial knowledge graph based on OpenStreetMap that provides a semantic representation of geographic entities from over 188 countries. WorldKG contains a higher number of representations of geographic entities compared to other knowledge graphs and can be used as an underlying data source for various applications such as geospatial question answering, geospatial data retrieval, and other cross-domain semantic data-driven applications.	https://paperswithcode.com/dataset/worldkg	21/09/2021						
4812	TVRecap	TVRecap a story generation dataset that requires generating detailed TV show episode recaps from a brief summary and a set of documents describing the characters involved. Unlike other story generation datasets, TVRecap contains stories that are authored by professional screenwriters and that feature complex interactions among multiple characters. Generating stories in TVRecap requires drawing relevant information from the lengthy provided documents about characters based on the brief summary. In addition, by swapping the input and output, TVRecap can serve as a challenging testbed for abstractive summarization.	https://paperswithcode.com/dataset/tvrecap	18/09/2021						
4813	HYouTube	HYouTube is a video for Video harmonization, which aims to adjust the foreground of a composite video to make it compatible with the background. The dataset was created by adjusting the foreground of real videos to create synthetic composite videos. It is based on Youtube-VOS	https://paperswithcode.com/dataset/hyoutube	18/09/2021						
4814	EFO-1-QA	EFO-1-QA is a new dataset to benchmark the combinatorial generalizability of Complex Query Answering (CQA) models by including 301 different queries types, which is 20 times larger than existing datasets.	https://paperswithcode.com/dataset/efo-1-qa	18/09/2021						
4815	ISIC 2020 Challenge Dataset	"The dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. Each image is associated with one of these individuals using a unique patient identifier. All malignant diagnoses have been confirmed via histopathology, and benign diagnoses have been confirmed using either expert agreement, longitudinal follow-up, or histopathology. A thorough publication describing all features of this dataset is available in the form of a pre-print that has not yet undergone peer review.
The dataset was generated by the International Skin Imaging Collaboration (ISIC) and images are from the following sources: Hospital Clínic de Barcelona, Medical University of Vienna, Memorial Sloan Kettering Cancer Center, Melanoma Institute Australia, University of Queensland, and the University of Athens Medical School.
The dataset was curated for the SIIM-ISIC Melanoma Classification Challenge hosted on Kaggle during the Summer of 2020.
DOI: https://doi.org/10.34970/2020-ds01"	https://paperswithcode.com/dataset/isic-2020-challenge-dataset		Official dataset of the SIIM-ISIC Melanoma Classification Challenge 2020					
4816	EUEN17037_Daylight_and_View_Standard_TestDataSet	EUEN17037 Daylight and View Standard Test Dataset.	https://paperswithcode.com/dataset/euen17037-daylight-and-view-standard	18/06/2021						
4817	quantumNoise	The dataset consists in many runs of the same quantum circuit on different IBM quantum machines. We used 9 different machines and for each one of them, we run 2000 executions of the circuit. The circuit has 9 differents measurement steps along it. To obtain the 9 outcome distributions, for each execution, parts of the circuit are appended 9 times (in the same call to the IBM API, thus, in the shortest possible time) measuring a new step each time. The calls to the IBM API followed two different strategies. One was adopted to maximize the number of calls to the interface, parallelizing the code with as many possible runs and even running 8000 shots per run but considering for 8 times 1000 out of the memory to get the probabilities. The other strategy was slower, without parallelization and with a minimum waiting time between subsequent executions. The latter was adopted to get more uniformly distributed executions in time.	https://paperswithcode.com/dataset/quantumnoise	23/09/2021						
4818	BiRdQA	"BiRdQA is a bilingual multiple-choice question answering dataset with 6614 English riddles and 8751 Chinese riddles.
Image source: https://arxiv.org/pdf/2109.11087v1.pdf"	https://paperswithcode.com/dataset/birdqa	23/09/2021						
4819	ParaShoot	ParaShoot is the first question answering dataset in modern Hebrew. The dataset follows the format and crowdsourcing methodology of SQuAD, and contains approximately 3000 annotated examples, similar to other question-answering datasets in low-resource languages.	https://paperswithcode.com/dataset/parashoot	23/09/2021						
4820	Cloud VR gaming network traffic data	Oculus Quest2 VR gaming network traffic data collected at the gaming server.	https://paperswithcode.com/dataset/cloud-vr-gaming-network-traffic-data	21/09/2021						
4821	CAMELS Multifield Dataset	"CMD is a publicly available collection of hundreds of thousands 2D maps and 3D grids containing different properties of the gas, dark matter, and stars from more than 2,000 different universes. The data has been generated from thousands of state-of-the-art (magneto-)hydrodynamic and gravity-only N-body simulations from the CAMELS project.
Each 2D map and 3D grid has a set of labels associated to it: 2 cosmological parameters characterizing fundamental properties of the Universe, and 4 astrophysical parameters parametrizing the strength of astrophysical processes such as feedback from supernova and supermassive black-holes.
The main task this dataset was designed is to perform a robust inference on the value of the cosmological parameters from each map and grid. The data itself was generated from two completely different set of simulations, and it is not obvious that training one model on one will work when predicting on the other. Since simulations of the real Universe may never be perfect, this dataset provides the data to tackle this problem.
Solving this problem will help cosmologists to constrain the value of the cosmological parameters with the highest accuracy and therefore unveil the mysteries of our Universe. CMD can also be used for many other tasks, such as field mapping and super-resolution."	https://paperswithcode.com/dataset/camels-multifield-dataset	22/09/2021	CAMELS Multifield Dataset					
4822	GermEval 2021 - Toxic, Engaging, & Fact-Claiming Comments test set	"The data set was provided as part of the GermEval 2021 competition for the identification of toxic, engaging, and fact-claiming comments.

in total: 4188 anonymized and annotated German Facebook comments 
training set: 3244 comments drawn from a Facebook page of a German political talk show between January till July 2019
test set: 944 comments drawn from a Facebook page of a German political talk show between September till December 2020

The data is described in
Risch, Stoll, Wilms, Wiegand. Overview of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments. Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS 2021, DOI 10.48415/2021/fhw5-x128"	https://paperswithcode.com/dataset/germeval-2021-toxic-engaging-fact-claiming	13/07/2021						
4823	Hilti SLAM Challenge	Hilti SLAM Challenge is a dataset for Simultaneous Localization and Mapping (SLAM) algorithms due to sparsity, varying illumination conditions, and dynamic objects. The sensor platform used to collect this dataset contains a number of visual, lidar and inertial sensors which have all been rigorously calibrated. All data is temporally aligned to support precise multi-sensor fusion. Each dataset includes accurate ground truth to allow direct testing of SLAM results. Raw data as well as intrinsic and extrinsic sensor calibration data from twelve datasets in various environments is provided. Each environment represents common scenarios found in building construction sites in various stages of completion.	https://paperswithcode.com/dataset/hilti-slam-challenge	23/09/2021						
4824	ERATO	ERATO is a large-scale multi-modal dataset for Pairwise Emotional Relationship Recognition (PERR). It has 31,182 video clips, lasting about 203 video hours. Different from the existing datasets, ERATO contains interaction-centric videos with multi-shots, varied video length, and multiple modalities including visual, audio and text	https://paperswithcode.com/dataset/erato	23/09/2021						
4825	CI-ToD	CI-ToD is a dataset for Consistency Identification in Task-oriented Dialog system.	https://paperswithcode.com/dataset/ci-tod	23/09/2021						
4826	Wrench	Wrench is a benchmark platform for thorough and standardized evaluation of Weak Supervision (WS). It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods.	https://paperswithcode.com/dataset/wrench	23/09/2021						
4827	CoWeSe	CoWeSe is a Spanish biomedical corpus consisting of 4.5GB (about 750M tokens) of clean plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains executed in 2020.	https://paperswithcode.com/dataset/cowese	16/09/2021	Corpus Web Salud Espanol					
4828	METEOR	METEOR is a complex traffic dataset which captures traffic patterns in unstructured scenarios in India. METEOR consists of more than 1000 one-minute video clips, over 2 million annotated frames with ego-vehicle trajectories, and more than 13 million bounding boxes for surrounding vehicles or traffic agents. METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics.	https://paperswithcode.com/dataset/meteor	16/09/2021						
4829	SketchHairSalon	SketchHairSalon is a dataset for hair generation containing thousands of annotated hair sketch-image pairs and corresponding hair mattes.	https://paperswithcode.com/dataset/sketchhairsalon	16/09/2021						
4830	XTD10	XTD10 is a dataset for cross-lingual image retrieval and tagging consisting of the MSCOCO2014 caption test dataset annotated in 7 languages that were collected using a crowdsourcing platform.	https://paperswithcode.com/dataset/xtd10	15/09/2021						
4831	FloDial	Flowchart Grounded Dialog Dataset (FloDial) is a corpus of troubleshooting dialogs between a user and an agent collected using Amazon Mechanical Turk. The dataset is accompanied with two knowledge sources over which the dialogs are grounded: (1) a set of troubleshooting flowcharts and (2) a set of FAQs which contains supplementary information about the domain not present in the flowchart. FloDial consists of 2,738 dialogs grounded on 12 different troubleshooting flowcharts.	https://paperswithcode.com/dataset/flodial	15/09/2021	Flowchart Grounded Dialogs Dataset					
4832	COME15K	COME15K is an RGB-D saliency detection dataset which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations.	https://paperswithcode.com/dataset/come15k	15/09/2021						
4833	safe-control-gym	safe-control-gym is an open-source benchmark suite that extends OpenAI's Gym API with (i) the ability to specify (and query) symbolic models and constraints and (ii) introduce simulated disturbances in the control inputs, measurements, and inertial properties. We provide implementations for three dynamic systems -- the cart-pole, 1D, and 2D quadrotor -- and two control tasks -- stabilization and trajectory tracking.	https://paperswithcode.com/dataset/safe-control-gym	13/09/2021						
4834	AStitchInLanguageModels	AStitchInLanguageModels is a dataset for the exploration of idiomaticity in pre-trained language models.	https://paperswithcode.com/dataset/astitchinlanguagemodels	09/09/2021						
4835	Diagnosis of COVID-19 and its clinical spectrum	"This dataset contains anonymized data from patients seen at the Hospital Israelita Albert Einstein, at São Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.
All data were anonymized following the best international practices and recommendations. All clinical data were standardized to have a mean of zero and a unit standard deviation."	https://paperswithcode.com/dataset/diagnosis-of-covid-19-and-its-clinical							
4836	WHPA	"This dataset was created as part of the following study, which was published in the Journal of Hydrology: A new framework for experimental design using Bayesian Evidential Learning: the case of wellhead protection area https://doi.org/10.1016/j.jhydrol.2021.126903.
The pre-print is available on arXiv: https://arxiv.org/pdf/2105.05539.pdf
Files description
This dataset contains 4148 simulation results, i.e., 4148 pairs of predictor/target.
bkt.npy contains the breakthrough curves from all 6 injection wells recorded at the pumping well.
pz.npy contains the 2D coordinates of the backtracked particles' end points, used to delineate the WHPA.
Introduction
The Wellhead Protection Area (WHPA) is a zone around a pumping well where human activities are limited in order to preserve water resources, usually based on how long dangerous chemicals in the area will take to reach the pumping well (according to local regulation). The flow velocity in the subsurface around the well determines it, and it can be computed numerically using particle tracking or transport simulation, or in practice using tracer testing. A groundwater model is typically calibrated against field data before being used to calculate the WHPA. In highly populated places where land occupation is a big concern, the introduction of such zones could have a large socioeconomic impact.
WHPA prediction
Different tracers emerge from six data sources (injection wells) scattered across the pumping well. Their job is to inject individual tracers into the system in order to predict their transport and record their breakthrough curves (BCs) at the pumping well location. 
Numerous particles are artificially positioned around the pumping well, and their origins are traced backward in time to identify the associated WHPA.
Our predictor and target will be generated using the USGS' open-source finite-difference code Modflow. To get different sets of predictors and targets, we will run different hydrologic models with one variable parameter, namely hydraulic conductivity in metres per day. To obtain a satisfactory heterogeneity in the hydraulic conductivity fields, which will control the shape and extent of our target, the PAs, we use sequential gaussian simulation based on arbitrarily defined variograms. The pumping well is located at the 1000, 500 metres mark and is surrounded by six injection wells."	https://paperswithcode.com/dataset/whpa	12/05/2021	Wellhead Protection Area prediction from breakthrough curves					
4837	IECSIL FIRE-2018 Shared Task	"The dataset is taken from the First shared task on Information Extractor for Conversational Systems in Indian Languages (IECSIL) . It consists of 15,48,570 Hindi words in Devanagari script and corresponding NER labels. Each sentence end is marked by \newline"" tag. Fig. 1 shows a snapshot of one sentence in the dataset. Our Dataset has nine classes, namely, Datenum, Event, Location, Name, Number, Occupation, Organization, Other, Things.
Image taken from paper:  https://www.researchgate.net/publication/349190662_Analysis_of_Contextual_and_Non-contextual_Word_Embedding_Models_for_Hindi_NER_with_Web_Application_for_Data_Collection"	https://paperswithcode.com/dataset/iecsil-fire-2018-shared-task	20/02/2018						
4838	Spectre-v1	"Description

A dataset of assembly functions that are vulnerable to Spectre-V1 attack.

Motivation

Several techniques have been proposed to detect vulnerable Spectre gadgets in widely deployed commercial software. Unfortunately, detection techniques proposed so far rely on hand-written rules which fall short in covering subtle variations of known Spectre gadgets as well as demand a huge amount of time to analyze each conditional branch in software. Moreover, detection tool evaluations are based only on a handful of these gadgets, as it requires arduous effort to craft new gadgets manually.

Potential Use Cases

Generating assembly code.
Evaluating new detection tools."	https://paperswithcode.com/dataset/spectre-v1	25/06/2020						
4839	MAVS	MAVS is an audio-visual smartphone dataset captured in five different recent smartphones. This new dataset contains 103 subjects captured in three different sessions considering the different real-world scenarios. Three different languages are acquired in this dataset to include the problem of language dependency of the speaker recognition systems.	https://paperswithcode.com/dataset/mavs	09/09/2021	Multilingual Audio-Visual Smartphone dataset					
4840	TIAGE	TIAGE is a topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, three tasks can be conducted to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation.	https://paperswithcode.com/dataset/tiage	09/09/2021						
4841	FusedChat	FusedChat is an inter-mode dialogue dataset. It contains dialogue sessions fusing task-oriented dialogues (TOD) and open-domain dialogues (ODD). Based on MultiWOZ, FusedChat appends or prepends an ODD to every existing TOD. See more details in the paper.	https://paperswithcode.com/dataset/fusedchat	09/09/2021						
4842	Panoptic nuScenes	Panoptic nuScenes is a benchmark dataset that extends the popular nuScenes dataset with point-wise groundtruth annotations for semantic segmentation, panoptic segmentation, and panoptic tracking tasks.	https://paperswithcode.com/dataset/panoptic-nuscenes	08/09/2021						
4843	BUG	BUG is a large-scale gender bias dataset of 108K diverse real-world English sentences, sampled semiautomatically from large corpora using lexical syntactic pattern matching	https://paperswithcode.com/dataset/bug	08/09/2021						
4844	D3D-HOI	"D3D-HOI is a dataset of monocular videos with ground truth annotations of 3D object pose, shape and part motion during human-object interactions. The dataset consists of several common articulated objects captured from diverse real-world scenes and camera viewpoints. Each manipulated object (e.g., microwave oven) is represented with a matching 3D parametric model. This data allows researchers to evaluate the reconstruction quality of articulated objects and establish a benchmark for this challenging task.
Image source: https://github.com/facebookresearch/d3d-hoi"	https://paperswithcode.com/dataset/d3d-hoi	19/08/2021						
4845	MuViHand	MuViHand is a dataset for 3D Hand Pose Estimation that consists of multi-view videos of the hand along with ground-truth 3D pose labels. The dataset includes more than 402,000 synthetic hand images available in 4,560 videos. The videos have been simultaneously captured from six different angles with complex backgrounds and random levels of dynamic lighting. The data has been captured from 10 distinct animated subjects using 12 cameras in a semi-circle topology.	https://paperswithcode.com/dataset/muvihand	24/09/2021						
4846	ItaCoLA	ItaCoLA is a corpus for monolingual and cross-lingual acceptability judgments which contains almost 10,000 sentences with acceptability judgments.	https://paperswithcode.com/dataset/itacola	24/09/2021						
4847	Paint4Poem	Paint4Poem consists of 301 high-quality poem-painting pairs collected manually from an influential modern Chinese artist Feng Zikai.	https://paperswithcode.com/dataset/paint4poem	23/09/2021						
4848	MOLD	MOLD is a Marathi dataset for offensive language identification	https://paperswithcode.com/dataset/mold	08/09/2021	Marathi Offensive Language Dataset					
4849	ROF	ROF is a dataset for occluded face recognition that contains faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks.	https://paperswithcode.com/dataset/rof	08/09/2021	Real World Occluded Faces					
4850	Mr. TYDI	Mr. TyDi is a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data.	https://paperswithcode.com/dataset/mr-tydi	19/08/2021						
4851	ChMusic	ChMusic is a traditional Chinese music dataset for training model and performance evaluation of musical instrument recognition. This dataset cover 11 musical instruments, consisting of Erhu, Pipa, Sanxian, Dizi, Suona, Zhuiqin, Zhongruan, Liuqin, Guzheng, Yangqin and Sheng.	https://paperswithcode.com/dataset/chmusic	19/08/2021						
4852	TFRD	"TFRD is a dataset to evaluate machine learning modelling methods for theTemperature field reconstruction of heat source systems (TFR-HSS).
The password for the dataset download is ""tfrd"""	https://paperswithcode.com/dataset/tfrd	17/08/2021	Temperature Field Reconstruction Dataset					
4853	Fishyscapes	Fishyscapes is a public benchmark for uncertainty estimation in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects in front of the vehicle.	https://paperswithcode.com/dataset/fishyscapes	05/04/2019						
4854	EU-ADR	The EU-ADR corpus is a biomedical relation extraction dataset that contains 100 abstracts, with relations between drug, disorder, and targets.	https://paperswithcode.com/dataset/eu-adr		EU-ADR					
4855	CAT	CAT is a specialized dataset for co-saliency detection - one of the core tasks in the field of computer vision. This dataset is intended for both helping to assess the performance of vision algorithms and supporting research that aims to exploit large volumes of annotated data, e.g., for training deep neural networks. CAT consists of 33,500 images	https://paperswithcode.com/dataset/cat	04/08/2021	Context Adjustment Training					
4856	FewGLUE_64_labeled	"Introduction
The FewGLUE_64_labeled dataset is a new version of FewGLUE dataset. It contains a 64-sample training set, a development set (the original SuperGLUE development set), a test set, and an unlabeled set. It is constructed to facilitate the research of few-shot learning for natural language understanding tasks.
Compared with the original FewGLUE dataset, it differs in the number of labeled data examples in the training set, where the original FewGLUE has 32 training examples while FewGLUE_64_labeled has 64 labeled examples. Purposes for constructing a new version of FewGLUE dataset include:


To answer the questions that what is the best performance that few-shot learning can achieve and whether it is possible to further close the performance gap between few-shot learning and fully-supervised systems.


To explore to which degree the number of labeled training examples influences the few-shot performance.


Please refer to the FewNLU paper as well as the FewNLU leaderboard for more details.
Acknowledgement
Part of the FewGLUE_64_labeled dataset is based on the original 32-sample version of FewGLUE. We collect them together in one package for the convenience of usage. We appreciate all the contributors who made their dataset public, which greatly advanced few-shot learning as well as the FewNLU project."	https://paperswithcode.com/dataset/fewglue-64-labeled	27/09/2021	A new version of FewGLUE with 64 training examples					
4857	DSSE-200	The DSSE-200 is a complex document layout dataset including various dataset styles. The dataset contains 200 images from pictures, PPT, brochure documents, old newspapers and scanned documents.	https://paperswithcode.com/dataset/dsse-200	01/07/2017						
4858	PASS	"PASS is a large-scale image dataset, containing 1.4 million images, that does not include any humans and which can be used for high-quality pretraining while significantly reducing privacy concerns.
Image source: https://arxiv.org/pdf/2109.13228v1.pdf"	https://paperswithcode.com/dataset/pass	27/09/2021	Pictures without humAns for Self-Supervision					
4859	VQA-MHUG	VQA-MHUG is a 49-participant dataset of multimodal human gaze on both images and questions during visual question answering (VQA) collected using a high-speed eye tracker.	https://paperswithcode.com/dataset/vqa-mhug	27/09/2021						
4860	MultiDoc2Dial	MultiDoc2Dial is a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. We aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents.	https://paperswithcode.com/dataset/multidoc2dial	26/09/2021	MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents					
4861	Doc2Dial	For goal-oriented document-grounded dialogs, it often involves complex contexts for identifying the most relevant information, which requires better understanding of the inter-relations between conversations and documents. Meanwhile, many online user-oriented documents use both semi-structured and unstructured contents for guiding users to access information of different contexts. Thus, we create a new goal-oriented document-grounded dialogue dataset that captures more diverse scenarios derived from various document contents from multiple domains such ssa.gov and studentaid.gov. For data collection, we propose a novel pipeline approach for dialogue data construction, which has been adapted and evaluated for several domains.	https://paperswithcode.com/dataset/doc2dial-1	12/11/2020	Doc2Dial: Document-grounded Dialogue					
4862	TCP-CI	This dataset is a benchmark of 25 open-source subjects with 21.5k builds and 3.6k failed builds that enables a fair comparison and evaluation of Test Case Prioritization (TCP) techniques. We made our data collection tools available, which can be used to extend and update the subjects. The description of the structure and files of the dataset can be also found in the documentation of the data collection tool.	https://paperswithcode.com/dataset/tcp-ci	27/09/2021	Test Case Prioritization in CI Contexts					
4863	VVAD-LRS3	"A dataset for Visual Voice Activity Detection extracted from the LRS3 dataset.
The dataset contains data to train a Visual Voice Activity Detection(VVAD). The data comes in 4 different flavors:

faceImages: A series of images of faces with the corresponding label True for speaking and False for not speaking
lipImages: A series of images of lips with the corresponding label True for speaking and False for not speaking
faceFeatures: A series of feature maps extracted with dlibs face landmark detection of faces with the corresponding label True for speaking and False for not speaking
lipFeatures: A series of feature maps extracted with dlibs face landmark detection of lips with the corresponding label True for speaking and False for not speaking

Image source: https://arxiv.org/pdf/2109.13789v1.pdf"	https://paperswithcode.com/dataset/vvad-lrs3	28/09/2021						
4864	OpenViDial 2.0	"OpenViDial 2.0 is a larger-scale open-domain multi-modal dialogue dataset compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a total number of 5.6 million dialogue turns extracted from either movies or TV series from different resources, and each dialogue turn is paired with its corresponding visual context. 
Image source: https://github.com/ShannonAI/OpenViDial"	https://paperswithcode.com/dataset/openvidial-2-0	27/09/2021						
4865	LEAKAGE-PERSONA Dataset	This is the synthetic dataset used for training a model which alerts users for potential leakages of personal information.	https://paperswithcode.com/dataset/leakage-persona-dataset							
4866	Lincolnbeet	"The Lincolnbeet dataset is an object detection dataset designed to encourage research in the identification of items in environments with high levels of occlusion, and in the development of better approaches to evaluate object detection models in practical scenarios. This dataset was introduced in the paper: ""Towards practical object detection for weed spraying in precision agriculture"". 
The dataset contains 4402 images that contain weed plants and sugar beets which are located with object detection labels. The image size is 1920 x 1080 pixels, and the labels included in the dataset are in COCOjson, XML, and darknets formats."	https://paperswithcode.com/dataset/lincolnbeet	14/09/2021						
4867	MFAQ	MFAQ is a multilingual FAQ dataset publicly available. It contains around 6M FAQ pairs from the web, in 21 different languages. Although this is significantly larger than existing FAQ retrieval datasets, it comes with its own challenges: duplication of content and uneven distribution of topics.	https://paperswithcode.com/dataset/mfaq	27/09/2021						
4868	JDDC 2.0	JDDC 2.0 is a large-scale multimodal multi-turn dialogue dataset collected from a mainstream Chinese E-commerce platform JD.com, containing about 246 thousand dialogue sessions, 3 million utterances, and 507 thousand images, along with product knowledge bases and image category annotations. The dataset is divided into the training set, the validation set, and the test set according to the ratio of 80%, 10%, and 10%.	https://paperswithcode.com/dataset/jddc-2-0	27/09/2021						
4869	MaRVL	Multicultural Reasoning over Vision and Language (MaRVL) is a dataset based on an ImageNet-style hierarchy representative of many languages and cultures (Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish). The selection of both concepts and images is entirely driven by native speakers. Afterwards, we elicit statements from native speakers about pairs of images. The task consists in discriminating whether each grounded statement is true or false.	https://paperswithcode.com/dataset/marvl	28/09/2021	Multicultural Reasoning over Vision and Language					
4870	REFLACX	"The REFLACX dataset contains eye-tracking data for 3,032 readings of chest x-rays by five radiologists. The dictated reports were transcribed and have timestamps synchronized with the eye-tracking data. 
Localization labels for abnormalities are very costly, and the collection of eye-tracking data and reports for implicit localization labels may be an alternative for scaling up data collection. One of the potential uses for these data is in additional supervision for training computer vision models. For more details, check the Physionet page and the dataset description paper (""REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays"")."	https://paperswithcode.com/dataset/reflacx	27/09/2021	Reports and eye-tracking data for localization of abnormalities in chest x-rays					
4871	EDGAR-CORPUS	"EDGAR-CORPUS is a novel corpus comprising annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use JSON format. 
Image source: https://arxiv.org/pdf/2109.14394v1.pdf"	https://paperswithcode.com/dataset/edgar-corpus	29/09/2021						
4872	RAFT	"The RAFT benchmark (Realworld Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment.
RAFT is a few-shot classification benchmark that tests language models:

across multiple domains (lit reviews, medical data, tweets, customer interaction, etc.)
on economically valuable classification tasks (someone inherently cares about the task)
with evaluation that mirrors deployment (50 labeled examples per task, info retrieval allowed, hidden test set)

Description from: https://raft.elicit.org/
Image source: https://raft.elicit.org/"	https://paperswithcode.com/dataset/raft	28/09/2021	Realworld Annotated Few-shot Tasks					
4873	StoryDB	StoryDB is a broad multi-language dataset of narratives. StoryDB is a corpus of texts that includes stories in 42 different languages. Every language includes 500+ stories. Some of the languages include more than 20 000 stories. Every story is indexed across languages and labeled with tags such as a genre or a topic. The corpus shows rich topical and language variation and can serve as a resource for the study of the role of narrative in natural language processing across various languages including low resource ones.	https://paperswithcode.com/dataset/storydb	29/09/2021						
4874	JARVIS-DFT	JARVIS-DFT is a repository of density functional theory based calculation data for materials.	https://paperswithcode.com/dataset/jarvis-dft-formation-energy	03/07/2020						
4875	MiniHack	MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL). MiniHack includes a collection of example environments that can be used to test various capabilities of RL agents, as well as serve as building blocks for researchers wishing to develop their own environments. MiniHack's navigation tasks challenge the agent to reach the goal position by overcoming various difficulties on their way, such as fighting monsters in corridors, crossing a river by pushing boulders into it, navigating through complex, procedurally generated mazes, etc. MiniHack's skill acquisition tasks enable utilising the rich diversity of NetHack objects, monsters and dungeon features, and the interactions between them. The skill acquisition tasks feature a large action space (75 actions), where the actions are instantiated differently depending on which object they are acting on.	https://paperswithcode.com/dataset/minihack	27/09/2021						
4876	SCIMAT	SCIMAT is a large question-answer dataset for mathematics and science problems; such dataset can have impact on online education, intelligent tutoring and automated grading.	https://paperswithcode.com/dataset/scimat	30/09/2021						
4877	iShape	iShape is an irregular shape dataset for instance segmentation. iShape contains six sub-datasets with one real and five synthetics, each represents a scene of a typical irregular shape.	https://paperswithcode.com/dataset/ishape	30/09/2021						
4878	Riedones3D	Riedones3D is a dataset of 2,070 scans of coins. With this dataset, the authors propose two benchmarks, one for point cloud registration, essential for coin die recognition, and a benchmark of coin die clustering	https://paperswithcode.com/dataset/riedones3d	30/09/2021						
4879	Contextualised Polyseme Word Sense Dataset v2	This is a revised and extended second version of a Contextualised Polyseme Word Sense Dataset. The dataset contains two human annotated measures of word sense similarity for polysemic target words used in contexts invoking different sense interpretations. The first set contains graded similarity judgements for highlighted target words displayed in two different contexts. The second set contains co-predication acceptability judgements for sentence constructions combining the sentence pairs from the first set.	https://paperswithcode.com/dataset/contextualised-polyseme-word-sense-dataset-v2	27/09/2021						
4880	DVSMOTION20	This dataset is designed to enhance the progress of event-based optical flow algorithms. The data was collected using the IniVation DAViS346 camera, which has a 346 x 260 spatial resolution. The dataset is classified into camera motion data (stationary scene and moving camera) and object motion data (stationary camera and moving objects). The camera motion data contains four real indoor sequences (namely, checkerboard, classroom, conference room, and conference room translation) with ground truth motion inferred from IMU. The movement of the camera in this category was restricted by a gimbal, and the IMU was calibrated before each collection.  The object motion data includes two real sequences (called hands and cars) containing multiple object motions. This category does not have ground-truth motion since the object motion cannot be inferred from IMU.	https://paperswithcode.com/dataset/dvsmotion20	28/03/2020						
4881	New Dataset From Paper	Dataset consists of json files with labels of semantically meaningful components of sketches.	https://paperswithcode.com/dataset/new-dataset-from-paper	19/01/2019						
4882	BKAI-IGH NeoPolyp-Small	This dataset contains 1200 images (1000 WLI images and 200 FICE images) with fine-grained segmentation annotations. The training set consists of 1000 images, and the test set consists of 200 images. All polyps are classified into neoplastic or non-neoplastic classes denoted by red and green colors, respectively.  This dataset is a part of a bigger dataset called NeoPolyp.	https://paperswithcode.com/dataset/bkai-igh-neopolyp-small	11/07/2021						
4883	OV	"Description
OV dataset is the camera calibration dataset. There are 16 lenses ranging from 90° to 180° FOV:

2012-A0 (single-plane target)
3136-H0 (single-plane target)
5501-C4 (single-plane target)
130108MP (single-plane target)
ov00—ov07 (corner target)
ov00—ov03 (cube target)

Note: We also provide the other datasets evaluated in the BabelCalib paper: Kalibr, OCamCalib and UZH.
Format
The datasets have a Deltille format. The Deltille detector is a robust deltille and checkerboard detector. It comes with detector library, example detector code, and MATLAB bindings. BabelCalib provides functions for calibration and evaluation using the Deltille software's outputs. Calibration from Deltille detections requires format conversion which is peformed by import_ODT. Please see a complete calibration example from Deltille data."	https://paperswithcode.com/dataset/ov	18/09/2021						
4884	Refer-YouTube-VOS	"There exist previous works [6, 10] that constructed referring segmentation datasets for videos. Gavrilyuk et al. 6 extended the A2D [33] and J-HMDB 9 datasets with natural sentences; the datasets focus on describing the ‘actors’ and ‘actions’ appearing in videos, therefore the instance annotations are limited to only a few object categories corresponding to the dominant ‘actors’ performing a salient ‘action’. Khoreva et al. 10 built a dataset based on DAVIS [25], but the scales are barely sufficient to learn an end-to-end model from scratch
Youtube-VOS has 4,519 high-resolution videos with 94 common object categories. Each video has pixel-level instance segmentation annotation at every 5 frames in 30-fps videos, and their durations are around 3 to 6 seconds.
We employed Amazon Mechanical Turk to annotate referring expressions. To ensure the quality of the annotations, we selected around 50 turkers after a validation test. Each turker was given a pair of videos, the original video and the mask-overlaid one with the target object highlighted, and was asked to provide a discriminative sentence within 20 words that describes the target object accurately. We collected two kinds of annotations, which describe the highlighted object (1) based on a whole video (Full-video expression) and (2) using only the
first frame of the video (First-frame expression). After the initial annotation, we conducted verification and cleaning jobs for all annotations, and dropped objects if an object cannot be localized using language expressions only. 
The followings are the statistics and analysis of the two annotation types of the dataset after the verification.
Full-video expression: Youtube-VOS has 6,459 and 1,063 unique objects in train and validation split, respectively. Among them, we cover 6,388 unique objects in 3,471 videos (6, 388/6, 459 = 98.9%) with 12,913 expressions in train split and 1,063 unique objects in 507 videos (1, 063/1, 063 = 100%) with 2,096 expressions in validation split. On average, each video has 3.8 language expressions and each expression has 10.0 words. 
First-frame expression: There are 6,006 unique objects in 3,412 videos (6, 006 /6, 459 = 93.0%) with 10,897 expressions in train split and 1,030 unique objects in 507 videos (1, 030/1, 063 = 96.9%) with 1,993 expressions in validation split. The number of annotated objects is lower than that of the full-video expressions because using only the first frame makes annotation more ambiguous and inconsistent and we dropped more annotations during the verification. On average,
each video has 3.2 language expressions and each expression has 7.5 words."	https://paperswithcode.com/dataset/refer-youtube-vos	01/08/2020						
4885	Corn Seeds Dataset	"This dataset is the images of corn seeds considering the top and bottom view independently (two images for one corn seed: top and bottom). There are four classes of the corn seed (Broken-B, Discolored-D, Silkcut-S, and Pure-P) 17802 images are labeled by the experts at the AdTech Corp. and 26K images were unlabeled out of which 9k images were labeled using the Active Learning (BatchBALD)
We have created three different dataset: (1). Primary dataset: contains the 17802 images labeled by the experts. Top-view(8901) and Bottom-view(8901).
(2). Dataset with fake images: We generated fake images using Conditional GAN (BigGAN) as follows: broken-2937, discolored-5823, pure-2937, silkcut-5823 instances and added them into the train set to balance the data set.
(3). Balanced dataset: In this case of adding newly captured images labeled using the Batch Active Learning method, new 9000 labeled images are added into the primary dataset. This new dataset contains 26,802 images split into train and validation set 80: 20, respectively. Contains the 17802 images and the 9K images labeled by the Active Learning (BatchBALD)."	https://paperswithcode.com/dataset/corn-seeds-dataset	15/08/2021						
4886	TLDR9+	"TLDR9+ is a large-scale summarization dataset containing over 9 million training instances extracted from Reddit discussion forum. This dataset is specifically gathered to perform extreme summarization (i.e., generating one-sentence summary in high compression and abstraction) and is more than twice larger than the previously proposed dataset. With the help of human annotations, a more fine-grained dataset is distilled by sampling High-Quality instances from TLDR9+ and call it TLDRHQ. dataset.
Image source: https://arxiv.org/pdf/2110.01159v1.pdf"	https://paperswithcode.com/dataset/tldr9	04/10/2021						
4887	LexGLUE	"Legal General Language Understanding Evaluation (LexGLUE) benchmark is a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way.
Image source: https://arxiv.org/pdf/2110.00976v1.pdf"	https://paperswithcode.com/dataset/lexglue	03/10/2021						
4888	Riseholme-2021	"Risholme-2021 contains >3.5K images of strawberries at various growth stages along with anomalous instances. Data collection was performed in the strawberry research farm at the Riseholme campus of the University of Lincoln in UK. For more details, please check out ""Homepage"" down below."	https://paperswithcode.com/dataset/riseholme-2021	21/09/2021						
4889	KG20C	"KG20C is a Knowledge Graph about high quality papers from 20 top computer science Conferences. It can serve as a standard benchmark dataset in scholarly data analysis for several tasks, including knowledge graph embedding, link prediction, recommendation systems, and question answering . 
For more information and download, please see the dataset homepage."	https://paperswithcode.com/dataset/kg20c	01/09/2019	A scholarly knowledge graph benchmark dataset					
4890	CCIHP	"CCIHP dataset is devoted to fine-grained description of people in the wild with localized & characterized semantic attributes. It contains 20 attribute classes and 20 characteristic classes split into 3 categories (size, pattern and color). The dataset has been introduced in this paper: Loesch, A., & Audigier, R. (2021, September). Describe me if you can! Characterized instance-level human parsing. In 2021 IEEE International Conference on Image Processing (ICIP) (pp. 2528-2532). IEEE.
The annotations were made with Pixano, an opensource, smart annotation tool for computer vision applications: https://pixano.cea.fr/"	https://paperswithcode.com/dataset/ccihp	19/09/2021	Characterized Crowd Instance-level Human Parsing					
4891	COVID-19 Contact Tracing Survey	A survey of Israelis about their attitudes towards COVID-19 contact tracing apps	https://paperswithcode.com/dataset/covid-19-contact-tracing-survey	05/10/2021						
4892	COVID-19 Contact Tracing Survey in Israel	A survey of Israelis about their attitudes towards COVID-19 contact tracing apps	https://paperswithcode.com/dataset/covid-19-contact-tracing-survey-in-israel							
4893	CaDIS	CaDIS: a Cataract Dataset for Image Segmentation is a dataset for semantic segmentation created by Digital Surgery Ltd. on top of the CATARACTS dataset. CaDIS consists of 4670 images sampled from the 25 videos on CATARACTS' training set. Each pixel in each image is labeled with its respective instrument or anatomical class from a set of 36 identified classes. More details about the dataset could be found in the paper (https://arxiv.org/pdf/1906.11586.pdf).	https://paperswithcode.com/dataset/cadis	27/06/2019	Cataract Dataset for Image Segmentation					
4894	!Optimizer 2021 Data	The data used for !Optimizer 2021 competition, based on seven biological model organisms.	https://paperswithcode.com/dataset/optimizer-2021-data	02/10/2021						
4895	Galaxy Zoo DECaLS	"Approx. 300,000 images of galaxies labelled by shape.
Labels are from www.galaxyzoo.org volunteers, and are noisy.
Images are from the DECaLS telescope survey.
Also includes predictions from an ensemble of EfficientNets, each using MC Dropout and a novel probabilistic loss function."	https://paperswithcode.com/dataset/galaxy-zoo-decals	30/12/2020						
4896	FooDI-ML	"Food Drinks and groceries Images Multi Lingual (FooDI-ML) is a dataset that contains over 1.5M unique images and over 9.5M store names, product names descriptions, and collection sections gathered from the Glovo application. The data made available corresponds to food, drinks and groceries products from 37 countries in Europe, the Middle East, Africa and Latin America. The dataset comprehends 33 languages, including 870K samples of languages of countries from Eastern Europe and Western Asia such as Ukrainian and Kazakh, which have been so far underrepresented in publicly available visiolinguistic datasets. The dataset also includes widely spoken languages such as Spanish and English.
Description from: FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions
Image source: https://github.com/Glovo/foodi-ml-dataset"	https://paperswithcode.com/dataset/foodi-ml	05/10/2021	Food Drinks and groceries Images Multi Lingual					
4897	BRAX	"Brax is a differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators. Brax is written in JAX and is designed for use on acceleration hardware. It is both efficient for single-device simulation, and scalable to massively parallel simulation on multiple devices, without the need for pesky datacenters.
Description from: https://github.com/google/brax
Image source: https://github.com/google/brax"	https://paperswithcode.com/dataset/brax	24/06/2021						
4898	TOAD-GAN	"A procedurally generated jump'n'run game with control over level similarity.
Image source: https://github.com/Mawiszus/TOAD-GAN"	https://paperswithcode.com/dataset/toad-gan	04/08/2020						
4899	RNADesign	An environment for RNA design given structure constraints with structures from different datasets to choose from.	https://paperswithcode.com/dataset/rnadesign	31/12/2018						
4900	CARL	"CARL (context adaptive RL) provides highly configurable contextual extensions to several well-known RL environments. It's designed to test your agent's generalization capabilities in all scenarios where intra-task generalization is important.
Benchmarks include:


OpenAI gym classic control suite extended with several physics context features like gravity or friction


OpenAI gym Box2D BipedalWalker, LunarLander and CarRacing, each with their own modification possibilities like new vehicles to race


All Brax locomotion environments with exposed internal features like joint strength or torso mass


Super Mario (TOAD-GAN), a procedurally generated jump'n'run game with control over level similarity


RNADesign, an environment for RNA design given structure constraints with structures from different datasets to choose from


Description from: CARL
Image source: https://github.com/automl/CARL"	https://paperswithcode.com/dataset/carl-1	05/10/2021	Context Adaptive RL					
4901	WMT 2020	"WMT 2020 is a collection of datasets used in shared tasks of the Fifth Conference on Machine Translation. The conference builds on a series of annual workshops and conferences on  Statistical Machine Translation.
The conference featured ten shared tasks:

a news translation task,
a biomedical translation task,
a similar language translation task,
an unsupervised and very low resource translation task,
an automatic post-editing task,
a metrics task (assess MT quality given reference translation),
a quality estimation task (assess MT quality without access to any reference),
a parallel corpus filtering and alignment task,
a lifelong learning in MT task,
a chat translation task.

Source: https://www.statmt.org/wmt20/"	https://paperswithcode.com/dataset/wmt-2020	19/11/2020						
4902	Multirotor-Gym	Multirotor gym environment for learning control policies for various unmanned aerial vehicles.	https://paperswithcode.com/dataset/multirotor-gym	15/07/2020						
4903	PANC	"Enables research on early detection of sexual predators in chats (eSPD).
It is made from the sexual predator identification dataset from PAN12 and from the dataset ChatCoder2.
It provides both full-length predator chats from PervertedJustice as well as short segments of non-predator chats. Together these can be used to evaluate eSPD systems."	https://paperswithcode.com/dataset/panc	01/08/2021						
4904	STPLS3D	"Our project (STPLS3D) aims to provide a large-scale aerial photogrammetry dataset with synthetic and real annotated 3D point clouds for semantic and instance segmentation tasks.
Although various 3D datasets with different functions and scales have been proposed recently, it remains challenging for individuals to complete the whole pipeline of large-scale data collection, sanitization, and annotation (e.g., semantic and instance labels). Moreover, the created datasets usually suffer from extremely imbalanced class distribution or partial low-quality data samples. Motivated by this, we explore the procedurally synthetic 3D data generation paradigm to equip individuals with the full capability of creating large-scale annotated photogrammetry point clouds. Specifically, we introduce a synthetic aerial photogrammetry point clouds generation pipeline that takes full advantage of open geospatial data sources and off-the-shelf commercial packages. Unlike generating synthetic data in virtual games, where the simulated data usually have limited gaming environments created by artists, the proposed pipeline simulates the reconstruction process of the real environment by following the same UAV flight pattern on a wide variety of synthetic terrain shapes and building densities, which ensure similar quality, noise pattern, and diversity with real data. In addition, the precise semantic and instance annotations can be generated fully automatically, avoiding the expensive and time-consuming manual annotation process.  Based on the proposed pipeline, we present a richly-annotated synthetic 3D aerial photogrammetry point cloud dataset, termed STPLS3D, with more than 16 km^2 of landscapes and up to 18 fine-grained semantic categories. For verification purposes, we also provide a parallel dataset collected from four areas in the real environment."	https://paperswithcode.com/dataset/stpls3d	17/03/2022						
4905	Nico-illust	This dataset contains over 400,000 images (illustrations) from Niconico Seiga and Niconico Shunga	https://paperswithcode.com/dataset/nico-illust		Nico-Illust					
4906	Restaurant-ACOS	"The Restaurant-ACOS dataset is constructed based on the SemEval 2016 Restaurant dataset (Pontiki et al., 2016) and its expansion datasets (Fan et al., 2019; Xu et al., 2020).
The SemEval 2016 Restaurant dataset (Pontiki et al., 2016) was annotated with explicit and implicit aspects, categories, and sentiment. (Fan et al., 2019; Xu et al., 2020) further added the opinion annotations. We integrate their annotations to construct aspect-category-opinion-sentiment quadruples and further annotate the implicit opinions. The Restaurant-ACOS dataset contains 2286 sentences with 3658 quadruples.
It is worth noting that the Restaurant-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc."	https://paperswithcode.com/dataset/restaurant-acos	01/08/2021						
4907	Laptop-ACOS	"Laptop-ACOS is a brand new Laptop dataset collected from the Amazon platform in the years 2017 and 2018 (covering ten types of laptops under six brands such as ASUS, Acer, Samsung, Lenovo, MBP, MSI, and so on). It contains 4,076 review sentences, much larger than the SemEval Laptop datasets.
For Laptop-ACOS, we annotate the four elements and their corresponding quadruples all by ourselves. We employ the aspect categories defined in the SemEval 2016 Laptop dataset. The Laptop-ACOS dataset contains 4076 sentences with 5758 quadruples. As we have mentioned, a large percentage of the quadruples contain implicit aspects or implicit opinions .  By comparing two datasets, it can be observed that Laptop-ACOS has a higher percentage of implicit opinions than Restaurant-ACOS . It is worth noting that the Laptop-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc."	https://paperswithcode.com/dataset/laptop-acos	01/08/2021						
4908	Interference suppression techniques for OPM-based MEG: Opportunities and challenges	"OPM data

Auditory evoked field paradigm during participant movement
Motor-beta power changes during a finger-tapping paradigm"	https://paperswithcode.com/dataset/interference-suppression-techniques-for-opm	06/10/2021						
4909	Molecule3D	Molecule3D is a new benchmark that includes a dataset with precise ground-state geometries of approximately 4 million molecules derived from density functional theory (DFT). It also provides a set of software tools for data processing, splitting, training, and evaluation, etc.	https://paperswithcode.com/dataset/molecule3d	30/09/2021						
4910	aethel	A dataset of approximately 75,000  phrases and sentences, syntactically analyzed as typelogical derivations (i.e. proofs of modal intuitionistic linear logic, or programs of the corresponding λ calculus). Analyses were obtained by transforming the dependency graphs of the Lassy-Small corpus.	https://paperswithcode.com/dataset/aethel	29/12/2019	Automatically Extracted Theorems from Lassy					
4911	MovingFashion	"MovingFashion is a dataset for video-to-shop, the task of retrieving clothes which are worn in social media videos. MovingFashion is composed of 14,855 social videos, each one of them associated with e-commerce ""shop"" images where the corresponding clothing items are clearly portrayed."	https://paperswithcode.com/dataset/movingfashion	06/10/2021						
4912	Mila Simulated Floods	"Mila Simulated Floods Dataset is a 1.5 square km virtual world using the Unity3D game engine including urban, suburban and rural areas.
The urban environment contains skyscrapers, large buildings, and roads, as well as objects such as traffic items and vehicles. The rural environment consists of a landscape of grassy hills, forests, and mountains, with sparse houses and other buildings such as a church, and no roads. The rural and urban areas make up for 1 square km of our virtual world. 
The suburban environment is a residential area of 0.5 square km with many individual houses with front yards.
To gather the simulated dataset, we captured before and after flood pairs from 2000 viewpoints with the following modalities:

non-flooded RGB image, depth map, segmentation map
flooded RGB image, binary mask of the flooded area, segmentation map

The camera was placed about 1.5 above ground, and has a field of view of 120 degree, and the resolution of the images is 1200 x 900. At each viewpoint, we took 10 pictures, by varying slightly the position of the camera in order to augment the dataset."	https://paperswithcode.com/dataset/mila-simulated-floods	06/10/2021						
4913	ETH Kinect Dataset	This dataset contains 27 ROS bags of point clouds produced by a Kinect based the ground truth obtained from a Vicon pose capture system. These runs cover 3 environments of increasing complexity, with 3 types of motions at 3 different speeds. This dataset can be used with our ICP Mapper to track the pose of the Kinect and to explore parameters of ICP algorithms.	https://paperswithcode.com/dataset/eth-kinect-dataset		ETH Kinect Dataset					
4914	ETH Laser Registration Datasets	"This group of datasets was recorded with the aim to test point cloud registration algorithms in specific environments and conditions. Special care is taken regarding the precision of the ""ground truth"" positions of the scanner, which is in the millimeter range, using a theodolite."	https://paperswithcode.com/dataset/eth-laser-registration-datasets		ETH Laser Registration Datasets					
4915	SpaceNet 7	Satellite imagery analytics have numerous human development and disaster response applications, particularly when time series methods are involved. For example, quantifying population statistics is fundamental to 67 of the 232 United Nations Sustainable Development Goals, but the World Bank estimates that more than 100 countries currently lack effective Civil Registration systems. The SpaceNet 7 Multi-Temporal Urban Development Challenge aims to help address this deficit and develop novel computer vision methods for non-video time series data. In this challenge, participants will identify and track buildings in satellite imagery time series collected over rapidly urbanizing areas. The competition centers around a new open source dataset of Planet satellite imagery mosaics, which includes 24 images (one per month) covering ~100 unique geographies. The dataset will comprise over 40,000 square kilometers of imagery and exhaustive polygon labels of building footprints in the imagery, totaling over 10 million individual annotations. Challenge participants will be asked to track building construction over time, thereby directly assessing urbanization.	https://paperswithcode.com/dataset/spacenet-7	08/02/2021	Multi-Temporal Urban Development SpaceNet Dataset					
4916	Labeled Retinal Optical Coherence Tomography Dataset for Classification of Normal, Drusen, and CNV Cases	"This dataset consists of more than 16,000 retinal OCT B-scans from 441 cases (Normal: 120, Drusen: 160, CNV: 161) and is acquired at Noor Eye Hospital, Tehran, Iran. Images are labeled by a retinal specialist.
The structure of the folders are as below:
- CNV, DRUSEN, NORMAL folders
- Within each class, folders are separated patient-wise with numbers from 1 to <number_of_patients>.
- Within each patient folder, images (B-scans) are labeled with <0XX_LABEL> format where <XX> is the B-scan number, and <LABEL> is the specialist's selected label for that specific B-scan.
The excel spreadsheet (data_information.csv) includes information such as ""Patient ID"", ""Class"", ""Eye"", ""B-scan"", ""Label"", and ""Directory"" for all images (16823 rows, 6 columns).
The python code (read_data.py) includes code for loading images and labels as NumPy arrays. The written function outputs the input data as an array with shape (number_of_images, imageSize, imageSize, 3) and output data as a list of labels (Normal: 0, Drusen: 1, CNV: 2). There are two different options for reading the files:
- Option 1: Reading all images. This would result in 16822 images.
- Option 2: Reading the worst-case condition images for each volume (i.e., if a patient was detected as a CNV case, only CNV-appearing B-scans were included for training procedure and normal and drusen B-scans of that patient are excluded from the dataset). This would result in 12649 images."	https://paperswithcode.com/dataset/labeled-retinal-optical-coherence-tomography	06/10/2021						
4917	Symbolic Mathematics	A personalized subset of Symbolic Mathematics dataset, initially introduced in the paper Deep Learning for Symbolic Mathematics (Lample et al.). We used this subset for our paper Pretrained Language Models are Symbolic Mathematics Solvers Too! (Noorbakhsh et al.).	https://paperswithcode.com/dataset/symbolic-mathematics	03/10/2021						
4918	AraCovid19-SSD	AraCovid19-SSD is a manually annotated Arabic COVID-19 sarcasm and sentiment detection dataset containing 5,162 tweets.	https://paperswithcode.com/dataset/aracovid19-ssd	05/10/2021						
4919	eBDtheque	"The eBDtheque database is a selection of one hundred comic pages from America, Japan (manga) and Europe.
Image source: http://ebdtheque.univ-lr.fr/database/"	https://paperswithcode.com/dataset/ebdtheque	07/10/2021						
4920	DCM	"The DCM dataset is composed of 772 annotated images from 27 golden age comic books. We freely collected them from the free public domain collection of digitized comic books Digital Comics Museum. One album per available publisher was selected to get as many different styles as possible. We made ground-truth bounding boxes of all panels, all characters (body + faces), small or big, human-like or animal-like.
Image source: https://gitlab.univ-lr.fr/crigau02/dcm-dataset/-/tree/master"	https://paperswithcode.com/dataset/dcm							
4921	Aristo-v4	The Aristo Tuple KB contains a collection of high-precision, domain-targeted (subject,relation,object) tuples extracted from text using a high-precision extraction pipeline, and guided by domain vocabulary constraints.  The dataset was introduced by the paper Domain-Targeted, High Precision Knowledge Extraction.	https://paperswithcode.com/dataset/aristo-v4	01/03/2017	Aristo Tuple KB Version 4					
4922	HowSumm	"HowSumm is a large-scale query-focused multi-document summarization dataset. It is focused on summarization of various sources to create HowTo guides. It is derived from wikiHow articles.
HowSumm is partitioned into HowSumm-Step where target summary is relatively short (avg. 90 words) and HowSumm-Method where target summary is a concatenation of several steps an therefore longer (avg. 500 words). HowSumm-Method and HowSumm-Step contain 11,121 and 84348 instances, respectively.
Description from: HowSumm
Image source: HowSumm"	https://paperswithcode.com/dataset/howsumm	07/10/2021						
4923	DSIOD	This dataset contains data which enables the evaluation of metamodels and approches for targeted test case selection without setting up test environments or performing test runs. The dataset is split into different scenarios: Each scenario comes with one or more tabular datasets containing the inputs and outputs of different test cases (concrete scenarios). A configuration file describes which of the columns are inputs and outputs and explains the different parameters. The config also contains verbal descriptions of the scenarios. Additionally, animations of the scenarios are available.	https://paperswithcode.com/dataset/dsiod	06/10/2021	Driving Scenario Input Output Dataset					
4924	RWD-10K	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/rwd-10k	18/09/2021	Rogue Wave Dataset-10K					
4925	Odysseus	"A major reason for the lack of a realistic Trojan detection method has been the unavailability of a large-scale benchmark dataset, consisting of clean and Trojan models. Here we introduce Odysseus the largest public dataset that contains over 3,000 trained clean and Trojan models based on Pytorch. 
While creating Odysseus, we focused on several factors such as mapping type, model architectures, fooling rate and validation accuracy of each model, and also the type of trigger. These models are trained on CIFAR10, Fashion-MNIST, and MNIST datasets. For each dataset, clean and Trojan models are trained for 4 different architectures. Namely Resent18, VGG19, Densenet, and GoogleNet for CIFAR10 and Fashion-MNIST and 4 custom-designed architectures for MNIST. We also considered various sources to target label mapping for the Trojan models."	https://paperswithcode.com/dataset/odysseus	16/07/2020	Clean and Trojan Models					
4926	IndicTTS	A special corpus of Indian languages covering 13 major languages of India. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text. We hope that these recordings will be useful for researchers and speech technologists working on synthesis and recognition. You can request zip archives of the entire database here.	https://paperswithcode.com/dataset/indictts							
4927	CGHD1152	"1152 Images
144 Circuits
12 Drafter
48,563 Object (Symbol, Structural, Text) Annotations"	https://paperswithcode.com/dataset/cghd1152	21/07/2021	Circuit Graph Hand Drawn 1152					
4928	A Curb Dataset	This is a dataset with curb annotations by using 3D LiDAR data and we build this dataset based on the SemanticKITTI dataset.	https://paperswithcode.com/dataset/a-curb-dataset	08/10/2021						
4929	WenetSpeech	"WenetSpeech is a multi-domain Mandarin corpus consisting of 10,000+ hours high-quality labeled speech, 2,400+ hours weakly labelled speech, and about 10,000 hours unlabeled speech, with 22,400+ hours in total. The authors collected the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics, and noisy conditions. An optical character recognition (OCR) based method is introduced to generate the audio/text segmentation candidates for the YouTube data on its corresponding video captions.
Image source: https://github.com/wenet-e2e/wenetspeech"	https://paperswithcode.com/dataset/wenetspeech	07/10/2021						
4930	MOD20	MOD20 is an action recognition dataset consisting of videos collected from YouTube and our own drone. The dataset contains 2,324 videos lasting a total of 240 minutes. The actions were selected from challenging and complex scenarios, and cover multiple viewpoints, from ground-level to bird's-eye view. The substantial variation in body size, number of people, viewpoints, camera motion, and background makes this dataset challenging for action recognition. The action classes, 720×720 size un-distorted clips and multi-viewpoint video selection extend the dataset's applicability to a wider research community.	https://paperswithcode.com/dataset/mod20	07/10/2021						
4931	NGAFID-MC	NGNGAFID-MC consists of over 7500 labeled flights, representing over 11,500 hours of per second flight data recorder readings of 23 sensor parameters.	https://paperswithcode.com/dataset/ngafid-mc	07/10/2021						
4932	FOD-A	FOD in Airports (FOD-A) is an image dataset of FOD, Foreign Object Degris, which consists of 31 object categories and over 30,000 annotation instances. The object categories have been selected based on guidance from prior documentation and related research by the Federal Aviation Administration (FAA).	https://paperswithcode.com/dataset/fod-a	06/10/2021						
4933	V-HICO	"V-HICO is a dataset for human-object interaction in videos. There are 6,594 videos, including 5,297 training videos, 635 validation videos, 608 test videos, and 54 unseen test videos, of human-object interaction. To test the performance of models on common human-object interaction classes and generalization to new human-object interaction classes, we provide two test splits, the first one has the same human-object interaction classes in the training split while the second one consists of unseen novel classes.
V-HICO consists of 244 object classes and 99 action classes. There are 756 action-object pairwise classes in total. The unseen test dataset contains 51 object classes and 32 action classes with 52 action-object pairwise classes. All videos are labeled with text annotations of the human action and the associated object. The test and unseen dataset contain the annotations of both human and object bounding boxes."	https://paperswithcode.com/dataset/v-hico	07/10/2021						
4934	VOC 2012	see detailed use case on code implementation of the paper 'Tell Me Where To Look: Guided Attention Inference Networks'	https://paperswithcode.com/dataset/voc-2012	27/02/2018	The PASCAL Visual Object Classes Challenge 2012					
4935	SignalTrain LA2A Dataset	"LA-2A Compressor data to accompany the paper ""SignalTrain: Profiling Audio Compressors with Deep Neural Networks,"" https://arxiv.org/abs/1905.11928
Accompanying computer code: https://github.com/drscotthawley/signaltrain
A collection of recorded data from an analog Teletronix LA-2A opto-electronic compressor, for various settings of the Peak Reduction knob.  Other knobs were kept constant.  
Audio samples present in these files are either 'randomly generated', or downloaded audio clips with Create Commons licenses, or are property of Scott Hawley freely distributed as part of this dataset. 
Data taken by Ben Colburn, supervised by Scott Hawley
Dataset used in:


""Efficient neural networks for real-time analog audio effect modeling"" by C. Steinmetz & J. Reiss, 2021. https://arxiv.org/abs/2102.06200


“Exploring quality and generalizability in parameterized neural audio effects,"" by W. Mitchell and S. H. Hawley, 149th Audio Engineering Society Convention (AES), 2020.  https://arxiv.org/abs/2006.05584


""SignalTrain: Profiling Audio Compressors with Deep Neural Networks,"" 147th Audio Engineering Society Convention (AES), 2019. https://arxiv.org/abs/1905.11928"	https://paperswithcode.com/dataset/signaltrain-la2a-dataset	28/05/2019						
4936	CityUHK-X-BEV	BEV Crowd-Counting dataset extended from CityUHK-X	https://paperswithcode.com/dataset/cityuhk-x-bev	10/10/2021						
4937	TBCOV	"TBCOV is a large-scale Twitter dataset comprising more than two billion multilingual tweets related to the COVID-19 pandemic collected worldwide over a continuous period of more than one year. Several state-of-the-art deep learning models are used to enrich the data with important attributes, including sentiment labels, named-entities (e.g., mentions of persons, organizations, locations), user types, and gender information. A geotagging method is proposed to assign country, state, county, and city information to tweets, enabling a myriad of data analysis tasks to understand real-world issues.
Description from: TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels
Image source: https://arxiv.org/pdf/2110.03664v1.pdf"	https://paperswithcode.com/dataset/tbcov	04/10/2021						
4938	Bridge Data	"Bridge Data is a large multi-domain and multi-task dataset, with 7,200 demonstrations constituting 71 tasks across 10 environments. The dataset is collected using a low-cost yet versatile 6-DoF WidowX250 robot arm and contains 7,200 demonstrations of a robot performing 71 kitchen tasks across 10 environments with varying lighting, robot positions, and backgrounds. It can be used to boosting generalization of robotic skills and empirically study how it can improve the learning of new tasks in new environments. 
Image source: https://arxiv.org/pdf/2109.13396v1.pdf"	https://paperswithcode.com/dataset/bridge-data	27/09/2021						
4939	BuildingNet	BuildingNet is a large-scale dataset of 3D building models whose exteriors are consistently labeled. The dataset consists on 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles.	https://paperswithcode.com/dataset/buildingnet	11/10/2021						
4940	EDFace-Celeb-1M	EDFace-Celeb-1M is a public Ethnically Diverse Face dataset which is used to benchmark the task of face hallucination. The dataset includes 1.7 million photos that cover different countries, with balanced race composition.	https://paperswithcode.com/dataset/edface-celeb-1m	11/10/2021						
4941	MetFaces	MetFaces is an image dataset of human faces extracted from works of art. The dataset consists of 1336 high-quality PNG images at 1024×1024 resolution. The images were downloaded via the Metropolitan Museum of Art Collection API, and automatically aligned and cropped using dlib. Various automatic filters were used to prune the set.	https://paperswithcode.com/dataset/metfaces	11/06/2020						
4942	DCCW	"| Name | Purpose | 
|------|---------|
| FM100P | Evaluation of the single palette sorting |
| KHTP | Evaluation of the palette pair sorting |
| LHSP | Evaluation of the palette similarity measurement |
| Perceptual Study | Perceptual Study |"	https://paperswithcode.com/dataset/dccw	19/07/2021						
4943	Energy Consumption Curves of 499 Customers from Spain	Predictions of energy consumption are crucial for energy retailers to minimize deviations from energy acquired in the day-ahead market and the actual consumption of their customers. The increasing spread of smartmeters means that retailers have access to hourly consumption values of all their contracted customers in realtime. Using machine learning algorithms, these hourly values can be used to calculate predictions for the future energy consumption of the customers. The present data set allows the training and validation of AI-based prediction models.	https://paperswithcode.com/dataset/energy-consumption-curves-of-499-customers	06/08/2021						
4944	EM-POSE	Electromagnetic measurements obtained from 12 wireless sensors, paired with the corresponding ground-truth SMPL poses. Approx. 37 minutes recorded with 5 participants.	https://paperswithcode.com/dataset/em-pose	11/10/2021						
4945	GeoMNIST	A simple dataset consisting of three geometric shapes (Triangle, Rectangle, Ellipsoid) of similar sizes but different orientations.	https://paperswithcode.com/dataset/geomnist	11/10/2021	Geometric Shapes MNIST					
4946	Gait Dataset	"Details about the creation of the dataset can be seen in https://arxiv.org/abs/2110.06139. 
@misc{sa2021classification,
      title={Classification of anomalous gait using Machine Learning techniques and embedded sensors}, 
      author={T. R. D. Sa and C. M. S. Figueiredo},
      year={2021},
      eprint={2110.06139},
      archivePrefix={arXiv},
      primaryClass={eess.SP}
}
The dataset can be download through the link posted in github repository."	https://paperswithcode.com/dataset/gait-dataset	08/10/2021	Human Gait Dataset					
4947	STR-2021	The STR-2021 dataset has 5,500 English sentence pairs manually annotated for semantic relatedness using a comparative annotation framework.	https://paperswithcode.com/dataset/str-2021	10/10/2021						
4948	MUNO21	MUNO21 is a large-scale and comprehensive dataset for the map update task. It includes time series of aerial images and map data to capture the evolution of both the physical road network and real street maps over time -- we collect NAIP aerial images at each of four years over the eight-year timespan from 2012–2019, and OSM extracts from each year during the same timespan.	https://paperswithcode.com/dataset/muno21	10/10/2021						
4949	KOHTD	"Kazakh offline Handwritten Text dataset (KOHTD) has 3000 handwritten exam papers and more than 140335 segmented images and there are approximately 922010 symbols. It can serve researchers in the field of handwriting recognition tasks by using deep and machine learning.
Image source: https://github.com/abdoelsayed2016/KOHTD"	https://paperswithcode.com/dataset/kohtd	22/09/2021	Kazakh Offline Handwritten Text Dataset					
4950	QMAR	"QMAR is an RGB multi-view Quality of Human Movement Assessment  dataset.
QMAR was recorded using 6 Primesense cameras (3 different frontal views and 3 different side views) with 38 healthy subjects, 8 female and 30 male. The subjects were trained by a physiotherapist to perform two different types of movements while simulating three ailments, resulting in five overall possibilities: a return Walk to approximately the original position while simulating Parkinsons (W-P), Stroke (W-S) and Limp (W-L), and Standing up and Sitting down with Parkinson (SS-P) and Stroke (SS-S).
The dataset includes RGB (and depth and skeleton) data, although in the current version contains only the RGB data. Depth/skeleton data can be obtinaed on request, however note they are available for two views only.
All documents and papers that use the QMAR dataset, or any derived part of the dataset, should cite the following paper: F. Sardari, A. Paiement, S. Hannuna, M. Mirmehdi; VI-Net: View-Invariant Quality of Human Movement Assessment, Sensors, 2020, 20, 5258."	https://paperswithcode.com/dataset/qmar	15/01/2021	Quality of Movement Assessment for Rehabilitation					
4951	Real Life Violence Situations Dataset	"This dataset has the following citation: M. Soliman, M. Kamal, M. Nashed, Y. Mostafa, B. Chawky, D. Khattab, “ Violence Recognition from Videos using Deep Learning Techniques”, Proc. 9th International Conference on Intelligent Computing and Information Systems (ICICIS'19), Cairo, pp. 79-84, 2019. please use it in case of using the dataset in research or engineering purpose )
when we start our Graduation Project Violence Recognition from Videos we found that there is shortage in available datasets related to violence between individuals so we decide to create new big dataset with variety of scenes
Content
Our Dataset Contains 1000 Violence and 1000 non-violence videos collected from youtube videos, violence videos in our dataset contain many real street fights situations in several environments and conditions.
also non-violence videos from our dataset are collected from many different human actions like sports, eating, walking …etc."	https://paperswithcode.com/dataset/real-life-violence-situations-dataset							
4952	ImageNet 50 samples per class	"This ImageNet version contains only 50 training images per class while the original testing set remains unchanged. It is one of the datasets comprising the data-efficient image classification (DEIC) benchmark.
It was proposed to challenge the generalization capabilities of modern image classifiers."	https://paperswithcode.com/dataset/imagenet-50-samples-per-class	30/08/2021						
4953	DEIC Benchmark	"DEIC is a benchmark for measuring the data efficiency of models in the context of image classification.
It is composed of 6 datasets that contain a small number of training samples per class (i.e., 30 < x < 80).
It covers multiple image domains (i.e., natural images, fine-grained recognition, medical images, remote sensing, handwriting recognition) and data types (i.e., RGB, grayscale, multi-spectral)."	https://paperswithcode.com/dataset/deic-benchmark	30/08/2021	Data-Efficient Image Classification Benchmark					
4954	Deep Sea Treasure Pareto-Front	"The dataset contains two Pareto-fronts:
- The Pareto-front for the 2-objective problem
- The Pareto-front for the 3-objective problem
Each Pareto-front contains a set of points, with coordinates given by their objectives. The dataset also contains 1 possible action sequence that leads to this point. If there are multiple possible paths leading to the same point, only 1 was kept."	https://paperswithcode.com/dataset/deep-sea-treasure-pareto-front	13/10/2021	Deep Sea Treasure Pareto-Front					
4955	Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems	"The dataset contains training and evaluation data for 12 languages:
- Vietnamese
- Romanian
- Latvian
- Czech
- Polish
- Slovak
- Irish
- Hungarian
- French
- Turkish
- Spanish
- Croatian
For each language, one training, one development and one testing set acquired from Wikipedia articles is provided. Moreover, each language dataset contains (substantially larger) training set collected from (general) Web texts. All sets, except for Wikipedia and Web training sets that can contain similar sentences, are disjoint. Data are segmented into sentences which are further word tokenized."	https://paperswithcode.com/dataset/multilingual-dataset-for-training-and	01/05/2018						
4956	Ego4D	"Ego4D is a massive-scale egocentric video dataset and benchmark suite. It offers 3,025 hours of daily life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, a host of new benchmark challenges are presented, centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, the aim is to push the frontier of first-person perception.
Description from: Facebook AI
Paper: Ego4D: Around the World in 3,000 Hours of Egocentric Video
GitHub: https://github.com/EGO4D"	https://paperswithcode.com/dataset/ego4d							
4957	EigenWorms	Caenorhabditis elegans is a roundworm commonly used as a model organism in the study of genetics. The movement of these worms is known to be a useful indicator for understanding behavioural genetics. Brown {\em et al.}1 describe a system for recording the motion of worms on an agar plate and measuring a range of human-defined features2. It has been shown that the space of shapes Caenorhabditis elegans adopts on an agar plate can be represented by combinations of six base shapes, or eigenworms. Once the worm outline is extracted, each frame of worm motion can be captured by six scalars representing the amplitudes along each dimension when the shape is projected onto the six eigenworms. Using data collected for the work described in1, we address the problem of classifying individual worms as wild-type or mutant based on the time series. The data were extracted from the C. elegans behavioural database 3. We have 259 cases, which we split 131 train and 128 test. We have truncated each series to the shortest usable. Each series has 17984 observations. Each worm is classified as either wild-type (the N2 reference strain) or one of four mutant types: goa-1; unc-1; unc-38 and unc-63. 1 A. Brown, E. Yemini, L. Grundy, T. Jucikas, and W. Schafer, A dictionary of behavioral motifs reveals clusters of genes affecting caenorhabditis elegans locomotion, Proceedings of the National Academy of Sciences of the United States of America (PNAS), vol. 10, no. 2, pp. 791 796, 2013. 2 E. Yemini, T. Jucikas, L. Grundy, A. Brown, and W. Schafer,  A database of caenorhabditis elegans behavioral phenotypes, Nature Methods, vol. 10, pp. 877 879, 2013. 3 C. elegans behavioural database	https://paperswithcode.com/dataset/eigenworms	31/10/2018						
4958	HUMAN4D	"HUMAN4D is a large and multimodal 4D dataset that contains a variety of human activities simultaneously captured by a professional marker-based MoCap, a volumetric capture and an audio recording system. By capturing 2 female and $2$ male professional actors performing various full-body movements and expressions, HUMAN4D provides a diverse set of motions and poses encountered as part of single- and multi-person daily, physical and social activities (jumping, dancing, etc. ), along with multi-RGBD (mRGBD), volumetric and audio data.
Description from: HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media
Image source: https://github.com/tofis/human4d_dataset"	https://paperswithcode.com/dataset/human4d	14/10/2021						
4959	ConditionalQA	ConditionalQA is a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply.	https://paperswithcode.com/dataset/conditionalqa	13/10/2021						
4960	PyTorrent	PyTorrent  contains 218,814 Python package libraries from PyPI and Anaconda environment. This is because earlier studies have shown that much of the code is redundant and Python packages from these environments are better in quality and are well-documented. PyTorrent enables users (such as data scientists, students, etc.) to build off the shelf machine learning models directly without spending months of effort on large infrastructure.	https://paperswithcode.com/dataset/pytorrent	04/10/2021	PyTorrent					
4961	Multimodal Emoji Prediction	The twitter emoji dataset obtained from CodaLab comprises of 50 thousand tweets along with the associated emoji label. Each tweet in the dataset has a corresponding numerical label which maps to a specific emoji. The emojis are of the 20 most frequent emojis and hence the labels range from 0 to 19	https://paperswithcode.com/dataset/multimodal-emoji-prediction	23/02/2017						
4962	DeepGlobe	"We observe that satellite imagery is a powerful source of information as it contains more structured and uniform data, compared to traditional images. Although computer vision community has been accomplishing hard tasks on everyday image datasets using deep learning, satellite images are only recently gaining attention for maps and population analysis. This workshop aims at bringing together a diverse set of researchers to advance the state-of-the-art in satellite image analysis.
To direct more attention to such approaches, we propose DeepGlobe Satellite Image Understanding Challenge, structured around three different satellite image understanding tasks. The datasets created and released for this competition may serve as reference benchmarks for future research in satellite image analysis. Furthermore, since the challenge tasks will involve ""in the wild"" forms of classic computer vision problems, these datasets have the potential to become valuable testbeds for the design of robust vision algorithms, beyond the area of remote sensing."	https://paperswithcode.com/dataset/deepglobe	17/05/2018						
4963	Massachusetts Roads Dataset	"The datasets introduced in Chapter 6 of my PhD thesis are below. See the thesis for more details. If you use any of these datasets for research purposes you should use the following citation in any resulting publications:
@phdthesis{MnihThesis,
    author = {Volodymyr Mnih},
    title = {Machine Learning for Aerial Image Labeling},
    school = {University of Toronto},
    year = {2013}
}"	https://paperswithcode.com/dataset/massachusetts-roads-dataset	10/04/2013	Road and Building Detection Datasets - Massachusetts Roads Dataset					
4964	Chikusei Dataset	"The airborne hyperspectral dataset was taken by Headwall Hyperspec-VNIR-C imaging sensor over agricultural and urban areas in Chikusei, Ibaraki, Japan, on July 29, 2014 between the times 9:56 to 10:53 UTC+9. The central point of the scene is located at coordinates: 36.294946N, 140.008380E. The hyperspectral dataset has 128 bands in the spectral range from 363 nm to 1018 nm. The scene consists of 2517x2335 pixels and the ground sampling distance was 2.5 m. Ground truth of 19 classes was collected via a field survey and visual inspection using high-resolution color images obtained by Canon EOS 5D Mark II together with the hyperspectral data. The hyperspectral data and ground truth were made available to the scientific community in the ENVI and MATLAB formats at http://park.itc.u-tokyo.ac.jp/sal/hyperdata. More details of the experiment are presented in the technical report given below.
In order to use the datasets, please fulfill the following three requirements:
1) Giving an acknowledgement as follows:
The authors gratefully acknowledge Space Application Laboratory, Department of Advanced Interdisciplinary Studies, the University of Tokyo for providing the hyperspectral data.
2) Using the following license for hyperspectral data:
http://creativecommons.org/licenses/by/3.0/
3) This dataset was made public by Dr. Naoto Yokoya and Prof. Akira Iwasaki from the University of Tokyo. Please cite:
In WORD:
N. Yokoya and A. Iwasaki, ""Airborne hyperspectral data over Chikusei,"" Space Appl. Lab., Univ. Tokyo, Japan, Tech. Rep. SAL-2016-05-27, May 2016.
In LaTex:
@techreport{NYokoya2016,
author = {N. Yokoya and A. Iwasaki},
title = {Airborne hyperspectral data over Chikusei},
institution = {Space Application Laboratory, University of Tokyo},
number = {SAL-2016-05-27},
address = {Japan},
month = {May},
year = 2016,
}"	https://paperswithcode.com/dataset/chikusei-dataset	10/05/2016	Airborne hyperspectral data taken over Chikusei					
4965	LoveDA	"5987 high spatial resolution (0.3 m) remote sensing images from Nanjing, Changzhou, and Wuhan
Focus on different geographical environments between Urban and Rural
Advance both semantic segmentation and domain adaptation tasks
Three considerable challenges:
Multi-scale objects
Complex background samples
Inconsistent class distributions



Two contests are held on the Codalab:
<b>LoveDA Semantic Segmentation Challenge</b>, <b>LoveDA Unsupervised Domain Adaptation Challenge</b>"	https://paperswithcode.com/dataset/loveda	17/10/2021	Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation					
4966	FMFCC-A	FMFCC-A is a large publicly-available Mandarin dataset for synthetic speech detection, which contains 40,000 synthesized Mandarin utterances that generated by 11 Mandarin TTS systems and two Mandarin VC systems, and 10,000 genuine Mandarin utterance collected from 58 speakers. The FMFCCA dataset is divided into the training, development and evaluation sets, which are used for the research of detection of synthesised Mandarin speech under various previously unknown speech synthesis systems or audio post-processing operations.	https://paperswithcode.com/dataset/fmfcc-a	18/10/2021						
4967	POG	Object detection dataset featuring people walking on grass captured aboard a UAV. This data sets include precise meta data information about altitude, viewing angle and others.	https://paperswithcode.com/dataset/pog		People On Grass					
4968	DialFact	"DialFact is a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether a response carries verifiable factual information; 2) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue response to be supported, refuted, or not enough information.
Description from: DialFact: A Benchmark for Fact-Checking in Dialogue"	https://paperswithcode.com/dataset/dialfact	15/10/2021						
4969	BBQ	Bias Benchmark for QA (BBQ) is a dataset consisting of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine different social dimensions relevant for U.S. English-speaking contexts.	https://paperswithcode.com/dataset/bbq	15/10/2021	Bias Benchmark for QA					
4970	Acappella	"Acappella comprises around 46 hours of a cappella solo singing videos sourced from YouTbe, sampled across different singers and languages. Four languages are considered: English, Spanish, Hindi and others.  
This dateset was designed for Audiovisual singing voice separation, although it can be used for any self-supervised audio-visual task such us: audio-guided  lip reading, audio-visual learning,"	https://paperswithcode.com/dataset/acappella	20/04/2021						
4971	CCQA	CCQA is  new web-scale dataset for in-domain model pre-training. CCQA is a novel QA dataset based on the Common Crawl project. Using the readily available schema.org annotation, around 130 million multilingual question-answer pairs are extracted, including about 60 million English data-points.	https://paperswithcode.com/dataset/ccqa	14/10/2021						
4972	SHREC'19	Shape matching plays an important role in geometry processing and shape analysis. In the last decades, much research has been devoted to improve the quality of matching between surfaces. This huge effort is motivated by several applications such as object retrieval, animation and information transfer just to name a few. Shape matching is usually divided into two main categories: rigid and non rigid matching. In both cases, the standard evaluation is usually performed on shapes that share the same connectivity, in other words, shapes represented by the same mesh. This is mainly due to the availability of a “natural” ground truth that is given for these shapes. Indeed, in most cases the consistent connectivity directly induces a ground truth correspondence between vertices. However, this standard practice obviously does not allow to estimate the robustness of a method with respect to different connectivity. With this track, we propose a benchmark to evaluate the performance of point-to-point matching pipelines when the shapes to be matched have different connectivity (see Figure 1). We consider the concurrent presence of 1) different meshing, 2) rigid transformation in 3D space, 3) non-rigid deformations, 4) different vertex density, ranging from 5K to more than 50K, and 5) topological changes induced by mesh gluing in areas of contact. The correspondence between these shapes is obtained through the recently proposed registration pipeline FARM 1. This method provides a high-quality registration of the SMPL model 2 to a large set of human meshes coming from different datasets from which we obtain a well-defined correspondence for all the meshes registered and SMPL itself.	https://paperswithcode.com/dataset/shrec-19	27/07/2018	SHREC'19 track Matching Humans with Different Connectivity					
4973	TOSCA	Hi-resolution three-dimensional nonrigid shapes in a variety of poses for non-rigid shape similarity and correspondence experiments. The database contains a total of 80 objects, including 11 cats, 9 dogs, 3 wolves, 8 horses, 6 centaurs, 4 gorillas, 12 female figures, and two different male figures, containing 7 and 20 poses. Typical vertex count is about 50,000. Objects within the same class have the same triangulation and an equal number of vertices numbered in a compatible way. This can be used as a per-vertex ground truth correspondence in correspondence experiments. Two representations are available: MATLAB file (.mat) and ASCII text files containing the 1-based list of triangular faces (.tri), and a list of vertex XYZ coordinates (.vert). A .png thumbnail is available for each object.	https://paperswithcode.com/dataset/tosca		TOSCA high-resolution					
4974	SHREC'16 Partial Benchmark	"Finding a correspondence between two shapes is a fundamental task in computer graphics and geometry processing with applications ranging from texture mapping to animation. A particularly challenging and widely studied setting is when shapes are allowed to undergo quasi-isometric deformations, as it happens when we consider articulated bodies in different poses. An even more interesting scenario is partial correspondence, where one is shown only a subset of the shape, and has to match it with a deformable version thereof. Partial correspondence problems arise in numerous applications that involve real data acquisition by 3D sensors, which inevitably leads to missing parts due to occlusions and partial views.
We propose a benchmark to evaluate the performance of algorithms for establishing correspondences between a full shape and its deformed versions, under the presence of different amounts and kinds of partiality."	https://paperswithcode.com/dataset/shrec-16-partial-benchmark		SHREC 2016 TRACK: PARTIAL MATCHING OF DEFORMABLE SHAPES					
4975	BEAMetrics	BEAMetrics (Benchmark to Evaluate Automatic Metrics) is resource to make research into new metrics for evaluation of generated language easier to evaluate. BEAMetrics users can quickly compare existing and new metrics with human judgements across a diverse set of tasks, quality dimensions (fluency vs. coherence vs. informativeness etc), and languages.	https://paperswithcode.com/dataset/beametrics	18/10/2021						
4976	CoDa	"The Color Dataset (CoDa) is a probing dataset to evaluate the representation of visual properties in language models. CoDa consists of color distributions for 521 common objects, which are split into 3 groups: Single, Multi, and Any. 
The default configuration of CoDa uses 10 CLIP-style templates (e.g. ""A photo of a [object]""), and 10 cloze-style templates (e.g. ""Everyone knows most [object] are [color]."""	https://paperswithcode.com/dataset/coda	15/10/2021	The Color Dataset					
4977	NYU-VPR	NYU-VPR is a dataset for Visual place recognition (VPR) that contains more than 200,000 images over a 2km×2km area near the New York University campus, taken within the whole year of 2016.	https://paperswithcode.com/dataset/nyu-vpr	18/10/2021						
4978	CodRep	"Five curated datasets of one-liner commits from open-source projects. In total,
they are composed of 58069 one-liner commits."	https://paperswithcode.com/dataset/codrep	24/12/2018						
4979	MAAD	The Model for Attended Awareness in Driving (MAAD) is a dataset of third-person estimates of a driver’s attended awareness. It consists of videos of a scene, as seen by a person performing a task in the scene, along with noisily registered ego-centric gaze sequences from that person.	https://paperswithcode.com/dataset/maad	16/10/2021						
4980	PubTables-1M	"The goal of PubTables-1M is to create a large, detailed, high-quality dataset for training and evaluating a wide variety of models for the tasks of table detection, table structure recognition, and functional analysis. It contains:

460,589 annotated document pages containing tables for table detection.
947,642 fully annotated tables including text content and complete location (bounding box) information for table structure recognition and functional analysis.
Full bounding boxes in both image and PDF coordinates for all table rows, columns, and cells (including blank cells), as well as other annotated structures such as column headers and projected row headers.
Rendered images of all tables and pages.
Bounding boxes and text for all words appearing in each table and page image.
Additional cell properties not used in the current model training.

Additionally, cells in the headers are canonicalized and we implement multiple quality control steps to ensure the annotations are as free of noise as possible. For more details, please see our paper."	https://paperswithcode.com/dataset/pubtables-1m	30/09/2021	PubMed Tables One Million					
4981	Experiment-data-for-UM-S-TM	"0.This is experiment data for the following article:
@misc{liu2021topic,
      title={Topic Model Supervised by Understanding Map}, 
      author={Gangli Liu},
      year={2021},
      eprint={2110.06043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


*.txt files are the data of Table 4 of the paper.


The top lines of all the *.txt files are contents of the artificial documents. Column names are  : ""Topic"", ""Distance"", ""Topic-len"", ""alpha""/""Noise"" , ""doc concept-length"", and ""Votes counter"".


3.Coding of file names of *.txt files see ""Table 4: Discovered SCOM of six documents"". ""all_topic"" means the candidate topic set is all the topics in a domain.
4.For the ""300docs-mentioned-in-section3.2.xlsx"" file, its name tells its contents."	https://paperswithcode.com/dataset/experiment-data-for-um-s-tm	12/10/2021						
4982	CNewSum	CNewSum is a large-scale Chinese news summarization dataset which consists of 304,307 documents and human-written summaries for the news feed. It has long documents with high-abstractive summaries, which can encourage document-level understanding and generation for current summarization models. An additional distinguishing feature of CNewSum is that its test set contains adequacy and deducibility annotations for the summaries.	https://paperswithcode.com/dataset/cnewsum	21/10/2021						
4983	UVO	"UVO is a new benchmark for open-world class-agnostic object segmentation in videos. Besides shifting the problem focus to the open-world setup, UVO is significantly larger, providing approximately 8 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VOS and YouTube-VIS. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. Some highlights of the dataset include:


High quality instance masks densely annotated at 30 fps on 1024 YouTube videos and 1fps on 10337 videos from Kinetics dataset


Open-world: annotating all objects in each video, 13.5 objects per video on average


Diverse object categories: 57% of objects are not covered by COCO categories"	https://paperswithcode.com/dataset/uvo		Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation					
4984	ASIRRA	"Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords.
Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface:
Asirra is unique because of its partnership with Petfinder.com, the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research."	https://paperswithcode.com/dataset/asirra	02/02/2014	(Animal Species Image Recognition for Restricting Access					
4985	OpenImages-v6	"OpenImages V6 is a large-scale dataset ,
consists of 9 million training images, 41,620 validation
samples, and 125,456 test samples. It is a partially annotated dataset, with 9,600 trainable classes"	https://paperswithcode.com/dataset/openimages-v6	02/11/2018						
4986	NACA Airfoils	The training datasets consisting of NACA 4- and 5-digit airfoils, at different flight conditions, were generated using Javafoil.	https://paperswithcode.com/dataset/naca-airfoils	24/09/2021						
4987	TAU-NIGENS Spatial Sound Events 2020	The TAU-NIGENS Spatial Sound Events 2020 dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a trajectory of its direction-of-arrival (DoA) to the recording point, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the NIGENS general sound events database. These recordings serve as the development dataset for the DCASE 2020 Sound Event Localization and Detection Task of the DCASE 2020 Challenge.	https://paperswithcode.com/dataset/tau-nigens-spatial-sound-events-2020	06/04/2020	TAU-NIGENS Spatial Sound Events 2020					
4988	ICASSP 2021 Acoustic Echo Cancellation Challenge	The ICASSP 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report good performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 2,500 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source two large test sets, and we open source an online subjective test framework for researchers to quickly test their results. The winners of this challenge will be selected based on the average Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.	https://paperswithcode.com/dataset/icassp-2021-acoustic-echo-cancellation	10/09/2020						
4989	INTERSPEECH 2021 Acoustic Echo Cancellation Challenge	The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended to stimulate research in the area of acoustic echo cancellation (AEC), which is an important part of speech enhancement and still a top issue in audio communication and conferencing systems. Many recent AEC studies report reasonable performance on synthetic datasets where the train and test samples come from the same underlying distribution. However, the AEC performance often degrades significantly on real recordings. Also, most of the conventional objective metrics such as echo return loss enhancement (ERLE) and perceptual evaluation of speech quality (PESQ) do not correlate well with subjective speech quality tests in the presence of background noise and reverberation found in realistic environments. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 5,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We open source an online subjective test framework based on ITU-T P.808 for researchers to quickly test their results. The winners of this challenge will be selected based on the average P.808 Mean Opinion Score (MOS) achieved across all different single talk and double talk scenarios.	https://paperswithcode.com/dataset/interspeech-2021-acoustic-echo-cancellation	01/06/2021						
4990	SCICAP	"SCICAP is a large-scale image captioning dataset that contains real-world scientific figures and captions. SCICAP was constructed using more than two million images from over 290,000 papers collected and released by arXiv.
Image source: https://arxiv.org/pdf/2110.11624v1.pdf"	https://paperswithcode.com/dataset/scicap	22/10/2021						
4991	FIRESTARTER 2 - dataset and notebooks	"Data used in the paper ""FIRESTARTER 2: Dynamic Code Generation for Processor Stress Tests"", as well as notebooks to generate plots."	https://paperswithcode.com/dataset/firestarter-2-dataset-and-notebooks	25/06/2021						
4992	NTU RGB+D 2D	NTU RGB+D 2D is a curated version of NTU RGB+D often used for skeleton-based action prediction and synthesis. It contains less number of actions.	https://paperswithcode.com/dataset/ntu-rgb-d-2d	11/04/2016						
4993	Small-Bench NLP	Small-Bench NLP is a benchmark for small efficient neural language models trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks on the publicly available GLUE datasets and a leaderboard to track the progress of the community.	https://paperswithcode.com/dataset/small-bench-nlp	22/09/2021						
4994	SpaceNet 2	"SpaceNet 2: Building Detection v2 - is a dataset for building footprint detection in geographically diverse settings from very high resolution satellite images. It contains over 302,701 building footprints, 3/8-band Worldview-3 satellite imagery at 0.3m pixel res., across 5 cities (Rio de Janeiro, Las Vegas, Paris, Shanghai, Khartoum), and covers areas that are both urban and suburban in nature. The dataset was split using 60%/20%/20% for train/test/validation.
The main use case for the detection of building footprints from satellite imagery is to aid foundational mapping."	https://paperswithcode.com/dataset/spacenet-2	03/07/2018	SpaceNet 2: Building Detection v2					
4995	Turn-Level Goals Dataset	"This dataset is a record of the active learning data collected from interacting with PersonaGPT to fine-tune its actions toward turn-level goals, which are text descriptions of decoding goals for each response in a conversation. 
There are 11 possible turn-level goals that can be used to condition the PersonaGPT response at each turn of the conversation. To encode new turn-level goals, use the ActiveGym environment in https://github.com/af1tang/convogym to collect new active learning data."	https://paperswithcode.com/dataset/turn-level-goals-dataset	25/10/2021						
4996	Coveo Data Challenge Dataset	"The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for ""In-session prediction for purchase intent and recommendations"". The challenge addresses the growing need for reliable predictions within the boundaries of a shopping session, as customer intentions can be different depending on the occasion. The need for efficient procedures for personalization is even clearer if we consider the e-commerce landscape more broadly: outside of giant digital retailers, the constraints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers. We release a new session-based dataset including more than 30M fine-grained browsing events (product detail, add, purchase), enriched by linguistic behavior (queries made by shoppers, with items clicked and items not clicked after the query) and catalog meta-data (images, text, pricing information). On this dataset, we ask participants to showcase innovative solutions for two open problems: a recommendation task (where a model is shown some events at the start of a session, and it is asked to predict future product interactions); an intent prediction task, where a model is shown a session containing an add-to-cart event, and it is asked to predict whether the item will be bought before the end of the session."	https://paperswithcode.com/dataset/coveo-data-challenge-dataset	19/04/2021						
4997	VerSe	"Spine or vertebral segmentation is a crucial step in all applications regarding automated quantification of spinal morphology and pathology. With the advent of deep learning, for such a task on computed tomography (CT) scans, a big and varied data is a primary sought-after resource. However, a large-scale, public dataset is currently unavailable.
VerSe is a large scale, multi-detector, multi-site, CT spine dataset consisting of 374 scans from 355 patients. The challenge was held in two iterations in conjunction with MICCAI 2019 and 2020. The tasks evaluated for include: vertebral labelling and segmentation.
Image source: https://arxiv.org/pdf/2001.09193v5.pdf"	https://paperswithcode.com/dataset/verse-1	24/01/2020	Large Scale Vertebrae Segmentation Challenge					
4998	The Berka Dataset	The Berka dataset is a collection of financial information from a Czech bank. The dataset deals with over 5,300 bank clients with approximately 1,000,000 transactions. Additionally, the bank represented in the dataset has extended close to 700 loans and issued nearly 900 credit cards, all of which are represented in the data.	https://paperswithcode.com/dataset/the-berka-dataset							
4999	SSP-3D	SSP-3D is an evaluation dataset consisting of 311 images of sportspersons in tight-fitted clothes, with a variety of body shapes and poses. The images were collected from the Sports-1M dataset. SSP-3D is intended for use as a benchmark for body shape prediction methods. Pseudo-ground-truth 3D shape labels (using the SMPL body model) were obtained via multi-frame optimisation with shape consistency between frames, as described here.	https://paperswithcode.com/dataset/ssp-3d	21/09/2020	Sports Shape and Pose 3D					
5000	SpaceNet 1	"SpaceNet 1: Building Detection v1 is a dataset for building footprint detection. The data is comprised of 382,534 building footprints, covering an area of 2,544 sq. km of 3/8 band WorldView-2 imagery (0.5 m pixel res.) across the city of Rio de Janeiro, Brazil. The images are processed as 200m×200m tiles with associated building footprint vectors for training.
The main use case for the detection of building footprints from satellite imagery is to aid foundational mapping."	https://paperswithcode.com/dataset/spacenet-1	03/07/2018	SpaceNet 1: Building Detection v1					
5001	Carbon Intensity 2020	Energy production and carbon intensity datasets for the regions Germany, Great Britain, France (all via the ENTSO-E Transparency Platform) and California (via California ISO) for the entire year 2020 +-10 days.	https://paperswithcode.com/dataset/carbon-intensity-2020	25/10/2021	Carbon Intensity Data of Germany, Great Britain, France, and California in 2020					
5002	SMLM CEP152-Complex FITS Images	"The following files comprise 19 sets of 40,000 images, each set corresponding to a different rendering sigma as described in the paper.

Extracting *

To extract the files, execute the following command (under Linux):
cat paper_data.tar.gz.* | tar xzvf -

Organisation *

The data are grouped into 19 directories, corresponding to the sigma value they were rendered at. These values are
10, 9, 8.1,  7.29, 6.56, 5.9, 5.31, 4.78, 4.3, 3.87, 3.65, 3.28, 2.95, 2.66, 2.39, 2.15, 1.94, 1.743, 1.57, 1.41
Each directory contains 40,000 FITS files - a NASA floating point image standard. The images are single channel and un-normalised. This structure is ready to be used

Recreating the data *

If you have the time and compute power, you can regenerate this data set with as many or as few images as you prefer, at any sigma level. The original experimental data is available at <fill in later>.
To recreate the data set you need to download the CEPRender program, available on github: https://github.com/OniDaito/CEPrender - details on how to use this program are available with the code."	https://paperswithcode.com/dataset/smlm-cep152-complex-fits-images	14/10/2021	SMLM CEP152-Complex FITS Images					
5003	IndoNLI	IndoNLI is the first human-elicited NLI dataset for Indonesian consisting of nearly 18K sentence pairs annotated by crowd workers and experts.	https://paperswithcode.com/dataset/indonli	27/10/2021						
5004	SALAMI	"Comes from https://ddmal.music.mcgill.ca/research/SALAMI/:
SALAMI is an innovative and ambitious computational musicology project. To date, musical analysis has been conducted by individuals and on a small scale. Our computational approach, combined with the huge volume of data now available from such source as the Internet Archive, will: a) deliver a very substantive corpus of musical analyses in a common framework for use by music scholars, students and beyond; and, b) establish a methodology and tooling which will enable others to add to this in the future and to broaden the application of the techniques we establish. A resource of SALAMI’s magnitude empowers musicologists to approach their work in a new and different way, starting with the data, and to ask research questions that have not been possible before.
There are two resources available on this site:

Annotation data: Visit this page to access the annotation data and to learn about how it was collected.
Blog: Here you’ll find updates about SALAMI features and tools. The older posts recount the data collection process.
Background: The background page gives an overview of the SALAMI project. It consists mainly of the proposal for the Digging Into Data grant that SALAMI was awarded in 2009.

Through a Digging Into Data grant, this research was supported by the Social Sciences and Humanities Research Council of Canada, by the National Science Foundation, and by JISC."	https://paperswithcode.com/dataset/salami		Structural Analysis of Large Amounts of Music Information					
5005	Lower-limb Kinematics and Kinetics During Continuously Varying Human Locomotion	This dataset reports the lower-limb kinematics and kinetics of ten able-bodied subjects walking at multiple inclines (± 0°, 5°, and 10°) and speeds (0.8 m/s, 1 m/s, and 1.2 m/s), running over level-ground at multiple speeds (1.8 m/s, 2 m/s, 2.2 m/s, and 2.4 m/s), walking and running with constant acceleration and deceleration (± 0.2 m/s2, and 0.5 m/s2), and stair ascent/descent with multiple stair inclines (± 20°, 25°, 30°, and 35°). This dataset also includes sit-stand transitions, walk-run transitions, and walk-stairs transitions. Data were recorded by a Vicon motion capture system and, for applicable tasks, a Bertec instrumented treadmill. This dataset can aid in the development of kinematic models of multi-activity human locomotion and the design and control of agile wearable robots.	https://paperswithcode.com/dataset/lower-limb-kinematics-and-kinetics-during	27/08/2021						
5006	TMBuD	TMBuD is a dataset for building recognition and 3D reconstruction of human made structures in urban scenarios. The dataset features 160 images of buildings from Timişoara, Romania, with a resolution of 768 x 1024 pixels each. The proposed dataset will allow proper evaluation of salient edges and semantic segmentation of images focusing on the street view perspective	https://paperswithcode.com/dataset/tmbud	27/10/2021						
5007	Surgical Hands	Surgical Hands is a dataset that provides multi-instance articulated hand pose annotations for in-vivo videos. The dataset contains 76 video clips from 28 publicly available surgical videos and over 8.1k annotated hand pose instances.	https://paperswithcode.com/dataset/surgical-hands	12/01/2021						
5008	A Dataset of Multispectral Potato Plants Images	The dataset contains aerial agricultural images of a potato field with manual labels of healthy and stressed plant regions. The images were collected with a Parrot Sequoia multispectral camera carried by a 3DR Solo drone flying at an altitude of 3 meters. The dataset consists of RGB images with a resolution of 750×750 pixels, and spectral monochrome red, green, red-edge, and near-infrared images with a resolution of 416×416 pixels, and XML files with annotated bounding boxes of healthy and stressed potato crop.	https://paperswithcode.com/dataset/a-dataset-of-multispectral-potato-plants	14/06/2021						
5009	CIFAR-10N	This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.	https://paperswithcode.com/dataset/cifar-10n	22/10/2021	Real-World Human Annotations					
5010	CIFAR-100N	This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk.	https://paperswithcode.com/dataset/cifar-100n	22/10/2021	Real-World Human Annotations					
5011	MedMNIST v2	"MedMNIST v2 is a large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with the corresponding classification labels, so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various data scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision and machine learning.
Description  and image from: MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification
Each subset keeps the same license as that of the source dataset. Please also cite the corresponding paper of source data if you use any subset of MedMNIST."	https://paperswithcode.com/dataset/medmnist-v2	27/10/2021						
5012	OpenBMAT	"Open Broadcast Media Audio from TV (OpenBMAT) is an open, annotated dataset for the task of music detection that contains over 27 hours of TV broadcast audio from 4 countries distributed over 1647 one-minute long excerpts. It is designed to encompass several essential features for any music detection dataset and is the first one to include annotations about the loudness of music in relation to other simultaneous non-music sounds. OpenBMAT has been cross-annotated by 3 annotators obtaining high inter-annotator agreement percentages, which validates the annotation methodology and ensures the annotations reliability.
Source: Open Broadcast Media Audio from TV (OpenBMAT)"	https://paperswithcode.com/dataset/openbmat	28/08/2019	Open Broadcast Media Audio from TV					
5013	IndoNLG	IndoNLG is a benchmark to measure natural language generation (NLG) progress in three low-resource—yet widely spoken—languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks.	https://paperswithcode.com/dataset/indonlg	16/04/2021						
5014	Continual World	Continual World is a benchmark consisting of realistic and meaningfully diverse robotic tasks built on top of Meta-World as a testbed.	https://paperswithcode.com/dataset/continual-world	23/05/2021						
5015	Natural Instructions	Natural-Instructions is a dataset of 61 distinct tasks, their human-authored instructions and 193k task instances. The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema.	https://paperswithcode.com/dataset/natural-instructions	18/04/2021						
5016	Pyxis	Pyxis is a performance dataset for specialized accelerators on sparse data. Pyxis collects accelerator designs and real execution performance statistics. Currently, there are 73.8 K instances in Pyxis.	https://paperswithcode.com/dataset/pyxis	08/10/2021						
5017	OPERAnet	OPERAnet is a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors. Approximately 8 hours of annotated measurements are provided, which are collected across two different rooms from 6 participants performing 6 activities, namely, sitting down on a chair, standing from sit, lying down on the ground, standing from the floor, walking and body rotating. The dataset has been acquired from four synchronized modalities for the purpose of passive Human Activity Recognition (HAR) as well as localization and crowd counting.	https://paperswithcode.com/dataset/operanet	08/10/2021						
5018	AI-TOD	AI-TOD comes with 700,621 object instances for eight categories across 28,036 aerial images. Compared to existing object detection datasets in aerial images, the mean size of objects in AI-TOD is about 12.8 pixels, which is much smaller than others.	https://paperswithcode.com/dataset/ai-tod	10/01/2021	Tiny Object Detection in Aerial Images					
5019	URLB	URLB consists of two phases: reward-free pre-training and downstream task adaptation with extrinsic rewards. Building on the DeepMind Control Suite, it provides twelve continuous control tasks from three domains for evaluation.	https://paperswithcode.com/dataset/urlb	28/10/2021	Unsupervised Reinforcement Learning Benchmark					
5020	CoVA	"We labeled 7,740 webpage screenshots spanning 408 domains (Amazon, Walmart, Target, etc.). Each of these webpages contains exactly one labeled price, title, and image. All other web elements are labeled as background. On average, there are 90 web elements in a webpage.
Webpage screenshots and bounding boxes can be obtained here
Train-Val-Test split
We create a cross-domain split which ensures that each of the train, val and test sets contains webpages from different domains. Specifically, we construct a 3 : 1 : 1 split based on the number of distinct domains. We observed that the top-5 domains (based on number of samples) were Amazon, EBay, Walmart, Etsy, and Target. So, we created 5 different splits for 5-Fold Cross Validation such that each of the major domains is present in one of the 5 splits for test data."	https://paperswithcode.com/dataset/cova	24/10/2021	CoVA dataset for Webpage Object Detection / Information Extraction					
5021	Persian Reverse Dictionary Dataset	The Persian Reverse Dictionary Dataset is a collection of 855217 words along with the phrases describing them. The phrases were extracted from the top three most well-known Persian dictionaries (including Amid, Moeen, and Dehkhoda), Persian Wikipedia, and a Persian Wordnet (called Farsnet).	https://paperswithcode.com/dataset/persian-reverse-dictionary-dataset	01/05/2021						
5022	CADB	"To the best of our knowledge, there is no prior dataset specifically constructed for composition assessment. To support the research on this task, we build a dataset upon the existing AADB dataset, from which we collect a total of 9,958 real-world photos. We adopt a composition rating scale from 1 to 5, where a larger score indicates better composition. We make annotation guidelines for composition quality rating and train five individual raters who specialize in fine art. So for each image, we can obtain five composition scores ranging from 1 to 5. Given the subjective nature of human aesthetic activity, we perform sanity check and consistency analysis. We use 240 additional “sanity check” images during annotating to roughly verify the validness of our annotations. We also examine the consistency of composition ratings provided by five individual raters (see Supplementary). We average the composition scores as the ground-truth composition mean score for each image. More details about our CADB dataset will be elaborated in Supplementary. Besides, we observe the content bias in our CADB dataset, that is, there are some biased categories whose score distributions are concentrated in a very narrow interval. After removing 461 biased images, we split the remaining images into 8,547 training images and 950 test images, in which the test set is made less biased for better evaluation (see Supplementary).
Cited from ""Image Composition Assessment with Saliency-augmented Multi-pattern Pooling"" Zhang, Bo and Niu, Li and Zhang, Liqing
Citation
@article{zhang2021image,
  title={Image Composition Assessment with Saliency-augmented Multi-pattern Pooling},
  author={Zhang, Bo and Niu, Li and Zhang, Liqing},
  journal={arXiv preprint arXiv:2104.03133},
  year={2021}
}"	https://paperswithcode.com/dataset/cadb	07/04/2021	Composition Assessment DataBase					
5023	RWanda Built-up Region Segmentation	We create Rwanda built-up regions dataset, a different and versatile in nature from previously available datasets. The varying structure size and formation, irregular patterns of construction, buildings in forests and deserts, and the existence of mud houses make it very challenging. A total of 787 satellite images of size 256 × 256 are collected at a high resolution (HR) of 1.193 meters per pixel and hand tagged for built-up region segmentation using an online tool Label-Box.	https://paperswithcode.com/dataset/rwanda-built-up-region-segmentation	05/07/2020						
5024	Genome-wide miRNA detection	"We've made available several genome-wide datasets, which can be used for training microRNA (miRNA) classifiers. The hairpin sequences available are from the genomes of: Homo sapiens, Arabidopsis thaliana, Anopheles gambiae, Caenorhabditis elegans and Drosophila melanogaster. Hairpin.s are small RNA sequences that naturaly folds into a hairpin-structure. However, not all hairpins have clear function (they are not miRNAs).
Each dataset provides the genome data divided into sequences and a set of computed features for predictions. Each sequence has one label: i) “positive”: meaning that it is a well-known pre-miRNA, according to miRBase v21; or ii) “unlabeled”: indicating that the sequence has not (yet) a known function and could be a possible candidate to novel pre-miRNA. Due to the fact that selecting an informative feature set is very important for a good pre-miRNA classifier, a representative feature set with large discriminative power has been calculated and it is provided, as well, for each genome. This feature set contains typical information about sequence, topology and structure."	https://paperswithcode.com/dataset/genome-wide-mirna-detection	01/08/2019	Genome-wide hairpins datasets of animals and plants for novel miRNA prediction					
5025	GSM8K	"GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ − ×÷) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning.
Image source: https://arxiv.org/pdf/2110.14168v1.pdf"	https://paperswithcode.com/dataset/gsm8k	27/10/2021						
5026	Inter4K	A video dataset for benchmarking upsampling methods. Inter4K contains 1,000 ultra-high resolution videos with 60 frames per second (fps) from online resources. The dataset provides standardized video resolutions at ultra-high definition (UHD/4K), quad-high definition (QHD/2K), full-high definition (FHD/1080p), (standard) high definition (HD/720p), one quarter of full HD (qHD/520p) and one ninth of a full HD (nHD/360p). We use frame rates of 60, 50, 30, 24 and 15 fps for each resolution. Based on this standardization, both super-resolution and frame interpolation tests can be performed for different scaling sizes ($\times 2$, $\times 3$ and $\times 4$). In this paper, we use Inter4K to address frame upsampling and interpolation. Inter4K provides both standardized UHD resolution and 60 fps for all of videos by also containing a diverse set of 1,000 5-second videos. Differences between scenes originate from the equipment (e.g., professional 4K cameras or phones), lighting conditions, variations in movements, actions or objects. The dataset is divided into 800 videos for training, 100 videos for validation and 100 videos for testing.	https://paperswithcode.com/dataset/inter4k	01/11/2021						
5027	TUDA	"Overall duration per microphone: about 36 hours (31 hrs train / 2.5 hrs dev / 2.5 hrs test)
Count of microphones: 3 (Microsoft Kinect, Yamaha, Samson)
Count of wave-files per microphone: about 14500
Overall count of participations: 180 (130 male / 50 female)"	https://paperswithcode.com/dataset/tuda	11/12/2015						
5028	Market-1501-C	"Market-1501-C is an evaluation set that consists of algorithmically generated corruptions applied to the Market-1501 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."	https://paperswithcode.com/dataset/market-1501-c	01/11/2021	Market-1501-C					
5029	MSMT17-C	"MSMT17-C is an evaluation set that consists of algorithmically generated corruptions applied to the MSMT17 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."	https://paperswithcode.com/dataset/msmt17-c	01/11/2021	MSMT17-C					
5030	CUHK03-C	"CUHK03-C is an evaluation set that consists of algorithmically generated corruptions applied to the CUHK03 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."	https://paperswithcode.com/dataset/cuhk03-c	01/11/2021	CUHK03-C					
5031	SYSU-MM01-C	"SYSU-MM01-C is an evaluation set that consists of algorithmically generated corruptions applied to the SYSU-MM01 test-set.  These corruptions consist of Noise: Gaussian, shot,
impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."	https://paperswithcode.com/dataset/sysu-mm01-c	01/11/2021	SYSU-MM01-C					
5032	"Supporting data for ""Multi-Stage Malaria Parasites Recognition by Deep Learning"""	"Malaria, a mosquito-borne infectious disease affecting humans and other animals, is widespread in the tropical and subtropical regions. Microscopy is the most common method in diagnosing the malaria parasite from stained blood smears. However, this procedure is time-consuming, error-prone, and requires a well-trained professional. Moreover, the recognition of a malaria parasite through a microscope is still a challenging process, especially in distinguishing multiple stages of parasites.
Here is a large-scale dataset of unseen malaria parasites for a Multi-stage Malaria Recognition experiment. This includes test and training images of parasitized cells, test and training images of leukocytes, test and training images of gametocytes, test and training images of uninfected cells, test and training images of red blood cells, test and training images of ring cells, test and training images of  schizont cells, test and training images of trophozoite cells. 
Related P. vivax (malaria) infected human blood smear data is available in the BBBC repository and can be accessed with accession No. BBBC041 https://bbbc.broadinstitute.org/BBBC041
A related large scale malaria dataset consisting of 13,780 both malaria parasites and RBCs testing images are available in the National Library of Medicine (NLM) respository and can be accessed with accession No. PUB9932.  https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html"	https://paperswithcode.com/dataset/supporting-data-for-multi-stage-malaria	21/04/2020						
5033	Mouse Grooming Behavior	"This dataset was generated to characterize mouse grooming behavior.
Mouse grooming serves many adaptive functions such as coat and body care, stress reduction, de-arousal, social functions, thermoregulation, nociception, as well as other functions. Alteration of this behavior is measured and used for mouse pre-clinical models of human psychiatric illnesses.
Grooming behavior in mice contains a variety of visually diverse syntaxes including but not limited to paw licking, face washing, and flank licking.
Additionally, this dataset includes visually diverse mice including 157 individual mice spanning 60 different inbred and F1 hybrid mouse strains. This feature is a stark difference to most other mouse behavior datasets, which typically only include 1-2 inbred strains.
This dataset includes 1,253 video clips of mice behaving in an open field imaged from top-down perspective. Each video clip contains a 112x112 video tubelet cropped around the center of mass of the mouse as it walks around the open field arena.
Video clips are of variable length totaling 2,637,363 frames.
Annotators were required to provide a ""Grooming"" or ""Not Grooming"" annotation for each frame. Frames where annotators disagree is also provided."	https://paperswithcode.com/dataset/mouse-grooming-behavior	15/03/2021						
5034	PQ-decaNLP	Multitask learning has led to significant advances in Natural Language Processing, including the decaNLP benchmark where question answering is used to frame 10 natural language understanding tasks in a single model. PQ-decaNLP is a crowd-sourced corpus of paraphrased questions, annotated with paraphrase phenomena. This enables analysis of how transformations such as swapping the class labels and changing the sentence modality lead to a large performance degradation.	https://paperswithcode.com/dataset/pq-decanlp	27/10/2021	Paraphrase Questions - decaNLP					
5035	map2seq	7,672 human written natural language navigation instructions for routes in OpenStreetMap with a focus on visual landmarks. Validated in Street View.	https://paperswithcode.com/dataset/map2seq	30/12/2020						
5036	DrugProt	DrugProt corpus, where domain experts have exhaustively labeled:(a) all chemical and gene mentions, and (b) all binary relationships between them corresponding to a specific set of biologically relevant relation types (DrugProt relation classes).	https://paperswithcode.com/dataset/drugprot-1		DrugProt					
5037	RegDB-C	RegDB-C is an evaluation set that consists of algorithmically generated corruptions applied to the RegDB test-set (color images). These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions.	https://paperswithcode.com/dataset/regdb-c	01/11/2021	RegDB-C					
5038	Building air quality and pandemic risk simulation	"The original paper contains a high-level explanation of the dataset characteristics, and potential use cases of the dataset. ArchABM can help to quantify the impact of some of these building- and company policy-related measures. 
Baseline experiment
A baseline case with no measures and reduced ventilation is first studied. An schedule is set for each event type, along with their minimum and maximum duration $\tau$, and the number of repetitions. The mask is not used anywhere ($m_e = 0$). Meetings and the lunch activity are considered to be collective events. 
The places' capacity refers to the maximum number of people that can be present in that specified space. A low natural ventilation rate is established ($\lambda_a = 1.5$, and $\lambda_a = 0.5$ for poor ventilated rooms) and there is no mechanical ventilation ($\lambda_r = 0$). 
Building-related experiments


Larger building: each room's area (and thus each room's volume) is increased by 20%. This measure needs to take into account the increase in costs, which would mean an increase of almost 20% in the final construction costs as well.


Separate workspaces: the open office is divided into three identical offices, each one with 110 $m^2$, 16 people (48/3), and a capacity of 20 (60/3).


Better natural ventilation: windows are opened everywhere except in restrooms for better outdoor air supply. $\lambda_a$ is increased up to 5 $h^{-1}$.


Better mechanical ventilation: the flow rate $Q_{AC}$ of the AC system is incremented, assuming a 20% filter efficiency $\varepsilon_{filter}$, a 10% of removal in ducts $\varepsilon_{ducts}$ and no additional $\varepsilon_{extra}$ removal measures. Adding AC to the building would mean an increase of 14% in the building overall costs.


Policy-related experiments


Shifts between workers: this would imply a reduction in the number of people present in each room. For this experiment, the population is reduced by 40%, resulting in 29 people in the open office, 4 in the IT Office, and 1 in each chief office, summing up to 36 people. This measure also entails a non-quantifiable cost to the company. 


Limit duration of events: the duration of meetings is limited to a maximum of 30 minutes, setting $\tau = [0.\hat{3} - 0.5] h$. The duration of coffee breaks would be limited to 5 minutes, meaning $\tau = 0.08\hat{3} h$, and lunch would be of 20 minutes, $\tau = 0.\hat{3} h$.


Use of masks: in this case, the mask use is mandatory, meaning that $m_f = 1$ and the mask efficiency, $m_e$, is set to 0.75 in the offices and meeting rooms, to 0.5 in the restrooms, to 0.3 for coffee breaks and leaving it at 0 for lunch breaks, representing the absence of masks while eating."	https://paperswithcode.com/dataset/building-air-quality-and-pandemic-risk	02/11/2021						
5039	LSVTD	LSVTD is a large scale video text dataset for promoting the video text spotting community, which contains 100 text videos from 22 different real-life scenarios. LSVTD covers a wide range of 13 indoor (eg. bookstore, shopping mall) and 9 outdoor scenarios, which is more than 3 times the diversity of IC15.	https://paperswithcode.com/dataset/lsvtd	08/03/2019						
5040	DriverMHG	Driver Micro Hand Gestures (DriverMHG) is a dataset for dynamic recognition of driver micro hand gestures, which consists of RGB, depth and infrared modalities.	https://paperswithcode.com/dataset/drivermhg	02/03/2020						
5041	BCI Competition Datasets	"The goal of the ""BCI Competition"" is to validate signal processing and classification methods for Brain-Computer Interfaces (BCIs)."	https://paperswithcode.com/dataset/bci-competition-datasets	05/04/2018						
5042	UQuAD	Large scale machine reading comprehension dataset in Urdu language.	https://paperswithcode.com/dataset/uquad	02/11/2021	Urdu Question Answering Dataset					
5043	Adaptiope	Adaptiope is a domain adaptation dataset with 123 classes in the three domains synthetic, product and real life. One of the main goals of Adaptiope is to offer a clean and well curated set of images for domain adaptation. This was necessary as many other common datasets in the area suffer from label noise and low quality images. Additionally, Adaptiope's class set was chosen in a way that minimizes the overlap with the class set of the commonly used ImageNet pretraining, therefore preventing information leakage in a domain adaptation setup.	https://paperswithcode.com/dataset/adaptiope	05/01/2021						
5044	Modern Office-31	Modern Office-31 is a refurbished version of the commonly used Office-31 dataset. Modern Office-31 rectifies many of the annotation errors and low quality images in the Amazon domain of the original Office-31 dataset. Additionally, this dataset adds another synthetic domain based on the Adaptiope dataset.	https://paperswithcode.com/dataset/modern-office-31	05/01/2021						
5045	AVASpeech-SMAD	We propose a dataset, AVASpeech-SMAD, to assist speech and music activity detection research. With frame-level music labels, the proposed dataset extends the existing AVASpeech dataset, which originally consists of 45 hours of audio and speech activity labels. To the best of our knowledge, the proposed AVASpeech-SMAD is the first open-source dataset that features strong polyphonic labels for both music and speech. The dataset was manually annotated and verified via an iterative cross-checking process. A simple automatic examination was also implemented to further improve the quality of the labels. Evaluation results from two state-of-the-art SMAD systems are also provided as a benchmark for future reference.	https://paperswithcode.com/dataset/avaspeech-smad	09/11/2021	AVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection Dataset with Label Co-Occurrence					
5046	Ballroom	"This data set includes beat and bar annotations of the ballroom dataset, introduced by Gouyon et al. 1.
1 Gouyon F., A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, and P. Cano. An experimental comparison of audio tempo induction algorithms. Transactions on Audio, Speech and Language Processing 14(5), pp.1832-1844, 2006."	https://paperswithcode.com/dataset/ballroom							
5047	Beatles	"This dataset includes the beat and downbeat annotations for  Beatles albums. The annotations are provided by M. E. P. Davies et. al 1.
M. E. P. Davies, N. Degara, and M. D. Plumbley, “Evaluation methods for musical audio beat tracking algorithms,” in Technical Report C4DM-TR-09-06, Centre for Digital Music, Queen Mary University of London, 2009."	https://paperswithcode.com/dataset/beatles							
5048	Rock Corpus	"This dataset contains 200 famous songs in different genres (mostly in rock) and the beats and downbeat annotations are provided by  T. de Clercq and D. Temperley 1.
1 T. de Clercq and D. Temperley., “A corpus analysis of
rock harmony,” Popular Music, vol. 30, no. 1, pp. 47–
70, 2011."	https://paperswithcode.com/dataset/rock-corpus							
5049	Carnatic	"This dataset includes music time information i.e. Beat, Bar, and meter annotations of the Indian Carnatic music dataset. The dataset is gathered by A. Srinivasamurthy and X. Serra 1.
1 A. Srinivasamurthy and X. Serra, “A supervised approach to hierarchical metrical cycle tracking from audio music recordings,” in In Proc. of the IEEE Int. Conference on Acoustics, Speech, and Signal Processing
(ICASSP), 2014."	https://paperswithcode.com/dataset/carnatic							
5050	SINGA:PURA	"This repository contains the SINGA:PURA dataset, a strongly-labelled polyphonic urban sound dataset with spatiotemporal context. The data were collected via a number of recording units deployed across Singapore as a part of a wireless acoustic sensor network. These recordings were made as part of a project to identify and mitigate noise sources in Singapore, but also possess a wider applicability to sound event detection, classification, and localization. The taxonomy we used for the labels in this dataset has been designed to be compatible with other existing datasets for urban sound tagging while also able to capture sound events unique to the Singaporean context. Please refer to our conference paper published in APSIPA 2021 (which is found in this repository as the file ""APSIPA.pdf"") or download the readme (""Readme.md"") for more details regarding the data collection, annotation, and processing methodologies for the creation of the dataset."	https://paperswithcode.com/dataset/singa-pura	03/11/2021	SINGApore: Polyphonic URban Audio					
5051	ACAV100M	ACAV100M processes 140 million full-length videos (total duration 1,030 years) which are used to produce a dataset of 100 million 10-second clips (31 years) with high audio-visual correspondence. This is two orders of magnitude larger than the current largest video dataset used in the audio-visual learning literature, i.e., AudioSet (8 months), and twice as large as the largest video dataset in the literature, i.e., HowTo100M (15 years).	https://paperswithcode.com/dataset/acav100m	26/01/2021	Automatically Curated Audio-Visual					
5052	LAION-400M	"LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.
⚠️  Disclaimer & Content Warning (from the authors)
Our filtering protocol only removed NSFW images detected as illegal, but the dataset still has NSFW content accordingly marked in the metadata. When freely navigating through the dataset, keep in mind that it is a large-scale, non-curated set crawled from the internet for research purposes, such that collected links may lead to discomforting and disturbing content. Therefore, please use the demo links with caution. You can extract a “safe” subset by filtering out samples drawn with NSFW or via stricter CLIP filtering.
There is a certain degree of duplication because we used URL+text as deduplication criteria. The same image with the same caption may sit at different URLs, causing duplicates. The same image with other captions is not, however, considered duplicated.
Using KNN clustering should make it easy to further deduplicate by image content."	https://paperswithcode.com/dataset/laion-400m	03/11/2021						
5053	DeepNets-1M	"The DeepNets-1M dataset is composed of neural network architectures represented as graphs where nodes are operations (convolution, pooling, etc.) and edges correspond to the forward pass flow of data through the network.
DeepNets-1M has 1 million training architectures and 1402 in-distribution (ID) and out-of-distribution (OOD) evaluation architectures: 
500 validation and 500 testing ID architectures, 
100 wide OOD architectures, 
100 deep OOD architectures, 
100 dense OOD architectures, 
100 OOD archtectures without batch normalization, and 
2 predefined architectures (ResNet-50 and 12 layer Visual Transformer).
For 1402 evaluation architectures, DeepNets-1M includes accuracies of the networks on CIFAR-10 and ImageNet after training them with stochastic gradient descent (SGD).
Besides accuracy, other properties of evaluation architectures are included: accuracy on noisy images, inference and convergence time. These properties of architectures can enable training neural architecture search models.
The DeepNets-1M is used to train and evaluate parameter prediction models such as Graph HyperNetworks. These models can predict all parameters for a given network (graph) in a single forward pass and the results can be compared to optimizing parameters with SGD."	https://paperswithcode.com/dataset/deepnets-1m	25/10/2021						
5054	RLV	We provide video observations of humans performing two simple tasks  in natural environments.  The tasks are pushing and drawer opening.	https://paperswithcode.com/dataset/rlv	12/11/2020	Reinforcement Learning with Videos					
5055	Earth’s Mantle Convection	"The dataset, generated from a scientific simulation, consists of a time series (251 steps) of 3D scalar fields on a spherical 180x201x360 grid covering 500 Myr of geological time. Each time step is 2 Myrs, and the fields are:

temperature [degrees K],
three Cartesian velocity components [m/s],
thermal conductivity anomaly [Watt/m/K],
thermal expansivity anomaly [1/K],
temperature anomaly [degrees K], and
spin transition-induced density anomaly [kg/m^3].

The simulation was performed in double precision, however, to reduce downloading time, we provide the data in single precision. Each file was saved in a NetCDF Climate and Forecast (CF) convention format, with each 3D scalar field being a function of latitude [degrees north], longitude [degrees east], and radius [km]. The model’s inner and outer radii are 3485 km and 6371 km, respectively."	https://paperswithcode.com/dataset/earths-mantle-convection	28/10/2020						
5056	FEAFA+	FEAFA+ is a dataset for Facial expression analysis and 3D Facial animation. It includes 150 video sequences from FEAFA and DISFA, with a total of 230,184 frames being manually annotated on floating-point intensity value of 24 redefined AUs using the Expression Quantitative Tool.	https://paperswithcode.com/dataset/feafa	04/11/2021						
5057	GO21	GO21 is a biomedical knowledge graph that models genes, proteins, drugs, and the hierarchy of the biological processes they participate in. It consists of 806,136 triples with 21 relations and 89127 entities. GO21 can be used for knowledge graph completion tasks (link prediction) as well as hierarchical reasoning tasks, such as ancestor-descendant prediction task proposed in the paper.	https://paperswithcode.com/dataset/go21	28/10/2021						
5058	A Datacube for the analysis of wildfires in Greece	"This dataset is meant to be used to develop models for next-day fire hazard forecasting in Greece. It contains data from 2009 to 2020 at a 1km x 1km x 1 daily grid.
Check the Jupyter notebook for an example showing how to access the dataset."	https://paperswithcode.com/dataset/a-datacube-for-the-analysis-of-wildfires-in	24/06/2021						
5059	CLUES	CLUES (Constrained Language Understanding Evaluation Standard) is a benchmark for evaluating the few-shot learning capabilities of NLU models.	https://paperswithcode.com/dataset/clues	04/11/2021	Constrained Language Understanding Evaluation Standard					
5060	Only Time Will Tell	Simulation results of time-respecting and time-ignoring horizon of code review network at Microsoft as JSON. For further details, please look at https://github.com/michaeldorner/only-time-will-tell	https://paperswithcode.com/dataset/only-time-will-tell	14/10/2021	Time-respecting and time-ignoring horizon of code review network at Microsoft					
5061	LRA	"Long-range arena (LRA) is an effort toward systematic evaluation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning.
Description from: Long Range Arena : A Benchmark for Efficient Transformers"	https://paperswithcode.com/dataset/lra	08/11/2020	Long-Range Arena					
5062	AdvGLUE	"Adversarial GLUE (AdvGLUE) is a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations.
Description from: Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"	https://paperswithcode.com/dataset/advglue	04/11/2021	Adversarial GLUE					
5063	CoDEx Medium	"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. 
Source: CoDEx: A Comprehensive Knowledge Graph Completion Benchmark"	https://paperswithcode.com/dataset/codex-medium	16/09/2020						
5064	CoDEx Large	"CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. 
Source: CoDEx: A Comprehensive Knowledge Graph Completion Benchmark"	https://paperswithcode.com/dataset/codex-large	16/09/2020						
5065	IDDA	IDDA is a large scale, synthetic dataset for semantic segmentation with more than 100 different source visual domains. The dataset has been created to explicitly address the challenges of domain shift between training and test data in various weather and view point conditions, in seven different city types.	https://paperswithcode.com/dataset/idda	17/04/2020						
5066	SyRIP	SyRIP is a hybrid synthetic and real infant pose (SyRIP) dataset with small yet diverse real infant images as well as generated synthetic infant poses and (2) a multi-stage invariant representation learning strategy that could transfer the knowledge from the adjacent domains of adult poses and synthetic infant images into our fine-tuned domain-adapted infant pose	https://paperswithcode.com/dataset/syrip	13/10/2020						
5067	RobustBench	RobustBench is a benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models.	https://paperswithcode.com/dataset/robustbench	19/10/2020						
5068	WaveFake	WaveFake is a dataset for audio deepfake detection. The dataset consists of a large-scale dataset of over 100K generated audio clips.	https://paperswithcode.com/dataset/wavefake	04/11/2021						
5069	WWU DUNEuro reference data set	"The provided dataset consists of high-quality realistic head models and combined EEG/MEG data which can be used for state-of-the-art methods in brain research, such as modern finite element methods (FEM) to compute the EEG/MEG forward problems using the software toolbox DUNEuro (http://duneuro.org).
For further details see DOI: 10.5281/zenodo.3888380."	https://paperswithcode.com/dataset/wwu-duneuro-reference-data-set	09/01/2019	The WWU DUNEuro reference data set for combined EEG/MEG source analysis					
5070	VSLID	"VSLID stands for Very Small Lego Image Dataset. It has a bit over 1800 images of piles of LEGO bricks of 85 different types. There are between 1 and 10 bricks per image. Backgrounds and lighting conditions vary. All images are annotated with a list of the visible bricks. The images can have two resolutions, so rescaling them is recommended before usage.
There are three folders each containing two subfolders named Renders and Sets. The Renders subfolder contains the photos in PNG format, while the Sets subfolder contains a brick_sets.csv file recording just the identifiers of the bricks present in each image and a labels.csv file additionally recording the time of day the photo was taken at and its background."	https://paperswithcode.com/dataset/vslid	04/08/2020	Very Small Lego Image Dataset					
5071	WORD	WORD is a dataset for organ semantic segmentation that contains 150 abdominal CT volumes (30,495 slices) and each volume has 16 organs with fine pixel-level annotations and scribble-based sparse annotation, which may be the largest dataset with whole abdominal organs annotation.	https://paperswithcode.com/dataset/word	03/11/2021	Whole abdominal  Organs Dataset					
5072	REAL-M	Real-M is a crowd-sourced speech-separation corpus of real-life mixtures. The mixtures are recorded in different acoustic environments using a wide variety of recording devices such as laptops and smartphones, thus reflecting more closely potential application scenarios.	https://paperswithcode.com/dataset/real-m	20/10/2021						
5073	RIKEN Microstructural Imaging Metadatabase	The RIKEN Microstructural Imaging Metadatabase is a semantic web-based imaging database in which image metadata are described using the Resource Description Framework (RDF) and detailed biological properties observed in the images can be represented as Linked Open Data. The metadata are used to develop a large-scale imaging viewer that provides a straightforward graphical user interface to visualise a large microstructural tiling image at the gigabyte level.	https://paperswithcode.com/dataset/riken-microstructural-imaging-metadatabase	20/10/2021						
5074	BOBSL	"BOBSL is a large-scale dataset of British Sign Language (BSL). It comprises 1,962 episodes (approximately 1,400 hours) of BSL-interpreted BBC broadcast footage accompanied by written English subtitles. From horror, period and medical dramas, history, nature and science documentaries, sitcoms, children’s shows and programs covering cooking, beauty, business and travel, BOBSL covers a wide range of topics. The dataset features a total of 39 signers. Distinct signers appear in the training, validation and test sets for signer-independent evaluation.
Description from: BOBSL: BBC-Oxford British Sign Language Dataset"	https://paperswithcode.com/dataset/bobsl	05/11/2021	BBC-Oxford British Sign Language					
5075	AdobeVFR syn	"Subset of AdobeVFR. The dataset contains images depicting English text and consists of 1000 synthetic images for training and 100 for testing, for each of 2383 font classes. The training and test sets are called VFR_syn_train and VFR_syn_val, respectively.
The other part of AdobeVFR consists of ""real-world text images""."	https://paperswithcode.com/dataset/adobevfr	12/07/2015	Adobe Visual Font Recognition synthetic dataset					
5076	Explor_all	Explor_all font image dataset https://drive.google.com/file/d/1P2DbNbVw4Q__WcV1YdzE7zsDKilmd3pO/view	https://paperswithcode.com/dataset/explor-all	21/10/2021	Explor_all					
5077	SDSS Galaxies	"This is a dataset of 306,006 galaxies whose coordinates are taken from the 
Sloan Digital Sky Survey Data Release 7 and a modified catalogue from 
Brinchmann+2003 and Wilman+2010. This volume complete sample has 
an r-band absolute magnitude limit of $M_r\leq-20$ and a redshift limit of 
$z\leq0.08$. See Arora+2019 for details.
This catalogue covers a wide range of environments from clusters to groups 
and field systems. The galaxy images are taken from the Dark Energy 
Spectroscopic Instrument, and contain $g$, $r$, and $z$ bands."	https://paperswithcode.com/dataset/sdss-galaxies	02/11/2021	SDSS galaxies as imaged by DESI					
5078	VFR-447	"A synthetic dataset containing 447 typefaces with only one font variation for each typeface, created for visual font recognition.

Each class in VFR-447 and VFR-2420 has 1,000 synthetic word images, which are evenly split into 500 training and 500 testing. There are no common words between the training and testing images.
To model the realistic use cases, we add moderate distortions and noise to the synthetic data."	https://paperswithcode.com/dataset/vfr-447	01/06/2014						
5079	VFR-2420	"A synthetic dataset containing word images of 447 typefaces with font variations for each typeface, created for visual font recognition.

We collect in total 447 typefaces, each with different number of variations resulting from combinations of different styles, e.g., regular, semibold, bold, black, and italic, leading to 2,420 font classes in the end.
Each class in VFR-447 and VFR-2420 has 1,000 synthetic word images, which are evenly split into 500 training and 500 testing. There are no common words between the training and testing images.
To model the realistic use cases, we add moderate distortions and noise to the synthetic data."	https://paperswithcode.com/dataset/vfr-2420	01/06/2014						
5080	VFR-Wild	"325 word images intended for font recognition, whose fonts are included in VFR-447 (and VFR-2420).

(...) 325 real world test images for the font classes we have in the training set. These images were collected from typography forums, such as myfonts.com, where people post these images seeking help from experts to identify the fonts. Compared with the synthetic data, these images typically have much larger appearance variations caused by scale, background, lighting, noise, perspective distortions, and compression artifacts. We manually cropped the texts from these images with a bounding box to normalize the text size approximately to the same scale as the synthetic data."	https://paperswithcode.com/dataset/vfr-wild	01/06/2014						
5081	AdobeVFR real	"Subset of AdobeVFR. The dataset contains ""real-world text images"".

We collected 201,780 text images from various typography forums, where people post these images seeking help from experts to identify the fonts. Most of them come with hand-annotated font labels which may be inaccurate. (...) Finally, we obtain 4,384 real-world test images with reliable labels, covering 617 classes (out of 2,383). (...) Removing the 4,384 labeled images from the full set, we are left with 197,396 unlabeled real- world images which we denote as VFR_real_u.

The labeled images form VFR_real_test. The other part of AdobeVFR consists of synthetic data (with 2383 classes)."	https://paperswithcode.com/dataset/adobevfr-real	12/07/2015	Adobe Visual Font Recognition real-world images dataset					
5082	Federated Stack Overflow	This dataset is derived from the Stack Overflow Data hosted by kaggle.com and available to query through Kernels using the BigQuery API: https://www.kaggle.com/stackoverflow/stackoverflow	https://paperswithcode.com/dataset/federated-stack-overflow							
5083	BPCIS	"BPCIS is collection of 364 bacterial phase contrast images and corresponding label matrices for instance segmentation. Labels were made according to fluorescence channels where possible. Prior to manual annotation, images were automatically cropped into microcolonies and tiled into ensemble images to reduce the empty (non-cell) image regions for training and testing. Subsequent to annotation, we performed non-rigid registration of phase contrast to cell masks. 
Species include Escherichia coli, Shigella flexneri, Francisella tularensis subsp. novicida, Acinetobacter baylyi, Burkholderia thailandensis, Helicobacter pylori, Caulobacter crescentus , Streptomyces  pristinaespiralis, Vibrio cholerae, Serratia  proteamaculans, Pseudomonas aeruginosa, Staphylococcus aureus, and Bacillus subtilis. E. coli mutant CS703-1 and H. pylori were treated with Aztreonam. Included are independent treatments of S. flexneri  with cephalexin and A22. Also included are mixtures of E. coli and S. protreamaculans, mixtures of P. aeruginosa and S. aureus, and P. aeruginosa, S. aureus, V. cholerae, and B. subtilis. 
This dataset represents a wide range of morphological and optical phenotypes both common and uncommon to bacterial microscopy. All manual annotaton was performed by Kevin J. Cutler. Micrographs were captured by Kevin J. Cutler, Teresa Lo, Paul A. Wiggins, and Maxime Jacq."	https://paperswithcode.com/dataset/bpcis	05/11/2021	Bacterial Phase Contrast for Instance Segementation					
5084	Audio demo files	"Audio files that supplement ""Treatise on Hearing: The Temporal Auditory Imaging Theory Inspired by Optics and Communication""."	https://paperswithcode.com/dataset/audio-demo-files	08/11/2021						
5085	SustainBench	"SustainBench is a collection of 15 benchmark tasks across 7 sustainable development goals (SDGs), including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land. The goals for SustainBench are to:

lower the barriers to entry for the machine learning community to contribute to measuring and achieving the SDGs;
provide standard benchmarks for evaluating machine learning models on tasks across a variety of SDGs; and
encourage the development of novel machine learning methods where improved model performance facilitates progress towards the SDGs."	https://paperswithcode.com/dataset/sustainbench	08/11/2021						
5086	HC18	Automated measurement of fetal head circumference using 2D ultrasound images	https://paperswithcode.com/dataset/hc18							
5087	BCSS	The BCSS dataset contains over 20,000 segmentation annotations of tissue regions from breast cancer images from The Cancer Genome Atlas (TCGA). This large-scale dataset was annotated through the collaborative effort of pathologists, pathology residents, and medical students using the Digital Slide Archive.  It enables the generation of highly accurate machine-learning models for tissue segmentation.	https://paperswithcode.com/dataset/bcss	01/09/2019	Breast Cancer Semantic Segmentation					
5088	unarXive	"A scholarly data set with publications’ full-text, annotated in-text citations, and links to metadata.
The unarXive data set contains

One million papers in plain text
63 million citation contexts
39 million reference strings
A citation network of 16 million connections

The data is generated from all LaTeX sources on arXiv from 1991–2020/07 and therefore of higher quality than data generated from PDF files.
Furthermore, as all citing papers are available in full text, citation contexts of arbitrary size can be extracted.
Typical uses of the data set are approaches in

Citation recommendation
Citation context analysis
Reference string parsing

The code for generating the data set is publicly available."	https://paperswithcode.com/dataset/unarxive	02/03/2020						
5089	Next2You data and results dataset	"This record serves as an index to the other dataset releases that are part of the paper ""Next2You: Robust Copresence Detection Based on Channel State Information"" by Mikhail Fomichev, Luis F. Abanto-Leon, Max Stiegler, Alejandro Molina, Jakob Link, Matthias Hollick, in ACM Transactions on Internet of Things (2021)."	https://paperswithcode.com/dataset/next2you-data-and-results-dataset	09/11/2021	"Index of Supplementary Files from ""Next2You: Robust Copresence Detection Based on Channel State Information"""					
5090	GRB	"Graph Robustness Benchmark (GRB) provides scalable, unified, modular, and reproducible evaluation on the adversarial robustness of graph machine learning models. GRB has elaborated datasets, unified evaluation pipeline, modular coding framework, and reproducible leaderboards, which facilitate the developments of graph adversarial learning, summarizing existing progress and generating insights into future research.
GitHub: https://github.com/thudm/grb"	https://paperswithcode.com/dataset/grb	08/11/2021	Graph Robustness Benchmark					
5091	NAO	Natural Adversarial Objects (NAO) is a new dataset to evaluate the robustness of object detection models. NAO contains 7,934 images and 9,943 objects that are unmodified and representative of real-world scenarios, but cause state-of-the-art detection models to misclassify with high confidence.	https://paperswithcode.com/dataset/nao	07/11/2021	Natural Adversarial Object					
5092	Retinal-Lesions	"Over 1.5K images selected from the public Kaggle DR Detection dataset;
Five DR grades (DR0 / DR1 / DR2 / DR3 / DR4), re-labeled by a panel of 45 experienced ophthalmologists;
Eight retinal lesion classes, including microaneurysm, intraretinal hemorrhage, hard exudate, cotton-wool spot, vitreous hemorrhage, preretinal hemorrhage, neovascularization and fibrous proliferation;
Over 34K expert-labeled pixel-level lesion segments;
Multi-task, i.e., lesion segmentation, lesion classification, and DR grading."	https://paperswithcode.com/dataset/retinal-lesions	25/12/2019						
5093	DSurVD	"A large-scale dataset, namely Distorted Surveillance Video Database (DSurVD), which can be downloaded from the link: https://sites.google.com/site/sorsyuanyuan/home/dsurvd
Image source: https://sites.google.com/site/sorsyuanyuan/home/dsurvd"	https://paperswithcode.com/dataset/dsurvd		Distorted Surveillance Video Database					
5094	WildReceipt	"WildReceipt is a collection  of receipts. 
It contains, for each photo, of a list of OCRs - with bounding box, text, and class. 
It contains 1765 photos, with 25 classes, and 50000 text boxes. 
The goal is to benchmark ""key information extraction"" - extracting key information from documents. There are two different modalities - text and visual features - which is an interesting problem. 
Potential uses - extracting information from documents.
The dataset is pending release."	https://paperswithcode.com/dataset/wildreceipt	26/03/2021						
5095	ParsTwiner	An open, broad-coverage corpus for informal Persian named entity recognition was collected from Twitter.	https://paperswithcode.com/dataset/parstwiner	01/11/2021						
5096	ESC50	"The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.
The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories.
Reference: https://dl.acm.org/doi/10.1145/2733373.2806390"	https://paperswithcode.com/dataset/esc50		ESC: Dataset for Environmental Sound Classification					
5097	Kinetics-Sound	This is a subset of Kinetics-400, introduced in Look, Listen and Learn by Relja Arandjelovic and Andrew Zisserman.	https://paperswithcode.com/dataset/kinetics-sound							
5098	PhysioNet Challenge 2018	"Data for this challenge were contributed by the Massachusetts General Hospital’s (MGH) Computational Clinical Neurophysiology Laboratory (CCNL), and the Clinical Data Animation Laboratory (CDAC). The dataset includes 1,985 subjects which were monitored at an MGH sleep laboratory for the diagnosis of sleep disorders. The data were partitioned into balanced training (n = 994), and test sets (n = 989).
The sleep stages of the subjects were annotated by clinical staff at the MGH according to the American Academy of Sleep Medicine (AASM) manual for the scoring of sleep. More specifically, the following six sleep stages were annotated in 30 second contiguous intervals: wakefulness, stage 1, stage 2, stage 3, rapid eye movement (REM), and undefined.
Certified sleep technologists at the MGH also annotated waveforms for the presence of arousals that interrupted the sleep of the subjects. The annotated arousals were classified as either: spontaneous arousals, respiratory effort related arousals (RERA), bruxisms, hypoventilations, hypopneas, apneas (central, obstructive and mixed), vocalizations, snores, periodic leg movements, Cheyne-Stokes breathing or partial airway obstructions.
The subjects had a variety of physiological signals recorded as they slept through the night including: electroencephalography (EEG), electrooculography (EOG), electromyography (EMG), electrocardiology (EKG), and oxygen saturation (SaO2). Excluding SaO2, all signals were sampled to 200 Hz and were measured in microvolts. For analytic convenience, SaO2 was resampled to 200 Hz, and is measured as a percentage."	https://paperswithcode.com/dataset/physionet-challenge-2018		You Snooze You Win - The PhysioNet Computing in Cardiology Challenge 2018					
5099	MoviePlotEvents	"A version of the CMU Movie Summary Corpus (http://www.cs.cmu.edu/~ark/personas/), which was originally scraped from plot summaries from Wikipedia, with some cleaning and sentences turned into events & sorted into ""genres"" (via LDA)."	https://paperswithcode.com/dataset/movieplotevents	05/06/2017	CMU Movie Summary Corpus with Events					
5100	CMU Movie Summary Corpus	"Dataset [46 M] and readme: 42,306 movie plot summaries extracted from Wikipedia + aligned metadata extracted from Freebase, including:
Movie box office revenue, genre, release date, runtime, and language
Character names and aligned information about the actors who portray them, including gender and estimated age at the time of the movie's release
Supplement: Stanford CoreNLP-processed summaries [628 M]. All of the plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref)."	https://paperswithcode.com/dataset/cmu-movie-summary-corpus	04/08/2013						
5101	Scifi TV Shows	"A collection of long-running (80+ episodes) science fiction TV show synopses, scraped from Fandom.com wikis. Collected Nov 2017. Each episode is considered a ""story"".
Contains plot summaries from :

Babylon 5 (https://babylon5.fandom.com/wiki/Main_Page) - 84 stories
Doctor Who (https://tardis.fandom.com/wiki/Doctor_Who_Wiki) - 311 stories
Doctor Who spin-offs - 95 stories
Farscape (https://farscape.fandom.com/wiki/Farscape_Encyclopedia_Project:Main_Page) - 90 stories
Fringe (https://fringe.fandom.com/wiki/FringeWiki) - 87 stories
Futurama (https://futurama.fandom.com/wiki/Futurama_Wiki) - 87 stories
Stargate (https://stargate.fandom.com/wiki/Stargate_Wiki) - 351 stories
Star Trek (https://memory-alpha.fandom.com/wiki/Star_Trek) - 701 stories
Star Wars books (https://starwars.fandom.com/wiki/Main_Page) - 205 stories
Star Wars Rebels - 65 stories
X-Files (https://x-files.fandom.com/wiki/Main_Page) - 200 stories

Total: 2276 stories
Dataset is ""eventified"" and generalized (see LJ Martin, P Ammanabrolu, X Wang, W Hancock, S Singh, B Harrison, and MO Riedl. Event Representations for Automated Story Generation with Deep Neural Nets, Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), 2018. for details on these processes.) and split into train-test-validation sets for converting events into full sentences."	https://paperswithcode.com/dataset/scifi-tv-plots	08/09/2019	Scifi TV Show Plot Summaries & Events					
5102	Embrapa ADD 256	"This is a detailed description of the dataset, a data sheet for the dataset as proposed by Gebru et al.
Motivation for Dataset Creation
Why was the dataset created?
Embrapa ADD 256 (Apples by Drones Detection Dataset — 256 × 256) was created
to provide images and annotation for research on *apple detection in orchards for UAV-based monitoring in apple production. 
What (other) tasks could the dataset be used for?
Apple detection in low-resolution scenarios, similar to the aerial
images employed here.
Who funded the creation of the dataset?
The building of the ADD256 dataset was supported by the Embrapa SEG
Project 01.14.09.001.05.04, Image-based metrology for Precision
Agriculture and Phenotyping, and FAPESP under 
grant (2017/19282-7).
Dataset Composition
What are the instances?
Each instance consists of an RGB image and an annotation describing apples 
locations as circular markers (i.e., presenting center and radius).
How many instances of each type are there?
The dataset consists of 1,139 images containing 2,471 apples. 
What data does each instance consist of?
Each instance contains an 8-bits RGB image. Its corresponding annotation
is found in the JSON files: each apple marker is composed by its center (cx, cy)
and its radius (in pixels), as seen below:
""gebler-003-06.jpg"": [
  {
    ""cx"": 116,
    ""cy"": 117,
    ""r"": 10
  },
  {
    ""cx"": 134,
    ""cy"": 113,
    ""r"": 10
  },
  {
    ""cx"": 221,
    ""cy"": 95,
    ""r"": 11
  },
  {
    ""cx"": 206,
    ""cy"": 61,
    ""r"": 11
  },
  {
    ""cx"": 92,
    ""cy"": 1,
    ""r"": 10
  }
],

Dataset.ipynb is a Jupyter Notebook presenting a code example for reading 
the data as a PyTorch's Dataset (it should be straightforward to adapt the code
for other frameworks as Keras/TensorFlow, fastai/PyTorch, Scikit-learn, etc.)
Is everything included or does the data rely on external resources?
Everything is included in the dataset.
Are there recommended data splits or evaluation measures?
The dataset comes with specified train/test splits. The splits are found
in lists stored as JSON files.
|         |  Number of images  | Number of annotated apples |
  | ---     | ---                | ---                        |
  |Training | 1,025              |  2,204                     | 
  |Test     |   114              |    267                     |
  |Total    | 1,139              |  2,471                     |             
Dataset recommended split.
Standard measures from the information retrieval and computer vision
literature should be employed: precision and recall, F1-score and
average precision as seen in COCO
and Pascal VOC.
What experiments were initially run on this dataset?
The first experiments run on this dataset are described in A methodology for detection 
and location of fruits in apples orchards from aerial images
by Santos & Gebler (2021).
Data Collection Process
How was the data collected?
The data employed in the development of the methodology came from two plots located
at the Embrapa’s Temperate Climate Fruit Growing Experimental Station at Vacaria-RS
(28°30’58.2”S, 50°52’52.2”W). Plants of 
the varieties Fuji and Gala are present in the dataset, in equal proportions. The images were 
taken during December 13, 2018, by an UAV (DJI Phantom 4 Pro) that flew over the rows of 
the field at a height of 12 m. The images mix nadir and non-nadir views, allowing a more extensive view of the 
canopies. A subset from the images was random selected and 256 × 256 pixels patches were extracted.
Who was involved in the data collection process?
T. T. Santos and L. Gebler captured the images in
field. T. T. Santos performed the annotation.
How was the data associated with each instance acquired?
The circular markers were annotated using the VGG Image Annotator (VIA).
WARNING: Find non-ripe apples in low-resolution images of orchards is a challenging task even for humans. ADD256 was annotated by a single 
annotator. So, users of this dataset should consider it a noisy dataset. 
Data Preprocessing
What preprocessing/cleaning was done?
No preprocessing was applied.
Dataset Distribution
How is the dataset distributed?
The dataset is available at GitHub.
When will the dataset be released/first distributed?
The dataset was released in October 2021.
What license (if any) is it distributed under?
The data is released under Creative Commons BY-NC 4.0 (Attribution-NonCommercial 4.0 International license). 
There is a request to cite the corresponding paper if the dataset is used. For
commercial use, contact Embrapa Agricultural Informatics business office.
Are there any fees or access/export restrictions?
There are no fees or restrictions. For commercial use, contact Embrapa
Agricultural Informatics business office.
Dataset Maintenance
Who is supporting/hosting/maintaining the dataset?
The dataset is hosted at Embrapa Agricultural Informatics and all
comments or requests can be sent to Thiago T. Santos
(maintainer).
Will the dataset be updated?
There is no scheduled updates. 
If others want to extend/augment/build on this dataset, is there a mechanism for them to do so?
Contributors should contact the maintainer by e-mail.
No warranty
The maintainers and their institutions are exempt from any liability,
judicial or extrajudicial, for any losses or damages arising from the
use of the data contained in the image database."	https://paperswithcode.com/dataset/embrapa-add-256	20/10/2021	Embrapa Apples by Drones Detection Dataset					
5103	Cryptics	"Official dataset of Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP.
See github.com/jsrozner/decrypt and https://doi.org/10.5061/dryad.n02v6wwzp"	https://paperswithcode.com/dataset/cryptics	17/04/2021						
5104	mini-ImageNet-LT	mini-ImageNet was proposed by Matching networks for one-shot learning for few-shot learning evaluation, in an attempt to have a dataset like ImageNet while requiring fewer resources. Similar to the statistics for CIFAR-100-LT with an imbalance factor of 100, we construct a long-tailed variant of mini-ImageNet that features all the 100 classes and an imbalanced training set with $N_1 = 500$ and $N_K = 5$ images. For evaluation, both the validation and test sets are balanced and contain 10K images, 100 samples for each of the 100 categories.	https://paperswithcode.com/dataset/mini-imagenet-lt	10/11/2021						
5105	SentiMix	Sentiment analysis of codemixed tweets.	https://paperswithcode.com/dataset/sentimix	10/08/2020						
5106	fNIRS2MW	"The Tufts fNIRS to Mental Workload (fNIRS2MW) open-access dataset is a new dataset for building machine learning classifiers that can consume a short window (30 seconds) of multivariate fNIRS recordings and predict the mental workload intensity of the user during that window.
You can use this dataset for tasks like

time series classification using sliding windows
domain adaptation or domain generalization (how well does your classifier generalize to a new subject?)
fairness of time series classifiers (does performance of your classifier vary by subject race or gender?)

Useful Links:

Project Website (and data download links): https://tufts-hci-lab.github.io/code_and_datasets/fNIRS2MW.html
Code for benchmarks: https://github.com/tufts-ml/fNIRS-mental-workload-classifiers
DataSheet documentation: https://github.com/tufts-ml/fNIRS-mental-workload-classifiers/blob/main/Datasheet-Tufts-fNIRS2MW.pdf
Academic Paper (published at NeurIPS Datasets & Benchmarks '21) https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/bd686fd640be98efaae0091fa301e613-Paper-round2.pdf

Motivation
We are interested in building brain computer interfaces (BCIs) that would help out everyday computer users working at a desktop or laptop. In our target future use case, a user would actively use a keyboard and mouse as usual, but also wear a non-intrusive headband sensor that would passively provide real-time measurements of brain activity to the computer. Based on moment-to-moment estimates of mental workload, the computer could adjust the interface to support the user.
Functional near-infrared spectroscopy (fNIRS) is a promising sensor technology for achieving this goal of ""everyday BCI"", compared to alternatives like EEG or fMRI. We have developed a prototype fNIRS probe mounted on a headband that we used to collect this dataset (see our paper for details).
Dataset Overview
For a complete dataset summary, see our public DataSheet PDF
For each participant (68 recommended; 87 total), the dataset contains the following records obtained during one 30-60 minute experimental session. Each subject contributes just over 21 minutes of fNIRS data from the desired n-back experimental conditions, with remaining time related to rest or instruction periods.


fNIRS recordings

Multivariate (D=8) time-series representing brain activity throughout the session, recorded by a sensor probe placed on the forehead and secured via headband
All measurements are recorded at a regular sampling rate of 5.2 Hz.
At each timestep, we record 8 real-valued measurements, one for each combination of
2 blood chemical concentration changes (oxygenated hemoglobin and deoxygenated hemoglobin)
2 optical data types used for the measurement (intensity and phase)
2 spatial locations on the forehead.


The units of each measurement are micro-moles of (oxy-/deoxy-)hemoglobin per liter of tissue.



Activity labels

Annotations of the experimental task activity the subject performed throughout the session, including instruction, rest, and active experiment segments.
We label each segment of the active experiment as one of four possible n-back working memory intensity levels (0-back, 1-back, 2-back, or 3-back). Increased intensity levels are intended to induce an increased level of cognitive workload.
For all experiments reported in the paper, we focus on a binary task (0 vs 2 back)



Demographics

The participant’s age, gender, race, handedness, and other attributes. This lets us measure and audit performance by subpopulation (e.g. how does the classifier perform on white subjects vs. black subjects).



Publications
The Tufts fNIRS Mental Workload Dataset &amp; Benchmark for Brain-Computer Interfaces that Generalize
Zhe Huang, Liang Wang, Giles Blaney, Christopher Slaughter, Devon McKeon, Ziyu Zhou, Robert Jacob, and Michael C. Hughes
To appear in the Proceedings of Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks , 2021

Link to Paper PDF: <a href=""https://openreview.net/pdf?id=QzNHE7QHhut"">https://openreview.net/pdf?id=QzNHE7QHhut</a>"	https://paperswithcode.com/dataset/fnirs2mw	01/11/2021	The Tufts fNIRS to Mental Workload Dataset					
5107	Multilingual Terms of Service	The first annotated corpus for multilingual analysis of potentially unfair clauses in online Terms of Service. The data set comprises a total of 100 contracts, obtained from 25 documents annotated in four different languages: English, German, Italian, and Polish. For each contract, potentially unfair clauses for the consumer are annotated, for nine different unfairness categories.	https://paperswithcode.com/dataset/multilingual-terms-of-service	10/11/2021	Multilingual Terms of Service					
5108	"Archival bundle of the data used for ""Predictive Auto-scaling with OpenStack Monasca"" (UCC 2021)"	"Follow the instructions provided in the companion repo to automatically download and decompress the archive. The following files are included:
| File                                                    | Description                                                    |
| :------------------------------------------------------ | :------------------------------------------------------------- |
| amphora-x64-haproxy.qcow2                             | Image used to create Octavia amphorae                          |
| distwalk-{lin,mlp,rnn,stc}-&lt;INCREMENTAL-ID&gt;.log       | distwalk run log                                             |
| distwalk-{lin,mlp,rnn,stc}-&lt;INCREMENTAL-ID&gt;-pred.json | Predictive metric data exported from Monasca DB                |
| distwalk-{lin,mlp,rnn,stc}-&lt;INCREMENTAL-ID&gt;-real.json | Actual metric data exported from Monasca DB                    |
| distwalk-{lin,mlp,rnn,stc}-&lt;INCREMENTAL-ID&gt;-times.csv | Client-side response time for each request sent during a run   |
| model_dumps/*                                         | Dumps of the models and data scalers used for the validation   |
| predictor.log                                         | monasca-predictor log                                        |
| predictor-times.log                                   | monasca-predictor log (timing info only)                     |
| predictor-times-{lin,mlp,rnn}.{csv,log}               | monasca-predictor log (timing info only, group by predictor) |
| super_steep_behavior.csv                              | Dataset used to train MLP and RNN models                       |
| test_behavior_02_distwalk-6t_last100.dat              | distwalk load trace                                          |
| ubuntu-20.04-min-distwalk.img                         | Image used to create Nova instances for the scaling group      |"	https://paperswithcode.com/dataset/archival-bundle-of-the-data-used-for	03/11/2021						
5109	MONK's Problems	There are three MONK's problems.  The domains for all MONK's problems are the same (described below).  One of the MONK's problems has noise added. For each problem, the domain has been partitioned into a train and test set.	https://paperswithcode.com/dataset/monk-s-problems							
5110	KDD Cup 1999	This is the data set used for The Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between bad'' connections, called intrusions or attacks, andgood'' normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment.	https://paperswithcode.com/dataset/kdd-cup-1999-data-data-set	01/01/1999						
5111	Arcene	ARCENE was obtained by merging three mass-spectrometry datasets to obtain enough training and test data for a benchmark. The original features indicate the abundance of proteins in human sera having a given mass value. Based on those features one must separate cancer patients from healthy patients. We added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized.	https://paperswithcode.com/dataset/arcene	29/02/2008						
5112	DukeMTMC-VideoReID	"The DukeMTMC-VideoReID (Duke Multi-Tracking Multi-Camera Video-based ReIDentification) dataset is a subset of the DukeMTMC for video-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian video datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 4832 tracklets of 1812 identities in total, and each tracklet has 168 frames on average.
NOTE: This dataset has been retracted.
Source: Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning"	https://paperswithcode.com/dataset/dukemtmc-videoreid	01/06/2018						
5113	MTOP	A multilingual task-oriented semantic parsing dataset covering 6 languages and 11 domains.	https://paperswithcode.com/dataset/mtop		Multilingual Task-Oriented Semantic Parsing					
5114	Emotional Dialogue Acts	"Emotional Dialogue Acts data contains dialogue act labels for existing emotion multi-modal conversational datasets.
We chose two popular multimodal emotion datasets: Multimodal EmotionLines Dataset (MELD) and Interactive Emotional dyadic MOtion CAPture database (IEMOCAP). 
EDAs reveal associations between dialogue acts and emotional states in a natural-conversational language such as Accept/Agree dialogue acts often occur with the Joy emotion, Apology with Sadness, and Thanking with Joy."	https://paperswithcode.com/dataset/emotional-dialogue-acts	11/05/2020						
5115	Santesteban VTO	Physics-based simulated garments on top of SMPL bodies. The data is generated used a modified version of ARCSim and sequences from the CMU Motion Capture Database converted to SMPL format in SURREAL. Each simulated sequence is stored as a .pkl file that contains the following data:	https://paperswithcode.com/dataset/santesteban-vto							
5116	Lemons quality control dataset	Lemon dataset has been prepared to investigate the possibilities to tackle the issue of fruit quality control. It contains 2690 annotated images (1056 x 1056 pixels). Raw lemon images have been captured using the procedure described in the following blogpost and manually annotated using CVAT.	https://paperswithcode.com/dataset/lemons-quality-control-dataset	29/07/2020						
5117	Douban Conversation Corpus	"We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. 
|      |Train|Val| Test         | 
| ------------- |:-------------:|:-------------:|:-------------:|
| session-response pairs  | 1m|50k| 10k |
| Avg. positive response per session     | 1|1| 1.18    | 
| Fless Kappa | N\A|N\A|0.41      | 
| Min turn per session | 3|3| 3      | 
| Max ture per session | 98|91|45    | 
| Average turn per session | 6.69|6.75|5.95    | 
| Average Word per utterance | 18.56|18.50|20.74   | 
The test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.
<br>
As far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0
Data template
label \t conversation utterances (splited by \t) \t response"	https://paperswithcode.com/dataset/douban-conversation-corpus	06/12/2016	Douban Conversation Corpus					
5118	E-commerce	"We release E-commerce Dialogue Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of E-commerical Conversation Corpus are shown in the following table. 
|      |Train|Val| Test         |
| ------------- |:-------------:|:-------------:|:-------------:|
| Session-response pairs  | 1m|10k| 10k |
| Avg. positive response per session|1|1|1|
| Min turn per session|3|3|3|
| Max ture per session|10|10|10|
| Average turn per session|5.51|5.48|5.64
| Average Word per utterance|7.02|6.99|7.11
The full corpus can be downloaded from https://drive.google.com/file/d/154J-neBo20ABtSmJDvm7DK0eTuieAuvw/view?usp=sharing."	https://paperswithcode.com/dataset/e-commerce-1	24/06/2018	E-commerce					
5119	RRS	"|           | Train | Validation | Test    | Ranking Test |
| --------- | ----- | ---------- | ------- | ------------ |
| size      | 0.4M  | 50K        | 5K      | 800          |
| pos:neg   | 1:1   | 1:9        | 1.2:8.8 | -            |
| avg turns | 5.0   | 5.0        | 5.0     | 5.0          |
Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper."	https://paperswithcode.com/dataset/rrs	13/10/2021	Restoration-200k for Response Selection					
5120	RRS Ranking Test	"|           | Train | Validation | Test    | Ranking Test |
| --------- | ----- | ---------- | ------- | ------------ |
| size      | 0.4M  | 50K        | 5K      | 800          |
| pos:neg   | 1:1   | 1:9        | 1.2:8.8 | -            |
| avg turns | 5.0   | 5.0        | 5.0     | 5.0          |
Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper."	https://paperswithcode.com/dataset/rrs-ranking-test	13/10/2021	Restoration-200k for Response Selection with Ranking Test Set					
5121	Duolingo STAPLE Shared Task	This is the dataset for the 2020 Duolingo shared task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Sentence prompts, along with automatic translations, and high-coverage sets of translation paraphrases weighted by user response are provided in 5 language pairs. Starter code for this task can be found here: github.com/duolingo/duolingo-sharedtask-2020/. More details on the data set and task are available at: sharedtask.duolingo.com	https://paperswithcode.com/dataset/duolingo-staple-shared-task	01/07/2020						
5122	Duolingo Bandit Notifications	Replication datasets (200 million rows) used in experiments by Yancey & Settles (2020). (2019-06-11)	https://paperswithcode.com/dataset/duolingo-notifications-data	23/08/2020						
5123	Duolingo SLAM Shared Task	This repository contains gzipped files containing more than 2 million tokens (words) from answers submitted by more than 6,000 students over the course of their first 30 days of using Duolingo. It also contains baseline starter code written in Python. There are three data sets, corresponding to three different language courses. More details on the data set and task are available at: http://sharedtask.duolingo.com. (2018-01-10)	https://paperswithcode.com/dataset/duolingo-slam-shared-task	01/06/2018						
5124	Duolingo Spaced Repetition Data	This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in experiments by Settles & Meeder (2016). For more details and replication source code, visit: https://github.com/duolingo/halflife-regression (2016-06-07)	https://paperswithcode.com/dataset/duolingo-spaced-repetition-data	01/08/2016						
5125	SubSumE	"SubSumE Dataset
This repository contains the SubSumE dataset for subjective document summarization. See the paper and the talk for details on dataset creation. Also check out our work SuDocu on example-based document summarization.
Dataset Files
Download the dataset from here.
The dataset contains :

Simplified text from 48 Wikipedia pages of the states in the US. Additionally, all the sentences in these documents
are put together in a single file processed_state_sentences.csv and are assigned a unique sentence id that 
is used in summary json files. 
Intent-based summaries created by human annotators.

Each datapoint file in the directory user_summary_jsons contains a json containing summaries of Wikipedia pages
of eight states with following keys:

intent : Summarization intent provided to human annotators for generating the summary
summaries: List of summary jsons for eight states assigned to the annotator. Each json in the list contains following keys:
state_name: Name of the state
sentence_ids: Global ids of sentences (wrt processed_state_sentences.csv) present in the summary
sentences: List of sentences present in the summary
use_keywords: Keywords used by the annotator to search the document when creating summaries"	https://paperswithcode.com/dataset/subsume	01/11/2021						
5126	AnswerSumm	AnswerSumm is a dataset of 4,631 CQA threads for answer summarization, curated by professional linguists.	https://paperswithcode.com/dataset/answersumm	11/11/2021						
5127	MultiSV	MultiSV is a corpus designed for training and evaluating text-independent multi-channel speaker verification systems. It can be readily used also for experiments with dereverberation, denoising, and speech enhancement.	https://paperswithcode.com/dataset/multisv	11/11/2021						
5128	ANIM	It comprises synthetic mesh sequences from Deformation Transfer for Triangle Meshes.	https://paperswithcode.com/dataset/anim	01/08/2004	ANimals in Motion					
5129	AMA	Articulated Mesh Animation (AMA) is a real-world dataset containing 10 mesh sequences depicting 3 different humans performing various actions	https://paperswithcode.com/dataset/ama	08/08/2008	Articulated Mesh Animation					
5130	CAPE	"The CAPE dataset is a 3D dynamic dataset of clothed humans, featuring:

3D mesh registrations of accurate scans of clothed people in motion, captured at 60 FPS;
Consistent SMPL mesh topology, all frames in correspondence;
Precise, captured minimally clothed body shape under clothing;
Clothed bodies of large pose variations;
Both posed and unposed (i.e. in canonical pose) clothed body for each frame;
SMPL body pose parameters for each frame;
(New!) High-quality raw scan data of several subjects and sequences along with texture is available. Please first register as a user and send us your request."	https://paperswithcode.com/dataset/cape		Clothed Auto Person Encoding					
5131	TSSB	The time series segmentation benchmark (TSSB) currently contains 66 annotated time series (TS) with 2-7 segments. Each TS is constructed from one of the UEA & UCR time series classification datasets. We group TS by label and concatenate them to create segments with distinctive temporal patterns and statistical properties. We annotate the offsets at which we concatenated the segments as change points (CPs). Addtionally, we apply resampling to control the dataset resolution and add approximate, hand-selected window sizes that are able to capture temporal patterns.	https://paperswithcode.com/dataset/tssb	26/10/2021	Time Series Segmentation Benchmark					
5132	Samoa Measles Outbreak 2019	Dataset contains cumulative reported cases, hospital admission and discharge, and mortality data as parsed from the publicly available press releases by the Ministry of Health and National Emergency Operations Centre (NEOC) of the Government of Samoa. The data spans the initial press release at the end of September 2019 through to the final press release at the end of January 2020.	https://paperswithcode.com/dataset/samoa-measles-outbreak-2019	30/03/2021						
5133	WPC	The WPC (Waterloo Point Cloud) database is a dataset for subjective and objective quality assessment of point clouds.	https://paperswithcode.com/dataset/wpc	10/11/2021	Waterloo Point Cloud					
5134	ArgKP-2021	"Data set covering a set of debatable topics, where for each topic and stance, a set of triplets of the form &lt;argument, KP, label&gt; is provided. The data set is based on the ArgKP data set, which contains arguments contributed by the crowd on 28 debatable topics, split by their stance towards the topic, and KPs written by an expert for those topics. Crowd annotations were collected to determine whether a KP represents an argument, i.e., is a match for an
argument.
The arguments in ArgKP are a subset of the IBM-ArgQ-Rank-30kArgs data set.
For a test set, we extended ArgKP, adding three new debatable topics, that were also not part of IBM-ArgQ-Rank-30kArgs. The test set was collected specifically for KPA-2021, and was carefully designed to be similar in various aspects to the training data 2 . For each topic, crowd sourced arguments were collected, expert KPs generated, and match/no match annotations for argument/KP pairs obtained, resulting in a data set compatible with the ArgKP format. Arguments collection strictly adhered to the guidelines, quality measures, and post processing used for the collection of arguments in IBM-ArgQ-Rank-30kArgs, while the generation of expert KPs, collection of match annotations, and final data set creation strictly adhered to the manner in which ArgKP was created.
Source: Overview of the 2021 Key Point Analysis Shared Task"	https://paperswithcode.com/dataset/argkp-2021	04/07/2021						
5135	Arendt	"Digital Edition: Essays from Hannah Arendt
We have created a NER dataset from the digital edition ""Sechs Essays"" by Hannah Arendt. It consists of 23 documents from the period 1932-1976, which are available as TEI files online (see https://hannah-arendt-edition.net/3p.html?lang=de). 

This NER Dataset ist licensed under a
Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Germany (CC BY-NC-SA 3.0 DE).
From the original TEI files we build an NER dataset with tags distributed as shown in the following Table:
Tag | # All | # Train | # Test | # Devel
----|-------|---------|--------|---------
person | 1,702 | 1,303 | 182 | 217
place        | 1,087 | 891 | 111 | 85 
ethnicity    | 1,093 | 867 | 115 | 111 
organisation | 455 | 377 | 39 | 39 
event        | 57 | 49 | 6 | 2  
language     | 20 | 14 | 4 | 2  
not tagged   | 153,223 | 121,154 | 16,101 | 15,968
In the original TEI files the class person is additionally divided into ""person"", ""biblicalFigure"", ""ficticiousPerson"", ""deity"", and ""mythologicalFigure"", but some of these different ""person"" sub classes had too few examples. Therefore we have combined these classes into a general class for persons. Furthermore, the class place was divided into ""place"" and ""country"". In the original TEI files some countries are also tagged as places. Therefore we combined both classes into one class for general places. Finally there was a class ""ship"". But in the whole edition there were only 4 examples of this class. That is why we decided to exclude this class from our NER dataset.
We provide the dataset in two formats together with a partition into a train, dev, and testset. The first one is an easy format similar to the well-known CONLL-X format and the second one is an easy json format with the following structure:
It consists of a list of samples. Each sample is in turn a list of words or special characters. These in turn are represented as a two-element list, where the first element is the word itself and the second element is the corresponding target tag. Here is an example:
[[['Peter','B-person'],[Müller,'I-person'],['lebt','O'],['in','O'], ['Frankfurt','B-place'],['am','I-place'],['Main','I-place'],['.','O']],[['Gebürtig','O'],['stammt','O'],['er','O'],['aus','O'],['Berlin','B-place']]"	https://paperswithcode.com/dataset/arendt	23/04/2021						
5136	Sturm	"Digital Edition: Sturm Edition
Source:
Schrade, Torsten: „Startseite“, in: DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde, erarbeitet und herausgegeben von Marjam Trautmann und Torsten Schrade. Mainz, Akademie der Wissenschaften und der Literatur, Version 1 vom 16. Jul. 2018.
This NER Dataset is available under the license
Creative Commons Attribution 4.0 International (CC-BY 4.0)
From the Sturm Edition we have built a NER dataset. These are 174 letters from the years 1914-1922, which are available online in TEI format (see https://sturm-edition.de/id/S.0000001). It contains as tagged entities only persons, places and dates. From the original TEI files we build an NER dataset with tags distributed as shown in the following Table
Tag | # All | # Train | # Test | # Devel 
----|-------|---------|--------|--------
pers         | 930  | 763 | 83 | 84 
date         | 722 | 612 | 59 | 51
place        | 492 | 374 | 59 | 59 
not tagged   | 33,809 | 27,047 | 3,306 | 3,456
We provide the dataset in two formats together with a partition into a train, dev, and testset. The first one is an easy format similar to the well-known CONLL-X format and the second one is an easy json format with the following structure:
It consists of a list of samples. Each sample is in turn a list of words or special characters. These in turn are represented as a two-element list, where the first element is the word itself and the second element is the corresponding target tag. Here is an example:
[[['Peter','B-pers'],[Müller,'I-pers'],['lebt','O'],['in','O'], ['Frankfurt','B-place'],['am','I-place'],['Main','I-place'],['.','O']],[['Gebürtig','O'],['stammt','O'],['er','O'],['aus','O'],['Berlin','B-place']]"	https://paperswithcode.com/dataset/sturm	23/04/2021						
5137	patients' data	The dataset describes 150 patients with the following demographic characteristics : sex, age, HOMA-IR , systolic and diastolic blood pressure , and LDL-Cholesterol . These patients were followed for 28 years . The characteristics are the mean of the following measurement each year. In each year a liver biopsy was taken to record the stage of fibrosis and then the count of transition  among stages is recorded in the columns called ( lambda ij) where the i,j represents the stages where the patients move between them . In other word , for each patient , there is a column for the transition count this patient had made from stage 0 to stage 1 , then there is another column for the transition counts he made from stage 1 to stage 2 in this follow up 28 years , and so on , that is to mean there are 9 columns for the count of each transition for each patient as there are 9 transitions that could be made : from 0 to 1 , from 1 to 2 , from 2 to 3 , from 3 to 4 , from 1 to 0 , from 2 to 1 , from 3 to 2 , from 2 to 0  and from 3 to 1 . these transition counts are the dependent or the response variable , while the demographic characteristics are the predictors or the independent variables . Using Poisson regression model is used to relate these counts with the risk factors for NAFLD .	https://paperswithcode.com/dataset/patients-data	12/11/2021	NAFLD risk factors & fibrosis stages					
5138	DataCLUE	DataCLUE is the first Data-Centric benchmark applied in NLP field.	https://paperswithcode.com/dataset/dataclue	16/11/2021						
5139	Biased-Cars	We introduce a challenging new dataset for simultaneous object category and viewpoint classification—the Biased-Cars dataset. Our dataset features photo-realistic outdoor scene data with fine control over scene clutter (trees, street furniture, and pedestrians), car colors, object occlusions, diverse backgrounds (building/road textures) and lighting conditions (sky maps). Biased-Cars consists of 15K images of five different car models seen from viewpoints varying between 0-90 degrees of azimuth, and 0-50 degrees of zenith across multiple scales. Our dataset offers complete control over the joint distribution of categories, viewpoints, and other scene parameters, and the use of physically based rendering ensures photo-realism.	https://paperswithcode.com/dataset/biased-cars	15/07/2020						
5140	VGGFace2 HQ	"A high-resolution version of VGGFace2 for academic face editing purposes.
This project uses GFPGAN for image restoration and insightface for data preprocessing (crop and align)."	https://paperswithcode.com/dataset/vggface2-hq	17/11/2021	VGGFace2 HQ					
5141	GINC	GINC (Generative In-Context learning Dataset) is a small-scale synthetic dataset for studying in-context learning. The pretraining data is generated by a mixture of HMMs and the in-context learning prompt examples are also generated from HMMs (either from the mixture or not). The prompt examples are out-of-distribution with respect to the pretraining data since every example is independent, concatenated, and separated by delimiters. The GitHub repository provides code to generate GINC-style datasets of varying vocabulary sizes, number of HMMs, and other parameters.	https://paperswithcode.com/dataset/ginc	03/11/2021	Generative IN-Context learning Dataset					
5142	HGP	Hands Guns and Phones (HGP) dataset contains 2199 images (1989 for training an 210 for testing) of people using guns or phones in real-world scenarios (people making phones reviews, shooting drills, or making calls). Every image of this dataset is labeled with the bounding boxes of Hands, Phones and Guns. All the aforementioned images were collected from Youtube videos and have different sizes.	https://paperswithcode.com/dataset/hgp	17/11/2021	Hands Guns and Phones Dataset					
5143	THGP	Temporal Hands Guns and Phones (THGP) dataset, is a collection of 5960 video frames (5000 for training and 960 for testing). The training part is composed with 50 videos of 100 frames (720 × 720 pixels). This dataset contains 20 videos of shooting drills, 20 videos of armed robberies, and 10 videos of people making calls. The testing part contains 48 videos of 20 frames (720 × 720). Videos contained in the testing dataset includes phone calls, gun reviews, shooting drills, people making calls, and armed robberies at convenience stores. This dataset is labeled with the bounding boxes of hands, phones, and guns.	https://paperswithcode.com/dataset/thgp	17/11/2021	Temporal Hands Guns and Phones Dataset					
5144	ARCT	Freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims.	https://paperswithcode.com/dataset/arct	20/06/2018	Argument Reasoning Comprehension Task					
5145	Pan-STARRS	Pan-STARRS is a system for wide-field astronomical imaging developed and operated by the Institute for Astronomy at the University of Hawaii. Pan-STARRS1 (PS1) is the first part of Pan-STARRS to be completed and is the basis for both Data Releases 1 and 2 (DR1 and DR2).  The PS1 survey used a 1.8 meter telescope and its 1.4 Gigapixel camera  to image the sky in five broadband  filters (g, r, i, z, y).	https://paperswithcode.com/dataset/pan-starrs		Panoramic Survey Telescope and Rapid Response System (Pan-STARRS)					
5146	CAR	"CAR contains visual attributes for objects in the Cityscapes dataset.
For each object in an image, we have a list of attributes that depend on the category of the object. For instance, a vehicle category has a visibility attribute while a pedestrian has an activity attribute (walking, standing, etc.). 
The objective of this dataset is to ease the development of better algorithms for self-driving vehicles as that requires a complete understanding of the entire scene with all of its details including attributes of all objects.
We chose Cityscapes as it already contains different types of useful annotations and adding attributes to that will remove a huge burden over developing algorithms with self or semi supervision."	https://paperswithcode.com/dataset/car	16/11/2021	Cityscapes Attributes Recognition					
5147	Robotic Interestingness	Robotic Interestingness dataset was created to promote the development visual interesting scene prediction for such purpose, for robots to better sense the world.	https://paperswithcode.com/dataset/robotic-interestingness	01/11/2019	Robotic Interestingness: A Dataset to Push the Limits of Online Visual Interesting Scene Prediction					
5148	Haze4k	Haze4k is a synthesized dataset with 4,000 hazy images, in which each hazy image has the associate ground truths of a latent clean image, a transmission map, and an atmospheric light ma	https://paperswithcode.com/dataset/haze4k	06/08/2021						
5149	ChEBI-20	"Dataset contains 33,010 molecule-description pairs split into 80\%/10\%/10\% train/val/test splits. The goal of the task is to retrieve the relevant molecule for a natural language description. It is defined as follows:
To push the boundaries of multimodal models, we present a new IR task: \textbf{Text2Mol}.
Given a text query and list of molecules without any reference textual information (represented, for example, as SMILES strings, graphs, or other equivalent representations) retrieve the molecule corresponding to the query. From a text description of a molecule, the model must incorporate the information in the description into a semantic representation which can be used to directly retrieve the molecule. This requires the integration of two very different types of information: the structured knowledge represented by text and the chemical properties present in molecular graphs. We assume there is only one correct (relevant) molecule for each description, so we consider two measures for this task: Hits@1 and mean reciprocal rank (MRR). 
80\% of the data is used for training. Retrieval is done against the entire corpus of molecules (train, val, test)."	https://paperswithcode.com/dataset/chebi-20	01/11/2021						
5150	WikiContradiction	WikiContradiction is a novel wiki dataset for self-contradiction Wikipedia article detection.	https://paperswithcode.com/dataset/wikicontradiction	16/11/2021						
5151	OpenFWI	OpenFWI is a collection of large-scale open-source benchmark datasets for seismic full waveform inversion (FWI). OpenFWI is catered for the geoscience and machine learning community to facilitate diversified, rigorous  and reproducible research on machine learning-based FWI.	https://paperswithcode.com/dataset/openfwi	04/11/2021						
5152	B-Pref	B-Pref is a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities.	https://paperswithcode.com/dataset/b-pref	04/11/2021						
5153	Product Page	Product Page is a large-scale and realistic dataset of webpages. The dataset contains 51,701 manually labeled product pages from 8,175 real e-commerce websites. The pages can be rendered entirely in a web browser and are suitable for computer vision applications. This makes it substantially richer and more diverse than other datasets proposed for element representation learning, classification and prediction on the web.	https://paperswithcode.com/dataset/product-page	03/11/2021						
5154	IconQA	"Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images in the daily-life context. Icon question answering (IconQA) is a benchmark which aims to highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning in real-world diagram word problems. For this benchmark, a large-scale IconQA dataset is built that consists of three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. Compared to existing VQA benchmarks, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning.
Description from: IconQA"	https://paperswithcode.com/dataset/iconqa	25/10/2021	Icon Question Answering					
5155	VoiceBank-SLR	"Because there is no publicly available free dataset for
speech dereverberation, we prepared a dataset based on the
clean speech from VoiceBank-DEMAND [26] (discard the
noisy speech) and convolved them with the room impulse
response (RIR) from OpenSLR."	https://paperswithcode.com/dataset/voicebank-slr	12/10/2021						
5156	LIVE-VQC	The great variations of videographic skills in videography, camera designs, compression and processing protocols, communication and bandwidth environments, and displays leads to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, complex and often commingled distortions that are difficult or impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we have constructed a large-scale video quality assessment database containing 585 videos of unique content , captured using 101 different devices (43 device models) by 80 different users with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores , resulting in an average of 240 recorded human opinions per video . This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.	https://paperswithcode.com/dataset/live-vqc	05/03/2018	LIVE Video Quality Challenge (VQC) Database					
5157	KoNViD-1k	"Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. A lot of existing VQA databases cover small numbers of video sequences with artificial distortions. When testing newly developed Quality of Experience (QoE) models and metrics, they are commonly evaluated against subjective data from such databases, that are the result of perception experiments. However, since the aim of these QoE models is to accurately predict natural videos, these artificially distorted video databases are an insufficient basis for learning. Additionally, the small sizes make them only marginally usable for state-of-the-art learning systems, such as deep learning. In order to give a better basis for development and evaluation of objective VQA methods, we have created a larger datasets of natural, real-world video sequences with corresponding subjective mean opinion scores (MOS) gathered through crowdsourcing.
​
We took YFCC100m as a baseline database, consisting of 793436 Creative Commons (CC) video sequences, filtered them through multiple steps to ensure that the video sequences are representative of the whole spectrum of available video content, types of distortions, and subjective quality. The resulting 1200 videos are available to download, alongside the subjective data and evaluation of the best-performing techniques available for multiple video attributes. Namely, we have evaluated blur, colorfulness, contrast, spatial information, temporal information and video quality."	https://paperswithcode.com/dataset/konvid-1k	10/05/2017	KoNViD-1k VQA Database					
5158	YouTube-UGC	This YouTube dataset is a sampling from thousands of User Generated Content (UGC) as uploaded to YouTube distributed under the Creative Commons license. This dataset was created in order to assist in the advancement of video compression and quality assessment research of UGC videos.	https://paperswithcode.com/dataset/youtube-ugc	13/04/2019	YouTube UGC dataset					
5159	LIVE-FB LSVQ	"No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, ""in-the-wild"" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 real-world distorted videos and 117, 000 space-time localized video patches (""v-patches""), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process."	https://paperswithcode.com/dataset/live-fb-lsvq	27/11/2020	LIVE-FB Large-Scale Social Video Quality (LSVQ) Database					
5160	LIVE-ETRI	"The video deployed parameter space is continuously increasing to provide more realistic and immersive experiences to global streaming and social media viewers. However, increments in video parameters such as spatial resolution or frame rate are inevitably associated with larger data volumes. Transmitting increasingly voluminous videos through limited bandwidth networks in a perceptually optimal way is a present challenge affecting billions of viewers. One recent practice adopted by the video service providers is space-time resolution adaptation in conjunction with video compression. Consequently, it is important to understand how different levels of space-time subsampling and compression affect the perceptual quality of videos.
Towards making progress in this direction, we constructed a large new resource, called the ETRI-LIVE Space-Time Subsampled Video Quality (ETRI-LIVE-STSVQ) database, containing 437 videos generated by applying various levels of combined space-time subsampling and video compression on 15 diverse video contents. We also conducted a large-scale human study on the new dataset, collecting about 15,000 subjective judgments of video quality. The ETRI-LIVE STSVQ database is being made publicly and freely available with the desire to improve future research and development on topics such as video quality modeling and perceptual video coding."	https://paperswithcode.com/dataset/live-etri	29/01/2021	ETRI-LIVE Space-Time Subsampled Video Quality (STSVQ) Database					
5161	FLIR-aligned	This recently released multispectral (multi-)object detection dataset contains around 10k manually-annotated thermal images with their corresponding reference visible images, collected during daytime and nighttime. We only kept the 3 more frequent classes which are “bicycle”, “car” and “per- son”. We manually removed the misaligned visible-thermal image pairs and ended with 4,129 well-aligned image pairs for training and 1,013 image pairs for test. This new aligned dataset can be downloaded here: http:// shorturl.at/ahAY4	https://paperswithcode.com/dataset/flir-aligned							
5162	P3M-10k	"P3M-10k contains 10421 high-resolution real-world
face-blurred portrait images, along with their manually labeled alpha mattes. The Dataset is
aimed to aid research efforts in the area of portrait image matting and related topics."	https://paperswithcode.com/dataset/p3m-10k	29/04/2021	Privacy-Preserving Portrait Matting Dataset					
5163	SLUE	"Spoken Language Understanding Evaluation (SLUE) is a suite of benchmark tasks  for spoken language understanding evaluation. It consists of limited-size labeled training sets and corresponding evaluation sets. This resource would allow the research community to track progress, evaluate pre-trained representations for higher-level tasks, and study open questions such as the utility of pipeline versus end-to-end approaches. The first phase of the SLUE benchmark suite consists of named entity recognition (NER), sentiment analysis (SA), and ASR on the corresponding datasets.
Corpus includes:

SLUE-VoxPopuli: consists of ASR and NER tasks - CC0 license
SLUE-VoxCeleb: consists of ASR and SA tasks - CCBY 4.0 license"	https://paperswithcode.com/dataset/slue	19/11/2021	Spoken Language Understanding Evaluation					
5164	SOSD	"SOSD is a collection of dataset to benchmark the lookup performance of learned indexes.
SOSD currently includes eight different datasets. Each dataset consists of 200 million 64-bit unsigned integers (keys) with very few duplicates (if at all):
amzn represents book sale popularity data.
face is an upsampled version of a Facebook user ID dataset.
logn and norm are lognormal (0, 2) and normal distributions, respectively.
osmc is uniformly sampled OpenStreetMap locations represented as Google S2 CellIds.
uden is dense integers.
uspr is uniformly distributed sparse integers.
wiki is Wikipedia article edit timestamps.
In addition, there are 32-bit versions of all datasets (except osmc and wiki) with similar CDFs. We use different parameters, (0, 1), for logn in the 32-bit case to reduce the number of duplicates."	https://paperswithcode.com/dataset/sosd	01/08/2019	Searching on Sorted Data					
5165	ClevrTex	"ClevrTex is a new benchmark designed as the next challenge to compare, evaluate and analyze algorithms for unsupervised multi-object segmentation. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques.
Image source: Karazija et al."	https://paperswithcode.com/dataset/clevrtex	19/11/2021						
5166	LegalNERo	"LegalNERo is a manually annotated corpus for named entity recognition in the Romanian legal domain. 
It provides gold annotations for organizations, locations, persons, time and legal resources mentioned in legal documents.
Additionally it offers GEONAMES codes for the named entities annotated as location (where a link could be established). 
The LegalNERo corpus is available in different formats: span-based, token-based and RDF. 
The Linguistic Linked Open Data (LLOD) version is provided in RDF-Turtle format."	https://paperswithcode.com/dataset/legalnero	01/11/2021	Romanian Named Entity Recognition in the Legal domain					
5167	Evidence Inference 2.0	The dataset consists of biomedical articles describing randomized control trials (RCTs) that compare multiple treatments. Each of these articles will have multiple questions, or 'prompts' associated with them. These prompts will ask about the relationship between an intervention and comparator with respect to an outcome, as reported in the trial. For example, a prompt may ask about the reported effects of aspirin as compared to placebo on the duration of headaches.	https://paperswithcode.com/dataset/evidence-inference-2-0	08/05/2020						
5168	RTASC	The ROBIN Technical Acquisition Speech Corpus (ROBINTASC) was developed within the ROBIN project. Its main purpose was to improve the behaviour of a conversational agent, allowing human-machine interaction in the context of purchasing technical equipment. It contains over 6 hours of read speech in Romanian language. We provide text files, associated speech files (WAV, 44.1KHz, 16-bit, single channel), annotated text files in CoNLL-U format.	https://paperswithcode.com/dataset/rtasc	22/11/2021	ROBIN Technical Acquisition Speech Corpus					
5169	The ComMA Dataset v0.2	"The ComMA Dataset v0.2 is a multilingual dataset annotated with a hierarchical, fine-grained tagset marking different types of aggression and the ""context"" in which they occur. The context, here, is defined by the conversational thread in which a specific comment occurs and also the ""type"" of discursive role that the comment is performing with respect to the previous comment. The initial dataset, being discussed here (and made available as part of the ComMA@ICON shared task), consists of a total 15,000 annotated comments in four languages - Meitei, Bangla, Hindi, and Indian English - collected from various social media platforms such as YouTube, Facebook, Twitter and Telegram. As is usual on social media websites, a large number of these comments are multilingual, mostly code-mixed with English."	https://paperswithcode.com/dataset/the-comma-dataset-v0-2	19/11/2021						
5170	COV19-CT-DB	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/cov19-ct-db	22/11/2021						
5171	Medical Bottles	"Original dataset for ""HIGH PRECISION MEDICINE BOTTLES VISION ONLINE INSPECTION SYSTEM AND CLASSIFICATION BASED ON MULTI-FEATURES AND ENSEMBLE LEARNING VIA INDEPENDENCE TEST"""	https://paperswithcode.com/dataset/medical-bottles	05/01/2021						
5172	RedCaps	"RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. The data is collected from a manually curated set of subreddits (350 total), which give coarse image labels and allow steering of the dataset composition without labeling individual instances.
Terms of use: Uses of RedCaps are subject to Reddit API terms. Users must comply with Reddit User Agreeement, Content Policy, and Privacy Policy.
Usage Restrictions: RedCaps should only be used for non-commercial research. RedCaps should not be used for any tasks that involve identifying features related to people (facial recognition, gender, age, ethnicity identification, etc.) or make decisions that impact people (mortgages, job applications, criminal sentences; or moderation decisions about user-uploaded data that could result in bans from a website). Any commercial and for-profit uses of RedCaps are restricted – it should not be used to train models that will be deployed in production systems as part of a product offered by businesses or government agencies
Refer to the datasheet in the paper more details.
Image source: https://redcaps.xyz/download"	https://paperswithcode.com/dataset/redcaps	22/11/2021						
5173	Translated TACRED	533 parallel examples sampled from TACRED, translated into Russian and Korean (and 3 additional examples in Russian), accompanied with tranlsation of a list of trigger words collected for the different relations.	https://paperswithcode.com/dataset/translated-tacred							
5174	CytoImageNet	"CytoImageNet is a large-scale pretraining dataset of microscopy images (890K, 894 classes). In the paper, CytoImageNet pretraining yielded features competitive to and different from ImageNet pretrained features on downstream microscopy tasks.

It was constructed from 40 openly available microscopy datasets. 
Weak labels (from experimental metadata) were assigned to each image in the dataset.
Images are of varying sizes.

The primary purpose of the dataset is to be used for pretraining as a pretext task for learning useful bioimage representations. However, it may be used for validation or exploratory analysis."	https://paperswithcode.com/dataset/cytoimagenet	23/11/2021	CytoImageNet: A large-scale pretraining dataset for bioimage transfer learning					
5175	MP-3DHP: Multi-Person 3D Human Pose Dataset	Multi-Person 3D HumanPose Dataset (MP-3DHP) is a depth sensor-based dataset, which was constructed to facilitate the development of multi-person 3D pose estimation methods targeting real-world challenges. The dataset includes 177k training data and 33k validation data where both the 3D human poses and body segments are avaliable. The dataset also include 9k clean background data and 4k testing data including multi-person 3D poses.	https://paperswithcode.com/dataset/mp-3dhp-multi-person-3d-human-pose-dataset	12/12/2020						
5176	3D Lane Synthetic Dataset	This is a synthetic dataset constructed to stimulate the development and evaluation of 3D lane detection methods.	https://paperswithcode.com/dataset/3d-lane-synthetic-dataset	24/03/2020						
5177	CEAHB2021-5	"Ancient books script identification of Chinese ethnic minorities with deep convolutional neural networks via multi-branch and spatial pyramid pooling
Automatic classification of ancient books is an important component of the digital platform of ancient books. In view of the ancient books script identification task of different ethnic minorities in China, we build a dataset of Chinese ethnic ancient handwritten books(Tai Le, Tibet, Naxi, Yi, Shui), crop and standardize preprocessing images of ancient books. 
Source:CEAHB2021-5"	https://paperswithcode.com/dataset/ceahb2021-5		Chinese Ethnic Ancient Handwritten Books database					
5178	TLHDIBD2021	"Hybrid-CBF: A hybrid classification and binarization framework for historical Tai Le document image binarization
The binarization of historical documents is very important and more challenging than the binarization of ordinary documents. As a result of the serious noise pollution found on the historical Tai Le documents, a new hybrid classification and binarization framework (Hybrid-CBF) is proposed for the binarization of historical Tai Le document images. The Tai Le historical document image binarization dataset (TLHDIBD2021) containing 2,780 image pairs is constructed. Due to the different degrees of document background pollution, the single method has a poor effect on the binarization of historical Tai Le documents. First, Hybrid-CBF clusters the historical Tai Le document images according to the noise level estimation to obtain document images with different noise levels. Second, the corresponding optimal binarization method is used for historical Tai Le documents with different noise levels. In Hybrid-CBF, two binarization methods of historical Tai Le documents based on a deep neural network are proposed.
Source:TLHDIBD2021"	https://paperswithcode.com/dataset/tlhdibd2021		Tai Le historical document image binarization dataset					
5179	4DMatch	A benchmark for matching and registration of partial point clouds with time-varying geometry. It is constructed using randomly selected 1761 sequences from DeformingThings4D.	https://paperswithcode.com/dataset/4dmatch	24/11/2021						
5180	WDC Products	"Many e-shops have started to mark-up product data within their HTML pages using the schema.org vocabulary. The Web Data Commons project regularly extracts such data from the Common Crawl, a large public web crawl. The Web Data Commons Training and Test Sets for Large-Scale Product Matching contain product offers from different e-shops in the form of binary product pairs (with corresponding label ""match"" or ""no match"") for four product categories, computers, cameras, watches and shoes. 
In order to support the evaluation of machine learning-based matching methods, the data is split into training, validation and test sets. For each product category, we provide training sets in four different sizes (2.000-70.000 pairs). Furthermore there are sets of ids for each training set for a possible validation split (stratified random draw) available. The test set for each product category consists of 1.100 product pairs. The labels of the test sets were manually checked while those of the training sets were derived using shared product identifiers from the Web via weak supervision. 
The data stems from the WDC Product Data Corpus for Large-Scale Product Matching - Version 2.0 which consists of 26 million product offers originating from 79 thousand websites."	https://paperswithcode.com/dataset/wdc-products							
5181	Evaluating registrations of serial sections with distortions of the ground truths. Supplemental data	This is the supplemental data for our paper on how to benchmark registrations of serial sections with ground truths. There are three main modalities and one further, as a reference.	https://paperswithcode.com/dataset/evaluating-registrations-of-serial-sections	22/11/2020						
5182	UTFPR-SBD3	"The semantic segmentation of clothes is a challenging task due to the wide variety of clothing styles, layers and shapes.
The UTFPR-SBD3 contains 4,500 images manually annotated at pixel level in 18 classes plus background.
To ensure the high quality of the dataset, all images were manually annotated at the pixel level using JS Segment Annotator, 2 a free web-based image annotation tool. The raw images were carefully selected to avoid, as far as possible, classes with low number of instances."	https://paperswithcode.com/dataset/utfpr-sbd3	01/11/2021						
5183	FGraDA	Previous research for adapting a general neural machine translation (NMT) model into a specific domain usually neglects the diversity in translation within the same domain, which is a core problem for domain adaptation in real- world scenarios. One representative of such challenging scenarios is to deploy a translation system for a conference with a specific topic, e.g., global warming or coronavirus, where there are usually extremely less resources due to the limited schedule. To motivate wider investigation in such a scenario, we present a real-world fine-grained domain adaptation task in machine translation (FGraDA). The FGraDA dataset consists of Chinese-English translation task for four sub-domains of information technology: autonomous vehicles, AI education, real-time networks, and smart phone. Each sub-domain is equipped with a development set and test set for evaluation pur- poses. To be closer to reality, FGraDA does not employ any in-domain bilingual training data but provides bilingual dictionaries and wiki knowledge base, which can be easier obtained within a short time. We benchmark the fine-grained domain adaptation task and present in-depth analyses showing that there are still challenging problems to further improve the performance with heterogeneous resources.	https://paperswithcode.com/dataset/fgrada	07/11/2021	Fine-Grained Domain Adaptation Dataset					
5184	IMDB-WIKI-SbS	IMDB-WIKI-SbS is a new large-scale dataset for evaluation pairwise comparisons, building on the success of a well-known benchmark for computer vision systems IMDB-WIKI. This dataset uses the age information offered by IMDB-WIKI as ground truth while providing a balanced distribution of ages and genders of people in photos.	https://paperswithcode.com/dataset/imdb-wiki-sbs							
5185	LIRIS human activities dataset	"The LIRIS human activities dataset contains (gray/rgb/depth) videos showing people performing various activities taken from daily life (discussing, telphone calls, giving an item etc.). The dataset is fully annotated, where the annotation not only contains information on the action class but also its spatial and temporal positions in the video. It was originally shot for the ICPR-HARL 2012 competition.
The dataset has been shot with two different cameras:
Subset D1 has been shot with a MS Kinect module mounted on a remotely controlled Wany robotics Pekee II mobile robot which is part of the LIRIS-VOIR platform.
Subset D2 has been shot with a sony consumer camcorder"	https://paperswithcode.com/dataset/liris-human-activities-dataset							
5186	CoVaxLies v1	CoVaxLies v1 includes 17 known Misinformation Targets (MisTs) found on Twitter about the covid-19 vaccines. Language experts annotated tweets as Relevant or Not Relevant, and then further annotated Relevant tweets with Stance towards each MisT. This collection is a first step in providing large-scale resources for misinformation detection and misinformation stance identification.	https://paperswithcode.com/dataset/covaxlies-v1	26/11/2021						
5187	Freibrug Cars	An object-centric dataset consiting of 52 RGB sequences of cars	https://paperswithcode.com/dataset/freibrug-cars	01/11/2015						
5188	LSUI	We released a large-scale underwater image (LSUI) dataset including 5004 image pairs, which involve richer underwater scenes (lighting conditions, water types and target categories) and better visual quality reference images than the existing ones.	https://paperswithcode.com/dataset/lsui	23/11/2021	Large Scale  Underwater Image Dataset					
5189	notebookcdg	"Inspired by Wang et al. 2021, we decided to utilize the top-voted and well-documented Kaggle notebooks to construct the notebookCDGdataset
We collected the top 10% highly-voted notebooks from the top 20 popular competitions on Kaggle (e.g. Titanic). We checked the data policy of each of the 20 competitions, none of them has copyright issues. We also contacted the Kaggle administrators to make sure our data collection complies with the platform’s policy.
In total, we collected 3,944 notebooks as raw data. After data preprocessing, the final dataset contains 2,476 notebooks out of the 3,944 notebooks from the raw data. It has 28,625 code–documentation pairs. The overall code-to-markdown ratio is 2.2195
Download notebookCDG dataset"	https://paperswithcode.com/dataset/notebookcdg	31/03/2021						
5190	Abt-Buy	"The Abt-Buy dataset for entity resolution derives from the online retailers Abt.com and Buy.com. The dataset contains 1081 entities from abt.com and 1092 entities from buy.com as well as a gold standard (perfect mapping) with 1097 matching record pairs between the two data sources.  The common attributes between the two data sources are: product name, product description and product price. 
The dataset was initially published in the repository of the Database Group of the University of Leipzig:
https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution
To enable the reproducibility of the results and the comparability of the performance of different matchers on the Abt-Buy matching task, the dataset was split into fixed train, validation and test sets. 
The fixed splits are provided in the CompERBench repository: 
http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"	https://paperswithcode.com/dataset/abt-buy	01/09/2010						
5191	Amazon-Google	"The Amazon-Google dataset for entity resolution derives from the online retailers Amazon.com and  the product search service of Google accessible through the Google Base Data API. The dataset contains 1363 entities from amazon.com and 3226 google products as well as a gold standard (perfect mapping) with 1300 matching record pairs between the two data sources. The common attributes between the two data sources are: product name, product description, manufacturer and price.
The dataset was initially published in the repository of the Database Group of the University of Leipzig: https://dbs.uni-leipzig.de/research/projects/object_matching/benchmark_datasets_for_entity_resolution
To enable the reproducibility of the results and the comparability of the performance of different matchers on the Amazon-Google matching task, the dataset was split into fixed train, validation and test sets. The fixed splits are provided in the CompERBench repository:
http://data.dws.informatik.uni-mannheim.de/benchmarkmatchingtasks/index.html"	https://paperswithcode.com/dataset/amazon-google	01/09/2010						
5192	MusicBrainz20K	"The MusicBrainz20K dataset for entity resolution and entity clustering is based on real records about songs from the MusicBrainz database. Each record is described with the following attributes: artist, title, album, year and length. The records have been modified with the DAPO 1 data generator. The generated dataset consists of five sources and approximately 20K records describing 10K unique song entities. It contains duplicates for 50% of the original records in two to five sources which are generated with a high degree of corruption to stress-test the entity resolution and clustering approaches.
1 Hildebrandt, Kai, et al. ""Large-scale data pollution with Apache Spark."" IEEE Transactions on Big Data 6.2 (2017): 396-411."	https://paperswithcode.com/dataset/musicbrainz20k	01/09/2017						
5193	Vehicle-1M	"Vehicle-1M  involves vehicle images captured across day and night, from head or rear, by multiple surveillance cameras installed in cities. There are totally 936,051 images from 55,527 vehicles and 400 vehicle models in the dataset. Each image is attached with a vehicle ID label denoting its identity in real world as well as a vehicle model label indicating the make, model and year of the vehicle(i.e. ""Audi-A6-2013""). All publications using Vehicle-1M dataset should cite the paper below:
Haiyun Guo, Chaoyang Zhao, Zhiwei Liu, Jinqiao Wang, Hanqing Lu: Learning coarse-to-fine structured feature embedding for vehicle re-identification. AAAI 2018."	https://paperswithcode.com/dataset/vehicle-1m	08/02/2018						
5194	WikiNEuRal	WikiNEuRal is a high-quality automatically-generated dataset for Multilingual Named Entity Recognition.	https://paperswithcode.com/dataset/wikineural	01/11/2021						
5195	Corrosion Image Data Set for Automating Scientific Assessment of Materials	"The study of material corrosion is an important research area, with corrosion degradation of metallic structures causing expenses up to 4% of the global domestic product annually along with major safety risks worldwide. Unfortunately, large-scale and timely scientific discovery of materials has been hindered by the lack of standardized corrosion experimental data in the public domain for developing machine learning models. Obtaining such data is challenging due to the expert knowledge and time required to conduct these scientific experiments and assess corrosion levels. We curate a novel dataset consisting of 600 images annotated with expert corrosion ratings obtained over 10 years of laboratory corrosion testing by material scientists. Based on this data set, we find that non-experts even when rigorously trained with domain guidelines to rate corrosion fail to match expert ratings. Challenges include limited data, image artifacts, and millimeter-precision corrosion. This motivates us to explore the viability of deep learning approaches to tackle this benchmark classification task. We study (i) convolutional neural networks powered with rich domain-specific image augmentation techniques tuned to our data, and (ii) a recent self-supervised representation learning approach either pretrained on ImageNet or trained on our data. We demonstrate that pretrained ResNet-18 and HR-Net models with tuned augmentations can reach up to 0.83 accuracy. With this corrosion data set, we open the door for the design of more advanced deep learning models to support this real-world task, while driving innovative new research to bridge computer vision and material innovation.
[Disclaimer]
By downloading this code and/or using this data, you agree to abide by all of the rules and regulations.
1. Researcher shall use the dataset only for non-commercial research and educational purposes.
2. The authors with Worcester Polytechnic Institute and US Army Research Lab make no representations or warranties regarding the dataset, including but not limited to warranties of non-infringement or fitness for a particular purpose.
3. Researcher accepts full responsibility for his or her use of the dataset and shall defend and indemnify the authors, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the dataset, including but not limited to Researcher's use of any copies of copyrighted images that he or she may create from the dataset.
4. Researcher may provide research associates and colleagues with access to the dataset provided that they first agree to be bound by these terms and conditions.
5. The authors reserve the right to terminate Researcher's access to or usage of the dataset at any time.
6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.
7. The law of the State of Massachusetts shall apply to all disputes under this agreement.
[Citation]
Anyone that uses this data set must cite and acknowledge our BMVC data set paper: 
@InProceedings{yin2021BMVC, author = {Yin, Biao and Josselyn, Nicholas and Considine, Thomas and Kelley, John and Rinderspacher, Berend and Jensen, Robert and Snyder, James and Zhang, Ziming and Rundensteiner, Elke}, title = {Corrosion Image Data Set for Automating Scientific Assessment of Materials}, booktitle = {British Machine Vision Conference (BMVC)}, year = {2021}}"	https://paperswithcode.com/dataset/corrosion-image-data-set-for-automating	22/11/2021						
5196	ClimART	Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. This has made them a popular target for neural network-based emulators. However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking. To fill this gap, we build a large dataset, ClimART, with more than \emph{10 million samples from present, pre-industrial, and future climate conditions}, based on the Canadian Earth System Model. ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed.	https://paperswithcode.com/dataset/climart	29/11/2021	Climate Atmospheric Radiative Transfer					
5197	IATOS Dataset	Archivos con audios de toses de personas grabadas por celular, segmentados por COVID positivo y negativo según resultado de test RT-PCR.	https://paperswithcode.com/dataset/iatos-dataset	27/04/2021						
5198	GPR1200	"Most publications that aim to optimize neural networks for CBIR, train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples. Classes and images were manually selected from six publicly available datasets of different image areas, ensuring high class diversity and clean class boundaries.
This dataset can therefore be used for benchmarking image descriptor systems on their generalizability."	https://paperswithcode.com/dataset/gpr1200	25/11/2021						
5199	Orchard	Orchard is a diagnostic dataset for systematically evaluating hierarchical reasoning in state-of-the-art neural sequence models	https://paperswithcode.com/dataset/orchard	28/11/2021	A Benchmark For Measuring Systematic Generalization of Multi-Hierarchical Reasoning					
5200	GVFC	"This is a new dataset of news headlines and their frames related to the issue of gun violence in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. The articles in this dataset are drawn from a sample of news articles from a list of 30 top U.S. news websites defined in terms of traffic to the websites; and collected from four time periods over the course of 2018 in order to capture a diversity of articles.
We include in this dataset, headlines of news articles and their annotations, the accompanying images and text- and image-derived features. We also include the codebook protocol, which includes all of the variables for annotations and their definitions that are applied by the annotators."	https://paperswithcode.com/dataset/gvfc	01/11/2019	Gun Violence Frame Corpus					
5201	A dataset of neonatal EEG recordings with seizures annotations	Neonatal seizures are a common emergency in the neonatal intensive care unit (NICU). There are many questions yet to be answered regarding the temporal/spatial characteristics of seizures from different pathologies, response to medication, effects on neurodevelopment and optimal detection. This dataset contains EEG recordings from human neonates and the visual interpretation of the EEG by the human expert. Multi-channel EEG was recorded from 79 term neonates admitted to the neonatal intensive care unit (NICU) at the Helsinki University Hospital. The median recording duration was 74 minutes (IQR: 64 to 96 minutes). EEGs were annotated by three experts for the presence of seizures. An average of 460 seizures were annotated per expert in the dataset, 39 neonates had seizures by consensus and 22 were seizure free by consensus. The dataset can be used as a reference set of neonatal seizures, for the development of automated methods of seizure detection and other EEG analysis, as well as for the analysis of inter-observer agreement.	https://paperswithcode.com/dataset/a-dataset-of-neonatal-eeg-recordings-with	05/06/2018						
5202	MMPTRACK	"Multi-camera Multiple People Tracking (MMPTRACK) dataset has about 9.6 hours of videos, with over half a million frame-wise annotations. The dataset is densely annotated, e.g., per-frame bounding boxes and person identities are available, as well as camera calibration parameters. Our dataset is recorded with 15 frames per second (FPS) in five diverse and challenging environment settings., e.g., retail, lobby, industry, cafe, and office. This is by far the largest publicly available multi-camera multiple people tracking dataset. 
We expect the availability of such large-scale multi-camera multiple people tracking dataset will encourage more participants in this research topic. This dataset is also valuable for the evaluation of other tasks, such as multi-view people detection and monocular multiple people tracking."	https://paperswithcode.com/dataset/mmptrack	30/11/2021	Multi-camera Multiple People Tracking Dataset					
5203	MIS-Check Dam	"Dataset annotated by domain experts using images from Google static map APIs (Google
Static Maps, 2021) for instance segmentation and object detection tasks."	https://paperswithcode.com/dataset/mis-check-dam	30/11/2021	Minor Irrigation Structures- Check Dam					
5204	fluocells	"By releasing this dataset, we aim at providing a new testbed for computer vision techniques using Deep Learning. The main peculiarity is the shift from the domain of ""natural images"" proper of common benchmark dataset to biological imaging. We anticipate that the advantages of doing so could be two-fold: i) fostering research in biomedical-related fields - for which popular pre-trained models perform typically poorly - and ii) promoting methodological research in deep learning by addressing peculiar requirements of these images. Possible applications include but are not limited to semantic segmentation, object detection and object counting. The data consist of 283 high-resolution pictures (1600x1200 pixels) of mice brain slices acquired through a fluorescence microscope. The final goal is to individuate and count neurons highlighted in the pictures by means of a marker, so to assess the result of a biological experiment. The corresponding ground-truth labels were generated through a hybrid approach involving semi-automatic and manual semantic segmentation. The result consists of black (0) and white (255) images having pixel-level annotations of where the stained neurons are located. For more information, please refer to Morelli, R. et al., 2021. Automating cell counting in fluorescent microscopy through deep learning with c-ResUnet. Scientific reports. https://doi.org/10.1038/s41598-021-01929-5. The collection of original images was supported by funding from the University of Bologna (RFO 2018) and the European Space Agency (Research agreement collaboration 4000123556)."	https://paperswithcode.com/dataset/fluocells	25/11/2021	Fluorescent Neuronal Cells					
5205	MSU Video Alignment and Retrieval Benchmark Suite	Frame-to-frame video alignment/synchronization	https://paperswithcode.com/dataset/msu-video-alignment-and-retrieval-benchmark	14/01/2022	MSU Video Alignment and Retrieval Benchmark Suite					
5206	Manually annotated 3-digit occupation codes from the Norwegian 1950 census	Manually annotated 3-digit occupation codes from the Norwegian full count 1950 population census.	https://paperswithcode.com/dataset/manually-annotated-3-digit-occupation-codes	07/06/2021						
5207	Manually annotated 3-digit occupation code training set from the Norwegian 1950 census	"The Norwegian Historical Data Centre, 2021, ""Manually annotated 3-digit occupation code training set from the Norwegian 1950 census"", https://doi.org/10.18710/7JWAZX, DataverseNO, V1"	https://paperswithcode.com/dataset/manually-annotated-3-digit-occupation-code	07/06/2021						
5208	DeepSport Dataset	"This basketball dataset was acquired under the Walloon region project DeepSport, using the Keemotion system installed in multiple arenas.
We would like to thanks both Keemotion for letting us use their system for raw image acquisition during live productions, and the LNB for the rights on their images."	https://paperswithcode.com/dataset/deepsport-dataset	23/07/2020						
5209	CNTD	"Chinese and Naxi scene text detection data set, labelme to json.
Source:CNTD"	https://paperswithcode.com/dataset/cntd		Chinese and Naxi text detection					
5210	Olivetti face	This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge.	https://paperswithcode.com/dataset/olivetti-face		Olivetti face					
5211	Sentence-level argument annotation	The dataset is based on a debate.org crawl. It is restricted to a subset of four out of the total 23 categories -- politics, society, economics and science -- and contains additional annotations. 3 human annotators familiar with linguistics segmented these documents and labeled them as being of medium or low quality, to exclude low quality documents. The annotators were then asked to indicate the beginning of each new argument and to label argumentative sentences summarizing the aspects of the post as conclusion and outside of argumentation. In this way, we obtained a ground truth of labeled arguments on a sentence level (Krippendorff's alpha=0.24 based on 20 documents and three annotators).	https://paperswithcode.com/dataset/sentence-level-argument-annotation	30/11/2021						
5212	debatepedia	"Debatepedia is a debate platform that lists arguments to a topic on one 
page, including subtitles, structuring the arguments into different aspects."	https://paperswithcode.com/dataset/debatepedia-1							
5213	debate.org	Debate.org is a debate platform that is organized in rounds where each of two opponents submits posts arguing for their side.	https://paperswithcode.com/dataset/debate-org							
5214	Student Essay	Student Essay is widely used in research on argument segmentation	https://paperswithcode.com/dataset/student-essay							
5215	Chaoyang	"Chaoyang dataset contains 1111 normal, 842 serrated, 1404 adenocarcinoma, 664 adenoma, and 705 normal, 321 serrated, 840 adenocarcinoma, 273 adenoma samples for training and testing, respectively. This noisy dataset is constructed in the real scenario.  

Details: Colon slides from Chaoyang hospital, the patch size is 512 × 512. We invited 3 professional pathologists to label the patches, respectively. We took the parts of labeled patches with consensus results from 3 pathologists as the testing set. Others we used as the training set. For the samples with inconsistent labeling opinions of the three doctors in the training set (this part accounts for about 40%), we randomly selected the opinions from one of the three doctors."	https://paperswithcode.com/dataset/chaoyang	05/12/2021						
5216	CRC	"Request access: cadpath.ai@impdiagnostics.com
The CRC dataset contains 1133 colorectal biopsy and polypectomy slides and is the result of our ongoing efforts to contribute to CRC diagnosis with a reference dataset. We aim to detect high-grade lesions with high sensitivity. High-grade lesions encompass conventional adenomas with high-grade dysplasia (including intra-mucosal carcinomas) and invasive adenocarcinomas. In addition, we also intend to identify low-grade lesions (corresponding to conventional adenomas with low-grade dysplasia). Accordingly, we created three diagnostic categories for the algorithm, labelled as non-neoplastic, low-grade and high-grade lesions."	https://paperswithcode.com/dataset/crc	13/07/2021						
5217	MEFB	The MEFB consists of a test set of 100 image pairs.	https://paperswithcode.com/dataset/mefb		benchmark of multi-exposure image fusion					
5218	RISeC	"We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize (e.g., the primitive instructionsin MILK) and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation.
We thus frame the semantic comprehension of procedural text such as recipes, as fairly generic NLP subtasks, covering (i) entity recognition (ingredients, tools and actions), (ii) relation extraction (what ingredients and tools are involved in the actions), and (iii) zero anaphora resolution (link actions to implicit arguments, e.g., results from previous recipe steps). 
Further, our Recipe Instruction Semantic Corpus (RISeC) dataset includes textual descriptions for the zero anaphora, to facilitate language generation thereof. Besides the dataset itself, we contribute a pipeline neural architecture that addresses entity and relation extractionas well an identification of zero anaphora."	https://paperswithcode.com/dataset/risec	01/12/2020	Recipe Instruction Semantics Corpus					
5219	FALLMUD	"FAscicle Lower Leg Muscle Ultrasound Dataset is a dataset composed of 812 ultrasound images of lower leg muscles to analyze muscle weaknesses and prevent injuries.
It combines the datasets provided by two articles, “Estimating Full Regional Skeletal Muscle Fibre Orientation from B-Mode Ultrasound Images Using Convolutional, Residual, and Deconvolutional Neural Networks” published by Ryan Cunningham et al. and “Automated Analysis of Musculoskeletal Ultrasound Images Using Deep Learning” published by Neil Cronin, with complementary annotations. The dataset has been introduced in this paper: Michard, H., Luvison, B., Pham, Q. C., Morales-Artacho, A. J., & Guilhem, G. (2021, August). AW-Net: automatic muscle structure analysis on B-mode ultrasound images for injury prevention. In Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (pp. 1-9)."	https://paperswithcode.com/dataset/fallmud	01/08/2021	FAscicle Lower Leg Muscle Ultrasound Dataset					
5220	BCNB	"Breast cancer (BC) has become the greatest threat to women’s health worldwide. Clinically, identification of axillary lymph node (ALN) metastasis and other tumor clinical characteristics such as ER, PR, and so on, are important for evaluating the prognosis and guiding the treatment for BC patients.
Several studies intended to predict the ALN status and other tumor clinical characteristics by clinicopathological data and genetic testing score. However, due to the relatively poor predictive values and high genetic testing costs, these methods are often limited. Recently, deep learning (DL) has enabled rapid advances in computational pathology, DL can perform high-throughput feature extraction on medical images and analyze the correlation between primary tumor features and above status. So far, there is no relevant research on preoperatively predicting ALN metastasis and other tumor clinical characteristics based on WSIs of primary BC samples.
Our paper has introduced a new dataset of Early Breast Cancer Core-Needle Biopsy WSI (BCNB), which includes core-needle biopsy whole slide images (WSIs) of early breast cancer patients and the corresponding clinical data. The WSIs have been examined and annotated by two independent and experienced pathologists blinded to all patient-related information.
Based on this dataset, we have studied the deep learning algorithm for predicting the metastatic status of ALN preoperatively by using multiple instance learning (MIL), and have achieved the best AUC of 0.831 in the independent test cohort. For more details, please review our paper. 
There are WSIs of 1058 patients,  and only part of tumor regions are annotated in WSIs. Except for the WSIs, we have also provided the clinical characteristics of each patient, which includes age, tumor size, tumor type, ER, PR, HER2, HER2 expression, histological grading, surgical, Ki67, molecular subtype, number of lymph node metastases, and the metastatic status of axillary lymph node (ALN). The dataset has been desensitized, and not contained the privacy information of patients.
Based on this dataset, we have studied the prediction of the metastatic status of axillary lymph node (ALN) in our paper, which is a weakly supervised classification task. However, other researches based on our dataset are also feasible, such as the prediction of histological grading, molecular subtype, HER2, ER, and PR. We do not limit the specific content for your research, and any research based on our dataset is welcome.
Please note that the dataset is only used for education and research, and the usage for commercial and clinical applications is not allowed. The usage of this dataset must follow the license."	https://paperswithcode.com/dataset/bcdalnmp	04/12/2021	Early Breast Cancer Core-Needle Biopsy WSI					
5221	LHC Olympics 2020	"These are the official datasets for the LHC Olympics 2020 Anomaly Detection Challenge. Each ""black box"" contains 1M events meant to be representative of actual LHC data. These events may include signal(s) and the challenge consists of finding these signals using the method of your choice. We have uploaded a total of THREE black boxes to be used for the challenge.
In addition, we include a background sample of 1M events meant to aid in the challenge. The background sample consists of QCD dijet events simulated using Pythia8 and Delphes 3.4.1. Be warned that both the physics and the detector modeling for this simulation may not exactly reflect the ""data"" in the black boxes. For both background and black box data, events are selected using a single fat-jet (R=1) trigger with pT threshold of 1.2 TeV.
These events are stored as pandas dataframes saved to compressed h5 format. For each event, all reconstructed particles are assumed to be massless and are recorded in detector coordinates (pT, eta, phi). More detailed information such as particle charge is not included. Events are zero padded to constant size arrays of 700 particles. The array format is therefore (Nevents=1M, 2100).
For more information, including a complete description of the challenge and an example Jupyter notebook illustrating how to read and process the events, see the official LHC Olympics 2020 webpage here.
UPDATE: November 23, 2020
Now that the challenge is over, we have uploaded the solutions to Black Boxes 1 and 3. They are simple ASCII files (events_LHCO2020_BlackBox1.masterkey and events_LHCO2020_BlackBox3.masterkey) where each line is the truth label -- 0 for background and 1 (and 2 in the case of BB3) for signal -- of each event in the corresponding h5 files (same ordering). For more information about the solutions, please visit the LHCO2020 webpage.
UPDATE: February 11, 2021
We have uploaded the Delphes detector cards and Pythia command files used to produce the Black Box datasets."	https://paperswithcode.com/dataset/lhc-olympics-2020	20/01/2021	LHC Olympics 2020 Anomaly Detection Challenge					
5222	Banglish	"A Bilingual Dataset for Bangla and English Voice Commands
Colloquial Bangla has adopted many English words due to colonial influence. In conversational Bangla, it is quite common to speak in a mixture of English and Bangla. This phenomenon, prevalent in conversational language is known as code-switching (CS). CS is defined as the continuous alternation between two languages in a single conversation. Thus, in Bangla natural language processing, it is often necessary to map a single base command to its many different variants - spoken in multiple mixtures of English and Bangla. In order to facilitate this, we have curated a dataset centered around common browser commands."	https://paperswithcode.com/dataset/banglish	17/08/2021						
5223	washed_contract	Dataset contains about 48K contracts which are open source on Etherscan.	https://paperswithcode.com/dataset/washed-contract	05/12/2021						
5224	TekGen	"The Dataset is part of the KELM corpus
This is the Wikipedia text--Wikidata KG aligned corpus used to train the data-to-text generation model. Please note that this is a corpus generated with distant supervision and should not be used as gold standard for evaluation.
It consists of 3 files:
https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-train.tsv
https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-validation.tsv
https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/quadruples-test.tsv

Each file contains one example per line. Each example is a json object with three fields:
triples: A list of triples of the form (subject, relation, object). eg. (Person X, award received, Award Y). If the triple has a subproperty, then it is quadruple instead. eg. (Person X, Award Y, received on, Date Z).

serialized triples: triples concatenated together as used for input to T5. The format is ""&lt;subject&gt; &lt;relation&gt; &lt;object&gt;"" where some subjects have multiple relations, e.g. ""&lt;subject&gt; &lt;relation1&gt; &lt;object1&gt; &lt;relation2&gt; &lt;object2&gt; &lt;relation3&gt; &lt;object3&gt;"". For more details on how these relations are grouped, please refer to the paper.

sentence: The wikipedia sentence aligned to these triples.

The names, aliases and Wikidata Ids of the entities can be found in https://storage.googleapis.com/gresearch/kelm-corpus/updated-2021/entities.jsonl."	https://paperswithcode.com/dataset/tekgen	23/10/2020						
5225	DaReCzech	"DareCzech
DaReCzech is a dataset for text relevance ranking in Czech. The dataset consists of more than 1.6M annotated query-documents pairs, which makes it one of the largest available datasets for this task.
Obtaining the Annotated Data
Please, first read a disclaimer that contains the terms of use. If you comply with them, send an email to srch.vyzkum@firma.seznam.cz and the link to the dataset will be sent to you."	https://paperswithcode.com/dataset/dareczech	03/12/2021	Dataset for text relevance ranking in Czech					
5226	MAD	"MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval.
MAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed. 
MAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. 
Take a look at the paper for additional information.
From the authors on availability: ""Due to copyright constraints, MAD’s videos will not be publicly released. However, we will provide all necessary features for our experiments’ reproducibility and promote future research in this direction"""	https://paperswithcode.com/dataset/mad	01/12/2021						
5227	Crowd 11	"This dataset defines a total of 11 crowd motion patterns and it is composed of over 6000 video sequences with an average length of 100 frames per sequence.
This documentation presents how to download and process the Crowd-11 dataset.
If you use this dataset, please cite our paper:
Camille Dupont, Luis Tobias, and Bertrand Luvison. ""Crowd-11: A Dataset for Fine Grained Crowd Behaviour Analysis."" In  Computer Vision and Pattern Recognition Workshops (CVPRW), 2017.
Since this dataset is a composition of web videos and already existing datasets, we ask you to download and accept licence of each source and dataset. 
The construction of the Crowd-11 dataset is composed of two steps:
Step 1: Retrieve videos of interest from the web and/or pre-existing datasets
Retrieve the pre-existing datasets of interest
The pre-existing datasets are:
| DATASET NAME          | url                                                               | $SOURCE_NAME  |
| -------------         |:-----------------------------------------------------------------:| -------------:|
| UMN                   | http://mha.cs.umn.edu/proj_events.shtml#crowd                     | umn |
| AGORASET              | https://www.sites.univ-rennes2.fr/costel/corpetti/agoraset/Site/AGORASET.html | agoraset |
| PETS                  | http://www.cvg.reading.ac.uk/PETS2009/a.html#s3                   | pets |
| HOCKEY FIGHT          | http://visilab.etsii.uclm.es/personas/oscar/FightDetection/       | hockey |
| MOVIES                | http://visilab.etsii.uclm.es/personas/oscar/FightDetection/       | peliculas |
| CUHK                  | http://www.ee.cuhk.edu.hk/~jshao/CUHKcrowd_files/cuhk_crowd_dataset.htm  | cuhk |
| WWW                   | http://www.ee.cuhk.edu.hk/~jshao/WWWCrowdDataset.html             | www |
| WORLDEXPO'10 CROWD COUNTING  | http://www.ee.cuhk.edu.hk/~xgwang/expo.html                | shanghai |
| VIOLENT-FLOWS         | http://www.openu.ac.il/home/hassner/data/violentflows/            | violent_flow |
These datasets should be stored in their ""existing_datasets/$SOURCE_NAME/"" folder:
.
└── existing_datasets
    ├── agoraset
    ├── cuhk
    ├── hockey
    ├── peliculas
    │   ├── fights
    │   └── noFights
    ├── pets
    ├── shanghai
    ├── umn
    ├── violent_flow
    └── www  
Copy the videos of interest from the datasets of interest
The list of the videos of interest is in existing_datasets_urls.csv. To extract them into the VOI folder, execute:
python existing_datasets_gathering.py
The VOI folder should have the following structure:
.
└── VOI
    ├── agoraset
    ├── cuhk
    ├── hockey
    ├── peliculas
    ├── pets
    ├── shanghai
    ├── umn
    ├── violent_flow
    └── www  
Download the videos of interest from the web
The web sources are:
| SOURCE NAME           | url                                | $SOURCE_NAME  |
| -------------         |:----------------------------------:| -------------:|
| YOUTUBE               | https://www.youtube.com/           | youtube       |
| GETTYIMAGES           | http://www.gettyimages.fr/         | gettyimages   |
| POND5                 | https://www.pond5.com/             | pond5         |
The list of the web urls to download is in web_urls.csv.
The web_urls.csv file's structure is as follows :  
| $SOURCE NAME          | URL                   | OUTPUT_NAME  | TS_MULTIPLIER  |
| -------------         |:---------------------:| ------------:| --------------:|
We do not provide the script to download them, but many tools exist to do it (pytube, urllib, etc...). 
Note: a few videos have a ts_multiplier field. These video are in slow motion and the ts_multiplier is provided to speed them up (cf. SETPTS option in avconv).
The downloaded videos should be stored in their VOI/$SOURCE_NAME folder, which should now have the following structure:
.
└── VOI
    ├── agoraset
    ├── cuhk
    ├── gettyimages
    ├── hockey
    ├── peliculas
    ├── pets
    ├── pond5
    ├── shanghai
    ├── umn
    ├── violent_flow
    ├── youtube
    └── www  
Step 2: Processing original videos into the Crowd-11 dataset
Once the VOI folder is complete, a preprocessing step is required in order to crop and trim the original videos into the Crowd-11 dataset.
The preprocessing.csv file's structure is as follows :  
| Videoname     | Label     | Frame_start  | Frame_end  | Top_left  |  Top_right | Width | Height | $SOURCE_NAME | Scene_number | Crop_number |
| ------------- |:---------:| ------------:| ----------:| ---------:| ----------:| -----:| ------:| ------------:| ------------:| -----------:|
Installation:
You need to have avconv installed: 
sudo apt-get install avconv
Then, you need to install several python package. A virtualeenv installation is recommended:
virtualenv -p python3 py
source py/bin/activate
pip install sk-video
Execution (in the virtualenv):
python script_formating.py"	https://paperswithcode.com/dataset/crowd11	01/07/2017	A Dataset for Fine Grained Crowd Behaviour Analysis					
5228	AAAC	DeepA2 is a modular framework for deep argument analysis. DeepA2 datasets contain comprehensive logical reconstructions of informally presented arguments in short argumentative texts. This item references two two synthetic DeepA2 datasets for artificial argument analysis: AAAC01 and AAAC02.	https://paperswithcode.com/dataset/aaac	04/10/2021	Artificial Argument Analysis Corpus					
5229	SuperCaustics	"SuperCaustics is a simulation tool made in Unreal Engine for generating massive computer vision datasets that include transparent objects.
SuperCaustics is a real-time, open-source simulation of transparent objects designed for deep learning applications. SuperCaustics features extensive modules for stochastic environment creation; uses hardware ray-tracing to support caustics, dispersion, and refraction; and enables generating massive datasets with multi-modal, pixel-perfect ground truth annotations.
<p align=""center"">
  <img src=""https://github.com/MMehdiMousavi/SuperCaustics/raw/main/Assets/SuperCaustics.gif"" alt=""drawing"" width=""600""/>
</p>"	https://paperswithcode.com/dataset/supercaustics	23/07/2021						
5230	"Supplementary data: ""Revealing drivers and risks for power grid frequency stability with explainable AI"""	"This repository contains processed data and result files for the paper ""Revealing drivers and risks for power grid frequency stability with explainable AI""."	https://paperswithcode.com/dataset/supplementary-data	22/07/2021						
5231	Pre-Processed Power Grid Frequency Time Series	This repository contains ready-to-use frequency time series as well as the corresponding pre-processing scripts in python.	https://paperswithcode.com/dataset/pre-processed-power-grid-frequency-time	22/04/2020						
5232	PartImageNet	PartImageNet is a large, high-quality dataset with part segmentation annotations. It consists of 158 classes from ImageNet with approximately 24000 images. PartImageNet offers part-level annotations on a general set of classes with non-rigid, articulated objects, while having an order of magnitude larger size compared to existing datasets. It can be utilized in multiple vision tasks including but not limited to: Part Discovery, Semantic Segmentation, Few-shot Learning.	https://paperswithcode.com/dataset/partimagenet	02/12/2021						
5233	CDC fluview	"country- and state-level historical ILI data from
2010 to 2018 from the CDC (CDC)."	https://paperswithcode.com/dataset/cdc-fluview	23/01/2020	National, Regional, and State Level Outpatient Illness and Viral Surveillance					
5234	MDBD	In order to study the interaction of several early visual cues (luminance, color, stereo, motion) during boundary detection in challenging natural scenes, we have built a multi-cue video dataset composed of short binocular video sequences of natural scenes using a consumer-grade Fujifilm stereo camera (Mély, Kim, McGill, Guo and Serre, 2016). We considered a variety of places (from university campuses to street scenes and parks) and seasons to minimize possible biases. We attempted to capture more challenging scenes for boundary detection by framing a few dominant objects in each shot under a variety of appearances. Representative sample keyframes are shown on the figure below. The dataset contains 100 scenes, each consisting of a left and right view short (10-frame) color sequence. Each sequence was sampled at a rate of 30 frames per second. Each frame has a resolution of 1280 by 720 pixels.	https://paperswithcode.com/dataset/mdbd		Multicue Dataset for Edge Detection					
5235	NASA C-MAPSS-2	The generation of data-driven prognostics models requires the availability of datasets with run-to-failure trajectories. In order to contribute to the development of these methods, the dataset provides a new realistic dataset of run-to-failure trajectories for a small fleet of aircraft engines under realistic flight conditions. The damage propagation modelling used for the generation of this synthetic dataset builds on the modeling strategy from previous work . The dataset was generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data set is been provided by the Prognostics CoE at NASA Ames in collaboration with ETH Zurich and PARC.	https://paperswithcode.com/dataset/nasa-c-mapss-2		Turbofan Engine Degradation Simulation Data Set-2					
5236	INSTANCE	INSTANCE is a data collection of more than 1.3 million seismic waveforms originating from a selection of about 54,000 earthquakes occurred since 2005 in Italy and surrounding regions and seismic noise recordings randomly extracted from event free time windows of the continuous waveforms archive. The purpose is to provide reference datasets useful to develop and test seismic data processing routines based on machine learning and deep learning frameworks. The primary source of this information is ISIDe (Italian Seismological Instrumental and Parametric Data-Base) for earthquakes and the Italian node of EIDA (http://eida.ingv.it) for seismic data. All the waveforms have been sized to a 120 s window, preprocessed and resampled at 100 Hz. For each trace we provide a large number of parameters as metadata, either derived from event information or computed from trace data. Associated metadata allow for the identification of the source, the station, the path travelled by seismic waves and assessment of the trace quality. The total size of the data collection is about 330 GB. Waveforms files are available either in counts or ground motion units in hdf5 format to facilitate fast access from commonly used machine learning frameworks.	https://paperswithcode.com/dataset/instance	01/03/2021	the Italian seismic dataset for machine learning					
5237	GIF Reply Dataset	"The released GIF Reply dataset contains 1,562,701 real text-GIF conversation turns on Twitter. In these conversations, 115,586 unique GIFs are used. Metadata, including OCR extracted text, annotated tags, and object names, are also available for some GIFs in this dataset.
For more details about the dataset, check out our Github repo (https://github.com/xingyaoww/gif-reply/), and our paper (https://arxiv.org/pdf/2109.12212.pdf),"	https://paperswithcode.com/dataset/gif-reply-dataset	24/09/2021						
5238	JUSTICE	The dataset contains 3304 cases from the Supreme Court of the United States from 1955 to 2021. Each case has the case's identifiers as well as the facts of the case and the decision outcome. Other related datasets rarely included the facts of the case which could prove to be helpful in natural language processing applications. One potential use case of this dataset is determining the outcome of a case using its facts.	https://paperswithcode.com/dataset/justice	06/12/2021	JUSTICE: A Dataset for Supreme Court’s Judgment Prediction					
5239	MultiviewC	The MultiviewC dataset mainly contributes to multiview cattle action recognition, 3D objection detection and tracking. We build a novel synthetic dataset MultiviewC through UE4 based on real cattle video dataset which is offered by CISRO. The format of our data set has been adjusted on the basis of MultiviewX for set-up, annotation and files structure.	https://paperswithcode.com/dataset/multiviewc	07/12/2021						
5240	VFP290K	Vision-based Fallen Person (VFP290K) is a novel, large-scale dataset for the detection of fallen persons composed of fallen person images collected in various real-world scenarios. VFP290K consists of 294,714 frames of fallen persons extracted from 178 videos, including 131 scenes in 49 locations.	https://paperswithcode.com/dataset/vfp290k	11/10/2021	VFP290K: A Large-Scale Benchmark Dataset for Vision-based Fallen Person Detection					
5241	Adressa	"The Adressa Dataset is a news dataset that includes news articles (in Norwegian) in connection with anonymized users. We hope that this dataset will be helpful to achieve a better understanding of the news articles in conjunction with their readers.
This dataset is published with the collaboration of Norwegian University of Science and Technology (NTNU) and Adressavisen (local newspaper in Trondheim, Norway) as a part of RecTech project on recommendation technology. For further details of the project and the dataset please refer to the paper mentioned below for citations."	https://paperswithcode.com/dataset/adressa	01/02/2018	SmartMedia Adressa News Dataset					
5242	PsyQA	PsyQA is a Chinese Dataset for generating long counseling text for mental health support.	https://paperswithcode.com/dataset/psyqa	03/06/2021	PsyQA					
5243	CALVIN	CALVIN (Composing Actions from Language and Vision), is an open-source simulated benchmark to learn long-horizon language-conditioned robot manipulation tasks.	https://paperswithcode.com/dataset/calvin-composing-actions-from-language-and	06/12/2021	Composing Actions from Language and Vision					
5244	BIG-bench	The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities.	https://paperswithcode.com/dataset/big-bench							
5245	MIT-BIH Malignant Ventricular Ectopy Database (VFDB)	This database includes 22 half-hour ECG recordings of subjects who experienced episodes of sustained ventricular tachycardia, ventricular flutter, and ventricular fibrillation. The reference annotation (.atr) files contain only rhythm labels (no beat labels).	https://paperswithcode.com/dataset/mit-bih-malignant-ventricular-ectopy-database	03/08/1999						
5246	AF Classification from a Short Single Lead ECG Recording - The PhysioNet Computing in Cardiology Challenge 2017	The 2017 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify, from a single short ECG lead recording (between 30 s and 60 s in length), whether the recording shows normal sinus rhythm, atrial fibrillation (AF), an alternative rhythm, or is too noisy to be classified.	https://paperswithcode.com/dataset/af-classification-from-a-short-single-lead	01/02/2017						
5247	MultiSports	Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our dataset is characterized with important properties of high diversity, dense annotation, and high quality. Our MultiSports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future.	https://paperswithcode.com/dataset/multisports	16/05/2021						
5248	ECG in High Intensity Exercise Dataset	"The data presented here was extracted from a larger dataset collected through a collaboration between the Embedded Systems Laboratory (ESL) of the Swiss Federal Institute of Technology in Lausanne (EPFL), Switzerland and the Institute of Sports Sciences of the University of Lausanne (ISSUL). In this dataset, we report the extracted segments used for an analysis of R peak detection algorithms during high intensity exercise.
Protocol of the experiments
The protocol of the experiment was the following.
22 subjects performing a cardio-pulmonary maximal exercise test on a cycle ergometer, using a gas mask. A single-lead electrocardiogram (ECG) was measured using the BIOPAC system.
An initial 3 min of rest were recorded.
After this baseline, the subjects started cycling at a power of 60W or 90W depending on their fitness level.
Then, the power of the cycle ergometer was increased by 30W every 3 min till exhaustion (in terms of maximum oxygen uptake or VO2max).
Finally, physiology experts assessed the so-called ventilatory thresholds and the VO2max based on the pulmonary data (volume of oxygen and CO2).

Description of the extracted dataset
The characteristics of the dataset are the following:
We report only 20 out of 22 subjects that were used for the analysis, because for two subjects the signals were too corrupted or not complete. Specifically, subjects 5 and 12 were discarded.
The ECG signal was sampled at 500 Hz and then downsampled at 250 Hz. The original ECG signal were measured at maximum 10 mV. Then, they were scaled down by a factor of 1000, hence the data is represented in uV.
For each subject, 5 segments of 20 s were extracted from the ECG recordings and chosen based on different phases of the maximal exercise test (i.e., before and after the so-called second ventilatory threshold or VT2, before and in the middle of VO2max, and during the recovery after exhaustion) to represent different intensities of physical activity.

seg1 --> [VT2-50,VT2-30]
seg2 --> [VT2+60,VT2+80]
seg3 --> [VO2max-50,VO2max-30]
seg4 --> [VO2max-10,VO2max+10]
seg5 --> [VO2max+60,VO2max+80]
The R peak locations were manually annotated in all segments and reviewed by a physician of the Lausanne University Hospital, CHUV. Only segment 5 of subject 9 could not be annotated since there was a problem with the input signal. So, the total number of segments extracted were 20 * 5 - 1 = 99.

Format of the extracted dataset
The dataset is divided in two main folders:
The folder `ecg_segments/` contains the ECG signals saved in two formats, `.csv` and `.mat`. This folder includes both raw (`ecg_raw`) and processed (`ecg`) signals. The processing consists of a morphological filtering and a relative energy non filtering method to enhance the R peaks. The `.csv` files contain only the signal, while the `.mat` files include the signal, the time vector within the maximal stress test, the sampling frequency and the unit of the signal amplitude (uV, as we mentioned before).
The folder `manual_annotations/` contains the sample indices of the annotated R peaks in `.csv` format. The annotation was done on the processed signals."	https://paperswithcode.com/dataset/ecg-in-high-intensity-exercise-dataset	25/11/2021						
5249	Concepticon	This resource, our Concepticon, links concept labels from different conceptlists to concept sets. Each concept set is given a unique identifier, a unique label, and a human-readable definition. Concept sets are further structured by defining different relations between the concepts, as you can see in the graphic to the right, which displays the relations between concept sets linked to the concept set SIBLING. The resource can be used for various purposes. Serving as a rich reference for new and existing databases in diachronic and synchronic linguistics, it allows researchers a quick access to studies on semantic change, cross-linguistic polysemies, and semantic associations.	https://paperswithcode.com/dataset/concepticon	01/05/2016	Concepticon. A Resource for the Linking of Concept Lists					
5250	BOVText	BOVText is a new large-scale benchmark dataset named Bilingual, Open World Video Text(BOVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. All data are collected from KuaiShou and YouTube	https://paperswithcode.com/dataset/bovtext	09/12/2021						
5251	HIU-DMTL-Data	See the paper for more details.	https://paperswithcode.com/dataset/hiu-dmtl-data	24/07/2021	Hand Image Understanding via Deep Multi-Task Learning.					
5252	Aircraft Context Dataset	The Aircraft Context Dataset, a composition of two inter-compatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, is intended for training and evaluating classification, detection and segmentation models in aerial domains. Additionally, a set of relevant meta-parameters can be used to quantify dataset variability as well as the impact of environmental conditions on model performance.	https://paperswithcode.com/dataset/aircraft-context-dataset	17/10/2021						
5253	PTR	PTR is a new large-scale diagnostic visual reasoning dataset for research around part-based conceptual, relational and physical reasoning. PTR contains around 70k RGBD synthetic images with ground truth object and part level annotations regarding semantic instance segmentation, color attributes, spatial and geometric relationships, and certain physical properties such as stability. These images are paired with 700k machine-generated questions covering various types of reasoning types.	https://paperswithcode.com/dataset/ptr	01/12/2021						
5254	RealFaceDB	"The dataset contains patches of facial reflectance as described in the paper, namely the diffuse albedo, diffuse normals, specular albedo, specular normals, as well as the shape in UV space. For the shape, reconstructed meshes have been registered to a common topology and the XYZ values of the points have been mapped to the RGB in UV coordinates and interpolated to complete the UV map. From the complete UV maps of 6144x4096 pixels, patches of 512x512 pixels have been sampled. The dataset contains 7500 such patches (1500 of each datatype) that are anonymized, randomized and sampled so that they do not contain identifiable features.
To obtain access to the dataset, you need to complete and sign a licence agreement, which should be completed by a full-time academic staff member (not a student). To obtain the licence agreement and the dataset please send an email to Alexandros Lattas (a.lattas@imperial.ac.uk) and Stylianos Moschoglou (s.moschoglou@imperial.ac.uk). Please contact us through your academic email and include your name and position. We will verify your request and contact you regarding how to download the dataset. Note that the agreement requires that:
The data must be used for non-commercial research and education purposes only.
You agree not to copy, sell, trade, or exploit the model for any commercial purposes.
You must destroy the data after 2 years since the first download.
If you will be publishing any work using this dataset, please cite the following paper."	https://paperswithcode.com/dataset/realfacedb	30/06/2020						
5255	Yelp-Fraud	"Yelp-Fraud is a multi-relational graph dataset built upon the Yelp spam review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models.

Dataset Statistics

| # Nodes  |  %Fraud Nodes (Class=1) |
|-------|--------|
|  45,954 | 14.5   | 
| Relation  | # Edges |
|--------|--------|
  |   R-U-R    |  49,315 |
 |  R-T-R  |  573,616  |
|  R-S-R  |  3,402,743 |
 |  All |  3,846,979  |

Graph Construction

The Yelp spam review dataset includes hotel and restaurant reviews filtered (spam) and recommended (legitimate) by Yelp. We conduct a spam review detection task on the Yelp-Fraud dataset which is a binary classification task. We take 32 handcrafted features from SpEagle paper as the raw node features for Yelp-Fraud. Based on previous studies which show that opinion fraudsters have connections in user, product, review text, and time, we take reviews as nodes in the graph and design three relations: 1) R-U-R: it connects reviews posted by the same user; 2) R-S-R: it connects reviews under the same product with the same star rating (1-5 stars); 3) R-T-R: it connects two reviews under the same product posted in the same month. 
To download the dataset, please visit this Github repo. For any other questions, please email ytongdou(AT)gmail.com for inquiry."	https://paperswithcode.com/dataset/yelpchi	19/08/2020	Multi-relational Graph Dataset for Yelp Spam Review Detection					
5256	Amazon-Fraud	"Amazon-Fraud is a multi-relational graph dataset built upon the Amazon review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models.

Dataset Statistics

| # Nodes  |  %Fraud Nodes  (Class=1)|
|-------|--------|
|  11,944 | 9.5   | 
| Relation  | # Edges |
|--------|--------|
  |   U-P-U    |  175,608 |
 |  U-S-U  |  3,566,479  |
|  U-V-U  |  1,036,737 |
 |  All |  4,398,392  |

Graph Construction

The Amazon dataset includes product reviews under the Musical Instruments category. Similar to this paper, we label users with more than 80% helpful votes as benign entities and users with less than 20% helpful votes as fraudulent entities. we conduct a fraudulent user detection task on the Amazon-Fraud dataset, which is a binary classification task. We take 25 handcrafted features from this paper as the raw node features for Amazon-Fraud. We take users as nodes in the graph and design three relations: 1) U-P-U: it connects users reviewing at least one same product; 2) U-S-V: it connects users having at least one same star rating within one week; 3) U-V-U: it connects users with top 5% mutual review text similarities (measured by TF-IDF) among all users.
To download the dataset, please visit this Github repo. For any other questions, please email ytongdou(AT)gmail.com for inquiry."	https://paperswithcode.com/dataset/amazon-fraud	19/08/2020	Multi-relational Graph Dataset for Amazon Fraudulent Account Detection					
5257	ForgeryNet	We construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.	https://paperswithcode.com/dataset/forgerynet	09/03/2021	ForgeryNet					
5258	ECG Heartbeat Categorization Dataset	"This dataset consists of a series of CSV files. Each of these CSV files contain a matrix, with each row representing an example in that portion of the dataset. The final element of each row denotes the class to which that example belongs.
Acknowledgements: Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. ""ECG Heartbeat Classification: A Deep Transferable Representation."" arXiv preprint arXiv:1805.00794 (2018).
Inspiration: Can you identify myocardial infarction?"	https://paperswithcode.com/dataset/ecg-heartbeat-categorization-dataset							
5259	Image-based size estimation of broccoli heads under varying degrees of occlusion	"This publicly available dataset contains 1613 RGB-D images of field-grown broccoli plants. The dataset also includes the polygon and circle annotations of the broccoli heads.
The broccoli heads in the images were subject to various degrees of natural and man-made leaf occlusion. The images were acquired in July and August 2020 on a broccoli field in Sexbierum (The Netherlands). The broccoli cultivar was Ironman.
The dataset belongs to the paper “Image-based size estimation of broccoli heads under varying degrees of occlusion”. This paper has been published at Biosystems Engineering journal: https://doi.org/10.1016/j.biosystemseng.2021.06.001Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/https-doi-org-10-4121-13603787-v2	22/06/2021						
5260	3D Datasets of Broccoli in the Field	This work was undertaken by members of the Lincoln Centre for Autonomous Systems, University of Lincoln, UK. The four data collection sessions were conducted at three different sites in Lincolnshire, UK and one in Murcia, Spain (see Fig. 1). The sessions were conducted at the beginning and towards the end of harvesting season in UK and at the end of the harvest in Spain. The variety of broccoli plants grown in UK is called Iron Man whilst the variety grown in Spain is called Titanium.The weather during UK data capture included a mixture of different conditions including sunny, overcast and raining with broccoli varying in maturity levels from small to larger to already harvested, while the conditions for data capture in Spain included strong sunlight and mature plants at the very end of the harvesting season. The tractor was driven through the broccoli field at a slow walking speed with two rows of broccoli plants being imaged by the RGB-D sensor.	https://paperswithcode.com/dataset/3d-datasets-of-broccoli-in-the-field	04/11/2015	3D Datasets of Broccoli in the Field					
5261	Ladybird Cobbitty 2017 Brassica Dataset	This data set contains weekly scans of cauliflower and broccoli covering a ten week growth cycle from transplant to harvest. The data set includes ground-truth, physical characteristics of the crop; environmental data collected by a weather station and a soil-senor network; and scans of the crop performed by an autonomous agricultural robot, which include stereo colour, thermal and hyperspectral imagery. The crop were planted at Lansdowne Farm, a University of Sydney agricultural research and teaching facility. Lansdowne Farm is located in Cobbitty, a suburb 70km south-west of Sydney in New South Wales (NSW), Australia. Four 80 metre raised crop beds were prepared with a North-South orientation. Approximately 144 Brassica were planted in each bed. Cauliflower were planted in the first and third bed (from west to east). Broccoli were planted in the second and fourth beds.	https://paperswithcode.com/dataset/ladybird-cobbitty-2017-brassica-dataset	21/03/2019						
5262	ShadowLink	"ShadowLink dataset is designed to evaluate the impact of entity overshadowing on the task of entity disambiguation. Paper: ""Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing"" by Vera Provatorova, Svitlana Vakulenko, Samarth Bhargav, Evangelos Kanoulas. EMNLP 2021."	https://paperswithcode.com/dataset/shadowlink	24/08/2021						
5263	Large Labelled Logo Dataset (L3D)	"It is composed of around 770k of color 256x256 RGB images extracted from the European Union Intellectual Property Office (EUIPO) open registry. Each of them is associated to multiple labels that classify the figurative and textual elements that appear in the images. These annotations have been classified by the EUIPO evaluators using the Vienna classification, a hierarchical classification of figurative marks.
We suggest it to be used for:
1. Unconditional trademark generation
2. Conditional trademark generation.
3. Multi-label logo classification (Vienna classification).
4. Optical Character Recognition.
5. Conditional trade
6. Image segmentation.
7. Image retrieval"	https://paperswithcode.com/dataset/large-labelled-logo-dataset-l3d	10/12/2021						
5264	ValueNet	We present a new large-scale human value dataset called ValueNet, which contains human attitudes on 21,374 text scenarios. The dataset is organized in ten dimensions that conform to the basic human value theory in intercultural research.	https://paperswithcode.com/dataset/valuenet	12/12/2021						
5265	MFA	"The MFA (Many Faces of Anger) dataset includes 200 in-the-wild videos from North American and Persian cultures with fine-grained labels of: 'annoyed', 'anger', 'disgust', 'hatred' and 'furious' and 13 related emojis.
Image source: https://arxiv.org/pdf/2112.05267.pdf"	https://paperswithcode.com/dataset/mfa	10/12/2021	Many Faces of Anger					
5266	Statutory Interpretation Data Set	"This dataset contains a set of sentences by extracting all the sentences mentioning the term from the court decisions retrieved from the Caselaw access project data.
In total the corpus consists of 26,959 sentences.
The sentences are classified into four categories according to their usefulness for the interpretation:

high value - sentence intended to define or elaborate on the meaning of the term
certain value - sentence that provides grounds to elaborate on the term's meaning
potential value - sentence that provides additional information beyond what is known from the provision the term comes from
no value - no additional information over what is known from the provision"	https://paperswithcode.com/dataset/statutory-interpretation-data-set	14/12/2021						
5267	The RoboCup Rescue Dataset	"In this paper, we introduce a victim dataset for the RoboCup Rescue competitions. The RoboCup Rescue robots have to collect points within several disciplines, e.g. a search task within an area to survey simulated baby doll (victim).
When a robot comes across a victim, a heat detector does not completely proof if this is a living being and not just a heat emitting somewhat else. Further investigations are necessary so that a face detection could prove the existence of a victim. Lots of face detection approaches can be found in literature, which manly are used for human face recognition. These cannot be straightforward used for victim faces which are, in case of the RoboCup Rescue competitions, typically dolls. Thus we present the results of standard approaches and developed an own approach via bag-of-visual-words (BoVW)."	https://paperswithcode.com/dataset/the-robocub-rescue-dataset	06/08/2018						
5268	PHANTOM	"To evaluate the presented approaches, we created the Physical Anomalous Trajectory or Motion (PHANTOM) dataset consisting of six classes featuring everyday objects or physical setups, and showing nine different kinds of anomalies. We designed our classes to evaluate detection of various modes of video abnormalities that
are generally excluded in video AD settings.
The train and test sets of each class contain approximately 30 videos of varying lengths. The train set contains only normal videos, while the test set is evenly balanced between normal and anomalous videos. The classes were designed to be of varying difficulties and to feature different types of anomalies. For example, the window class was filmed in multiple lighting scenarios to increase variance. The normal videos include motion that follows an expected trajectory (pendulum, keyboard) or an expected movement (window). The sushi class features procedural motion, while candle and magnets feature more subtle motion that only appears locally.
The anomalous videos can feature an interference of the regular motion (window, candle, magnets), an added or removed step in the usual procedure (sushi), motion that follows a different trajectory (pendulum, keyboard), or contains a different object (pendulum)."	https://paperswithcode.com/dataset/phantom	14/12/2021	Physical Anomalous Trajectory or Motion (PHANTOM)					
5269	GOF	Optical Flow in challenging scenes with gyroscope readings!	https://paperswithcode.com/dataset/gof	25/03/2021	Gyroscope Optical Flow					
5270	PatternNet	PatternNet is a large-scale high-resolution remote sensing dataset collected for remote sensing image retrieval. There are 38 classes and each class has 800 images of size 256×256 pixels. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities. The following table shows the classes and the corresponding spatial resolutions. The figure shows some example images from each class.	https://paperswithcode.com/dataset/patternnet							
5271	HT1080WT cells - 3D collagen type I matrices	"Human fibrosarcoma HT1080WT (ATCC) cells at low cell densities embedded in 3D collagen type I matrices 1. The time-lapse videos were recorded every 2 minutes for 16.7 hours and covered a field of view of 1002 pixels × 1004 pixels with a pixel size of 0.802 μm/pixel The videos were pre-processed to correct frame-to-frame drift artifacts, resulting in a final size of 983 pixels × 985 pixels pixels.
1 Hasini Jayatilaka, Anjil Giri, Michelle Karl, Ivie Aifuwa, Nicholaus J Trenton, Jude M Phillip, Shyam Khatau, and Denis Wirtz. EB1 and cytoplasmic dynein mediate protrusion dynamics for efficient 3-dimensional cell migration. FASEB J., 32(3):1207–1221, 2018. ISSN 0892-6638. doi: 10.1096/fj.201700444RR."	https://paperswithcode.com/dataset/ht1080wt-cells-3d-collagen-type-i-matrices	16/12/2021	HT1080WT cells embedded in 3D collagen type I matrices - manual annotations for cell instance segmentation and tracking					
5272	LfGP Data	The expert data and trained models used for our Learning from Guided Play paper. For details on use, see our open source repository at https://github.com/utiasSTARS/lfgp.	https://paperswithcode.com/dataset/lfgp-data	16/12/2021	Learning from Guided Play Expert Data and Trained Models					
5273	deepMTJ	"deepMTJ: Muscle-Tendon Junction Tracking in Ultrasound Images
deepMTJ is a machine learning approach for automatically tracking of muscle-tendon junctions (MTJ) in ultrasound images. Our method is based on a convolutional neural network trained to infer MTJ positions across various ultrasound systems from different vendors, collected in independent laboratories from diverse observers, on distinct muscles and movements. We built deepMTJ to support clinical biomechanists and locomotion researchers with an open-source tool for gait analyses.
Introduction into the deepMTJ dataset
This repository contains the full test dataset used for deepMTJ performance assessments, the trained TensorFlow (Keras) model and a Link to the code repository of deepMTJ. Furthermore, we provide online predictions using deepMTJ via a  (For multiple and large file predictions) and via deepmtj.org (Cloud based predictions).


The dataset comprises 1344 images of muscle-tendon junctions recorded with 3 ultrasound imaging systems (Aixplorer V6, Esaote MyLab60, Telemed ArtUs), on 2 muscles (Lateral Gastrocnemius, Medial Gastrocnemius), and 2 movements (isometric maximum voluntary contractions, passive torque movements). 


We have included the ground truth labels for each image. These reference labels are the computed mean from 4 specialist labels. Specialist annotators had 2-10 years of experience in biomechanical and clinical research investigating muscles and tendons in 2-9 ultrasound studies in the past 2 years."	https://paperswithcode.com/dataset/deepmtj	24/11/2021	Muscle-Tendon Junction Tracking in Ultrasound Images					
5274	PeopleSansPeople	In recent years, person detection and human pose estimation have made great strides, helped by large-scale labeled datasets. However, these datasets had no guarantees or analysis of human activities, poses, or context diversity. Additionally, privacy, legal, safety, and ethical concerns may limit the ability to collect more human data. An emerging alternative to real-world data that alleviates some of these issues is synthetic data. However, creation of synthetic data generators is incredibly challenging and prevents researchers from exploring their usefulness. Therefore, we release a human-centric synthetic data generator PeopleSansPeople which contains simulation-ready 3D human assets, a parameterized lighting and camera system, and generates 2D and 3D bounding box, instance and semantic segmentation, and COCO pose labels. Using PeopleSansPeople, we performed benchmark synthetic data training using a Detectron2 Keypoint R-CNN variant 1. We found that pre-training a network using synthetic data and fine-tuning on target real-world data (few-shot transfer to limited subsets of COCO-person train 2) resulted in a keypoint AP of 60.37±0.48 (COCO test-dev2017) outperforming models trained with the same real data alone (keypoint AP of 55.80) and pre-trained with ImageNet (keypoint AP of 57.50). This freely-available data generator should enable a wide range of research into the emerging field of simulation to real transfer learning in the critical area of human-centric computer vision.	https://paperswithcode.com/dataset/peoplesanspeople	17/12/2021	PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision					
5275	CPPE-5	"CPPE - 5 (Medical Personal Protective Equipment) is a new challenging dataset with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories.
Some features of this dataset are:

high quality images and annotations (~4.6 bounding boxes per image)
real-life images unlike any current such dataset
majority of non-iconic images (allowing easy deployment to real-world environments)"	https://paperswithcode.com/dataset/cppe-5	15/12/2021	Medical Personal Protective Equipment Dataset					
5276	MPOSE2021	MPOSE2021, a dataset for real-time short-time HAR, suitable for both pose-based and RGB-based methodologies. It includes 15,429 sequences from 100 actors and different scenarios, with limited frames per scene (between 20 and 30). In contrast to other publicly available datasets, the peculiarity of having a constrained number of time steps stimulates the development of real-time methodologies that perform HAR with low latency and high throughput.	https://paperswithcode.com/dataset/mpose2021	01/07/2021	MPOSE2021 Dataset for Short-time Human Action Recognition					
5277	Semantic Segmentation Vineyard Rows	"Test dataset for Semantic Segmentation.
The datasets includes 500 RGB - images with the relative single-channel binary masks.
Images are taken from the vineyards in Grugliasco - Turin - Piedmont Region -Italy"	https://paperswithcode.com/dataset/semantic-segmentation-vineyard-rows	12/03/2021						
5278	QST	QST contains 1,167 video clips that are cut out from 216 time-lapse 4K videos collected from YouTube, which can be used for a variety of tasks, such as (high-resolution) video generation, (high-resolution) video prediction, (high-resolution) image generation, texture generation, image inpainting, image/video super-resolution, image/video colorization, image/video animating, etc. Each short clip contains multiple frames (from a minimum of 58 frames to a maximum of 1,200 frames, a total of 285,446 frames), and the resolution of each frame is more than 1,024 x 1,024. Specifically, QST consists of a training set (containing 1000 clips, totally 244,930 frames), a validation set (containing 100 clips, totally 23,200 frames), and a testing set (containing 67 clips, totally 17,316 frames). Click here (Key: qst1) to download the QST dataset.	https://paperswithcode.com/dataset/qst	11/08/2020	Quick Sky Time					
5279	Stanford 3D Scanning Repository	In recent years, the number of range scanners and surface reconstruction algorithms has been growing rapidly. Many researchers, however, do not have access to scanning facilities or dense polygonal models. The purpose of this repository is to make some range data and detailed reconstructions available to the public.	https://paperswithcode.com/dataset/stanford-3d-scanning-repository							
5280	Montreal Archive of Sleep Studies	"The Montreal Archive of Sleep Studies (MASS) is an open-access and collaborative database of laboratory-based polysomnography (PSG) recordings O’Reilly, C., et al. (2014) J Seep Res, 23(6):628-635. Its goal is to provide a standard and easily accessible source of data for benchmarking the various systems developed to help the automation of sleep analysis. It also provides a readily available source of data for fast validation of experimental results and for exploratory analyses. Finally, it is a shared resource that can be used to foster large-scale collaborations in sleep studies.
MASS is composed of cohorts themselves comprising subsets. Recordings within subsets is kept as homogeneous as possible, whereas it is more heterogeneous between subsets. To allow inter-study comparisons, researchers validating their results on MASS are encouraged to specify which portion of the database they used in their assessment (e.g., MASS-C1 for the whole cohort 1, MASS-C1/SS1-SS3 for subsets 1, 2 and 3 of cohort 1).
Currently, the first MASS cohort available is described in O’Reilly, C., et al. (2014) J Seep Res, 23(6):628-635. This cohort comprises polysomnograms of 200 complete nights recorded in 97 men and 103 women of age varying between 18 and 76 years (mean: 38.3 years, SD: 18.9 years). It has been split into five different subsets.
Source: Montreal Archive of Sleep Studies: an open-access resource for instrument benchmarking and exploratory research"	https://paperswithcode.com/dataset/montreal-archive-of-sleep-studies	09/06/2014	Montreal Archive of Sleep Studies					
5281	SyntheticChairSketch	"The dataset contains naive and stylized sketches for a chair category of the ShapeNetCore dataset. Each chair shape folder contains two subfolders: ""naive"" and ""stylized"", representing two rendering styles.
https://cvssp.org/data/SyntheticChairSketch/"	https://paperswithcode.com/dataset/syntheticchairsketch	12/11/2020						
5282	Cross-cultural pop song mood ratings (US, KR, BR)	"Mood ratings of 8 emotions gathered across 360 pop songs
166 raters from US, S.Korea and Brazil
MIR features from Spotify"	https://paperswithcode.com/dataset/pop-song-mood-ratings-us-kr-br	02/08/2021						
5283	DISRPT2019	The DISRPT 2019 workshop introduces the first iteration of a cross-formalism shared task on discourse unit segmentation. Since all major discourse parsing frameworks imply a segmentation of texts into segments, learning segmentations for and from diverse resources is a promising area for converging methods and insights. We provide training, development and test datasets from all available languages and treebanks in the RST, SDRT and PDTB formalisms, using a uniform format. Because different corpora, languages and frameworks use different guidelines for segmentation, the shared task is meant to promote design of flexible methods for dealing with various guidelines, and help to push forward the discussion of standards for discourse units. For datasets which have treebanks, we will evaluate in two different scenarios: with and without gold syntax, or otherwise using provided automatic parses for comparison.	https://paperswithcode.com/dataset/disrpt2019	06/06/2019	DISRPT2019 shared task on Discourse Unit Segmentation and Connective Detection					
5284	DISRPT2021	"The DISRPT 2021 shared task, co-located with CODI 2021 at EMNLP, introduces the second iteration of a cross-formalism shared task on discourse unit segmentation and connective detection, as well as the first iteration of a cross-formalism discourse relation classification task.
We provide training, development and test datasets from all available languages and treebanks in the RST, SDRT and PDTB formalisms, using a uniform format. Because different corpora, languages and frameworks use different guidelines, the shared task is meant to promote design of flexible methods for dealing with various guidelines, and help to push forward the discussion of standards for computational approaches to discourse relations. We include data for evaluation with and without gold syntax, or otherwise using provided automatic parses for comparison to gold syntax data."	https://paperswithcode.com/dataset/disrpt2021	21/11/2021	DISRPT2021 shared task on Discourse Unit Segmentation, Connective Detection and Discourse Relation Classification					
5285	PuzzTe	Puzzles dataset: comparison, knight&knaves, and zebra puzzles.	https://paperswithcode.com/dataset/puzzte	10/12/2021	Puzzle Textual Entailment					
5286	CODD	The Cooperative Driving dataset is a synthetic dataset generated using CARLA that contains lidar data from multiple vehicles navigating simultaneously through a diverse set of driving scenarios. This dataset was created to enable further research in multi-agent perception (cooperative perception) including cooperative 3D object detection, cooperative object tracking, multi-agent SLAM and point cloud registration. Towards that goal, all the frames have been labelled with ground-truth sensor pose and 3D object bounding boxes.	https://paperswithcode.com/dataset/codd	23/11/2021	Cooperative Driving Dataset					
5287	GUM	"GUM is an open source multilayer English corpus of richly annotated texts from twelve text types. Annotations include:

Multiple POS tags, morphological features and lemmatization
Sentence segmentation and rough speech act
Document structure in TEI XML (paragraphs, headings, figures, etc.)
ISO date/time annotations
Speaker and addressee information (where relevant)
Constituent and dependency syntax
Information status (given, accessible, new, split antecedent)
Entity and coreference annotation, including bridging anaphora
Entity linking (Wikification)
Discourse parses in Rhetorical Structure Theory and discourse dependencies"	https://paperswithcode.com/dataset/gum		Georgetown University Multilayer corpus					
5288	AMALGUM	AMALGUM is a machine annotated multilayer corpus following the same design and annotation layers as GUM, but substantially larger (around 4M tokens). The goal of this corpus is to close the gap between high quality, richly annotated, but small datasets, and the larger but shallowly annotated corpora that are often scraped from the Web.	https://paperswithcode.com/dataset/amalgum	18/06/2020	A Machine Annotated Lookalike of GUM					
5289	Faster__convergence_MOEAD	"Dataset with raw outputs of experiments connected to the GitHub repository:
https://github.com/yurilavinas/MOEADr/tree/ECJ
The size of the data is big, make sure to have enough space."	https://paperswithcode.com/dataset/faster-convergence-moead							
5290	Webis-STEREO-21	We present the Webis-STEREO-21 dataset, a massive collection of Scientific Text Reuse in Open-access publications. It contains more than 91 million cases of reused text passages found in 4.2 million unique open-access publications. Featuring a high coverage of scientific disciplines and varieties of reuse, as well as comprehensive metadata to contextualize each case, our dataset addresses the most salient shortcomings of previous ones on scientific writing. Webis-STEREO-21 allows for tackling a wide range of research questions from different scientific backgrounds, facilitating both qualitative and quantitative analysis of the phenomenon as well as a first-time grounding on the base rate of text reuse in scientific publications.	https://paperswithcode.com/dataset/webis-stereo-21	22/12/2021						
5291	TUAC	"A new subset of the popular open source electroencephalogram (EEG) corpus – TUH EEG:
- The Temple University Artifact Corpus (TUAR) consists of high yield artifact files annotated using a five-way classification system:
1. Chewing (CHEW): An artifact resulting from the tensing and relaxing of the jaw muscles.
2. Electrode (ELEC): An artifact that encompasses various electrode related phenomena.
3. Eye Movement (EYEM): A spike-like waveform created during patient eye movement.
4. Muscle (MUSC): A common artifact with high frequency, sharp waves corresponding to patient movement.
5. Shiver (SHIV): A specific and sustained sharp wave artifact that occurs when a patient shivers.
- EEG artifacts are waveforms that are not of cerebral origin and may have been affected by several
external and physiological factors.
- These artifacts cause false alarms in seizure prediction machine learning systems.
This corpus was developed to support research and evaluation of artifact suppression technology."	https://paperswithcode.com/dataset/tuar		Temple University Artifact Corpus					
5292	NepaliNewsCorpus	"Nepali News Corpus
Raw nepali text scrapped from several online websites.
Total file size of concatenated text file -> 4.6 GB.
This raw text   corpus can be used for tasks such as casual language modelling and masked language modelling."	https://paperswithcode.com/dataset/nepalinewscorpus							
5293	Epinions social network	"This is a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.
Nodes   75879
Edges   508837
Nodes in largest WCC    75877 (1.000)
Edges in largest WCC    508836 (1.000)
Nodes in largest SCC    32223 (0.425)
Edges in largest SCC    443506 (0.872)
Average clustering coefficient  0.1378
Number of triangles     1624481
Fraction of closed triangles    0.0229
Diameter (longest shortest path)    14
90-percentile effective diameter    5"	https://paperswithcode.com/dataset/epinions-social-network	26/12/2021						
5294	AFLW-19	"The original AFLW provides at most 21 points for each face, but excluding coordinates for invisible landmarks, causing difficulties for training most of the existing baseline approaches. To make fair comparisons, the authors manually annotate the coordinates of these invisible landmarks to enable training of those baseline approaches. The new annotation does not include
two ear points because it is very difficult to decide the location of invisible ears. This causes the point number of AFLW-19 to be 19.
The original AFLW does not provide train-test partition. AFLW-19 adopts a partition with 20,000 images for training and 4,386 images for testing (AFLW-Full). In addition,  a frontal subset  (AFLW-Frontal) is proposed where all landmarks are visible (totally 1,165 images).
The new 19-point annotation file is available at the project page."	https://paperswithcode.com/dataset/aflw-19	01/06/2016	The 19 landmark variant of AFLW.					
5295	Covid Dataset	The COVID-19 CT dataset is constructed by Shenzhen Research Institute of Big Data (SRIBD), Future Network of Intelligence Institute (FNii) and CUHKSZ-JD Joint AI Lab, Chinese University of Hongkong, Shenzhen, China, which contains 368 medical findings in Chinese and 1,104 chest CT scans from the First Affiliated Hospital of Jinan University Guangzhou and the Fifth Affiliated Hospital of Sun Yat-sen University Zhuhai in China. Please see more details in our TNNLS paper - Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning.	https://paperswithcode.com/dataset/covid-dataset							
5296	M2DGR	"We collected long-term challenging sequences for ground robots both indoors and outdoors with a complete sensor suite, which includes six surround-view fish-eye cameras, a sky-pointing fish-eye camera, a perspective color camera, an event camera, an infrared camera, a 32-beam LIDAR, two GNSS receivers, and two IMUs. To our knowledge, this is the first SLAM dataset focusing on ground robot navigation with such rich sensory information.
We recorded trajectories in a few challenging scenarios like lifts, complete darkness, which can easily fail existing localization solutions. These situations are commonly faced in ground robot applications, while they are seldom discussed in previous datasets.
We launched a comprehensive benchmark for ground robot navigation. On this benchmark, we evaluated existing state-of-the-art SLAM algorithms of various designs and analyzed their characteristics and defects individually."	https://paperswithcode.com/dataset/m2dgr	19/12/2021	a Multi-modal and Multi-scenario SLAM Dataset for Ground Robots					
5297	Grasping dataset: suction-based	A small and simple dataset featuring RGB-D images and heightmaps of various objects in a bin with manually annotated suctionable regions	https://paperswithcode.com/dataset/grasping-dataset-suction-based	23/03/2018	suction-based-grasping-dataset					
5298	IWSLT 2017	The IWSLT 2017 translation dataset.	https://paperswithcode.com/dataset/iwslt-2017							
5299	AU Dataset for Visuo-Haptic Object Recognition for Robots	Multimodal object recognition is still an emerging field. Thus, publicly available datasets are still rare and of small size. This dataset was developed to help fill this void and presents multimodal data for 63 objects with some visual and haptic ambiguity. The dataset contains visual, kinesthetic and tactile (audio/vibrations) data. To completely solve sensory ambiguity, sensory integration/fusion would be required. This report describes the creation and structure of the dataset. The first section explains the underlying approach used to capture the visual and haptic properties of the objects. The second section describes the technical aspects (experimental setup) needed for the collection of the data. The third section introduces the objects, while the final section describes the structure and content of the dataset.	https://paperswithcode.com/dataset/au-dataset-for-visuo-haptic-object	18/06/2021						
5300	Pre-trained Transliterated Embeddings for Indian Languages	We release various types of word embeddings for multiple Indian languages. Please note that for a majority of our work, we had transliterated the corpora to the Devanagiri script and the script is changed. Word Embedding models using FastText, ElMo, and cross-lingual models based on an orthogonal alignment of monolingual models for all pairs of these languages.	https://paperswithcode.com/dataset/pre-trained-transliterated-embeddings-for	27/12/2021						
5301	Phishing and Benign Websites	An annotated dataset of 38,800 phishing and benign websites.	https://paperswithcode.com/dataset/phishing-and-benign-websites	14/05/2021						
5302	2012 i2b2 Temporal Relations	The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.	https://paperswithcode.com/dataset/2012-i2b2-temporal-relations	05/04/2021	2012 i2b2 Clinical Temporal Relations					
5303	eICU-CRD	"The eICU Collaborative Research Database is a large multi-center critical care database made available by Philips Healthcare in partnership with the MIT Laboratory for Computational Physiology.
The eICU Collaborative Research Database holds data associated with over 200,000 patient stays, providing a large sample size for research studies."	https://paperswithcode.com/dataset/eicu-crd	11/09/2018	eICU Collaborative Research Database					
5304	Evolutionary Illusion Generator Illusions	A dataset of illusions generated by the AI model EIGen.	https://paperswithcode.com/dataset/evolutionary-illusion-generator-illusions	25/12/2021						
5305	PredNet Grayscale Model Weights	A pretrained PredNet neural network, used in EIGen to generate grayscale illusions.	https://paperswithcode.com/dataset/prednet-grayscale-model-weights	25/12/2021						
5306	PredNet Color Model Weights	A pretrained PredNet neural network, used in EIGen to generate color illusions.	https://paperswithcode.com/dataset/prednet-color-model-weights							
5307	MetaGraspNet 1	"There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One particular impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings.
Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations.
To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. In particular, metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. We present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types, and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. This repository contains the first phase of MetaGraspNet benchmark dataset which includes detailed object detection, segmentation, layout annotations, and a script for layout-weighted performance metric (https://github.com/y2863/MetaGraspNet )."	https://paperswithcode.com/dataset/metagraspnet-1	08/12/2021	MetaGraspNet difficulty 1 - easy					
5308	MetaGraspNet 2	"There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One particular impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings.
Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations.
To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. In particular, metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. We present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types, and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. This repository contains the first phase of MetaGraspNet benchmark dataset which includes detailed object detection, segmentation, layout annotations, and a script for layout-weighted performance metric (https://github.com/y2863/MetaGraspNet )."	https://paperswithcode.com/dataset/metagraspnet-2	08/12/2021	MetaGraspNet difficulty 2 - medium					
5309	MetaGraspNet 3	"There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One particular impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings.
Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations.
To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. In particular, metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. We present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types, and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. This repository contains the first phase of MetaGraspNet benchmark dataset which includes detailed object detection, segmentation, layout annotations, and a script for layout-weighted performance metric (https://github.com/y2863/MetaGraspNet )."	https://paperswithcode.com/dataset/metagraspnet-3	08/12/2021	MetaGraspNet difficulty 3 - hard 1					
5310	MetaGraspNet 4	"There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One particular impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings.
Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations.
To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. In particular, metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. We present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types, and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. This repository contains the first phase of MetaGraspNet benchmark dataset which includes detailed object detection, segmentation, layout annotations, and a script for layout-weighted performance metric (https://github.com/y2863/MetaGraspNet )."	https://paperswithcode.com/dataset/metagraspnet-4	08/12/2021	MetaGraspNet difficulty 4 - hard 2					
5311	MetaGraspNet 5	"There has been increasing interest in smart factories powered by robotics systems to tackle repetitive, laborious tasks. One particular impactful yet challenging task in robotics-powered smart factory applications is robotic grasping: using robotic arms to grasp objects autonomously in different settings.
Robotic grasping requires a variety of computer vision tasks such as object detection, segmentation, grasp prediction, pick planning, etc. While significant progress has been made in leveraging of machine learning for robotic grasping, particularly with deep learning, a big challenge remains in the need for large-scale, high-quality RGBD datasets that cover a wide diversity of scenarios and permutations.
To tackle this big, diverse data problem, we are inspired by the recent rise in the concept of metaverse, which has greatly closed the gap between virtual worlds and the physical world. In particular, metaverses allow us to create digital twins of real-world manufacturing scenarios and to virtually create different scenarios from which large volumes of data can be generated for training models. We present MetaGraspNet: a large-scale benchmark dataset for vision-driven robotic grasping via physics-based metaverse synthesis. The proposed dataset contains 100,000 images and 25 different object types, and is split into 5 difficulties to evaluate object detection and segmentation model performance in different grasping scenarios. We also propose a new layout-weighted performance metric alongside the dataset for evaluating object detection and segmentation performance in a manner that is more appropriate for robotic grasp applications compared to existing general-purpose performance metrics. This repository contains the first phase of MetaGraspNet benchmark dataset which includes detailed object detection, segmentation, layout annotations, and a script for layout-weighted performance metric (https://github.com/y2863/MetaGraspNet )."	https://paperswithcode.com/dataset/metagraspnet-5	08/12/2021	MetaGraspNet difficulty 5 - very hard					
5312	Multiview Manipulation Data	Accompanying expert data and trained models for 2021 IROS paper on Multiview Manipulation.	https://paperswithcode.com/dataset/multiview-manipulation-data	28/04/2021	Multiview Manipulation Expert Data and Trained Models					
5313	GMVD	The GMVD dataset consists of synthetic scenes captured using the GTA-V and Unity graphics engines. The dataset covers a variety of scenes, along with different conditions including day time variations (morning, afternoon, evening, night) and weather variations (sunny, cloudy, rainy, snowy). The purpose of the dataset is twofold. The first is to benchmark the generalization capabilities of Multi-View Detection algorithms. The second purpose is to serve as a synthetic training source from which the trained models can be directly applied on real-world data.	https://paperswithcode.com/dataset/gmvd	24/09/2021	Generalized Multi-View Detection Dataset					
5314	NLC2CMD	"The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural
language processing to the command line. Participants were tasked with building models
that can transform descriptions of command line tasks in English to their Bash syntax."	https://paperswithcode.com/dataset/nlc2cmd	03/03/2021						
5315	2018 n2c2 (Track 2) - Adverse Drug Events and Medication Extraction	"Abstract
Objective
This article summarizes the preparation, organization, evaluation, and results of Track 2 of the 2018 National NLP Clinical Challenges shared task. Track 2 focused on extraction of adverse drug events (ADEs) from clinical records and evaluated 3 tasks: concept extraction, relation classification, and end-to-end systems. We perform an analysis of the results to identify the state of the art in these tasks, learn from it, and build on it.
Materials and Methods
For all tasks, teams were given raw text of narrative discharge summaries, and in all the tasks, participants proposed deep learning–based methods with hand-designed features. In the concept extraction task, participants used sequence labelling models (bidirectional long short-term memory being the most popular), whereas in the relation classification task, they also experimented with instance-based classifiers (namely support vector machines and rules). Ensemble methods were also popular.
Results
A total of 28 teams participated in task 1, with 21 teams in tasks 2 and 3. The best performing systems set a high performance bar with F1 scores of 0.9418 for concept extraction, 0.9630 for relation classification, and 0.8905 for end-to-end. However, the results were much lower for concepts and relations of Reasons and ADEs. These were often missed because local context is insufficient to identify them.
Conclusions
This challenge shows that clinical concept extraction and relation classification systems have a high performance for many concept types, but significant improvement is still required for ADEs and Reasons. Incorporating the larger context or outside knowledge will likely improve the performance of future systems."	https://paperswithcode.com/dataset/2018-n2c2-track-2-adverse-drug-events-and	02/01/2022	2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records					
5316	SPARTQA -	We take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SpaRTQA.   https://aclanthology.org/2021.naacl-main.364/	https://paperswithcode.com/dataset/spartqa-1	01/06/2021	SPAtial Reasoning on Textual Question Answering.					
5317	Moon Phases	"Dates with Moon phases extended days until next phase (1992/1/4 to 2027/12/20)
Incorporate lunar data to your research.
The moon affects multiple physical things on the earth, such as the ocean tides, the behavior of living organisms as well as humans
Moon Phases data
0 = New Moon
1 = first day after New Moon
2 = second day after New Moon
. . .
10 = First Quarter
11 = first day after First Quarter
12 = second day after First Quarter
. . .
20 = Full Moon
21 = first day after Full Moon
22 = second day after Full Moon
. . .
30 = Third Quarter
31 = first day after Third Quarter
32 = second day after Third Quarter
. . ."	https://paperswithcode.com/dataset/moon-phases	14/12/2021	moon phases and derived					
5318	CLIPS	"CLIPS, ovvero Corpora e Lessici dell'Italiano Parlato e Scritto, è uno degli otto progetti (Progetto n. 2) del Cluster C18 ""LINGUISTICA COMPUTAZIONALE: RICERCHE MONOLINGUI E MULTILINGUI"" (Legge 488), finanziato dal Ministero dell'Istruzione, dell'Università e della Ricerca (MIUR)."	https://paperswithcode.com/dataset/clips		Corpora e Lessici dell'Italiano Parlato e Scritto					
5319	Phone call network for 2 years in a Euro country	"We employ a nationwide phone call dataset from Jan. 2015 to Dec. 2016.
The log interaction duration and log interaction frequency in each phase (intermediate results) are both provided. Currently, we upload the Results folder to Google Drive.
(https://drive.google.com/drive/folders/1h4rHZvzzQO7niYMelbzToJZernOij1dv?usp=sharing)
Please download the files from google drive for replication purposes.
In each file, we list tie ranges and interactions in all phases. 
For example, in 'Results/Graph_season_TR_Duration.txt', the former eight columns are tie range and the latter eight columns are log interaction duration.
Tie range is calculated by the length of the second shortest path of two nodes. 
'-1' means that one node of this connection has no interaction with others in this phase.
'100' means that there is no second path between two nodes, indicating that the tie range is infinite.
'101' means that the degree of one node is 1, indicating that the tie range is infinite.  
Differential privacy is applied to protect the privacy of users. 
Concretely, we add a Gaussian noise with &mu;=0, &sigma;=5 to log interactions.
When reproducing the results, please remove all numpy.log in the codes, and minus a &sigma; for the calculation of error bars."	https://paperswithcode.com/dataset/phone-call-network-for-2-years-in-a-euro	03/01/2022	Phone call network for 2 years in a Euro country					
5320	RodoSol-ALPR	"This dataset, called RodoSol-ALPR dataset, contains 20,000 images captured by static cameras located at pay tolls owned by the Rodovia do Sol (RodoSol) concessionaire, which operates 67.5 kilometers of a highway (ES-060) in the Brazilian state of Espírito Santo.
There are images of different types of vehicles (e.g., cars, motorcycles, buses and trucks), captured during the day and night, from distinct lanes, on clear and rainy days, and the distance from the vehicle to the camera varies slightly. All images have a resolution of 1,280 × 720 pixels.
An important feature of the proposed dataset is that it has images of two different license plate (LP) layouts: Brazilian and Mercosur (to maintain consistency with existing works, we refer to “Brazilian” as the standard used in Brazil before the adoption of the Mercosur standard). 
Every image has the following information available in a text file: the vehicle’s type (car or motorcycle), the LP’s layout (Brazilian or Mercosul), its text (e.g., ABC-1234), and the position (x, y) of each of its four corners. We labeled the corners instead of just the LP bounding box to enable the training of methods that explore LP rectification, as well as the application of a wider range of data augmentation techniques.
Regarding privacy concerns related to our dataset, we remark that in Brazil the LPs are related to the respective vehicles, i.e., no public information is available about the vehicle drivers/owners. Moreover, all human faces (e.g., drivers or RodoSol’s employees) were manually redacted (i.e., blurred) in each image."	https://paperswithcode.com/dataset/rodosol-alpr	06/02/2022						
5321	LTFT	"Dataset originally conceived for multi-face tracking/detection for highly crowded scenarios. In these scenarios, the face is the only part that can be used to track the individuals.
All our videos present novel crowd scenes recorded at near-eye level, where faces are visible enough to be analysed at the microscopic level, while also benefiting from a macroscopic view of the crowd. It includes:


Face detections of 715 unique subjects along with instructions to download the synchronized video.


More than 75k face detections annotated.


A density ranging from 3 to 13 people/frame.


6 indoor and 4 outdoor videos. 8/10 videos are totally unconstrained, 2/10 feature 3 re-appearances per subject.


Our dataset may be useful for:


Face tracking, especially relevant for crowded scenarios (typically from video-surveillance cameras).


Heavily occluded body tracking (in many videos, only the face is mostly visible).


Face recognition.


Face detection for partially occluded faces."	https://paperswithcode.com/dataset/ltft	28/07/2021	Long-Term Face Tracking					
5322	IMS Bearing Dataset	Bearing acceleration data from three run-to-failure experiments on a loaded shaft. The data set was provided by the Center for Intelligent Maintenance Systems (IMS), University of Cincinnati.	https://paperswithcode.com/dataset/ims-bearing-dataset	01/01/2022						
5323	PRONOSTIA Bearing Dataset	"The PRONOSTIA (also called FEMTO) bearing dataset consists of 17 accelerated run-to-failures on a small bearing test rig. Both acceleration and temperature data was collected for each experiment.
The dataset was used in the 2012 IEEE Prognostic Challenge. The dataset is from FEMTO-ST Institute in France."	https://paperswithcode.com/dataset/pronostia-bearing-dataset	01/06/2012						
5324	LSA64	The sign database for the Argentinian Sign Language, created with the goal of producing a dictionary for LSA and training an automatic sign recognizer, includes 3200 videos where 10 non-expert subjects executed 5 repetitions of 64 different types of signs. Signs were selected among the most commonly used ones in the LSA lexicon, including both verbs and nouns.	https://paperswithcode.com/dataset/lsa64	01/10/2016	LSA64: A Dataset for Argentinian Sign Language					
5325	Makeup216	Makeup216 contains a variety and representation of logo (captured from the real world) and is among the largest and most complex logo datasets in the field. It comprises of 216 logos and 157 brands, including 10,019 images and 37,018 annotated logo objects.	https://paperswithcode.com/dataset/makeup216	13/12/2021						
5326	MVHand	MVHand is a new multi-view hand posture dataset to obtain complete 3D point clouds of the hand in the real world.	https://paperswithcode.com/dataset/mvhand	13/12/2021						
5327	Wikidated 1.0	Wikidated 1.0 is a dataset of Wikidata's full revision history, which encodes changes between Wikidata revisions as sets of deletions and additions of RDF triples. It constitutes one of the first large datasets of an evolving knowledge graph, a recently emerging research subject in the Semantic Web community.	https://paperswithcode.com/dataset/wikidated-1-0	09/12/2021						
5328	Drosophila Immunity Time-Course Data	"The data used for all results in this paper can be found here. This directory contains:

GeneData.csv: Contains temporal gene expression measurements for 1735 genes at 17 time points. Measurements are provided as the $\log_2$-fold change from first time point. Hours corresponding to each time point are defined in the R script 3_Results.R in our GitHub repository. This dataset is derived from a larger gene expression dataset collected by Schlamp et al. (2021). 
PriorMatrix.csv: A 1735 x 1735 prior adjacency matrix. Each entry is 0, 1, or NA to indicate that a biological relationship between the corresponding two genes is unlikely, likely, or unknown according to external databases.

Further details about the collection of this data can be found in Section 4.1 and Appendix C of our paper. The R script 3_Results.R shows how these CSV files are read and used for our analysis."	https://paperswithcode.com/dataset/drosophila-immunity-time-course-data							
5329	VGG-Sound Sync	"VGG-Sound Sync is an audio-visual synchronisation benchmark based on videos collected from YouTube. VGG-Sound Sync contains over 100k video clips, spanning 160 classes and can be downloaded here.
Note, only the test clips are included here, please use the training clips in the original VGG-Sound to train your models ( classes are same with the ones in the test clips). Each line in the json file has been defined by:
# YouTube ID, start seconds, label"	https://paperswithcode.com/dataset/vgg-sound-sync	08/12/2021						
5330	BRATS21	The RSNA-ASNR-MICCAI BraTS 2021 challenge utilizes multi-institutional pre-operative baseline multi-parametric magnetic resonance imaging (mpMRI) scans, and focuses on the evaluation of state-of-the-art methods for (Task 1) the segmentation of intrinsically heterogeneous brain glioblastoma sub-regions in mpMRI scans. Furthemore, this BraTS 2021 challenge also focuses on the evaluation of (Task 2) classification methods to predict the MGMT promoter methylation status.	https://paperswithcode.com/dataset/brats21		RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2021					
5331	MetaVD	"MetaVD is a Meta Video Dataset for enhancing human action recognition datasets.
It provides human-annotated relationship labels between action classes across human action recognition datasets.
MetaVD is proposed in the following paper:
Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi. ""MetaVD: A Meta Video Dataset for enhancing human action recognition datasets."" Computer Vision and Image Understanding 212 (2021): 103276. [link]
MetaVD integrates the following datasets: UCF101, HMDB51, ActivityNet, STAIR Actions, Charades, Kinetics-700
This repository does NOT provide videos in the datasets. 
For information on how to download the videos, please refer to the website of each dataset."	https://paperswithcode.com/dataset/metavd	01/11/2021						
5332	ProSLU	"In the paper, to bridge the research gap, we propose a new and important task, Profile-based Spoken Language Understanding (ProSLU), which requires a model not only depends on the text but also on the given supporting profile information.
We further introduce a Chinese human-annotated dataset, with over 5K utterances annotated with intent and slots, and corresponding supporting profile information. 
In total, we provide three types of supporting profile information: 
(1) Knowledge Graph (KG) consists of entities with rich attributes, 
(2) User Profile (UP) is composed of user settings and information,
(3) Context Awareness(CA) is user state and environmental information."	https://paperswithcode.com/dataset/proslu	22/12/2021	Profile-based Spoken Language Understanding					
5333	SERV-CT	"Endoscopic stereo reconstruction for surgical scenes gives rise to specific problems, including the lack of clear corner features, highly specular surface properties, and the presence of blood and smoke. These issues present difficulties for both stereo reconstruction itself and also for standardised dataset production. We present a stereo-endoscopic reconstruction validation dataset based on cone-beam CT (SERV-CT). Two ex vivo small porcine full torso cadavers were placed within the view of the endoscope with both the endoscope and target anatomy visible in the CT scan. Subsequent orientation of the endoscope was manually aligned to match the stereoscopic view and benchmark disparities, depths and occlusions are calculated. The requirement of a CT scan limited the number of stereo pairs to 8 from each ex vivo sample. For the second sample an RGB surface was acquired to aid alignment of smooth, featureless surfaces. Repeated manual alignments showed an RMS disparity accuracy of around 2 pixels and a depth accuracy of about 2 mm. A simplified reference dataset is provided consisting of endoscope image pairs with corresponding calibration, disparities, depths, and occlusions covering the majority of the endoscopic image and a range of tissue types, including smooth specular surfaces, as well as significant variation of depth. 
The SERV-CT dataset provides an easy-to-use stereoscopic validation for surgical applications with smooth reference disparities and depths covering the majority of the endoscopic image."	https://paperswithcode.com/dataset/serv-ct	22/12/2020	SERV-CT: A disparity dataset from CT for validation of endoscopic 3D reconstruction					
5334	ASL-Skeleton3D	The ASL-Skeleton3D introduces a representation based on mapping into the three-dimensional space the coordinates of the signers in the ASLLVD dataset. This enables a more accurate observation of the body parts and the signs articulation, allowing researchers to better understand the language and explore other approaches to the SLR field.	https://paperswithcode.com/dataset/asl-skeleton3d	07/09/2021						
5335	ASL-Phono	The ASL-Phono introduces a novel linguistics-based representation, which describes the signs in the ASLLVD dataset in terms of a set of attributes of the American Sign Language phonology.	https://paperswithcode.com/dataset/asl-phono	07/09/2021						
5336	ASLLVD	The American Sign Language Lexicon Video Dataset (ASLLVD) consists of videos of >3,300 ASL signs in citation form, each produced by 1-6 native ASL signers, for a total of almost 9,800 tokens. This dataset includes multiple synchronized videos showing the signing from different angles. Linguistic annotations include gloss labels, sign start and end time codes, start and end handshape labels for both hands, morphological and articulatory classifications of sign type. For compound signs, the dataset includes annotations for each morpheme. To facilitate computer vision-based sign language recognition, the dataset also includes numeric ID labels for sign variants, video sequences in uncompressed-raw format, and camera calibration sequences.	https://paperswithcode.com/dataset/asllvd		American Sign Language Lexicon Video Dataset					
5337	MRSpineSeg Challenge	"1、 Competition name:
The 2nd China Society of Image and Graphics (CSIG) Image and Graphics Technology Challenge: MRSpineSeg Challenge: Automated Multi-class Segmentation of Spinal Structures on Volumetric MR Images.
2、 Purpose：
Degenerative spine diseases (e.g., lumbar disc herniation, spinal stenosis, etc.) have become important diseases affecting the health and quality of life of the elderly and working people. These degenerative spinal diseases often cause changes in the structural morphology and mechanical systems of the spine, resulting in pain, such as lumbar disc herniation, reduced disc height, and nerve compression. Magnetic resonance imaging (MRI) as a non-invasive examination method, it has good soft tissue imaging and no radiation. It is a reliable screening method for degenerative spine diseases. In clinical practice, the treatment of degenerative spinal disorders depends largely on physicians’ experience and lacks accurate quantitative analysis tools. 3D automatic segmentation (MR) images of multiclass spinal structures by MRI are a prerequisite for 3D reconstruction of spinal structures. It can provide quantitative analysis tools for building biomechanical models of the spine, simulating stresses in spinal structures, and assessing the prognosis of different treatment options for degenerative spinal diseases.
This competition aims to gather global developers to explore efficient and accurate 3D automatic segmentation of spinal structure in MR images by using artificial intelligence technology. The spinal structure to be segmented includes 10 vertebrae and 9 intervertebral discs.
3、 Organizer:
Qianjin,Feng, School of Biomedical Engineering, Southern Medical University, Guangdong Key Laboratory of medical image processing, China.
4、 Requirements for competition participants：
It is open to the whole society. Personnel from colleges and universities, scientific research institutions and enterprises can sign up for the competition. The maximum number of each team is four. Each person can only participate in one team. After team registration, the team information cannot be changed.
Note: all personnel who have access to the competition data are prohibited from participating in the competition. Those who have not access to the competition data of Southern Medical University can also participate in the competition. Southern Medical University has the right of final interpretation.
5、 Timeline:
Registration (March 30, 2021 – May 22, 2021)
6、Citation:
1 Shumao Pang, Chunlan Pang, Lei Zhao, Yangfan Chen, Zhihai Su, Yujia Zhou, Meiyan Huang, Wei Yang, Hai Lu, Qianjin Feng*. SpineParseNet: Spine Parsing for Volumetric MR Image by a Two-Stage Segmentation Framework with Semantic Image Representation [J]. IEEE Transactions on Medical Imaging, 2021, 40(1): 262-273.
2 Shumao Pang, Chunlan Pang, Zhihai Su, Liyan Lin, Lei Zhao, Yangfan Chen, Yujia Zhou, Hai Lu, Qianjin Feng*. DGMSNet: Spine Segmentation for MR Image by a Detection-Guided Mixed-supervised Segmentation Network [J]. Medical Image Analysis, 2022, 102261."	https://paperswithcode.com/dataset/mrspineseg-challenge	21/09/2020						
5338	DIDI Dataset	The dataset contains digital ink drawings of diagrams with dynamic drawing information. The dataset aims to foster research in interactive graphical symbolic understanding. The dataset was obtained using a prompted data collection effort.	https://paperswithcode.com/dataset/didi-dataset		The DIDI dataset: Digital Ink Diagram data					
5339	OULU-NPU	"The Oulu-NPU face presentation attack detection database consists of 4950 real access and attack videos. These videos were recorded using the front cameras of six mobile devices (Samsung Galaxy S6 edge, HTC Desire EYE, MEIZU X5, ASUS Zenfone Selfie, Sony XPERIA C5 Ultra Dual and OPPO N3) in three sessions with different illumination conditions and background scenes. The presentation attack types considered in the OULU-NPU database are print and video-replay. The 2D face artefacts were created using two printers and two display devices. 
The videos of the 55 subjects are divided into three subject-disjoint subsets for training, development and testing. Four test protocols are used to evaluate the generalization capability of face PAD methods across three covariates: unknown environmental conditions (namely illumination and background scene), acquisition devices and presentation attack instruments (PAI). Each of the four unambiguously defined evaluation protocols introduces at least one previously unseen condition to the test set, which enables a fair comparison on the generalization capabilities between new and existing approaches.
Image Source: https://www.researchgate.net/profile/Neil-Robertson/publication/333834759/figure/fig5/AS:897964780302339@1591102895306/Samples-from-the-OULU-NPU-database-From-top-to-bottom-is-the-three-sessions-with_W640.jpg"	https://paperswithcode.com/dataset/oulu-npu							
5340	Wiki-One	"This dataset is a Wikipedia dump, split by relations to perform Few-Shot Knowledge Graph Completion. 
\begin{table}[]
\begin{tabular}{@{}lllccl@{}}
\textbf{Dataset}  & \textbf{# Ent}    & \textbf{# Rel} & \textbf{# Triplets}                 & \textbf{Train/Dev/Test}               \ 
Wiki-One & 4,838,244 & 822                              & 5,829,240                   & 133/16/34                   \
\end{tabular}
\caption{Datasets used in the experiments. }
\end{table}"	https://paperswithcode.com/dataset/wiki-one	27/08/2018						
5341	FACTIFY	FACTIFY is a dataset on multi-modal fact verification. It contains images, textual claim, reference textual documenta and image. The task is to classify the claims into support, not-enough-evidence and refute categories with the help of the supporting data. We aim to combat fake news in the social media era by providing this multi-modal dataset. Factify contains 50,000 claims accompanied with 100,000 images, split into training, validation and test sets.	https://paperswithcode.com/dataset/factify	01/11/2021	a dataset on multi-modal fact verification					
5342	DurLAR	"DurLAR is a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery for multi-modal autonomous driving applications. Compared to existing autonomous driving task datasets, DurLAR has the following novel features:  

High vertical resolution LiDAR with 128 channels, which is twice that of any existing datasets, full 360 degree depth, range accuracy to ±2 cm at 20-50m.  
Ambient illumination (near infrared) and reflectivity panoramic imagery are made available in the Mono16 format (2048 × 128 resolution), with this being only dataset to make this provision.  
No rolling shutter effect, as our flash LiDAR captures all 128 channels simultaneously.  
Ambient illumination data is recorded via an on-board lux meter, which is again not available in previous datasets.  
High-fidelity GNSS/INS available via an onboard OxTS navigation unit operating at 100 Hz and receiving position and timing data from multiple GNSS con-stellations in addition to GPS.  
KITTI data format adopted as the de facto dataset format such that it can be parsed using both the DurLAR development kit and existing KITTI-compatible tools.   
Diversity over repeated locations such that the dataset has been collected under diverse environmental and weather conditions over the same driving route with additional variations in the time of day relative to environmental conditions.

Sensor placement


LiDAR: Ouster OS1-128 LiDAR sensor with 128 channels vertical resolution


Stereo Camera: Carnegie Robotics MultiSense S21 stereo camera with grayscale, colour, and IR enhanced imagers, 2048x1088 @ 2MP resolution


GNSS/INS: OxTS RT3000v3 global navigation satellite and inertial navigation system, supporting localization from GPS, GLONASS, BeiDou, Galileo, PPP and SBAS constellations


Lux Meter: Yocto Light V3, a USB ambient light sensor (lux meter), measuring ambient light up to 100,000 lux"	https://paperswithcode.com/dataset/durlar	01/12/2021	A High-Fidelity 128-Channel LiDAR Dataset with Panoramic Ambient and Reflectivity Imagery					
5343	BLP	"A blackout poetry dataset constructed from publicly available short stories and large poems. The dataset consists of two variants: 8K and 16K examples of passages along with a poem generated from the passage and the indices of the words in the passage from which words in the poem have been selected. The dataset also contains perplexity scores for each of the poems indicating the language quality of the poems. 
The dataset was constructed synthetically, and hence contains multiple poor poems and frequent grammatical errors. However, it is a great starting point for the task of applying machine learning to blackout poetry generation."	https://paperswithcode.com/dataset/blp16	01/11/2021	Blackout Poetry Dataset					
5344	NICO	I.I.D. hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing in- stable performances of these models. In literature, however, the Non-I.I.D. image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO, which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with Con- vNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings.	https://paperswithcode.com/dataset/nico	07/06/2019	Non-I.I.D. Image dataset with Contexts					
5345	Atari 100k	Atari Games for only 100k environment steps. (400k frames with frame-skip=4).	https://paperswithcode.com/dataset/atari-100k	01/03/2019						
5346	GOTOV	"Stylianos ParaschiakosStylianos Paraschiakos, Beekman M. (Marian), Knobbe A. (Arno), Cachucho R. (Ricardo), Slagboom P. (Eline)
Wearable sensor-based data of physical activities and indirect calorimetry for 35 (14 female, 21 male) healthy older individuals (over 60 years old). The data has been collected from different body locations and devices: 3x GeneActives accelerometers (ankle, wrist, and chest), 1x Equivital (chest) and COSMED (mask and belt on chest). The 35 individuals followed a protocol of 16 activities of daily living for approximately an hour and a half in a semi-lab environment. These include different types or paces of indoor and outdoor activities with low (lying down, sitting), mid (standing, household activities) and high (walking and cycling) levels of intensity. Additionally, some activities can be specified at different granularities. The study took place at LUMC, between February and May 2015."	https://paperswithcode.com/dataset/gotov		Human Physical Activity and Energy Expenditure Dataset on Older Individuals					
5347	SaL-Lightning	SaL-Lightning is a dataset for research in the field of Search as Learning. It contains detailed recordings, pre- and post-knowledge assessments of 114 participants, interaction data on real-world search behavior, as well as resource features of a user study. This data diversity has the potential to help researchers answer diverse questions tied to the entire online learning framework, from individual psychological aspects, over usability tests and data visualization over retrieval and ranking issues existing in the technology that enables this process.	https://paperswithcode.com/dataset/sal-lightning	07/01/2022						
5348	Weibo21	Weibo21 is a benchmark of fake news dataset for multi-domain fake news detection (MFND) with domain label annotated, which consists of 4,488 fake news and 4,640 real news from 9 different domains.	https://paperswithcode.com/dataset/weibo21	04/01/2022						
5349	H2O	"The Human-to-Human-or-Object Interaction Dataset (H2O) dataset is a dataset for Human-Object Interaction (HOI) detection. It consists in determining and locating the list of triplets <subject,verb,target> which describe all the simultaneous interactions in an image.
H²O is composed of the 10 301 images from V-COCO dataset to which are added 3 635 images which mostly contain interactions between people."	https://paperswithcode.com/dataset/h2o	07/01/2022						
5350	ERD	ERD (Educational Resource Discovery) is a corpus of 39,728 manually labeled web resources and 659 queries from NLP, Computer Vision (CV), and Statistics (STATS) for educational resource discovery.	https://paperswithcode.com/dataset/erd	07/01/2022	Educational Resource Discovery					
5351	LoRa RF	This is a large-scale RF fingerprinting dataset, collected from 25 different LoRa-enabled IoT transmitting devices using USRP B210 receivers. Our dataset consists of a large number of SigMF-compliant binary files representing the I/Q time-domain samples and their corresponding FFT-based files of LoRa transmissions.	https://paperswithcode.com/dataset/lora-rf	06/01/2022						
5352	Turath-150K	"Turath-150K is a database of images of the Arab world that reflect objects, activities, and scenarios commonly found there.
Broadly, the database consists of objects, activities, and scenarios commonly encountered in the Arab World (from Mauritania in the West of Africa to Iraq). More specifically, there exist 3 distinct benchmark databases; Turath-Standard, Turath-Art, and Turath-UNESCO."	https://paperswithcode.com/dataset/turath-150k	01/01/2022						
5353	FS2K	FS2K is a high-quality Facial Sketch Synthesis (FSS). It consists of 2,104 image-sketch pairs spanning three types of sketch styles, image backgrounds, lighting conditions, skin colors, and facial attributes. FS2K differs from previous FSS datasets in difficulty, diversity, and scalability, and should thus facilitate the progress of FSS research.	https://paperswithcode.com/dataset/fs2k	31/12/2021						
5354	KIND	KIND is an Italian dataset for Named-Entity Recognition. It contains more than one million tokens with the annotation covering three classes: persons, locations, and organizations. Most of the dataset (around 600K tokens) contains manual gold annotations in three different domains: news, literature, and political discourses.	https://paperswithcode.com/dataset/kind	30/12/2021	Kessler Italian Named-entities Dataset					
5355	SFU-HW-Tracks	SFU-HW-Tracks is a dataset for Object Tracking on raw video sequences that contains object annotations with unique object identities (IDs) for the High Efficiency Video Coding (HEVC) v1 Common Test Conditions (CTC) sequences. Ground-truth annotations for 13 sequences were prepared and released as the dataset called SFU-	https://paperswithcode.com/dataset/sfu-hw-tracks	30/12/2021						
5356	YACLC	YACLC is a large scale, multidimensional annotated Chinese learner corpus. To construct the corpus, the aurhots first obtain a large number of topic-rich texts generated by Chinese as Foreign Language (CFL) learners. The authors collected and annotated 32,124 sentences written by CFL learners from the lang-8 platform. Each sentence is annotated by 10 annotators. After post processing, a total of 469,000 revised sentences are obtained.	https://paperswithcode.com/dataset/yaclc	30/12/2021	Yet Another Chinese Learner Corpus					
5357	The Benchmark	"The Benchmark is a collection of datasets for Monocular Height Estimation. It consists of two datasets: GTAH and AHN.
GTAH (Grand Theft Auto for Height estimation) is a large-scale synthetic dataset which is obtained from the game Grand Theft Auto, under different imaging conditions. GTAH contains 28,627 height maps in total and each with a resolution of 1920×1080. For each height map, there are three corresponding RGB images that are captured under different weather conditions."	https://paperswithcode.com/dataset/the-benchmark	30/12/2021						
5358	CUGE	"CUGE is a Chinese Language Understanding and Generation Evaluation benchmark with the following features: (1) Hierarchical benchmark framework, where datasets are principally selected and organized with a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy, where different levels of model performance are provided based on the hierarchical framework.
CUGE covers 7 important language capabilities, 17 mainstream NLP tasks and 19 representative datasets. It includes tasks like: word segmentation, part of speech tagging, reading comprehension and document retrieval."	https://paperswithcode.com/dataset/cuge	27/12/2021						
5359	N-Omniglot	N-Omniglot is a neuromorphic dataset for few-shot learning. It contains 1,623 categories of handwritten characters, with only 20 samples per class.	https://paperswithcode.com/dataset/n-omniglot	25/12/2021						
5360	nvBench	nvBench is a large-scale NL2VIS (natural languagge to visualisations) benchmark, containing 25,750 (NL, VIS) pairs from 750 tables over 105 domains, synthesized from (NL, SQL) benchmarks to support cross-domain NLPVIS (Natural Language Query to Visualization) task.	https://paperswithcode.com/dataset/nvbench	24/12/2021						
5361	BPOD	Brown Pedestrian Odometry Dataset (BPOD) is a dataset for benchmarking visual odometry algorithms in head-mounted pedestrian settings. This dataset was captured using synchronized global and rolling shutter stereo cameras in 12 diverse indoor and outdoor locations on Brown University's campus. Compared to existing datasets, BPOD contains more image blur and self-rotation, which are common in pedestrian odometry but rare elsewhere. Ground-truth trajectories are generated from stick-on markers placed along the pedestrian’s path, and the pedestrian's position is documented using a third-person video.	https://paperswithcode.com/dataset/bpod	24/12/2021						
5362	HSPACE	HSPACE (Human-SPACE) is a large-scale photo-realistic dataset of animated humans placed in complex synthetic indoor and outdoor environments. For all frames the dataset provides 3d pose and shape ground truth, as well as other rich image annotations including human segmentation, body part localisation semantics, and temporal correspondences.	https://paperswithcode.com/dataset/hspace	23/12/2021	Human-SPACE					
5363	PandaSet	PandaSet is a dataset produced by a complete, high-precision autonomous vehicle sensor kit with a no-cost commercial license. The dataset was collected using one 360x360 mechanical spinning LiDAR, one forward-facing, long-range LiDRAR, and 6 cameras. The datasets contains more than 100 scenes, each of which is 8 seconds long, and provides 28 types of labels for object classification and 37 types of annotations for semantic segmentation.	https://paperswithcode.com/dataset/pandaset	23/12/2021						
5364	EgoBody	EgoBody is a novel large-scale dataset for social interactions in complex 3D scenes.	https://paperswithcode.com/dataset/egobody	14/12/2021						
5365	RLD	RLD (Responsive Listener Dataset) is a conversation video corpus collected from the public resources featuring 67 speakers, 76 listeners with three different attitudes. Through non-verbal signals response to the speakers' words, intonations, or behaviors in real-time, listeners show how they are engaged in dialogue.	https://paperswithcode.com/dataset/rld	27/12/2021	Responsive Listener Dataset					
5366	EMDS-6	In EMDS-6, there are 21 classes of environmental microorganisms (EMs). In each calss, there are 40 EM original images and their corresponding binary groud truth images. In ground truth images, the foreground is white and background is black.	https://paperswithcode.com/dataset/emds-6	14/12/2021						
5367	Industrial Benchmark Dataset for Customer Escalation Prediction	"This is a real-world industrial benchmark dataset from a major medical device manufacturer for the prediction of customer escalations. The dataset contains features derived from IoT (machine log) and enterprise data including labels for escalation from a fleet of thousands of customers of high-end medical devices. 
The dataset accompanies the publication ""System Design for a Data-driven and Explainable Customer Sentiment Monitor"" (submitted). We provide an anonymized version of data collected over a period of two years.
The dataset should fuel the research and development of new machine learning algorithms to better cope with real-world data challenges including sparse and noisy labels, and concept drifts. Additional challenges are the optimal fusion of enterprise and log-based features for the prediction task. Thereby, the interpretability of designed prediction models should be ensured in order to have practical relevancy. 
Supporting software
Kindly use the corresponding GitHub repository (https://github.com/annguy/customer-sentiment-monitor) to design and benchmark your algorithms. 
Citation and Contact
If you use this dataset please cite the following publication:
@ARTICLE{9520354,
  author={Nguyen, An and Foerstel, Stefan and Kittler, Thomas and Kurzyukov, Andrey and Schwinn, Leo and Zanca, Dario and Hipp, Tobias and Jun, Sun Da and Schrapp, Michael and Rothgang, Eva and Eskofier, Bjoern},
  journal={IEEE Access}, 
  title={System Design for a Data-Driven and Explainable Customer Sentiment Monitor Using IoT and Enterprise Data}, 
  year={2021},
  volume={9},
  number={},
  pages={117140-117152},
  doi={10.1109/ACCESS.2021.3106791}}
If you would like to get in touch, please contact an.nguyen@fau.de."	https://paperswithcode.com/dataset/industrial-benchmark-dataset-for-customer	21/12/2020						
5368	CeyMo	CeyMo is a novel benchmark dataset for road marking detection which covers a wide variety of challenging urban, sub-urban and rural road scenarios. The dataset consists of 2887 total images of 1920 × 1080 resolution with 4706 road marking instances belonging to 11 classes. The test set is divided into six categories: normal, crowded, dazzle light, night, rain and shadow.	https://paperswithcode.com/dataset/ceymo	22/10/2021						
5369	DSIFN-CD	The dataset is manually collected from Google Earth. It consists of six large bi-temporal high resolution images covering six cities (i.e., Beijing, Chengdu, Shenzhen, Chongqing, Wuhan, Xian) in China. The five large image-pairs (i.e., Beijing, Chengdu, Shenzhen, Chongqing, Wuhan) are clipped into 394 subimage pairs with sizes of 512×512. After data augmentation, a collection of 3940 bi-temporal image pairs is acquired. Xian image pair is clipped into 48 image pairs for model testing. There are 3600 image pairs in the training dataset, 340 image paris in the validation dataset, and 48 image pairs in the test dataset.	https://paperswithcode.com/dataset/dsifn-cd	01/06/2020						
5370	SCROLLS	"SCROLLS (Standardized CompaRison Over Long Language Sequences) is an NLP benchmark consisting of a suite of tasks that require reasoning over long texts. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. The dataset is made available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.
The SCROLLS benchmark contains the datasets GovReport, SummScreenFD, QMSum, QASPER, NarrativeQA, QuALITY and ContractNLI."	https://paperswithcode.com/dataset/scrolls	10/01/2022	Standardized CompaRison Over Long Language Sequences					
5371	CrossMoDA	**CrossMoDA is a large and multi-class benchmark for unsupervised cross-modality Domain Adaptation. The goal of the challenge is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are commonly performed using contrast-enhanced T1 (ceT1) MR imaging.	https://paperswithcode.com/dataset/crossmoda	08/01/2022	Cross-Modality Domain Adaptation					
5372	DADA-seg	DADA-seg is a pixel-wise annotated accident dataset, which contains a variety of critical scenarios from traffic accidents. It is used for semantic segmentation.	https://paperswithcode.com/dataset/dada-seg	09/12/2021						
5373	MyoPS	MyoPS is a dataset for myocardial pathology segmentation combining three-sequence cardiac magnetic resonance (CMR) images, which was first proposed in the MyoPS challenge, in conjunction with MICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images, allowing algorithms to combine the complementary information from the three CMR sequences for pathology segment	https://paperswithcode.com/dataset/myops	10/01/2022						
5374	3D-BSLS-6D	"Dataset consist of both real captures from Photoneo PhoXi structured light scanner devices annotated by hand and synthetic samples produced by custom generator. In comparison with existing datasets for 6D pose estimation, some notable differences include:

most of the captured bins are texture-less, made from uniform, single-colored materials,
all bins are of cuboid shape with different proportions. Compared to objects with complex geometry, bins consist of flat faces with edges, which are not guaranteed to be seen in the capture due to occlusion. Surface models of these bins are not provided, just their approximate bounding boxes,
PhoXi scanner provides high-resolution 3D geometry data, but no RGB data, with a rough and noisy gray-scale intensity image being the closest equivalent,
captures come from different devices with various intrinsic camera parameters.  3D point clouds contain these parameters implicitly as opposed to RGBD images.

Due to its currently limited size, we recommend cross-validation instead of an explicit train-validation split. We plan to add more samples into the dataset. NOTE: Annotation files with suffixes _bad, _ish or _catastrophic should be ignored. It was not possible to annotate them correctly with our current toolset."	https://paperswithcode.com/dataset/3d-bsls-6d	17/12/2021	3D scans of Bins by Structured-Light Scanner for 6D pose estimation					
5375	FR-FS	"The FR-FS dataset contains 417 videos collected from FIV dataset and Pingchang 2018 Winter Olympic Games. FR-FS contains the critical movements of the athlete’s take-off, rotation, and landing. Among them, 276 are smooth landing videos, and 141 are fall videos.
To test the generalization performance of our proposed model, we randomly select 50% of the videos from the fall and landing videos as the training set and the testing set."	https://paperswithcode.com/dataset/fr-fs	11/01/2022	Fall Recognition in Figure Skating					
5376	results-A	The result-A datasets are images obtained by fusing infrared and visible light images.	https://paperswithcode.com/dataset/results-a							
5377	GrailQA	GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot.	https://paperswithcode.com/dataset/grailqa	16/11/2020	Strongly Generalizable Question Answering					
5378	EurekaAlert	This dataset contains around 5000 scholarly articles and their corresponding easy summary from eureka alert blog, the dataset can be used for the combined task of summarization and simplification.	https://paperswithcode.com/dataset/eurekaalert	13/07/2020	Eureka Alert					
5379	UAV-VeID	"Data Collection
We simulate real scenarios as much as possible during the UAV videos collection. 
Specifically, UAV videos are collected from different locations with distinct backgrounds and lighting conditions, e.g., including highways, urban road intersections, parking lots, etc. 
For vehicles at parking lots, we adopt various UAV sport modes such as cruising and rotating to record vehicles. 
This strategy introduces viewpoint and scale changes, as well as partial occlusions to images of the same vehicle. 
For moving vehicles, we use two UAVs to simultaneously shoot videos from different viewpoints and heights. 
This strategy introduces viewpoint, scale, and background changes. 
The flying height of UAVs ranges from 15 to 60 meters, leading to different scales of vehicle images. 
The vertical angle of UAV camera ranges from 40 to 80 degrees, which leads to different viewpoints of vehicle images. 
The videos are recorded at 30 frames per second (fps), with the resolution of 2704 × 1520 pixels and 4096 × 2160 pixels, respectively. 
The UAV-VeID is constructed from 80 video sequences selected from raw UAV videos.


Annotation
We annotate vehicles from collected videos to construct the UAV-VeID. 
In each video clip, 1 video frame is sampled every one second to construct a video frame dataset. 
The dataset annotation is hence conducted based on those sample video frames.
To finish the vehicle annotation, 6 domain experts are involved to manually locate and annotate the identities of vehicles from each video frame.
The data annotation procedure takes 1000 man-hours and finally results in a dataset containing 41,917 vehicle bounding boxes of 4601 vehicles. 
Each vehicle is annotated by at least two bounding boxes. 


Dataset partition
The UAV-VeID dataset is split into the training set, validation set, and testing set, among which the training set contains 18,709 images with 1,797 IDs, the validation set contains 4,150 images with 596 IDs, and the testing set contains 19,058 images with 2,208 IDs. 
The validation set is further divided into a query set (""val_q_label.txt"" 3,554 images) and a gallery set (""val_g_label.txt"" 596 images). 
The testing set is further divided into a query set (""test_q_label.txt"" 16,850 images) and a gallery set (""test_g_label.txt"" 2,208 images). 


Download
Please sign the Agreement(UAV-VeID_AGREEMENT.pdf) and thereby agrees to observe the restrictions listed in this document. 
After filling it, please send the electrical version to us. After confirming your information, we will send the download link and password to you via Email.


Contact
Shangzhi Teng, Email: tengshangzhi@126.com"	https://paperswithcode.com/dataset/uav-veid							
5380	ITB	Informative Tracking Benchmark (ITB) is a small and informative tracking benchmark with 7% out of 1.2 M frames of existing and newly collected datasets, which enables efficient evaluation while ensuring effectiveness. Specifically, the authors designed a quality assessment mechanism to select the most informative sequences from existing benchmarks taking into account 1) challenging level, 2) discriminative strength, 3) and density of appearance variations. Furthermore, they collect additional sequences to ensure the diversity and balance of tracking scenarios, leading to a total of 20 sequences for each scenario.	https://paperswithcode.com/dataset/itb	13/12/2021	Informative Tracking Benchmark					
5381	PhysNLU	PhysNLU is a collection of 4 core datasets related to sentence classification, ordering, and coherence of physics explanations based on related tasks. Each dataset comprises explanations extracted from Wikipedia including derivations and mathematical language.	https://paperswithcode.com/dataset/physnlu	12/01/2022						
5382	Incidents1M	Incidents1M is a large-scale multi-label dataset for incident detection which contains 977,088 images, with 43 incident and 49 place categories. It is an evolution of the Incidents dataset that doubles the dataset size and includes more incident labels.	https://paperswithcode.com/dataset/incidents1m	11/01/2022						
5383	CVSS	CVSS is a massively multilingual-to-English speech to speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice  speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems	https://paperswithcode.com/dataset/cvss	11/01/2022						
5384	PerCQA	PerCQA is the first Persian dataset for CQA (Community Question Answering). This dataset contains the questions and answers crawled from the most well-known Persian forum.	https://paperswithcode.com/dataset/percqa	25/12/2021						
5385	ArtImage	ArtImage is a synthetic dataset of articulated object models of 5 categories from PartNet-Mobility for articulated object tasks in category level.	https://paperswithcode.com/dataset/artimage	14/12/2021						
5386	ASCEND	ASCEND (A Spontaneous Chinese-English Dataset) introduces a high-quality resource of spontaneous multi-turn conversational dialogue Chinese code-switching corpus collected in Hong Kong. ASCEND includes 23 bilinguals that are fluent in both Chinese and English and consists of 10.62 hours clean speech corpus.	https://paperswithcode.com/dataset/ascend	12/12/2021						
5387	MetaEval	MetaEval is a collection of 101 NLP tasks. It consists of 101 tasks in a benchmark that can be used for future probing and transfer learning.	https://paperswithcode.com/dataset/metaeval	10/12/2021						
5388	Learn2Reg	Learn2Reg is a dataset for medical image registration. Learn2Reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR), availability of annotations, as well as intra- and inter-patient registration evaluation.	https://paperswithcode.com/dataset/learn2reg	08/12/2021						
5389	LOOK	LOOK is a large-scale dataset for eye contact detection in the wild, which focuses on diverse and unconstrained scenarios for real-world generalization. The dataset focuses on real-world scenarios for autonomous vehicles with no control over the environment or the distance of pedestrians	https://paperswithcode.com/dataset/look	08/12/2021						
5390	VocBench	VocBench is a framework that benchmark the performance of state-of-the art neural vocoders. VocBench uses a systematic study to evaluate different neural vocoders in a shared environment that enables a fair comparison between them.	https://paperswithcode.com/dataset/vocbench	06/12/2021						
5391	ES-ImageNet	ES-ImageNet is a large-scale event-stream dataset for SNNs and neuromorphic vision. It consists of about 1.3 M samples converted from ILSVRC2012 in 1000 different categories. ES-ImageNet is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software	https://paperswithcode.com/dataset/es-imagenet	23/10/2021						
5392	CD&S	The Corn Disease and Severity (CD&S) dataset consists of 511, 524, and 562, field acquired raw images, corresponding to three common foliar corn diseases, namely Northern Leaf Blight (NLB), Gray Leaf Spot (GLS), and Northern Leaf Spot.	https://paperswithcode.com/dataset/cd-s	22/10/2021	Corn Disease and Severity					
5393	NOD	This is a high-quality large-scale Night Object Detection (NOD) dataset of outdoor images targeting low-light object detection. The dataset contains more than 7K images and 46K annotated objects (with bounding boxes) that belong to classes: person, bicycle, and car. The photos were taken on the streets at evening hours, and thus all images present low-light conditions to a varying degree of severity.	https://paperswithcode.com/dataset/nod	20/10/2021	Night Object Detection					
5394	Curlie	Curlie dataset is a dataset with more than 1M websites in 92 languages with relative labels collected from Curlie, the largest multilingual crowdsourced Web directory. The dataset contains 14 website categories aligned across languages. It is used for language-agnostic website embedding and classification	https://paperswithcode.com/dataset/curlie	10/01/2022						
5395	BNATURE	This is a dataset for Bengali Captioning from Images.	https://paperswithcode.com/dataset/bnature	20/12/2021						
5396	DeepLesion	"The National Institutes of Health’s Clinical Center has made a large-scale dataset of CT images publicly available to help the scientific community improve detection accuracy of lesions. While most publicly available medical image datasets have less than a thousand lesions, this dataset, named DeepLesion, has over 32,000 annotated lesions (220GB) identified on CT images.
DeepLesion, a dataset with 32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients. There are a variety of lesion types in this dataset, such as lung nodules, liver tumors, enlarged lymph nodes, and so on. It has the potential to be used in various medical image applications"	https://paperswithcode.com/dataset/deeplesion	04/10/2017						
5397	BRACS	BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin & Eosin (H&E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories.	https://paperswithcode.com/dataset/bracs	08/11/2021	BReAst Carcinoma Subtyping					
5398	EEGEyeNet	EEEyeNet is a dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. It consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms.	https://paperswithcode.com/dataset/eegeyenet	06/11/2021						
5399	PaSa	PaSa is a dataset to train Machine Learning algorithms to automate the highlighting of patent paragraphs with semantic annotations. It consists of 150k samples obtained by traversing USPTO patents over a decade	https://paperswithcode.com/dataset/pasa	06/11/2021						
5400	CUB-GHA	CUB-GHA is a dataset for fine-grained classification with human attention annotations. The dataset collects human gaze data for the fine-grained classification dataset CUB and builds a dataset named CUB-GHA (Gaze-based Human Attention).	https://paperswithcode.com/dataset/cub-gha	02/11/2021	CUB Gaze-based Human Attention					
5401	UMLS	Source: Convolutional 2D Knowledge Graph Embeddings	https://paperswithcode.com/dataset/umls	05/07/2017						
5402	FFHQ-Text	"FFHQ-Text is a small-scale face image dataset with large-scale facial attributes, designed for text-to-face generation & manipulation, text-guided facial image manipulation, and other vision-related tasks.
This dataset is an extension of the NVIDIA Flickr-Faces-HQ Dataset (FFHQ), which is the selected top 760 female FFHQ images that only contain one complete human face."	https://paperswithcode.com/dataset/ffhq-text	12/01/2022						
5403	Semantic Question Similarity in Arabic	"NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic
This dataset contains 11,997 pairs of questions in MSA Arabic that are assigned either a label of 0, for no semantic similarity, or 1 otherwise."	https://paperswithcode.com/dataset/semantic-question-similarity-in-arabic	12/09/2019	NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic					
5404	Sepehr_RumTel01	The expansion of social networks has accelerated the transmission of information and news at every communities. Over the past few years, the number of users, audiences and social networking publishers, are increased dramatically too. Among the massive amounts of information and news reported on these networks, we are faced with issues that have not been verified which is called “rumors”. Identifying rumors on social networks is carried out in the form of rumor detection approaches; the massive amount of these news and information force to use the machine learning techniques. The most important problem with auto-detection approaches is the lack of a database of rumors. For that matter, in this article, a collection of rumors published on the social network “telegrams” have been collected. These data are gathered from five Persian-language channels that have specially reviewed this issue. The collected data set contains 3283 messages with 2829 attachments, having a volume of over 1.6 gigabytes. This dataset can also be used for different purposes of natural language processing.	https://paperswithcode.com/dataset/sepehr-rumtel01	12/01/2019						
5405	BanglaEmotion	"BanglaEmotion is a manually annotated Bangla Emotion corpus, which incorporates the diversity of fine-grained emotion expressions in social-media text. More fine-grained emotion labels are considered such as Sadness, Happiness, Disgust, Surprise, Fear and Anger - which are, according to Paul Ekman (1999), the six basic emotion categories. For this task, a large amount of raw text data are collected from the user’s comments on two different Facebook groups (Ekattor TV and Airport Magistrates) and from the public post of a popular blogger and activist Dr. Imran H Sarker. These comments are mostly reactions to ongoing socio-political issues and towards the economic success and failure of Bangladesh. A total of 32923 comments are scraped from the three sources aforementioned above. Out of these, a total of 6314 comments were annotated into the six categories. The distribution of the annotated corpus is as follows:
sad = 1341
happy = 1908
disgust = 703
surprise = 562
fear = 384
angry = 1416
A balanced set is also provided from the above data and split the dataset into training and test set of equal ratio. A proportion of 5:1 is used for training and evaluation purposes. More information on the dataset and the experiments on it could be found in our paper (related links below)."	https://paperswithcode.com/dataset/banglaemotion	20/11/2020	BanglaEmotion: A Benchmark Dataset for Bangla Textual Emotion Analysis					
5406	Supplementary material for PyTea	Manuals and test set.	https://paperswithcode.com/dataset/supplementary-material-for-pytea	16/12/2021						
5407	EEG Motor Movement/Imagery Dataset	This data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers.	https://paperswithcode.com/dataset/eeg-motor-movement-imagery-dataset	06/01/2022						
5408	DeepCom-Java	The Java dataset introduced in DeepCom (Deep Code Comment Generation), commonly used to evaluate automated code summarization.	https://paperswithcode.com/dataset/deepcom-java							
5409	ParallelCorpus-Python	The Python dataset introduced in the Parallel Corpus paper (A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation), commonly used for evaluating automated code summarization.	https://paperswithcode.com/dataset/parallelcorpus-python							
5410	Hybrid-DeepCom-Java	The Java dataset introduced in Hybrid-DeepCom (Deep code comment generation with hybrid lexical and syntactical information), commonly used to evaluate automated code summarization. It is basically a further version of DeepCom-Java.	https://paperswithcode.com/dataset/hybrid-deepcom-java							
5411	UFPR-ADMR-v2	"The UFPR-ADMR-v2 dataset contains 5,000 dial meter images obtained on-site by employees of the Energy Company of Paraná (Copel), which serves more than 4M consuming units in the Brazilian state of Paraná. The images were acquired with many different cameras and are available in the JPG format with 320×640 or 640×320 pixels (depending on the camera orientation). More details are available in our paper.
The dataset is split into three subsets: training (3,000 images), validation (1,000 images) and testing (1,000 images). Every image has the following annotations available in a .txt file: the counter’s corners (x1, y1), (x2, y2), (x3, y3), (x4, y4). The corners can be used to rectify the counter patch and represent, respectively, the top-left, top-right, bottom-right, and bottom-left corners. For each dial, the current position (x, y, w, h) and the corresponding reading (the final reading as well as the approximate reading with one decimal place precision). All counters of the dataset (regardless of meter type) have 4 or 5 dials; thus, 22,410 dials were manually annotated."	https://paperswithcode.com/dataset/ufpr-admr-v2	08/01/2022						
5412	ICEWS	A repository that contains political events with a specific timestamp. These political events relate entities (e.g. countries, presidents...) to a number of other entities via logical predicates (e.g. ’Make a visit’ or ’Express intent to meet or negotiate’).	https://paperswithcode.com/dataset/icews	10/09/2018						
5413	XQLFW	"An evaluation protocol for face verification focusing on a large intra-pair image quality difference.
Real-world face recognition applications often deal with suboptimal image quality or resolution due to different capturing conditions such as various subject-to-camera distances, poor camera settings, or motion blur. This characteristic has an unignorable effect on performance. Recent cross-resolution face recognition approaches used simple, arbitrary, and unrealistic down- and up-scaling techniques to measure robustness against real-world edge-cases in image quality. Thus, we propose a new standardized benchmark dataset and evaluation protocol derived from the famous Labeled Faces in the Wild (LFW). In contrast to previous derivatives, which focus on pose, age, similarity, and adversarial attacks, our Cross-Quality Labeled Faces in the Wild (XQLFW) maximizes the quality difference. It contains only more realistic synthetically degraded images when necessary. Our proposed dataset is then used to further investigate the influence of image quality on several state-of-the-art approaches. With XQLFW, we show that these models perform differently in cross-quality cases, and hence, the generalizing capability is not accurately predicted by their performance on LFW. Additionally, we report baseline accuracy with recent deep learning models explicitly trained for cross-resolution applications and evaluate the susceptibility to image quality."	https://paperswithcode.com/dataset/xqlfw	23/08/2021	Cross-Quality Labeled Faces in the Wild					
5414	PSI	The IUPUI-CSRC Pedestrian Situated Intent (PSI) benchmark dataset has two innovative labels besides comprehensive computer vision annotations. The first novel label is the dynamic intent changes for the pedestrians to cross in front of the ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one is the text-based explanations of the driver reasoning process when estimating pedestrian intents and predicting their behaviors during the interaction period.	https://paperswithcode.com/dataset/psi	01/01/2022	IUPUI-CSRC Pedestrian Situated Intent					
5415	E-scooter Rider Detection Benchmark Dataset	A small benchmark dataset for e-scooter rider detection task, and a trained model to support the detection of e-scooter riders from RGB images collected from natural road scenes.	https://paperswithcode.com/dataset/e-scooter-rider-detection-benchmark-dataset	28/11/2021	IUPUI-CSRC-E-Scooter					
5416	Klexikon	"The dataset introduces document alignments between German Wikipedia and the children's lexicon Klexikon.
The source texts in Wikipedia are both written in a more complex language than Klexikon, and also significantly longer, which makes this a suitable application for both summarization and simplification.
In fact, previous research has so far only focused on either of the two, but not comprehensively been studied as a joint task."	https://paperswithcode.com/dataset/klexikon	18/01/2022	Klexikon: A German Dataset for Joint Summarization and Simplification					
5417	H²O Interaction	"H²O is an image dataset annotated for Human-to-human-or-object interaction detection. H²O is composed of the images from V-COCO dataset to which are added images which mostly contain interactions between people. The dataset has been introduced in this paper: Orcesi, A., Audigier, R., Toukam, F. P., & Luvison, B. (2021, December). Detecting Human-to-Human-or-Object (H 2 O) Interactions with DIABOLO. In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021) (pp. 1-8). IEEE.
The annotations were made with Pixano, an opensource, smart annotation tool for computer vision applications: https://pixano.cea.fr/"	https://paperswithcode.com/dataset/h2o-interaction	13/12/2021	Human-to-Human-or-Object Interaction					
5418	CLEAR	CLEAR is a continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). CLEAR is built from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. The pipeline makes use of pretrained vision language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning.	https://paperswithcode.com/dataset/clear	17/01/2022						
5419	Common Phone	Common Phone is a gender-balanced, multilingual corpus recorded from more than 76.000 contributors via Mozilla's Common Voice project. It comprises around 116 hours of speech enriched with automatically generated phonetic segmentation.	https://paperswithcode.com/dataset/common-phone	15/01/2022						
5420	TempQA-WD	TempQA-WD is a benchmark dataset for temporal reasoning designed to encourage research in extending the present approaches to target a more challenging set of complex reasoning tasks. Specifically, the benchmark is a temporal question answering dataset with the following advantages: (a) it is based on Wikidata, which is the most frequently curated, openly available knowledge base, (b) it includes intermediate sparql queries to facilitate the evaluation of semantic parsing based approaches for KBQA, and (c) it generalizes to multiple knowledge bases: Freebase and Wikidata.	https://paperswithcode.com/dataset/tempqa-wd	15/01/2022						
5421	BigDatasetGAN	"BigDatasetGAN is a dataset for pixel-wise ImageNet segmentation. It consists of large synthetic datasets from BigGAN & VQGAN.
Image source: https://arxiv.org/pdf/2201.04684v1.pdf"	https://paperswithcode.com/dataset/bigdatasetgan	12/01/2022						
5422	LaFAN1	"Ubisoft La Forge Animation Dataset (""LAFAN1"")
Ubisoft La Forge Animation dataset and accompanying code for the SIGGRAPH 2020 paper Robust Motion In-betweening.
Shot in May 2017.
This dataset can be used under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (see license.txt).
If you use this dataset or transition benchmarking code, please consider citing the paper:
@article{harvey2020robust,
author    = {Félix G. Harvey and Mike Yurick and Derek Nowrouzezahrai and Christopher Pal},
title     = {Robust Motion In-Betweening},
booktitle = {ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH)},
publisher = {ACM},
volume    = {39},
number    = {4},
year      = {2020}
}
You may also want to consider the following papers, as they also use this dataset (or parts of it):


Learned Motion Matching (Holden et al., 2020)


Subspace Neural Physics: Fast Data-Driven Interactive Simulation (Holden et al., 2019)


DReCon: Data-Driven Responsive Control of Physics-Based Characters (Bergamin et al., 2019)


Robust Solving of Optical Motion Capture Data by Denoising (Holden, 2018)


Recurrent Transition Networks for Character Locomotion (Harvey et al., 2018)


Data
The animation data is contained in the lafan1.zip file.
All the animation sequences are in the BVH file format.
There are 5 subjects in the dataset, 77 sequences, and 496,672 motion frames at 30fps (~4.6 hours).
Every BVH file is named with the following convention: [theme][take number]_[subject ID].bvh.
Any sequences sharing the same theme and take_number were recorded at the same time in the studio.
Themes are high level indicators of the actions in the sequences.
The following themes are present in the LaFAN1 dataset:
|   Theme         |  Description                                  |Number of sequences|
|:----------------|:------------------------------------------    |:-----------------:|
| Obstacles       |  Locomotion on uneven terrain                 |17                 |
| Walk            |  Walking locomotion, with different styles    |12                 |
| Dance           |  Free dancing                                 |8                  |
| Fall and get up |  Falling on the ground and getting back up    |6                  |
| Aiming          |  Locomotion while handling or aiming a gun    |5                  |
| Ground          |  Locomotion while crawling and crouching      |5                  |
| Multiple actions|  Miscellaneous/multiple movements per sequence|4                  |
| Run             |  Jogging/Running locomotion                   |4                  |
| Fight           |  Various fight movements                      |3                  |
| Jumps           |  Locomotion with one and two-leg jumps        |3                  |
| Fight and sports|  Fight and sports movements                   |2                  |
| Push and stumble|  Pushing, stumbling and recovery              |3                  |
| Push and fall   |  Pushing, falling, and getting up             |2                  |
| Sprint          |  Sprinting locomotion                         |2                  |
| Push            |  Pushing adversary                            |1                  |
© [2018] Ubisoft Entertainment. All Rights Reserved"	https://paperswithcode.com/dataset/lafan1	01/07/2020	Ubisoft La Forge Animation Dataset					
5423	CI-AVSR	Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR) is a dataset for in-car command recognition in the Cantonese language with both video and audio data. It consists of 4,984 samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese speakers. Furthermore, the dataset is augmented using common in-car background noises to simulate real environments, producing a dataset 10 times larger than the collected one.	https://paperswithcode.com/dataset/ci-avsr	11/01/2022						
5424	COLDataset	COLDataset is a dataset to facilitate Chinese offensive language detection and model evaluation. It include a Chinese offensive language dataset containing 37k annotated sentences.	https://paperswithcode.com/dataset/coldataset	16/01/2022						
5425	CPP simulated evaluation	"In this repository you can find all the elaborate results that were used for the simulated evaluation of an innovative, optimized for real-life use, STC-based, multi-robot Coverage Path Planning (mCPP) algorithm. For this evaluation were introduced in ""Apostolidis, S. D., Kapoutsis, P. C., Kapoutsis, A. C., & Kosmatopoulos, E. B. (2022). Cooperative multi-UAV coverage mission planning platform for remote sensing applications. Autonomous Robots, 1-28."" 20 ROIs, of different shapes and areas, that may include obstacles inside them. These ROIs along with some benchmark results can be found here: https://github.com/savvas-ap/cpp-simulated-evaluations"	https://paperswithcode.com/dataset/cpp-simulated-evaluation	01/06/2021						
5426	PerPaDa	PerPaDa is a Persian paraphrase dataset that is collected from users' input in a plagiarism detection system.	https://paperswithcode.com/dataset/perpada	17/01/2022						
5427	RuMedBench	RuMedBench is a benchmark dataset for Russian medical language understanding.	https://paperswithcode.com/dataset/rumedbench	17/01/2022						
5428	MuLVE	Multi-Language Vocabulary Evaluation Data Set (MuLVE) is a dataset consisting of vocabulary cards and real-life user answers, labeled indicating whether the user answer is correct or incorrect.	https://paperswithcode.com/dataset/mulve	17/01/2022	Multi-Language Vocabulary Evaluation					
5429	WebUAV-3M	WebUAV-3M is a new million-scale Unmanned Aerial Vehicle (UAV) tracking benchmark consisting of 4,485 videos with more than 3M frames from the Internet. An efficient and scalable Semi-Automatic Target Annotation (SATA) pipeline is devised to label the tremendous WebUAV-3M in every frame. The densely bounding box annotated WebUAV-3M one of the largest public UAV tracking benchmark.	https://paperswithcode.com/dataset/webuav-3m	19/01/2022	WebUAV-3M					
5430	Grep-BiasIR	Grep-BiasIR is a novel thoroughly-audited dataset which aim to facilitate the studies of gender bias in the retrieved results of IR systems.	https://paperswithcode.com/dataset/grep-biasir	19/01/2022						
5431	FIG-Loneliness	FIG-Loneliness (FIne-Grained Loneliness) is a dataset collected by using Reddit posts in two young adult-focused forums and two loneliness related forums consisting of a diverse age group. Annotations by trained human annotators for binary and fine-grained loneliness classifications of the posts are provided.	https://paperswithcode.com/dataset/fig-loneliness	19/01/2022						
5432	IKEA Object State Dataset	IKEA Object State Dataset is a new dataset that contains IKEA furniture 3D models, RGBD video of the assembly process, the 6DoF pose of furniture parts and their bounding box.	https://paperswithcode.com/dataset/ikea-object-state-dataset	16/11/2021						
5433	KazNERD	KazNERD is a dataset for Kazakh named entity recognition. The dataset was built as there is a clear need for publicly available annotated corpora in Kazakh, as well as annotation guidelines containing straightforward--but rigorous--rules and examples. The dataset annotation, based on the IOB2 scheme, was carried out on television news text by two native Kazakh speakers under the supervision of the first author. The resulting dataset contains 112,702 sentences and 136,333 annotations for 25 entity classes.	https://paperswithcode.com/dataset/kaznerd	26/11/2021						
5434	PhoMT	PhoMT is a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs for machine translation.	https://paperswithcode.com/dataset/phomt	23/10/2021						
5435	HS-BAN	HS-BAN is a binary class hate speech (HS) dataset in Bangla language consisting of more than 50,000 labeled comments, including 40.17% hate and rest are non hate speech.	https://paperswithcode.com/dataset/hs-ban	03/12/2021						
5436	PASTRIE	Prepositions Annotated with Supersense Tags in Reddit International English (PASTRIE) is a new corpus containing manually annotated preposition supersenses of English data from presumed speakers of four L1s: English, French, German, and Spanish	https://paperswithcode.com/dataset/pastrie	23/10/2021	Prepositions Annotated with Supersense Tags in Reddit International English					
5437	COPA-SSE	Semi-Structured Explanations for COPA (COPA-SSE) is a new crowdsourced dataset of 9,747 semi-structured, English common sense explanations for COPA questions. The explanations are formatted as a set of triple-like common sense statements with ConceptNet relations but freely written concepts. This semi-structured format strikes a balance between the high quality but low coverage of structured data and the lower quality but high coverage of free-form crowdsourcing. Each explanation also includes a set of human-given quality ratings. With their familiar format, the explanations are geared towards commonsense reasoners operating on knowledge graphs and serve as a starting point for ongoing work on improving such systems.	https://paperswithcode.com/dataset/copa-sse	18/01/2022						
5438	The People’s Speech	The People's Speech is a free-to-download 30,000-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under CC-BY-SA (with a CC-BY subset). The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions.	https://paperswithcode.com/dataset/the-peoples-speech	17/11/2021						
5439	Korean Table Question Answering	Korean tabular dataset is a collection of 1.4M tables with corresponding descriptions for unsupervised pre-training language models. Korean table question answering corpus consists of 70k pairs of questions and answers created by crowd-sourced workers.	https://paperswithcode.com/dataset/korean-table-question-answering	17/01/2022						
5440	EventNarrative	EventNarrative is a knowledge graph-to-text dataset from publicly available open-world knowledge graphs. EventNarrative consists of approximately 230,000 graphs and their corresponding natural language text.	https://paperswithcode.com/dataset/eventnarrative	30/10/2021						
5441	Synthetic Visual Inspections	"Synthetic visual inspection data of structural elements in bridges. The data is generated using the OpenIPDM toolbox ""Generate Synthetic Dataset"". For further details about the data generation and the properties of the dataset, refer to the software manual at https://github.com/CivML-PolyMtl/OpenIPDM/blob/main/Help"	https://paperswithcode.com/dataset/synthetic-visual-inspections	20/01/2022						
5442	PP-HumanSeg14K	A large-scale video portrait dataset that contains 291 videos from 23 conference scenes with 14K frames. This dataset contains various teleconferencing scenes, various actions of the participants, interference of passers-by and illumination change.	https://paperswithcode.com/dataset/pp-humanseg14k	14/12/2021						
5443	MMAC Captions	We provide a dataset called MMAC Captions for sensor-augmented egocentric-video captioning. The dataset contains 5,002 activity descriptions by extending the CMU-MMAC dataset. A number of activity description examples can be found in the homepage.	https://paperswithcode.com/dataset/mmac-captions	07/09/2021						
5444	ARKitScenes	ARKitScenes is an RGB-D dataset captured with the widely available Apple LiDAR scanner. Along with the per-frame raw data (Wide Camera RGB, Ultra Wide camera RGB, LiDar scanner depth, IMU) the authors also provide the estimated ARKit camera pose and ARKit scene reconstruction for each iPad Pro sequence. In addition to the raw and processed data from the mobile device, ARKit.	https://paperswithcode.com/dataset/arkitscenes	17/11/2021						
5445	LasHeR	LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night.	https://paperswithcode.com/dataset/lasher	27/04/2021						
5446	Symmetry-OOD	Symmetry-OOD is a dataset for symmetry perception by deep neural networks.	https://paperswithcode.com/dataset/symmetry-ood	08/12/2021						
5447	Urdu Online Reviews	This corpus was constructed by collecting 10,008 reviews from various domains, including sports, food, software, politics, and entertainment. Human annotators manually tagged the reviews into positive (n = 3662), negative (n = 2619), and neutral (n = 3727) categories.	https://paperswithcode.com/dataset/urdu-online-reviews	28/06/2021	Urdu Online Reviews					
5448	CVGL Camera Calibration Dataset	The dataset has been generated using Town 1 and Town 2 of CARLA Simulator. The dataset consists of $48$ camera configurations with each town having $24$ configurations. The parameters modified for generating the configurations include  $fov$, $x$, $y$, $z$, pitch, yaw, and roll. Here, $fov$ is the field of view, (x, y, z) is the translation while (pitch, yaw, and roll) is the rotation between the cameras. The total number of image pairs is $79,320$, out of which $18,083$ belong to Town 1 while $61,237$ belong to Town 2, the difference in the number of images is due to the length of the tracks.	https://paperswithcode.com/dataset/cvgl-camera-calibration-dataset	23/01/2022						
5449	Tsinghua-Daimler Cyclist Benchmark	"The Tsinghua-Daimler Cyclist Benchmark provides a benchmark dataset for cyclist detection. Bounding Box based labels are provided for the classes: (""pedestrian"", ""cyclist"", ""motorcyclist"", ""tricyclist"", ""wheelchairuser"", ""mopedrider"")."	https://paperswithcode.com/dataset/tsinghua-daimler-cyclist-benchmark							
5450	Data and Material for 'Interpersonal Conflicts During Code Review'	"About the study
This study was exploring the landscape of interpersonal conflicts during code review in following areas:
- how these conflicts look like
- what role do they play in software development
- what are their consequences
- what factors do play role in their appearance and severity
- what strategies can be used to prevent and manage conflicts
Methodology
The study collected 22 interviews with developers. Using qualitative thematic analysis, the authors analysed the anonymised interviews for the form and context of interpersonal conflicts during code review and potential strategies to prevent and manage them. The analysis was conducted in NVivo 12, a software for qualitative analysis.
To support validity of the analysis, it was revised through an ""Audit Trail"" like process. The analysis, study methodology and results have been examined by a senior researcher within the team. The files included in the audit are included in this folder as well.
Dataset Content

This folder contains online material for the study clarifying and extending information for reviewers and readers interested in the methodology. 

README.txt - information on the content and purpose of this online material

The Analysis

conflicts_analysis.nvpx - NVivo 12 file containing the complete results, transcripts, codings and definitions used and created throught the analysis
definitions.pdf - definitions of the final higher themes identified in the analysis
codes.pdf - complete list of codes and themes coded during the analysis

Supplementary documents:

sample_descriptives.pdf - Description of individual participants and their characteristics
interview_structure.pdf - Structure of the interviews conducted in the study
participant_consent_form.pdf - Participant Consent that has been signed by the participants in the study
transcripts.pdf - a file containing the complete set of anonymised transcripts
Audit - Folder containing files submitted for the Audit Trail except of files constant throughout the study like Participant Consent or Transcripts of the interviews.

Audit Trail
Audit trail is a procedure to validate results of qualitative analysis. It requires the researcher to provide detailed information on how they conducted the analysis to auditors external to the analysis. The goal of a formal audit is to examine both the process and product of the inquiry, and determine the trustworthiness of the findings. This folder contains files to perform the audit trail for the study.
Goal
The goal of the audit is:
- to get acquainted with the methodology and results of the analysis and related documentation
- to review whether the results of the analysis are a good representation of the data
- to review whether the analysis does not breach knowledge available to software engineering, unless well supported
- to review whether the results are useful for the community
- to control for issues and shortcomings of the analysis
Files
The ""Audit"" folder contains following files (We recommend reviewing them in this order):
- analysisplan_results.pdf - text summarising background, methodology and results of the study
- results - folder with the latex files to generate analysisplan_results.pdf
- memos.pdf - researcher notes on issues and decision making during the analysis
- definitions_audit.pdf - definitions of higher level themes included in the results report
- codes_audit.pdf - extended definitions file containing all the codes and themes
- conflicts_analysis_audit.nvpx - NVivo file with complete coding"	https://paperswithcode.com/dataset/data-and-material-for-interpersonal-conflicts	14/01/2022						
5451	DKhate	"A corpus of Offensive Language and Hate Speech Detection for Danish
This DKhate dataset contains 3600 comments from the web annotated for offensive language, following the Zampieri et al. / OLID scheme.
Submissions and benchmarks for the OffensEval 2020 Danish track are also included."	https://paperswithcode.com/dataset/dkhate	13/08/2019						
5452	Broad Twitter Corpus	This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire.	https://paperswithcode.com/dataset/broad-twitter-corpus	01/12/2016						
5453	DanFEVER	We present a dataset, DANFEVER, intended for claim verification in Danish. The dataset builds upon the task framing of the FEVER fact extraction and verification challenge. DANFEVER can be used for creating models for detecting mis- & disinformation in Danish as well as for verification in multilingual settings.	https://paperswithcode.com/dataset/danfever	01/05/2021						
5454	DAGW	"It’s hard to develop good tools for processing Danish with computers when no large and wide-coverage dataset of Danish text is readily available. To address this, the Danish Gigaword Project (DAGW) maintains a corpus for Danish with over a billion words. The general goals are to create a dataset that is:

representative;
accessible;
a suitable common starting point for Danish NLP models."	https://paperswithcode.com/dataset/dagw	01/05/2021	Danish Gigaword					
5455	NASA Crew Exploration Vehicle (CEV) Software Event Log	Extensible Event Stream (XES) software event log obtained through instrumenting the NASA CEV class using the tool available at {https://svn.win.tue.nl/repos/prom/XPort/}. This event log contains method-call level events describing a single run of an exhaustive unit test suite for the Crew Exploration Vehicle (CEV) example available and documented at {http://babelfish.arc.nasa.gov/trac/jpf/wiki/projects/jpf-statechart} (trac) {http://babelfish.arc.nasa.gov/hg/jpf/jpf-statechart} (mercurial repository). Note that the life-cycle information in this log corresponds to method call (start) and return (complete), and captures a method-call hierarchy. We attached a slightly preprocessed variant of this event log, where the execution of each unit test method is represented as a separate trace.	https://paperswithcode.com/dataset/nasa-crew-exploration-vehicle-cev-software							
5456	FloorPlanCAD	FloorPlanCAD is a large-scale real-world CAD drawing dataset containing over 15,000 floor plans, ranging from residential to commercial buildings.	https://paperswithcode.com/dataset/floorplancad	15/05/2021						
5457	DABS	DABS is a domain-agnostic benchmark for self-supervised learning to encourage research and progress towards domain-agnostic methods.	https://paperswithcode.com/dataset/dabs	23/11/2021	Domain-Agnostic Benchmark for Self-supervised learning					
5458	CamGes	"The size of the data set is about 1GB.
The data set consists of 900 image sequences of 9 gesture classes, which are defined by 3 primitive hand shapes and 3 primitive motions. Therefore, the target task for this data set is to classify different shapes as well as different motions at a time."	https://paperswithcode.com/dataset/camgesdata		Cambridge Hand Gesture Dataset					
5459	AWARE	"The peer-reviewed paper of AWARE dataset is published in ASEW 2021, and can be accessed through: http://doi.org/10.1109/ASEW52652.2021.00049. Kindly cite this paper when using AWARE dataset.
Aspect-Based Sentiment Analysis (ABSA) aims to identify the opinion (sentiment) with respect to a specific aspect. Since there is a lack of smartphone apps reviews dataset that is annotated to support the ABSA task, we present AWARE: ABSA Warehouse of Apps REviews.
AWARE contains apps reviews from three different domains (Productivity, Social Networking, and Games), as each domain has its distinct functionalities and audience. Each sentence is annotated with three labels, as follows: 
Aspect Term: a term that exists in the sentence and describes an aspect of the app that is expressed by the sentiment. A term value of “N/A” means that the term is not explicitly mentioned in the sentence.
Aspect Category: one of the pre-defined set of domain-specific categories that represent an aspect of the app (e.g., security, usability, etc.).
Sentiment: positive or negative.
Note: games domain does not contain aspect terms.
We provide a comprehensive dataset of 11323 sentences from the three domains, where each sentence is additionally annotated with a Boolean value indicating whether the sentence expresses a positive/negative opinion. In addition, we provide three separate datasets, one for each domain, containing only sentences that express opinions. The file named “AWARE_metadata.csv” contains a description of the dataset’s columns.
How AWARE can be used?
We designed AWARE such that it can be used to serve various tasks. The tasks can be, but are not limited to:
Sentiment Analysis.
Aspect Term Extraction.
Aspect Category Classification.
Aspect Sentiment Analysis.
Explicit/Implicit Aspect Term Classification.
Opinion/Not-Opinion Classification.
Furthermore, researchers can experiment with and investigate the effects of different domains on users' feedback.
Source: AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation"	https://paperswithcode.com/dataset/aware	19/11/2021	AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation					
5460	ManyTypes4TypeScript	Type Inference dataset for TypeScript. Click on DOI tag for dataset files.	https://paperswithcode.com/dataset/manytypes4typescript	07/03/2022						
5461	Survey answers	Please see paper for questions. These are the answers to the surveys, processed and included in the paper via knitr	https://paperswithcode.com/dataset/survey-answers	25/01/2022	Answers to surveys in both papers, as well as processed answers					
5462	MARIDA	"MARIDA (Marine Debris Archive) is the first dataset based on the multispectral Sentinel-2 (S2) satellite data, which distinguishes Marine Debris from various marine features that co-exist, including Sargassum macroalgae, Ships, Natural Organic Material, Waves, Wakes, Foam, dissimilar water types (i.e., Clear, Turbid Water, Sediment-Laden Water, Shallow Water), and Clouds. MARIDA is an open-access dataset which enables the research community to explore the spectral behaviour of certain floating materials, sea state features and water types, to develop and evaluate Marine Debris detection solutions based on artificial intelligence and deep learning architectures, as well as satellite pre-processing pipelines.  Although it is designed to be beneficial for several machine learning tasks, it primarily aims to benchmark weakly supervised pixel-level semantic segmentation learning methods. 
MARIDA can be downloaded from the repository Zenodo (https://doi.org/10.5281/zenodo.5151941). A quick start guide for all ML benchmarks and the detailed overview of the dataset are available at https://marine-debris.github.io/."	https://paperswithcode.com/dataset/marida	07/01/2022	Marine Debris Archive					
5463	A ground-truth dataset to identify bots in GitHub	This dataset is a ground truth dataset that is used to identify bots. Each account in this dataset is rated by at least 3 raters with a high interrater agreement.	https://paperswithcode.com/dataset/a-ground-truth-dataset-to-identify-bots-in	07/10/2020						
5464	ISBNet	ISBNet is a dataset of images of recyclables. It is hand collected by our group at the International School of Beijing. The trash in these images was gathered from trash bins around the school. ISBNet totals 889 images distributed across 5 classes: cans (74), landfill (410), paper (182), plastic (122), and tetra pak (101). The data acquisition process involved using a piece of black poster paper as a background; this would create enough contrast for trash belonging to the paper category. These pictures were taken with an iPhone 8 and an iPhone XS. We recorded the trash bin in which the piece of trash originated from and any trash generating landmarks nearby.  Please refer to the paper (ThanosNet: A Novel Trash Classification Method Using Metadata) for more about the format of the metadata.	https://paperswithcode.com/dataset/isbnet	19/03/2021						
5465	TUH EEG Seizure Corpus	"Our goal is to enable deep learning research in neuroscience by releasing the largest publicly available unencumbered database of EEG recordings. This ongoing project currently includes over 30,000 EEGs spanning the years from 2002 to present. Data collected can be used for both research and commercialization purposes.
Iyad Obeid and Joseph Picone. The temple university hospital eeg data corpus. Frontiers in neuroscience,10:196, 2016."	https://paperswithcode.com/dataset/tuh-eeg-seizure-corpus		Temple University Hospital (TUH) EEG Corpus					
5466	PPG  Dalia	PPG-DaLiA is a publicly available dataset for PPG-based heart rate estimation. This multimodal dataset features physiological and motion data, recorded from both a wrist- and a chest-worn device, of 15 subjects while performing a wide range of activities under close to real-life conditions. The included ECG data provides heart rate ground truth. The included PPG- and 3D-accelerometer data can be used for heart rate estimation, while compensating for motion artefacts.	https://paperswithcode.com/dataset/ppg-dalia-1	30/07/2019	PPG Field Study Dataset					
5467	IGLUE	The Image-Grounded Language Understanding Evaluation (IGLUE) benchmark brings together—by both aggregating pre-existing datasets and creating new ones—visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. The benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups.	https://paperswithcode.com/dataset/iglue	27/01/2022	Image-Grounded Language Understanding Evaluation					
5468	ModelNet40-C	"ModelNet40-C is a comprehensive dataset to benchmark the corruption robustness of 3D point cloud recognition.
We create ModelNet40-C based on the ModelNet40 validation set with 15 corruption types and 5 severity levels for each corruption type including density, noise, and transformation corruption patterns. Our dataset contains 185,000 distinct point clouds that help provide a comprehensive picture of model robustness."	https://paperswithcode.com/dataset/modelnet40-c	28/01/2022	ModelNet-C					
5469	Expressive Gaussian mixture models for high-dimensional statistical modelling: simulated data and neural network model files	"Neural network model files and Madgraph event generator outputs used as inputs to the results presented in the paper ""Learning to discover: expressive Gaussian mixture models for multi-dimensional simulation and parameter inference in the physical sciences"" arXiv:2108.11481; 2022 Mach. Learn.: Sci. Technol. 3 015021 
Code and model files can be found at:
https://github.com/darrendavidprice/science-discovery/tree/master/expressive_gaussian_mixture_models"	https://paperswithcode.com/dataset/expressive-gaussian-mixture-models-for-high	10/12/2021						
5470	Simulated EM showers data	"Electromagnetic (EM) showers simulated dataset. The data contains 16,577 showers. The data includes information about the tracklets: position coordinates, direction and shower id, and about the showers: shower id, initial particle position and direction, shower energy.  
Generation is done using FairShip framework. Showers energy follows gamma distribution with parameters alpha=1.4, beta = 0.5. Polar angle is simulated using log-normal distribution with parameters nu=0,3, sigma=0.7. 
The data is in csv native format. Total dataset size is 669.2 MB."	https://paperswithcode.com/dataset/simulated-em-showers-data	14/10/2021						
5471	Validation Dataset	"AlgorithmComparison:
Comparison of algorithms on benchmark test cases. Details on included in the paper. 10 cases for each algorithm / benchmark test. Optimum.txt file includes the history of best optimum and SamplePointsResults.txt file contains results for all the black-box function evaluations. Last colum represents the objective value. GlobalOptimum.txt represents the global optimum for that specific test case.
AcquisitionFunctionComparison:
Comparison of acquisition functions within the MixMOBO framework. 10 cases for each acqusition function / benchmark test. File structure similar to AlgorithmComparison, except for ZDT6 where Optimum.txt file represents the current Pareto-optimal solution.
ArchitectedMaterialOptimization:
Optimization progress history of architected material optimization given in Optimum.txt file. -1, 0, 1, 2 represent unit cell A,B,C,D. The last value represent buckling load -1*P_c."	https://paperswithcode.com/dataset/validation-dataset	30/01/2022						
5472	QALD-9-Plus	"QALD-9-Plus Dataset Description
QALD-9-Plus is the dataset for Knowledge Graph Question Answering (KGQA) based on well-known QALD-9.
QALD-9-Plus enables to train and test KGQA systems over DBpedia and Wikidata using questions in 9 different languages: English, German, Russian, French, Armenian, Belarusian, Lithuanian, Bashkir, and Ukrainian.
Some of the questions have several alternative writings in particular languages which enables to evaluate the robustness of KGQA systems and train paraphrasing models.
As the questions' translations were provided by native speakers, they are considered as ""gold standard"", therefore, machine translation tools can be trained and evaluated on the dataset.
Dataset Statistics
|       |  en |  de | fr |  ru  |  uk |  lt |  be |  ba | hy | # questions DBpedia | # questions Wikidata |
|-------|:---:|:---:|:--:|:----:|:---:|:---:|:---:|:---:|:--:|:-----------:|:-----------:|
| Train | 408 | 543 | 260 | 1203 | 447 | 468 | 441 | 284 | 80 |     408     |     371     |
| Test  | 150 | 176 | 26 |  348 | 176 | 186 | 155 | 117 | 20 |     150     |     136     |
Given the numbers, it is obvious that some of the languages are covered more than once i.e., there is more than one translation for a particular question.
For example, there are 1203 Russian translations available while only 408 unique questions exist in the training subset (i.e., 2.9 Russian translations per one question).
The availability of such parallel corpora enables the researchers, developers and other dataset users to address the paraphrasing task."	https://paperswithcode.com/dataset/qald-9-plus	31/01/2022						
5473	SuperMUDI	"The Super-resolution of Multi-Dimensional Diffusion MRI
(Super MUDI) dataset  contains the data of four healthyhuman subjects with ages range between 19 and 46 years.
For each subject 1,344 MRI volumes are provided. Theimaging device was clinical 3T Philips Achieva Scanner
(Best, Netherlands) with a 32-channel adult head coil.
The Super MUDI Challenge comprises two tasks:
isotropic, and anisotropic super-resolution. The names of
these tasks were derived from the acquisition strategies of
the low-resolution MRI data. The objective of using two
down-sampling strategies is to compare the combinations of
the down-sampling methods and the super-resolution
approaches that can best to be used in a clinical scheme to
obtain simulated high-quality and high-fidelity MRI images
while reducing the acquisition time. In the anisotropic
subsampling the volume has high in-plane resolution
(2.5mm ×2.5mm), but thick axial slice (5mm), while in the
isotropic subsampling the volume has low resolution (5mm)
in all the directions.
For our experiments, we use one subject each for training
and validation, and two for testing.
Reference: Marco Pizzolato, Marco Palombo, Jana Hutter, Vish-
wesh Nash, Fan Zhang, and Noemi Gyori, “Super-
resolution of Multi Dimensional Diffusion MRI data,”
Mar. 2020"	https://paperswithcode.com/dataset/supermudi							
5474	Violent-Flows	"Crowd Violence \ Non-violence Database and benchmark: A database of real-world, video footage of crowd violence, along with standard benchmark protocols designed to test both violent/non-violent classification and violence outbreak detections. The data set contains 246 videos. All the videos were downloaded from YouTube. The shortest clip duration is 1.04 seconds, the longest clip is 6.52 seconds, and the average length of a video clip is 3.60 seconds.
Introduced in:
Tal Hassner, Yossi. Itcher, and Orit Kliper-Gross, Violent Flows: Real-Time Detection of Violent Crowd Behavior, 3rd IEEE International Workshop on Socially Intelligent Surveillance and Monitoring (SISM) at the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Rhode Island, June 2012 ."	https://paperswithcode.com/dataset/violent-flows	21/06/2012						
5475	WikiConvert	Wiki-Convert is a 900,000+ sentences dataset of precise number annotations from English Wikipedia. It relies on Wiki contributors' annotations in the form of a {{Convert}} template.	https://paperswithcode.com/dataset/wikiconvert	01/11/2021						
5476	CVIT PIB	We present sentence aligned parallel corpora across 10 Indian Languages - Hindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi, Punjabi, and English - many of which are categorized as low resource. The corpora are compiled from online sources which have content shared across languages. The corpora presented significantly extends present resources that are either not large enough or are restricted to a specific domain (such as health). We also provide a separate test corpus compiled from an independent online source that can be independently used for validating the performance in 10 Indian languages. Alongside, we report on the methods of constructing such corpora using tools enabled by recent advances in machine translation and cross-lingual retrieval using deep neural network based methods.	https://paperswithcode.com/dataset/cvit-pib	14/05/2020						
5477	AMR3.0	Abstract Meaning Representation (AMR) Annotation Release 3.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 59,255 English natural language sentences from broadcast conversations, newswire, weblogs, web discussion forums, fiction and web text.	https://paperswithcode.com/dataset/amr3-0		Abstract Meaning Representation (AMR) Annotation Release 3.0					
5478	ShapeNetCore	ShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+, a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore.	https://paperswithcode.com/dataset/shapenetcore	09/12/2015						
5479	AbdomenCT-1K	"We present a large and diverse abdominal CT organ segmentation dataset, termed AbdomenCT-1K, with more than 1000 (1K) CT scans from 12 medical centers, including multi-phase, multi-vendor, and multi-disease cases. 
Furthermore, we conduct a large-scale study for liver, kidney, spleen, and pancreas segmentation and reveal the unsolved segmentation problems of the SOTA methods, such as the limited generalization ability on distinct medical centers, phases, and unseen diseases. 
To advance the unsolved problems, we further build four organ segmentation benchmarks for fully supervised, semi-supervised, weakly supervised, and continual learning, which are currently challenging and active research topics. Accordingly, we develop a simple and effective method for each benchmark, which can be used as out-of-the-box methods and strong baselines. 
We believe the AbdomenCT-1K dataset will promote future in-depth research towards clinical applicable abdominal organ segmentation methods."	https://paperswithcode.com/dataset/abdomenct-1k	27/07/2021						
5480	CodeContests	"CodeContests is a competitive programming dataset for machine-learning. This dataset was used when training AlphaCode.
It consists of programming problems, from a variety of sources.
Problems include test cases in the form of paired inputs and outputs, as well as both correct and incorrect human solutions in a variety of languages."	https://paperswithcode.com/dataset/codecontests	02/02/2022						
5481	NR-HCPI	NR-HCPI (Non-redundant Human CPI dataset)	https://paperswithcode.com/dataset/nr-hcpi	28/01/2022	Non-redundant Human CPI dataset					
5482	MedVidQA	The MedVidQA dataset contains the collection of 3, 010 manually created health-related questions and timestamps as visual answers to those questions from trusted video sources, such as accredited medical schools with an established reputation, health institutes, health education, and medical practitioners.	https://paperswithcode.com/dataset/medvidqa	30/01/2022	Medical Video Question Answering					
5483	MedVidCL (Medical Video Classification)	The MedVidCL dataset contains a collection of 6, 617 videos annotated into ‘medical instructional’, ‘medical non-instructional' and ‘non-medical’ classes.  A two-step approach is used to construct the MedVidCL dataset. In the first step, the videos annotated by health informatics experts are used  to train a machine learning model that predicts the given video to one of the three aforementioned classes. In the second step, only the high-confidence videos are used and health informatics experts assess the model’s predicted video category and update the category wherever needed.	https://paperswithcode.com/dataset/medvidcl	30/01/2022						
5484	Illness-dataset	A dataset for evaluating text classification, domain adaptation, and active learning models. The dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer's, Parkinson's, Cancer, and Diabetes.	https://paperswithcode.com/dataset/illness-dataset	05/12/2021	Illness multi-domain textual dataset					
5485	IEEE-CIS 3rd Technical Challenge	"The IEEE Computational Intelligence Society ran a competition from July to November 2021 for predicting and optimizing based on renewable energy data.
The data was from six buildings and six solar installations at the Monash University, Clayton campus, in Melbourne, Victoria, Australia.
The data was at 15 minute resolution and from the years 2016 to 2020.
Competitors had to predict solar generation and building electricity usage for the months of October and November 2020, given perfect weather forecasts from the Australian BOM and European ECMWF.
Then, they had to submit a schedule for classes to minimize electricity cost based on peak demand and (known) electricity prices for the month.
Citation: C. Bergmeir, F. de Nijs et al. ""IEEE-CIS technical challenge on predict+optimize for renewable energy scheduling,"" 2021. [Online]. Available: https://dx.doi.org/10.21227/1x9c-0161"	https://paperswithcode.com/dataset/ieee-cis-3rd-technical-challenge	01/07/2021	IEEE-CIS Technical Challenge on Predict+Optimize for Renewable Energy Scheduling					
5486	LARa	LARa is the first freely accessible logistics-dataset for human activity recognition. In the ’Innovationlab Hybrid Services in Logistics’ at TU Dortmund University, two picking and one packing scenarios with 14 subjects were recorded using OMoCap, IMUs, and an RGB camera. 758 minutes of recordings were labeled by 12 annotators in 474 person-hours. The subsequent revision was carried out by 4 revisers in 143 person-hours. All the given data have been labeled and categorised into 8 activity classes and 19 binary coarse-semantic descriptions, also called attributes.	https://paperswithcode.com/dataset/lara	22/07/2020	Logistic Activity Recognition Challenge					
5487	Topic modeling topic coverage dataset	"A prevalent use case of topic models is that of topic discovery.
However, most of the topic model evaluation methods rely on abstract metrics such as perplexity or topic coherence.
The topic coverage approach is to measure the models' performance by matching model-generated topics to topics discovered by humans.
This way, the models are evaluated in the context of their use, by essentially simulating
topic modeling in a fixed setting defined by a text collection and a set of reference topics.
Reference topics represent a ground truth that can be used to evaluate both topic models and other measures of model performance.
The coverage approach enables large-scale automatic evaluation of both existing and future topic models.
The topic coverage dataset consists of two text collections and two sets of reference topics.
These two sub-datasets correspond to two domains (news text and biological text) 
where topic models are used for topic discovery in large text collections.
The reference topics consist of model-generated topics inspected, selected, and curated by humans.
Each dataset contains a corpus of preprocessed (tokenized) texts and a set of reference topics, 
each represented by a list of words and text documents.
The dataset details, including the instruction for the use of the data and supporting code, are here:
https://github.com/dkorenci/topic_coverage/blob/main/data.readme.txt
The coverage measures that can be used to evaluate topic models are described in the accompanying paper, 
whereas the code and the instructions can be found in the github repo."	https://paperswithcode.com/dataset/topic-modeling-topic-coverage-dataset	31/08/2021						
5488	OpenML-CC18	"We advocate the use of curated, comprehensive benchmark suites of machine learning datasets, backed by standardized OpenML-based interfaces and complementary software toolkits written in Python, Java and R. We demonstrate how to easily execute comprehensive benchmarking studies using standardized OpenML-based benchmarking suites and complementary software toolkits written in Python, Java and R. Major distinguishing features of OpenML benchmark suites are (i) ease of use through standardized data formats, APIs, and existing client libraries; (ii) machine-readable meta-information regarding the contents of the suite; and (iii) online sharing of results, enabling large scale comparisons. As a first such suite, we propose the OpenML-CC18, a machine learning benchmark suite of 72 classification datasets carefully curated from the thousands of datasets on OpenML.
The inclusion criteria are:
* classification tasks on dense data set
independent observations
* number of classes >= 2, each class with at least 20 observations and ratio of minority to majority class must exceed 5%
* 500 <= number of observations <= 100000
* number of features after one-hot-encoding < 5000
* no artificial data sets
* no subsets of larger data sets nor binarizations of other data sets
* no data sets which are perfectly predictable by using a single feature or by using a simple decision tree
* source or reference available
If you use this benchmarking suite, please cite:
Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G. Mantovani, Jan N. van Rijn and Joaquin Vanschoren. “OpenML Benchmarking Suites” arXiv:1708.03731v2 [stats.ML] (2019).
@article{oml-benchmarking-suites,
title={OpenML Benchmarking Suites},
author={Bernd Bischl and Giuseppe Casalicchio and Matthias Feurer and Frank Hutter and Michel Lang and Rafael G. Mantovani and Jan N. van Rijn and Joaquin Vanschoren},
year={2019},
journal={arXiv:1708.03731v2 [stat.ML]}
}"	https://paperswithcode.com/dataset/openml-cc18	11/08/2017						
5489	Cornell Movie Review Data	movie-review data for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative)	https://paperswithcode.com/dataset/cornell-movie-review-data	28/05/2002						
5490	NDPSID - WACV 2019	This database offers iris images (with and without contact lenses) of the same eyes captured shortly one after another with illumination coming from two different locations. 5,796 iris images in total were acquired by the LG IrisAccess 4000 sensor from 119 subjects. This set is divided into four subsets used in the experiments: (a) 1,800 images of irises wearing regular (with dot-like pattern) textured contact lenses, as shown in Fig. 6a in the wAcv 2019 paper; (b) 864 images of irises wearing irregular (without dot-like pattern) textured contact lenses, as shown in Fig. 6b in the WACV 2019 paper; (c) 1,728 images of irises wearing clear contact lenses (without any visible pattern), and (d) 1,404 images of authentic irises without any contact.	https://paperswithcode.com/dataset/ndpsid-wacv-2019	18/11/2018	Notre Dame Photometric Stereo Iris Dataset					
5491	CAT: Context Adjustment Training	"CAT is a specialized dataset for co-saliency detection. This dataset is intended for both helping to assess the performance of vision algorithms and supporting research that aims to exploit large volumes of annotated data, e.g., for training deep neural networks.
Scale & Features
- A total number of 33500 image samples.
- 280 semantic groups affiliated to 15 superclasses.
- High-quality mask annotations.
- Diverse visual context with multiple foreground objects."	https://paperswithcode.com/dataset/cat-context-adjustment-training	31/01/2022						
5492	Colored-MNIST(with spurious correlation)	This is a dataset with spurious correlations which can be used to evaluate machine learning methods for out-of-distribution generalization, causal inference, and related field.	https://paperswithcode.com/dataset/colored-mnist-spurious-correlation	05/07/2019						
5493	JaQuAD	JaQuAD (Japanese Question Answering Dataset) is a question answering dataset in Japanese that consists of 39,696 extractive question-answer pairs on Japanese Wikipedia articles.	https://paperswithcode.com/dataset/jaquad	03/02/2022						
5494	Tool clustering dataset	"Tool Database for image-set clustering
This database was generated to evaluate a robotic application dealing with image-set clustering. The goal is to sort and store tools on an table in an unsupervised way, from pixel inputs. Pictures contains objects that can be found in a shop-floor. Each picture contains only one object. There are five different conditions, for each condition, lighting conditions and background are changed. For each condition, four picture of each object are taken under different orientations."	https://paperswithcode.com/dataset/tool-clustering-dataset	06/07/2017						
5495	EmoFilm	"EmoFilm is a multilingual emotional speech corpus comprising 1115 audio instances produced in English, Italian, and Spanish languages. The audio clips (with a mean length of 3.5 sec. and std 1.2 sec.) were extracted in wave format (uncompressed, mono, 48 kHz sample rate and 16-bit) from 43 films (original in English and their over-dubbed Italian and Spanish versions). Genres including comedy, drama, horror, and thriller were considered; anger, contempt, happiness, fear, and sadness emotional states were taken into account. EmoFilm has been presented at Interspeech 2018:
Emilia Parada-Cabaleiro, Giovanni Costantini, Anton Batliner, Alice Baird, and Björn Schuller (2018), Categorical vs Dimensional Perception of Italian Emotional Speech, in Proc. of Interspeech, Hyderabad, India, pp. 3638-3642."	https://paperswithcode.com/dataset/emofilm		Emotional speech from Films					
5496	SES	Currently, an essential point in speech synthesis is the addressing of the variability of human speech. One of the main sources of this diversity is the emotional state of the speaker. Most of the recent work in this area has been focused on the prosodic aspects of speech and on rule-based formant synthesis experiments. Even when adopting an improved voice source, we cannot achieve a smiling happy voice or the menacing quality of cold anger. For this reason, we have performed two experiments aimed at developing a concatenative emotional synthesiser, a synthesiser that can copy the quality of an emotional voice without an explicit mathematical model.	https://paperswithcode.com/dataset/ses		Spanish Emotional Speech					
5497	AESI	The development of ecologically valid procedures for collecting reliable and unbiased emotional data towards computer interfaces with social and affective intelligence targeting patients with mental disorders. Following its development, presented with, the Athens Emotional States Inventory (AESI) proposes the design, recording and validation of an audiovisual database for five emotional states: anger, fear, joy, sadness and neutral. The items of the AESI consist of sentences each having content indicative of the corresponding emotion. Emotional content was assessed through a survey of 40 young participants with a questionnaire following the Latin square design. The emotional sentences that were correctly identified by 85% of the participants were recorded in a soundproof room with microphones and cameras. A preliminary validation of AESI is performed through automatic emotion recognition experiments from speech. The resulting database contains 696 recorded utterances in Greek language by 20 native speakers and has a total duration of approximately 28 min. Speech classification results yield accuracy up to 75.15% for automatically recognizing the emotions in AESI. These results indicate the usefulness of our approach for collecting emotional data with reliable content, balanced across classes and with reduced environmental variability.	https://paperswithcode.com/dataset/aesi		Athens Emotional States Inventory					
5498	Yeast colony morphologies	"Data for the paper entitled Quantifying yeast colony morphologies with feature engineering from time-lapse photography by A. Goldschmidt et al. (https://arxiv.org/abs/2201.05259)
This project is a collaboration between Dudley Lab at the Pacific NW Research Institute and the J. Nathan Kutz group at the University of Washington.
Summary: Baker's yeast (Saccharomyces cerevisiae) is a model organism for studying the morphology that emerges at the scale of multi-cell colonies. To look at how morphology develops, we collect a dataset of time-lapse photographs of the growth of different strains of S. cerevisiae."	https://paperswithcode.com/dataset/yeast-colony-morphologies	13/01/2022	Quantifying yeast colony morphologies with feature engineering from time-lapse photography					
5499	ChEMBL v.27	The standardised ChEMBL v.27 data set. Originally was taken from https://www.ebi.ac.uk/chembl/. Standardisation procedure is described in the HyFactor article, doi:10.26434/chemrxiv-2021-18x0d	https://paperswithcode.com/dataset/chembl-v-27	06/12/2021						
5500	MOSES	"The set is based on the ZINC Clean Leads collection. It contains 4,591,276 molecules in total, filtered by molecular weight in the range from 250 to 350 Daltons, a number of rotatable bonds not greater than 7, and XlogP less than or equal to 3.5. We removed molecules containing charged atoms or atoms besides C, N, S, O, F, Cl, Br, H or cycles longer than 8 atoms. The molecules were filtered via medicinal chemistry filters (MCFs) and PAINS filters.
The dataset contains 1,936,962 molecular structures. For experiments, we split the dataset into a training, test and scaffold test sets containing around 1.6M, 176k, and 176k molecules respectively. The scaffold test set contains unique Bemis-Murcko scaffolds that were not present in the training and test sets. We use this set to assess how well the model can generate previously unobserved scaffolds."	https://paperswithcode.com/dataset/moses	29/11/2018	Molecular sets (MOSES)					
5501	BDD100K-weather(OOD Setting)	BDD100K-weather is a dataset which is inherited from BDD100K using image attribute labels for Out-of-Distribution object detection. All images in BDD100K are categorized into six domains, including clear, overcast, foggy, partly cloudy, rainy and snowy. Clear and overcast are used for training while the rest is used for testing, moreover, per training domain is sampled 1.5k images at most while per testing domain is sampled 0.5k images at most. Thus, we have BDD100K-weather (paper is under review).	https://paperswithcode.com/dataset/bdd100k-weather-ood-setting	12/05/2018						
5502	CoAuthor	CoAuthor is a dataset designed for revealing GPT-3's capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions.	https://paperswithcode.com/dataset/coauthor	18/01/2022						
5503	Met	The Met dataset is a large-scale dataset for Instance-Level Recognition (ILR) in the artwork domain. It relies on the open access collection from the Metropolitan Museum of Art (The Met) in New York to form the training set, which consists of about 400k images from more than 224k classes, with artworks of world-level geographic coverage and chronological periods dating back to the Paleolithic period. Each museum exhibit corresponds to a unique artwork, and defines its own class. The training set exhibits a long-tail distribution with more than half of the classes represented by a single image, making it a special case of few-shot learning.	https://paperswithcode.com/dataset/met	03/02/2022						
5504	ContractNLI	"ContractNLI is a dataset for document-level natural language inference (NLI) on contracts whose goal is to automate/support a time-consuming procedure of contract review. In this task, a system is given a set of hypotheses (such as “Some obligations of Agreement may survive termination.”) and a contract, and it is asked to classify whether each hypothesis is entailed by, contradicting to or not mentioned by (neutral to) the contract as well as identifying evidence for the decision as spans in the contract.
ContractNLI is the first dataset to utilize NLI for contracts and is also the largest corpus of annotated contracts (as of September 2021). ContractNLI is an interesting challenge to work on from a machine learning perspective (the label distribution is imbalanced and it is naturally multi-task, all the while training data being scarce) and from a linguistic perspective (linguistic characteristics of contracts, particularly negations by exceptions, make the problem difficult)."	https://paperswithcode.com/dataset/contractnli	05/10/2021	ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts					
5505	Data Science Problems	Evaluate a natural language code generation model on real data science pedagogical notebooks! Data Science Problems (DSP) includes well-posed data science problems in Markdown along with unit tests to verify correctness and a Docker environment for reproducible execution. About 1/3 of notebooks in this benchmark also include data dependencies, so this benchmark not only can test a model's ability to chain together complex tasks, but also evaluate the solutions on real data! See our paper Training and Evaluating a Jupyter Notebook Data Science Assistant for more details about state of the art results and other properties of the dataset.	https://paperswithcode.com/dataset/data-science-problems	30/01/2022						
5506	Visual Fields	"28,943 Humphrey Visual Field (HVF) tests from 3,871 patients and 7,428 eyes. 
This file contains sensitivity values, TD values, age, laterality (left or right eye) and gender when specified. Sensitivity and TD values are stored both in long format (as a vector) and provided as an 8 x 9 matrix. The latter is meant to preserve the original spatial organization of the data, which is particularly useful in spatial-aware processing often employed in machine learning. All visual field data are stored as a right eye. Empty matrix cells are filled with a fixed value (100).
Institution: University of Washington
Data Collection: between 1998 and 2018.
Please cite:  Giovanni Montesano, Andrew Chen, Randy Lu, Cecilia S. Lee, Aaron Y. Lee; UWHVF: A Real-World, Open Source Dataset of Perimetry Tests From the Humphrey Field Analyzer at the University of Washington. Trans. Vis. Sci. Tech. 2022;11(1):2. doi: https://doi.org/10.1167/tvst.11.1.1."	https://paperswithcode.com/dataset/visual-fields	03/01/2022	UWHVF: A real-world, open source dataset of Humphrey Visual Fields (HVF) from the University of Washington					
5507	UI5k	"This dataset contains 54,987 UI screenshots and the metadata from 7,748 Android applications belonging to 25 application categories
Download link: https://www.dropbox.com/sh/kfkhevxykzwputb/AAAhL6ipmOg4zZn4jUL_myF0a?dl=0"	https://paperswithcode.com/dataset/ui5k	12/03/2021	Mobile App User Interface Dataset					
5508	NEWSKVQA	NEWSKVQA is a new dataset of 12K news videos spanning across 156 hours with 1M multiple-choice question-answer pairs covering 8263 unique entities.	https://paperswithcode.com/dataset/newskvqa	08/02/2022						
5509	Drone vs Bird	"For the Drone-vs-Bird Detection Challenge 2021, 77 different video sequences have been made available as training data. These video sequences originate from the previous installment of the challenge and were collected using MPEG4-coded static cameras by the SafeShore project, by the Fraunhofer IOSB research institute and by the ALADDIN2 project. On average, the video sequences consist of 1,384 frames, while each frame contains 1.12 annotated drones. The video sequences are recorded with both static cameras and moving cameras and the resolution varies between 720×576 and 3840×2160 pixels. In total, 8 different types of drones exist in the dataset , i.e. 3 with fixed wings and 5 rotary ones. For each video, a separate annotation file is provided, which contains the frame number and the bounding box (expressed as [topx topy width height]) for the frames in which drones enter the scenes.
Source: Drone-vs-Bird Detection Challenge at IEEE AVSS2021"	https://paperswithcode.com/dataset/drone-vs-bird		Drone vs Bird Detection Challenge					
5510	DELAUNAY	"DELAUNAY is a dataset of abstract paintings and non-figurative art objects labelled by the artists' names. This dataset provides a middle ground between natural images and artificial patterns and can thus be used in a variety of contexts, for example to investigate the sample efficiency of humans and artificial neural networks.
The dataset comprises 11,503 images from 53 categories, i.e. artists (mean number of images per artist: 217.04; standard deviation: 58.55), along with the associated URLs. These samples are split between a training set of 9202 images and a test set of 2301 images."	https://paperswithcode.com/dataset/delaunay	28/01/2022						
5511	Medical Question Pairs	"Medical Question Pairs (MQP) Dataset
This repository contains a dataset of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors. The dataset is described in detail in our paper.
Methodology
We present our doctors with a list of 1524 patient-asked questions randomly sampled from the publicly available crawl of HealthTap. Each question results in one similar and one different pair through the following instructions provided to the labelers:

Rewrite the original question in a different way while maintaining the same intent. Restructure the syntax as much as possible and change medical details that would not impact your response.
 e.g. ""I'm a 22-y-o female"" could become ""My 26 year old daughter""
Come up with a related but dissimilar question for which the answer to the original question would be WRONG OR IRRELEVANT. Use similar key words.

The first instruction generates a positive question pair (similar) and the second generates a negative question pair (different). With the above instructions, we intentionally frame the task such that positive question pairs can look very different by superficial metrics, and negative question pairs can conversely look very similar. This ensures that the task is not trivial.
Dataset format
The dataset is formatted as dr_id, question_1, question_2, label. We used 11 different doctors for this task so dr_id ranges from 1 to 11. The label is 1 if the question pair is similar and 0 otherwise.
Dataset statistics
The final dataset contains 4567 unique questions. The minimum, maximum, median and average number of tokens in these questions are 4, 81, 20 and 22.675 respectively showing there is reasonable variance in the length of the questions. The shortest question is Are fibroadenomas malignant?
An off-the-shelf medical entity recognizer finds around 1000 unique medical entities in the questions. Some of the top entity mentions were: physician, pregnancy, pain, lasting weeks, menstruation, emotional state, cancer, visual function, headache, bleeding, fever, sexual intercourse"	https://paperswithcode.com/dataset/medical-question-pairs	04/08/2020	Medical Question Pairs (MQP) Dataset					
5512	InfiniteRep	"InfiniteRep is a synthetic, open-source dataset for fitness and physical therapy (PT) applications. It includes 1k videos of diverse avatars performing multiple repetitions of common exercises. It includes significant variation in the environment, lighting conditions, avatar demographics, and movement trajectories. From cadence to kinematic trajectory, each rep is done slightly differently -- just like real humans. InfiniteRep videos are accompanied by a rich set of pixel-perfect labels and annotations, including frame-specific repetition counts.
The dataset features:  

100 videos per exercise, spanning 5 to 10 repetitions each (1,000 videos total) 
7 unique indoor scenes
Realistic environmental occlusion (+ corresponding labels)
Diverse lighting conditions 
Varied body shape, skin tones, and clothing 
Rich annotations for 2D and 3D supervision  

Exercises
The dataset currently includes the following exercises:  

Pushups  
Alternating Bicep Curls (with dumbbells)  
Delt Flys (with dumbbells)
Squats
Bird Dogs
Supermans
Bicycle Crunches
Leg Raises
Front Raises (with dumbbells)
Overhead Press (with dumbbells)

Annotations
The dataset includes the following annotations:  

Bounding boxes  
Segmentation masks  
Keypoints 
Joint angles (quaternions) 
Percent occlusion 
Avatar characteristics
Camera position 
and more 

Want depth labels? They are not included in the dataset but we can send them to you. Email us at info@toinfinity.ai. 
Download
Download the dataset: toinfinity.ai/infiniterep  
Github repo with additional documentation: https://github.com/toinfinityai/InfiniteRep
Need more data?
Infinity AI specializes in generating custom synthetic data. If you need more (or different data), drop us a line at info@toinfinity.ai (we read every email)."	https://paperswithcode.com/dataset/infiniterep	03/02/2022	InfiniteRep					
5513	Extended heartSeg	The dataset X of this work is an extension of the heartSeg dataset. Each sample x ∈ X is an RGB image capturing the heart region of Medaka (Oryzias latipes) hatchlings from a constant ventral view. Since the body of Medaka is see-through, noninvasive studies regarding the internal organs and the whole circulatory system are practicable. A Medaka’s heart contains three parts: the atrium, the ventricle, and the bulbus. The atrium receives deoxygenated blood from the circulatory system and delivers it to the ventricle, which forwards it into the bulbus. The bulbus is the heart’s exit chamber and provides the gill arches with a constant blood flow. The blood flow through these three chambers was captured in 63 short recordings (around 11 seconds with 24 frames per second each) in total, from which the single image samples x ∈ X are extracted. The dataset is split into training and test data following the heartSeg dataset with ntrain = 565 samples in the training set Xtrain and ntest = 165 samples in the test set Xtest. The RGB image samples have a 640 × 480 pixels resolution.	https://paperswithcode.com/dataset/extended-heartseg	08/02/2022						
5514	Real spreading processes in multilayer networks	"Presented data contains the record of five spreading campaigns that occurred in a virtual world platform. Users distributed avatars between each other during the campaigns. The processes varied in time and range and were either incentivized or not incentivized. Campaign data is accompanied by events. The data can be used to build a multilayer network to place the campaigns in a wider context. To the best of the authors knowledge, the study is the first publicly available dataset containing a complete real multilayer social network together, along with five complete spreading processes in it.
Full description available in Jankowski, J., Michalski, R., & Bródka, P. (2017). A multilayer network dataset of interaction and influence spreading in a virtual world. Scientific data, 4(1), 1-9. https://www.nature.com/articles/sdata2017144"	https://paperswithcode.com/dataset/real-spreading-processes-in-multilayer	11/10/2017	A multilayer network dataset of interaction and influence spreading in a virtual world					
5515	IndicGLUE	"We now introduce IndicGLUE, the Indic General
Language Understanding Evaluation Benchmark,
which is a collection of various NLP tasks as de-
scribed below. The goal is to provide an evaluation
benchmark for natural language understanding ca-
pabilities of NLP models on diverse tasks and mul-
tiple Indian languages."	https://paperswithcode.com/dataset/indicglue	08/11/2020	Indic General Language Understanding Evaluation Benchmark					
5516	Natural Sprites	This csv consists of (x-position, y-position, area) tuples of three views (left, middle, right) of downscaled binary masks with aspect ratio kept (64 x 128) from the 2019 YouTube-VIS challenge, which can be found at https://competitions.codalab.org/competitions/20127#participate-get-data. Extracting pairs from this csv results in 234,652 transitions in the given statistics. These statistics can be used to augment ground truth factor distributions with natural transitions, which we demonstrate with spriteworld. For details, we refer to our paper, which can be found at https://openreview.net/forum?id=EbIDjBynYJ8.	https://paperswithcode.com/dataset/natural-sprites	21/07/2020						
5517	KITTI-Masks	"This Dataset consists of 2120 sequences of binary masks of pedestrians. The sequence length varies between 2-710. For details, we refer to our paper. It is based on the original KITTI Segmentation challenge which can be found at https://www.vision.rwth-aachen.de/page/mots 
A detailed description can be found at: https://openreview.net/pdf?id=EbIDjBynYJ8
An example dataloader can be found at: 
https://github.com/bethgelab/slow_disentanglement/"	https://paperswithcode.com/dataset/kitti-masks	21/07/2020						
5518	3DIdent	"Novel benchmark which features aspects of natural scenes, e.g. a complex 3D object and different lighting conditions, while still providing access to the continuous ground-truth factors.
We use the Blender rendering engine to create visually complex 3D images. Each image in the dataset shows a colored 3D object which is located and rotated above a colored ground in a 3D space. Additionally, each scene contains a colored spotlight which is focused on the object and located on a half-circle around the scene. The observations are encoded with an RGB color space, and the spatial resolution is 224x224 pixels.
The images are rendered based on a 10-dimensional latent, where: (1) three dimensions describe the XYZ position, (2) three dimensions describe the rotation of the object in Euler angles, (3) two dimensions describe the color of the object and the ground of the scene, respectively, and (4) two dimensions describe the position and color of the spotlight. We use the HSV color space to describe the color of the object and the ground with only one latent each by having the latent factor control the hue value.
The training set and test set contain 250,000 and 25,000 observation-latent pairs, respectively, whereby the latents are uniformly sampled from the unit hyperrectangle."	https://paperswithcode.com/dataset/3dident	17/02/2021						
5519	Causal3DIdent	Update on 3DIdent, where we introduce six additional object classes (Hare, Dragon, Cow, Armadillo, Horse, and Head), and impose a causal graph over the latent variables. For further details, see Appendix B in the associated paper (https://arxiv.org/abs/2106.04619).	https://paperswithcode.com/dataset/causal3dident	08/06/2021						
5520	CENTER-TBI	"The CENTER-TBI database contains prospectively collected data of more than 4,500 patients with TBI in Europe. The Registry and Acute Care data has been collected during a 3 years’ period (2015-2017) in 65 centers in Europe. For all patients, outcome data has been collected up to 2 years after injury.
The CENTER-TBI investigators welcome all forms of collaboration and data sharing. Interested scientists may obtain access to the CENTER-TBI clinical, imaging, high resolution ICU and biomarker data for the purposes of scientific investigation, teaching or planning clinical research studies. Obtaining access to and using CENTER-TBI data requires adherence to the CENTER-TBI Data Use Agreement and harmonized procedures for the data access requests as outlined in the documents listed below.
The application process includes submission of an online application form. The application must include the investigator’s institutional affiliation and the proposed uses of the CENTER-TBI data. CENTER-TBI data may not be used for commercial products or redistributed in any way."	https://paperswithcode.com/dataset/center-tbi	10/02/2022	Collaborative European NeuroTrauma Effectiveness Research in TBI					
5521	PCFG SET	"The Probabilistic Context Free Grammar String Edit Task (PCFG SET) dataset is a dataset with sequence to sequence problems specifically designed to test different aspects of compositional generalisation. In particular, the dataset contains splits to test for systematicity, productivity, substitutivity, localism and overgeneralisation.
The input alphabet of PCFG SET contains three types of words: words for unary and binary functions that represent \emph{string edit operations} (e.g. $\texttt{append}, \texttt{copy}, \texttt{reverse})$, elements to form the string sequences that these functions can be applied to (e.g. $\texttt{A}, \texttt{B}, \texttt{A1}, \texttt{B1}$), and a separator to separate the arguments of a binary function ($\texttt{,}$). The input sequences that are formed with this alphabet are sequences describing how a series of such operations are to be applied to a string argument. For instance:

$\texttt{repeat A B C }$ 
$\texttt{echo remove_first D K , E F}$ 
$\texttt{append swap F G H , repeat I J}$ 

The input sequences are generated with a PCFG, whose production probabilities are learned with EM to match the depth and length distributions in a corpus with English sentences.
The output of a PCFG SET sequence, representing its meaning, is constructed by recursively applying the string edit operations specified in the sequence. For instance: 

$\texttt{repeat A B C }$  &  $\rightarrow$  &  $\texttt{A B C A B C}$ 
$\texttt{echo remove_first D K , E F}$  & $\rightarrow$ & $\texttt{E F F}$
$\texttt{append swap F G H , repeat I J}$  & $\rightarrow$ & $\texttt{H G F I J I J }$

The string alphabet used for the construction of the dataset has 520 distinct elements, the length of the string arguments to a functions is limited to 5.The dataset contains around 100 thousand examples in total.  A full description of the dataset can be found in Hupkes et al (2020)."	https://paperswithcode.com/dataset/pcfg-set	22/08/2019	Probabilistic Context Free Grammar String Edit Task					
5522	deepMTJ_IEEEtbme	This dataset comprises 1344 expert annotated images of muscle-tendon junctions recorded with 3 ultrasound imaging systems (Aixplorer V6, Esaote MyLab60, Telemed ArtUs), on 2 muscles (Lateral Gastrocnemius, Medial Gastrocnemius), and 2 movements (isometric maximum voluntary contractions, passive torque movements).	https://paperswithcode.com/dataset/deepmtj-ieeetbme	10/02/2022						
5523	EquiBind data	"The protein-ligand complexes of PDBBind v2020 preprocessed as described in the paper ""EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction"" with associated code at https://github.com/HannesStark/EquiBind
Contained are 19119 complexes of PDBBinds total 19433 protein-ligand complexes. Excluded are those for which the ligand files could not be loaded using RDKit.
Paper Abstract:
Predicting how a drug-like molecule binds to a specific protein target is a core problem in drug discovery. An extremely fast computational binding method would enable key applications such as fast virtual screening or drug engineering. Existing methods are computationally expensive as they rely on heavy candidate sampling coupled with scoring, ranking, and fine-tuning steps. We challenge this paradigm with EQUIBIND, an SE(3)-equivariant geometric deep learning model performing direct-shot prediction of both i) the receptor binding location (blind docking) and ii) the ligand’s bound pose and orientation. EquiBind achieves significant speed-ups and better quality compared to traditional and recent baselines. Further, we show extra improvements when coupling it with existing fine-tuning techniques at the cost of increased running time. Finally, we propose a novel and fast fine-tuning model that adjusts torsion angles of a ligand’s rotatable bonds based on closed-form global minima of the von Mises angular distance to a given input atomic point cloud, avoiding previous expensive differential evolution strategies for energy minimization."	https://paperswithcode.com/dataset/equibind-data	07/02/2022	EquiBind preprocessing of PDBBind v2020					
5524	SoundDescs	"We introduce a new audio dataset called SoundDescs that can be used for tasks such as text to audio retrieval, audio captioning etc. This dataset contains 32,979 pairs of audio files and text descriptions. There are 23 categories found in SoundDescs including but not limited to nature, clocks, fire etc.
SoundDescs can be downloaded from here and retrieval results for this dataset can be found in the associated paper Audio Retrieval with Natural Language Queries: A Benchmark Study."	https://paperswithcode.com/dataset/sounddescs	17/12/2021						
5525	C3D features for PHD2GIF	"The feature files are named with the youtube IDs.
https://drive.google.com/drive/folders/10-6hkQxMKMGwLXANxfPRE7xw5PKiMjLn?usp=sharing"	https://paperswithcode.com/dataset/c3d-features-for-phd2gif	04/09/2021						
5526	Rice Dataset Commeo and Osmancik	"ata Set Name: Rice Dataset (Commeo and Osmancik)
Abstract: A total of 3810 rice grain's images were taken for the two species (Cammeo and Osmancik), processed and feature inferences were made. 7 morphological features were obtained for each grain of rice.  
Source:
Ilkay CINAR
Graduate School of Natural and Applied Sciences, 
Selcuk University, Konya, TURKEY
ilkay_cinar@hotmail.com
Murat KOKLU
Faculty of Technology, 
Selcuk University, Konya, TURKEY.
mkoklu@selcuk.edu.tr
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/
Relevant Information: In order to classify the rice varieties (Cammeo and Osmancik) used, preliminary processing was applied to the pictures obtained with computer vision system and a total of 3810 rice grains were obtained. Furthermore, 7 morphological features have been inferred for each grain.  A data set has been created for the properties obtained.
Attribute Information:
1. Area: Returns the number of pixels within the boundaries of the rice grain.
2. Perimeter: Calculates the circumference by calculating the distance between pixels around the boundaries of the rice grain.
3. Major Axis Length: The longest line that can be drawn on the rice grain, i.e. the main axis distance, gives.
4. Minor Axis Length: The shortest line that can be drawn on the rice grain, i.e. the small axis distance, gives.
5. Eccentricity: It measures how round the ellipse, which has the same moments as the rice grain, is.
6. Convex Area: Returns the pixel count of the smallest convex shell of the region formed by the rice grain.
7. Extent: Returns the ratio of the region formed by the rice grain to the bounding box pixels
8. Class: Commeo and Osmancik.
Relevant Papers / Citation Requests / Acknowledgements:
Cinar, I. and Koklu, M. (2019). Classification of Rice Varieties Using Artificial Intelligence Methods. International Journal of Intelligent Systems and Applications in Engineering, vol.7, no.3 (Sep. 2019), pp.188-194. https://doi.org/10.18201/ijisae.2019355381."	https://paperswithcode.com/dataset/rice-dataset-commeo-and-osmancik	11/02/2022	Rice Dataset Commeo and Osmancik					
5527	Rice Image Dataset	"Citation Request: See the articles for more detailed information on the data.
Koklu, M., Cinar, I., & Taspinar, Y. S. (2021). Classification of rice varieties with deep learning methods. Computers and Electronics in Agriculture, 187, 106285. https://doi.org/10.1016/j.compag.2021.106285
Cinar, I., & Koklu, M. (2021). Determination of Effective and Specific Physical Features of Rice Varieties by Computer Vision In Exterior Quality Inspection. Selcuk Journal of Agriculture and Food Sciences, 35(3), 229-243. https://doi.org/10.15316/SJAFS.2021.252
Cinar, I., & Koklu, M. (2022). Identification of Rice Varieties Using Machine Learning Algorithms. Journal of Agricultural Sciences https://doi.org/10.15832/ankutbd.862482
Cinar, I., & Koklu, M. (2019). Classification of Rice Varieties Using Artificial Intelligence Methods. International Journal of Intelligent Systems and Applications in Engineering, 7(3), 188-194. https://doi.org/10.18201/ijisae.2019355381
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/"	https://paperswithcode.com/dataset/rice-image-dataset	11/02/2022	Rice Image Dataset					
5528	Grapevine Leaves Image Dataset	"KOKLU Murat (a), UNLERSEN M. Fahri (b), OZKAN Ilker Ali (a), ASLAN M. Fatih(c), SABANCI Kadir (c)
(a) Department of Computer Engineering, Selcuk University, Turkey, Konya, Turkey
(b) Department of Electrical and Electronics Engineering, Necmettin Erbakan University, Konya, Turkey
(c) Department of Electrical-Electronic Engineering, Karamanoglu Mehmetbey University, Karaman, Turkey
Citation Request :
Koklu, M., Unlersen, M. F., Ozkan, I. A., Aslan, M. F., & Sabanci, K. (2022). A CNN-SVM study based on selected deep features for grapevine leaves classification. Measurement, 188, 110425. Doi:https://doi.org/10.1016/j.measurement.2021.110425
Link: https://doi.org/10.1016/j.measurement.2021.110425
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/
Highlights
• Classification of five classes of grapevine leaves by MobileNetv2 CNN Model.
• Classification of features using SVMs with different kernel functions.
• Implementing a feature selection algorithm for high classification percentage.
• Classification with highest accuracy using CNN-SVM Cubic model.
Abstract: The main product of grapevines is grapes that are consumed fresh or processed. In addition, grapevine leaves are harvested once a year as a by-product. The species of grapevine leaves are important in terms of price and taste. In this study, deep learning-based classification is conducted by using images of grapevine leaves. For this purpose, images of 500 vine leaves belonging to 5 species were taken with a special self-illuminating system. Later, this number was increased to 2500 with data augmentation methods. The classification was conducted with a state-of-art CNN model fine-tuned MobileNetv2. As the second approach, features were extracted from pre-trained MobileNetv2′s Logits layer and classification was made using various SVM kernels. As the third approach, 1000 features extracted from MobileNetv2′s Logits layer were selected by the Chi-Squares method and reduced to 250. Then, classification was made with various SVM kernels using the selected features. The most successful method was obtained by extracting features from the Logits layer and reducing the feature with the Chi-Squares method. The most successful SVM kernel was Cubic. The classification success of the system has been determined as 97.60%. It was observed that feature selection increased the classification success although the number of features used in classification decreased.
Keywords: Deep learning, Transfer learning, SVM, Grapevine leaves, Leaf identification"	https://paperswithcode.com/dataset/grapevine-leaves-image-dataset	11/02/2022	Grapevine Leaves Image Dataset					
5529	Acoustic Extinguisher Fire Dataset	"Yavuz Selim TASPINAR, Murat KOKLU and Mustafa ALTIN
Citation Request :
1: KOKLU M., TASPINAR Y.S.,  (2021).  Determining the Extinguishing Status of Fuel Flames With Sound Wave by Machine Learning Methods.  IEEE Access, 9, pp.86207-86216, Doi: 10.1109/ACCESS.2021.3088612
Link: https://ieeexplore.ieee.org/document/9452168 (Open Access)
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9452168
2: TASPINAR Y.S., KOKLU M., ALTIN M., (2021).  Classification of Flame Extinction Based on Acoustic Oscillations using Artificial Intelligence Methods.  Case Studies in Thermal Engineering, 28, 101561, Doi: 10.1016/j.csite.2021.101561
Link: https://www.sciencedirect.com/science/article/pii/S2214157X21007243  (Open Access) https://www.sciencedirect.com/sdfe/reader/pii/S2214157X21007243/pdf
3: TASPINAR Y.S., KOKLU M., ALTIN M., (2022).  Acoustic-Driven Airflow Flame Extinguishing System Design and Analysis of Capabilities of Low Frequency in Different Fuels.  Fire Technology, Doi: 10.1007/s10694-021-01208-9
Link: https://link.springer.com/content/pdf/10.1007/s10694-021-01208-9.pdf""
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/
SHORT DESCRIPTION: The dataset was obtained as a result of the extinguishing tests of four different fuel flames with a sound wave extinguishing system. The sound wave fire-extinguishing system consists of 4 subwoofers with a total power of 4,000 Watt placed in the collimator cabinet. There are two amplifiers that enable the sound come to these subwoofers as boosted. Power supply that powers the system and filter circuit ensuring that the sound frequencies are properly transmitted to the system is located within the control unit. While computer is used as frequency source, anemometer was used to measure the airflow resulted from sound waves during the extinguishing phase of the flame, and a decibel meter to measure the sound intensity. An infrared thermometer was used to measure the temperature of the flame and the fuel can, and a camera is installed to detect the extinction time of the flame. A total of 17,442 tests were conducted with this experimental setup. The experiments are planned as follows:
1. Three different liquid fuels and LPG fuel were used to create the flame.
2. 5 different sizes of liquid fuel cans are used to achieve different size of flames.
3. Half and full gas adjustment is used for LPG fuel.
4. While carrying out each experiment, the fuel container, at 10 cm distance, was moved forward up to 190 cm by increasing the distance by 10 cm each time.
5. Along with the fuel container, anemometer and decibel meter were moved forward in the same dimensions.
6. Fire extinguishing experiments was conducted with 54 different frequency sound waves at each distance and flame size.
Throughout the flame extinguishing experiments, the data obtained from each measurement device was recorded and a dataset was created. The dataset includes the features of fuel container size representing the flame size, fuel type, frequency, decibel, distance, airflow and flame extinction. Accordingly, 6 input features and 1 output feature will be used in models. The explanation of a total of seven features for liquid fuels in the dataset is given in Table 1, and the explanation of 7 features for LPG fuel is given in Table 2.
The status property (flame extinction or non-extinction states) can be predicted by using six features in the dataset. Status and fuel features are categorical, while other features are numerical. 8,759 of the 17,442 test results are the non-extinguishing state of the flame. 8,683 of them are the extinction state of the flame. According to these numbers, it can be said that the class distribution of the dataset is almost equal.""                
KEYWORDS: Fire, Extinguishing System, Sound wave, Machine learning, Fire safety, Low frequency, Acoustic"	https://paperswithcode.com/dataset/acoustic-extinguisher-fire-dataset	11/02/2022	Acoustic Extinguisher Fire Dataset					
5530	Adult Data Set	"Data Set Information:
Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))
Prediction task is to determine whether a person makes over 50K a year."	https://paperswithcode.com/dataset/adult-data-set	01/05/1996						
5531	CVR	This data set includes votes for each of the U.S. House of Representatives Congressmen on the 16 key votes identified by the CQA. The CQA lists nine different types of votes: voted for, paired for, and announced for (these three simplified to yea), voted against, paired against, and announced against (these three simplified to nay), voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known (these three simplified to an unknown disposition).	https://paperswithcode.com/dataset/cvr	27/04/1987	Congressional Voting Records Data Set					
5532	MMLU	"MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model’s blind spots.
Image source: https://arxiv.org/pdf/2009.03300v3.pdf"	https://paperswithcode.com/dataset/mmlu	07/09/2020	Massive Multitask Language Understanding					
5533	Cyberbullying Classification	"As social media usage becomes increasingly prevalent in every age group, a vast majority of citizens rely on this essential medium for day-to-day communication. Social media’s ubiquity means that cyberbullying can effectively impact anyone at any time or anywhere, and the relative anonymity
of the internet makes such personal attacks more difficult to stop than traditional bullying.
On April 15th, 2020, UNICEF issued a warning in response to the increased risk of cyberbullying during the COVID-19 pandemic due to widespread school closures, increased screen time, and decreased face-to-face social interaction. The statistics of cyberbullying are outright alarming: 36.5% of middle and high school students have felt cyberbullied and 87% have observed cyberbullying, with effects ranging from decreased academic performance to depression to suicidal thoughts.
In light of all of this, this dataset contains more than 47000 tweets labelled according to the class of cyberbullying:
Age;
Ethnicity;
Gender;
Religion;
Other type of cyberbullying;
Not cyberbullying
The data has been balanced in order to contain ~8000 of each class.
Trigger Warning These tweets either describe a bullying event or are the offense themselves, therefore explore it to the point where you feel comfortable."	https://paperswithcode.com/dataset/cyberbullying-classification	17/01/2022						
5534	Research Artifact - GitHub Sponsors: Exploring a New Way to Contribute to Open Source	"This is a research artifact for the ICSE'22 paper ""GitHub Sponsors: Exploring a New Way to Contribute to Open Source"". The following three research questions were constructed to guide the study.

RQ1: Who participates in GitHub Sponsors?
RQ1.1: What are the characteristics of sponsored developers?
RQ1.2: What are the characteristics of sponsors?


RQ2: What characteristics make developers more likely to receive sponsorship?
RQ3: What are developers' perceived challenges and benefits related to sponsoring?
RQ3.1: Why are develpers looking for sponsors?
RQ3.2: What is the impact of (not) getting sponsorship?
RQ3.3: Why are developers sponsoring?



This artifact is a repository including lists of studied repositories on GitHub, a dataset for the network diagram for answering RQ1, the features for sponsored and non-sponsored developers for RQ2, the features for sponsors for RQ2, and survey material and coding of responses for RQ3."	https://paperswithcode.com/dataset/research-artifact-github-sponsors-exploring-a	09/02/2022						
5535	PETRAW	"PETRAW data set was composed of 150 sequences of peg transfer training sessions. The objective of the peg transfer session is to transfer 6 blocks from the left to the right and back. Each block must be extracted from a peg with one hand, transferred to the other hand, and inserted in a peg at the other side of the board.
All cases were acquired by a non-medical expert on the LTSI Laboratory from the University of Rennes. The data set was divided into a training data set composed of 90 cases and a test data set composed of 60 cases. A case was composed of kinematic data, a video, semantic segmentation of each frame, and workflow annotation."	https://paperswithcode.com/dataset/petraw	11/02/2022	PEg TRAnsfer Workflow recognition by different modalities					
5536	ProteinKG25	ProteinKG25 is a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and proteins entities. ProteinKG25 contains 4,990,097 triplets (4,879,951 Protein-GO triplets and 110,146 GO-GO triplets), 612,483 entities (565,254 proteins and 47,229 GO terms) and 31 relations.	https://paperswithcode.com/dataset/proteinkg25	23/01/2022						
5537	SHERLOCK	SHERLOCK is a corpus of 363K commonsense inferences grounded in 103K images. Annotators highlight localized clues (color bubbles) and draw plausible abductive inferences about them (speech bubbles). It can be used for testing machine capacity for abductive reasoning beyond literal image contents.	https://paperswithcode.com/dataset/sherlock	10/02/2022						
5538	Deep Soccer Captioning	Deep Soccer Captioning is a dataset consists of 22k caption-clip pairs and three visual features (images, optical flow, inpainting) for 500 hours of SoccerNet videos.	https://paperswithcode.com/dataset/deep-soccer-captioning	11/02/2022						
5539	ASC (TIL, 19 tasks)	A set of 19 ASC datasets (reviews of 19 products) producing a sequence of 19 tasks. Each dataset represents a task. The datasets are from 4 sources: (1) HL5Domains (Hu and Liu, 2004) with reviews of 5 products; (2) Liu3Domains (Liu et al., 2015) with reviews of 3 products; (3) Ding9Domains (Ding et al., 2008) with reviews of 9 products; and (4) SemEval14 with reviews of 2 products - SemEval 2014 Task 4 for laptop and restaurant. For (1), (2) and (3), we split about 10% of the original data as the validate data, another about 10% of the original data as the testing data. For (4), We use 150 examples from the training set for validation. To be consistent with existing research(Tang et al., 2016), examples belonging to the conflicting polarity (both positive and negative sentiments are expressed about an aspect term) are not used. Statistics and details of the 19 datasets are given on Page https://github.com/ZixuanKe/PyContinual.	https://paperswithcode.com/dataset/asc-til-19-tasks	06/12/2021	Task Incremental Aspect Sentiment Classification					
5540	ArgSciChat	ArgSciChat is an argumentative dialogue dataset. It consists of 498 messages collected from 41 dialogues on 20 scientific papers. It can be used to evaluate conversational agents and further encourage research on argumentative scientific agents.	https://paperswithcode.com/dataset/argscichat	14/02/2022						
5541	Dynamic OLAT Dataset	"To provide ground truth supervision for video consistency modeling, we build up a high-quality dynamic OLAT dataset.
Our capture system consists of a light stage setup with 114 LED light sources and Phantom Flex4K-GS camera (global shutter, stationary 4K ultra-high-speed camera at 1000 fps), resulting in dynamic OLAT imageset recording at 25 fps using the overlapping method.
Our dynamic OLAT dataset provides sufficient semantic, temporal and lighting consistency supervision to train our neural video portrait relighting scheme, which can generalize to in-the-wild scenarios."	https://paperswithcode.com/dataset/dynamic-olat-dataset	01/04/2021	ShanghaiTech MARS Dynamic OLAT Dataset					
5542	USR-TopicalChat	This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric.	https://paperswithcode.com/dataset/usr-topicalchat	01/05/2020						
5543	USR-PersonaChat	This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric.	https://paperswithcode.com/dataset/usr-personachat	01/05/2020						
5544	Memotion Analysis	A multimodal dataset for sentiment analysis on internet memes.	https://paperswithcode.com/dataset/memotion-analysis	09/08/2020	SemEval-2020 Task 8: Memotion Analysis -- The Visuo-Lingual Metaphor!					
5545	HelloWorld	HelloWorld is a dataset of kinesthetic demonstrations collected using a Franka Emika Panda robot. During the data collection, the robot was made to write the lower case letters $h, e, l, o, w, r, d$ one at a time on a horizontal surface and the $x$ and $y$ coordinates of the end-effector were recorded. Multiple demonstrations were collected for each letter. These demonstrations can be used for kinesthetic teaching. See further details here.	https://paperswithcode.com/dataset/helloworld	14/02/2022	HelloWorld					
5546	CIP	The CIP dataset is composed of 2 subsets, containing low-cost (MPU9250) and high-end (MTwAwinda) Magnetic, Angular Rate, and Gravity (MARG) sensor data respectively. It provides data for the analysis of the complete inertial pose pipeline, from raw measurements, to sensor-to-segment calibration, multi-sensor fusion, skeleton kinematics, to the complete human pose. Multiple trials were collected with 21 and 10 subjects respectively, performing 6 types of movements (ranging from calibration, to daily-activities, range-of-motion and random). It presents a high degree of variability and complex dynamics while containing common sources of error found on real conditions. This amounts to 3.5M samples, synchronized with a ground-truth inertial motion capture system (Xsens) at 60hz. This dataset may contribute to assess, benchmark and develop novel algorithms for each of the pipelines' processing steps, with applications in classic or data-driven inertial pose estimation algorithms, human movement understanding and forecasting and ergonomic assessment in industrial or rehabilitation settings.	https://paperswithcode.com/dataset/cip	12/02/2022	Complete Inertial Pose					
5547	wildFireClimateChangeTweets	"Here I provided the datasets I used for this analysis. It includes the tweets I streamed using the Tweepy package on Python during the peach of the wildfire season in late summer/early fall of 2020.
The files include:
1- Public: 
39 items of CSV files for the dates (day by day) starting 09/06/2020 to 09/23/2020 
  1 file combined for all the dates (+185k of tweets with the desired keywords) 
2- Government
  Locals: 
  CA: Counties of Napa, Mendocino, Santa Clara, Sonoma, and Fresno 
  OR: City of Salem, and Counties of Lane, Clackamas, Jackson, and Multnomah 
  CO: County of Larimer and Boulder Cities of Boulder, Grand Junction, Glenwood Springs, and Cortex
  15 accounts in total 
 State-level: 
  Governors of three states, California, Colorado, and Oregon 
  Congress representatives: 5 for each state 
  18 accounts in total 
Federal: 6 senators, California, Colorado, and Oregon
Information Provided for each tweet:
 Date/Time 
 UserID 
 UserName 
 Tweet's text 
 Number of retweets 
 Number of likes
Provide:"	https://paperswithcode.com/dataset/wildfireclimatechangetweets	12/02/2022						
5548	DSC (10 tasks)	"A set of 10 DSC datasets (reviews of 10 products) to produce sequences of tasks. The products are Sports, Toys, Tools, Video, Pet, Musical, Movies, Garden, Offices, and Kindle. 2500 positive and 2500 negative training reviews per task . The validation reviews are with 250 positive and 250 negative and the test reviews are with 250
positive and 250 negative reviews. The detailed statistic on page https://github.com/ZixuanKe/PyContinual"	https://paperswithcode.com/dataset/dsc-10-tasks	18/12/2021	Task Incremental Document Sentiment Classification					
5549	Weibo-Douban	"This dataset is used for user identity linkage across two online social networks in Chinese. It contains two popular Chinese social platforms: Sina Weibo\footnote{https://weibo.com} and Douban\footnote{https://www.douban.com}. 
Details:
* 9,714 users and 117,218 relations in Weibo; 9,526 users and 120,245 relations in Douban; 1,397 pair of matched users.
* Approximate power-law degree distribution and high aggregation coefficient.
* Multiple text attributes available, including username, geographical location and recent (text) posts of the users.
* Construction time: April 2020."	https://paperswithcode.com/dataset/weibo-douban	02/12/2020	WD					
5550	Ransomware PCAP repository	"This is a repository of PCAP files obtained by executing ransomware binaries and capturing the network traffic created when encrypting a set of files shared from an SMB server. There are 94 samples from 32 different ransomware families downloaded from malware-traffic-analysis and hybrid-analysis. There is a link to an info page for each sample, offering some information about the sample and about the scenario where it ran ('More info' column in the table).
You can download 10% of the packets from each traffic trace for free. If you find it useful and you want to download the whole samples, we ask for your e-mail and institution name, in order to keep a record of hoy many people are interested in these files. This helps us to keep this repository up and include more samples (as it proves that it is interesting for the community). We do not send you any kind of spam. We will only send you a link to download the full pcap files. In order to refer to this repository please include the link in your paper, cite the repository shared in IEEE dataPort (here) and/or cite this paper in which the repository is explained in more detail
We also offer a text file containing a description of all the input/output operations that appear in the SMB traffic. We had to create our own software in order to extract this information from large pcap files."	https://paperswithcode.com/dataset/ransomware-pcap-repository	15/02/2022						
5551	20Newsgroup (10 tasks)	"This dataset has 20 classes and each class has about 1000 documents. The data split for train/validation/test is 1600/200/200. We created 10 tasks, 2
classes per task.  Since this is topic-based text classification data, the classes are very different and have little shared knowledge. As mentioned above, this application (and dataset) is mainly used to show a CL model's ability to overcome forgetting. Detailed statistics please on page https://github.com/ZixuanKe/PyContinual"	https://paperswithcode.com/dataset/20newsgroup-10-tasks	05/12/2021						
5552	NMED-T	"Losorelli, Steven, Nguyen, Duc T., Dmochowski, Jacek P., and Kaneshiro, Blair
This dataset contains cortical (EEG) and behavioral data collected during natural music listening. Dense-array EEG was recorded from 20 adult participants who each heard a set of 10 full-length songs with electronically produced beats at various tempos. In a separate subsequent listen, each participant tapped to the beat of a 35-second excerpt from each song. Participants also delivered ratings of familiarity and enjoyment for each full-length song during the EEG recording. Finally, the dataset includes basic demographic information about the participants, as well as Matlab scripts to perform the illustrated analyses presented in the paper introducing the dataset (Losorelli et al., 2017). Cleaned and aggregated data are published in Matlab format; raw EEG is published in Matlab format, while raw tapping data are published in .txt format. Stimulus audio is not published, but metadata links are provided."	https://paperswithcode.com/dataset/nmed-t		Naturalistic Music EEG Dataset - Tempo					
5553	F-CelebA (10 tasks)	"F-CelebA - This dataset is adapted from federated learning. Federated learning
is an emerging machine learning paradigm with an emphasis on data privacy. The idea is to train
through model aggregation rather than conventional data aggregation and keep local data staying
on the local device. This dataset naturally consists of similar tasks and each of the 10 tasks contains images of a celebrity labeled by whether he/she is smiling or not. More detailed please check page https://github.com/ZixuanKe/CAT"	https://paperswithcode.com/dataset/f-celeba-10-tasks	18/12/2021	Federated-CelebA (10 tasks)					
5554	ADIMA	ADIMA is a novel, linguistically diverse, ethically sourced, expert annotated and well-balanced multilingual profanity detection audio dataset comprising of 11,775 audio samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique users.	https://paperswithcode.com/dataset/adima	16/02/2022						
5555	VizWiz-VQA-Grounding	"The VizWiz-VQA-Grounding dataset is a dataset that visually grounds answers to visual questions asked by people with visual impairments.
Training Set:

6,494 examples

Validation Set:

1,131 examples

Test Set:

2,373 examples"	https://paperswithcode.com/dataset/vizwiz-vqa-grounding	04/02/2022						
5556	Wukong	Wukong is a large-scale Chinese cross-modal dataset for benchmarking different multi-modal pre-training methods to facilitate the Vision-Language Pre-training (VLP). This dataset contains 100 million Chinese image-text pairs from the web. This base query list is taken from and is filtered according to the frequency of Chinese words and phrases.	https://paperswithcode.com/dataset/wukong	14/02/2022						
5557	MeLa BitChute	"MeLa BitChute is a near-complete dataset of over 3M videos from 61K channels over 2.5 years (June 2019 to December 2021) from the social video hosting platform BitChute, a commonly used alternative to YouTube. Additionally, the dataset includes a variety of video-level metadata, including comments, channel descriptions, and views for each video.
The dataset contains data from 3,036,190 videos, 61,229 channels, and 11,434,571 comments between June 28th, 2019 and December 31st, 2021. This dataset provides timestamped activities and estimates on views for the majority of channels and videos on the platform, allowing researchers to align BitChute videos with behavior on other platforms. Therefore, this dataset can facilitate both studies of BitChute in isolation and studies of BitChute’s role in the larger ecosystem."	https://paperswithcode.com/dataset/mela-bitchute	10/02/2022						
5558	UAV_udc	https://github.com/zzr-idam/Under-Display-Camera-UAV	https://paperswithcode.com/dataset/uav-udc	13/02/2022						
5559	LAW	"The Laboratory for Web Algorithmics (LAW) was established in 2002 at the Dipartimento di Scienze dell'Informazione (now merged in the Computer Science Department) of the Università degli studi di Milano.
The LAW is part of the NADINE FET EU project.
Research at LAW concerns all algorithmic aspects of the study of the web and of social networks."	https://paperswithcode.com/dataset/law		The Laboratory for Web Algorithmics					
5560	Taillard Instances	"Taillar's permutation flow shop, the job shop, and the open shop scheduling problems instances:
We restrict ourselves to basic problems: the processing times are fixed, there are neither set-up times nor due dates nor release dates, etc. Then, the objective is the minimization of the makespan."	https://paperswithcode.com/dataset/taillard-instances	21/12/1999	Taillard Instances for Job Shop Scheduling					
5561	FICS PCB Image Collection (FPIC)	"Optical images of printed circuit boards as well as detailed annotations of any text, logos, and surface-mount devices (SMDs). There are several hundred samples spanning a wide variety of manufacturing locations, sizes, node technology, applications, and more.

pcb_image: Optical images of each PCB surface and rear, tagged with a unique identifier. 
color_checker: Pallette to account for environmental illumination factors as well as a scale reference for the photo resolution. Each pcb image indicates which color checker it is associated with.
ocr_annotation: Optical Character Recognition annotations. This includes polygon boundaries around all relevant text on a PCB image. Whether the piece of text is on the board or a device, whether it is a logo or not, orientation, and more are noted within the columns of the csv.
smd_annotation: Surface-mount Device (SMD) annotations. This includes polygon boundaries around all relevant SMD devices such as resistors, capacitors, inductors, transistors, diodes, LEDs, and more. Along with each component, its associated silkscreen designator ('L', 'R', 'C', 'U', etc.) is recorded.
vtp_annotation: Vias, traces, and pins (VTP) annotations. These are regions of connectivity between SMDs on a PCB. Few annotations currently exist, this is considered in 'beta' mode currently. 

metadata: Holds two files corresponding to information about image files. 

pcb.csv holds information about the physical PCB samples such as their color, online item description, and any notes.
color_checker.csv indicates the pixels per millimeter (ppmm) of any image associated with that color checker, whether an X-Rite ColorChecker Passport or Nano was used, what camera performed the acquisition, and any relevant notes. 



Each annotation file is designed to be compatible with the S3A application (https://gitlab.com/ficsresearch/s3a or https://pypi.org/project/s3a/), a Python tool for visualizing polygon annotations on an image."	https://paperswithcode.com/dataset/fics-pcb-image-collection-fpic	17/02/2022						
5562	HuSHeM	At the Isfahan Fertility and Infertility Center, semen samples were collected from fifteen patients. The sperm samples were fixed and stained using the Diff-Quick method. Using an Olympus CX21 microscope with a  ×100 objective lens and a  ×10 eyepiece and a Sony color camera (Model No SSC-DC58AP), 725 images were taken. The resolution of each image was 576×720 pixels. From these images, the sperm heads were cropped and classified into five classes by three specialists. The classes are Normal, Pyriform, Tapered, Amorphous, and Others. After the classification, only the samples which there was a collective consensus about their class were kept in the dataset. Four classes of Normal, Pyriform, Tapered, and Amorphous are included in this dataset.  The resulting dataset of sperm heads denoted as Human Sperm Head Morphology dataset (HuSHeM) consists of four folders, each corresponding to a specific set of sperm shapes. The folder names reflect the shape of the contained images. There are  54 Normal, 53 Tapered, 57 Pyriform, and 52 Amorphous sperm heads. The images of sperm heads are in the RGB format with the size of 131×131 pixels.	https://paperswithcode.com/dataset/hushem	05/01/2018	Human Sperm Head Morphology Dataset					
5563	SCIAN	Dataset of sperm head images with expert-classification labels. The dataset contains 1854 sperm head images obtained from six semen smears and classified by three Chilean referent domain experts according to World Health Organization (WHO) criteria, in one of the following classes: normal, tapered, pyriform, small and amorphous. This gold-standard is aimed for use in evaluating and comparing not only known techniques, but also future improvements to present approaches for classification of human sperm heads for semen analysis.	https://paperswithcode.com/dataset/scian		SCIAN Gold-standard for Morphological Sperm Analysis					
5564	TransCG	"TransCG is the first large-scale real-world dataset for transparent object depth completion and grasping, which contains 57,715 RGB-D images of 51 transparent objects and many opaque objects captured from different perspectives (~240 viewpoints) of 130 scenes under real-world settings. The samples are captured by two different types of cameras (Realsense D435 & L515).
The following data is provided:

The 3D model of the transparent object;
The 6dpose of the transparent object in each viewpoint of each scene;
The raw RGB-D image, and the ground-truth refined depth image;
The mask of the transparent objects;
The ground-truth surface normals of every sample."	https://paperswithcode.com/dataset/transcg	17/02/2022	TransCG					
5565	Synth-Colon	"Synthetic dataset for polyp segmentation. It
is the first dataset generated using zero annotations from medical professionals.
The dataset is composed of 20 000 images with a resolution of 500×500. SynthColon additionally includes realistic colon images generated with a CycleGAN
and the Kvasir training set images. Synth-Colon can also be used for the colon
depth estimation task  because it provides depth and 3D information for each
image. . In summary, Synth-Colon
includes:
– Synthetic images of the colon and one polyp.
– Masks indicating the location of the polyp.
– Realistic images of the colon and polyps. Generated using CycleGAN and
the Kvasir dataset.
– Depth images of the colon and polyp.
– 3D meshes of the colon and polyp in OBJ format."	https://paperswithcode.com/dataset/synth-colon	17/02/2022						
5566	Munich Sentinel2 Crop Segmentation	"Contains squared blocks of 48×48 pixels including 13 Sentinel-2 bands. 
Each 480-m block was mined from a large geographical area of interest (102 km × 42 km) located north of Munich, Germany."	https://paperswithcode.com/dataset/munich-sentinel2-crop-segmentation	06/02/2018						
5567	A collection of LFR benchmark graphs	"This dataset is a collection of undirected and unweighted LFR benchmark graphs as proposed by Lancichinetti et al. 1. We generated the graphs using the code provided by Santo Fortunato on his personal website 2, embedded in our evaluation framework 3, with two different parameter sets. Let N denote the number of vertices in the network, then
  Maximum community size: 0.2N (Set A); 0.1N (Set B)
  Minimum community size: 0.05N (Set A); 10 (Set B)
  Maximum node degree: 0.19N (Set A); 0.19N (Set B)
  Community size distribution exponent: 1.0 (Set A); 1.0 (Set B)
  Degree distribution exponent: 2.0 (Set A); 2.0 (Set B).

All other parameters assume default values. We provide graphs with different combinations of average degree, network size and mixing parameter for the given parameter sets:
Set A: For average degrees in {15, 25, 50} we provide network sizes in {300, 600, 1200}, each with 20 different mixing parameters linearly spaced in [0.2, 0.8]. For each configuration we provide 100 benchmark graphs.
Set A: For average degrees in {15, 25, 50} we provide mixing parameters in {0.35, 0.45, 0.55}, each with network sizes in {300, 450, 600, 900, 1200, 1800, 2400, 3600, 4800, 6200, 9600, 19200}. For each configuration we provide 50 benchmark graphs.
Set B: For average degrees in {20} we provide network sizes in {300, 600, 1200, 2400}, each with 20 different mixing parameters linearly spaced in [0.2, 0.8]. For each configuration we provide 100 benchmark graphs.

Benchmark graphs are given in edge list format. Further, for each benchmark graph we provide ground truth communities as membership list and as structured datatype (.json), its generating random seeds and basic network statistics.
1 Lancichinetti A, Fortunato S, Radicchi F (2008) Benchmark graphs for testing community detection algorithms. Physical Review E 78(4):046110,https://doi.org/10.1103/PhysRevE.78.046110
2 https://www.santofortunato.net/resources, Accessed: 19 Jan 2021
3 https://github.com/synwalk/synwalk-analysis, Accessed: 19 Jan 2021"	https://paperswithcode.com/dataset/a-collection-of-lfr-benchmark-graphs	21/01/2021						
5568	Volunteer task execution events in Galaxy Zoo and The Milky Way citizen science projects	"Context of the data sets
The Zooniverse platform (www.zooniverse.org) has successfully built a large community of volunteers contributing to citizen science projects. Galaxy Zoo and the Milky Way Project were hosted there.
The original Galaxy Zoo project was launched in July 2007, but has since been redesigned and relaunched three times, building each time on the success of its predecessor. In 2010, the Zooniverse launched the third iteration of Galaxy Zoo, called Galaxy Zoo: Hubble, but for simplicity, we use the term Galaxy Zoo throughout this text to refer to this project. Each volunteer classifying on Galaxy Zoo is presented with a galaxy from the Sloan Digital Sky Survey (SDSS) or the Hubble Space Telescope as well as a decision tree of questions with answers represented by a fairly simple icon. The task is straightforward, and no specialist knowledge is required to execute it. 
Tasks in the Milky Way Project exhibit a larger cognitive load than those in Galaxy Zoo. Volunteers are asked to draw ellipses onto the image to mark the locations of bubbles. A short, online tutorial shows how to use the tool, along with examples of prominent bubbles. As a secondary task, users can also mark rectangular areas of interest, which can be labeled as small bubbles, green knots, dark nebulae, star clusters, galaxies, fuzzy red objects, or “other.” Users can add as many annotations as they wish before submitting the image, at which point they’re given another image for annotation.
Description of the raw data
In this repository, each file is a project, each line on a file is one classification record. The lines contain three pieces of information separated by commas ("",""). The first information is the classification id, which uniquely identifies the classification in the data set. The second information is the volunteer id, which uniquely identifies, in the data set, the volunteer who carried out the classification. The third information is the date and time in which the classification was carried out.
The data set from the Galaxy Zoo project consists of records of 9,667,586 tasks executed by 86,413 volunteers over 840 days, starting on April 17th, 2010. The data set from the Milky Way Project consists of records from 643,408 tasks executed by 23,889 volunteers over 670 days, starting on December 3rd, 2010. 
These datasets were provided by Arfon Smith and Robert Simpson, from the Zooniverse platform, in October, 2012. To understand how volunteers make their contributions in these citizen science projects, Ponciano, Brasileiro, Simpson and Smith (2014) analyzed both data sets considering a volunteer engagement perspective. 
Metrics derived from the data set
Ponciano, Brasileiro, Simpson and Smith (2014) proposed and computed the following metrics on the data set: Frequency, or the number of days in which the volunteer was actively executing tasks in the project. Daily productivity, or the average number of tasks the volunteer executed per day in which he or she was active. Typical session duration, or the short, continuous period of time the volunteer devoted to execute tasks on the project. A session begins when a volunteer starts a task execution, but it may end for a variety of reasons, such as the volunteer achieving the time he or she wanted to devote to the project, or that person getting tired or bored because of something related to the task performed. The typical session duration is the median of the duration of all the volunteer’s contribution sessions. Devoted time, or the total time the volunteer has spent executing tasks on the project. It's calculated as the sum of the duration of all the volunteer’s contribution sessions.
They generate statistical probability distributions to the volunteer engagement characteristics that fit the parameters of  Zipf and Log Normal. The results reported in the study reveal many characteristics of the distributions of volunteer participation in the projects. For example, they show that the majority of the volunteers perform tasks in just one day and do not come back, but those who come back contribute the larger proportion of tasks executed. For more information about methods and results of the first study that analysed the data set, please, see Ponciano, Brasileiro, Simpson and Smith (2014).
In a subsequent study, Ponciano and Brasileiro (2014) deepened the study by considering a new framework to study volunteer engagement. In this new study, new metrics and a clustering approach were used to identify groups of volunteers who exhibit a similar engagement profile. A new set of metrics is designed to measure the engagement of participants that exhibit an ongoing contribution and have contributed in at least two different days, so they focus on participants that are more likely to fit into the voluntarism definition. In this perspective, they formalyzed the following metrics: Activity ratio, Daily devoted time, Relative activity duration, and Variation in periodicity. Their results show that the volunteers in such projects can be grouped into five distinct engagement profiles that we label as follows: hardworking, spasmodic, persistent, lasting, and moderate. For more information about the method and results on the engagement profiles see Ponciano and Brasileiro (2014).
Reporting the use of the data set
The data sets stored in this repository are freely available to be used at the Creative Commons Attribution licence. In case you use the data set, please, include in your work a citation of the previous studies Ponciano, Brasileiro, Simpson and Smith (2014) and Ponciano and Brasileiro (2014) that were the first to characterize the data from Galaxy Zoo and the Milky Way Project in a volunteer engagement perspective. After that, you may also inform the Zooniverse platform that you have used data from Galaxy Zoo and the Milky Way Project. To do so, you can use this form indicated by the platform at the publication page.
References
Lesandro Ponciano, Francisco Brasileiro, Robert Simpson and Arfon Smith. ""Volunteers' Engagement in Human Computation Astronomy Projects"". Computing in Science and Engineering  vol. 16, no. 6, pp. 52-59 (2014) DOI: 10.1109/MCSE.2014.4
Lesandro Ponciano and Francisco Brasileiro. ""Finding Volunteers' Engagement Profiles in Human Computation for Citizen Science Projects"". Human Computation  vol. 1, no. 2, pp. 245-264  (2014). DOI: 10.15346/hc.v1i2.12"	https://paperswithcode.com/dataset/data-sets-of-volunteer-task-execution-events	09/01/2015						
5569	Motor Imagery dataset	"From dataset repository for ""2020 International BCI Competition"":
https://osf.io/pq7vb/?view_only=08e7108d89fd42bab2adbd6b98fb683d"	https://paperswithcode.com/dataset/motor-imagery-dataset							
5570	Error Grids for multi-fidelity benchmark functions in mf2	"Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset

Collection of Error Grid data files. Intended purpose is to allow confirmation of analysis and to perform future analysis on other error measurement methods that are included."	https://paperswithcode.com/dataset/error-grids-for-multi-fidelity-benchmark	19/02/2021						
5571	KITTI'15 MSplus	"Extension of the official KITTI'15 dataset. independently moving instance segmentation ground truth to cover all moving objects, not just a selection of cars and vans.

Instance Motion Segmentation of all moving objects
Binary Motion Segmentation (background/foreground)
Validation Masks

Dataset contains:

Instance Motion Segmenation for the training split of the KITTI'15 dataset"	https://paperswithcode.com/dataset/kitti-15-msplus	22/11/2021	KITTI'15 Instance Motion Segmentation - extension					
5572	DLR-ACD	"The DLR-ACD dataset is a collection of aerial images for crowd counting and density estimation, as well as for person localization at mass events. It contains 33 large aerial images acquired through 16 different flight campaigns at various mass events and over urban scenes involving crowds, such as sport events, city centers, open-air fairs and festivals.
The images were captured with standard DSLR cameras installed on a helicopter, and their spatial resolution (or ground sampling distance – GSD) ranges from 4.5 to 15 cm/pixel. The dataset was labeled manually with point-annotations on individual people and contains 226,291 person annotations in total, ranging from 285 to 24,368 annotations per image."	https://paperswithcode.com/dataset/dlr-acd	27/09/2019						
5573	CBCT Walnut	"The scans are performed using a custom-built, highly flexible X-ray CT scanner, the FleX-ray scanner, developed by XRE nvand located in the FleX-ray Lab at the Centrum Wiskunde & Informatica (CWI) in Amsterdam, Netherlands. The general purpose of the FleX-ray Lab is to conduct proof of concept experiments directly accessible to researchers in the field of mathematics and computer science. The scanner consists of a cone-beam microfocus X-ray point source that projects polychromatic X-rays onto a 1536-by-1944 pixels, 14-bit flat panel detector (Dexella 1512NDT) and a rotation stage in-between, upon which a sample is mounted. All three components are mounted on translation stages which allow them to move independently from one another.
Please refer to the paper for all further technical details.
The complete data set can be found via the following links: 1-8 https://doi.org/10.5281/zenodo.2686725 , 9-16 https://doi.org/10.5281/zenodo.2686970, 17-24 https://doi.org/10.5281/zenodo.2687386, 25-32 https://doi.org/10.5281/zenodo.2687634, 33-37 https://doi.org/10.5281/zenodo.2687896, 38-42 https://doi.org/10.5281/zenodo.2688111
The corresponding Python scripts for loading, pre-processing and reconstructing the projection data in the way described in the paper can be found on github https://github.com/cicwi/WalnutReconstructionCodes"	https://paperswithcode.com/dataset/cbct-walnut	09/05/2019	Cone-Beam X-Ray CT Data Collection Designed for Machine Learning					
5574	NON-LINEAR PHASE NOISE MITIGATION OVER SYSTEMS USING CONSTELLATION SHAPING: EXPERIMENTAL DATASET	"This dataset contains the full set of experimental waveforms that were used to produce the article ""Non-Linear Phase Noise Mitigation over Systems using Constellation Shaping"", published in the Journal of Lightwave Technology with DOI: 10.1109/JLT.2019.2917308."	https://paperswithcode.com/dataset/non-linear-phase-noise-mitigation-over		Dario Pilori					
5575	V2X-SIM	"V2X-Sim, short for vehicle-to-everything simulation, is the a synthetic collaborative perception dataset in autonomous driving developed by AI4CE Lab at NYU and MediaBrain Group at SJTU to facilitate collaborative perception between multiple vehicles and roadside infrastructure. Data is collected from both roadside and vehicles when they are presented near the same intersection. With information from both the roadside infrastructure and vehicles, the dataset aims to encourage research on collaborative perception tasks.
Although not collected from the real world, highly realistic traffic simulation software is used to ensure the representativeness of the dataset compared to real-world driving scenarios. To be more exact, the traffic flow of the recording files is managed by CARLA-SUMO co-simulation, and three town maps from CARLA are currently used to increase the diversity of the dataset.
Here is a tutorial showing how to load the dataset: https://ai4ce.github.io/V2X-Sim/tutorial.html"	https://paperswithcode.com/dataset/v2x-sim	17/02/2022						
5576	Typography-MNIST	Typography-MNIST is a dataset comprising of 565,292 MNIST-style grayscale images representing 1,812 unique glyphs in varied styles of 1,355 Google-fonts. The glyph-list contains common characters from over 150 of the modern and historical language scripts with symbol sets, and each font-style represents varying subsets of the total unique glyphs. The dataset has been developed as part of the Cognitive Type project which aims to develop eye-tracking tools for real-time mapping of type to cognition and to create computational tools that allow for the easy design of typefaces with cognitive properties such as readability.	https://paperswithcode.com/dataset/typography-mnist	12/02/2022						
5577	ASOS Digital Experiments Dataset	"A novel dataset that can support the end-to-end design and running of Online Controlled Experiments (OCE) with adaptive stopping.
See OSF page for the schema and datasheet."	https://paperswithcode.com/dataset/asos-digital-experiments-dataset	29/10/2021						
5578	NYT11-HRL	"Preprocessed version of NYT11.
Each relational triple is formatted as follows:
rtext : relation type
em1 : source entity mention
em2 : target entity mention
tags : the proposed entity annotation scheme for the sentence
0 : $O$ non-entity
1 : $S_I$ inside of a source entity
2 : $T_I$ inside of a target entity
3 : $O_I$ inside of not-concerned entity
4 : $S_B$ begin of a source entity
5 : $T_B$ begin of a target entity
6 : $O_B$ begin of not-concerned entity"	https://paperswithcode.com/dataset/nyt11-hrl	09/11/2018						
5579	VaccineLies	A Natural Language Resource for Learning to Recognize Misinformation about the COVID-19 and HPV Vaccines.	https://paperswithcode.com/dataset/vaccinelies	18/02/2022						
5580	CoVaxLies v2	CoVaxLies v2 includes 47 Misinformation Targets (MisTs) found on Twitter about the COVID-19 vaccines. Language experts annotated tweets as Relevant or Not Relevant, and then further annotated Relevant tweets with Stance towards each MisT. This collection is a first step in providing large-scale resources for misinformation detection and misinformation stance identification.	https://paperswithcode.com/dataset/covaxlies-v2	18/02/2022						
5581	EUCA dataset	"EUCA dataset description
Associated Paper: 
EUCA: the End-User-Centered Explainable AI Framework
Authors:
Weina Jin, Jianyu Fan, Diane Gromala, Philippe Pasquier, Ghassan Hamarneh
Introduction:
EUCA dataset is for modelling personalized or interactive explainable AI. It contains 309 data points of 32 end-users' preferences on 12 forms of explanation (including feature-, example-, and rule-based explanations). The data were collected from a user study on 32 layperson participants in the Greater Vancouver city area in 2019-2020. In the user study, the participants (P01-P32) were presented with AI-assisted critical tasks on house price prediction, health status prediction, purchasing a self-driving car, and studying for a biological exam 1. Within each task and for its given explanation goal 2, the participants selected and rank the explanatory forms 3 that they saw the most suitable. 
1 EUCA_EndUserXAI_ExplanatoryFormRanking.csv
Column description:

Index - Participants' number
Case - task-explanation goal combination
accept to use AI? trust it? - Participants response to whether they will use AI given the task and explanation goal
require explanation? - Participants response to the question whether they request an explanation for the AI
1st, 2nd, 3rd, ... - Explanatory form card selection and ranking 
     cards fulfill requirement? - After the card selection, participants were asked whether the selected card combination fulfill their explainability requirement.

2  EUCA_EndUserXAI_demography.csv
It contains the participants demographics, including their age, gender, educational background, and their knowledge and attitudes toward AI.
EUCA dataset zip file for download
More Context for EUCA Dataset
1 Critical tasks
There are four tasks. Task label and their corresponding task titles are:
house - Selling your house 
car - Buying an autonomous driving vehicle
health - Personal health decision
bird - Learning bird species
Please refer to EUCA quantatative data analysis report for the storyboard of the tasks and explanation goals presented in the user study.
2  Explanation goal
End-users may have different goals/purposes to check an explanation from AI. The EUCA dataset includes the following 11 explanation goals, with its [label] in the dataset, full name and description

[trust] Calibrate trust: trust is a key to
   establish human-AI decision-making partnership. Since users can
   easily distrust or overtrust AI, it is important to calibrate the
   trust to reflect the capabilities of AI systems.

[safe] Ensure safety: users need to ensure
   safety of the decision consequences.


[bias] - Detect bias: users need to ensure the
   decision is impartial and unbiased.


[unexpect] Resolve disagreement with AI: the AI
   prediction is unexpected and there are
   disagreements between users and AI. 


[expected] - Expected: the AI's prediction is
    expected and aligns with users'
    expectations.


[differentiate] Differentiate similar instances: due to
   the consequences of wrong decisions, users sometimes need to discern
   similar instances or outcomes. For example, a doctor differentiates
   whether the diagnosis is a benign or malignant tumor.


[learning] Learn: users need to gain knowledge,
   improve their problem-solving skills, and discover new knowledge


[control] Improve: users seek causal factors to
   control and improve the predicted outcome.


[communicate] Communicate with stakeholders: many
   critical decision-making processes involve multiple stakeholders,
   and users need to discuss the decision with them.


[report] Generate reports: users need to utilize
    the explanations to perform particular tasks such as report
    production. For example, a radiologist generates a medical report on
    a patient's X-ray image.


[multi] Trade-off multiple objectives: AI may be
    optimized on an incomplete objective while the users seek to fulfill
    multiple objectives in real-world applications. For example, a
    doctor needs to ensure a treatment plan is effective as well as has
    acceptable patient adherence. Ethical and legal requirements may
    also be included as objectives.


3 Explanatory form
The following 12 explanatory forms are end-user-friendly, i.e.: no technical knowledge is required for the end-user to interpret the explanation.

Feature-Based Explanation
Feature Attribution - fa    
Note: for tasks that has image as input data, the feature attribution is denoted by the following two cards:
ir:  important regions (a.k.a. heat map or saliency map)
irc: important regions with their feature contribution percentage


Feature Shape - fs

Feature Interaction - fi


Example-Based Explanation

Similar Example - se
Typical Example - te

Counterfactual Example - ce

Note: for contractual example, there were two visual variations used in the user study:
cet:  counterfactual example with transition from one example to the counterfactual one
ceh:  counterfactual example with the contrastive feature highlighted



Rule-Based Explanation

Rule - rt
Decision Tree - dt

Decision Flow - df


Supplementary Information

Input
Output
Performance
Dataset - prior  (output prediction with prior distribution of each class in the training set)

Note: occasionally there is a wild card, which means the participant draw the card by themselves. It is indicated as 'wc'.
For visual examples of each explanatory form card, please refer to the Explanatory_form_labels.pdf document.
Link to the details on users' requirements on different explanatory forms
Code and report for EUCA data quantatitve analysis

EUCA data analysis code
EUCA quantatative data analysis report

EUCA data citation
@article{jin2021euca,
   title={EUCA: the End-User-Centered Explainable AI Framework},
      author={Weina Jin and Jianyu Fan and Diane Gromala and Philippe Pasquier and Ghassan Hamarneh},
      year={2021},
      eprint={2102.02437},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}"	https://paperswithcode.com/dataset/euca-dataset	04/02/2021						
5582	ADFI Dataset	"ADFI Dataset is an image dataset for anomaly detection methods with a focus on industrial inspection.
Each category sub dataset comprises a training set of images and a test set of images with various kinds of defects as well as images without defects.
Supplementary information: ADFI provides a cloud service that automatically creates machine learning models for anomaly detection.
You can create anomaly detection models with these datasets for free on the ADFI website."	https://paperswithcode.com/dataset/adfi-dataset-anomaly-detection-dataset	22/02/2022	Anomaly Detection Datasets for Visual Inspection					
5583	MuLD	MuLD (Multitask Long Document Benchmark) is a set of 6 NLP tasks where the inputs consist of at least 10,000 words. The benchmark covers a wide variety of task types including translation, summarization, question answering, and classification. Additionally there is a range of output lengths from a single word classification label all the way up to an output longer than the input text.	https://paperswithcode.com/dataset/muld	15/02/2022	Multitask Long Document Benchmark					
5584	Roman Republican Coin Dataset	"Based on Crawford’s work, we collect the most diverse and extensive image dataset of the reverse sides. For most of the Roman Republic coin classes, the obverse side depicts more discriminative information than the observe side. Our dataset has 228 motif classes, including 100 classes that are the main classes for training and testing, which we call the main dataset RRCD-Main. The images of the additional 128 classes constitute the disjoint test set, RRCD-Disjoint, which we allocate to assess the generalization ability of our models. Therefore, the training and testing can be evaluated on completely disjoint datasets. To the best of our knowledge, RRCD is the most diverse dataset proposed while it is
the largest dataset of the Roman Republican coins"	https://paperswithcode.com/dataset/roman-republican-coin-dataset	01/01/2020						
5585	Malnutrition data	"The malnutrition data, from the United Nations Children's Fund  data warehouse, include two variables, stunted growth and the prevalence of low birth weight, collected in 77 countries from 1985 to 2019. Stunted growth is defined as the proportion of newborns aging from 0 to 59 months with a low height-for-age measurement (below two standard deviations). The stunted growth data represent a point sparseness case with 4-23 recordings per nation. The low birth weight data are a partial sparseness case, with recordings during 2000-2015 only.
It can be used in the functional data analysis and the sparse functional data fitting."	https://paperswithcode.com/dataset/malnutrition-data	14/03/2021	malnutrition data from UN					
5586	MuMiN	"MuMiN is a misinformation graph dataset containing rich social media data (tweets, replies, users, images, articles, hashtags), spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade.
MuMiN fills a gap in the existing misinformation datasets in multiple ways:

By having a large amount of social media information which have been semantically linked to fact-checked claims on an individual basis.
By featuring 41 languages, enabling evaluation of multilingual misinformation detection models.
By featuring both tweets, articles, images, social connections and hashtags, enabling multimodal approaches to misinformation detection.

MuMiN features two node classification tasks, related to the veracity of a claim:

Claim classification: Determine the veracity of a claim, given its social network context.
Tweet classification: Determine the likelihood that a social media post to be fact-checked is discussing a misleading claim, given its social network context.

To use the dataset, see the ""Getting Started"" guide and tutorial at the MuMiN website."	https://paperswithcode.com/dataset/mumin	23/02/2022						
5587	MuMiN-small	This is the small version of the MuMiN dataset.	https://paperswithcode.com/dataset/mumin-small	23/02/2022						
5588	MuMiN-medium	This is the medium version of the MuMiN dataset.	https://paperswithcode.com/dataset/mumin-medium	23/02/2022						
5589	MuMiN-large	This is the large version of the MuMiN dataset.	https://paperswithcode.com/dataset/mumin-large	23/02/2022						
5590	TopiOCQA	TopiOCQA (pronounced Tapioca) is an open-domain conversational dataset with topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with information-seeking questions and free-form answers. On average, a conversation in the dataset spans 13 question-answer turns and involves four topics (documents). TopiOCQA poses a challenging test-bed for models, where efficient retrieval is required on multiple turns of the same conversation, in conjunction with constructing valid responses using conversational history.	https://paperswithcode.com/dataset/topiocqa	02/10/2021						
5591	KuaiRec	KuaiRec is a real-world dataset collected from the recommendation logs of the video-sharing mobile app Kuaishou. For now, it is the first dataset that contains a fully observed user-item interaction matrix. For the term “fully observed”, we mean there are almost no missing values in the user-item matrix, i.e., each user has viewed each video and then left feedback.	https://paperswithcode.com/dataset/kuairec	22/02/2022						
5592	iFLYTEK	iFLYTEK and ChangGuang Satellite jointly held the challenge of extracting cultivated land from high-resolution remote sensing images.	https://paperswithcode.com/dataset/iflytek	22/02/2022	iFLYTEK					
5593	Supplementary material	Funding Covid-19 research: Insights from an exploratory analysis using open data infrastructures - Supplementary material	https://paperswithcode.com/dataset/supplementary-material		Funding Covid-19 research: Insights from an exploratory analysis using open data infrastructures - Supplementary material					
5594	20000 utterances	20000 utterances	https://paperswithcode.com/dataset/20000-utterances	28/02/2022	20000 utterances					
5595	MuVi	"A dataset of music videos with continuous valence/arousal ratings as well as emotion tags. 
A unique feature is that ratings are provided in 3 modalities: 


muted video


music only


music and video together


The github provides: 


video_urls.csv: Contains the YouTube ids of MuVi dataset. We can also provide all the media files (for all modalities) upon e-mail request.


participant_data.csv: We provide the anonymised profile and demographic information of the annotators.


media_data.csv: Contains the static annotations which describe the media item’s overall emotion. The terms that were used are based on the GEMS-28 term list.


av_data.csv: Includes the dynamic (continuous) annotations for Valence and Arousal."	https://paperswithcode.com/dataset/muvi		MusicVideos					
5596	GF-PA66 3D XCT (composite material 3D tomography)	"Stack of 2D gray images of glass fiber-reinforced polyamide 66 (GF-PA66) 3D X-ray Computed Tomography (XCT) specimen.
Usage: 2D/3D image segmentation
Format: HDF5
Libraries to read HDF5 files:
1) silx: https://github.com/silx-kit/silx
2) h5py: https://www.h5py.org/ 
3) pymicro: https://github.com/heprom/pymicro"	https://paperswithcode.com/dataset/gf-pa66-3d-xct-composite-material-3d	25/11/2021	Glass fiber-reinforced polyamide 66 (GF-PA66) 3D X-ray Computed Tomography (XCT)					
5597	SMCOVID19-CT	"We present a real data analysis of a CT experiment that was conducted in Italy for 8 months and involved more than 100,000 CT app users.
SM-Covid-19 uses a NO-SQL data storage system to ensure scalability and performance. At regular intervals, SM-Covid-19 backend generates a complete dump of the dataset. The dump is converted into a relational database stored into a CSV formatted file to allow the open data to be easy to consult and process. The CSV file is structured as follows: 
PID1 and PID2 fields are pre-processed via SHA256 hash with a seed stored into the SoftMining backend system. The CSV is finally cleaned to remove duplicates. 
•   Date of the contact (dd/MM/YYYY) 
•   Time of the contact (HH:MM:SS) 
•   PID1 (256-bit hex) 
•   PID2 (256-bit hex) 
•   Contact duration (Integer, in seconds) 
Contact distance (Float, in meters)"	https://paperswithcode.com/dataset/smcovid19-ct	15/05/2021	Contact Tracing Data (from Italian SM-COVID-19 App)					
5598	Icon645	"Icon645 is a large-scale dataset of icon images that cover a wide range of objects:

645,687 colored icons
377 different icon classes

These collected icon classes are frequently mentioned in the IconQA questions. In this work, we use the icon data to pre-train backbone networks on the icon classification task in order to extract semantic representations from abstract diagrams in IconQA. On top of pre-training encoders, the large-scale icon data could also contribute to open research on abstract aesthetics and symbolic visual understanding."	https://paperswithcode.com/dataset/icon645	25/10/2021						
5599	Study data	"Challenges in Migrating Imperative Deep Learning Programs to Graph Execution: An Empirical Study
File Descriptions
File | Description
--- | ---
commit_categorizations.csv | Categorizations for the commits in our dataset.
commits.csv | Information for the commits in our dataset
datasets.csv | Contains the names and descriptions of our datasets.
issue_categorizations.csv | Categorizations for the chosen issues from our dataset.
issues.csv | Information for the issues in our dataset.
pipeline_stages.csv | DL pipeline stages and their respective descriptions.
problem_categories.csv | Problem categories and their respective descriptions.
problem_causes.csv | Problem causes and their respective descriptions.
problem_fixes.csv | Problem fixes and their respective descriptions.
problem_symptoms.csv | Problem symptoms and their respective descriptions.
studied_subjects_commits.csv | Project data for commits.
studied_subjects_issues.csv | Project data for issues.
Column Descriptions
commit_categorizations.csv
Column | Description
--- | ---
tf.function related fix? | TRUE when a bug fix related to tf.function was found and FALSE otherwise. If FALSE, subsequent column values will be blank.
stage | DL pipeline stage where the problem fix was found.
issue_categorizations.csv
Column | Description
--- | ---
tf.function related problem? | TRUE when a bug related to tf.function was found and FALSE otherwise. If FALSE, subsequent column values will be blank.
stage | DL pipeline stage where the problem was found.
GH_id | GitHub issue unique identifier.
issues.csv
Column | Description
--- | ---
GH_id | GitHub issue unique identifier."	https://paperswithcode.com/dataset/study-data	21/01/2022						
5600	MCVQA	"The MCVQA dataset consists of 248, 349 training questions and 121, 512 validation
questions for real images in Hindi and Code-mixed. For each Hindi question, we also provide its 10 corresponding answers in Hindi."	https://paperswithcode.com/dataset/mcvqa	04/12/2020	Multilingual and Code-mixed Visual Question Answering					
5601	AirSim Stereo Synthetic Dataset	Synthetic Dataset created in AirSim	https://paperswithcode.com/dataset/airsim-stereo-synthetic-dataset	01/01/2022						
5602	V4V	"Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the Vision for Vitals dataset was introduced. It is a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population.
It contains more than 150+ subjects with over 1300+ videos along with ground truth heart rate and respiration rate annotations. It also includes blood pressure waveform signals as part of its physiological data."	https://paperswithcode.com/dataset/v4v	22/09/2021	Vision for Vitals					
5603	EvoGym	EvoGym is a large-scale benchmark for co-optimizing the design and control of soft robots.	https://paperswithcode.com/dataset/evogym	24/01/2022	Evolution Gym					
5604	TraVLR	TraVLR is a synthetic dataset comprising four visio-linguistic reasoning tasks. Each example encodes the scene bimodally such that either modality can be dropped during training/testing with no loss of relevant information. TraVLR's training and testing distributions are also constrained along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation.	https://paperswithcode.com/dataset/travlr	21/11/2021						
5605	Iconary	Iconary dataset is for testing multimodal communication with drawings and text.	https://paperswithcode.com/dataset/iconary	01/12/2021						
5606	image-goal-nav-dataset	A dataset for Image-Goal Navigation in Habitat based on Gibson scenes.	https://paperswithcode.com/dataset/image-goal-nav-dataset	24/02/2022						
5607	2D Moving Clusters	"Contains $10^7$ points, sampled from 20 clusters, with incremental concept drift - On each batch (of size 1000) the mean of each of the clusters moves a random (small) length in some random direction, the means move independently of each other.
This dataset should be used sequentially, in batches of $1000$."	https://paperswithcode.com/dataset/2d-moving-clusters	01/03/2022						
5608	KMIR	KMIR (Knowledge Memorization, Identification, and Reasoning) is a benchmark that covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. KMIR can be used for evaluating knowledge memorization, identification and reasoning abilities of language models.	https://paperswithcode.com/dataset/kmir	28/02/2022	Knowledge Memorization, Identification, and Reasoning					
5609	Tecnocampus Hand Image Database	"The acquisition over the VIS and TIR data was performed by a commercial thermal camera Testo 882-3. We have used a second external camera to obtain the NIR data. In this case we have built a NIR camera using a webcam changing the default optical filter for a couple of Kodak filters for IR. We have also used a printed circuit board with 16 infra-red LEDs that provide the infra-red illumination.
In order to alleviate the variability on the way the users present their hand we have used a kind of removable mask/template. Users had to put their hand in a neoprene surface with the help of a hand mask. Once the hand is placed the mask was removed and the three images (VIS, NIR, TIR) were shot. The same process was repeated with the palmar hand side, but using the mask in the opposite position (flip up to down).
Once the first acquisition was finished (no more than a minute) the user performed some exercise in order to change hand heat conditions. This step was carried out in less than 30 seconds so that when finished, the user proceeded again with the second acquisition. For each session a total of 12 hand images were captured per user."	https://paperswithcode.com/dataset/tecnocampus-hand-image-database	03/06/2013						
5610	CARL Database	"Visible and thermal images have been acquired using a thermographic camera TESTO 880-3, equipped with an uncooled detector with a spectral sensitivity range from 8 to 14 μm and provided with a germanium optical lens, and an approximate cost of 8.000 EUR. For the NIR a customized Logitech Quickcam messenger E2500 has been used, provided with a Silicon based CMOS image sensor with a sensibility to the overall visible spectrum and the half part of the NIR (until 1.000 nm approximately) with a cost of approx. 30 EUR. We have replaced the default optical filter of this camera by a couple of Kodak daylight filters for IR interspersed between optical and sensor. They both have similar spectrum responses and are coded as wratten filter 87 and 87C, respectively. In addition, we have used a special purpose printed circuit board (PCB) with a set of 16 infrared leds (IRED) with a range of emission from 820 to 1.000 nm in order to provide the required illumination.
The thermographic camera provides a resolution of 160×120 pixels for thermal images and 640×480 for visible images, while the webcam provides a still picture maximum resolution of 640×480 for near-infrared images and this has been the final resolution selected for our experiments.
A couple of halogen focus disposed 30 degrees away from the frontal direction and about 3 m away from the user, match the artificial light of the room. Note that all the tripods and structures have fixed markings on the ground."	https://paperswithcode.com/dataset/carl-database	24/02/2022						
5611	EasyCall corpus	EasyCall corpus is a dysarthric speech command dataset in Italian. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure.	https://paperswithcode.com/dataset/easycall-corpus	06/04/2021						
5612	TOPv2	"Task Oriented Parsing v2 (TOPv2) representations for intent-slot based dialog systems.
Provided under the CC-BY-SA license. Please cite the accompanying paper when
using this dataset -
@inproceedings{chen-etal-2020-low-resource,
    title={Low-Resource Domain Adaptation for Compositional Task-Oriented
        Semantic Parsing},
    author={Xilun Chen and Asish Ghoshal and Yashar Mehdad and Luke Zettlemoyer
        and Sonal Gupta},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in
        Natural Language Processing (EMNLP)},
    year={2020},
    publisher = ""Association for Computational Linguistics""
}
CHANGELOG:<br>
03/10/2021 (V1.1): Added the low-resource splits used in the paper.<br>
09/18/2020 (V1.0): Initial release.
TOPv2 is a multi-domain task-oriented semantic parsing dataset. It is an extension to the TOP dataset (http://fb.me/semanticparsingdialog) with 6 additional domains and 137k new samples.
In total, TOPv2 has 8 domains (alarm, event, messaging, music, navigation, reminder, timer, weather) and 180k samples randomly split into train, eval, and test sets for each domain. Please refer to the paper for more data statistics.
Note: As TOPv2 data is provided on a per-domain basis, the UNSUPPORTED utterances in the original TOP dataset were removed as they could not be mapped to any domain.
The training, evaluation and test sets for each domain are provided as tab-separated value (TSV) files with file names of ""domain_split.tsv"".
The first row of each file contains the column headers, while each following row is of the format:
domain <tab> utterance <tab> semantic_parse
where the semantic_parse follows the same format as the original TOP dataset.
e.g. event <tab> Art fairs this weekend in Detroit <tab> [IN:GET_EVENT [SL:CATEGORY_EVENT Art fairs ] [SL:DATE_TIME this weekend ] in [SL:LOCATION Detroit ] ]
The low-resource splits used in our experiments are provided in the
low_resource_splits subdirectory, including training and validation sets from the reminder and weather domains under 10, 25, 50, 100, 250, 500 and 1000 SPIS."	https://paperswithcode.com/dataset/topv2	18/09/2020	Task Oriented Parsing v2					
5613	Microsoft Academic Graph	"The Microsoft Academic Graph is a heterogeneous graph containing scientific publication records, citation relationships between those publications, as well as authors, institutions, journals, conferences, and fields of study.
Documentation"	https://paperswithcode.com/dataset/microsoft-academic-graph	18/05/2015						
5614	RefSeer	"A data set containing citations, citation contexts, and papers.
Download instructions"	https://paperswithcode.com/dataset/refseer	19/02/2015						
5615	GLips	The German Lipreading dataset consists of 250,000 publicly available videos of the faces of speakers of the Hessian Parliament, which was processed for word-level lip reading using an automatic pipeline. The format is similar to that of the English language Lip Reading in the Wild (LRW) dataset, with each H264-compressed MPEG-4 video encoding one word of interest in a context of 1.16 seconds duration, which yields compatibility for studying transfer learning between both datasets. Choosing video material based on naturally spoken language in a natural environment ensures more robust results for real-world applications than artificially generated datasets with as little noise as possible. The 500 different spoken words ranging between 4-18 characters in length each have 500 instances and separate MPEG-4 audio- and text metadata-files, originating from 1018 parliamentary sessions. Additionally, the complete TextGrid files containing the segmentation information of those sessions are also included. The size of the uncompressed dataset is 15GB.	https://paperswithcode.com/dataset/glips	27/02/2022	German Lips					
5616	Burr classification images	"Original images and images with RUSTICO filters applied
Also a csv with classes is included"	https://paperswithcode.com/dataset/burr-classification-images	17/11/2021						
5617	DFDM	We created a new dataset, named DFDM, with 6,450 Deepfake videos generated by different Autoencoder models. Specifically, five Autoencoder models with variations in encoder, decoder, intermediate layer, and input resolution, respectively, have been selected to generate Deepfakes based on the same input. We have  observed the visible but subtle visual differences among different Deepfakes, demonstrating the evidence of model attribution artifacts.	https://paperswithcode.com/dataset/dfdm	25/02/2022	Deepfake videos generated from different models					
5618	Intel Lab Data	"This dataset contains data collected from 54 sensors deployed in the Intel Berkeley Research lab between February 28th and April 5th, 2004.
Mica2Dot sensors with weatherboards collected timestamped topology information, along with humidity, temperature, light, and voltage values once every 31 seconds. Data was collected using the TinyDB in-network query processing system, built on the TinyOS platform."	https://paperswithcode.com/dataset/intel-lab-data							
5619	Moléne Dataset	The French national meteorological service published an open-access dataset of hourly weather observations in Brittany, France, for the month of January 2014. In addition to the graph of ground weather stations, the dataset contains hourly readings of those stations. Readings include temperatures, wind characteristics, rain, and other information.	https://paperswithcode.com/dataset/mo							
5620	VID Dataset	The Visual-Inertial-Dynamical (VID) dataset not only focuses on traditional six degrees of freedom (6-DOF) pose estimation, but also provides dynamical characteristics of the flight platform for external force perception or dynamics-aided estimation. The VID dataset contains hardware synchronized imagery and inertial measurements, with accurate ground truth trajectories for evaluating common visual-inertial estimators. Moreover, the proposed dataset highlights rotor speed and motor current measurements, control inputs, and ground truth 6-axis force data to evaluate external force estimation. To the best of our knowledge, the proposed VID dataset is the first public dataset containing visual-inertial and complete dynamical information in the real world for pose and external force evaluation.	https://paperswithcode.com/dataset/vid-dataset	20/03/2021	The Visual-Inertial-Dynamical Multirotor Dataset					
5621	NVALT-8	"Te NVALT-8 study (m=200 participants) examined if nadroparin combined with
chemotherapy could reduce cancer relapse after surgical removal of a non-small cell lung tumour."	https://paperswithcode.com/dataset/nvalt-8							
5622	NVALT-11	The NVALT-11 study considered the effect of profylactic brain radiation versus observation in ($m$=174) patients with advanced non-small cell lung cancer.	https://paperswithcode.com/dataset/nvalt-11							
5623	AKB-48	AKB-48 is a large-scale Articulated object Knowledge Base which consists of 2,037 real-world 3D articulated object models of 48 categories.	https://paperswithcode.com/dataset/akb-48	17/02/2022						
5624	MetaShift	MetaShift is a collection of 12,868 sets of natural images across 410 classes. It can be used to benchmark and evaluate how robust machine learning models are to data shifts.	https://paperswithcode.com/dataset/metashift	14/02/2022						
5625	"Dataset for the Article ""Does the Venue of Scientific Conferences Leverage their Impact? A Large Scale study on Computer Science"	"Is there any correlation between the impact of a scientific conference and the venue where it takes place?
It seems that no one has tackled this issue before, so we decided to explore the possible implications.
From the one hand, we considered the number of citations as indicator of the impact of a conference; from the other hand, we considered specific touristic indexes that characterize the venue.
In this work we report on the results of the large scale analysis we conducted on the bibliographic data we extracted from nearly 4000 conference series in the Computer Science area and over 2.5 million papers spanning more than 30 years of research. Interestingly, we found out that the two aspects are indeed related and this is shown by the detailed analysis of the data."	https://paperswithcode.com/dataset/dataset-for-the-article-does-the-venue-of	31/05/2021						
5626	WSJ Dow Jones Stock Data	Please see code repository. https://github.com/nlandolfi/acc2022treelinearcascades_stocks	https://paperswithcode.com/dataset/wsj-dow-jones-stock-data	15/02/2022						
5627	ZInd	The Zillow Indoor Dataset (ZInD) provides extensive visual data that covers a real world distribution of unfurnished residential homes. It consists of primary 360º panoramas with annotated room layouts, windows, doors and openings (W/D/O), merged rooms, secondary localized panoramas, and final 2D floor plans. The figure above illustrates the various representations (from left to right beyond capture): Room layout with W/D/O annotations, merged layouts, 3D textured mesh, and final 2D floor plan.	https://paperswithcode.com/dataset/zind		Zillow Indoor Dataset					
5628	IMDB-Clean	We have cleaned the noisy IMDB-WIKI dataset using a constrained clustering method, resulting this new benchmark for in-the-wild age estimation. The annotations also allow this dataset to use for some other tasks, like gender classification and face recognition/verification. For more details, please refer to our FPAge paper.	https://paperswithcode.com/dataset/imdb-clean	21/06/2021						
5629	TriggerCit 2021 Thailand / Nepal floods	Twitter dataset related to flood events onsets in Thailand and Nepal, focused on September 26/27, 2022, June 16/17 2021 and July 01/02 2021. The dataset has been processed with a VisualCit pipeline in order to automatically filter a relevant subset of posts through automated image analysis, using deep learning techniques. The posts were then geolocated using the CIME algorithm. Additional information about the data collection and data processing are described in http://arxiv.org/abs/2202.12014	https://paperswithcode.com/dataset/triggercit-2021-thailand-nepal-floods	24/02/2022	Twitter dataset of flood-related images for September 2021, Thailand and June/July 2021, Nepal floods					
5630	CRC100K	"This is a set of 100,000 non-overlapping image patches from hematoxylin & eosin (H&E) stained histological images of human colorectal cancer (CRC) and normal tissue.
All images are 224x224 pixels (px) at 0.5 microns per pixel (MPP).
For tissue classification; the classes are: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM).
The images were manually extracted from N=86 H&E stained human cancer tissue slides from formalin-fixed paraffin-embedded (FFPE) samples from the NCT Biobank (National Center for Tumor Diseases, Heidelberg, Germany) and the UMM pathology archive (University Medical Center Mannheim, Mannheim, Germany). Tissue samples contained CRC primary tumor slides and tumor tissue from CRC liver metastases; normal tissue classes were augmented with non-tumorous regions from gastrectomy specimen to increase variability."	https://paperswithcode.com/dataset/crc100k	07/04/2018	100,000 histological images of human colorectal cancer and healthy tissue					
5631	Dafonts Free	"This is a dataset of 18624 fonts labeled as 100% Free and Public domain / GPL / OFL on https://www.dafont.com/ with .ttf and .otf extensions.
Code used to create it can be found at: https://github.com/duskvirkus/dafonts-free"	https://paperswithcode.com/dataset/dafonts-free	06/03/2022						
5632	Mathematical Mathematics Memes	"Dataset Description
This dataset contains +10k math memes. Memes were approved by admins before being shared with group members. Thus all memes follow the community standards. Memes are about college math or above.
Acknowledgements
Thanks to the Mathematical Mathematics Memes community for sharing OC memes.
Inspiration

Generate more high-quality math memes.
Detect hateful or abusive memes.
Study the popularity of the memes.
Extract text and predict popularity.

Copyright
Copyright of all images kept by their respective owners. All images posted on Facebook are subject to fair use.
 Disclaimer: These memes were collected by blind web scraping, and I have not reviewed the vast majority of them. I do not agree with any of the sentiments contained therein."	https://paperswithcode.com/dataset/mathematical-mathematics-memes							
5633	EmoSpeech	EmoSpeech contains keywords with diverse emotions and background sounds, presented to explore new challenges in audio analysis.	https://paperswithcode.com/dataset/emospeech	18/10/2019						
5634	Earth on Canvas	"A Zero-Shot Sketch-based Inter-Modal Object Retrieval Scheme for Remote Sensing Images
WITH the advancement in sensor technology, huge amounts of data are being collected from various satellites. Hence, the task of target-based data retrieval and acquisition has become exceedingly challenging. Existing satellites essentially scan a vast overlapping region of the Earth using various sensing techniques, like multi-spectral, hyperspectral, Synthetic Aperture Radar (SAR), video, and compressed sensing, to name a few. With increasing complexity and different sensing techniques at our disposal, it has become our primary interest to design efficient algorithms to retrieve data from multiple data modalities, given the complementary information that is captured by different sensors. This type of problem is referred to as inter-modal data retrieval. In remote sensing (RS), there are primarily two important types of problems, i.e., land-cover classification and object detection. In this work, we focus on the target-based object retrieval part, which falls under the realm of object detection in RS. Object retrieval essentially requires high-resolution imagery for objects to be distinctly visible in the image. The main challenge with the conventional retrieval approach using large-scale databases is that, quite often, we do not have any query image sample of the target class at our disposal. The target of interest solely exists as a perception to the user in the form of an imprecise sketch. In such situations where a photo query is absent, it can be immensely useful if we can promptly make a quick hand-made sketch of the target. Sketches are a highly symbolic and hieroglyphic representation of data. One can exploit the notion of this minimalistic representative of sketch queries for sketch-based image retrieval (SBIR) framework. While dealing with satellite images, it is imperative to collect as many samples of images as possible for each object class for object recognition with a high success rate. However, in general, there exists a considerable number of classes for which we seldom have any training data samples. Therefore, for such classes, we can use the zero-shot learning (ZSL) strategy. The ZSL approach aims to solve a task without receiving any example of that task during the training phase. This makes the network capable of handling an unseen class (new class) sample obtained during the inference phase upon deployment of the network. Hence, we propose the aerial sketch-image dataset, namely Earth on Canvas dataset.
Classes in this dataset:
Airplane, Baseball Diamond, Buildings, Freeway, Golf Course, Harbor, Intersection, Mobile home park, Overpass, Parking lot,  River, Runway, Storage tank, Tennis court."	https://paperswithcode.com/dataset/ushasi-chaudhuri	12/08/2020						
5635	NTU-X	NTU-X is an extended version of popular NTU dataset.	https://paperswithcode.com/dataset/ntu-x	27/01/2021						
5636	LSFB Datasets	"Sign Language Datasets for French Belgian Sign Language
This dataset is built upon the work of Belgian linguists from the University of Namur. During eight years, they've collected and annotated 50 hours of videos depicting sign language conversation. 100 signers were recorded, making it one of the most representative sign language corpus. 
The annotation has been sanitized and enriched with metadata to construct two, easy to use, datasets for sign language recognition. One for continuous sign language recognition and the other for isolated sign recognition. 
LSFB-CONT
The dataset for continuous sign language recognition is made of over 25h of video clips. Each clip is associated with a time-aligned annotation file containing the start and the end of each sign along with a gloss (label) associated with all unique signs. Mediapipe pose and hands information were also computed for each video clip and these metadata are made available in the dataset.
LSFB-ISOL
The isolated version of the dataset contains only clips showing one isolated sign issued from the LSFB-CONT dataset. We chose to keep all the signs with at least 40 examples, leading to a dataset containing over 50 000 clips for 635 different glosses (labels). The Mediapipe metadata is also available for this dataset."	https://paperswithcode.com/dataset/lsfb-datasets	18/07/2021	French Belgian Sign Language Datasets					
5637	TR_AR_S2S	"Dubbed series are gaining a lot of popularity in recent years with strong support from major media service providers. Such popularity is fueled by studies that showed that dubbed versions of TV shows are more popular than their subtitled equivalents. 
This work proposes an unsupervised approach to construct speech-to-speech corpus, aligned on short segment levels, to produce a parallel speech corpus in the source- and target- languages. Our methodology exploits video frames, speech recognition, machine translation, and noisy frames removal algorithms to match segments in both languages."	https://paperswithcode.com/dataset/tr-ar-s2s	07/03/2022						
5638	Kubric	"Kubric is a data generation pipeline for creating semi-realistic synthetic multi-object videos with rich annotations such as instance segmentation masks, depth maps, and optical flow.
It also presents a series of 13 different generated datasets for tasks ranging from studying 3D NeRF models to optical flow estimation.
Kubric is mainly built on-top of pybullet (for physics simulation) and Blender (for rendering); however, the code is kept modular to potentially support different rendering backends."	https://paperswithcode.com/dataset/kubric	07/03/2022						
5639	Pretrained Models of the Benchmarking Algorithms for UVCGAN	"The pretrained models from four image translation algorithms: ACL-GAN, Council-GAN, CycleGAN, and U-GAT-IT on three benchmarking datasets: Selfie2Anime, CelebA_gender, CelebA_glasses.
We trained the models to provide benchmarks for the algorithm we detailed in the paper ""UVCGAN: UNet Vision Transformer Cycle-consistent GAN for Unpaired Image-to-Image Translation."". 
We only trained a model if a pretrained model is provided by a benchmarking algorithm."	https://paperswithcode.com/dataset/pretrained-models-of-the-benchmarking	04/03/2022						
5640	Fitness-AQA	Largest, first-of-its-kind, in-the-wild, fine-grained workout/exercise posture analysis dataset, covering three different exercises: BackSquat, Barbell Row, and Overhead Press. Seven different types of exercise errors are covered. Unlabeled data is also provided to facilitate self-supervised learning.	https://paperswithcode.com/dataset/fitness-aqa	28/02/2022	Fitness Action Quality Assessment					
5641	WITS	"This dataset is an extension of MASAC, a multimodal, multi-party, Hindi-English code-mixed dialogue dataset compiled from the popular Indian TV show,
‘Sarabhai v/s Sarabhai’.  WITS was created by augmenting MASAC with natural language explanations for each sarcastic dialogue. The dataset consists of the transcribed sarcastic dialogues from 55 episodes of the TV show, along with audio and video multimodal signals. It was designed to facilitate Sarcasm Explanation in Dialogue (SED), a novel task aimed at generating a natural language explanation for a given sarcastic dialogue, that spells out the intended irony. Each data instance in WITS is associated with a corresponding video, audio, and textual transcript where the last utterance is sarcastic in nature. All the final selected explanations contain the following attributes:
• Sarcasm source: The speaker in the dialog who is being sarcastic.
• Sarcasm target: The person/ thing towards whom the sarcasm is directed.
• Action word: Verb/ action used to describe how the sarcasm is taking place.  e.g. mocking, insults, taunts, etc.
• Description: A description of the scene to help contextualize the sarcasm."	https://paperswithcode.com/dataset/wits	12/03/2022	Why Is This Sarcastic?					
5642	FloW	"Marine wastes are severely threatening marine animals and their habitat, also causing an impact on human life through toxic substances transportation and accumulation. To prevent the wastes especially the plastic trash from getting into the ocean, it is essential to detect and clean the floating wastes in inland waters efficiently like in rivers, lakes, and canals.


FloW is the first dataset for floating waste detection in inland waters. It contains a vision-based sub-dataset, FloW-Img, and a multimodal dataset, FloW-RI which contains the spatial and temporal calibrated image and millimeter-wave radar data.


By publishing Flow, it is hoped that more attention from research communities could be paid to floating waste detection in inland waters as well as the challenging small object detection over the water surface. In addition, waste detection based on millimeter-wave radar data or the fusion of image and radar data is also a novel task and FloW provides accessible real-world data."	https://paperswithcode.com/dataset/flow							
5643	ATOM3D	"ATOM3D is a unified collection of datasets concerning the three-dimensional structure of biomolecules, including proteins, small molecules, and nucleic acids. These datasets are specifically designed to provide a benchmark for machine learning methods which operate on 3D molecular structure, and represent a variety of important structural, functional, and engineering tasks. All datasets are provided in a standardized format along with a Python package containing processing code, utilities, models, and dataloaders for common machine learning frameworks such as PyTorch. ATOM3D is designed to be a living database, where datasets are updated and tasks are added as the field progresses.
Description from: https://www.atom3d.ai/"	https://paperswithcode.com/dataset/atom3d	07/12/2020						
5644	SILG	Symbolic Interactive Language Grounding (SILG) is a multi-environment benchmark which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM, Messenger, NetHack), as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes (ALFWorld, Touchdown). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language specification, and plan complexity.	https://paperswithcode.com/dataset/silg	20/10/2021	Symbolic Interactive Language Grounding					
5645	MPSGaze	This is a synthetic dataset containing full images (instead of only cropped faces) that provides ground truth 3D gaze directions for multiple people in one image.	https://paperswithcode.com/dataset/mpsgaze	01/03/2022	Multi-Person Swap Gaze Dataset					
5646	I.PHI	I.PHI processes the Packard Humanities Institute (PHI) database of ancient Greek inscriptions including the geographical and chronological metadata into a machine actionable format. The processed dataset is referred to as I.PHI.	https://paperswithcode.com/dataset/i-phi	09/03/2022						
5647	Human Activity Recognition	We provide six different datasets with diverse range of activities	https://paperswithcode.com/dataset/human-activity-recognition	06/07/2020						
5648	ImageNet-Patch	"ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches
Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations."	https://paperswithcode.com/dataset/imagenet-patch	07/03/2022						
5649	RENOIR	A dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes.	https://paperswithcode.com/dataset/renoir	29/09/2014						
5650	Fingerprint inpainting and denoising	"Synthetic training set: This set is constructed in the following two steps and will be used for estimation/training purposes. i) 84,000 275 pixel x 400 pixel ground-truth fingerprint images without any noise or scratches, but with random transformations (at most five pixels translation and +/-10 degrees rotation) were generated by using the software Anguli: Synthetic Fingerprint Generator. ii) 84,000 275 pixel x 400 pixel degraded fingerprint images were generated by applying random artifacts (blur, brightness, contrast, elastic transformation, occlusion, scratch, resolution, rotation) and backgrounds to the ground-truth fingerprint images. In total, it contains 168,000 fingerprint images (84,000 fingerprints, and two impressions - one ground-truth and one degraded - per fingerprint).
Synthetic test set: This set is constructed similarly to the synthetic training set and will be used to evaluate the reconstruction performance. In total, it contains 16,800 fingerprint images (8,400 fingerprints and two impressions - one ground-truth and one degraded - per fingerprint). Since this set will be used for the purpose of evaluating the reconstruction performance, only the degraded and not the ground-truth fingerprint images will be provided to participants.
Real test set: This set is constructed by systematically drawing fingerprint images with varying sizes from publicly available datasets. In total, it contains 1680 fingerprint images (140 fingerprints and 12 impressions - high-quality scans under operational conditions - per fingerprint).
Description from: Fingerprint inpainting and denoising (WCCI'18, ECCV'18)"	https://paperswithcode.com/dataset/fingerprint-inpainting-and-denoising							
5651	Cross-View Time Dataset	The appearance of the world varies dramatically not only from place to place but also from hour to hour and month to month. Every day billions of images capture this complex relationship, many of which are associated with precise time and location metadata. We propose to use these images to construct a global-scale, dynamic map of visual appearance attributes. Such a map enables fine-grained understanding of the expected appearance at any geographic location and time. Our approach integrates dense overhead imagery with location and time metadata into a general framework capable of mapping a wide variety of visual attributes. A key feature of our approach is that it requires no manual data annotation. We demonstrate how this approach can support various applications, including image-driven mapping, image geolocalization, and metadata verification.	https://paperswithcode.com/dataset/cross-view-time-dataset	29/12/2020						
5652	Cross-View Time Dataset (Cross-Camera Split)	"The standard evaluation protocol of Cross-View Time dataset allows for certain cameras to be shared between training and testing sets. This protocol can emulate scenarios in which we need to verify the authenticity of images from a particular set of devices and locations. Considering the ubiquity of surveillance systems (CCTV) nowadays, this is a common scenario, especially for big cities and high visibility events (e.g., protests, musical concerts, terrorist attempts, sports events). In such cases, we can leverage the availability of historical photographs of that device and collect additional images from previous days, months, and years. This would allow the model to better capture the particularities of how time influences the appearance of that specific place, probably leading to a better verification accuracy. However, there might be cases in which data is originated from heterogeneous sources, such as social media. In this sense, it is essential that models are optimized on camera-disjoint sets to avoid learning sensor-specific characteristics that might not generalize accordingly for new imagery during inference.
With this in mind, we propose a novel organization for CVT dataset. We split available data into training and testing sets, ensuring that all images from a single camera are assigned to the same set. During this division, we aimed to keep the size of each set roughly similar to the original splits, allowing models to be optimized with similar amounts of data."	https://paperswithcode.com/dataset/cross-view-time-dataset-cross-camera-split	30/03/2022						
5653	MVTEC 3D-AD	MVTec 3D Anomaly Detection Dataset (MVTec 3D-AD) is a comprehensive 3D dataset for the task of unsupervised anomaly detection and localization. It contains over 4000 high-resolution scans acquired by an industrial 3D sensor. Each of the 10 different object categories comprises a set of defect-free training and validation samples and a test set of samples with various kinds of defects. Precise ground-truth annotations are provided for each anomalous test sample.	https://paperswithcode.com/dataset/mvtec-3d-ad	16/12/2021	THE MVTEC 3D ANOMALY DETECTION DATASET					
5654	BODMAS	"We collaborate with Blue Hexagon to release a dataset containing timestamped malware samples and well-curated family information for research purposes. 
The BODMAS dataset contains 57,293 malware samples and 77,142 benign samples collected from August 2019 to September 2020,  with carefully curated family information (581 families).
We also provide preprocessed feature vectors and metadata available to everyone.
The malware binaries can be obtained per request.
Source: BODMAS: An Open Dataset for Learning based Temporal Analysis of PE Malware"	https://paperswithcode.com/dataset/bodmas		Blue Hexagon Open Dataset for Malware AnalysiS					
5655	StepGame	A Benchmark for Robust Multi-Hop Spatial Reasoning in Texts	https://paperswithcode.com/dataset/stepgame	22/02/2022	StepGame					
5656	NASA Perseverance	Samples from NASA Perseverance and set of GAN generated synthetic images from Neural Mars.	https://paperswithcode.com/dataset/nasa-perseverance	04/01/2022						
5657	Moroccan Monay dataset	A dataset of all Moroccan money	https://paperswithcode.com/dataset/moroccan-monay-dataset							
5658	CANDOR Corpus	The CANDOR corpus is a large, novel, multimodal corpus of 1,656 recorded conversations in spoken English. This 7+ million word, 850 hour corpus totals over 1TB of audio, video, and transcripts, with moment-to-moment measures of vocal, facial, and semantic expression, along with an extensive survey of speaker post conversation reflections.	https://paperswithcode.com/dataset/candor-corpus-1	01/03/2022	CANDOR = Conversation: A Naturalistic Dataset of Online Recordings					
5659	ML guided Logic synthesis	Logic synthesis is a challenging and widely-researched combinatorial optimization problem during integrated circuit (IC) design. It transforms a high-level description of hardware in a programming language like Verilog into an optimized digital circuit netlist, a network of interconnected Boolean logic gates, that implements the function. Spurred by the success of ML in solving combinatorial and graph problems in other domains, there is growing interest in the design of ML-guided logic synthesis tools. Yet, there are no standard datasets or prototypical learning tasks defined for this problem domain. Here, we describe OpenABC-D,a large-scale, labeled dataset produced by synthesizing open source designs with a leading open-source logic synthesis tool and illustrate its use in developing, evaluating and benchmarking ML-guided logic synthesis. OpenABC-D has intermediate and final outputs in the form of 870,000 And-Inverter-Graphs (AIGs) produced from 1500 synthesis runs plus labels such as the optimized node counts, and de-lay. We define a generic learning problem on this dataset and benchmark existing solutions for it. The codes related to dataset creation and benchmark models are available athttps://github.com/NYU-MLDA/OpenABC.git.	https://paperswithcode.com/dataset/ml-guided-logic-synthesis	21/10/2021						
5660	NELA-GT-2021	NELA-GT-2021 is the fourth installment of the NELA-GT datasets, NELA-GT-2021. The dataset contains 1.8M articles from 367 outlets between January 1st, 2021 and December 31st, 2021. Just as in past releases of the dataset, NELA-GT-2021 includes outlet-level veracity labels from Media Bias/Fact Check and tweets embedded in collected news articles.	https://paperswithcode.com/dataset/nela-gt-2021	10/03/2022						
5661	K-SportsSum	K-SportsSum is a sports game summarization dataset with two characteristics: (1) K-SportsSum collects a large amount of data from massive games. It has 7,854 commentary-news pairs. To improve the quality, K-SportsSum employs a manual cleaning process; (2) Different from existing datasets, to narrow the knowledge gap, K-SportsSum further provides a large-scale knowledge corpus that contains the information of 523 sports teams and 14,724 sports players.	https://paperswithcode.com/dataset/k-sportssum	24/11/2021						
5662	SportsSum	SportsSum is a Chinese sports game summarization dataset that contains 5,428 soccer games of live commentaries and the corresponding news articles.	https://paperswithcode.com/dataset/sportssum							
5663	SKM-TEA	"The SKM-TEA dataset pairs raw quantitative knee MRI (qMRI) data, image data, and dense labels of tissues and pathology for end-to-end exploration and evaluation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies.
Challenge Tracks
DICOM Track: The DICOM benchmarking track uses scanner-generated DICOM images as the input for image segmentation and detection tasks.
Raw Data Track: The Raw Data benchmarking track uses raw MRI data (i.e. k-space) as the input for image reconstruction, segmentation and detection tasks."	https://paperswithcode.com/dataset/skm-tea	14/03/2022	Stanford Knee MRI with Multi-Task Evaluation					
5664	ILPC22-Small	"A small dataset from the Inductive Link Prediction Challenge 2022. 
Training graph contains 10K entities, 96 relations, 78K triples. 
Inference graph contains 7K entities, 96 relations, 21K triples.
Validation and test triples to predict belong to the inference graph."	https://paperswithcode.com/dataset/ilpc22-small	03/03/2022	ILPC22-Small					
5665	ILPC22-Large	"A large dataset from the Inductive Link Prediction Challenge 2022. 
Training graph contains 46K entities, 130 relations, 202K triples. 
Inference graph contains 30K entities, 130 relations, 77K triples.
Validation and test triples to predict belong to the inference graph."	https://paperswithcode.com/dataset/ilpc22-large	03/03/2022	ILPC22-Large					
5666	PRIME	"This dataset contains both infeasible and feasible data points as described in PRIME. The descriptors of the collected data are presented in the table below.
|                  | # of Infeasible | # of Feasible | Max Runtime (ms) | Min Runtime (ms) | Average Runtime (ms) |
|------------------|-----------------|---------------|------------------|------------------|----------------------|
| MobileNetEdgeTPU |          384355 |        115711 |         16352.26 |           252.22 |               529.13 |
| MobilenetV2      |          744718 |        255414 |          7398.13 |           191.35 |               375.05 |
| MobilenetV3      |          797460 |        202672 |          7001.46 |           405.19 |               993.75 |
| M4               |          791984 |        208148 |         35881.35 |           335.59 |               794.33 |
| M5               |          698618 |        301514 |         35363.55 |           202.55 |               440.52 |
| M6               |          756468 |        243664 |          4236.90 |           127.79 |               301.74 |
| UNet             |          449578 |         51128 |        124987.51 |           610.96 |              3681.75 |
| T-RNN Dec        |          405607 |         94459 |          4447.74 |           128.05 |               662.44 |
| T-RNN Enc        |          410933 |         88880 |          5112.82 |           127.97 |               731.20 |"	https://paperswithcode.com/dataset/prime	17/03/2022						
5667	BBAI Dataset	"This dataset is for evaluating the task of Black-box Multi-agent Integration which focuses on combining the capabilities of multiple black-box conversational agents at scale. It provides data to explore two main frameworks of exploration: question agent pairing and question response pairing.
Overall this dataset contains 5550 utterances with 19 question-response pairs per question (one from each of the 19 agents), 105,450 in total across 37 domains. The utterances are split into 3700 utterances (100 examples per domain) for the training set and 1850 (50 per domain) for the test set. The train and test sets respectively contain 2399 and 1186 utterances with at least one positive question-response pair. In the remaining examples, none of the agents were able to achieve annotator agreement (>= 3)."	https://paperswithcode.com/dataset/bbai-dataset	15/03/2022	Black-box Agent Integration					
5668	MSP-Podcast	"The MSP-Podcast corpus contains speech segments from podcast recordings which are perceptually annotated using crowdsourcing. The collection of this corpus is an ongoing process. Version 1.7 of the corpus has 62,140 speaking turns (100hrs).
Key features of this corpus:

We download available audio recordings with common license. We only use the podcasts that have less restrictive licenses, so we can modify, sell and distribute the corpus (you can use it for commercial product!). 
Most of the segments in a regular podcasts are neutral. We use machine learning techniques trained with available data to retrieve candidate segments. These segments are emotionally annotated with crowdsourcing. This approach allows us to spend our resources on speech segments that are likely to convey emotions.
We annotate categorical emotions and attribute based labels at the speaking turn label
This is an ongoing effort, where we currently have 62,140 speaking turns (100h). We collect approximately 10,000-13,000 new speaking turns per year. Our goal is to reach 400 hours."	https://paperswithcode.com/dataset/msp-podcast		A large naturalistic speech emotional dataset					
5669	Thermal focus image database	"The database was acquired using a thermographic camera TESTO 880-3. This camera is equipped with an uncooled detector and has a spectral sensitivity range from 8 to 14 μm. It has a removable German optic lens. It provides the following main features:
Image resolution: 160 × 120 pixels.
Optical field/min. focus distance: 32° × 24°/0.1 m.
Thermal sensitivity (NETD) <0.1 °C at 30 °C.
Geometric resolution: 3.5 mrad.
Detector type: FPA 160 × 120 pixels, temperature-stabilized.
The database consists of several image sets. In each set, the camera acquires one image of the scene at each lens position. In our case we have manually moved the lens in 1 mm steps which provides a total of 96 positions. Thus, each set consists of 96 different images of the one scene. For this purpose, we have attached a millimeter tape to the objective, and used a stable tripod in order to acquire the same scene for each scene position. to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset
We acquire different kinds of images according to their information content and depth of focus. It should be easier to focus an image with large amount of detail, because blurring in such an image will generally be more evident. As in visible images, it should be more difficult to completely focus an image with several objects, when each object is located at a different focal distance. We analyzed only static scenes, because of the need for comparability (same position and temperature).
We have constructed 10 different databases, as follows.

Telematic equipment (TE): this consists of four sets of images of the same scene (an item of telematic equipment). Set one (TE1) is acquired at one meter distance from the scene to the camera, set two (TE2) is acquired at two meters, set three (TE3) at three meters and set four (TE4) at 4 m. Obviously, when moving the camera away from the scene, more objects appear in the image. On the other hand, these four databases contain a scene that can be considered to be contained in a flat plane. Thus, it is acquiring mainly a two dimensional object with a single point of focus.
Electronic circuit (EC): this consists of a single set of images of the same scene (an electronic circuit with components at different temperatures and distances from the camera). It is important to emphasize that, in this case, we are acquiring a very near object, in which there is a range of depth. Thus, it is not possible to focus the whole image simultaneously.
Laptop transformer (LT): this consists of a single set of images of the same scene (the transformer of a laptop computer).
Corridor and fluorescents (CF): this consists of a single set of images of a single scene (a corridor at the university, illuminated by several ceiling fluorescents).
Heater (H): this consists of a single set of images of a heater. This scene contains a large amount of detail because the metallic parts are warmer than the spaces between.
Face (F): this consists of a single set of images of a human face. This sequence contains a scene that is not fully static because of involuntary physical movement (eyes, breathing, etc.).
Hand (Ha): this consists of a single set of images of a hand. The hand rests on a black surface.
The database consists of 10 × 96 = 960 images."	https://paperswithcode.com/dataset/thermal-focus-image-database	02/11/2011						
5670	TR-News	"This dataset is collected from various global and local news sources. Toponyms are manually annotated in the articles with the corresponding entries from GeoNames. In total, the dataset
consists of 118 articles."	https://paperswithcode.com/dataset/tr-news	04/05/2018						
5671	Reddit Conversation Corpus	Reddit Conversation Corpus (RCC) consists of conversations, scraped from Reddit, for a 20 month period from November 2016 until August 2018. To ensure the quality and diversity of topics, 95 subreddits are selected from which conversations are collected. In total, RCC contains 9.2 million 3-turn conversations.	https://paperswithcode.com/dataset/reddit-conversation-corpus							
5672	IEEE-CIS Fraud Detection	"Can you detect fraud from customer transactions?
Imagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren’t thinking about the data science that determined your fate.
Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. “Press 1 if you really tried to spend $500 on cheddar cheese.”
While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle.
IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with the world’s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge.
In this competition, you’ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.
If successful, you’ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives.
Acknowledgements:
Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually."	https://paperswithcode.com/dataset/ieee-cis-fraud-detection	15/07/2019						
5673	Kinetics-100	Kinetics-100 is a dataset split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples. The 100 classes are further split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively.  Link to the selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/kinetics-100	https://paperswithcode.com/dataset/kinetics-100							
5674	Something-Something-100	Something-Something-100 is a dataset split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples. The 100 classes were split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively. Link to exactly selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/smsm-100	https://paperswithcode.com/dataset/something-something-100							
5675	Slovenian Twitter dataset 2018-2020	"A comprehensive set of all Slovenian tweets posted in the 2018-2020 period,
with retweet links and assigned hate speech classes.
Available at a public language resource repository CLARIN.SI."	https://paperswithcode.com/dataset/slovenian-twitter-dataset-2018-2020	31/05/2021						
5676	AIT-LDSv2.0	Synthetic log data suitable for evaluation of intrusion detection systems, federated learning, and alert aggregation.  Each of the 8 datasets corresponds to a testbed representing a small enterprise network including mail server, file share, WordPress server, VPN, firewall, etc. Normal user behavior is simulated to generate background noise over a time span of 4-6 days. At some point, a sequence of attack steps are launched against the network. Log data is collected from all hosts and includes Apache access and error logs, authentication logs, DNS logs, VPN logs, audit logs, Suricata logs, network traffic packet captures, horde logs, exim logs, syslog, and system monitoring logs. Attacks include scans (nmap, WPScan, dirb), webshell upload, password cracking, privilege escalation, remote command execution, and data exfiltration.	https://paperswithcode.com/dataset/ait-ldsv2-0	24/02/2022	AIT Log Data Set V2.0					
5677	eVED	Extended Vehicle Energy Dataset (eVED) is an extended version of the Vehicle Energy Dataset (VED), which is a large-scale dataset for vehicle energy consumption analysis. Compared with its original version, the extended VED (eVED) dataset is enhanced with accurate vehicle trip GPS coordinates, serving as a reliable basis to associate the VED trip records with external information e.g., road speed limit and intersections, from accessible map services to accumulate attributes that is relevant and essential in analyzing vehicle energy consumption.	https://paperswithcode.com/dataset/eved	16/03/2022	Extended Vehicle Energy Dataset					
5678	Multi-focus thermal database	"The database was acquired using a thermographic camera TESTO 882-3 equipped with an uncooled detector and a spectral sensitivity range from 8 to 14 μm. It has a removable German optic lens with these main features:
image resolution: 320 × 240 px,
spectral sensitivity: 8 to 14 μm,
thermal sensitivity (NETD)<0.06 °C at 30 °C,
geometric resolution (IFOV): 1.7 mrad,
detector type: silicon microbolometer uncooled, temperature stabilized,
FOV: 32° × 23°; focal distance: 15 mm; fixed aperture: f/0.95.
The database consists of six image sets. In each set, the camera acquires one image of the scene at each lens position. In our case we have manually moved the lens in 1 mm steps, which provides a total of 96 positions. Thus, each set consists of 96 different images of the one scene. For this purpose, we have attached a millimeter tape to the objective. We also used a stable tripod in order to acquire the same scene for each scene position and a dimmer to fix the bulb current.
We have acquired six image sets:
Image set 1: scene is made up of mobile phone and RS-232 interface in different distances and homogenous heat absorbing background. Distance between camera and the first object is 35 cm and its temperature is 41.2 °C. The distance between objects is 40 cm for all images sets. The maximum temperature of the second object is 32.9 °C.
Image set 2: scene is made up of mobile phone and RS-232 interface in different distances and homogenous heat absorbing background. Distance between camera and the first object is only 15 cm and its temperature is 39.4 °C. The maximum temperature of the second object is 55.9 °C.
Image set 3: scene is made up of two bulbs in different distances and non-homogenous background (partially black and partially white). The bulbs are acquired with a view to the holders. Distance between camera and the first object is 30 cm as in all bulb image sets. The temperature of 1st bulb is 51.7 °C and 2nd is 50.4 °C.
Image set 4: scene is made up of two bulbs in different distances and homogenous white background. The bulbs are acquired with a view to the holders. The temperature of the first bulb is 43.3 °C and 2nd is 41.3 °C.
Image set 5: scene is made up of two bulbs in different distances and homogenous white background. The bulbs are acquired without a view to the holders. The temperature of the first bulb is 57.0 °C and 2nd is 53.6 °C.
Image set 6: scene is made up of two bulbs in different distances and homogenous heat absorbing black background. The bulbs are acquired with a view to the holders. The temperature of the first bulb is 57.9 °C and 2nd is 54.7 °C.
The reference images, where both objects are sharp, were created very easily using this command in MATLAB: img = [img1(:,1:thr) img2(:,thr+1:end)]; where img1 and img2 is the image with a perfectly sharp object 1 and 2 respectively. Variable thr determines the border between these two objects."	https://paperswithcode.com/dataset/multi-focus-thermal-database	15/05/2013						
5679	i3DMM Test Dataset	A new dataset consisting of 64 people with different expressions and hairstyles.	https://paperswithcode.com/dataset/i3dmm-test-dataset	28/11/2020						
5680	NOAA Atmospheric Temperature Dataset	"This dataset contains meteorological observations (temperature) at the
land-based weather stations located in the United States, collected from the Online Climate
Data Directory of the National Oceanic and Atmospheric Administration (NOAA). The weather
stations are sampled from the Western and Southeastern states that have actively measured
meteorological observations during 2015. The 1-year sequential data of hourly temperature
records are divided into small sequences of 24 hours. For training, validation, and test a sequential
8-2-2 (months) split is used."	https://paperswithcode.com/dataset/noaa-atmospheric-temperature-dataset	01/01/2020						
5681	NEMO Sea Surface Temperature Dataset	"This dataset contains spatiotemporal sequences of SST generated by the
NEMO ocean engine.  The observations correspond to 250 randomly selected data sites
within a [0, 550] × [100, 650] square cropped from the area between 50 deg N − 65 deg N and 75W deg −
10W deg starting from 01-01-2016 to 12-31-2017. The data is divided into 24 sequences, each
lasting 30 days (extra days in each month are truncated). Data corresponding to 2016 are used for
training and the rest is used for validation and testing, in the equal sequential split."	https://paperswithcode.com/dataset/nemo-sea-surface-temperature-dataset	16/03/2022						
5682	JaNLI	"The Japanese Adversarial NLI (JaNLI) dataset is designed to require understanding of Japanese linguistic phenomena and illuminate the vulnerabilities of models.
Please see the paper Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference for details."	https://paperswithcode.com/dataset/janli	01/11/2021	Japanese Adversarial Natural Language Inference					
5683	Sachs	Sachs dataset measures the expression level of different proteins and phospholipids in human cells. It includes the simultaneous measurements of 11 phosphorylated proteins and phospholipids derived from thousands of individual primary immune system cells, subjected to both general and specific molecular interventions.	https://paperswithcode.com/dataset/sachs	16/03/2022	Sachs Protein Dataset					
5684	Full-Spectral Autofluorescence Lifetime Microscopic Images	"The dataset contains full-spectral autofluorescence lifetime microscopic images (FS-FLIM) acquired on unstained ex-vivo human lung tissue, where 100 4D hypercubes of 256x256 (spatial resolution) x 32 (time bins) x 512 (spectral channels from 500nm to 780nm). This dataset associates with our paper ""Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&E-Stained Histology Images"" (https://arxiv.org/abs/2202.07755) and ""Full spectrum fluorescence lifetime imaging with 0.5 nm spectral and 50 ps temporal resolution"" (https://doi.org/10.1038/s41467-021-26837-0). 
The FS-FLIM images provide transformative insights into human lung cancer with extra-dimensional information. This will enable visual and precise detection of early lung cancer. With the methodology in our co-registration paper, FS-FLIM images can be registered with H&E-stained histology images, allowing characterisation of tumour and surrounding cells at a celluar level with absolute autofluorescence lifetime.
The dataset can be used for various purposes, including signal processing for optimal lifetime reconstruction, advanced image analysis for automatic feature extraction of lung cancer, and cellular-level characterisation of lung cancer with absolute label-free autofluorescence lifetime values.
The dataset is available on the University of Edinburgh's DataShare (https://doi.org/10.7488/ds/3099 and https://doi.org/10.7488/ds/3421)"	https://paperswithcode.com/dataset/full-spectral-autofluorescence-lifetime	15/02/2022						
5685	XLING	The XLING BLI Dataset contains bilingual dictionaries for 28 language pairs. For each of the language pairs, there are 5 dictionary files: 4 training dictionaries of varying sizes (500, 1K, 3K, and 5K translation pairs) and one testing dictionary containing 2K test word pairs. All results reported in the above paper have been obtained on test dictionaries of respective language pairs.	https://paperswithcode.com/dataset/xling		XLING BLI Dataset					
5686	DATASET	Dataset used in research submitted to ICPC ERA 2022	https://paperswithcode.com/dataset/dataset	16/03/2022						
5687	Nations	The Nations dataset is a small knowledge graph with 14 entities, 55 relations, and 1992 triples describing countries and their political relationships. This dataset is available for download from https://github.com/ZhenfengLei/KGDatasets.	https://paperswithcode.com/dataset/nations							
5688	PanLex-BLI	PanLex-based bilingual lexicons for 210 language pairs	https://paperswithcode.com/dataset/panlex-bli		PanLex-based bilingual lexicons for 210 language pairs					
5689	FE108	"Large-scale single-object tracking dataset, containing 108 sequences with a total length of 1.5 hours. 
FE108 provides ground truth annotations on both the frame and event domain. 
The annotation frequency is up to 40Hz and 240Hz for the frame and event domains, respectively. 
FE108 is the largest event-frame-based dataset for single object tracking, and also offers the highest annotation frequency in the event domain."	https://paperswithcode.com/dataset/fe108	19/09/2021						
5690	IntHarmony	"This newly curated synthetic dataset specifies an additional reference region to guide image harmonization. There are 118,287 training images and 959 test images.  The dataset consists of objects, backgrounds, and people.
IntHarmony has the following information for each data instance: composite image, ground truth, foreground mask of the composite foreground, and a guide mask that provides information about the reference region to guide harmonization. 
IntHarmony is built on top of the MS-COCO dataset, and makes use of the instance masks provided in MS-COCO to simulate foreground and reference regions. First, a random instance mask is selected to pick the foreground region. The selected foreground region is then augmented using a wide set of meaningful augmentations focusing on luminance, contrast and color. Another random instance mask is used to get the reference guide mask. The original image is considered the ground truth. The instance masks and the augmentations are chosen at random to induce more generalizability to the network."	https://paperswithcode.com/dataset/intharmony	15/03/2022						
5691	ToxiGen	"A large-scale and machine-generated dataset of 274,186 toxic and benign statements about 13 minority groups. 
This dataset uses a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and
benign text with a massive pre-trained language model (GPT-3). Controlling machine generation in this way allows TOXIGEN to cover implicitly toxic text at a larger scale,
and about more demographic groups, than previous resources of human-written text. TOXIGEN can be used to fight human-written and machine-generated
toxicity."	https://paperswithcode.com/dataset/toxigen	17/03/2022						
5692	PET	"The dataset contains 45 documents containing narrative description of business process and their annotations.  Annotated with activities, gateways, actors, and flow information. 
Each document is composed of three files:
Doc_name.txt (Process description in CONLL format)
Doc_name.process-elements.IOB2.txt (Process elements annotated with IOB2 Schema in CONLL format)
Doc_name.relations.tsv (Process relations between process elements. Each line is a triplette (source, relation tag, target). Source and target are in the form: n_sent_x words range.)"	https://paperswithcode.com/dataset/pet	09/03/2022	PET: A new Dataset for Process Extraction from Natural Language Text					
5693	PET: A new Dataset for Process Extraction from Natural Language Text	"The dataset contains 45 documents containing narrative description of business process and their annotations.  Annotated with activities, gateways, actors, and flow information. 
Each document is composed of three files:
Doc_name.txt (Process description in CONLL format)
Doc_name.process-elements.IOB2.txt (Process elements annotated with IOB2 Schema in CONLL format)
Doc_name.relations.tsv (Process relations between process elements. Each line is a triplette (source, relation tag, target). Source and target are in the form: n_sent_x words range.)"	https://paperswithcode.com/dataset/pet-a-new-dataset-for-process-extraction-from	09/03/2022						
5694	EGDB	"This dataset contains transcriptions of the electric guitar performance of 240 tablatures, rendered with different tones. The goal is to contribute to automatic music transcription (AMT) of guitar music, a technically challenging task. 
Activity signals were captured by attaching a special hexaphonic pickup to each string of an electric guitar and using a JUCE program to control a digital audio workstation (DAW) to automatically re-render the audio recordings of the “Direct Input” (DI) using different amplifiers (Amps), including low-gain amps and high-gain ones. A new collecting pipeline was employed to reduce the effort of manual inspection. The final dataset contains six copies of 118 minutes of guitar playing, each copy being associated with a different timbre. The new dataset, named “EGDB,” is constructed in this way to account for the diverse timbre associated with electric guitar. Having multiple guitar tones makes it possible to test a trained model
on held-out unseen tones for generalizability. 
<span style=""color:grey; opacity: 0.6"">( Image Source: Frame Harirak )</span>"	https://paperswithcode.com/dataset/egdb	20/02/2022						
5695	DanceTrack	A large-scale multi-object tracking dataset for human tracking in occlusion, frequent crossover, uniform appearance and diverse body gestures. It is proposed to emphasize the importance of motion analysis in multi-object tracking instead of mainly appearance-matching-based diagram.	https://paperswithcode.com/dataset/dancetrack	29/11/2021						
5696	ChildCIdb	"A large-scale, first-of-its-kind database aimed at generating a better understanding of the way children interact with mobile devices during their development process. ChildCIdbv1 comprises data collected from 438 children, from 18 months to 8 years old, encompassing the first three development stages of Piaget's theory. Data collected spans interaction with screens using both finger and pen stylus, information regarding the previous experience of the child with mobile devices, the child’s grade level, and whether attention-deficit/hyperactivity disorder (ADHD) is present.
Use cases: 
Child age detection based on device interaction."	https://paperswithcode.com/dataset/childcidb	02/02/2021	ChildCIdbv1					
5697	VidHarm	VidHarm is a professionally annotated dataset for detection of harmful content in video. Include 3589 annotate video clips from a variety of film trailers. In contrast to previous approaches which mostly use meta data from long sequences, it uses the raw video and focus on short clips.	https://paperswithcode.com/dataset/vidharm	15/06/2021						
5698	Spatial Commonsense Graph Dataset	Dataset built from partial reconstructions of real-world indoor scenes using RGB-D sequences from ScanNet, aimed at estimating the unknown position of an object (e.g. where is the bag?) given a partial 3D scan of a scene. The dataset mostly consists of bedrooms, bathrooms, and living rooms. Some room types like closet and gym only have a few instances.	https://paperswithcode.com/dataset/spatial-commonsense-graph-dataset	10/03/2022	Spatial Commonsense Graph for Object Localisation in Partial Scenes					
5699	HOPE-Image	"The NVIDIA HOPE datasets consist of RGBD images and video sequences with labeled 6-DoF poses for 28 toy grocery objects. The toy grocery objects are readily available for purchase and have ideal size and weight for robotic manipulation. 3D textured meshes for generating synthetic training data are provided.
The HOPE-Image dataset shows the objects in 50 scenes from 10 household/office environments, and contains 188 test images taken in 8 environments, with a total of 40 scenes (unique camera and object poses). Up to 5 lighting variations are captured for each scene, including backlighting and angled direct lighting with cast shadows. Scenes are cluttered with varying levels of occlusion.
An additional 50 validation images are included from 2 environments in 10 scene arrangements.
Within each scene, up to 5 lighting variations are captured with the same camera and object poses. For example, the captures in valid/scene_0000/*.json all depict the same camera pose and arrangement of objects, but each individual capture (0000.json, 0001.json, ...) has a different lighting condition. For this reason, each image should be treated independently for purposes of pose prediction. The most favorable lighting condition for each scene is found in image 0000.json.
Images were captured using a RealSense D415 RGBD camera. Systematic errors were observed in the depth values relative to the estimated distance of a calibration grid. To correct for this, depth frames are scaled by a factor of 0.98042517 before registering to RGB. Annotations were made manually using these corrected RGBD frames.
NOTE: Only validation set annotations are included. Test annotations are managed by the BOP challenge."	https://paperswithcode.com/dataset/hope-image	11/03/2022	Household Objects for Pose Estimation					
5700	HOPE-Video	"The HOPE-Video dataset contains 10 video sequences (2038 frames) with 5-20 objects on a tabletop scene captured by a robot arm-mounted RealSense D415 RGBD camera. In each sequence, the camera is moved to capture multiple views of a set of objects in the robotic workspace. First COLMAP was applied to refine the camera poses (keyframes at 6~fps) provided by forward kinematics and RGB calibration from RealSense to Baxter's wrist camera. 3D dense point cloud was then generated via CascadeStereo (included for each sequence in 'scene.ply'). Ground truth poses for the HOPE objects models in the world coordinate system were annotated manually using the CascadeStereo point clouds. The following are provided for each frame:
Camera intrinsics/extrinsics
RGB images of 640x480
Depth images of 640x480
3D scene reconstruction from CascadeStereo
*Object pose annotation in the camera frame
Objects consist of a set of 28 toy grocery items selected for compatibility with robot manipulation and widespread availability. Textured models were generated by an EinScan-SE 3D Scanner, units were converted to centimeters, and the centers/rotations of the meshes were aligned to a canonical pose."	https://paperswithcode.com/dataset/hope-video	11/03/2022	Household Objects for Pose Estimation					
5701	Heritage Health Prize	"Heritage Provider Network is providing Competition Entrants with deidentified member data collected during a forty-eight month period that is allocated among three data sets (the ""Data Sets""). Competition Entrants will use the Data Sets to develop and test their algorithms for accurately predicting the number of days that the members will spend in a hospital (inpatient or emergency room visit) during the 12-month period following the Data Set cut-off date.
HHP_release3.zip contains the latest files, so you can ignore HHP_release2.zip. SampleEntry.CSV shows you how an entry should look.
Data Sets will be released to Entrants after registration on the Website according to the following schedule:
April 4, 2011   Claims Table - Y1 and DaysInHospital Table - Y2
May 4, 2011 All other Data Sets except Labs Table and Rx Table
June 4, 2011    Labs Table and Rx Table
Entrants are welcome to use other data to develop and test their algorithms and entries until 11:59:59 UTC on April 4, 2012 if the data are (i) freely available to all other Entrants and (i) published (or a link provided) to the data in the External Data portion of the Forum within one (1) week of an entry submission using the other data. Entrants may not use any data other than the Data Sets after 11:59:59 UTC on April 4, 2012 without prior approval.
Tables
Each of the Data Sets will be comprised of tables as follows:
a. Members Table, which will include:
i. MemberID (a unique member ID)
ii. AgeAtFirstClaim (member's age when first claim was made in the Data Set period)
iii. Sex
b. Claims Table, which will include:
i. MemberID
ii. ProviderID (the ID of the doctor or specialist providing the service)
iii. Vendor (the company that issues the bill)
iv. PCP (member's primary care physician)
v. Year (the year of the claim, Y1, Y2, Y3)
vi. Specialty
vii. PlaceSvc (place where the member was treated)
viii. PayDelay (the delay between the claim and the day the claim was paid for)
ix. LengthOfStay
x. DSFS (days since first service that year)
xi. PrimaryConditionGroup (a generalization of the primary diagnosis codes)
xii. CharlsonIndex (a generalization of the diagnosis codes in the form of a categorized comorbidity score)
xiii. ProcedureGroup (a generalization of the CPT code or treatment code)
xiv. SupLOS (a flag that indicates if LengthOfStay is null because it has been suppressed)
c. Labs Table, which will contain certain details of lab tests provided to members.
d. RX Table, which will contain certain details of prescriptions filled by members.
e. DaysInHospital Tables - Y2 and Y3, which will contain the number of days of hospitalization for each eligible member during Y2 and Y3 and will include:
i. MemberID;
ii. ClaimsTruncated (a flag for members who have had claims suppressed. If the flag is 1 for member xxx in DaysInHospital_Y2, some claims for member xxx will have been suppressed in Y1).
iii. DaysInHospital (the number of days in hospital Y2 or Y3, as applicable).
These two Tables are intended for use by Entrants to train and validate their algorithms. DaysInHospital Tables are based on the Claims Table with admissions in Y2 or Y3, as applicable. As a privacy measure, any member who spent more than two weeks in hospital is grouped; they are treated as though they spent 15 days in hospital.
f. Target - is ""DaysInHospital_Y4"" but doesn't include DaysInHospital. DaysInHospital data for Y4 are to be filled in by Entrants to produce entries. Seem SampleEntry.csv as an example."	https://paperswithcode.com/dataset/heritage-health-prize							
5702	Chest x-ray landmark dataset	Set of landmark annotations for JSRT, Montgomery, Shenzhen and a subset of Padchest datasets	https://paperswithcode.com/dataset/chest-x-ray-landmark-dataset	21/03/2022						
5703	Montgomery County X-ray Set	X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health andHuman Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays areabnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities,including effusions and miliary patterns. The data set includes radiology readings available as a text files and summary of its content	https://paperswithcode.com/dataset/montgomery-county-x-ray-set							
5704	Shenzhen Hospital X-ray Set	X-ray images in this data set have been collected by Shenzhen No.3 Hospital in Shenzhen, Guangdong providence,China. The x-rays were acquired as part of the routine care at Shenzhen Hospital. The set contains images in JPEG format. There are 326 normal x-raysand 336 abnormal x-rays showing various manifestations of tuberculosis.	https://paperswithcode.com/dataset/shenzhen-hospital-x-ray-set							
5705	ARQMath2 - Task 1	"The goal of ARQMath is to advance techniques for mathematical information retrieval, in particular, retrieving answers to mathematical questions (Task 1), and formula retrieval (Task 2).
Using the question posts from Math Stack Exchange, participating systems are given a question or a formula from a question and asked to return a ranked list of either potential answers to the question or potentially useful formulae (in the case of a formula query). Relevance is determined by the expected utility of each returned item. These tasks allow participating teams to explore leveraging math notation together with text to improve the quality of retrieval results."	https://paperswithcode.com/dataset/arqmath2		The second Answer Retrieval for Questions on Math lab - Task 1					
5706	ACDC (Adverse Conditions Dataset with Correspondences)	"We introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images which are evenly distributed between fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content.
ACDC supports two tasks:
1. standard semantic segmentation
2. uncertainty-aware semantic segmentation"	https://paperswithcode.com/dataset/acdc-adverse-conditions-dataset-with		Adverse Conditions Dataset with Correspondences					
5707	RR	Review-Rebuttal (RR) dataset is introduced to facilitate the study of argument pair extraction in the peer review and rebuttal domain.	https://paperswithcode.com/dataset/rr	01/11/2020	Review-Rebuttal					
5708	XYSquares	"Synthetic dataset intended for benchmarking disentanglement frameworks.
XYSquares is adversarial in nature; the distance between any two observations in the dataset is constant when measured using a pixel-wise distance function. It is usually impossible for VAE frameworks that use pixel-wise losses to disentangle this dataset.
The dataset is constructed from 3 non-overlapping red, green and blue squares that are each $8\times8$ pixels in size. Each square can move along the $x$ and $y$ coordinates of an $8\times8$ grid. The resulting images are $64\times64$ pixels in size. With this construction the dataset has a total of 8 ground-truth factors for a total of $8^6 = 262144$ observations."	https://paperswithcode.com/dataset/xysquares	01/01/2022						
5709	OpenLane	OpenLane is the first real-world and the largest scaled 3D lane dataset to date. The dataset collects valuable contents from public perception dataset Waymo Open Dataset and provides lane&closest-in-path object(CIPO) annotation for 1000 segments. In short, OpenLane owns 200K frames and over 880K carefully annotated lanes. The OpenLane Dataset is publicly released to aid the research community in making advancements in 3D perception and autonomous driving technology.	https://paperswithcode.com/dataset/openlane	21/03/2022						
5710	3D Cars	"Car CAD models from ""3d object detection and viewpoint estimation with a deformable
3d cuboid model"" were used to generate the dataset. For each of the 199 car models, the authors generated $64\times64$ color renderings from 24 rotation angles each offset by 15 degrees, as well as from 4 different camera elevations."	https://paperswithcode.com/dataset/3d-cars	01/12/2015						
5711	Physical Audiovisual CommonSense	PACS (Physical Audiovisual CommonSense) is the first audiovisual benchmark annotated for physical commonsense attributes. PACS contains a total of 13,400 question-answer pairs, involving 1,377 unique physical commonsense questions and 1,526 videos. The dataset provides new opportunities to advance the research field of physical reasoning by bringing audio as a core component of this multimodal problem.	https://paperswithcode.com/dataset/pacs-commonsense	21/03/2022						
5712	EgoMon	"EgoMon Gaze & Video Dataset is an Egocentric (first person) Dataset that consists of 7 videos of 30 minutes, more or less, each one of them.
- 7 videos with the gaze information plotted on them.
- The same videos (without the gaze information plotted on them).
- A total of 13428 images, more or less, that corresponds to each frame per second of all these videos.
- 7 text files with the gaze data extracted from each video."	https://paperswithcode.com/dataset/egomon	28/08/2018	Egomon Gaze & Video dataset					
5713	ChangeIt	ChangeIt dataset with more than 2600 hours of video with state-changing actions published at CVPR 2022.	https://paperswithcode.com/dataset/changeit	22/03/2022						
5714	SynLiDAR	SynLiDAR is a large-scale synthetic LiDAR sequential point cloud dataset with point-wise annotations. 13 sequences of LiDAR point cloud with around 20k scans (over 19 billion points and 32 semantic classes) are collected from virtual urban cities, suburban towns, neighborhood, and harbor.	https://paperswithcode.com/dataset/synlidar							
5715	CUHK-SYSU-TBPS	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/cuhk-sysu-tbps	27/09/2021						
5716	PRW-TBPS	"Click to add a brief description of the dataset (Markdown and LaTeX enabled).
Provide:

a high-level explanation of the dataset characteristics
explain motivations and summary of its content
potential use cases of the dataset"	https://paperswithcode.com/dataset/prw-tbps	27/09/2021						
5717	VinDr-CXR	"VinDr-CXR is an open large-scale dataset of chest X-rays with radiologist’s annotations. It's bult from more than 100,000 raw images in DICOM format that were retrospectively collected from the Hospital 108 and the Hanoi Medical University Hospital, two of the largest hospitals in Vietnam. The published dataset consists of 18,000 postero-anterior (PA) view CXR scans that come with both the localization of critical findings and the classification of common thoracic diseases. These images were annotated by a group of 17 radiologists with at least 8 years of experience for the presence of 22 critical findings (local labels) and 6 diagnoses (global labels); each finding is localized with a bounding box. The local and global labels correspond to the “Findings” and “Impressions” sections, respectively, of a standard radiology report.  
The dataset is divided into two parts: the training set of 15,000 scans and the test set of 3,000 scans. Each image in the training set was independently labeled by 3 radiologists, while the annotation of each image in the test set was even more carefully treated and obtained from the consensus of 5 radiologists.
Description adopted from here."	https://paperswithcode.com/dataset/vindr-cx	20/03/2022						
5718	VinDr-PCXR	VinDr-PCXR is an open, large-scale pediatric chest X-ray dataset for interpretation of common thoracic diseases in children. The dataset contains 9,125 CXR scans retrospectively collected from a major pediatric hospital in Vietnam between 2020 and 2021. Each scan was manually annotated by a pediatric radiologist who has more than ten years of experience. The dataset was labeled for the presence of 36 critical findings and 15 diseases. It aims to aid research in the detection of multiple findings and diseases.	https://paperswithcode.com/dataset/vindr-pcxr	20/03/2022						
