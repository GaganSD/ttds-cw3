title,abstract,id
"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped
  Attention","  Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by the global
self-attention, various methods constrain the range of attention within a local
region to improve its efficiency. Consequently, their receptive fields in a
single attention layer are not large enough, resulting in insufficient context
modeling. To address this issue, we propose a Pale-Shaped self-Attention
(PS-Attention), which performs self-attention within a pale-shaped region.
Compared to the global self-attention, PS-Attention can reduce the computation
and memory costs significantly. Meanwhile, it can capture richer contextual
information under the similar computation complexity with previous local
self-attention mechanisms. Based on the PS-Attention, we develop a general
Vision Transformer backbone with a hierarchical architecture, named Pale
Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the
model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K
classification, outperforming the previous Vision Transformer backbones. For
downstream tasks, our Pale Transformer backbone performs better than the recent
state-of-the-art CSWin Transformer by a large margin on ADE20K semantic
segmentation and COCO object detection & instance segmentation. The code will
be released on https://github.com/BR-IDL/PaddleViT.
",a
"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped
  Attention","  Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by the global
self-attention, various methods constrain the range of attention within a local
region to improve its efficiency. Consequently, their receptive fields in a
single attention layer are not large enough, resulting in insufficient context
modeling. To address this issue, we propose a Pale-Shaped self-Attention
(PS-Attention), which performs self-attention within a pale-shaped region.
Compared to the global self-attention, PS-Attention can reduce the computation
and memory costs significantly. Meanwhile, it can capture richer contextual
information under the similar computation complexity with previous local
self-attention mechanisms. Based on the PS-Attention, we develop a general
Vision Transformer backbone with a hierarchical architecture, named Pale
Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the
model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K
classification, outperforming the previous Vision Transformer backbones. For
downstream tasks, our Pale Transformer backbone performs better than the recent
state-of-the-art CSWin Transformer by a large margin on ADE20K semantic
segmentation and COCO object detection & instance segmentation. The code will
be released on https://github.com/BR-IDL/PaddleViT.
",a
"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped
  Attention","  Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by the global
self-attention, various methods constrain the range of attention within a local
region to improve its efficiency. Consequently, their receptive fields in a
single attention layer are not large enough, resulting in insufficient context
modeling. To address this issue, we propose a Pale-Shaped self-Attention
(PS-Attention), which performs self-attention within a pale-shaped region.
Compared to the global self-attention, PS-Attention can reduce the computation
and memory costs significantly. Meanwhile, it can capture richer contextual
information under the similar computation complexity with previous local
self-attention mechanisms. Based on the PS-Attention, we develop a general
Vision Transformer backbone with a hierarchical architecture, named Pale
Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the
model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K
classification, outperforming the previous Vision Transformer backbones. For
downstream tasks, our Pale Transformer backbone performs better than the recent
state-of-the-art CSWin Transformer by a large margin on ADE20K semantic
segmentation and COCO object detection & instance segmentation. The code will
be released on https://github.com/BR-IDL/PaddleViT.
",a
"Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped
  Attention","  Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by the global
self-attention, various methods constrain the range of attention within a local
region to improve its efficiency. Consequently, their receptive fields in a
single attention layer are not large enough, resulting in insufficient context
modeling. To address this issue, we propose a Pale-Shaped self-Attention
(PS-Attention), which performs self-attention within a pale-shaped region.
Compared to the global self-attention, PS-Attention can reduce the computation
and memory costs significantly. Meanwhile, it can capture richer contextual
information under the similar computation complexity with previous local
self-attention mechanisms. Based on the PS-Attention, we develop a general
Vision Transformer backbone with a hierarchical architecture, named Pale
Transformer, which achieves 83.4%, 84.3%, and 84.9% Top-1 accuracy with the
model size of 22M, 48M, and 85M respectively for 224 ImageNet-1K
classification, outperforming the previous Vision Transformer backbones. For
downstream tasks, our Pale Transformer backbone performs better than the recent
state-of-the-art CSWin Transformer by a large margin on ADE20K semantic
segmentation and COCO object detection & instance segmentation. The code will
be released on https://github.com/BR-IDL/PaddleViT.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Vision Transformers with Patch Diversification,"  Vision transformer has demonstrated promising performance on challenging
computer vision tasks. However, directly training the vision transformers may
yield unstable and sub-optimal results. Recent works propose to improve the
performance of the vision transformers by modifying the transformer structures,
e.g., incorporating convolution layers. In contrast, we investigate an
orthogonal approach to stabilize the vision transformer training without
modifying the networks. We observe the instability of the training can be
attributed to the significant similarity across the extracted patch
representations. More specifically, for deep vision transformers, the
self-attention blocks tend to map different patches into similar latent
representations, yielding information loss and performance degradation. To
alleviate this problem, in this work, we introduce novel loss functions in
vision transformer training to explicitly encourage diversity across patch
representations for more discriminative feature extraction. We empirically show
that our proposed techniques stabilize the training and allow us to train wider
and deeper vision transformers. We further show the diversified features
significantly benefit the downstream tasks in transfer learning. For semantic
segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and
ADE20k. Our code is available at
https://github.com/ChengyueGongR/PatchVisionTransformer.
",a
Transformers in Vision: A Survey,"  Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
",a
Transformers in Vision: A Survey,"  Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
",a
Transformers in Vision: A Survey,"  Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
",a
Transformers in Vision: A Survey,"  Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
",a
CvT: Introducing Convolutions to Vision Transformers,"  We present in this paper a new architecture, named Convolutional vision
Transformer (CvT), that improves Vision Transformer (ViT) in performance and
efficiency by introducing convolutions into ViT to yield the best of both
designs. This is accomplished through two primary modifications: a hierarchy of
Transformers containing a new convolutional token embedding, and a
convolutional Transformer block leveraging a convolutional projection. These
changes introduce desirable properties of convolutional neural networks (CNNs)
to the ViT architecture (\ie shift, scale, and distortion invariance) while
maintaining the merits of Transformers (\ie dynamic attention, global context,
and better generalization). We validate CvT by conducting extensive
experiments, showing that this approach achieves state-of-the-art performance
over other Vision Transformers and ResNets on ImageNet-1k, with fewer
parameters and lower FLOPs. In addition, performance gains are maintained when
pretrained on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream
tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of
87.7\% on the ImageNet-1k val set. Finally, our results show that the
positional encoding, a crucial component in existing Vision Transformers, can
be safely removed in our model, simplifying the design for higher resolution
vision tasks. Code will be released at \url{https://github.com/leoxiaobin/CvT}.
",a
CvT: Introducing Convolutions to Vision Transformers,"  We present in this paper a new architecture, named Convolutional vision
Transformer (CvT), that improves Vision Transformer (ViT) in performance and
efficiency by introducing convolutions into ViT to yield the best of both
designs. This is accomplished through two primary modifications: a hierarchy of
Transformers containing a new convolutional token embedding, and a
convolutional Transformer block leveraging a convolutional projection. These
changes introduce desirable properties of convolutional neural networks (CNNs)
to the ViT architecture (\ie shift, scale, and distortion invariance) while
maintaining the merits of Transformers (\ie dynamic attention, global context,
and better generalization). We validate CvT by conducting extensive
experiments, showing that this approach achieves state-of-the-art performance
over other Vision Transformers and ResNets on ImageNet-1k, with fewer
parameters and lower FLOPs. In addition, performance gains are maintained when
pretrained on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream
tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of
87.7\% on the ImageNet-1k val set. Finally, our results show that the
positional encoding, a crucial component in existing Vision Transformers, can
be safely removed in our model, simplifying the design for higher resolution
vision tasks. Code will be released at \url{https://github.com/leoxiaobin/CvT}.
",a
CvT: Introducing Convolutions to Vision Transformers,"  We present in this paper a new architecture, named Convolutional vision
Transformer (CvT), that improves Vision Transformer (ViT) in performance and
efficiency by introducing convolutions into ViT to yield the best of both
designs. This is accomplished through two primary modifications: a hierarchy of
Transformers containing a new convolutional token embedding, and a
convolutional Transformer block leveraging a convolutional projection. These
changes introduce desirable properties of convolutional neural networks (CNNs)
to the ViT architecture (\ie shift, scale, and distortion invariance) while
maintaining the merits of Transformers (\ie dynamic attention, global context,
and better generalization). We validate CvT by conducting extensive
experiments, showing that this approach achieves state-of-the-art performance
over other Vision Transformers and ResNets on ImageNet-1k, with fewer
parameters and lower FLOPs. In addition, performance gains are maintained when
pretrained on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream
tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of
87.7\% on the ImageNet-1k val set. Finally, our results show that the
positional encoding, a crucial component in existing Vision Transformers, can
be safely removed in our model, simplifying the design for higher resolution
vision tasks. Code will be released at \url{https://github.com/leoxiaobin/CvT}.
",a
