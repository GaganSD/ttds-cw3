{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfVIh8zaIr4y",
    "outputId": "a19e99f1-b9c5-4057-bb96-4fb2ca1e2c38",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-22 22:59:02--  https://production-media.paperswithcode.com/about/datasets.json.gz\n",
      "production-media.paperswithcode.com (production-media.paperswithcode.com) をDNSに問いあわせています... 172.67.73.69, 104.26.13.155, 104.26.12.155\n",
      "production-media.paperswithcode.com (production-media.paperswithcode.com)|172.67.73.69|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 1840511 (1.8M) [binary/octet-stream]\n",
      "`datasets.json.gz' に保存中\n",
      "\n",
      "datasets.json.gz    100%[===================>]   1.75M   878KB/s 時間 2.0s       \n",
      "\n",
      "2022-03-22 22:59:05 (878 KB/s) - `datasets.json.gz' へ保存完了 [1840511/1840511]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://production-media.paperswithcode.com/about/datasets.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edinburgh_Zomato.html           kaggle_dataset_collect.ipynb\r\n",
      "PaperswithCode_dataset.ipynb    kaggle_dataset_df_page5.csv\r\n",
      "PaperswithCode_old.ipynb        kaggle_dataset_df_page500.csv\r\n",
      "TTDS_CW3_Report_Final.pdf       kaggle_datasets_collect.py\r\n",
      "Untitled.ipynb                  last.pickle\r\n",
      "Untitled2.ipynb                 papers-with-abstracts.json.gz\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                     paperwithcode_df.csv\r\n",
      "bm_25_result_df_covid_2021.csv  preprocessing.py\r\n",
      "bm_25_result_df_review_food.csv ranking.py\r\n",
      "check_ir_result.ipynb           ranking_paper_new.py\r\n",
      "dataset-metadata.json           result.log\r\n",
      "datasets.json                   tfidf_result_df_covid_2021.csv\r\n",
      "datasets.json.gz                tfidf_result_df_review_food.csv\r\n",
      "index_generator.py              \u001b[34mttds-cw3\u001b[m\u001b[m\r\n",
      "kaggle.json                     web_scraping_tripadvisor.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GyxKthd0Ixzw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: datasets.json: unknown suffix -- ignored\n",
      "datasets.json already exists -- do you wish to overwrite (y or n)? ^C\n"
     ]
    }
   ],
   "source": [
    "!gzip -d datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edinburgh_Zomato.html           kaggle_dataset_collect.ipynb\r\n",
      "PaperswithCode_dataset.ipynb    kaggle_dataset_df_page5.csv\r\n",
      "PaperswithCode_old.ipynb        kaggle_dataset_df_page500.csv\r\n",
      "TTDS_CW3_Report_Final.pdf       kaggle_datasets_collect.py\r\n",
      "Untitled.ipynb                  last.pickle\r\n",
      "Untitled2.ipynb                 papers-with-abstracts.json.gz\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                     paperwithcode_df.csv\r\n",
      "bm_25_result_df_covid_2021.csv  preprocessing.py\r\n",
      "bm_25_result_df_review_food.csv ranking.py\r\n",
      "check_ir_result.ipynb           ranking_paper_new.py\r\n",
      "dataset-metadata.json           result.log\r\n",
      "datasets.json                   tfidf_result_df_covid_2021.csv\r\n",
      "datasets.json.gz                tfidf_result_df_review_food.csv\r\n",
      "index_generator.py              \u001b[34mttds-cw3\u001b[m\u001b[m\r\n",
      "kaggle.json                     web_scraping_tripadvisor.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rWuoqpHUI63p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://paperswithcode.com/dataset/mnist',\n",
       "  'name': 'MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://yann.lecun.com/exdb/mnist/',\n",
       "  'description': 'The **MNIST** database (**Modified National Institute of Standards and Technology** database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\\r\\n\\r\\nSource: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)\\r\\nImage Source: [https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamples.png)',\n",
       "  'paper': {'title': 'Gradient-based learning applied to document recognition',\n",
       "   'url': 'http://arxiv.org/pdf/1102.0183.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-recognition'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'},\n",
       "   {'task': 'Token Classification',\n",
       "    'url': 'https://paperswithcode.com/task/token-classification'},\n",
       "   {'task': 'Sequence-to-sequence Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/sequence-to-sequence-language-modeling'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Core set discovery',\n",
       "    'url': 'https://paperswithcode.com/task/core-set-discovery'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Clustering Algorithms Evaluation',\n",
       "    'url': 'https://paperswithcode.com/task/clustering-algorithms-evaluation'},\n",
       "   {'task': 'General Classification',\n",
       "    'url': 'https://paperswithcode.com/task/classification'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'},\n",
       "   {'task': 'Adversarial Defense',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-defense'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'},\n",
       "   {'task': 'Network Pruning',\n",
       "    'url': 'https://paperswithcode.com/task/network-pruning'},\n",
       "   {'task': 'Classification with Binary Weight Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-weight-network'},\n",
       "   {'task': 'Sequential Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sequential-image-classification'},\n",
       "   {'task': 'Continuously Indexed Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/continuously-indexed-domain-adaptation'},\n",
       "   {'task': 'Unsupervised Image-To-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-to-image-translation'},\n",
       "   {'task': 'Intent Classification',\n",
       "    'url': 'https://paperswithcode.com/task/intent-classification'},\n",
       "   {'task': 'Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/summarization'},\n",
       "   {'task': 'Hard-label Attack',\n",
       "    'url': 'https://paperswithcode.com/task/hard-label-attack'},\n",
       "   {'task': 'Superpixel Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/superpixel-image-classification'},\n",
       "   {'task': 'Structured Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/structured-prediction'},\n",
       "   {'task': 'One-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-learning'},\n",
       "   {'task': 'Handwritten Digit Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/handwritten-digit-recognition'},\n",
       "   {'task': 'Unsupervised MNIST',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-mnist'},\n",
       "   {'task': 'Rotated MNIST',\n",
       "    'url': 'https://paperswithcode.com/task/rotated-mnist'},\n",
       "   {'task': 'Multi Label Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-text-classification-1'},\n",
       "   {'task': 'NER', 'url': 'https://paperswithcode.com/task/cg'},\n",
       "   {'task': 'POS', 'url': 'https://paperswithcode.com/task/pos'},\n",
       "   {'task': 'SENTER', 'url': 'https://paperswithcode.com/task/senter'},\n",
       "   {'task': 'Iloko Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/iloko-speech-recognition'},\n",
       "   {'task': 'TAG', 'url': 'https://paperswithcode.com/task/tag'},\n",
       "   {'task': 'AbbreviationDetection',\n",
       "    'url': 'https://paperswithcode.com/task/abbreviationdetection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['USPS-to-MNIST',\n",
       "   'MNIST-to-USPS',\n",
       "   'Rotating MNIST',\n",
       "   'Noisy MNIST (Motion)',\n",
       "   'Noisy MNIST (Contrast)',\n",
       "   'Noisy MNIST (AWGN)',\n",
       "   'MNIST (Conditional)',\n",
       "   'Indexed Rotating MNIST',\n",
       "   'Rotated MNIST',\n",
       "   'Moving MNIST',\n",
       "   'Sequential MNIST',\n",
       "   'SVNH-to-MNIST',\n",
       "   'MNIST-test',\n",
       "   'MNIST-full',\n",
       "   'MNIST',\n",
       "   '75 Superpixel MNIST'],\n",
       "  'num_papers': 5049,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/mnist',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.MNIST',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/mnist',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/mnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://graphneural.network/datasets/#mnist',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmclassification',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'repo': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/celeba',\n",
       "  'name': 'CelebA',\n",
       "  'full_name': 'CelebFaces Attributes Dataset',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html',\n",
       "  'description': 'CelebFaces Attributes dataset contains 202,599 face images of the size 178×218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age.\\r\\n\\r\\nSource: [Show, Attend and Translate: Unpaired Multi-Domain Image-to-Image Translation with Visual Attention](https://arxiv.org/abs/1811.07483)\\r\\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)',\n",
       "  'paper': {'title': 'Deep Learning Face Attributes in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-learning-face-attributes-in-the-wild'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Unsupervised Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-facial-landmark-detection'},\n",
       "   {'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Facial Expression Translation',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-translation'},\n",
       "   {'task': 'HairColor/Unbiased',\n",
       "    'url': 'https://paperswithcode.com/task/haircolor-unbiased'},\n",
       "   {'task': 'HairColor/Bias-conflicting',\n",
       "    'url': 'https://paperswithcode.com/task/haircolor-bias-conflicting'},\n",
       "   {'task': 'HeavyMakeup/Unbiased',\n",
       "    'url': 'https://paperswithcode.com/task/heavymakeup-unbiased'},\n",
       "   {'task': 'HeavyMakeup/Bias-conflicting',\n",
       "    'url': 'https://paperswithcode.com/task/heavymakeup-bias-conflicting'},\n",
       "   {'task': 'Physical Attribute Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/physical-attribute-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CelebA Aligned',\n",
       "   'CelebA 64x64',\n",
       "   'CelebA 256x256',\n",
       "   'CelebA 128 x 128',\n",
       "   'CelebA + AFLW Unaligned',\n",
       "   'CelebA 128x128',\n",
       "   'CelebA'],\n",
       "  'num_papers': 1924,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CelebA',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/celeb_a',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/jft-300m',\n",
       "  'name': 'JFT-300M',\n",
       "  'full_name': 'JFT-300M',\n",
       "  'homepage': '',\n",
       "  'description': '**JFT-300M** is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images.',\n",
       "  'paper': {'title': 'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era',\n",
       "   'url': 'https://paperswithcode.com/paper/revisiting-unreasonable-effectiveness-of-data'},\n",
       "  'introduced_date': '2017-07-10',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JFT-300M'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/glue',\n",
       "  'name': 'GLUE',\n",
       "  'full_name': 'General Language Understanding Evaluation benchmark',\n",
       "  'homepage': 'https://gluebenchmark.com/',\n",
       "  'description': 'General Language Understanding Evaluation (**GLUE**) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.\\r\\n\\r\\nSource: [Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models](https://arxiv.org/abs/1908.06725)\\r\\nImage Source: [https://gluebenchmark.com/](https://gluebenchmark.com/)',\n",
       "  'paper': {'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/glue-a-multi-task-benchmark-and-analysis'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'},\n",
       "   {'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Paraphrase Identification',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification'},\n",
       "   {'task': 'Natural Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-understanding'},\n",
       "   {'task': 'Community Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/community-question-answering'},\n",
       "   {'task': 'Paraphrase Generation',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-generation'},\n",
       "   {'task': 'Linguistic Acceptability',\n",
       "    'url': 'https://paperswithcode.com/task/linguistic-acceptability'},\n",
       "   {'task': 'Paraphrase Identification within Bi-Encoder',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification-within-bi-encoder'},\n",
       "   {'task': 'Semantic Textual Similarity within Bi-Encoder',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity-within-bi-encoder'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['GLUE STSB',\n",
       "   'GLUE SST2',\n",
       "   'GLUE RTE',\n",
       "   'GLUE QQP',\n",
       "   'GLUE QNLI',\n",
       "   'GLUE MNLI',\n",
       "   'GLUE COLA',\n",
       "   'GLUE WNLI',\n",
       "   'GLUE MRPC',\n",
       "   'GLUE',\n",
       "   'RTE',\n",
       "   'WNLI',\n",
       "   'QNLI',\n",
       "   'MultiNLI',\n",
       "   'Quora Question Pairs',\n",
       "   'STS Benchmark',\n",
       "   'MRPC',\n",
       "   'SST-2 Binary classification',\n",
       "   'CoLA'],\n",
       "  'num_papers': 1242,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/glue',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/gsarti/change_it',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#glue',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/glue',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/multinli',\n",
       "  'name': 'MultiNLI',\n",
       "  'full_name': 'Multi-Genre Natural Language Inference',\n",
       "  'homepage': 'https://cims.nyu.edu/~sbowman/multinli/',\n",
       "  'description': 'The **Multi-Genre Natural Language Inference** (**MultiNLI**) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like [SNLI](/dataset/snli). MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time.\\r\\n\\r\\nSource: [Semantic Sentence Matching with Densely-connectedRecurrent and Co-attentive Information](https://arxiv.org/abs/1805.11360)',\n",
       "  'paper': {'title': 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference',\n",
       "   'url': 'https://paperswithcode.com/paper/a-broad-coverage-challenge-corpus-for'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MultiNLI', 'MultiNLI Dev'],\n",
       "  'num_papers': 311,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/multi_nli',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/multi_nli_mismatch',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#multinli',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/multi_nli',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet',\n",
       "  'name': 'ImageNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://image-net.org/index.php',\n",
       "  'description': 'The **ImageNet** dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection.\\r\\nThe publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld.\\r\\nILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., “there are cars in this image” but “there are no tigers,” and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., “there is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels”.\\r\\nThe ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided.\\r\\n\\r\\n* Total number of non-empty WordNet synsets: 21841\\r\\n* Total number of images: 14197122\\r\\n* Number of images with bounding box annotations: 1,034,908\\r\\n* Number of synsets with SIFT features: 1000\\r\\n* Number of images with SIFT features: 1.2 million\\r\\n\\r\\nSource: [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\\r\\nImage Source: [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)',\n",
       "  'paper': {'title': 'ImageNet: A large-scale hierarchical image database',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2009.5206848'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Incremental Learning',\n",
       "    'url': 'https://paperswithcode.com/task/incremental-learning'},\n",
       "   {'task': 'Image Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/image-recognition'},\n",
       "   {'task': 'Small Data Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/small-data'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Adversarial Defense',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-defense'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'},\n",
       "   {'task': 'Zero-Shot Transfer Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-transfer-image-classification'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'},\n",
       "   {'task': 'Conditional Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/conditional-image-generation'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'},\n",
       "   {'task': 'Adversarial Robustness',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-robustness'},\n",
       "   {'task': 'Weakly-Supervised Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-localization'},\n",
       "   {'task': 'Partial Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/partial-domain-adaptation'},\n",
       "   {'task': 'Network Pruning',\n",
       "    'url': 'https://paperswithcode.com/task/network-pruning'},\n",
       "   {'task': 'Classification with Binary Weight Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-weight-network'},\n",
       "   {'task': 'Zero-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-object-detection'},\n",
       "   {'task': 'Quantization',\n",
       "    'url': 'https://paperswithcode.com/task/quantization'},\n",
       "   {'task': 'Sparse Learning',\n",
       "    'url': 'https://paperswithcode.com/task/sparse-learning'},\n",
       "   {'task': 'Binarization',\n",
       "    'url': 'https://paperswithcode.com/task/binarization'},\n",
       "   {'task': 'Knowledge Distillation',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-distillation'},\n",
       "   {'task': 'Classification with Binary Neural Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-neural-network'},\n",
       "   {'task': 'Unconditional Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/unconditional-image-generation'},\n",
       "   {'task': 'Neural Network Compression',\n",
       "    'url': 'https://paperswithcode.com/task/neural-network-compression'},\n",
       "   {'task': 'Self-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-image-classification'},\n",
       "   {'task': 'Robust classification',\n",
       "    'url': 'https://paperswithcode.com/task/robust-classification'},\n",
       "   {'task': 'Parameter Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/parameter-prediction'},\n",
       "   {'task': 'Classification Consistency',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification-shift-consistency'},\n",
       "   {'task': 'Biologically-plausible Training',\n",
       "    'url': 'https://paperswithcode.com/task/biologically-plausible-training'},\n",
       "   {'task': 'JPEG Decompression',\n",
       "    'url': 'https://paperswithcode.com/task/jpeg-decompression'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ImageNet V2',\n",
       "   'ImageNet 50 samples per class',\n",
       "   'ImageNet 512x512',\n",
       "   'ImageNet - 500 classes + 10 steps of 50 classes',\n",
       "   'ImageNet Detection',\n",
       "   'ImageNet-Caltech',\n",
       "   'ImageNet-50 (5 tasks) ',\n",
       "   'ImageNet-100',\n",
       "   'ImageNet ResNet-50 - 90 Epochs',\n",
       "   'ImageNet ResNet-50 - 60 Epochs',\n",
       "   'ImageNet ResNet-50 - 50 Epochs',\n",
       "   'ImageNet - ResNet 50 - 90% sparsity',\n",
       "   'ImageNet64x64',\n",
       "   'ImageNet 64x64',\n",
       "   'ImageNet - 500 classes + 50 steps of 10 classes',\n",
       "   'ImageNet - 500 classes + 5 steps of 100 classes',\n",
       "   'ImageNet-100 - 50 classes + 50 steps of 1 class',\n",
       "   'ImageNet-100 - 50 classes + 5 steps of 10 classes',\n",
       "   'ImageNet-100 - 50 classes + 25 steps of 2 classes',\n",
       "   'ImageNet-100 - 50 classes + 10 steps of 5 classes',\n",
       "   'NAS-Bench-201, ImageNet-16-120',\n",
       "   'Imagenet-dog-15',\n",
       "   'ImageNet-R',\n",
       "   'ImageNet-LT',\n",
       "   'ImageNet-A',\n",
       "   'ImageNet-10',\n",
       "   'ImageNet ReaL',\n",
       "   'ImageNet 32x32',\n",
       "   'ImageNet 128x128',\n",
       "   'ImageNet - 10% labeled data',\n",
       "   'ImageNet - 1% labeled data',\n",
       "   'ImageNet - 0-Shot',\n",
       "   'ImageNet (targeted PGD, max perturbation=16)',\n",
       "   'ImageNet (non-targeted PGD, max perturbation=4)',\n",
       "   'ImageNet (Fine-grained 6 Tasks)',\n",
       "   'ImageNet',\n",
       "   'ILSVRC 2016',\n",
       "   'ILSVRC 2015'],\n",
       "  'num_papers': 7779,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.ImageNet',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/imagenet-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/imagenet2012',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmclassification',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/penn-treebank',\n",
       "  'name': 'Penn Treebank',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html',\n",
       "  'description': 'The English **Penn Treebank** (**PTB**) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).\\r\\nThe corpus is also commonly used for character-level and word-level Language Modelling.\\r\\n\\r\\nSource: [Seq2Biseq: Bidirectional Output-wise Recurrent Neural Networks for Sequence Modelling](https://arxiv.org/abs/1904.04733)\\r\\nImage Source: [https://dl.acm.org/doi/10.5555/972470.972475](https://dl.acm.org/doi/10.5555/972470.972475)',\n",
       "  'paper': {'title': 'Building a Large Annotated Corpus of English: The Penn Treebank',\n",
       "   'url': 'http://dl.acm.org/citation.cfm?id=972470.972475'},\n",
       "  'introduced_date': '1993-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Dependency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/dependency-parsing'},\n",
       "   {'task': 'Part-Of-Speech Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/part-of-speech-tagging'},\n",
       "   {'task': 'Open Information Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/open-information-extraction'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Chunking', 'url': 'https://paperswithcode.com/task/chunking'},\n",
       "   {'task': 'Constituency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/constituency-parsing'},\n",
       "   {'task': 'Missing Elements',\n",
       "    'url': 'https://paperswithcode.com/task/missing-elements'},\n",
       "   {'task': 'Myocardial infarction detection',\n",
       "    'url': 'https://paperswithcode.com/task/myocardial-infarction-detection'},\n",
       "   {'task': 'Constituency Grammar Induction',\n",
       "    'url': 'https://paperswithcode.com/task/constituency-grammar-induction'},\n",
       "   {'task': 'Unsupervised Dependency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-dependency-parsing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PTB dataset, ECG lead II',\n",
       "   'PTB',\n",
       "   'Penn Treebank (Character Level) 3x1000 LSTM - 500 Epochs',\n",
       "   'Penn Treebank (Word Level)',\n",
       "   'Penn Treebank (Character Level)',\n",
       "   'Penn Treebank'],\n",
       "  'num_papers': 1185,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.PennTreebank',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/structured_prediction/dataset_readers/penn_tree_bank/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikitext-103',\n",
       "  'name': 'WikiText-103',\n",
       "  'full_name': 'WikiText-103',\n",
       "  'homepage': 'https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/',\n",
       "  'description': 'The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\r\\n\\r\\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\\r\\n\\r\\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\\r\\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)',\n",
       "  'paper': {'title': 'Pointer Sentinel Mixture Models',\n",
       "   'url': 'https://paperswithcode.com/paper/pointer-sentinel-mixture-models'},\n",
       "  'introduced_date': '2016-09-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiText-103'],\n",
       "  'num_papers': 264,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText103',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lfw',\n",
       "  'name': 'LFW',\n",
       "  'full_name': 'Labeled Faces in the Wild',\n",
       "  'homepage': 'http://vis-www.cs.umass.edu/lfw/',\n",
       "  'description': 'The **LFW** dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs.\\r\\n\\r\\nSource: [A Performance Evaluation of Loss Functions for Deep Face Recognition](https://arxiv.org/abs/1901.05903)\\r\\nImage Source: [http://vis-www.cs.umass.edu/lfw](http://vis-www.cs.umass.edu/lfw)',\n",
       "  'paper': {'title': 'Labeled faces in the wild: A database for studying face recognition in unconstrained environments',\n",
       "   'url': 'http://vis-www.cs.umass.edu/lfw/lfw.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': '3D FACE MODELING',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-modeling'},\n",
       "   {'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Face Quality Assessement',\n",
       "    'url': 'https://paperswithcode.com/task/face-quality-assessement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LFW', 'Labeled Faces in the Wild', 'LFW (Online Open Set)'],\n",
       "  'num_papers': 638,\n",
       "  'data_loaders': [{'url': 'https://github.com/pytorch/vision',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/lfw-deep-funneled-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/lfw-funneled-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/lfw-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/lfw',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikisql',\n",
       "  'name': 'WikiSQL',\n",
       "  'full_name': 'WikiSQL',\n",
       "  'homepage': 'https://github.com/salesforce/WikiSQL',\n",
       "  'description': '**WikiSQL** consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases.\\r\\n\\r\\nSource: [SQL-to-Text Generation with Graph-to-Sequence Model](https://arxiv.org/abs/1809.05255)\\r\\nImage Source: [https://blog.einstein.ai/how-to-talk-to-your-database/](https://blog.einstein.ai/how-to-talk-to-your-database/)',\n",
       "  'paper': {'title': 'Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/seq2sql-generating-structured-queries-from'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Code Generation',\n",
       "    'url': 'https://paperswithcode.com/task/code-generation'},\n",
       "   {'task': 'SQL-to-Text',\n",
       "    'url': 'https://paperswithcode.com/task/sql-to-text'},\n",
       "   {'task': 'Sql Chatbots',\n",
       "    'url': 'https://paperswithcode.com/task/sql-chatbots'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WikiSQL'],\n",
       "  'num_papers': 144,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wikisql',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#wikisql-semantic-parsing-task',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/salesforce/WikiSQL',\n",
       "    'repo': 'https://github.com/salesforce/WikiSQL',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/openai-gym',\n",
       "  'name': 'OpenAI Gym',\n",
       "  'full_name': 'OpenAI Gym',\n",
       "  'homepage': 'https://gym.openai.com/',\n",
       "  'description': '**OpenAI Gym** is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text.\\r\\n\\r\\nSource: [https://github.com/openai/gym](https://github.com/openai/gym)\\r\\nImage Source: [https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947](https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947)',\n",
       "  'paper': {'title': 'OpenAI Gym',\n",
       "   'url': 'https://paperswithcode.com/paper/openai-gym'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Continuous Control',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control'},\n",
       "   {'task': 'OpenAI Gym',\n",
       "    'url': 'https://paperswithcode.com/task/openai-gym'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OpenAI Gym',\n",
       "   'Cart Pole (OpenAI Gym)',\n",
       "   'Lunar Lander (OpenAI Gym)'],\n",
       "  'num_papers': 829,\n",
       "  'data_loaders': [{'url': 'https://github.com/openai/gym/blob/master/docs/environments.md',\n",
       "    'repo': 'https://github.com/openai/gym',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikitext-2',\n",
       "  'name': 'WikiText-2',\n",
       "  'full_name': 'WikiText-2',\n",
       "  'homepage': 'https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/',\n",
       "  'description': 'The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\\r\\n\\r\\nCompared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over 110 times larger. The WikiText dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\\r\\n\\r\\nSource: [The WikiText Long Term Dependency Language Modeling Dataset](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\\r\\nImage Source: [https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)',\n",
       "  'paper': {'title': 'Pointer Sentinel Mixture Models',\n",
       "   'url': 'https://paperswithcode.com/paper/pointer-sentinel-mixture-models'},\n",
       "  'introduced_date': '2016-09-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiText-2'],\n",
       "  'num_papers': 211,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wikitext',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.WikiText2',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikilarge',\n",
       "  'name': 'WikiLarge',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/XingxingZhang/dress',\n",
       "  'description': '**WikiLarge** comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references\\r\\n\\r\\nSource: [Semi-Supervised Text Simplification with Back-Translation and Asymmetric Denoising Autoencoders](https://arxiv.org/abs/2004.14693)\\r\\nImage Source: [https://arxiv.org/pdf/1904.02767.pdf](https://arxiv.org/pdf/1904.02767.pdf)',\n",
       "  'paper': {'title': 'Sentence Simplification with Deep Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/sentence-simplification-with-deep'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Simplification',\n",
       "    'url': 'https://paperswithcode.com/task/text-simplification'},\n",
       "   {'task': 'Lexical Simplification',\n",
       "    'url': 'https://paperswithcode.com/task/lexical-simplification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiLarge'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': [{'url': 'https://github.com/XingxingZhang/dress',\n",
       "    'repo': 'https://github.com/XingxingZhang/dress',\n",
       "    'frameworks': ['torch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/food-101',\n",
       "  'name': 'Food-101',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/',\n",
       "  'description': 'The  **Food-101** dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise.\\r\\n\\r\\nSource: [Combining Weakly and Webly Supervised Learning for Classifying Food Images](https://arxiv.org/abs/1712.08730)\\r\\nImage Source: [https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)',\n",
       "  'paper': {'title': 'Food-101 - Mining Discriminative Components with Random Forests',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10599-4_29'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Document Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-text-classification'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Multi-Modal Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-modal-document-classification'},\n",
       "   {'task': 'Multimodal Text and Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-text-and-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Food-101', 'Food-101N'],\n",
       "  'num_papers': 194,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/nateraw/food101',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/nateraw/sync_food101',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/nateraw/food101_old',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/food101',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/merty/nateraw-food101-copy',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/food101',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fashion-mnist',\n",
       "  'name': 'Fashion-MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/zalandoresearch/fashion-mnist',\n",
       "  'description': '**Fashion-MNIST** is a dataset comprising of 28×28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST.\\r\\n\\r\\nSource: [Generative Probabilistic Novelty Detection with Adversarial Autoencoders](https://arxiv.org/abs/1807.02588)\\r\\nImage Source: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)',\n",
       "  'paper': {'title': 'Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms',\n",
       "   'url': 'https://paperswithcode.com/paper/fashion-mnist-a-novel-image-dataset-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Outlier Detection',\n",
       "    'url': 'https://paperswithcode.com/task/outlier-detection'},\n",
       "   {'task': 'Clustering Algorithms Evaluation',\n",
       "    'url': 'https://paperswithcode.com/task/clustering-algorithms-evaluation'},\n",
       "   {'task': 'General Classification',\n",
       "    'url': 'https://paperswithcode.com/task/classification'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Fashion-MNIST', 'Rotated Fashion-MNIST'],\n",
       "  'num_papers': 1699,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/fashion_mnist',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.FashionMNIST',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/zalandoresearch/fashion-mnist',\n",
       "    'repo': 'https://github.com/zalandoresearch/fashion-mnist',\n",
       "    'frameworks': ['tf']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/fashion-mnist-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/fashion_mnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'repo': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/shapenet',\n",
       "  'name': 'ShapeNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.shapenet.org/',\n",
       "  'description': '**ShapeNet** is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).\\r\\n\\r\\nSource: [A review on deep learning techniques for 3D sensed data classification](https://arxiv.org/abs/1907.04444)\\r\\nImage Source: [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012)',\n",
       "  'paper': {'title': 'ShapeNet: An Information-Rich 3D Model Repository',\n",
       "   'url': 'https://paperswithcode.com/paper/shapenet-an-information-rich-3d-model'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud'],\n",
       "  'tasks': [{'task': '3D Object Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-reconstruction'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Novel View Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/novel-view-synthesis'},\n",
       "   {'task': 'Point Cloud Generation',\n",
       "    'url': 'https://paperswithcode.com/task/point-cloud-generation'},\n",
       "   {'task': 'Single-View 3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/single-view-3d-reconstruction'},\n",
       "   {'task': 'Point Cloud Completion',\n",
       "    'url': 'https://paperswithcode.com/task/point-cloud-completion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ShapeNet',\n",
       "   'ShapeNet Car',\n",
       "   'ShapeNet Chair',\n",
       "   'ShapeNet Airplane'],\n",
       "  'num_papers': 915,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cinic-10',\n",
       "  'name': 'CINIC-10',\n",
       "  'full_name': 'CINIC-10',\n",
       "  'homepage': 'https://github.com/BayesWatch/cinic-10',\n",
       "  'description': '**CINIC-10** is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images.\\r\\n\\r\\nSource: [Group Knowledge Transfer:Collaborative Training of Large CNNs on the Edge](https://arxiv.org/abs/2007.14513)\\r\\nImage Source: [https://arxiv.org/abs/1810.03505](https://arxiv.org/abs/1810.03505)',\n",
       "  'paper': {'title': 'CINIC-10 is not ImageNet or CIFAR-10',\n",
       "   'url': 'https://paperswithcode.com/paper/cinic-10-is-not-imagenet-or-cifar-10'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Sparse Learning',\n",
       "    'url': 'https://paperswithcode.com/task/sparse-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CINIC-10'],\n",
       "  'num_papers': 59,\n",
       "  'data_loaders': [{'url': 'https://github.com/BayesWatch/cinic-10',\n",
       "    'repo': 'https://github.com/BayesWatch/cinic-10',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/flickr30k',\n",
       "  'name': 'Flickr30k',\n",
       "  'full_name': 'Flickr30k',\n",
       "  'homepage': 'https://shannon.cs.illinois.edu/DenotationGraph/',\n",
       "  'description': 'The **Flickr30k** dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.\\r\\n\\r\\nSource: [Guiding Long-Short Term Memory for Image Caption Generation](https://arxiv.org/abs/1509.04942)\\r\\n\\r\\nImage Source: [Dual-Path Convolutional Image-Text Embedding with Instance Loss\\r\\n](https://arxiv.org/abs/1711.05535)',\n",
       "  'paper': {'title': 'From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions',\n",
       "   'url': 'https://paperswithcode.com/paper/from-image-descriptions-to-visual-denotations'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/image-captioning'},\n",
       "   {'task': 'Text-Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/texture-image-retrieval'},\n",
       "   {'task': 'Phrase Grounding',\n",
       "    'url': 'https://paperswithcode.com/task/phrase-grounding'},\n",
       "   {'task': 'Text-to-Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-image-retrieval'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Image-to-Text Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-text-retrieval'},\n",
       "   {'task': 'Zero-Shot Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-cross-modal-retrieval'},\n",
       "   {'task': 'Video Description',\n",
       "    'url': 'https://paperswithcode.com/task/video-description'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Flickr30k',\n",
       "   'Flickr',\n",
       "   'Flickr30k Captions test',\n",
       "   'Flickr30K 1K test'],\n",
       "  'num_papers': 397,\n",
       "  'data_loaders': [{'url': 'https://parl.ai/docs/tasks.html#flickr30k',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/coco',\n",
       "  'name': 'COCO',\n",
       "  'full_name': 'Microsoft Common Objects in Context',\n",
       "  'homepage': 'https://cocodataset.org/',\n",
       "  'description': 'The MS **COCO** (**Microsoft Common Objects in Context**) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.\\r\\n\\r\\n**Splits:**\\r\\nThe first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.\\r\\n\\r\\nBased on community feedback, in 2017 the training/validation split was changed from 83K/41K to 118K/5K. The new split uses the same images and annotations. The 2017 test set is a subset of 41K images of the 2015 test set. Additionally, the 2017 release contains a new unannotated dataset of 123K images.\\r\\n\\r\\n**Annotations:**\\r\\nThe dataset has annotations for\\r\\n\\r\\n* object detection: bounding boxes and per-instance segmentation masks with 80 object categories,\\r\\n* captioning: natural language descriptions of the images (see MS COCO Captions),\\r\\n* keypoints detection: containing more than 200,000 images and 250,000 person instances labeled with keypoints (17 possible keypoints, such as left eye, nose, right hip, right ankle),\\r\\n* stuff image segmentation – per-pixel segmentation masks with 91 stuff categories, such as grass, wall, sky (see MS COCO Stuff),\\r\\n* panoptic: full scene segmentation, with 80 thing categories (such as person, bicycle, elephant) and a subset of 91 stuff categories (grass, sky, road),\\r\\n* dense pose: more than 39,000 images and 56,000 person instances labeled with DensePose annotations – each labeled person is annotated with an instance id and a mapping between image pixels that belong to that person body and a template 3D model.\\r\\nThe annotations are publicly available only for training and validation images.\\r\\n\\r\\nSource: [https://cocodataset.org/](https://cocodataset.org/)\\r\\nImage Source: [https://cocodataset.org/](https://cocodataset.org/)',\n",
       "  'paper': {'title': 'Microsoft COCO: Common Objects in Context',\n",
       "   'url': 'https://paperswithcode.com/paper/microsoft-coco-common-objects-in-context'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Keypoint Detection',\n",
       "    'url': 'https://paperswithcode.com/task/keypoint-detection'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/image-captioning'},\n",
       "   {'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-person-pose-estimation'},\n",
       "   {'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'},\n",
       "   {'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': 'Text-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-image-generation'},\n",
       "   {'task': 'Text-Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/texture-image-retrieval'},\n",
       "   {'task': 'Unsupervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-semantic-segmentation'},\n",
       "   {'task': 'Conditional Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/conditional-image-generation'},\n",
       "   {'task': 'Question Generation',\n",
       "    'url': 'https://paperswithcode.com/task/question-generation'},\n",
       "   {'task': 'Few-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-object-detection'},\n",
       "   {'task': 'Layout-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/layout-to-image-generation'},\n",
       "   {'task': 'Single-object discovery',\n",
       "    'url': 'https://paperswithcode.com/task/single-object-discovery'},\n",
       "   {'task': 'Class-agnostic Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/class-agnostic-object-detection'},\n",
       "   {'task': 'Object Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/object-proposal-generation'},\n",
       "   {'task': 'Homography Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/homography-estimation'},\n",
       "   {'task': 'Real-time Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-instance-segmentation'},\n",
       "   {'task': 'Open World Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/open-world-object-detection'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Object Counting',\n",
       "    'url': 'https://paperswithcode.com/task/object-counting'},\n",
       "   {'task': 'Zero-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-object-detection'},\n",
       "   {'task': 'Real-Time Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-object-detection'},\n",
       "   {'task': 'Weakly-supervised instance segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-instance-segmentation'},\n",
       "   {'task': 'Robust Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/robust-object-detection'},\n",
       "   {'task': 'Multi-object discovery',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-discovery'},\n",
       "   {'task': 'Knowledge Distillation',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-distillation'},\n",
       "   {'task': 'One-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-object-detection'},\n",
       "   {'task': 'Active Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/active-object-detection'},\n",
       "   {'task': 'Zero-Shot Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-cross-modal-retrieval'},\n",
       "   {'task': 'One-Shot Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-instance-segmentation'},\n",
       "   {'task': 'Region Proposal',\n",
       "    'url': 'https://paperswithcode.com/task/region-proposal'},\n",
       "   {'task': 'Activeness Detection',\n",
       "    'url': 'https://paperswithcode.com/task/activeness-detection'},\n",
       "   {'task': 'Generalized Zero-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-object-detection'},\n",
       "   {'task': 'Zero-Shot Text-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-text-to-image-generation'},\n",
       "   {'task': 'Semi Supervised Learning for Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-learning-for-image-captioning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COCO 2017 (Sports, Food)',\n",
       "   'COCO 2017 (Outdoor, Accessories, Appliance, Truck)',\n",
       "   'COCO 2017 (Electronic, Indoor, Kitchen, Furniture)',\n",
       "   'COCO Visual Question Answering (VQA) abstract images 1.0 open ended',\n",
       "   'COCO Visual Question Answering (VQA) abstract 1.0 multiple choice',\n",
       "   'COCO 2017',\n",
       "   'COCO 2014',\n",
       "   'coco minval',\n",
       "   'COCO-Stuff-3',\n",
       "   'COCO-Stuff 256x256',\n",
       "   'COCO 256 x 256',\n",
       "   'COCO 2015',\n",
       "   'MS-COCO (10-shot)',\n",
       "   'MS-COCO',\n",
       "   'COCO Visual Question Answering (VQA) real images 2.0 open ended',\n",
       "   'COCO Visual Question Answering (VQA) real images 1.0 open ended',\n",
       "   'MSCOCO',\n",
       "   'DensePose-COCO',\n",
       "   'COCO_20k',\n",
       "   'COCO-Animals',\n",
       "   'COCO+',\n",
       "   'COCO test-dev',\n",
       "   'COCO test-challenge',\n",
       "   'COCO panoptic',\n",
       "   'COCO minival',\n",
       "   'COCO count-test',\n",
       "   'COCO Visual Question Answering (VQA) real images 1.0 multiple choice',\n",
       "   'COCO (image as query)',\n",
       "   'COCO'],\n",
       "  'num_papers': 5281,\n",
       "  'data_loaders': [{'url': 'https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-coco-instance-keypoint-detection',\n",
       "    'repo': 'https://github.com/facebookresearch/detectron2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CocoDetection',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/coco-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/coco',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#coco',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bsds500',\n",
       "  'name': 'BSDS500',\n",
       "  'full_name': 'Berkeley Segmentation Dataset 500',\n",
       "  'homepage': 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html',\n",
       "  'description': 'Berkeley Segmentation Data Set 500 (**BSDS500**) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test.\\r\\n\\r\\nSource: [Object Contour Detection with a Fully Convolutional Encoder-Decoder Network](https://arxiv.org/abs/1603.04530)\\r\\nImage Source: [https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html)',\n",
       "  'paper': {'title': 'Contour Detection and Hierarchical Image Segmentation',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2010.161'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Point cloud'],\n",
       "  'tasks': [{'task': 'JPEG Artifact Correction',\n",
       "    'url': 'https://paperswithcode.com/task/jpeg-artifact-correction'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BSDS500 (Quality 30 Grayscale)',\n",
       "   'BSDS500 (Quality 30 Color)',\n",
       "   'BSDS500 (Quality 20 Grayscale)',\n",
       "   'BSDS500 (Quality 20 Color)',\n",
       "   'BSDS500 (Quality 10 Grayscale)',\n",
       "   'BSDS500 (Quality 10 Color)',\n",
       "   'BSDS500'],\n",
       "  'num_papers': 191,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mhp',\n",
       "  'name': 'MHP',\n",
       "  'full_name': 'Multiple-Human Parsing',\n",
       "  'homepage': 'https://arxiv.org/pdf/1705.07206.pdf',\n",
       "  'description': 'The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting.\\r\\n\\r\\nSource: [Multiple-Human Parsing in the Wild](https://arxiv.org/pdf/1705.07206)\\r\\nImage Source: [Li et al](https://arxiv.org/pdf/1705.07206.pdf)',\n",
       "  'paper': {'title': 'Multiple-Human Parsing in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/multiple-human-parsing-in-the-wild'},\n",
       "  'introduced_date': '2017-05-19',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Human Part Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/human-part-segmentation'},\n",
       "   {'task': 'Human Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/human-parsing'},\n",
       "   {'task': 'Multi-Human Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/multi-human-parsing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MHP', 'MHP v2.0', 'MHP v1.0'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#mhp',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lrw',\n",
       "  'name': 'LRW',\n",
       "  'full_name': 'Lip Reading in the Wild',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html',\n",
       "  'description': 'The **Lip Reading in the Wild** (**LRW**) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances.\\r\\n\\r\\nSource: [Towards Pose-invariant Lip-Reading](https://arxiv.org/abs/1911.06095)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html)',\n",
       "  'paper': {'title': 'Lip Reading in the Wild',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-54184-6_6'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Lip to Speech Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/lip-to-speech-synthesis'},\n",
       "   {'task': 'Lipreading', 'url': 'https://paperswithcode.com/task/lipreading'},\n",
       "   {'task': 'Lip Reading',\n",
       "    'url': 'https://paperswithcode.com/task/lip-reading'},\n",
       "   {'task': 'Unconstrained Lip-synchronization',\n",
       "    'url': 'https://paperswithcode.com/task/lip-sync'},\n",
       "   {'task': 'Visual Keyword Spotting',\n",
       "    'url': 'https://paperswithcode.com/task/visual-keyword-spotting'},\n",
       "   {'task': 'Talking Face Generation',\n",
       "    'url': 'https://paperswithcode.com/task/talking-face-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LRW', 'Lip Reading in the Wild', 'Lipreading in the Wild'],\n",
       "  'num_papers': 104,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fddb',\n",
       "  'name': 'FDDB',\n",
       "  'full_name': 'Face Detection Dataset and Benchmark',\n",
       "  'homepage': 'http://vis-www.cs.umass.edu/fddb/',\n",
       "  'description': 'The **Face Detection Dataset and Benchmark** (**FDDB**) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included.\\r\\n\\r\\nSource: [A Comparison of CNN-based Face and Head Detectors for Real-Time Video Surveillance Applications](https://arxiv.org/abs/1809.03336)',\n",
       "  'paper': {'title': 'Fddb: A benchmark for face detection in unconstrained settings',\n",
       "   'url': 'http://vis-www.cs.umass.edu/fddb/fddb.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FDDB'],\n",
       "  'num_papers': 145,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/got-10k',\n",
       "  'name': 'GOT-10k',\n",
       "  'full_name': 'Generic Object Tracking Benchmark',\n",
       "  'homepage': 'http://got-10k.aitestunion.com/',\n",
       "  'description': 'The **GOT-10k** dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns.\\r\\n\\r\\nSource: [http://got-10k.aitestunion.com/](http://got-10k.aitestunion.com/)\\r\\nImage Source: [https://arxiv.org/pdf/1810.11981.pdf](https://arxiv.org/pdf/1810.11981.pdf)',\n",
       "  'paper': {'title': 'GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/got-10k-a-large-high-diversity-benchmark-for'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GOT-10k'],\n",
       "  'num_papers': 108,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cifar10',\n",
       "  'name': 'CIFAR10',\n",
       "  'full_name': None,\n",
       "  'homepage': None,\n",
       "  'description': '',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'},\n",
       "   {'task': 'Parameter Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/parameter-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dukemtmc-reid',\n",
       "  'name': 'DukeMTMC-reID',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'The **DukeMTMC-reID** (Duke Multi-Tracking Multi-Camera ReIDentification) dataset is a subset of the DukeMTMC for image-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian image datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 16,522 training images of 702 identities, 2,228 query images of the other 702 identities and 17,661 gallery images.\\r\\n\\r\\n**NOTE**: This dataset [has been retracted](https://exposing.ai/duke_mtmc/).\\r\\n\\r\\nSource: [Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification](https://arxiv.org/abs/1804.11027)',\n",
       "  'paper': {'title': 'Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/performance-measures-and-a-data-set-for-multi'},\n",
       "  'introduced_date': '2016-09-06',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Unsupervised Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-person-re-identification'},\n",
       "   {'task': 'Generalizable Person Re-identification',\n",
       "    'url': 'https://paperswithcode.com/task/generalizable-person-re-identification'},\n",
       "   {'task': 'Style Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/style-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DukeMTMC-reID',\n",
       "   'DukeMTMCreID',\n",
       "   'Market-1501->DukeMTMC-reID',\n",
       "   'MSMT17->DukeMTMC-reID'],\n",
       "  'num_papers': 269,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kitti',\n",
       "  'name': 'KITTI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cvlibs.net/datasets/kitti/',\n",
       "  'description': '**KITTI** (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. [Álvarez et al.](http://yann.lecun.com/exdb/publis/pdf/alvarez-eccv-12.pdf) generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. [Zhang et al.](http://www-video.eecs.berkeley.edu/papers/rzhang/zhang-icra-submission.pdf) annotated 252 (140 for training and 112 for testing) acquisitions – RGB and Velodyne scans – from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. [Ros et al.](http://refbase.cvc.uab.es/files/rrg2015.pdf) labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist.\\r\\n\\r\\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\\r\\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)',\n",
       "  'paper': {'title': 'Are we ready for autonomous driving? The KITTI vision benchmark suite',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6248074'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Birds Eye View Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/birds-eye-view-object-detection'},\n",
       "   {'task': 'Monocular 3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-3d-object-detection'},\n",
       "   {'task': 'Transfer Learning',\n",
       "    'url': 'https://paperswithcode.com/task/transfer-learning'},\n",
       "   {'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': 'Monocular Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-depth-estimation'},\n",
       "   {'task': 'Visual Place Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/visual-place-recognition'},\n",
       "   {'task': 'Image Dehazing',\n",
       "    'url': 'https://paperswithcode.com/task/image-dehazing'},\n",
       "   {'task': 'Optical Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/optical-flow-estimation'},\n",
       "   {'task': 'Novel View Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/novel-view-synthesis'},\n",
       "   {'task': 'Depth Completion',\n",
       "    'url': 'https://paperswithcode.com/task/depth-completion'},\n",
       "   {'task': 'Visual Odometry',\n",
       "    'url': 'https://paperswithcode.com/task/visual-odometry'},\n",
       "   {'task': 'Point Cloud Registration',\n",
       "    'url': 'https://paperswithcode.com/task/point-cloud-registration'},\n",
       "   {'task': 'Multiple Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multiple-object-tracking'},\n",
       "   {'task': 'Class-agnostic Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/class-agnostic-object-detection'},\n",
       "   {'task': 'Object Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/object-proposal-generation'},\n",
       "   {'task': 'Dense Pixel Correspondence Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/dense-pixel-correspondence-estimation'},\n",
       "   {'task': 'Real-time Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-instance-segmentation'},\n",
       "   {'task': 'Horizon Line Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/horizon-line-estimation'},\n",
       "   {'task': 'Vehicle Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/vehicle-pose-estimation'},\n",
       "   {'task': 'Stereo Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/stereo-depth-estimation'},\n",
       "   {'task': 'Scene Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-flow-estimation'},\n",
       "   {'task': '3D Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/3d-multi-object-tracking'},\n",
       "   {'task': 'Stereo Disparity Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/stereo-disparity-estimation'},\n",
       "   {'task': 'Monocular Cross-View Road Scene Parsing(Vehicle)',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-cross-view-road-scene-parsing'},\n",
       "   {'task': '3D Object Detection From Stereo Images',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection-from-stereo-images'},\n",
       "   {'task': 'Prediction Of Occupancy Grid Maps',\n",
       "    'url': 'https://paperswithcode.com/task/prediction-of-occupancy-grid-maps'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['2D KITTI Cars Moderate',\n",
       "   'KITTI 2015 Scene Flow Test',\n",
       "   'KITTI 2015 Scene Flow Training',\n",
       "   'KITTI Novel View Synthesis',\n",
       "   'KITTI 2015 - unsupervised',\n",
       "   'KITTI 2012 - unsupervised',\n",
       "   'KITTI Pedestrians Moderate val',\n",
       "   'KITTI Pedestrian Hard',\n",
       "   'KITTI Panoptic Segmentation',\n",
       "   'KITTI Object Tracking Evaluation 2012',\n",
       "   'KITTI Horizon',\n",
       "   'KITTI Depth Completion Eigen Split',\n",
       "   'KITTI Tracking test',\n",
       "   'KITTI2015',\n",
       "   'KITTI 2015 unsupervised',\n",
       "   'KITTI 2015 - 4x upscaling',\n",
       "   'KITTI 2015 - 2x upscaling',\n",
       "   'KITTI 2015',\n",
       "   'KITTI2012',\n",
       "   'KITTI 2012 unsupervised',\n",
       "   'KITTI 2012 - 4x upscaling',\n",
       "   'KITTI 2012 - 2x upscaling',\n",
       "   'KITTI 2012',\n",
       "   'PreSIL to KITTI',\n",
       "   'KITTI Semantic Segmentation',\n",
       "   'KITTI Pedestrians Moderate',\n",
       "   'KITTI Pedestrians Hard',\n",
       "   'KITTI Pedestrians Easy',\n",
       "   'KITTI Pedestrian Moderate val',\n",
       "   'KITTI Pedestrian Hard val',\n",
       "   'KITTI Pedestrian Easy val',\n",
       "   'KITTI Eigen split unsupervised',\n",
       "   'KITTI Eigen split',\n",
       "   'KITTI Depth Completion',\n",
       "   'KITTI Cyclists Moderate',\n",
       "   'KITTI Cyclists Hard',\n",
       "   'KITTI Cyclists Easy',\n",
       "   'KITTI Cyclist Moderate val',\n",
       "   'KITTI Cyclist Hard val',\n",
       "   'KITTI Cyclist Easy val',\n",
       "   'KITTI Cars Moderate val',\n",
       "   'KITTI Cars Moderate',\n",
       "   'KITTI Cars Hard val',\n",
       "   'KITTI Cars Hard',\n",
       "   'KITTI Cars Easy val',\n",
       "   'KITTI Cars Easy',\n",
       "   'KITTI'],\n",
       "  'num_papers': 1911,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/kitti',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmdetection3d/blob/master/docs/data_preparation.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection3d',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ucf101',\n",
       "  'name': 'UCF101',\n",
       "  'full_name': 'UCF101 Human Actions dataset',\n",
       "  'homepage': 'https://www.crcv.ucf.edu/data/UCF101.php',\n",
       "  'description': '**UCF101** dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 × 240.\\r\\n\\r\\nSource: [Two-stream Collaborative Learning with Spatial-Temporal Attention for Video Classification](https://arxiv.org/abs/1711.03273)\\r\\nImage Source: [https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)',\n",
       "  'paper': {'title': 'UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/ucf101-a-dataset-of-101-human-actions-classes'},\n",
       "  'introduced_date': '2012-12-03',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-detection'},\n",
       "   {'task': 'Video Generation',\n",
       "    'url': 'https://paperswithcode.com/task/video-generation'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Self-Supervised Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-action-recognition'},\n",
       "   {'task': 'Video Frame Interpolation',\n",
       "    'url': 'https://paperswithcode.com/task/video-frame-interpolation'},\n",
       "   {'task': 'Zero-Shot Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-action-recognition'},\n",
       "   {'task': 'Self-supervised Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-video-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCF101-24',\n",
       "   'UCF101',\n",
       "   'UCF-101 16 frames, Unconditional, Single GPU',\n",
       "   'UCF-101 16 frames, 64x64, Unconditional',\n",
       "   'UCF-101 16 frames, 128x128, Unconditional',\n",
       "   'UCF-101'],\n",
       "  'num_papers': 1025,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.UCF101',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/ucf101',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ucf101/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hmdb51',\n",
       "  'name': 'HMDB51',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database',\n",
       "  'description': 'The **HMDB51** dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,849 video clips from 51 action categories (such as “jump”, “kiss” and “laugh”), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance.\\r\\n\\r\\nSource: [Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors](https://arxiv.org/abs/1505.04868)\\r\\nImage Source: [https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database)',\n",
       "  'paper': {'title': 'HMDB: A large video database for human motion recognition',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2011.6126543'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Self-Supervised Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-action-recognition'},\n",
       "   {'task': 'Zero-Shot Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-action-recognition'},\n",
       "   {'task': 'Self-supervised Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-video-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCF-to-HMDBsmall',\n",
       "   'UCF-to-HMDBfull',\n",
       "   'UCF --> HMDB (full)',\n",
       "   'Olympic-to-HMDBsmall',\n",
       "   'HMDBsmall-to-UCF',\n",
       "   'HMDBfull-to-UCF',\n",
       "   'HMDB --> UCF (full)',\n",
       "   'HMDB51',\n",
       "   'HMDB-51'],\n",
       "  'num_papers': 512,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.HMDB51',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hmdb51/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mimic-iii',\n",
       "  'name': 'MIMIC-III',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://mimic.physionet.org/',\n",
       "  'description': 'The Medical Information Mart for Intensive Care III (**MIMIC-III**) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average.\\r\\n\\r\\nSource: [Interaction Matching for Long-Tail Multi-Label Classification](https://arxiv.org/abs/2005.08805)',\n",
       "  'paper': {'title': 'Mimic-iii, a freely accessible critical care database',\n",
       "   'url': 'http://www.nature.com/articles/sdata201635'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Medical', 'Tabular'],\n",
       "  'tasks': [{'task': 'Multivariate Time Series Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-forecasting'},\n",
       "   {'task': 'Multi-Label Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-text-classification'},\n",
       "   {'task': 'Mortality Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/mortality-prediction'},\n",
       "   {'task': 'Length-of-Stay prediction',\n",
       "    'url': 'https://paperswithcode.com/task/length-of-stay-prediction'},\n",
       "   {'task': 'Blood pressure estimation',\n",
       "    'url': 'https://paperswithcode.com/task/blood-pressure-estimation'},\n",
       "   {'task': 'Medical Code Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/medical-code-prediction'},\n",
       "   {'task': 'Multi-Label Classification Of Biomedical Texts',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification-of-biomedical'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MIMIC-III'],\n",
       "  'num_papers': 482,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tid2013',\n",
       "  'name': 'TID2013',\n",
       "  'full_name': 'TID2013',\n",
       "  'homepage': 'http://www.ponomarenko.info/tid2013.htm',\n",
       "  'description': '**TID2013** is a dataset for image quality assessment that contains 25 reference images and 3000 distorted images (25 reference images x 24 types of distortions x 5 levels of distortions). \\r\\n\\r\\nSource: [Lukin et al](https://www.researchgate.net/figure/Reference-images-in-databases-TID2008-and-TID2013_fig3_268038378)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/image-quality-assessment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TID2013'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/live',\n",
       "  'name': 'LIVE',\n",
       "  'full_name': 'Laboratory for Image & Video Engineering',\n",
       "  'homepage': 'http://live.ece.utexas.edu/research/Quality/',\n",
       "  'description': 'Briefly describe the dataset. Provide:\\r\\n\\r\\n* a high-level explanation of the dataset characteristics\\r\\n* explain motivations and summary of its content\\r\\n* potential use cases of the dataset\\r\\n\\r\\nIf the description or image is from a different paper, please refer to it as follows:\\r\\nSource: [title](url)\\r\\nImage Source: [title](url)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['LIVE'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/visual-genome',\n",
       "  'name': 'Visual Genome',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://visualgenome.org/',\n",
       "  'description': '**Visual Genome** contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships.\\r\\n\\r\\nSource: [RaAM: A Relation-aware Attention Model for Visual Question Answering](https://arxiv.org/abs/1903.12314)\\r\\nImage Source: [Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision/)',\n",
       "  'paper': {'title': 'Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-genome-connecting-language-and-vision'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Layout-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/layout-to-image-generation'},\n",
       "   {'task': 'Phrase Grounding',\n",
       "    'url': 'https://paperswithcode.com/task/phrase-grounding'},\n",
       "   {'task': 'Scene Graph Generation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-generation'},\n",
       "   {'task': 'Unsupervised KG-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-kg-to-text-generation'},\n",
       "   {'task': 'Scene Graph Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-detection'},\n",
       "   {'task': 'Predicate Classification',\n",
       "    'url': 'https://paperswithcode.com/task/predicate-classification'},\n",
       "   {'task': 'Scene Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-classification'},\n",
       "   {'task': 'Unsupervised semantic parsing',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-semantic-parsing'},\n",
       "   {'task': 'Bidirectional Relationship Classification',\n",
       "    'url': 'https://paperswithcode.com/task/bidirectional-relationship-classification'},\n",
       "   {'task': 'Unbiased Scene Graph Generation',\n",
       "    'url': 'https://paperswithcode.com/task/unbiased-scene-graph-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VG graph-text',\n",
       "   'Visual Genome 256x256',\n",
       "   'Visual Genome 64x64',\n",
       "   'Visual Genome 128x128',\n",
       "   'Visual Genome (subjects)',\n",
       "   'Visual Genome (pairs)',\n",
       "   'Visual Genome'],\n",
       "  'num_papers': 668,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/coco-stuff',\n",
       "  'name': 'COCO-Stuff',\n",
       "  'full_name': 'Common Objects in COntext-stuff',\n",
       "  'homepage': 'https://github.com/nightrome/cocostuff',\n",
       "  'description': 'The **Common Objects in COntext-stuff** (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class.\\r\\n\\r\\nSource: [Image Colorization: A Survey and Dataset](https://arxiv.org/abs/2008.10774)\\r\\nImage Source: [https://github.com/nightrome/cocostuff](https://github.com/nightrome/cocostuff)',\n",
       "  'paper': {'title': 'COCO-Stuff: Thing and Stuff Classes in Context',\n",
       "   'url': 'https://paperswithcode.com/paper/coco-stuff-thing-and-stuff-classes-in-context'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Unsupervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-semantic-segmentation'},\n",
       "   {'task': 'Layout-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/layout-to-image-generation'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COCO-Stuff full',\n",
       "   'COCO-Stuff-3',\n",
       "   'COCO-Stuff-15',\n",
       "   'COCO-Stuff test',\n",
       "   'COCO-Stuff Labels-to-Photos',\n",
       "   'COCO-Stuff 64x64',\n",
       "   'COCO-Stuff 128x128',\n",
       "   'COCO-Stuff'],\n",
       "  'num_papers': 124,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'repo': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/nightrome/cocostuff',\n",
       "    'repo': 'https://github.com/nightrome/cocostuff',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mars',\n",
       "  'name': 'MARS',\n",
       "  'full_name': 'Motion Analysis and Re-identification Set',\n",
       "  'homepage': 'http://zheng-lab.cecs.anu.edu.au/Project/project_mars.html',\n",
       "  'description': '**MARS** (**Motion Analysis and Re-identification Set**) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long).\\r\\n\\r\\nSource: [Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets](https://arxiv.org/abs/1706.06196)',\n",
       "  'paper': {'title': 'MARS: A Video Benchmark for Large-Scale Person Re-Identification',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-46466-4_52'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Video-Based Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/video-based-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MARS'],\n",
       "  'num_papers': 143,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ilids-vid',\n",
       "  'name': 'iLIDS-VID',\n",
       "  'full_name': 'iLIDS-VID',\n",
       "  'homepage': 'http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html',\n",
       "  'description': 'The **iLIDS-VID** dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions.\\n\\nSource: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)\\nImage Source: [http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html](http://www.eecs.qmul.ac.uk/~xiatian/downloads_qmul_iLIDS-VID_ReID_dataset.html)',\n",
       "  'paper': {'title': 'Unsupervised Person Re-identification by Deep Learning Tracklet Association',\n",
       "   'url': 'https://paperswithcode.com/paper/unsupervised-person-re-identification-by-deep-1'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Unsupervised Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-person-re-identification'},\n",
       "   {'task': 'Video-Based Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/video-based-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iLIDS-VID'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/market-1501',\n",
       "  'name': 'Market-1501',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.kaggle.com/pengcw1/market-1501/data',\n",
       "  'description': '**Market-1501** is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images.\\r\\n\\r\\nSource: [A Survey of Pruning Methods for Efficient Person Re-identification Across Domains](https://arxiv.org/abs/1907.02547)\\r\\nImage Source: [https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification](https://www.researchgate.net/publication/306358716_A_Discriminative_Null_Space_based_Deep_Learning_Approach_for_Person_Re-Identification)',\n",
       "  'paper': {'title': 'Scalable Person Re-Identification: A Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/scalable-person-re-identification-a-benchmark'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Unsupervised Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-person-re-identification'},\n",
       "   {'task': 'Generalizable Person Re-identification',\n",
       "    'url': 'https://paperswithcode.com/task/generalizable-person-re-identification'},\n",
       "   {'task': 'Pose Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/pose-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Market-1501',\n",
       "   'DukeMTMC-reID->Market-1501',\n",
       "   'Market to MSMT',\n",
       "   'Market to Duke',\n",
       "   'Duke to Market'],\n",
       "  'num_papers': 628,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/viper',\n",
       "  'name': 'VIPeR',\n",
       "  'full_name': 'Viewpoint Invariant Pedestrian Recognition',\n",
       "  'homepage': 'https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/data/datasets/image/viper.py',\n",
       "  'description': 'The **Viewpoint Invariant Pedestrian Recognition** (**VIPeR**) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front), 45°, 90° (right), 135°, and 180° (back).\\r\\n\\r\\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)\\r\\n\\r\\nImage Source: [Qin et al](https://www.researchgate.net/figure/Some-examples-from-the-VIPeR-dataset-Each-column-is-one-of-632-same-person-example_fig8_331482460)',\n",
       "  'paper': {'title': 'Evaluating appearance models for recognition, reacquisition, and tracking',\n",
       "   'url': 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.331.7285&rep=rep1&type=pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Metric Learning',\n",
       "    'url': 'https://paperswithcode.com/task/metric-learning'},\n",
       "   {'task': 'Patch Matching',\n",
       "    'url': 'https://paperswithcode.com/task/patch-matching'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VIPeR'],\n",
       "  'num_papers': 127,\n",
       "  'data_loaders': [{'url': 'https://github.com/KaiyangZhou/deep-person-reid',\n",
       "    'repo': 'https://github.com/KaiyangZhou/deep-person-reid',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cuhk01',\n",
       "  'name': 'CUHK01',\n",
       "  'full_name': 'CUHK Person Re-identification',\n",
       "  'homepage': 'http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html',\n",
       "  'description': 'This dataset contains 971 identities from two disjoint camera views. Each identity has two samples per camera view. It is used for Person Re-identification.\\r\\n\\r\\nPaper: [Li W., Zhao R., Wang X. (2013) Human Reidentification with Transferred Metric Learning. In: Lee K.M., Matsushita Y., Rehg J.M., Hu Z. (eds) Computer Vision – ACCV 2012. ACCV 2012. Lecture Notes in Computer Science, vol 7724. Springer, Berlin, Heidelberg](https://doi.org/10.1007/978-3-642-37331-2_3)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUHK01'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/prid2011',\n",
       "  'name': 'PRID2011',\n",
       "  'full_name': 'Person RE-ID 2011',\n",
       "  'homepage': None,\n",
       "  'description': 'PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances\\n\\nSource: [PaMM: Pose-aware Multi-shot Matching for Improving Person Re-identification](https://arxiv.org/abs/1705.06011)',\n",
       "  'paper': {'title': 'Mask R-CNN',\n",
       "   'url': 'https://paperswithcode.com/paper/mask-r-cnn'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Unsupervised Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PRID2011'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': [{'url': 'https://github.com/Higashiguchi-Shingo/Siamese2',\n",
       "    'repo': 'https://github.com/Higashiguchi-Shingo/Siamese2',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cuhk03',\n",
       "  'name': 'CUHK03',\n",
       "  'full_name': 'Chinese University of Hong Kong Re-identification',\n",
       "  'homepage': 'http://www.ee.cuhk.edu.hk/~xgwang/CUHK_identification.html',\n",
       "  'description': 'The **CUHK03** consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training\\r\\n\\r\\nSource: [Attention Driven Person Re-identification](https://arxiv.org/abs/1810.05866)\\r\\n\\r\\nImage Source: [Person Re-Identification Techniques for Intelligent Video Surveillance Systems\\r\\n](https://www.researchgate.net/publication/324031366_Person_Re-Identification_Techniques_for_Intelligent_Video_Surveillance_Systems)',\n",
       "  'paper': {'title': 'DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification',\n",
       "   'url': 'https://paperswithcode.com/paper/deepreid-deep-filter-pairing-neural-network'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Face Sketch Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/face-sketch-synthesis'},\n",
       "   {'task': 'Defocus Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/defocus-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUHK - Blur Detection Dataset',\n",
       "   'CUHK',\n",
       "   'CUHK03 labeled',\n",
       "   'CUHK03 detected',\n",
       "   'CUHK03 (detected)',\n",
       "   'CUHK03'],\n",
       "  'num_papers': 337,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vehicleid',\n",
       "  'name': 'VehicleID',\n",
       "  'full_name': 'PKU VehicleID',\n",
       "  'homepage': 'https://www.pkuml.org/resources/pku-vehicleid.html',\n",
       "  'description': 'The “**VehicleID**” dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.“MINI-cooper”, “Audi A6L” and “BWM 1 Series”).\\r\\n\\r\\nSource: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)\\r\\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)',\n",
       "  'paper': {'title': 'Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-relative-distance-learning-tell-the'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Vehicle Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/vehicle-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VehicleID',\n",
       "   'VehicleID Small',\n",
       "   'VehicleID Medium',\n",
       "   'VehicleID Large'],\n",
       "  'num_papers': 95,\n",
       "  'data_loaders': [{'url': 'https://github.com/michuanhaohao/reid-strong-baseline',\n",
       "    'repo': 'https://github.com/michuanhaohao/reid-strong-baseline',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bp4d',\n",
       "  'name': 'BP4D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html',\n",
       "  'description': 'The **BP4D**-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches.\\r\\nThe database includes forty-one participants (23 women, 18 men). They were 18 – 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions.\\r\\nThe database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression).\\r\\n\\r\\nSource: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)\\r\\nImage Source: [http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)',\n",
       "  'paper': {'title': 'BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database',\n",
       "   'url': 'https://doi.org/10.1016/j.imavis.2014.06.002'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'},\n",
       "   {'task': 'Facial Action Unit Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-action-unit-detection'},\n",
       "   {'task': 'Action Unit Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-unit-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BP4D'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/disfa',\n",
       "  'name': 'DISFA',\n",
       "  'full_name': 'Denver Intensity of Spontaneous Facial Action',\n",
       "  'homepage': 'http://mohammadmahoor.com/disfa/',\n",
       "  'description': 'The **Denver Intensity of Spontaneous Facial Action** (**DISFA**) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all.\\r\\n\\r\\nSource: [Deep Learning For Smile Recognition](https://arxiv.org/abs/1602.00172)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237](https://www.researchgate.net/figure/Examples-of-images-extracted-from-the-DISFA-dataset_fig5_301830237)',\n",
       "  'paper': {'title': 'DISFA: A Spontaneous Facial Action Intensity Database',\n",
       "   'url': 'https://doi.org/10.1109/T-AFFC.2013.4'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'},\n",
       "   {'task': 'Smile Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/smile-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DISFA'],\n",
       "  'num_papers': 88,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cub-200-2011',\n",
       "  'name': 'CUB-200-2011',\n",
       "  'full_name': 'Caltech-UCSD Birds-200-2011',\n",
       "  'homepage': 'http://www.vision.caltech.edu/visipedia/CUB-200-2011.html',\n",
       "  'description': 'The **Caltech-UCSD Birds-200-2011** (**CUB-200-2011**) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from [Reed et al.]( https://paperswithcode.com/paper/learning-deep-representations-of-fine-grained). They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions.\\r\\n\\r\\nSource: [Fine-grained Visual-textual Representation Learning](https://arxiv.org/abs/1709.00340)\\r\\nImage Source: [http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)',\n",
       "  'paper': {'title': 'The Caltech-UCSD Birds-200-2011 Dataset',\n",
       "   'url': 'http://www.vision.caltech.edu/visipedia/CUB-200-2011.html'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Generalized Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-few-shot-learning'},\n",
       "   {'task': 'Document Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-text-classification'},\n",
       "   {'task': 'Small Data Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/small-data'},\n",
       "   {'task': 'Text-to-Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-image-generation'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'},\n",
       "   {'task': 'Multi-Modal Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-modal-document-classification'},\n",
       "   {'task': 'Multimodal Deep Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-deep-learning'},\n",
       "   {'task': 'Long-tail learning with class descriptors',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning-with-class-descriptors'},\n",
       "   {'task': 'Multimodal Text and Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-text-and-image-classification'},\n",
       "   {'task': 'Fine-Grained Image Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-recognition'},\n",
       "   {'task': 'Bird Species Classification With Audio-Visual Data',\n",
       "    'url': 'https://paperswithcode.com/task/bird-species-classification-with-audio-visual'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUB-200-2011, 10 samples per class',\n",
       "   'CUB-200-2011 5-way (5-shot)',\n",
       "   'CUB-200-2011 5-way (1-shot)',\n",
       "   'Imbalanced CUB-200-2011',\n",
       "   'CUB-200-2011, 5 samples per class',\n",
       "   'CUB-200-2011, 30 samples per class',\n",
       "   'CUB-LT',\n",
       "   'CUB-200-2011 - 0-Shot',\n",
       "   'CUB-200 - 0-Shot Learning',\n",
       "   'CUB Birds',\n",
       "   'CUB 200 50-way (0-shot)',\n",
       "   'CUB 200 5-way 5-shot',\n",
       "   'CUB 200 5-way 1-shot',\n",
       "   'CUB 128 x 128',\n",
       "   'CUB',\n",
       "   'CUB-200-2011'],\n",
       "  'num_papers': 1159,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/caltech_birds2011',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun397',\n",
       "  'name': 'SUN397',\n",
       "  'full_name': 'SUN397',\n",
       "  'homepage': 'https://vision.princeton.edu/projects/2010/SUN/',\n",
       "  'description': 'The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition.\\r\\n\\r\\nImage Source: [The Selection of Useful Visual Words in Class-Imbalanced Image Classification](https://www.semanticscholar.org/paper/The-Selection-of-Useful-Visual-Words-in-Image-Chimlek-Pramokchon/2b23cff4d2072dfc85cf8b09f54475791690a68d)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN397'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/sun397',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fewrel',\n",
       "  'name': 'FewRel',\n",
       "  'full_name': 'Few-Shot Relation Classification Dataset',\n",
       "  'homepage': 'http://www.zhuhao.me/fewrel/',\n",
       "  'description': 'The **FewRel** (**Few-Shot Relation Classification Dataset**) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations).\\r\\n\\r\\nSource: [Neural Snowball for Few-Shot Relation Learning](https://arxiv.org/abs/1908.11007)\\r\\nImage Source: [https://www.aclweb.org/anthology/D18-1514.pdf](https://www.aclweb.org/anthology/D18-1514.pdf)',\n",
       "  'paper': {'title': 'FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation',\n",
       "   'url': 'https://paperswithcode.com/paper/fewrel-a-large-scale-supervised-few-shot'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Relation Classification',\n",
       "    'url': 'https://paperswithcode.com/task/relation-classification'},\n",
       "   {'task': 'Few-Shot Relation Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-relation-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FewRel'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/few_rel',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dureader',\n",
       "  'name': 'DuReader',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://ai.baidu.com/broad/subordinate?dataset=dureader',\n",
       "  'description': '**DuReader** is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations – each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)\\r\\nImage Source: [https://arxiv.org/pdf/1711.05073v4.pdf](https://arxiv.org/pdf/1711.05073v4.pdf)',\n",
       "  'paper': {'title': 'DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications',\n",
       "   'url': 'https://paperswithcode.com/paper/dureader-a-chinese-machine-reading'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Reading Comprehension (Zero-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-zero-shot'},\n",
       "   {'task': 'Reading Comprehension (One-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-one-shot'},\n",
       "   {'task': 'Reading Comprehension (Few-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-few-shot'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['DuReader'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/searchqa',\n",
       "  'name': 'SearchQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/nyu-dl/dl4ir-searchQA',\n",
       "  'description': 'SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. \\r\\n\\r\\nSource: [SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine](https://arxiv.org/pdf/1704.05179.pdf)',\n",
       "  'paper': {'title': 'SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine',\n",
       "   'url': 'https://paperswithcode.com/paper/searchqa-a-new-qa-dataset-augmented-with'},\n",
       "  'introduced_date': '2017-04-18',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SearchQA'],\n",
       "  'num_papers': 95,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/search_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/nyu-dl/dl4ir-searchQA',\n",
       "    'repo': 'https://github.com/nyu-dl/dl4ir-searchQA',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/coqa',\n",
       "  'name': 'CoQA',\n",
       "  'full_name': 'Conversational Question Answering Challenge',\n",
       "  'homepage': 'https://stanfordnlp.github.io/coqa/',\n",
       "  'description': '**CoQA** is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.\\r\\n\\r\\nCoQA contains 127,000+ questions with answers collected from 8000+ conversations. Each conversation is collected by pairing two crowdworkers to chat about a passage in the form of questions and answers. The unique features of CoQA include 1) the questions are conversational; 2) the answers can be free-form text; 3) each answer also comes with an evidence subsequence highlighted in the passage; and 4) the passages are collected from seven diverse domains. CoQA has a lot of challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning.\\r\\n\\r\\nSource: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)\\r\\nImage Source: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)',\n",
       "  'paper': {'title': 'CoQA: A Conversational Question Answering Challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/coqa-a-conversational-question-answering'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Generative Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/generative-question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CoQA'],\n",
       "  'num_papers': 132,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/coqa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#conversational-question-answering-challenge',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/coqa',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/movieqa',\n",
       "  'name': 'MovieQA',\n",
       "  'full_name': 'MovieQA',\n",
       "  'homepage': 'http://movieqa.cs.toronto.edu/home/',\n",
       "  'description': 'The **MovieQA** dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS.\\r\\n\\r\\nSource: [Movie Question Answering: Remembering the Textual Cues for Layered Visual Contents](https://arxiv.org/abs/1804.09412)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716](https://www.researchgate.net/figure/Examples-of-multiple-choice-QA-from-the-MovieQA-dataset-Each-question-has-5_fig2_321379716)',\n",
       "  'paper': {'title': 'MovieQA: Understanding Stories in Movies through Question-Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/movieqa-understanding-stories-in-movies'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/video-question-answering'},\n",
       "   {'task': 'Video Story QA',\n",
       "    'url': 'https://paperswithcode.com/task/video-story-qa'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MovieQA'],\n",
       "  'num_papers': 61,\n",
       "  'data_loaders': [{'url': 'https://github.com/thanhdat77/videoquestionsansweringdataset',\n",
       "    'repo': 'https://github.com/thanhdat77/videoquestionsansweringdataset',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hutter-prize',\n",
       "  'name': 'Hutter Prize',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://prize.hutter1.net/',\n",
       "  'description': 'The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.\\r\\n\\r\\nSource: [NLP Progress](http://nlpprogress.com/english/language_modeling.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Hutter Prize'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/icdar-2013',\n",
       "  'name': 'ICDAR 2013',\n",
       "  'full_name': 'ICDAR 2013',\n",
       "  'homepage': 'https://rrc.cvc.uab.es/?ch=2',\n",
       "  'description': 'The **ICDAR 2013** dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection.\\r\\n\\r\\nSource: [Single Shot Text Detector with Regional Attention](https://arxiv.org/abs/1709.00138)\\r\\nImage Source: [https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856](https://plos.figshare.com/articles/Detection_examples_of_the_proposed_method_on_the_ICDAR_2013_dataset_17_/5325856)',\n",
       "  'paper': {'title': 'ICDAR 2013 Robust Reading Competition',\n",
       "   'url': 'https://doi.org/10.1109/ICDAR.2013.221'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'},\n",
       "   {'task': 'Scene Text Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-recognition'},\n",
       "   {'task': 'Table Detection',\n",
       "    'url': 'https://paperswithcode.com/task/table-detection'},\n",
       "   {'task': 'Handwritten Chinese Text Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/handwritten-chinese-text-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ICDAR 2013', 'ICDAR2013'],\n",
       "  'num_papers': 183,\n",
       "  'data_loaders': [{'url': 'https://github.com/tanglang96/DataLoaders_DALI',\n",
       "    'repo': 'https://github.com/tanglang96/DataLoaders_DALI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://mindee.github.io/doctr/latest/datasets.html#doctr.datasets.IC13',\n",
       "    'repo': 'https://github.com/mindee/doctr',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/visual-madlibs',\n",
       "  'name': 'Visual Madlibs',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://tamaraberg.com/visualmadlibs/',\n",
       "  'description': 'Visual Madlibs is a dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.\\r\\n\\r\\nSource: [Visual Madlibs: Fill in the blank Image Generation and Question Answering](/paper/visual-madlibs-fill-in-the-blank-image)\\r\\nImage Source: [Yu et al](https://arxiv.org/pdf/1506.00278v1.pdf)',\n",
       "  'paper': {'title': 'Visual Madlibs: Fill in the blank Image Generation and Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-madlibs-fill-in-the-blank-image'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Visual Madlibs'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/daquar',\n",
       "  'name': 'DAQUAR',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/',\n",
       "  'description': 'DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images.\\r\\n\\r\\nSource: [A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input](/paper/a-multi-world-approach-to-question-answering)\\r\\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)',\n",
       "  'paper': {'title': 'A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input',\n",
       "   'url': 'https://paperswithcode.com/paper/a-multi-world-approach-to-question-answering'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DAQUAR'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/visual7w',\n",
       "  'name': 'Visual7W',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://ai.stanford.edu/~yukez/visual7w/',\n",
       "  'description': '**Visual7W** is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.\\r\\n\\r\\nSource: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)\\r\\nImage Source: [http://ai.stanford.edu/~yukez/visual7w/](http://ai.stanford.edu/~yukez/visual7w/)',\n",
       "  'paper': {'title': 'Visual7W: Grounded Question Answering in Images',\n",
       "   'url': 'https://paperswithcode.com/paper/visual7w-grounded-question-answering-in'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Image Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/image-comprehension'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Visual7W'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fm-iqa',\n",
       "  'name': 'FM-IQA',\n",
       "  'full_name': 'Freestyle Multilingual Image Question Answering',\n",
       "  'homepage': 'http://research.baidu.com/Downloads',\n",
       "  'description': '**FM-IQA** is a question-answering dataset containing over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations.\\r\\n\\r\\nSource: [Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering](/paper/are-you-talking-to-a-machine-dataset-and)',\n",
       "  'paper': {'title': 'Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/are-you-talking-to-a-machine-dataset-and'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['FM-IQA'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/newsqa',\n",
       "  'name': 'NewsQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/project/newsqa-dataset/',\n",
       "  'description': 'The **NewsQA** dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs.\\r\\n\\r\\n* Documents are CNN news articles.\\r\\n* Questions are written by human users in natural language.\\r\\n* Answers may be multiword passages of the source text.\\r\\n* Questions may be unanswerable.\\r\\n* NewsQA is collected using a 3-stage, siloed process.\\r\\n* Questioners see only an article’s headline and highlights.\\r\\n* Answerers see the question and the full article, then select an answer passage.\\r\\n* Validators see the article, the question, and a set of answers that they rank.\\r\\n* NewsQA is more natural and more challenging than previous datasets.\\r\\n\\r\\nSource: [https://www.microsoft.com/en-us/research/project/newsqa-dataset/](https://www.microsoft.com/en-us/research/project/newsqa-dataset/)\\r\\nImage Source: [Trischler et al](https://arxiv.org/pdf/1611.09830v3.pdf)',\n",
       "  'paper': {'title': 'NewsQA: A Machine Comprehension Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/newsqa-a-machine-comprehension-dataset'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['NewsQA'],\n",
       "  'num_papers': 170,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/newsqa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/triviaqa',\n",
       "  'name': 'TriviaQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://nlp.cs.washington.edu/triviaqa/',\n",
       "  'description': '**TriviaQA** is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets.\\r\\n\\r\\nSource: [Episodic Memory Reader: Learning What to Rememberfor Question Answering from Streaming Data](https://arxiv.org/abs/1903.06164)\\r\\nImage Source: [Joshi et al](https://arxiv.org/pdf/1705.03551v2.pdf)',\n",
       "  'paper': {'title': 'TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension',\n",
       "   'url': 'https://paperswithcode.com/paper/triviaqa-a-large-scale-distantly-supervised'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Question Generation',\n",
       "    'url': 'https://paperswithcode.com/task/question-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TriviaQA', 'KILT: TriviaQA'],\n",
       "  'num_papers': 274,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/trivia_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#triviaqa',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/trivia_qa',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/rc/dataset_readers/triviaqa/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/recipeqa',\n",
       "  'name': 'RecipeQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://hucvl.github.io/recipeqa/',\n",
       "  'description': 'RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge.\\r\\n\\r\\nSource: [RecipeQA](https://hucvl.github.io/recipeqa/)',\n",
       "  'paper': {'title': 'RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes',\n",
       "   'url': 'https://paperswithcode.com/paper/recipeqa-a-challenge-dataset-for-multimodal'},\n",
       "  'introduced_date': '2018-09-04',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'},\n",
       "   {'task': 'Natural Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RecipeQA'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/squad',\n",
       "  'name': 'SQuAD',\n",
       "  'full_name': 'Stanford Question Answering Dataset',\n",
       "  'homepage': 'https://rajpurkar.github.io/SQuAD-explorer/',\n",
       "  'description': 'The **Stanford Question Answering Dataset** (**SQuAD**) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\\r\\n\\r\\nSource: [Deep Learning Based Text Classification: A Comprehensive Review](https://arxiv.org/abs/2004.03705)\\r\\nImage Source: [https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Prime_number.html)',\n",
       "  'paper': {'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text',\n",
       "   'url': 'https://paperswithcode.com/paper/squad-100000-questions-for-machine'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Question Generation',\n",
       "    'url': 'https://paperswithcode.com/task/question-generation'},\n",
       "   {'task': 'text2text-generation',\n",
       "    'url': 'https://paperswithcode.com/task/text2text-generation'}],\n",
       "  'languages': ['Arabic'],\n",
       "  'variants': ['squad_v2',\n",
       "   'SQuAD',\n",
       "   'SQuAD2.0 dev',\n",
       "   'SQuAD2.0',\n",
       "   'SQuAD1.1 dev',\n",
       "   'SQuAD1.1'],\n",
       "  'num_papers': 1247,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/qwant/squad_fr',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/squad',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/GEM/squad',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/squad_v2',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/GEM/squad_v2',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#squad',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#squad2',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/squad',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.SQuAD2',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.SQuAD1',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/rc/dataset_readers/squad/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/narrativeqa',\n",
       "  'name': 'NarrativeQA',\n",
       "  'full_name': 'NarrativeQA',\n",
       "  'homepage': 'https://deepmind.com/research/open-source/narrativeqa',\n",
       "  'description': 'The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers.\\r\\n\\r\\nSource: [DeepMind](https://deepmind.com/research/open-source/narrativeqa)\\r\\nImage Source: [Kočiský et al ](https://arxiv.org/pdf/1712.07040v1.pdf)',\n",
       "  'paper': {'title': 'The NarrativeQA Reading Comprehension Challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/the-narrativeqa-reading-comprehension'},\n",
       "  'introduced_date': '2017-12-19',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['NarrativeQA'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/narrativeqa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/narrativeqa_manual',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#narrativeqa',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/clicr',\n",
       "  'name': 'CliCR',\n",
       "  'full_name': 'CliCR',\n",
       "  'homepage': 'https://github.com/clips/clicr',\n",
       "  'description': 'CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports.\\r\\n\\r\\nSource: [CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension](https://arxiv.org/pdf/1803.09720v1.pdf)',\n",
       "  'paper': {'title': 'CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension',\n",
       "   'url': 'https://paperswithcode.com/paper/clicr-a-dataset-of-clinical-case-reports-for'},\n",
       "  'introduced_date': '2018-03-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CliCR'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': [{'url': 'https://github.com/clips/clicr',\n",
       "    'repo': 'https://github.com/clips/clicr',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ms-marco',\n",
       "  'name': 'MS MARCO',\n",
       "  'full_name': 'Microsoft Machine Reading Comprehension Dataset',\n",
       "  'homepage': 'https://microsoft.github.io/msmarco/',\n",
       "  'description': 'The **MS MARCO** (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search.\\r\\nThe first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search.\\r\\n\\r\\nSource: [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/)\\r\\nImage Source: [https://arxiv.org/pdf/1809.08267.pdf](https://arxiv.org/pdf/1809.08267.pdf)',\n",
       "  'paper': {'title': 'MS MARCO: A Human Generated MAchine Reading COmprehension Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/ms-marco-a-human-generated-machine-reading'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Passage Re-Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/passage-re-ranking'},\n",
       "   {'task': 'Passage Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/passage-retrieval'},\n",
       "   {'task': 'TREC 2019 Passage Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/trec-2019-passage-ranking'},\n",
       "   {'task': 'Passage Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/passage-ranking'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MS MARCO', 'MSMARCO', 'MSMARCO (BEIR)'],\n",
       "  'num_papers': 351,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/ms_marco',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#ms_marco',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/multirc',\n",
       "  'name': 'MultiRC',\n",
       "  'full_name': 'Multi-Sentence Reading Comprehension',\n",
       "  'homepage': 'https://cogcomp.seas.upenn.edu/multirc/',\n",
       "  'description': '**MultiRC** (**Multi-Sentence Reading Comprehension**) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph.\\r\\nThe dataset was designed with three key challenges in mind:\\r\\n* The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually.\\r\\n* The correct answer(s) is not required to be a span in the text.\\r\\n* The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets.\\r\\nThe entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time.\\r\\n\\r\\nSource: [https://cogcomp.seas.upenn.edu/multirc/](https://cogcomp.seas.upenn.edu/multirc/)\\r\\nImage Source: [https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/](https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set/)',\n",
       "  'paper': {'title': 'Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences',\n",
       "   'url': 'https://paperswithcode.com/paper/looking-beyond-the-surface-a-challenge-set'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MultiRC'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/eraser_multi_rc',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hotpotqa',\n",
       "  'name': 'HotpotQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://hotpotqa.github.io/',\n",
       "  'description': '**HotpotQA** is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. \\r\\n\\r\\nA diverse range of reasoning strategies are featured in HotpotQA, including questions involving missing entities in the question, intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the open-domain fullwiki setting, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1, and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.\\r\\n\\r\\nSource: [Answering Complex Open-domain Questions Through Iterative Query Generation](https://arxiv.org/abs/1910.07000)\\r\\nImage Source: [Yang et al](https://arxiv.org/pdf/1809.09600v1.pdf)',\n",
       "  'paper': {'title': 'HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/hotpotqa-a-dataset-for-diverse-explainable'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['HotpotQA', 'HotpotQA (BEIR)'],\n",
       "  'num_papers': 212,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/hotpot_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#hotpotqa',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/race',\n",
       "  'name': 'RACE',\n",
       "  'full_name': 'ReAding Comprehension dataset from Examinations',\n",
       "  'homepage': 'https://www.cs.cmu.edu/~glai1/data/race/',\n",
       "  'description': 'The **ReAding Comprehension dataset from Examinations** (**RACE**) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts.\\r\\n\\r\\nSource: [Dynamic Fusion Networks for Machine Reading Comprehension](https://arxiv.org/abs/1711.04964)\\r\\nImage Source: [Lai et al](https://arxiv.org/pdf/1704.04683v5.pdf)',\n",
       "  'paper': {'title': 'RACE: Large-scale ReAding Comprehension Dataset From Examinations',\n",
       "   'url': 'https://paperswithcode.com/paper/race-large-scale-reading-comprehension'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Distractor Generation',\n",
       "    'url': 'https://paperswithcode.com/task/distractor-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RACE'],\n",
       "  'num_papers': 210,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/race',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/race',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/quac',\n",
       "  'name': 'QuAC',\n",
       "  'full_name': 'Question Answering in Context',\n",
       "  'homepage': 'https://quac.ai/',\n",
       "  'description': 'Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text.\\r\\n\\r\\nSource: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)\\r\\nImage Source: [https://paperswithcode.com/paper/quac-question-answering-in-context-1/](https://paperswithcode.com/paper/quac-question-answering-in-context-1/)',\n",
       "  'paper': {'title': 'QuAC: Question Answering in Context',\n",
       "   'url': 'https://paperswithcode.com/paper/quac-question-answering-in-context-1'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['QuAC'],\n",
       "  'num_papers': 103,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/quac',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#question-answering-in-context',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/quac-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/quac',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/rc/dataset_readers/quac/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wizard-of-oz',\n",
       "  'name': 'Wizard-of-Oz',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://arxiv.org/pdf/1606.03777.pdf',\n",
       "  'description': 'The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.\\r\\n\\r\\nDescription from [NLP Progress](http://nlpprogress.com/english/dialogue.html)\\r\\n\\r\\nImage source: [Mrkšić et al.](https://arxiv.org/pdf/1606.03777.pdf)',\n",
       "  'paper': {'title': 'Neural Belief Tracker: Data-Driven Dialogue State Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/neural-belief-tracker-data-driven-dialogue'},\n",
       "  'introduced_date': '2016-06-12',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Dialogue State Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/dialogue-state-tracking'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Wizard-of-Oz'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/woz_dialogue',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#woz-restuarant-reservation-goal-oriented-dialogue',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vcr',\n",
       "  'name': 'VCR',\n",
       "  'full_name': 'Visual Commonsense Reasoning',\n",
       "  'homepage': 'https://visualcommonsense.com/',\n",
       "  'description': '**Visual Commonsense Reasoning** (**VCR**) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes.\\r\\n\\r\\nSource: [Visual Commonsense R-CNN](https://arxiv.org/abs/2002.12204)\\r\\nImage Source: [From Recognition to Cognition: Visual Commonsense Reasoning](https://paperswithcode.com/paper/from-recognition-to-cognition-visual/)',\n",
       "  'paper': {'title': 'From Recognition to Cognition: Visual Commonsense Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/from-recognition-to-cognition-visual'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Visual Commonsense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/visual-commonsense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['VCR',\n",
       "   'VCR (QA-R) test',\n",
       "   'VCR (QA-R) dev',\n",
       "   'VCR (Q-AR) test',\n",
       "   'VCR (Q-AR) dev',\n",
       "   'VCR (Q-A) test',\n",
       "   'VCR (Q-A) dev'],\n",
       "  'num_papers': 70,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/swag',\n",
       "  'name': 'SWAG',\n",
       "  'full_name': 'Situations With Adversarial Generations',\n",
       "  'homepage': 'https://rowanzellers.com/swag/',\n",
       "  'description': 'Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning.\\r\\n\\r\\nThe dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video; the three incorrect answers are adversarially generated and human verified, so as to fool machines but not humans. The authors aim for SWAG to be a benchmark for evaluating grounded commonsense NLI and for learning representations.\\r\\n\\r\\nSource: [SWAG](https://rowanzellers.com/swag/)\\r\\nImage Source: [Zellers et al](https://arxiv.org/pdf/1808.05326v1.pdf)',\n",
       "  'paper': {'title': 'SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference',\n",
       "   'url': 'https://paperswithcode.com/paper/swag-a-large-scale-adversarial-dataset-for'},\n",
       "  'introduced_date': '2018-08-16',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SWAG'],\n",
       "  'num_papers': 90,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/swag',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/swag-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/mc/dataset_readers/swag/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/event2mind',\n",
       "  'name': 'Event2Mind',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://uwnlp.github.io/event2mind/',\n",
       "  'description': 'Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations.\\r\\n\\r\\nSource: [Event2Mind: Commonsense Inference on Events, Intents, and Reactions](/paper/event2mind-commonsense-inference-on-events)',\n",
       "  'paper': {'title': 'Event2Mind: Commonsense Inference on Events, Intents, and Reactions',\n",
       "   'url': 'https://paperswithcode.com/paper/event2mind-commonsense-inference-on-events'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Event2Mind', 'Event2Mind dev', 'Event2Mind test'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/event2Mind',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/xnli',\n",
       "  'name': 'XNLI',\n",
       "  'full_name': 'Cross-lingual Natural Language Inference',\n",
       "  'homepage': 'https://github.com/facebookresearch/XNLI',\n",
       "  'description': 'The **Cross-lingual Natural Language Inference** (**XNLI**) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples.\\r\\n\\r\\nSource: [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894)\\r\\nImage Source: [https://github.com/facebookresearch/XNLI](https://github.com/facebookresearch/XNLI)',\n",
       "  'paper': {'title': 'XNLI: Evaluating Cross-lingual Sentence Representations',\n",
       "   'url': 'https://paperswithcode.com/paper/xnli-evaluating-cross-lingual-sentence'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Chinese Sentence Pair Classification',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-sentence-pair-classification'},\n",
       "   {'task': 'Cross-Lingual Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-natural-language-inference'}],\n",
       "  'languages': ['French',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Chinese',\n",
       "   'Multilingual',\n",
       "   'Russian',\n",
       "   'Arabic',\n",
       "   'Bulgarian',\n",
       "   'Hindi',\n",
       "   'Thai',\n",
       "   'Turkish',\n",
       "   'Urdu',\n",
       "   'Vietnamese',\n",
       "   'Greek',\n",
       "   'Swahili'],\n",
       "  'variants': ['XNLI Dev',\n",
       "   'XNLI',\n",
       "   'XNLI Zero-Shot English-to-Spanish',\n",
       "   'XNLI Zero-Shot English-to-German',\n",
       "   'XNLI Zero-Shot English-to-French',\n",
       "   'XNLI French',\n",
       "   'XNLI Chinese Dev',\n",
       "   'XNLI Chinese'],\n",
       "  'num_papers': 190,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/xnli',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/xnli',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/facebookresearch/XNLI',\n",
       "    'repo': 'https://github.com/facebookresearch/XNLI',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/snli',\n",
       "  'name': 'SNLI',\n",
       "  'full_name': 'Stanford Natural Language Inference',\n",
       "  'homepage': 'https://nlp.stanford.edu/projects/snli/.',\n",
       "  'description': 'The **SNLI** dataset (**Stanford Natural Language Inference**) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as “entailment”, “neutral”, “contradiction” or “-”, where “-” indicates that an agreement could not be reached.\\r\\n\\r\\nSource: [Breaking NLI Systemswith Sentences that Require Simple Lexical Inferences](https://arxiv.org/abs/1805.02266)',\n",
       "  'paper': {'title': 'A large annotated corpus for learning natural language inference',\n",
       "   'url': 'https://paperswithcode.com/paper/a-large-annotated-corpus-for-learning-natural'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Few-Shot NLI',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-nli'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SNLI', 'SNLI (8 training examples per class)'],\n",
       "  'num_papers': 846,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/snli',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/shibing624/nli_zh',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#the-stanford-natural-language-inference-snli-corpus',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/snli',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/pair_classification/dataset_readers/snli/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/scitail',\n",
       "  'name': 'SciTail',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://allenai.org/data/scitail',\n",
       "  'description': 'The **SciTail** dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label.\\r\\n\\r\\nSource: [Allen Institute for AI](https://allenai.org/data/scitail)\\r\\nImage source: [Allen Institute for AI](https://allenai.org/data/scitail)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SciTail'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/scitail',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/chb-mit',\n",
       "  'name': 'CHB-MIT',\n",
       "  'full_name': 'CHB-MIT Scalp EEG',\n",
       "  'homepage': 'https://physionet.org/content/chbmit/1.0.0/',\n",
       "  'description': 'The **CHB-MIT** dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings.\\r\\n\\r\\nSource: [Learning Robust Features using Deep Learning for Automatic Seizure Detection](https://arxiv.org/abs/1608.00220)\\nImage Source: [https://archive.physionet.org/pn6/chbmit/](https://archive.physionet.org/pn6/chbmit/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Medical', 'EEG'],\n",
       "  'tasks': [{'task': 'Seizure Detection',\n",
       "    'url': 'https://paperswithcode.com/task/seizure-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CHB-MIT'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/2010-i2b2-va',\n",
       "  'name': '2010 i2b2/VA',\n",
       "  'full_name': '2010 i2b2/VA',\n",
       "  'homepage': 'https://www.i2b2.org/NLP/Relations/',\n",
       "  'description': '**2010 i2b2/VA** is a biomedical dataset.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Clinical Concept Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/clinical-concept-extraction'},\n",
       "   {'task': 'Medical Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/medical-named-entity-recognition'},\n",
       "   {'task': 'Clinical Assertion Status Detection',\n",
       "    'url': 'https://paperswithcode.com/task/clinical-assertion-status-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['2010 i2b2/VA'],\n",
       "  'num_papers': 10,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/afw',\n",
       "  'name': 'AFW',\n",
       "  'full_name': 'Annotated Faces in the Wild',\n",
       "  'homepage': '',\n",
       "  'description': '**AFW** (**Annotated Faces in the Wild**) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box.\\r\\n\\r\\nSource: [Face detection, pose estimation, and landmark localization in the wild](https://ieeexplore.ieee.org/document/6248014)',\n",
       "  'paper': {'title': 'Face detection, pose estimation, and landmark localization in the wild',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6248014'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'},\n",
       "   {'task': 'Face Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AFW', 'Annotated Faces in the Wild'],\n",
       "  'num_papers': 143,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tempeval-3',\n",
       "  'name': 'TempEval-3',\n",
       "  'full_name': 'TempEval-3: events, times, and temporal relations',\n",
       "  'homepage': 'https://aclanthology.org/S13-2001/',\n",
       "  'description': 'Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems – in each task and in general.\\r\\n\\r\\nWe present TempEval-3 Silver data, with 666K words, and TempEval-3 Platinum, an evaluation set with 6K words. Documents are annotation with EVENT and TIMEX3 spans and also TLINKs, following the TimeML standard.',\n",
       "  'paper': {'title': 'TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations',\n",
       "   'url': 'https://paperswithcode.com/paper/tempeval-3-evaluating-events-time-expressions'},\n",
       "  'introduced_date': '2012-06-22',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Temporal Information Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-information-extraction'},\n",
       "   {'task': 'Temporal Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-tagging'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TempEval-3'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/timebank',\n",
       "  'name': 'TimeBank',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2006T08',\n",
       "  'description': 'Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. \\r\\n\\r\\nSource: [Enriching TimeBank: Towards a more precise annotation of temporal relations in a text](/paper/enriching-timebank-towards-a-more-precise)',\n",
       "  'paper': {'title': 'Enriching TimeBank: Towards a more precise annotation of temporal relations in a text',\n",
       "   'url': 'https://paperswithcode.com/paper/enriching-timebank-towards-a-more-precise'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Temporal Information Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-information-extraction'},\n",
       "   {'task': 'Timex normalization',\n",
       "    'url': 'https://paperswithcode.com/task/timex-normalization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TimeBank'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/semeval-2010-task-8',\n",
       "  'name': 'SemEval-2010 Task 8',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.kozareva.com/downloads.html',\n",
       "  'description': 'The dataset for the **SemEval-2010 Task 8** is a dataset for multi-way classification of mutually exclusive semantic relations between pairs of nominals.',\n",
       "  'paper': {'title': 'SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals',\n",
       "   'url': 'https://paperswithcode.com/paper/semeval-2010-task-8-multi-way-classification'},\n",
       "  'introduced_date': '2019-11-23',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SemEval-2010 Task 8'],\n",
       "  'num_papers': 96,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/sem_eval_2010_task_8',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/iemocap',\n",
       "  'name': 'IEMOCAP',\n",
       "  'full_name': 'The Interactive Emotional Dyadic Motion Capture\\xa0(IEMOCAP) Database',\n",
       "  'homepage': 'https://sail.usc.edu/iemocap/iemocap_publication.htm',\n",
       "  'description': 'Multimodal Emotion Recognition **IEMOCAP** The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers.\\r\\n\\r\\nSource: [Multi-attention Recurrent Network for Human Communication Comprehension](https://arxiv.org/abs/1802.00923)\\r\\nImage Source: [https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf](https://sail.usc.edu/iemocap/Busso_2008_iemocap.pdf)',\n",
       "  'paper': {'title': 'IEMOCAP: interactive emotional dyadic motion capture database',\n",
       "   'url': 'https://doi.org/10.1007/s10579-008-9076-6'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Audio'],\n",
       "  'tasks': [{'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'},\n",
       "   {'task': 'Speech Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-emotion-recognition'},\n",
       "   {'task': 'Multimodal Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-emotion-recognition'},\n",
       "   {'task': 'Speech Emotion Recognition - 5-Fold',\n",
       "    'url': 'https://paperswithcode.com/task/speech-emotion-recognition-5-fold'},\n",
       "   {'task': 'Low resource - Speech Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/low-resource-speech-emotion-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IEMOCAP'],\n",
       "  'num_papers': 320,\n",
       "  'data_loaders': [{'url': 'https://github.com/macaixia84/dialogue-generation',\n",
       "    'repo': 'https://github.com/macaixia84/dialogue-generation',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/charades-sta',\n",
       "  'name': 'Charades-STA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/jiyanggao/TALL',\n",
       "  'description': 'Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations.\\r\\n\\r\\nSource: [TALL: Temporal Activity Localization via Language Query](/paper/tall-temporal-activity-localization-via)',\n",
       "  'paper': {'title': 'TALL: Temporal Activity Localization via Language Query',\n",
       "   'url': 'https://paperswithcode.com/paper/tall-temporal-activity-localization-via'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Temporal Localization',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Charades-STA'],\n",
       "  'num_papers': 86,\n",
       "  'data_loaders': [{'url': 'https://github.com/jiyanggao/TALL',\n",
       "    'repo': 'https://github.com/jiyanggao/TALL',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/senteval',\n",
       "  'name': 'SentEval',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://arxiv.org/abs/1803.05449',\n",
       "  'description': 'SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders.\\r\\n\\r\\nSource: [SentEval: An Evaluation Toolkit for Universal Sentence Representations](/paper/senteval-an-evaluation-toolkit-for-universal)',\n",
       "  'paper': {'title': 'SentEval: An Evaluation Toolkit for Universal Sentence Representations',\n",
       "   'url': 'https://paperswithcode.com/paper/senteval-an-evaluation-toolkit-for-universal'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Linear-Probe Classification',\n",
       "    'url': 'https://paperswithcode.com/task/linear-probe-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SentEval'],\n",
       "  'num_papers': 98,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jfleg',\n",
       "  'name': 'JFLEG',\n",
       "  'full_name': 'JHU FLuency-Extended GUG corpus',\n",
       "  'homepage': 'https://github.com/keisks/jfleg',\n",
       "  'description': 'JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. \\r\\n\\r\\nSource: [JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction](https://arxiv.org/pdf/1702.04066v1.pdf)',\n",
       "  'paper': {'title': 'JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction',\n",
       "   'url': 'https://paperswithcode.com/paper/jfleg-a-fluency-corpus-and-benchmark-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Grammatical Error Correction',\n",
       "    'url': 'https://paperswithcode.com/task/grammatical-error-correction'},\n",
       "   {'task': 'Grammatical Error Detection',\n",
       "    'url': 'https://paperswithcode.com/task/grammatical-error-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JFLEG', 'Restricted', 'Unrestricted', '_Restricted_'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/jfleg',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/keisks/jfleg',\n",
       "    'repo': 'https://github.com/keisks/jfleg',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/esc-50',\n",
       "  'name': 'ESC-50',\n",
       "  'full_name': 'ESC-50',\n",
       "  'homepage': 'https://github.com/karolpiczak/ESC-50',\n",
       "  'description': 'The **ESC-50** dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org.\\r\\n\\r\\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\\r\\nImage Source: [https://github.com/karolpiczak/ESC-50](https://github.com/karolpiczak/ESC-50)',\n",
       "  'paper': {'title': 'ESC: Dataset for Environmental Sound Classification',\n",
       "   'url': 'https://doi.org/10.1145/2733373.2806390'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Audio Classification',\n",
       "    'url': 'https://paperswithcode.com/task/audio-classification'},\n",
       "   {'task': 'Environmental Sound Classification',\n",
       "    'url': 'https://paperswithcode.com/task/environmental-sound-classification'},\n",
       "   {'task': 'Self-Supervised Audio Classification',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-audio-classification'},\n",
       "   {'task': 'Zero-Shot Environment Sound Classification',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-environment-sound-classification'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ESC-50'],\n",
       "  'num_papers': 129,\n",
       "  'data_loaders': [{'url': 'https://github.com/karolpiczak/ESC-50',\n",
       "    'repo': 'https://github.com/karolpiczak/ESC-50',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/quora-question-pairs',\n",
       "  'name': 'Quora Question Pairs',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs',\n",
       "  'description': '**Quora Question Pairs** (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other.\\r\\n\\r\\nSource: [Bilateral Multi-Perspective Matching for Natural Language Sentences](/paper/bilateral-multi-perspective-matching-for)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Paraphrase Identification',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification'},\n",
       "   {'task': 'Community Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/community-question-answering'},\n",
       "   {'task': 'Paraphrase Generation',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-generation'},\n",
       "   {'task': 'Paraphrase Identification within Bi-Encoder',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification-within-bi-encoder'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Quora Question Pairs'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': [{'url': 'https://docs.allennlp.org/models/main/models/pair_classification/dataset_readers/quora_paraphrase/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mmi',\n",
       "  'name': 'MMI',\n",
       "  'full_name': 'MMI Facial Expression Database',\n",
       "  'homepage': 'https://mmifacedb.eu/',\n",
       "  'description': 'The **MMI** Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters.\\r\\n\\r\\nSource: [https://mmifacedb.eu/](https://mmifacedb.eu/)\\r\\nImage Source: [https://mmifacedb.eu/](https://mmifacedb.eu/)',\n",
       "  'paper': {'title': 'Web-based database for facial expression analysis',\n",
       "   'url': 'https://doi.org/10.1109/ICME.2005.1521424'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MMI'],\n",
       "  'num_papers': 51,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jaffe',\n",
       "  'name': 'JAFFE',\n",
       "  'full_name': 'Japanese Female Facial Expression',\n",
       "  'homepage': 'https://zenodo.org/record/3451524',\n",
       "  'description': 'The **JAFFE** dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators.\\r\\n\\r\\nSource: [Balanced k-Means and Min-Cut Clustering](https://arxiv.org/abs/1411.6235)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190](https://www.researchgate.net/figure/Examples-of-facial-expression-images-from-the-JAFFE-database_fig11_51873190)',\n",
       "  'paper': {'title': 'Coding Facial Expressions with Gabor Wavelets',\n",
       "   'url': 'http://www.kasrl.org/jaffe_download.html'},\n",
       "  'introduced_date': '1998-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'},\n",
       "   {'task': 'Clustering Algorithms Evaluation',\n",
       "    'url': 'https://paperswithcode.com/task/clustering-algorithms-evaluation'},\n",
       "   {'task': 'Image/Document Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/imagedocument-clustering'}],\n",
       "  'languages': ['Japanese'],\n",
       "  'variants': ['JAFFE'],\n",
       "  'num_papers': 67,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/oulu-casia',\n",
       "  'name': 'Oulu-CASIA',\n",
       "  'full_name': 'Oulu-CASIA NIR&VIS facial expression database',\n",
       "  'homepage': 'https://www.oulu.fi/cmvs/node/41316',\n",
       "  'description': 'The **Oulu-CASIA** NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 × 240 pixels.\\r\\n\\r\\nSource: [Facial expression recognition from near-infrared videos](https://ieeexplore.ieee.org/abstract/document/4761697)\\r\\nImage Source: [https://arxiv.org/abs/1712.03474](https://arxiv.org/abs/1712.03474)',\n",
       "  'paper': {'title': 'Facial expression recognition from near-infrared videos',\n",
       "   'url': 'https://doi.org/10.1016/j.imavis.2011.07.002'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Oulu-CASIA', 'CASIA NIR-VIS 2.0', 'Oulu-CASIA NIR-VIS'],\n",
       "  'num_papers': 65,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sfew',\n",
       "  'name': 'SFEW',\n",
       "  'full_name': 'Static Facial Expression in the Wild',\n",
       "  'homepage': 'https://cs.anu.edu.au/few/AFEW.html',\n",
       "  'description': 'The Static Facial Expressions in the Wild (**SFEW**) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer.\\r\\n\\r\\nSource: [Deep Facial Expression Recognition: A Survey](https://arxiv.org/abs/1804.08348)\\r\\nImage Source: [https://computervisiononline.com/dataset/1105138659](https://computervisiononline.com/dataset/1105138659)',\n",
       "  'paper': {'title': 'Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2011.6130508'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SFEW'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/atis',\n",
       "  'name': 'ATIS',\n",
       "  'full_name': 'Airline Travel Information Systems',\n",
       "  'homepage': 'https://github.com/howl-anderson/ATIS_dataset/blob/master/README.en-US.md',\n",
       "  'description': 'The **ATIS** (**Airline Travel Information Systems**) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively.\\r\\n\\r\\nSource: [Spoken Language Intent Detection using Confusion2Vec](https://arxiv.org/abs/1904.03576)',\n",
       "  'paper': {'title': 'The ATIS Spoken Language Systems Pilot Corpus',\n",
       "   'url': 'https://www.aclweb.org/anthology/H90-1021/'},\n",
       "  'introduced_date': '1990-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Intent Detection',\n",
       "    'url': 'https://paperswithcode.com/task/intent-detection'},\n",
       "   {'task': 'Slot Filling',\n",
       "    'url': 'https://paperswithcode.com/task/slot-filling'},\n",
       "   {'task': 'SQL Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/sql-parsing'},\n",
       "   {'task': 'Open Intent Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/open-intent-discovery'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ATIS'],\n",
       "  'num_papers': 169,\n",
       "  'data_loaders': [{'url': 'https://github.com/howl-anderson/ATIS_dataset',\n",
       "    'repo': 'https://github.com/howl-anderson/ATIS_dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/activitynet',\n",
       "  'name': 'ActivityNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://activity-net.org/',\n",
       "  'description': 'The **ActivityNet** dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public.\\r\\n\\r\\nSource: [Dynamic Temporal Pyramid Network: A Closer Look at Multi-Scale Modeling for Activity Detection](https://arxiv.org/abs/1808.02536)',\n",
       "  'paper': {'title': 'ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/activitynet-a-large-scale-video-benchmark-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-detection'},\n",
       "   {'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Weakly Supervised Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-action-localization'},\n",
       "   {'task': 'Temporal Action Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-action-proposal-generation'},\n",
       "   {'task': 'Few Shot Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-temporal-action-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ActivityNet',\n",
       "   'ActivityNet-Entities',\n",
       "   'ActivityNet-1.3',\n",
       "   'ActivityNet-1.2'],\n",
       "  'num_papers': 378,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/activitynet/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/msra-td500',\n",
       "  'name': 'MSRA-TD500',\n",
       "  'full_name': 'MSRA Text Detection 500 Database',\n",
       "  'homepage': 'http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500)',\n",
       "  'description': 'The **MSRA-TD500** dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text.\\r\\n\\r\\nSource: [Detecting Text in the Wild with Deep Character Embedding Network](https://arxiv.org/abs/1901.00363)\\r\\nImage Source: [http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg](http://www.iapr-tc11.org/mediawiki/images/MSRA-TD500_Example.jpg)',\n",
       "  'paper': {'title': 'Detecting texts of arbitrary orientations in natural images',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6247787'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['MSRA-TD500'],\n",
       "  'num_papers': 94,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/icdar-2015',\n",
       "  'name': 'ICDAR 2015',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://iapr.org/archives/icdar2015/index.html%3Fp=254.html',\n",
       "  'description': '**ICDAR 2015** was a scene text detection used for the ICDAR 2015 conference.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ICDAR 2015'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/total-text',\n",
       "  'name': 'Total-Text',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/cs-chan/Total-Text-Dataset',\n",
       "  'description': '**Total-Text** is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively.\\r\\n\\r\\nSource: [Convolutional Character Networks](https://arxiv.org/abs/1910.07954)\\r\\nImage Source: [https://github.com/cs-chan/Total-Text-Dataset](https://github.com/cs-chan/Total-Text-Dataset)',\n",
       "  'paper': {'title': 'Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/total-text-a-comprehensive-dataset-for-scene'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['Total-Text'],\n",
       "  'num_papers': 83,\n",
       "  'data_loaders': [{'url': 'https://github.com/cs-chan/Total-Text-Dataset',\n",
       "    'repo': 'https://github.com/cs-chan/Total-Text-Dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dota',\n",
       "  'name': 'DOTA',\n",
       "  'full_name': 'Dataset for Object deTection in Aerial Images',\n",
       "  'homepage': 'https://captain-whu.github.io/DOTA/',\n",
       "  'description': 'DOTA is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. The images are collected from different sensors and platforms. Each image is of the size in the range from 800 × 800 to 20,000 × 20,000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. The instances in DOTA images are annotated by experts in aerial image interpretation by arbitrary (8 d.o.f.) quadrilateral. We will continue to update DOTA, to grow in size and scope to reflect evolving real-world conditions. Now it has three versions:\\r\\n\\r\\nDOTA-v1.0 contains 15 common categories, 2,806 images and 188, 282 instances. The proportions of the training set, validation set, and testing set in DOTA-v1.0 are 1/2, 1/6, and 1/3, respectively.\\r\\n\\r\\nDOTA-v1.5 uses the same images as DOTA-v1.0, but the extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, ”container crane” is added. It contains 403,318 instances in total. The number of images and dataset splits are the same as DOTA-v1.0. This version was released for the DOAI Challenge 2019 on Object Detection in Aerial Images in conjunction with IEEE CVPR 2019.\\r\\n\\r\\nDOTA-v2.0 collects more Google Earth, GF-2 Satellite, and aerial images. There are 18 common categories, 11,268 images and 1,793,658 instances in DOTA-v2.0. Compared to DOTA-v1.5, it further adds the new categories of ”airport” and ”helipad”. The 11,268 images of DOTA are split into training, validation, test-dev, and test-challenge sets. To avoid the problem of overfitting, the proportion of training and validation set is smaller than the test set. Furthermore, we have two test sets, namely test-dev and test-challenge. Training contains 1,830 images and 268,627 instances. Validation contains 593 images and 81,048 instances. We released the images and ground truths for training and validation sets. Test-dev contains 2,792 images and 353,346 instances. We released the images but not the ground truths. Test-challenge contains 6,053 images and 1,090,637 instances.\\r\\n\\r\\nSource: [https://captain-whu.github.io/DOTA/index.html](https://captain-whu.github.io/DOTA/index.html)\\r\\nImage Source: [https://captain-whu.github.io/DOTA/](https://captain-whu.github.io/DOTA/)',\n",
       "  'paper': {'title': 'DOTA: A Large-scale Dataset for Object Detection in Aerial Images',\n",
       "   'url': 'https://paperswithcode.com/paper/dota-a-large-scale-dataset-for-object'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'One-stage Anchor-free Oriented Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/one-stage-anchor-free-oriented-object-1'},\n",
       "   {'task': 'Small Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/small-object-detection'},\n",
       "   {'task': 'Object Detection In Aerial Images',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection-in-aerial-images'},\n",
       "   {'task': 'Dense Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/dense-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DOTA', 'DOTA 1.5', 'DOTA 1.0'],\n",
       "  'num_papers': 113,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hrsc2016',\n",
       "  'name': 'HRSC2016',\n",
       "  'full_name': 'High resolution ship collections 2016',\n",
       "  'homepage': 'https://www.kaggle.com/guofeng/hrsc2016',\n",
       "  'description': 'High-resolution ship collections 2016 (HRSC2016) is a data set used for scientific research. Currently, all of the images in HRSC2016 were collected from Google Earth.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'One-stage Anchor-free Oriented Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/one-stage-anchor-free-oriented-object-1'},\n",
       "   {'task': 'Object Detection In Aerial Images',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection-in-aerial-images'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': [],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/shanghaitech',\n",
       "  'name': 'ShanghaiTech',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/desenzhou/ShanghaiTechDataset',\n",
       "  'description': 'The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai.\\r\\n\\r\\nSource: [Iterative Crowd Counting](https://arxiv.org/abs/1807.09959)\\r\\n\\r\\nImage Source: [Li et al](https://www.researchgate.net/figure/Test-images-from-the-ShanghaiTech-A-28-dataset-The-goal-of-this-paper-is-to-calculate_fig1_322652466)',\n",
       "  'paper': {'title': 'Single-Image Crowd Counting via Multi-Column Convolutional Neural Network',\n",
       "   'url': 'https://paperswithcode.com/paper/single-image-crowd-counting-via-multi-column-1'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Crowd Counting',\n",
       "    'url': 'https://paperswithcode.com/task/crowd-counting'},\n",
       "   {'task': 'Abnormal Event Detection In Video',\n",
       "    'url': 'https://paperswithcode.com/task/abnormal-event-detection-in-video'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ShanghaiTech', 'ShanghaiTech A', 'ShanghaiTech B'],\n",
       "  'num_papers': 163,\n",
       "  'data_loaders': [{'url': 'https://github.com/desenzhou/ShanghaiTechDataset',\n",
       "    'repo': 'https://github.com/desenzhou/ShanghaiTechDataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ucsd',\n",
       "  'name': 'UCSD',\n",
       "  'full_name': 'UCSD Anomaly Detection Dataset',\n",
       "  'homepage': 'http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm',\n",
       "  'description': 'The **UCSD** Anomaly Detection Dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either:\\r\\nthe circulation of non pedestrian entities in the walkways\\r\\nanomalous pedestrian motion patterns\\r\\nCommonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. A few instances of people in wheelchair were also recorded. All abnormalities are naturally occurring, i.e. they were not staged for the purposes of assembling the dataset. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames.\\r\\n\\r\\nSource: [The UCSD Anomaly Detection Dataset](http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm)\\r\\nImage Source: [http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf](http://www.svcl.ucsd.edu/publications/conference/2010/cvpr2010/cvpr_anomaly_2010.pdf)',\n",
       "  'paper': {'title': 'Anomaly detection in crowded scenes',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2010.5539872'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Multimodal Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-activity-recognition'},\n",
       "   {'task': 'Abnormal Event Detection In Video',\n",
       "    'url': 'https://paperswithcode.com/task/abnormal-event-detection-in-video'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCSD', 'UCSD-MIT Human Motion'],\n",
       "  'num_papers': 118,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dcase-2017',\n",
       "  'name': 'DCASE 2017',\n",
       "  'full_name': 'DCASE 2017',\n",
       "  'homepage': 'http://dcase.community/challenge2017/index',\n",
       "  'description': 'The **DCASE 2017** rare sound events dataset contains isolated sound events for three classes: 148 crying babies (mean duration 2.25s), 139 glasses breaking (mean duration 1.16s), and 187 gun shots (mean duration 1.32s). As with the DCASE 2016 data, silences are not excluded from active event markings in the annotations. While this data set contains many samples per class, there are only three classes\\n\\nSource: [The NIGENS General Sound Events Database](https://arxiv.org/abs/1902.08314)\\nImage Source: [https://arxiv.org/pdf/1911.06878.pdf](https://arxiv.org/pdf/1911.06878.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Sound Event Detection',\n",
       "    'url': 'https://paperswithcode.com/task/sound-event-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DCASE 2017'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pandora',\n",
       "  'name': 'PANDORA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://psy.takelab.fer.hr/datasets/all/pandora/',\n",
       "  'description': 'PANDORA is the first large-scale dataset of Reddit comments labeled with three personality models (including the well-established Big 5 model) and demographics (age, gender, and location) for more than 10k users.\\r\\n\\r\\nSource: [PANDORA Talks: Personality and Demographics on Reddit](https://arxiv.org/pdf/2004.04460)',\n",
       "  'paper': {'title': 'PANDORA Talks: Personality and Demographics on Reddit',\n",
       "   'url': 'https://paperswithcode.com/paper/pandora-talks-personality-and-demographics-on'},\n",
       "  'introduced_date': '2020-04-09',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PANDORA'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ava',\n",
       "  'name': 'AVA',\n",
       "  'full_name': 'Atomic Visual Actions',\n",
       "  'homepage': 'http://research.google.com/ava/',\n",
       "  'description': '**AVA** is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:\\r\\n\\r\\n- Kinetics (AVA-Kinetics) - a crossover between AVA and Kinetics. In order to provide localized action labels on a wider variety of visual scenes, authors provide AVA action labels on videos from Kinetics-700, nearly doubling the number of total annotations, and increasing the number of unique videos by over 500x. \\r\\n- Actions (AvA Actions) - the AVA dataset densely annotates 80 atomic visual actions in 430 15-minute movie clips, where actions are localized in space and time, resulting in 1.62M action labels with multiple labels per human occurring frequently. \\r\\n- Spoken Activity (AVA ActiveSpeaker, AVA Speech). AVA ActiveSpeaker: associates speaking activity with a visible face, on the AVA v1.0 videos, resulting in 3.65 million frames labeled across ~39K face tracks. AVA Speech densely annotates audio-based speech activity in AVA v1.0 videos, and explicitly labels 3 background noise conditions, resulting in ~46K labeled segments spanning 45 hours of data.\\r\\nImage Source: [https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png](https://www.researchgate.net/profile/Paolo_Napoletano/publication/309327222/figure/fig1/AS:419620126248965@1477056642346/Sample-images-from-the-Aesthetic-Visual-Analysis-AVA-database-sorted-by-their-aesthetic.png)',\n",
       "  'paper': {'title': 'AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions',\n",
       "   'url': 'https://paperswithcode.com/paper/ava-a-video-dataset-of-spatio-temporally'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-detection'},\n",
       "   {'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'},\n",
       "   {'task': 'Aesthetics Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/aesthetics-quality-assessment'},\n",
       "   {'task': 'Self-Supervised Learning',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-learning'},\n",
       "   {'task': 'Speaker Diarization',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-diarization'},\n",
       "   {'task': 'Audio-Visual Active Speaker Detection',\n",
       "    'url': 'https://paperswithcode.com/task/audio-visual-active-speaker-detection'},\n",
       "   {'task': 'Activity Detection',\n",
       "    'url': 'https://paperswithcode.com/task/activity-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AVA v2.1', 'AVA-ActiveSpeaker', 'AVA-Speech'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/ava/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/epic-kitchens',\n",
       "  'name': 'EPIC-KITCHENS-55',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://epic-kitchens.github.io/2019',\n",
       "  'description': 'The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. ‘open fridge‘). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits.\\r\\n\\r\\nSource: [Egocentric Hand Track and Object-based Human Action Recognition](https://arxiv.org/abs/1905.00742)\\r\\nImage Source: [https://epic-kitchens.github.io/2020-100](https://epic-kitchens.github.io/2020-100)',\n",
       "  'paper': {'title': 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/scaling-egocentric-vision-the-epic-kitchens'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'},\n",
       "   {'task': 'Egocentric Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/egocentric-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['EPIC-KITCHENS-55'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/charades',\n",
       "  'name': 'Charades',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://vuchallenge.org/charades.html',\n",
       "  'description': 'The **Charades** dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video.\\r\\n\\r\\nSource: [Temporal Reasoning Graph for Activity Recognition](https://arxiv.org/abs/1908.09995)',\n",
       "  'paper': {'title': 'Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/hollywood-in-homes-crowdsourcing-data'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Video Classification',\n",
       "    'url': 'https://paperswithcode.com/task/video-classification'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-detection'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Charades'],\n",
       "  'num_papers': 245,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/otb-2015',\n",
       "  'name': 'OTB-2015',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cvlab.hanyang.ac.kr/tracker_benchmark/',\n",
       "  'description': '**OTB-2015**, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking.\\r\\nImage Source: [http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html)',\n",
       "  'paper': {'title': 'Object Tracking Benchmark',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2014.2388226'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OTB-2015'],\n",
       "  'num_papers': 158,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/otb-2013',\n",
       "  'name': 'OTB-2013',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html',\n",
       "  'description': 'OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark.\\r\\n\\r\\nSource: [Marvasti-Zadeh](https://arxiv.org/abs/2004.01382)',\n",
       "  'paper': {'title': 'Online Object Tracking: A Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/online-object-tracking-a-benchmark'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OTB-2013'],\n",
       "  'num_papers': 108,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lasot',\n",
       "  'name': 'LaSOT',\n",
       "  'full_name': 'Large-scale Single Object Tracking',\n",
       "  'homepage': 'https://cis.temple.edu/lasot/',\n",
       "  'description': 'LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated\\r\\ntracking benchmark. The average video length of LaSOT\\r\\nis more than 2,500 frames, and each sequence comprises\\r\\nvarious challenges deriving from the wild where target objects may disappear and re-appear again in the view.',\n",
       "  'paper': {'title': 'LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/lasot-a-high-quality-benchmark-for-large'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LaSOT'],\n",
       "  'num_papers': 130,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmtracking/blob/master/docs/dataset.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmtracking',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/trackingnet',\n",
       "  'name': 'TrackingNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://tracking-net.org/',\n",
       "  'description': '**TrackingNet** is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames.\\r\\n\\r\\nSource: [Learning the Model Update for Siamese Trackers](https://arxiv.org/abs/1908.00855)\\r\\nImage Source: [https://arxiv.org/abs/1803.10794](https://arxiv.org/abs/1803.10794)',\n",
       "  'paper': {'title': 'TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/trackingnet-a-large-scale-dataset-and'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TrackingNet'],\n",
       "  'num_papers': 100,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vot2018',\n",
       "  'name': 'VOT2018',\n",
       "  'full_name': 'VOT2018',\n",
       "  'homepage': 'https://www.votchallenge.net/vot2018/dataset.html',\n",
       "  'description': '**VOT2018** is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets.\\r\\n\\r\\nSource: [Remove Cosine Window from Correlation Filter-based Visual Trackers: When and How](https://arxiv.org/abs/1905.06648)\\r\\nImage Source: [https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770](https://www.researchgate.net/figure/Screenshots-of-the-tracking-result-from-the-proposed-algorithm-from-VOT2018-dataset-bag_fig2_336038770)',\n",
       "  'paper': {'title': 'The Sixth Visual Object Tracking VOT2018 Challenge Results',\n",
       "   'url': 'https://doi.org/10.1007/978-3-030-11009-3_1'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VOT2018', 'VOT-2018'],\n",
       "  'num_papers': 106,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vot2017',\n",
       "  'name': 'VOT2017',\n",
       "  'full_name': 'Visual Object Tracking Challenge',\n",
       "  'homepage': 'https://www.votchallenge.net/vot2017/dataset.html',\n",
       "  'description': '**VOT2017** is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes.\\r\\n\\r\\nSource: [GradNet: Gradient-Guided Network for Visual Object Tracking](https://arxiv.org/abs/1909.06800)\\r\\nImage Source: [https://ieeexplore.ieee.org/document/8265440](https://ieeexplore.ieee.org/document/8265440)',\n",
       "  'paper': {'title': 'The Visual Object Tracking VOT2017 Challenge Results',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2017.230'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VOT2017/18', 'VOT2017'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ag-news',\n",
       "  'name': 'AG News',\n",
       "  'full_name': 'AG’s News Corpus',\n",
       "  'homepage': 'http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html',\n",
       "  'description': \"**AG News** (**AG’s News Corpus**) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1509.01626.pdf](https://arxiv.org/pdf/1509.01626.pdf)\",\n",
       "  'paper': {'title': 'Character-level Convolutional Networks for Text Classification',\n",
       "   'url': 'https://paperswithcode.com/paper/character-level-convolutional-networks-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Short Text Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/short-text-clustering'},\n",
       "   {'task': 'Semi-Supervised Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-text-classification-1'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['AG News', 'AG News (200 Labels)', 'ag_news'],\n",
       "  'num_papers': 309,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/ag_news',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/pietrolesci/ag_news',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/ag_news_subset',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.AG_NEWS',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dbpedia',\n",
       "  'name': 'DBpedia',\n",
       "  'full_name': 'DBpedia',\n",
       "  'homepage': 'https://wiki.dbpedia.org/datasets',\n",
       "  'description': '**DBpedia** (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets.\\r\\n\\r\\nSource: [https://en.wikipedia.org/wiki/DBpedia](https://en.wikipedia.org/wiki/DBpedia)',\n",
       "  'paper': {'title': 'DBpedia: A Nucleus for a Web of Open Data',\n",
       "   'url': 'https://doi.org/10.1007/978-3-540-76298-0_52'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Graphs'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Entity Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/entity-retrieval'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DBpedia', 'DBpedia (BEIR)', 'dbpedia_14'],\n",
       "  'num_papers': 404,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/dbpedia_14',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.DBpedia',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sst',\n",
       "  'name': 'SST',\n",
       "  'full_name': 'Stanford Sentiment Treebank',\n",
       "  'homepage': 'https://nlp.stanford.edu/sentiment',\n",
       "  'description': 'The **Stanford Sentiment Treebank** is a corpus with fully labeled parse trees that allows for a\\r\\ncomplete analysis of the compositional effects of\\r\\nsentiment in language. The corpus is based on\\r\\nthe dataset introduced by Pang and Lee (2005) and\\r\\nconsists of 11,855 single sentences extracted from\\r\\nmovie reviews. It was parsed with the Stanford\\r\\nparser and includes a total of 215,154 unique phrases\\r\\nfrom those parse trees, each annotated by 3 human judges.\\r\\n\\r\\nEach phrase is labelled as either *negative*, *somewhat negative*, *neutral*, *somewhat positive* or *positive*.\\r\\nThe corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (*negative* or *somewhat negative* vs *somewhat positive* or *positive* with *neutral* sentences discarded) refer to the dataset as SST-2 or SST binary.',\n",
       "  'paper': {'title': 'Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank',\n",
       "   'url': 'https://paperswithcode.com/paper/recursive-deep-models-for-semantic'},\n",
       "  'introduced_date': '2013-10-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SST',\n",
       "   'SST-5 Fine-grained classification',\n",
       "   'SST-2 Binary classification'],\n",
       "  'num_papers': 1078,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/sst',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#stanford-sentiment-treebank-dataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#sst-sentiment-analysis',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/classification/dataset_readers/stanford_sentiment_tree_bank/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/subj',\n",
       "  'name': 'SUBJ',\n",
       "  'full_name': 'Subjectivity dataset',\n",
       "  'homepage': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
       "  'description': 'Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\\r\\n\\r\\nSource: [SUBJ](http://www.cs.cornell.edu/people/pabo/movie-review-data/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Subjectivity Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/subjectivity-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUBJ'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/bdd100k',\n",
       "  'name': 'BDD100K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.bdd100k.com/',\n",
       "  'description': 'Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page.',\n",
       "  'paper': {'title': 'BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/bdd100k-a-diverse-driving-video-database-with'},\n",
       "  'introduced_date': '2020-04-08',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': 'Lane Detection',\n",
       "    'url': 'https://paperswithcode.com/task/lane-detection'},\n",
       "   {'task': 'Multiple Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multiple-object-tracking'},\n",
       "   {'task': 'Video Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-segmentation'},\n",
       "   {'task': 'Video Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-instance-segmentation'},\n",
       "   {'task': 'Steering Control',\n",
       "    'url': 'https://paperswithcode.com/task/steering-control'},\n",
       "   {'task': 'Multi-Object Tracking and Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking-and-segmentation'},\n",
       "   {'task': 'Multiple Object Track and Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/multiple-object-track-and-segmentation'},\n",
       "   {'task': 'Traffic Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/traffic-object-detection'},\n",
       "   {'task': 'Amodal Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/amodal-panoptic-segmentation'},\n",
       "   {'task': 'Semi-Supervised Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-instance-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BDD100K', 'BDD100K-APS'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/gta5',\n",
       "  'name': 'GTA5',\n",
       "  'full_name': 'Grand Theft Auto 5',\n",
       "  'homepage': 'https://arxiv.org/pdf/1608.02192v1.pdf',\n",
       "  'description': 'The **GTA5** dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game **Grand Theft Auto 5** and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset.\\r\\n\\r\\nSource: [Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/1909.00781)\\r\\nImage Source: [Richter et al](https://arxiv.org/pdf/1608.02192v1.pdf)',\n",
       "  'paper': {'title': 'Playing for Data: Ground Truth from Computer Games',\n",
       "   'url': 'https://paperswithcode.com/paper/playing-for-data-ground-truth-from-computer'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Synthetic-to-Real Translation',\n",
       "    'url': 'https://paperswithcode.com/task/synthetic-to-real-translation'},\n",
       "   {'task': 'One-shot Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-unsupervised-domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GTA5', 'GTAV-to-Cityscapes Labels', 'GTA5 to Cityscapes'],\n",
       "  'num_papers': 234,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/movielens',\n",
       "  'name': 'MovieLens',\n",
       "  'full_name': 'MovieLens',\n",
       "  'homepage': 'https://grouplens.org/datasets/movielens/',\n",
       "  'description': 'The **MovieLens** datasets, first released in 1998, describe people’s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 — a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations.\\r\\n\\r\\nSource: [The MovieLens Datasets: History and Context](http://files.grouplens.org/papers/harper-tiis2015.pdf)\\r\\nImage Source: [http://files.grouplens.org/papers/harper-tiis2015.pdf](http://files.grouplens.org/papers/harper-tiis2015.pdf)',\n",
       "  'paper': {'title': 'The MovieLens Datasets: History and Context',\n",
       "   'url': 'https://doi.org/10.1145/2827872'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Tabular'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Click-Through Rate Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/click-through-rate-prediction'},\n",
       "   {'task': 'Knowledge Graph Completion',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graph-completion'},\n",
       "   {'task': 'Recommendation Systems (Item cold-start)',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems-item-cold-start'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MovieLens 25M',\n",
       "   'MovieLens',\n",
       "   'MovieLens-Latest',\n",
       "   'MovieLens 20M',\n",
       "   'MovieLens 1M',\n",
       "   'MovieLens 10M',\n",
       "   'MovieLens 100K'],\n",
       "  'num_papers': 645,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/movielens',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/middlebury',\n",
       "  'name': 'Middlebury',\n",
       "  'full_name': 'Middlebury Stereo',\n",
       "  'homepage': 'https://vision.middlebury.edu/stereo/data/',\n",
       "  'description': 'The **Middlebury** Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors.\\r\\n\\r\\nSource: [https://vision.middlebury.edu/stereo/data/](https://vision.middlebury.edu/stereo/data/)\\r\\nImage Source: [https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625](https://www.researchgate.net/figure/The-stereo-matching-results-on-the-Middlebury-dataset-From-left-to-right-each-set-of_fig3_273399625)',\n",
       "  'paper': {'title': 'A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms',\n",
       "   'url': 'https://doi.org/10.1023/A:1014573219977'},\n",
       "  'introduced_date': '2002-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Stereo'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Video Frame Interpolation',\n",
       "    'url': 'https://paperswithcode.com/task/video-frame-interpolation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Middlebury',\n",
       "   'Middlebury - 2x upscaling',\n",
       "   'Middlebury - 4x upscaling'],\n",
       "  'num_papers': 163,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/v-coco',\n",
       "  'name': 'V-COCO',\n",
       "  'full_name': 'Verbs in COCO',\n",
       "  'homepage': 'https://github.com/s-gupta/v-coco',\n",
       "  'description': '**Verbs in COCO** (**V-COCO**) is a dataset that builds off COCO for human-object interaction detection. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.\\r\\n\\r\\nSource: [Visual Compositional Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2007.12407)\\r\\nImage Source: [https://www.researchgate.net/figure/Pose-estimation-and-action-recognition-results-on-the-V-COCO-Dataset-16-which-has_fig9_339477856](https://www.researchgate.net/figure/Pose-estimation-and-action-recognition-results-on-the-V-COCO-Dataset-16-which-has_fig9_339477856)',\n",
       "  'paper': {'title': 'Visual Semantic Role Labeling',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-semantic-role-labeling'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Human-Object Interaction Detection',\n",
       "    'url': 'https://paperswithcode.com/task/human-object-interaction-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['V-COCO'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': [{'url': 'https://github.com/s-gupta/v-coco',\n",
       "    'repo': 'https://github.com/s-gupta/v-coco',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hico-det',\n",
       "  'name': 'HICO-DET',\n",
       "  'full_name': 'HICO-DET',\n",
       "  'homepage': 'http://www-personal.umich.edu/~ywchao/hico/',\n",
       "  'description': '**HICO-DET** is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects.\\r\\n\\r\\nSource: [Visual Compositional Learning for Human-Object Interaction Detection](https://arxiv.org/abs/2007.12407)\\r\\nImage Source: [http://www-personal.umich.edu/~ywchao/hico/](http://www-personal.umich.edu/~ywchao/hico/)',\n",
       "  'paper': {'title': 'Learning to Detect Human-Object Interactions',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-detect-human-object-interactions'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Human-Object Interaction Detection',\n",
       "    'url': 'https://paperswithcode.com/task/human-object-interaction-detection'},\n",
       "   {'task': 'Zero-Shot Human-Object Interaction Detection',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-human-object-interaction-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HICO-DET'],\n",
       "  'num_papers': 85,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/utd-mhad',\n",
       "  'name': 'UTD-MHAD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.utdallas.edu/~kehtar/UTD-MHAD.html',\n",
       "  'description': 'The **UTD-MHAD** dataset consists of 27 different actions performed by 8 subjects. Each subject repeated the action for 4 times, resulting in 861 action sequences in total. The RGB, depth, skeleton and the inertial sensor signals were recorded.\\r\\n\\r\\nSource: [Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition](https://arxiv.org/abs/1711.05941)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-shots-of-the-27-actions-in-the-UTD-MHAD-database_fig12_283090976](https://www.researchgate.net/figure/Sample-shots-of-the-27-actions-in-the-UTD-MHAD-database_fig12_283090976)',\n",
       "  'paper': {'title': 'UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor',\n",
       "   'url': 'https://doi.org/10.1109/ICIP.2015.7350781'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Multimodal Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UTD-MHAD'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpii',\n",
       "  'name': 'MPII',\n",
       "  'full_name': 'MPII Human Pose',\n",
       "  'homepage': 'http://human-pose.mpi-inf.mpg.de/',\n",
       "  'description': 'The **MPII** Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints.\\r\\n\\r\\nSource: [2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning](https://arxiv.org/abs/1802.09232)\\r\\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)',\n",
       "  'paper': {'title': '2D Human Pose Estimation: New Benchmark and State of the Art Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/2d-human-pose-estimation-new-benchmark-and'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Keypoint Detection',\n",
       "    'url': 'https://paperswithcode.com/task/keypoint-detection'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-person-pose-estimation'}],\n",
       "  'languages': ['Turkish'],\n",
       "  'variants': ['MPII', 'MPII Multi-Person', 'MPII Single Person'],\n",
       "  'num_papers': 356,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#mpii',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kinetics',\n",
       "  'name': 'Kinetics',\n",
       "  'full_name': 'Kinetics Human Action Video Dataset',\n",
       "  'homepage': 'https://deepmind.com/research/open-source/kinetics',\n",
       "  'description': 'The **Kinetics** dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube.\\r\\n\\r\\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)',\n",
       "  'paper': {'title': 'The Kinetics Human Action Video Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/the-kinetics-human-action-video-dataset'},\n",
       "  'introduced_date': '2017-05-19',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Video Classification',\n",
       "    'url': 'https://paperswithcode.com/task/video-classification'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'},\n",
       "   {'task': 'Video Generation',\n",
       "    'url': 'https://paperswithcode.com/task/video-generation'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Self-Supervised Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-action-recognition'},\n",
       "   {'task': 'Zero-Shot Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-action-recognition'},\n",
       "   {'task': 'Semantic Object Interaction Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-object-interaction-classification'},\n",
       "   {'task': 'Boundary Detection',\n",
       "    'url': 'https://paperswithcode.com/task/boundary-detection'},\n",
       "   {'task': 'Spatio-Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/spatio-temporal-action-localization'},\n",
       "   {'task': 'Event Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/event-segmentation'},\n",
       "   {'task': 'Text-to-Video Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-video-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AVA-Kinetics',\n",
       "   'Kinetics-400',\n",
       "   'Kinetics-600 48 frames, 64x64',\n",
       "   'Kinetics-600 12 frames, 64x64',\n",
       "   'Kinetics-600 12 frames, 128x128',\n",
       "   'Kinetics-700',\n",
       "   'Kinetics-600',\n",
       "   'Kinetics'],\n",
       "  'num_papers': 654,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.Kinetics400',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/msrc-12',\n",
       "  'name': 'MSRC-12',\n",
       "  'full_name': 'MSRC-12 Kinect Gesture Dataset',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/download/details.aspx?id=52283&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fcambridge%2Fprojects%2Fmsrc12%2F',\n",
       "  'description': 'The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. The data set includes 594 sequences and 719,359 frames—approximately six hours and 40 minutes—collected from 30 people performing 12 gestures. In total, there are 6,244 gesture instances. The motion files contain tracks of 20 joints estimated using the Kinect Pose Estimation pipeline. The body poses are captured at a sample rate of 30Hz with an accuracy of about two centimeters in joint positions.\\r\\n\\r\\nSource: [https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/](https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/)',\n",
       "  'paper': {'title': 'Instructing people for training gestural interactive systems',\n",
       "   'url': 'https://doi.org/10.1145/2207676.2208303'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/gesture-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'}],\n",
       "  'languages': ['Spanish'],\n",
       "  'variants': ['MSRC-12'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/timit',\n",
       "  'name': 'TIMIT',\n",
       "  'full_name': 'TIMIT Acoustic-Phonetic Continuous Speech Corpus',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC93S1',\n",
       "  'description': 'The **TIMIT** Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech.\\r\\n\\r\\nSource: [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/abs/1207.0580)\\r\\nImage Source: [https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/](https://roboticrun.wordpress.com/2016/06/21/timit-introduction-the-official-doc/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Speech'],\n",
       "  'tasks': [{'task': 'Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TIMIT'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/timit_asr',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/volleyball',\n",
       "  'name': 'Volleyball',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/mostafa-saad/deep-activity-rec#dataset',\n",
       "  'description': '**Volleyball** is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations.\\r\\n\\r\\nSource: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)\\r\\nImage Source: [https://github.com/mostafa-saad/deep-activity-rec#dataset](https://github.com/mostafa-saad/deep-activity-rec#dataset)',\n",
       "  'paper': {'title': 'A Hierarchical Deep Temporal Model for Group Activity Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/a-hierarchical-deep-temporal-model-for-group'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Group Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/group-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Volleyball'],\n",
       "  'num_papers': 41,\n",
       "  'data_loaders': [{'url': 'https://github.com/mostafa-saad/deep-activity-rec',\n",
       "    'repo': 'https://github.com/mostafa-saad/deep-activity-rec',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/collective-activity',\n",
       "  'name': 'Collective Activity',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://vhosts.eecs.umich.edu/vision//activity-dataset.html',\n",
       "  'description': 'The **Collective Activity** Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.\\r\\n\\r\\nSource: [http://vhosts.eecs.umich.edu/vision//activity-dataset.html](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)\\r\\nImage Source: [http://vhosts.eecs.umich.edu/vision//activity-dataset.html](http://vhosts.eecs.umich.edu/vision//activity-dataset.html)',\n",
       "  'paper': {'title': 'Improved Actor Relation Graph based Group Activity Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/video-understanding-based-on-human-action-and'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Group Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/group-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Collective Activity'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mot16',\n",
       "  'name': 'MOT16',\n",
       "  'full_name': 'Multiple Object Tracking 2016',\n",
       "  'homepage': 'https://motchallenge.net/data/MOT16/',\n",
       "  'description': 'The **MOT16** dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful.\\r\\n\\r\\nSource: [SOT for MOT](https://arxiv.org/abs/1712.01059)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746](https://www.researchgate.net/figure/Sample-results-on-the-sequence-MOT16-07-encoded-as-in-the-previous-figure-Table-1_fig3_309641746)',\n",
       "  'paper': {'title': 'MOT16: A Benchmark for Multi-Object Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/mot16-a-benchmark-for-multi-object-tracking'},\n",
       "  'introduced_date': '2016-03-02',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Online Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/online-multi-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MOT16'],\n",
       "  'num_papers': 107,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmtracking/blob/master/docs/dataset.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmtracking',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nus-wide',\n",
       "  'name': 'NUS-WIDE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://lms.comp.nus.edu.sg/wp-content/uploads/2019/research/nuswide/NUS-WIDE.html',\n",
       "  'description': 'The **NUS-WIDE** dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes.\\r\\n\\r\\nSource: [Parallel Grid Pooling for Data Augmentation](https://arxiv.org/abs/1803.11370)\\r\\n\\r\\nImage Source: [Li et al](https://www.researchgate.net/publication/273063258_Data_Clustering_Using_Side_Information_Dependent_Chinese_Restaurant_Processes)',\n",
       "  'paper': {'title': 'NUS-WIDE: a real-world web image database from National University of Singapore',\n",
       "   'url': 'https://doi.org/10.1145/1646396.1646452'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Multi-label zero-shot learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-zero-shot-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NUS-WIDE'],\n",
       "  'num_papers': 230,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-voc-2007',\n",
       "  'name': 'PASCAL VOC 2007',\n",
       "  'full_name': 'PASCAL VOC 2007',\n",
       "  'homepage': 'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/',\n",
       "  'description': '**PASCAL VOC 2007** is a dataset for image recognition. The twenty object classes that have been selected are:\\r\\n\\r\\nPerson: person\\r\\nAnimal: bird, cat, cow, dog, horse, sheep\\r\\nVehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\\r\\nIndoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\\r\\n\\r\\nThe dataset can be used for image classification and object detection tasks.\\r\\n\\r\\nImage Source: [Object Detection and Recognition in Images](https://arxiv.org/abs/1708.01241)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Open World Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/open-world-object-detection'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Object Counting',\n",
       "    'url': 'https://paperswithcode.com/task/object-counting'},\n",
       "   {'task': 'Zero-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-object-detection'},\n",
       "   {'task': 'Real-Time Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-object-detection'},\n",
       "   {'task': 'Robust Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/robust-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL VOC 2007', 'Pascal VOC 2007 count-test'],\n",
       "  'num_papers': 86,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/voc',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/celeba-hq',\n",
       "  'name': 'CelebA-HQ',\n",
       "  'full_name': 'CelebA-HQ',\n",
       "  'homepage': 'https://github.com/tkarras/progressive_growing_of_gans',\n",
       "  'description': 'The **CelebA-HQ** dataset is a high-quality version of CelebA that consists of 30,000 images at 1024×1024 resolution.\\r\\n\\r\\nSource: [IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis](https://arxiv.org/abs/1807.06358)\\r\\n\\r\\nImage Source: [https://github.com/tkarras/progressive_growing_of_gans](https://github.com/tkarras/progressive_growing_of_gans)',\n",
       "  'paper': {'title': 'Progressive Growing of GANs for Improved Quality, Stability, and Variation',\n",
       "   'url': 'https://paperswithcode.com/paper/progressive-growing-of-gans-for-improved'},\n",
       "  'introduced_date': '2017-10-27',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Multimodal Unsupervised Image-To-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-unsupervised-image-to-image'},\n",
       "   {'task': 'Image Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/image-quality-assessment'},\n",
       "   {'task': 'Blind Face Restoration',\n",
       "    'url': 'https://paperswithcode.com/task/blind-face-restoration'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CelebA-Test',\n",
       "   'CelebA-HQ 256x256',\n",
       "   'CelebA-HQ 1024x1024',\n",
       "   'Celeb-HQ 4x upscaling',\n",
       "   'CelebA-HQ 64x64',\n",
       "   'CelebA-HQ 128x128',\n",
       "   'CelebA-HQ'],\n",
       "  'num_papers': 398,\n",
       "  'data_loaders': [{'url': 'https://github.com/tkarras/progressive_growing_of_gans',\n",
       "    'repo': 'https://github.com/tkarras/progressive_growing_of_gans',\n",
       "    'frameworks': ['tf']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/celeb_a_hq',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/gtea',\n",
       "  'name': 'GTEA',\n",
       "  'full_name': 'Georgia Tech Egocentric Activity',\n",
       "  'homepage': 'http://cbs.ic.gatech.edu/fpv/',\n",
       "  'description': 'The Georgia Tech Egocentric Activities (**GTEA**) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute.\\r\\n\\r\\nSource: [TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation](https://arxiv.org/abs/1705.07818)\\r\\nImage Source: [http://cbs.ic.gatech.edu/fpv/](http://cbs.ic.gatech.edu/fpv/)',\n",
       "  'paper': {'title': 'Learning to recognize objects in egocentric activities',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2011.5995444'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Weakly Supervised Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-action-localization'},\n",
       "   {'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'},\n",
       "   {'task': 'Fine-Grained Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-action-detection'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['GTEA'],\n",
       "  'num_papers': 57,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/50-salads',\n",
       "  'name': '50 Salads',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cvip.computing.dundee.ac.uk/datasets/foodpreparation/50salads/',\n",
       "  'description': 'Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures – characterized by interactions between hands, tools, and manipulable objects – frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation.\\r\\n\\r\\nThe dataset includes\\r\\n\\r\\n    RGB video data 640×480 pixels at 30 Hz\\r\\n    Depth maps 640×480 pixels at 30 Hz\\r\\n    3-axis accelerometer data at 50 Hz of devices attached to a knife, a mixing spoon, a small spoon, a peeler, a glass, an oil bottle, and a pepper dispenser.\\r\\n    Synchronization parameters for temporal alignment of video and accelerometer data\\r\\n    Annotations as temporal intervals of pre- core- and post-phases of activities corresponding to steps in a recipe',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-09-08',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['50 Salads'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/div2k',\n",
       "  'name': 'DIV2K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/DIV2K/',\n",
       "  'description': '**DIV2K** is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild ×4 and realistic wild ×4 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image.\\r\\n\\r\\nSource: [Unsupervised Image Super-Resolution with an Indirect Supervised Path](https://arxiv.org/abs/1910.02593)',\n",
       "  'paper': {'title': 'NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2017.150'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'JPEG Artifact Correction',\n",
       "    'url': 'https://paperswithcode.com/task/jpeg-artifact-correction'},\n",
       "   {'task': 'Jpeg Compression Artifact Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/jpeg-compression-artifact-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DIV2K',\n",
       "   'DIV2K val - 16x upscaling',\n",
       "   'DIV2K val - 2x upscaling',\n",
       "   'DIV2K val - 4x upscaling'],\n",
       "  'num_papers': 251,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/div2k',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'repo': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/maestro',\n",
       "  'name': 'MAESTRO',\n",
       "  'full_name': 'MAESTRO',\n",
       "  'homepage': 'https://magenta.tensorflow.org/datasets/maestro',\n",
       "  'description': 'The **MAESTRO** dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with ∼3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1–48 kHz 16-bit PCM stereo).\\r\\n\\r\\nSource: [https://magenta.tensorflow.org/datasets/maestro](https://magenta.tensorflow.org/datasets/maestro)\\r\\nImage Source: [https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458](https://www.researchgate.net/figure/Results-generated-with-the-MAESTRO-dataset_fig3_333392458)',\n",
       "  'paper': {'title': 'Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/enabling-factorized-piano-music-modeling-and'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Interactive', 'Music'],\n",
       "  'tasks': [{'task': 'Audio Generation',\n",
       "    'url': 'https://paperswithcode.com/task/audio-generation'},\n",
       "   {'task': 'Music Transcription',\n",
       "    'url': 'https://paperswithcode.com/task/music-transcription'},\n",
       "   {'task': 'Music Generation',\n",
       "    'url': 'https://paperswithcode.com/task/music-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MAESTRO'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/casia-b',\n",
       "  'name': 'CASIA-B',\n",
       "  'full_name': 'CASIA-B',\n",
       "  'homepage': 'http://www.cbsr.ia.ac.cn/english/Gait%20Databases.asp',\n",
       "  'description': \"CASIA-B is a large multiview gait database, which is created in January 2005. There are 124 subjects, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered. Besides the video files, we still provide human silhouettes extracted from video files. The detailed information about Dataset B and an evaluation framework can be found in this paper .\\r\\n\\r\\nThe format of the video filename in Dataset B is 'xxx-mm-nn-ttt.avi', where\\r\\n\\r\\n    xxx: subject id, from 001 to 124.\\r\\n    mm: walking status, can be 'nm' (normal), 'cl' (in a coat) or 'bg' (with a bag).\\r\\n    nn: sequence number.\\r\\n    ttt: view angle, can be '000', '018', ..., '180'.\",\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Multiview Gait Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multiview-gait-recognition'},\n",
       "   {'task': 'Gait Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/gait-recognition'},\n",
       "   {'task': 'Gait Identification',\n",
       "    'url': 'https://paperswithcode.com/task/gait-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CASIA-B'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aflw',\n",
       "  'name': 'AFLW',\n",
       "  'full_name': 'Annotated Facial Landmarks in the Wild',\n",
       "  'homepage': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
       "  'description': 'The **Annotated Facial Landmarks in the Wild** (**AFLW**) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image.\\r\\n\\r\\nSource: [Nose, Eyes and Ears: Head Pose Estimation by Locating Facial Keypoints](https://arxiv.org/abs/1812.00739)',\n",
       "  'paper': {'title': 'Annotated Facial Landmarks in the Wild: A large-scale, real-world database for facial landmark localization',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2011.6130513'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'},\n",
       "   {'task': 'Low-Light Image Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/low-light-image-enhancement'},\n",
       "   {'task': 'Head Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/head-pose-estimation'},\n",
       "   {'task': 'Unsupervised Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-facial-landmark-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AFLW-PIFA (34 points)',\n",
       "   'AFLW-PIFA (21 points)',\n",
       "   'AFLW-MTFL',\n",
       "   'AFLW-LFPA',\n",
       "   'AFLW-Full',\n",
       "   'AFLW-Front',\n",
       "   'AFLW (Zhang CVPR 2018 crops)',\n",
       "   'AFLW'],\n",
       "  'num_papers': 125,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_face_keypoint.md#aflw-dataset',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/biwi',\n",
       "  'name': 'BIWI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.kaggle.com/kmader/biwi-kinect-head-pose-database',\n",
       "  'description': 'The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation.\\r\\n\\r\\nSource: [https://www.kaggle.com/kmader/biwi-kinect-head-pose-database](https://www.kaggle.com/kmader/biwi-kinect-head-pose-database)\\r\\nImage Source: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)',\n",
       "  'paper': {'title': 'Real Time Head Pose Estimation from Consumer Depth Cameras',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-23123-0_11'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Head Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/head-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BIWI'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stb',\n",
       "  'name': 'STB',\n",
       "  'full_name': 'Stereo Hand Pose Benchmark',\n",
       "  'homepage': 'https://github.com/zhjwustc/icip17_stereo_hand_pose_dataset',\n",
       "  'description': '3D hand pose data set created using stereo camera\\r\\n\\r\\n- contains 18,000 RGB images and paired depth images\\r\\n- 3D positions of hand joints (21 joints)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2016-10-08',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': '3D Canonical Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-canonical-hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['STB'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ycb-video',\n",
       "  'name': 'YCB-Video',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://rse-lab.cs.washington.edu/projects/posecnn/',\n",
       "  'description': 'The **YCB-Video** dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames.\\r\\n\\r\\nSource: [https://rse-lab.cs.washington.edu/projects/posecnn/](https://rse-lab.cs.washington.edu/projects/posecnn/)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565](https://www.researchgate.net/figure/Examples-of-refined-poses-on-the-YCB-Video-dataset-which-use-results-from-PoseCNN-Xiang_fig6_339663565)',\n",
       "  'paper': {'title': 'PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes',\n",
       "   'url': 'https://paperswithcode.com/paper/posecnn-a-convolutional-neural-network-for-6d'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '6D Pose Estimation using RGB',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation'},\n",
       "   {'task': '6D Pose Estimation using RGBD',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-using-rgbd'},\n",
       "   {'task': '6D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-1'},\n",
       "   {'task': 'Occluded 3D Object Symmetry Detection',\n",
       "    'url': 'https://paperswithcode.com/task/occluded-3d-object-symmetry-detection'},\n",
       "   {'task': 'Symmetry Detection',\n",
       "    'url': 'https://paperswithcode.com/task/symmetry-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['YCB-Video'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/apollocar3d',\n",
       "  'name': 'ApolloCar3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://apolloscape.auto/car_instance.html',\n",
       "  'description': '**ApolloCar3DT** is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. \\r\\n\\r\\nSource: [ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving](https://arxiv.org/pdf/1811.12222v2.pdf)\\r\\nImage Source: [http://apolloscape.auto/car_instance.html](http://apolloscape.auto/car_instance.html)',\n",
       "  'paper': {'title': 'ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving',\n",
       "   'url': 'https://paperswithcode.com/paper/apollocar3d-a-large-3d-car-instance'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Keypoint Detection',\n",
       "    'url': 'https://paperswithcode.com/task/keypoint-detection'},\n",
       "   {'task': 'Autonomous Vehicles',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-vehicles'},\n",
       "   {'task': '3D Car Instance Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/3d-car-instance-understanding'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'},\n",
       "   {'task': '6D Pose Estimation using RGB',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation'},\n",
       "   {'task': '3D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-pose-estimation'},\n",
       "   {'task': '6D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-1'},\n",
       "   {'task': 'Vehicle Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/vehicle-pose-estimation'},\n",
       "   {'task': '3D Shape Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-shape-reconstruction'},\n",
       "   {'task': '3D Shape Reconstruction From A Single 2D Image',\n",
       "    'url': 'https://paperswithcode.com/task/3d-shape-reconstruction-from-a-single-2d'},\n",
       "   {'task': 'Vehicle Key-Point and Orientation Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/vehicle-key-point-and-orientation-estimation'},\n",
       "   {'task': 'Car Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/car-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ApolloCar3D'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multi-ego',\n",
       "  'name': 'Multi-Ego',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/M-Elfeki/Multi-DPP',\n",
       "  'description': 'A new multi-view egocentric dataset, Multi-Ego. The dataset is recorded simultaneously by three cameras, covering a wide variety of real-life scenarios. The footage is annotated by multiple individuals under various summarization configurations, with a consensus analysis ensuring a reliable ground truth.\\r\\n\\r\\nSource: [Multi-Stream Dynamic Video Summarization](/paper/multi-view-egocentric-video-summarization)',\n",
       "  'paper': {'title': 'Multi-Stream Dynamic Video Summarization',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-view-egocentric-video-summarization'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/video-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Multi-Ego'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/M-Elfeki/Multi-DPP',\n",
       "    'repo': 'https://github.com/M-Elfeki/Multi-DPP',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/summe',\n",
       "  'name': 'SumMe',\n",
       "  'full_name': 'SumMe',\n",
       "  'homepage': 'https://gyglim.github.io/me/vsum/index.html',\n",
       "  'description': 'The **SumMe** dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total).\\r\\n\\r\\nSource: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)\\r\\nImage Source: [https://gyglim.github.io/me/vsum/index.html](https://gyglim.github.io/me/vsum/index.html)',\n",
       "  'paper': {'title': 'Creating Summaries from User Videos',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10584-0_33'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/video-summarization'},\n",
       "   {'task': 'Unsupervised Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-summarization'},\n",
       "   {'task': 'Supervised Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/supervised-video-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SumMe'],\n",
       "  'num_papers': 83,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/reddit-tifu',\n",
       "  'name': 'Reddit TIFU',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/ctr4si/MMN',\n",
       "  'description': '**Reddit TIFU** dataset is a newly collected Reddit dataset, where TIFU denotes the name of /r/tifu subbreddit.\\r\\nThere are 122,933 text-summary pairs in total.\\r\\n\\r\\nSource: [https://github.com/ctr4si/MMN](https://github.com/ctr4si/MMN)',\n",
       "  'paper': {'title': 'Abstractive Summarization of Reddit Posts with Multi-level Memory Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/abstractive-summarization-of-reddit-posts'},\n",
       "  'introduced_date': '2018-11-02',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Reddit TIFU'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/reddit_tifu',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/reddit_tifu',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/ctr4si/MMN',\n",
       "    'repo': 'https://github.com/ctr4si/MMN',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/davis-2016',\n",
       "  'name': 'DAVIS 2016',\n",
       "  'full_name': 'DAVIS 2016',\n",
       "  'homepage': 'https://davischallenge.org/davis2016/code.html',\n",
       "  'description': 'DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered.\\r\\n\\r\\nSource: [Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2008.01270)\\r\\nImage Source: [https://davischallenge.org/](https://davischallenge.org/)',\n",
       "  'paper': {'title': 'A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-segmentation'},\n",
       "   {'task': 'Semi-Supervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-video-object-segmentation'},\n",
       "   {'task': 'Video Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-salient-object-detection'},\n",
       "   {'task': 'Unsupervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DAVIS 2016', 'DAVIS-2016'],\n",
       "  'num_papers': 163,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fbms-59',\n",
       "  'name': 'FBMS-59',\n",
       "  'full_name': 'Freiburg-Berkeley Motion Segmentation',\n",
       "  'homepage': 'https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html',\n",
       "  'description': 'The **Freiburg-Berkeley Motion Segmentation** Dataset (**FBMS-59**) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets.\\n\\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)\\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-salient-object-detection'},\n",
       "   {'task': 'Unsupervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FBMS-59'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/camvid',\n",
       "  'name': 'CamVid',\n",
       "  'full_name': 'Cambridge-driving Labeled Video Database',\n",
       "  'homepage': 'http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/',\n",
       "  'description': '**CamVid** (**Cambridge-driving Labeled Video Database**) is a road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object\\r\\n\\r\\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\\r\\nImage Source: [http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)',\n",
       "  'paper': {'title': 'Semantic object classes in video: A high-definition ground truth database',\n",
       "   'url': 'https://doi.org/10.1016/j.patrec.2008.04.005'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'},\n",
       "   {'task': 'Video Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CamVid'],\n",
       "  'num_papers': 170,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/davis-2017',\n",
       "  'name': 'DAVIS 2017',\n",
       "  'full_name': 'DAVIS 2017',\n",
       "  'homepage': 'https://davischallenge.org/challenge2017/index.html',\n",
       "  'description': 'DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing\\r\\n\\r\\nSource: [Siam R-CNN: Visual Tracking by Re-Detection](https://arxiv.org/abs/1911.12836)\\r\\nImage Source: [https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902](https://www.researchgate.net/figure/LucidTracker-qualitative-results-on-DAVIS-17-test-dev-set-Frames-sampled-along-the_fig5_331792902)',\n",
       "  'paper': {'title': 'The 2017 DAVIS Challenge on Video Object Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/the-2017-davis-challenge-on-video-object'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-segmentation'},\n",
       "   {'task': 'Referring Expression Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/referring-expression-segmentation'},\n",
       "   {'task': 'Semi-Supervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-video-object-segmentation'},\n",
       "   {'task': 'Unsupervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-object-segmentation'},\n",
       "   {'task': 'Video Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-detection'},\n",
       "   {'task': 'Interactive Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-video-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DAVIS 2017',\n",
       "   'DAVIS 2017 (test-dev)',\n",
       "   'DAVIS 2017 (val)',\n",
       "   'DAVIS-2017'],\n",
       "  'num_papers': 143,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ccgbank',\n",
       "  'name': 'CCGbank',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2005T13',\n",
       "  'description': '**CCGbank** is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar derivations. It pairs syntactic derivations with sets of word-word dependencies which approximate the underlying predicate-argument structure.\\r\\nThe dataset contains 99.44% of the sentences in the Penn Treebank, for which it corrects a number of inconsistencies and errors in the original annotation.\\r\\n\\r\\nSource: [CCGbank](https://catalog.ldc.upenn.edu/LDC2005T13)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2005-05-15',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'CCG Supertagging',\n",
       "    'url': 'https://paperswithcode.com/task/ccg-supertagging'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://docs.allennlp.org/models/main/models/tagging/dataset_readers/ccgbank/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/svhn',\n",
       "  'name': 'SVHN',\n",
       "  'full_name': 'Street View House Numbers',\n",
       "  'homepage': 'http://ufldl.stanford.edu/housenumbers/',\n",
       "  'description': 'Street View House Numbers (**SVHN**) is a digit classification benchmark dataset that contains 600,000 32×32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process.\\r\\n\\r\\nSource: [Competitive Multi-scale Convolution](https://arxiv.org/abs/1511.05635)\\r\\nImage Source: [http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)',\n",
       "  'paper': {'title': 'Reading digits in natural images with unsupervised feature learning',\n",
       "   'url': 'http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'},\n",
       "   {'task': 'Sparse Representation-based Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sparse-representation-based-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['svhn,1000',\n",
       "   'SVHN, 4000 Labels',\n",
       "   'SVHN, 2000 Labels',\n",
       "   'SVHN-to-MNIST',\n",
       "   'SVHN, 500 Labels',\n",
       "   'SVHN, 40 Labels',\n",
       "   'SVHN, 250 Labels',\n",
       "   'SVHN, 1000 labels',\n",
       "   'SVHN'],\n",
       "  'num_papers': 2015,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/svhn',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.SVHN',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/the-street-view-house-numbers-svhn-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/svhn_cropped',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'repo': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/stl-10',\n",
       "  'name': 'STL-10',\n",
       "  'full_name': 'Self-Taught Learning 10',\n",
       "  'homepage': 'https://cs.stanford.edu/~acoates/stl10/',\n",
       "  'description': 'The **STL-10** is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96×96 pixels in size.\\r\\n\\r\\nSource: [Unsupervised Feature Learning with C-SVDDNet](https://arxiv.org/abs/1412.7259)\\r\\nImage Source: [https://cs.stanford.edu/~acoates/stl10/](https://cs.stanford.edu/~acoates/stl10/)',\n",
       "  'paper': {'title': 'An Analysis of Single-Layer Networks in Unsupervised Feature Learning',\n",
       "   'url': 'http://proceedings.mlr.press/v15/coates11a/coates11a.pdf'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'},\n",
       "   {'task': 'Self-Supervised Learning',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['STL-10', 'STL-10, 1000 Labels', 'STL-10, 5000 Labels'],\n",
       "  'num_papers': 545,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.STL10',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/stl10',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cifar-10',\n",
       "  'name': 'CIFAR-10',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cs.toronto.edu/~kriz/cifar.html',\n",
       "  'description': 'The **CIFAR-10** dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class.\\r\\n\\r\\nThe criteria for deciding whether an image belongs to a class were as follows:\\r\\n\\r\\n* The class name should be high on the list of likely answers to the question “What is in this picture?”\\r\\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\\r\\n* The image should contain only one prominent instance of the object to which the class refers.\\r\\nThe object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\\r\\n\\r\\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\\r\\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)',\n",
       "  'paper': {'title': 'Learning multiple layers of features from tiny images',\n",
       "   'url': 'https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Small Data Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/small-data'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'},\n",
       "   {'task': 'Adversarial Defense',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-defense'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'},\n",
       "   {'task': 'Conditional Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/conditional-image-generation'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'},\n",
       "   {'task': 'Adversarial Robustness',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-robustness'},\n",
       "   {'task': 'Network Pruning',\n",
       "    'url': 'https://paperswithcode.com/task/network-pruning'},\n",
       "   {'task': 'Classification with Binary Weight Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-weight-network'},\n",
       "   {'task': 'Sequential Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sequential-image-classification'},\n",
       "   {'task': 'Quantization',\n",
       "    'url': 'https://paperswithcode.com/task/quantization'},\n",
       "   {'task': 'Binarization',\n",
       "    'url': 'https://paperswithcode.com/task/binarization'},\n",
       "   {'task': 'Personalized Federated Learning',\n",
       "    'url': 'https://paperswithcode.com/task/personalized-federated-learning'},\n",
       "   {'task': 'Classification with Binary Neural Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-neural-network'},\n",
       "   {'task': 'Adversarial Attack',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-attack'},\n",
       "   {'task': 'Neural Network Compression',\n",
       "    'url': 'https://paperswithcode.com/task/neural-network-compression'},\n",
       "   {'task': 'Hard-label Attack',\n",
       "    'url': 'https://paperswithcode.com/task/hard-label-attack'},\n",
       "   {'task': 'Robust classification',\n",
       "    'url': 'https://paperswithcode.com/task/robust-classification'},\n",
       "   {'task': 'Parameter Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/parameter-prediction'},\n",
       "   {'task': 'Drawing Pictures',\n",
       "    'url': 'https://paperswithcode.com/task/drawing-pictures'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CIFAR10',\n",
       "   'Unlabeled CIFAR-10 vs CIFAR-100',\n",
       "   'CIFAR-10 (with noisy labels)',\n",
       "   'noise padded CIFAR-10',\n",
       "   'cifar10, 10 labels',\n",
       "   'cifar-10,4000',\n",
       "   'CIFAR10 100k',\n",
       "   'CIFAR-100 vs CIFAR-10',\n",
       "   'CIFAR-100 WRN-28-10 - 200 Epochs',\n",
       "   'CIFAR-10 model detecting CIFAR-10',\n",
       "   'CIFAR-10 image generation',\n",
       "   'CIFAR-10 WRN-28-10 - 200 Epochs',\n",
       "   'CIFAR-10-LT (ρ=100)',\n",
       "   'CIFAR-10-LT (ρ=10)',\n",
       "   'CIFAR-10, 80 Labels',\n",
       "   'CIFAR-10, 500 Labels',\n",
       "   'CIFAR-10, 20 Labels',\n",
       "   'CIFAR-10 vs CIFAR-100',\n",
       "   'CIFAR-10 ResNet-18 - 200 Epochs',\n",
       "   'CIFAR-10 (Conditional)',\n",
       "   'cifar10, 250 Labels',\n",
       "   'One-class CIFAR-10',\n",
       "   'CIFAR-10, 4000 Labels',\n",
       "   'CIFAR-10, 40 Labels',\n",
       "   'CIFAR-10, 250 Labels',\n",
       "   'CIFAR-10, 2000 Labels',\n",
       "   'CIFAR-10, 1000 Labels',\n",
       "   'CIFAR-10 Image Classification',\n",
       "   'CIFAR-10'],\n",
       "  'num_papers': 8162,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/cifar10',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR10',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/cifar-10-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/cifar10',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmclassification',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'repo': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/clothing1m',\n",
       "  'name': 'Clothing1M',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/Cysu/noisy_label',\n",
       "  'description': '**Clothing1M** contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively.\\r\\n\\r\\nSource: [Label-Noise Robust Generative Adversarial Networks](https://arxiv.org/abs/1811.11165)\\r\\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Xiao_Learning_From_Massive_2015_CVPR_paper.pdf)',\n",
       "  'paper': {'title': 'Learning From Massive Noisy Labeled Data for Image Classification',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-from-massive-noisy-labeled-data-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Learning with noisy labels',\n",
       "    'url': 'https://paperswithcode.com/task/learning-with-noisy-labels'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Clothing1M', 'Clothing1M (using clean data)'],\n",
       "  'num_papers': 157,\n",
       "  'data_loaders': [{'url': 'https://github.com/Cysu/noisy_label',\n",
       "    'repo': 'https://github.com/Cysu/noisy_label',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cifar-100',\n",
       "  'name': 'CIFAR-100',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cs.toronto.edu/~kriz/cifar.html',\n",
       "  'description': 'The **CIFAR-100** dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class.\\r\\n\\r\\nThe criteria for deciding whether an image belongs to a class were as follows:\\r\\n\\r\\n* The class name should be high on the list of likely answers to the question “What is in this picture?”\\r\\n* The image should be photo-realistic. Labelers were instructed to reject line drawings.\\r\\n* The image should contain only one prominent instance of the object to which the class refers.\\r\\n* The object may be partially occluded or seen from an unusual viewpoint as long as its identity is still clear to the labeler.\\r\\n\\r\\nSource: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)\\r\\nImage Source: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)',\n",
       "  'paper': {'title': 'Learning multiple layers of features from tiny images',\n",
       "   'url': 'https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'},\n",
       "   {'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Incremental Learning',\n",
       "    'url': 'https://paperswithcode.com/task/incremental-learning'},\n",
       "   {'task': 'Small Data Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/small-data'},\n",
       "   {'task': 'Adversarial Defense',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-defense'},\n",
       "   {'task': 'Conditional Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/conditional-image-generation'},\n",
       "   {'task': 'Network Pruning',\n",
       "    'url': 'https://paperswithcode.com/task/network-pruning'},\n",
       "   {'task': 'Classification with Binary Weight Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-weight-network'},\n",
       "   {'task': 'Binarization',\n",
       "    'url': 'https://paperswithcode.com/task/binarization'},\n",
       "   {'task': 'Personalized Federated Learning',\n",
       "    'url': 'https://paperswithcode.com/task/personalized-federated-learning'},\n",
       "   {'task': 'Classification with Binary Neural Network',\n",
       "    'url': 'https://paperswithcode.com/task/classification-with-binary-neural-network'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Unlabeled CIFAR-10 vs CIFAR-100',\n",
       "   'CIFAR-100 ResNet-18 - 200 Epochs',\n",
       "   'CIFAR-10, 2000 Labeled Samples',\n",
       "   'cifar-100, 10000 Labels',\n",
       "   'One-class CIFAR-100',\n",
       "   'Cifar100 (20 tasks)',\n",
       "   'CIFAR100 5-way (1-shot)',\n",
       "   'CIFAR-100-LT (ρ=100)',\n",
       "   'CIFAR-100-LT (ρ=10)',\n",
       "   'CIFAR-100, 5000Labels',\n",
       "   'CIFAR-100, 400 Labels',\n",
       "   'CIFAR-100, 2500 Labels',\n",
       "   'CIFAR-100, 1000 Labels',\n",
       "   'CIFAR-100 - 50 classes + 50 steps of 1 class',\n",
       "   'CIFAR-100 - 50 classes + 5 steps of 10 classes',\n",
       "   'CIFAR-100 - 50 classes + 25 steps of 2 classes',\n",
       "   'CIFAR-100 - 50 classes + 10 steps of 5 classes',\n",
       "   'CIFAR-100'],\n",
       "  'num_papers': 3859,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/cifar100',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CIFAR100',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets/cifar-100-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/cifar100',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmclassification/blob/master/docs/getting_started.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmclassification',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ade20k',\n",
       "  'name': 'ADE20K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://groups.csail.mit.edu/vision/datasets/ADE20K/',\n",
       "  'description': 'The **ADE20K** semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed.\\r\\n\\r\\nSource: [Cooperative Image Segmentation and Restoration in Adverse Environmental Conditions](https://arxiv.org/abs/1911.00679)\\r\\nImage Source: [https://groups.csail.mit.edu/vision/datasets/ADE20K/](https://groups.csail.mit.edu/vision/datasets/ADE20K/)',\n",
       "  'paper': {'title': 'Scene Parsing Through ADE20K Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/scene-parsing-through-ade20k-dataset'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'},\n",
       "   {'task': 'Overlapped 100-50',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-100-50'},\n",
       "   {'task': 'Overlapped 50-50',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-50-50'},\n",
       "   {'task': 'Overlapped 100-10',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-100-10'},\n",
       "   {'task': 'Overlapped 100-5',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-100-5'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ADE20K',\n",
       "   'ADE20K Labels-to-Photos',\n",
       "   'ADE20K-Outdoor Labels-to-Photos',\n",
       "   'ADE20K val'],\n",
       "  'num_papers': 388,\n",
       "  'data_loaders': [{'url': 'https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-ade20k-scene-parsing',\n",
       "    'repo': 'https://github.com/facebookresearch/detectron2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/scene_parse150',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'repo': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpii-human-pose',\n",
       "  'name': 'MPII Human Pose',\n",
       "  'full_name': 'MPII Human Pose',\n",
       "  'homepage': 'http://human-pose.mpi-inf.mpg.de/',\n",
       "  'description': '**MPII Human Pose** Dataset is a dataset for human pose estimation. It consists of around 25k images extracted from online videos. Each image contains one or more people, with over 40k people annotated in total. Among the 40k samples, ∼28k samples are for training and the remainder are for testing. Overall the dataset covers 410 human activities and each image is provided with an activity label. Images were extracted from a YouTube video and provided with preceding and following un-annotated frames.\\r\\n\\r\\nSource: [Accelerating Large-Kernel Convolution Using Summed-Area Tables](https://arxiv.org/abs/1906.11367)\\r\\nImage Source: [http://human-pose.mpi-inf.mpg.de/](http://human-pose.mpi-inf.mpg.de/)',\n",
       "  'paper': {'title': '2D Human Pose Estimation: New Benchmark and State of the Art Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/2d-human-pose-estimation-new-benchmark-and'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MPII Human Pose', 'MPII Single Person'],\n",
       "  'num_papers': 120,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/human3-6m',\n",
       "  'name': 'Human3.6M',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://vision.imar.ro/human3.6m/description.php',\n",
       "  'description': 'The **Human3.6M** dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos.\\r\\n\\r\\nSource: [Space-Time Representation of People Based on 3D Skeletal Data: A Review](https://arxiv.org/abs/1601.01006)\\r\\n\\r\\nImage Source: [Yu et al](https://www.researchgate.net/publication/320271480_Coupled_Multiview_Auto-Encoders_with_Locality-Sensitivity_for_3D_Human_Pose_Estimation)',\n",
       "  'paper': {'title': 'Human3.6m: Large scale datasets and predictive methods for 3D human sensing in natural environments',\n",
       "   'url': 'https://paperswithcode.com/paper/human36m-large-scale-datasets-and-predictive'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'},\n",
       "   {'task': '3D Absolute Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-absolute-human-pose-estimation'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'},\n",
       "   {'task': '3D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-pose-estimation'},\n",
       "   {'task': 'Human action generation',\n",
       "    'url': 'https://paperswithcode.com/task/human-action-generation'},\n",
       "   {'task': 'Human Pose Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/human-pose-forecasting'},\n",
       "   {'task': 'Pose Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/pose-retrieval'},\n",
       "   {'task': 'Root Joint Localization',\n",
       "    'url': 'https://paperswithcode.com/task/root-joint-localization'},\n",
       "   {'task': 'Monocular 3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-3d-human-pose-estimation'},\n",
       "   {'task': 'Weakly-supervised 3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-3d-human-pose-estimation'},\n",
       "   {'task': 'Multi-Hypotheses 3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-hypotheses-3d-human-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Human3.6M'],\n",
       "  'num_papers': 395,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/3d_body_mesh.md#human36m',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cihp',\n",
       "  'name': 'CIHP',\n",
       "  'full_name': 'Crowd Instance-level Human Parsing',\n",
       "  'homepage': 'http://sysu-hcp.net/lip/overview.php',\n",
       "  'description': 'The **Crowd Instance-level Human Parsing** (**CIHP**) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. The dataset can be used for the human part segmentation task.\\r\\n\\r\\nSource: [Parsing R-CNN for Instance-Level Human Analysis](https://arxiv.org/abs/1811.12596)\\r\\nImage Source: [https://arxiv.org/abs/1808.00157](https://arxiv.org/abs/1808.00157)',\n",
       "  'paper': {'title': 'Instance-level Human Parsing via Part Grouping Network',\n",
       "   'url': 'https://paperswithcode.com/paper/instance-level-human-parsing-via-part'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Human Part Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/human-part-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CIHP'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multimnist',\n",
       "  'name': 'MultiMNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'The **MultiMNIST** dataset is generated from MNIST. The training and tests are generated by overlaying a digit on top of another digit from the same set (training or test) but different class. Each digit is shifted up to 4 pixels in each direction resulting in a 36×36 image. Considering a digit in a 28×28 image is bounded in a 20×20 box, two digits bounding boxes on average have 80% overlap. For each digit in the MNIST dataset 1,000 MultiMNIST examples are generated, so the training set size is 60M and the test set size is 10M.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)\\r\\nImage Source: [Sabour et al](https://arxiv.org/pdf/1710.09829v2.pdf)',\n",
       "  'paper': {'title': 'Dynamic Routing Between Capsules',\n",
       "   'url': 'https://paperswithcode.com/paper/dynamic-routing-between-capsules'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MultiMNIST'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inaturalist',\n",
       "  'name': 'iNaturalist',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/visipedia/inat_comp/tree/master/2017',\n",
       "  'description': 'The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category “Plantae (Plant)” has 196,613 images from 2,101 categories; whereas the smallest super-category “Protozoa” only has 381 images from 4 categories.\\r\\n\\r\\nSource: [Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning](https://arxiv.org/abs/1806.06193)\\r\\nImage Source: [https://github.com/visipedia/inat_comp/tree/master/2017](https://github.com/visipedia/inat_comp/tree/master/2017)',\n",
       "  'paper': {'title': 'The iNaturalist Species Classification and Detection Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/the-inaturalist-species-classification-and'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iNaturalist',\n",
       "   'iNaturalist (227-way multi-shot)',\n",
       "   'iNaturalist 2018'],\n",
       "  'num_papers': 174,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/i_naturalist2017',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/visipedia/inat_comp',\n",
       "    'repo': 'https://github.com/visipedia/inat_comp',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/scannet',\n",
       "  'name': 'ScanNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.scan-net.org/',\n",
       "  'description': '**ScanNet** is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects.\\r\\n\\r\\nSource: [A Review of Point Cloud Semantic Segmentation](https://arxiv.org/abs/1908.08854)\\r\\nImage Source: [http://www.scan-net.org/](http://www.scan-net.org/)',\n",
       "  'paper': {'title': 'ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes',\n",
       "   'url': 'https://paperswithcode.com/paper/scannet-richly-annotated-3d-reconstructions'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'},\n",
       "   {'task': 'Scene Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-segmentation'},\n",
       "   {'task': 'Surface Normals Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/surface-normals-estimation'},\n",
       "   {'task': '3D Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-instance-segmentation-1'},\n",
       "   {'task': '3D Semantic Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-semantic-instance-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ScanNet', 'ScanNetV1', 'ScanNetV2', 'ScanNet(v2)'],\n",
       "  'num_papers': 435,\n",
       "  'data_loaders': [{'url': 'http://www.scan-net.org/',\n",
       "    'repo': 'https://github.com/ScanNet/ScanNet',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sbd',\n",
       "  'name': 'SBD',\n",
       "  'full_name': 'Semantic Boundaries Dataset',\n",
       "  'homepage': 'http://home.bharathh.info/pubs/codes/SBD/download.html',\n",
       "  'description': 'The **Semantic Boundaries Dataset** (**SBD**) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes.\\r\\n\\r\\nSource: [Weakly Supervised Object Boundaries](https://arxiv.org/abs/1511.07803)\\r\\nImage Source: [http://home.bharathh.info/pubs/codes/SBD/download.html](http://home.bharathh.info/pubs/codes/SBD/download.html)',\n",
       "  'paper': {'title': 'Semantic contours from inverse detectors',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2011.6126343'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'},\n",
       "   {'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'},\n",
       "   {'task': 'Semantic Contour Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-contour-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SBD', 'Sbd val'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.SBDataset',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sk-large',\n",
       "  'name': 'SK-LARGE',\n",
       "  'full_name': 'SK-LARGE',\n",
       "  'homepage': 'http://kaizhao.net/sk-large',\n",
       "  'description': '**SK-LARGE** is a benchmark dataset for object skeleton detection, built on the MS COCO dataset. It contains 1491 images, 746 for training and 745 for testing.\\n\\nSource: [DeepFlux for Skeletons in the Wild](https://arxiv.org/abs/1811.12608)\\nImage Source: [http://kaizhao.net/sk-large](http://kaizhao.net/sk-large)',\n",
       "  'paper': {'title': 'DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images',\n",
       "   'url': 'https://paperswithcode.com/paper/deepskeleton-learning-multi-task-scale'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Skeleton Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-skeleton-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SK-LARGE'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/indian-pines',\n",
       "  'name': 'Indian Pines',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes#Indian_Pines',\n",
       "  'description': '**Indian Pines** is a Hyperspectral image segmentation dataset. The input data consists of hyperspectral bands over a single landscape in Indiana, US, (Indian Pines data set) with 145×145 pixels. For each pixel, the data set contains 220 spectral reflectance bands which represent different portions of the electromagnetic spectrum in the wavelength range 0.4−2.5⋅10−6.\\r\\n\\r\\nSource: [Layer-Parallel Training of Deep Residual Neural Networks](https://arxiv.org/abs/1812.04352)\\r\\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Indian_Pines)',\n",
       "  'paper': {'title': '220 band AVIRIS hyperspectral image data set',\n",
       "   'url': 'https://purr.purdue.edu/publications/1947/1'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Hyperspectral images'],\n",
       "  'tasks': [{'task': 'Hyperspectral Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/hyperspectral-image-classification'},\n",
       "   {'task': 'Hyperspectral Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/hyperspectral-image-inpainting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Indian Pines'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pavia-university',\n",
       "  'name': 'Pavia University',\n",
       "  'full_name': 'Pavia University',\n",
       "  'homepage': 'http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University',\n",
       "  'description': 'The **Pavia University** dataset is a hyperspectral image dataset which gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy. The image consists of 610×340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42,776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow.\\n\\nSource: [Diversity in Machine Learning](https://arxiv.org/abs/1807.01477)\\nImage Source: [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University)',\n",
       "  'paper': {'title': 'Pavia centre and university',\n",
       "   'url': 'http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Hyperspectral images'],\n",
       "  'tasks': [{'task': 'Hyperspectral Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/hyperspectral-image-classification'},\n",
       "   {'task': 'Classification Of Hyperspectral Images',\n",
       "    'url': 'https://paperswithcode.com/task/classification-of-hyperspectral-images'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Pavia University'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rvl-cdip',\n",
       "  'name': 'RVL-CDIP',\n",
       "  'full_name': 'RVL-CDIP',\n",
       "  'homepage': 'https://www.cs.cmu.edu/~aharley/rvl-cdip/',\n",
       "  'description': 'The **RVL-CDIP** dataset consists of scanned document images belonging to 16 classes such as letter, form, email, resume, memo, etc. The dataset has 320,000 training, 40,000 validation and 40,000 test images. The images are characterized by low quality, noise, and low resolution, typically 100 dpi.\\r\\n\\r\\nSource: [Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning](https://arxiv.org/abs/2009.14457)\\r\\nImage Source: [https://www.cs.cmu.edu/~aharley/rvl-cdip/](https://www.cs.cmu.edu/~aharley/rvl-cdip/)',\n",
       "  'paper': {'title': 'Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval',\n",
       "   'url': 'https://paperswithcode.com/paper/evaluation-of-deep-convolutional-nets-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Document Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-image-classification'},\n",
       "   {'task': 'Document Layout Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/document-layout-analysis'},\n",
       "   {'task': 'Multi-Modal Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-modal-document-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RVL-CDIP'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/coco-captions',\n",
       "  'name': 'COCO Captions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/tylin/coco-caption',\n",
       "  'description': 'COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image.\\r\\n\\r\\nSource: [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325)',\n",
       "  'paper': {'title': 'Microsoft COCO Captions: Data Collection and Evaluation Server',\n",
       "   'url': 'https://paperswithcode.com/paper/microsoft-coco-captions-data-collection-and'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/image-captioning'},\n",
       "   {'task': 'Text-Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/texture-image-retrieval'},\n",
       "   {'task': 'Concept-To-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/concept-to-text-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COCO Captions', 'COCO Captions test', 'MSCOCO-1k'],\n",
       "  'num_papers': 72,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.CocoCaptions',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#coco_captions',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/coco_captions',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/tylin/coco-caption',\n",
       "    'repo': 'https://github.com/tylin/coco-caption',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/rotowire',\n",
       "  'name': 'RotoWire',\n",
       "  'full_name': 'RotoWire',\n",
       "  'homepage': 'https://github.com/harvardnlp/boxscore-data',\n",
       "  'description': 'This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the \"rotowire\" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.\\r\\n\\r\\nSource: [Challenges in Data-to-Document Generation](https://arxiv.org/abs/1707.08052)\\r\\nImage Source: [https://arxiv.org/pdf/1707.08052v1.pdf](https://arxiv.org/pdf/1707.08052v1.pdf)',\n",
       "  'paper': {'title': 'Challenges in Data-to-Document Generation',\n",
       "   'url': 'https://paperswithcode.com/paper/challenges-in-data-to-document-generation'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'},\n",
       "   {'task': 'Table-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/table-to-text-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RotoWire (Relation Generation)',\n",
       "   'Rotowire (Content Selection)',\n",
       "   'RotoWire (Content Ordering)',\n",
       "   'RotoWire'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': [{'url': 'https://github.com/harvardnlp/boxscore-data',\n",
       "    'repo': 'https://github.com/harvardnlp/boxscore-data',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikibio',\n",
       "  'name': 'WikiBio',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://rlebret.github.io/wikipedia-biography-dataset/',\n",
       "  'description': '**WikiBio** is a dataset introduced for generating biography notes based on information found in an infobox – a fact table describing a person. Each sample in the dataset contains the infobox and the first sentence of each biography article as reference. On average, each reference sentence has 26.1 words. The corpus contains 728,321 instances, which has been divided into three sub-parts to provide 582,659 for training, 72,831 for validation and 72,831 for testing.\\r\\n\\r\\nSource: [Table-to-Text: Describing Table Region with Natural Language](https://arxiv.org/abs/1805.11234)\\r\\nImage Source: [https://arxiv.org/pdf/1603.07771.pdf](https://arxiv.org/pdf/1603.07771.pdf)',\n",
       "  'paper': {'title': 'Neural Text Generation from Structured Data with Application to the Biography Domain',\n",
       "   'url': 'https://paperswithcode.com/paper/neural-text-generation-from-structured-data'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Table-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/table-to-text-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiBio'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wiki_bio',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wiki_bio',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dailydialog',\n",
       "  'name': 'DailyDialog',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://yanran.li/dailydialog',\n",
       "  'description': '**DailyDialog** is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn.\\r\\n\\r\\nSource: [http://yanran.li/dailydialog](http://yanran.li/dailydialog)\\r\\nImage Source: [https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/](https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn/)',\n",
       "  'paper': {'title': 'DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/dailydialog-a-manually-labelled-multi-turn'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Dialog'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DailyDialog'],\n",
       "  'num_papers': 198,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/daily_dialog',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#daily-dialog',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/webnlg',\n",
       "  'name': 'WebNLG',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://webnlg-challenge.loria.fr/',\n",
       "  'description': 'The **WebNLG** corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories.\\r\\n\\r\\nInitially, the dataset was used for the WebNLG natural language generation challenge which consists of mapping the sets of triplets to text, including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation.\\r\\nThe corpus is also used for a reverse task of triplets extraction.\\r\\n\\r\\nVersioning history of the dataset can be found [here](https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/).\\r\\n\\r\\nSource: [Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation](https://arxiv.org/abs/1904.03396)\\r\\nImage Source: [https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/](https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro/)',\n",
       "  'paper': {'title': 'Creating Training Corpora for NLG Micro-Planners',\n",
       "   'url': 'https://paperswithcode.com/paper/creating-training-corpora-for-nlg-micro'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'},\n",
       "   {'task': 'Table-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/table-to-text-generation'},\n",
       "   {'task': 'KG-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/kg-to-text'},\n",
       "   {'task': 'Joint Entity and Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/joint-entity-and-relation-extraction'},\n",
       "   {'task': 'Unsupervised KG-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-kg-to-text-generation'},\n",
       "   {'task': 'Graph-to-Sequence',\n",
       "    'url': 'https://paperswithcode.com/task/graph-to-sequence'},\n",
       "   {'task': 'Unsupervised semantic parsing',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-semantic-parsing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WebNLG 2.0 (Unconstrained)',\n",
       "   'WebNLG 2.0 (Constrained)',\n",
       "   'WebNLG (Unseen)',\n",
       "   'WebNLG (Seen)',\n",
       "   'WebNLG (All)',\n",
       "   'WebNLG (Constrained)',\n",
       "   'WebNLG(C)',\n",
       "   'WebNLG(U)',\n",
       "   'WebNLG en',\n",
       "   'WebNLG v2.1',\n",
       "   'WebNLG Full',\n",
       "   'WebNLG'],\n",
       "  'num_papers': 91,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/web_nlg',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/web_nlg',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/megaface',\n",
       "  'name': 'MegaFace',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': '**MegaFace** was a publicly available dataset which is used for evaluating the performance of face recognition algorithms with up to a million distractors (i.e., up to a million people who are not in the test set). MegaFace contains 1M images from 690K individuals with unconstrained pose, expression, lighting, and exposure. MegaFace captures many different subjects rather than many images of a small number of subjects. The gallery set of MegaFace is collected from a subset of Flickr. The probe set of MegaFace used in the challenge consists of two databases; Facescrub and FGNet. FGNet contains 975 images of 82 individuals, each with several images spanning ages from 0 to 69. Facescrub dataset contains more than 100K face images of 530 people. The MegaFace challenge evaluates performance of face recognition algorithms by increasing the numbers of “distractors” (going from 10 to 1M) in the gallery set. In order to evaluate the face recognition algorithms fairly, MegaFace challenge has two protocols including large or small training sets. If a training set has more than 0.5M images and 20K subjects, it is considered as large. Otherwise, it is considered as small.\\r\\n\\r\\n**NOTE**: This dataset [has been retired](https://exposing.ai/megaface/). \\r\\n\\r\\nSource: [A Deep Face Identification Network Enhanced by Facial Attributes Prediction](https://arxiv.org/abs/1805.00324)',\n",
       "  'paper': {'title': 'The MegaFace Benchmark: 1 Million Faces for Recognition at Scale',\n",
       "   'url': 'https://paperswithcode.com/paper/the-megaface-benchmark-1-million-faces-for'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Face Identification',\n",
       "    'url': 'https://paperswithcode.com/task/face-identification'},\n",
       "   {'task': 'Disguised Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/disguised-face-verification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MegaFace'],\n",
       "  'num_papers': 161,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ijb-b',\n",
       "  'name': 'IJB-B',\n",
       "  'full_name': 'IARPA Janus Benchmark-B',\n",
       "  'homepage': 'https://www.nist.gov/programs-projects/face-challenges',\n",
       "  'description': 'The **IJB-B** dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification.\\r\\n\\r\\nSource: [An Automatic System for Unconstrained Video-Based Face Recognition](https://arxiv.org/abs/1812.04058)\\r\\nImage Source: [https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf](https://www.vislab.ucr.edu/Biometrics2017/program_slides/Noblis_CVPRW_IJBB.pdf)',\n",
       "  'paper': {'title': 'IARPA Janus Benchmark-B Face Dataset',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2017.87'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Face Identification',\n",
       "    'url': 'https://paperswithcode.com/task/face-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IJB-B'],\n",
       "  'num_papers': 89,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ijb-a',\n",
       "  'name': 'IJB-A',\n",
       "  'full_name': 'IARPA Janus Benchmark A',\n",
       "  'homepage': 'https://www.nist.gov/programs-projects/face-challenges',\n",
       "  'description': 'The **IARPA Janus Benchmark A** (**IJB-A**) database is developed with the aim to augment more challenges to the face recognition task by collecting facial images with a wide variations in pose, illumination, expression, resolution and occlusion. IJB-A is constructed by collecting 5,712 images and 2,085 videos from 500 identities, with an average of 11.4 images and 4.2 videos per identity.\\r\\n\\r\\nSource: [von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification](https://arxiv.org/abs/1706.04264)\\r\\n\\r\\nImage Source: [Ruan et al](https://www.researchgate.net/figure/The-IARPA-Janus-Benchmark-A-IJB-A-dataset-face-verification-11-test-protocol-a_fig12_342756996)',\n",
       "  'paper': {'title': 'Pushing the Frontiers of Unconstrained Face Detection and Recognition: IARPA Janus Benchmark A',\n",
       "   'url': 'https://paperswithcode.com/paper/pushing-the-frontiers-of-unconstrained-face'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Face Identification',\n",
       "    'url': 'https://paperswithcode.com/task/face-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IJB-A'],\n",
       "  'num_papers': 143,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/300w',\n",
       "  'name': '300W',\n",
       "  'full_name': '300 Faces-In-The-Wild',\n",
       "  'homepage': 'https://ibug.doc.ic.ac.uk/resources/300-W/',\n",
       "  'description': 'The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as “party”, “conference”, “protests”, “football” and “celebrities”. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common “neutral” or “smile”, such as “surprise” or “scream”.\\r\\nImages were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases.\\r\\nMany images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.\\r\\n\\r\\nSource: [https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf](https://ibug.doc.ic.ac.uk/media/uploads/documents/sagonas_2016_imavis.pdf)\\r\\nImage Source: [https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg](https://www.researchgate.net/profile/Xuanyi_Dong/publication/323722412/figure/fig1/AS:679426136227845@1538999222829/Face-samples-from-300-W-dataset-Different-faces-have-different-styles-whereas-the-style_Q640.jpg)',\n",
       "  'paper': {'title': '300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2013.59'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Unsupervised Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-facial-landmark-detection'},\n",
       "   {'task': 'Medical Report Generation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-report-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['300W', '300W (Full)'],\n",
       "  'num_papers': 153,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_face_keypoint.md#300w-dataset',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fg-net',\n",
       "  'name': 'FG-NET',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://yanweifu.github.io/FG_NET_data/',\n",
       "  'description': 'FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years\\r\\n\\r\\nSource: [Large age-gap face verification by feature injection in deep networks](https://arxiv.org/abs/1602.06149)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621](https://www.researchgate.net/figure/Sample-images-from-the-FG-NET-Aging-database_fig1_220057621)',\n",
       "  'paper': {'title': 'Toward Automatic Simulation of Aging Effects on Face Images',\n",
       "   'url': 'https://doi.org/10.1109/34.993553'},\n",
       "  'introduced_date': '2002-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'},\n",
       "   {'task': 'Age-Invariant Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/age-invariant-face-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FG-NET', 'FGNET'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ijb-c',\n",
       "  'name': 'IJB-C',\n",
       "  'full_name': 'IARPA Janus Benchmark-C',\n",
       "  'homepage': 'https://www.nist.gov/programs-projects/face-challenges',\n",
       "  'description': 'The **IJB-C** dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images.\\r\\n\\r\\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\\r\\nImage Source: [https://noblis.org/wp-content/uploads/2018/03/icb2018.pdf](https://noblis.org/wp-content/uploads/2018/03/icb2018.pdf)',\n",
       "  'paper': {'title': 'IARPA Janus Benchmark - C: Face Dataset and Protocol',\n",
       "   'url': 'https://doi.org/10.1109/ICB2018.2018.00033'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IJB-C'],\n",
       "  'num_papers': 127,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-face',\n",
       "  'name': 'PASCAL Face',\n",
       "  'full_name': 'PASCAL Face',\n",
       "  'homepage': 'http://host.robots.ox.ac.uk/pascal/VOC/databases.html',\n",
       "  'description': 'The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance.\\r\\n\\r\\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\\r\\nImage Source: [https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926](https://www.researchgate.net/figure/Precision-recall-curves-on-PASCAL-face-dataset_fig7_332998926)',\n",
       "  'paper': {'title': 'Face detection by structural models',\n",
       "   'url': 'https://doi.org/10.1016/j.imavis.2013.12.004'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL Face'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aflw2000-3d',\n",
       "  'name': 'AFLW2000-3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm',\n",
       "  'description': '**AFLW2000-3D** is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector.\\r\\n\\r\\nSource: [https://www.tensorflow.org/datasets/catalog/aflw2k3d](https://www.tensorflow.org/datasets/catalog/aflw2k3d)\\r\\nImage Source: [http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm)',\n",
       "  'paper': {'title': 'Face Alignment Across Large Poses: A 3D Solution',\n",
       "   'url': 'https://paperswithcode.com/paper/face-alignment-across-large-poses-a-3d'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-reconstruction'},\n",
       "   {'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'},\n",
       "   {'task': 'Head Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/head-pose-estimation'},\n",
       "   {'task': '3D Facial Landmark Localization',\n",
       "    'url': 'https://paperswithcode.com/task/3d-facial-landmark-localization'},\n",
       "   {'task': '3D Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-alignment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AFLW2000', 'AFLW2000-3D'],\n",
       "  'num_papers': 85,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/aflw2k3d',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/florence',\n",
       "  'name': 'Florence',\n",
       "  'full_name': 'Florence 3D Faces',\n",
       "  'homepage': 'https://www.micc.unifi.it/resources/datasets/florence-3d-faces/',\n",
       "  'description': 'The **Florence** 3D faces dataset consists of:\\r\\n\\r\\n* High-resolution 3D scans of human faces from many subjects.\\r\\n* Several video sequences of varying resolution, conditions and zoom level for each subject.\\r\\nEach subject is recorded in the following situations:\\r\\n* In a controlled setting in HD video.\\r\\n* In a less-constrained (but still indoor) setting using a standard, PTZ surveillance camera.\\r\\n* In an unconstrained, outdoor environment under challenging recording conditions.\\r\\n\\r\\nSource: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)\\r\\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)',\n",
       "  'paper': {'title': 'The florence 2D/3D hybrid face dataset',\n",
       "   'url': 'https://doi.org/10.1145/2072572.2072597'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Florence'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/morph',\n",
       "  'name': 'MORPH',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://uncw.edu/oic/tech/morph.html',\n",
       "  'description': '**MORPH** is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old.\\r\\n\\r\\nSource: [Deep Ordinal Regression Forests](https://arxiv.org/abs/2008.03077)\\r\\nImage Source: [https://uncw.edu/oic/tech/morph.html](https://uncw.edu/oic/tech/morph.html)',\n",
       "  'paper': {'title': 'MORPH: A Longitudinal Image Database of Normal Adult Age-Progression',\n",
       "   'url': 'https://doi.org/10.1109/FGR.2006.78'},\n",
       "  'introduced_date': '2006-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'},\n",
       "   {'task': 'Age-Invariant Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/age-invariant-face-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MORPH', 'MORPH Album2'],\n",
       "  'num_papers': 123,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cufs',\n",
       "  'name': 'CUFS',\n",
       "  'full_name': 'CUHK Face Sketch Database',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html',\n",
       "  'description': 'CUHK Face Sketch database (CUFS) is for research on face sketch synthesis and face sketch recognition. It includes 188 faces from the Chinese University of Hong Kong (CUHK) student database, 123 faces from the AR database [1], and 295 faces from the XM2VTS database [2]. There are 606 faces in total. For each face, there is a sketch drawn by an artist based on a photo taken in a frontal pose, under normal lighting condition, and with a neutral expression.\\r\\n\\r\\n[1] A. M. Martinez, and R. Benavente, “The AR Face Database,” CVC Technical Report #24, June 1998.\\r\\n\\r\\n[2] K. Messer, J. Matas, J. Kittler, J. Luettin, and G. Maitre, “XM2VTSDB: the Extended of M2VTS Database,” in Proceedings of International Conference on Audio- and Video-Based Person Authentication, pp. 72-77, 1999.\\r\\n\\r\\nSource Paper: [X. Wang and X. Tang, “Face Photo-Sketch Synthesis and Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Vol. 31, 2009](https://ieeexplore.ieee.org/document/4624272)\\r\\n\\r\\nImage Source: [CUHK Face Sketch Database (CUFS)](http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html)\\r\\n\\r\\nSource: [CUHK Face Sketch Database (CUFS)](http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2009-11-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Sketch Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/face-sketch-synthesis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUFS'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cufsf',\n",
       "  'name': 'CUFSF',\n",
       "  'full_name': 'CUHK Face Sketch FERET Database',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/archive/cufsf/',\n",
       "  'description': 'The CUHK Face Sketch FERET (**CUFSF**) is a dataset for research on face sketch synthesis and face sketch recognition. It contains two types of face images: photo and sketch. Total 1,194 images (one image per subject) were collected with lighting variations from the FERET dataset. For each subject, a sketch is drawn with shape exaggeration.\\r\\n\\r\\nSource: [Deeply Coupled Auto-encoder Networks forCross-view Classification](https://arxiv.org/abs/1402.2031)\\r\\nImage Source: [http://mmlab.ie.cuhk.edu.hk/archive/cufsf/](http://mmlab.ie.cuhk.edu.hk/archive/cufsf/)',\n",
       "  'paper': {'title': 'Coupled information-theoretic encoding for face photo-sketch recognition',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2011.5995324'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Sketch Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/face-sketch-synthesis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUFSF'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/caltech-101',\n",
       "  'name': 'Caltech-101',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/',\n",
       "  'description': 'The Caltech101 dataset contains images from 101 object categories (e.g., “helicopter”, “elephant” and “chair” etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300×200 pixels.\\r\\n\\r\\nSource: [Simple and Efficient Learning using Privileged Information](https://arxiv.org/abs/1604.01518)',\n",
       "  'paper': {'title': 'Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2004.383'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'},\n",
       "   {'task': 'Semantic correspondence',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-correspondence'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Caltech-101', 'Caltech-101, 202 Labels'],\n",
       "  'num_papers': 304,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/caltech-101-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/caltech101',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/oxford-iiit-pets',\n",
       "  'name': 'Oxford-IIIT Pets',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/pets/',\n",
       "  'description': 'The Oxford-IIIT Pet Dataset has 37 categories with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation.\\r\\n\\r\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)',\n",
       "  'paper': {'title': 'Cats and dogs',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6248092'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Oxford-IIIT Pets'],\n",
       "  'num_papers': 55,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-cars',\n",
       "  'name': 'Stanford Cars',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ai.stanford.edu/~jkrause/cars/car_dataset.html',\n",
       "  'description': 'The **Stanford Cars** dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360×240.\\r\\n\\r\\nSource: [View Independent Vehicle Make, Model and Color Recognition Using Convolutional Neural Network](https://arxiv.org/abs/1702.01721)\\r\\nImage Source: [https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)',\n",
       "  'paper': {'title': '3D Object Representations for Fine-Grained Categorization',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2013.77'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford Cars',\n",
       "   'Stanford Cars 5-way (1-shot)',\n",
       "   'Stanford Cars 5-way (5-shot)',\n",
       "   'Stanford Cars (Fine-grained 6 Tasks)'],\n",
       "  'num_papers': 267,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/stanford-cars-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/cars196',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nabirds',\n",
       "  'name': 'NABirds',\n",
       "  'full_name': 'North America Birds',\n",
       "  'homepage': 'https://dl.allaboutbirds.org/nabirds',\n",
       "  'description': '**NABirds** V1 is a collection of 48,000 annotated photographs of the 400 species of birds that are commonly observed in North America. More than 100 photographs are available for each species, including separate annotations for males, females and juveniles that comprise 700 visual categories. This dataset is to be used for fine-grained visual categorization experiments.\\r\\n\\r\\nSource: [https://dl.allaboutbirds.org/nabirds](https://dl.allaboutbirds.org/nabirds)\\r\\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf)',\n",
       "  'paper': {'title': 'Building a Bird Recognition App and Large Scale Dataset With Citizen Scientists: The Fine Print in Fine-Grained Dataset Collection',\n",
       "   'url': 'https://paperswithcode.com/paper/building-a-bird-recognition-app-and-large'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NABirds'],\n",
       "  'num_papers': 57,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-dogs',\n",
       "  'name': 'Stanford Dogs',\n",
       "  'full_name': 'Stanford Dogs',\n",
       "  'homepage': 'http://vision.stanford.edu/aditya86/ImageNetDogs/',\n",
       "  'description': 'The **Stanford Dogs** dataset contains 20,580 images of 120 classes of dogs from around the world, which are divided into 12,000 images for training and 8,580 images for testing.\\n\\nSource: [Universal-to-Specific Framework for Complex Action Recognition](https://arxiv.org/abs/2007.06149)\\nImage Source: [https://www.tensorflow.org/datasets/catalog/stanford_dogs](https://www.tensorflow.org/datasets/catalog/stanford_dogs)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford Dogs',\n",
       "   'Stanford Dogs 5-way (1-shot)',\n",
       "   'Stanford Dogs 5-way (5-shot)'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/stanford_dogs',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ffhq',\n",
       "  'name': 'FFHQ',\n",
       "  'full_name': 'Flickr-Faces-HQ',\n",
       "  'homepage': 'https://github.com/NVlabs/ffhq-dataset',\n",
       "  'description': '**Flickr-Faces-HQ (FFHQ)** consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.\\r\\n\\r\\nSource: [Flickr-Faces-HQ Dataset (FFHQ)](https://github.com/NVlabs/ffhq-dataset)\\r\\nImage Source: [https://github.com/NVlabs/ffhq-dataset](https://github.com/NVlabs/ffhq-dataset)',\n",
       "  'paper': {'title': 'A Style-Based Generator Architecture for Generative Adversarial Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/a-style-based-generator-architecture-for'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/image-denoising'},\n",
       "   {'task': 'Facial Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/facial-inpainting'},\n",
       "   {'task': 'Face Hallucination',\n",
       "    'url': 'https://paperswithcode.com/task/face-hallucination'},\n",
       "   {'task': '3D-Aware Image Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/3d-aware-image-synthesis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FFHQ-U',\n",
       "   'FFHQ 512 x 512 - 4x upscaling',\n",
       "   'FFHQ 512 x 512 - 16x upscaling',\n",
       "   'FFHQ 256 x 256 - 4x upscaling',\n",
       "   'FFHQ 256 x 256',\n",
       "   'FFHQ 1024 x 1024 - 4x upscaling',\n",
       "   'FFHQ 1024 x 1024',\n",
       "   'FFHQ'],\n",
       "  'num_papers': 425,\n",
       "  'data_loaders': [{'url': 'https://github.com/NVlabs/ffhq-dataset',\n",
       "    'repo': 'https://github.com/NVlabs/ffhq-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/rafd',\n",
       "  'name': 'RaFD',\n",
       "  'full_name': 'Radboud Faces Database',\n",
       "  'homepage': 'http://www.socsci.ru.nl:8180/RaFD2/RaFD',\n",
       "  'description': 'The **Radboud Faces Database** (**RaFD**) is a set of pictures of 67 models (both adult and children, males and females) displaying 8 emotional expressions.\\r\\n\\r\\nSource: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)\\r\\nImage Source: [http://www.socsci.ru.nl:8180/RaFD2/RaFD](http://www.socsci.ru.nl:8180/RaFD2/RaFD)',\n",
       "  'paper': {'title': 'Presentation and validation of the radboud faces database',\n",
       "   'url': 'https://doi.org/10.1080/02699930903485076'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RaFD'],\n",
       "  'num_papers': 67,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikiqa',\n",
       "  'name': 'WikiQA',\n",
       "  'full_name': 'Wikipedia open-domain Question Answering',\n",
       "  'homepage': 'http://aka.ms/WikiQA',\n",
       "  'description': 'The **WikiQA** corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions.\\r\\n\\r\\nSource: [http://aka.ms/WikiQA](http://aka.ms/WikiQA)\\r\\nImage Source: [Yang et al](https://www.aclweb.org/anthology/D15-1237)',\n",
       "  'paper': {'title': 'WikiQA: A Challenge Dataset for Open-Domain Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/wikiqa-a-challenge-dataset-for-open-domain'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiQA'],\n",
       "  'num_papers': 149,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wiki_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#wikiqa',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/webquestions',\n",
       "  'name': 'WebQuestions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a',\n",
       "  'description': 'The **WebQuestions** dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities.\\r\\n\\r\\nExample questions (answers) in the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).\\r\\n\\r\\nSource: [Question Answering with Subgraph Embeddings](https://arxiv.org/abs/1406.3676)\\r\\nImage Source: [Berant et al](https://www.aclweb.org/anthology/D13-1160)',\n",
       "  'paper': {'title': 'Semantic Parsing on Freebase from Question-Answer Pairs',\n",
       "   'url': 'https://paperswithcode.com/paper/semantic-parsing-on-freebase-from-question'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'KG-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/kg-to-text'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WebQuestions'],\n",
       "  'num_papers': 127,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/web_questions',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#web-questions',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/web_questions',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/simplequestions',\n",
       "  'name': 'SimpleQuestions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/davidgolub/SimpleQA/tree/master/datasets/SimpleQuestions',\n",
       "  'description': '**SimpleQuestions** is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively.\\r\\n\\r\\nSource: [Hierarchical Memory Networks](https://arxiv.org/abs/1605.07427)\\r\\nImage Source: [https://paperswithcode.com/paper/large-scale-simple-question-answering-with/](https://paperswithcode.com/paper/large-scale-simple-question-answering-with/)',\n",
       "  'paper': {'title': 'Large-scale Simple Question Answering with Memory Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-simple-question-answering-with'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SimpleQuestions'],\n",
       "  'num_papers': 92,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/simple_questions_v2',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#simple-questions',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/davidgolub/SimpleQA',\n",
       "    'repo': 'https://github.com/davidgolub/SimpleQA',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/trecqa',\n",
       "  'name': 'TrecQA',\n",
       "  'full_name': 'Text Retrieval Conference Question Answering',\n",
       "  'homepage': 'https://trec.nist.gov/data/qa.html',\n",
       "  'description': '**Text Retrieval Conference Question Answering** (**TrecQA**) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split.\\r\\n\\r\\nSource: [A Gated Self-attention Memory Network for Answer Selection](https://arxiv.org/abs/1909.09696)',\n",
       "  'paper': {'title': 'What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA',\n",
       "   'url': 'https://www.aclweb.org/anthology/D07-1003/'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TrecQA'],\n",
       "  'num_papers': 51,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/trec',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikihop',\n",
       "  'name': 'WikiHop',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://qangaroo.cs.ucl.ac.uk/',\n",
       "  'description': '**WikiHop** is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents.\\r\\n\\r\\nThe dataset includes a masked variant, where all candidates and their mentions in the supporting documents are replaced by random but consistent placeholder tokens.\\r\\n\\r\\nSource: [Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs](https://arxiv.org/abs/1905.07374)\\r\\nImage Source: [http://qangaroo.cs.ucl.ac.uk/](http://qangaroo.cs.ucl.ac.uk/)',\n",
       "  'paper': {'title': 'Constructing Datasets for Multi-hop Reading Comprehension Across Documents',\n",
       "   'url': 'https://paperswithcode.com/paper/constructing-datasets-for-multi-hop-reading'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Paraphrase Identification',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiHop'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wiki_hop',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/rc/dataset_readers/qangaroo/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tusimple',\n",
       "  'name': 'TuSimple',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/TuSimple/tusimple-benchmark',\n",
       "  'description': 'The **TuSimple** dataset consists of 6,408 road images on US highways. The resolution of image is 1280×720. The dataset is composed of 3,626 for training, 358 for validation, and 2,782 for testing called the TuSimple test set of which the images are under different weather conditions.\\r\\n\\r\\nSource: [End-to-End Lane Marker Detection via Row-wise Classification](https://arxiv.org/abs/2005.08630)\\r\\nImage Source: [https://www.researchgate.net/figure/a-Example-from-TuSimple-dataset-b-Derived-dataset-for-training-coordinate-network_fig4_330589970](https://www.researchgate.net/figure/a-Example-from-TuSimple-dataset-b-Derived-dataset-for-training-coordinate-network_fig4_330589970)',\n",
       "  'paper': {'title': 'TuSimple benchmark',\n",
       "   'url': 'https://github.com/TuSimple/tusimple-benchmark'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Lane Detection',\n",
       "    'url': 'https://paperswithcode.com/task/lane-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TuSimple'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://github.com/TuSimple/tusimple-benchmark',\n",
       "    'repo': 'https://github.com/TuSimple/tusimple-benchmark',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/gtsrb',\n",
       "  'name': 'GTSRB',\n",
       "  'full_name': 'German Traffic Sign Recognition Benchmark',\n",
       "  'homepage': 'https://benchmark.ini.rub.de/',\n",
       "  'description': 'The **German Traffic Sign Recognition Benchmark** (**GTSRB**) contains 43 classes of traffic signs, split into 39,209 training images and 12,630 test images. The images have varying light conditions and rich backgrounds.\\r\\n\\r\\nSource: [Invisible Backdoor Attacks Against Deep Neural Networks](https://arxiv.org/abs/1909.02742)\\r\\nImage Source: [https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388](https://www.researchgate.net/figure/An-example-of-the-43-traffic-sign-classes-of-GTSRB-dataset_fig9_311896388)',\n",
       "  'paper': {'title': 'computer: Benchmarking machine learning algorithms for traffic sign recognition',\n",
       "   'url': 'http://www.sciencedirect.com/science/article/pii/S0893608012000457'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Traffic Sign Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/traffic-sign-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GTSRB', 'Synth Signs-to-GTSRB', 'SYNSIG-to-GTSRB'],\n",
       "  'num_papers': 183,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tsinghua-tencent-100k',\n",
       "  'name': 'Tsinghua-Tencent 100K',\n",
       "  'full_name': 'Traffic-Sign Detection and Classification in the Wild',\n",
       "  'homepage': 'https://cg.cs.tsinghua.edu.cn/traffic-sign/',\n",
       "  'description': 'Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. We call this benchmark Tsinghua-Tencent 100K. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Traffic Sign Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/traffic-sign-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tsinghua-Tencent 100K'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pa-100k',\n",
       "  'name': 'PA-100K',\n",
       "  'full_name': 'PA-100K Dataset',\n",
       "  'homepage': 'https://github.com/xh-liu/HydraPlus-Net',\n",
       "  'description': '**PA-100K** is a recent-proposed large pedestrian attribute dataset, with 100,000 images in total collected from outdoor surveillance cameras. It is split into 80,000 images for the training set, and 10,000 for the validation set and 10,000 for the test set. This dataset is labeled by 26 binary attributes. The common features existing in both selected dataset is that the images are blurry due to the relatively low resolution and the positive ratio of each binary attribute is low.\\r\\n\\r\\nSource: [Localization Guided Learning for Pedestrian Attribute Recognition](https://arxiv.org/abs/1808.09102)\\r\\nImage Source: [https://github.com/xh-liu/HydraPlus-Net](https://github.com/xh-liu/HydraPlus-Net)',\n",
       "  'paper': {'title': 'HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/hydraplus-net-attentive-deep-features-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pedestrian Attribute Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-attribute-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PA-100K'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://github.com/xh-liu/HydraPlus-Net',\n",
       "    'repo': 'https://github.com/xh-liu/HydraPlus-Net',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/peta',\n",
       "  'name': 'PETA',\n",
       "  'full_name': 'Pedestrian Attribute',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/projects/PETA.html',\n",
       "  'description': 'The PEdesTrian Attribute dataset (**PETA**) is a dataset fore recognizing pedestrian attributes, such as gender and clothing style, at a far distance. It is of interest in video surveillance scenarios where face and body close-shots and hardly available. It consists of 19,000 pedestrian images with 65 attributes (61 binary and 4 multi-class). Those images contain 8705 persons.\\r\\n\\r\\nSource: [Attribute Aware Pooling for Pedestrian Attribute Recognition](https://arxiv.org/abs/1907.11837)\\r\\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/PETA.html](http://mmlab.ie.cuhk.edu.hk/projects/PETA.html)',\n",
       "  'paper': {'title': 'Pedestrian Attribute Recognition At Far Distance',\n",
       "   'url': 'https://doi.org/10.1145/2647868.2654966'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pedestrian Attribute Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-attribute-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PETA'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rap',\n",
       "  'name': 'RAP',\n",
       "  'full_name': 'Richly Annotated Pedestrian',\n",
       "  'homepage': 'http://www.rapdataset.com/',\n",
       "  'description': 'The **Richly Annotated Pedestrian** (**RAP**) dataset is a dataset for pedestrian attribute recognition. It contains 41,585 images collected from indoor surveillance cameras. Each image is annotated with 72 attributes, while only 51 binary attributes with the positive ratio above 1% are selected for evaluation. There are 33,268 images for the training set and 8,317 for testing.\\r\\n\\r\\nSource: [Localization Guided Learning for Pedestrian Attribute Recognition](https://arxiv.org/abs/1808.09102)\\r\\nImage Source: [http://www.rapdataset.com/rapv1.html](http://www.rapdataset.com/rapv1.html)',\n",
       "  'paper': {'title': 'A Richly Annotated Dataset for Pedestrian Attribute Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/a-richly-annotated-dataset-for-pedestrian'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pedestrian Attribute Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-attribute-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RAP'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/phc-u373',\n",
       "  'name': 'PhC-U373',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Briefly describe the dataset. Provide:\\r\\n\\r\\n* a high-level explanation of the dataset characteristics\\r\\n* explain motivations and summary of its content\\r\\n* potential use cases of the dataset\\r\\n\\r\\nIf the description or image is from a different paper, please refer to it as follows:\\r\\nSource: [title](url)\\r\\nImage Source: [title](url)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Cell Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/cell-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PhC-U373'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/drive',\n",
       "  'name': 'DRIVE',\n",
       "  'full_name': 'Digital Retinal Images for Vessel Extraction',\n",
       "  'homepage': 'https://drive.grand-challenge.org/',\n",
       "  'description': 'The **Digital Retinal Images for Vessel Extraction** (**DRIVE**) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). \\r\\n\\r\\nThe set of 40 images was equally divided into 20 images for the training set and 20 images for the testing set. Inside both sets, for each image, there is circular field of view (FOV) mask of diameter that is approximately 540 pixels. Inside training set, for each image, one manual segmentation by an ophthalmological expert has been applied. Inside testing set, for each image, two manual segmentations have been applied by two different observers, where the first observer segmentation is accepted as the ground-truth for performance evaluation.\\r\\n\\r\\nSource: [Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation](https://arxiv.org/abs/1403.1735)\\r\\nImage Source: [https://drive.grand-challenge.org/](https://drive.grand-challenge.org/)',\n",
       "  'paper': {'title': 'Ridge-based vessel segmentation in color images of the retina',\n",
       "   'url': 'https://doi.org/10.1109/TMI.2004.825627'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Retinal Vessel Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/retinal-vessel-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DRIVE'],\n",
       "  'num_papers': 170,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/stare',\n",
       "  'name': 'STARE',\n",
       "  'full_name': 'Structured Analysis of the Retina',\n",
       "  'homepage': 'https://cecas.clemson.edu/~ahoover/stare/',\n",
       "  'description': 'The **STARE** (**Structured Analysis of the Retina**) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700×605) color fundus images. For each image, two groups of annotations are provided..\\r\\n\\r\\nSource: [DPN: Detail-Preserving Network with High Resolution Representation for Efficient Segmentation of Retinal Vessels](https://arxiv.org/abs/2009.12053)\\r\\nImage Source: [https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756](https://www.researchgate.net/figure/Results-of-the-different-methods-applied-to-the-STARE-dataset-a-original-image-b_fig4_279215756)',\n",
       "  'paper': {'title': 'Locating blood vessels in retinal images by piecewise threshold probing of a matched filter response',\n",
       "   'url': 'http://cecas.clemson.edu/~ahoover/stare/'},\n",
       "  'introduced_date': '1998-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Retinal Vessel Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/retinal-vessel-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['STARE'],\n",
       "  'num_papers': 84,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/chase-db1',\n",
       "  'name': 'CHASE_DB1',\n",
       "  'full_name': 'CHASE_DB1',\n",
       "  'homepage': 'https://blogs.kingston.ac.uk/retinal/chasedb1/',\n",
       "  'description': '**CHASE_DB1** is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999×960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts.\\r\\n\\r\\nSource: [MixModule: Mixed CNN Kernel Module for Medical Image Segmentation](https://arxiv.org/abs/1910.08728)\\r\\nImage Source: [https://www.mdpi.com/2073-8994/9/11/276](https://www.mdpi.com/2073-8994/9/11/276)',\n",
       "  'paper': {'title': 'An Ensemble Classification-Based Approach Applied to Retinal Blood Vessel Segmentation',\n",
       "   'url': 'https://doi.org/10.1109/TBME.2012.2205687'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Retinal Vessel Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/retinal-vessel-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CHASE_DB1'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/luna',\n",
       "  'name': 'LUNA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://luna16.grand-challenge.org/',\n",
       "  'description': 'The **LUNA** challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In [LUNA16](https://paperswithcode.com/dataset/luna16), participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified.\\r\\n\\r\\nSource: [Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge](/paper/validation-comparison-and-combination-of)',\n",
       "  'paper': {'title': 'Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/validation-comparison-and-combination-of'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Lung Nodule Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/lung-nodule-segmentation'},\n",
       "   {'task': 'Computed Tomography (CT)',\n",
       "    'url': 'https://paperswithcode.com/task/computed-tomography-ct'},\n",
       "   {'task': 'Lung Nodule Detection',\n",
       "    'url': 'https://paperswithcode.com/task/lung-nodule-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LUNA', 'LUNA2016 FPRED', 'LUNA16'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tox21-1',\n",
       "  'name': 'Tox21',\n",
       "  'full_name': 'Tox21 Machine Learning Data Set',\n",
       "  'homepage': 'http://bioinf.jku.at/research/DeepTox/tox21.html',\n",
       "  'description': 'The **Tox21** data set comprises 12,060 training samples and 647 test samples that represent chemical compounds. There are 801 \"dense features\" that represent chemical descriptors, such as molecular weight, solubility or surface area, and 272,776 \"sparse features\" that represent chemical substructures (ECFP10, DFS6, DFS8; stored in Matrix Market Format ). Machine learning methods can either use sparse or dense data or combine them. For each sample there are 12 binary labels that represent the outcome (active/inactive) of 12 different toxicological experiments. Note that the label matrix contains many missing values (NAs). The original data source and Tox21 challenge site is https://tripod.nih.gov/tox21/challenge/.\\n\\nSource: [Tox21 Machine Learning Data Set](http://bioinf.jku.at/research/DeepTox/tox21.html)\\nImage Source: [https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full](https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080/full)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Drug Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/drug-discovery'},\n",
       "   {'task': 'Graph Regression',\n",
       "    'url': 'https://paperswithcode.com/task/graph-regression'},\n",
       "   {'task': 'Molecular Property Prediction (1-shot))',\n",
       "    'url': 'https://paperswithcode.com/task/molecular-property-prediction-1-shot'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tox21', 'Tox21 '],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/qm9',\n",
       "  'name': 'QM9',\n",
       "  'full_name': 'Quantum Machines 9',\n",
       "  'homepage': 'http://quantum-machine.org/datasets/',\n",
       "  'description': '**QM9** provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.\\n\\nSource: [QM9 Dataset](http://quantum-machine.org/datasets/)\\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/ci300415d](https://pubs.acs.org/doi/pdf/10.1021/ci300415d)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Drug Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/drug-discovery'},\n",
       "   {'task': 'Formation Energy',\n",
       "    'url': 'https://paperswithcode.com/task/formation-energy'},\n",
       "   {'task': 'NMR J-coupling',\n",
       "    'url': 'https://paperswithcode.com/task/nmr-j-coupling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['QM9'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.QM9Dataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#qm9',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/douban',\n",
       "  'name': 'Douban',\n",
       "  'full_name': 'Douban Conversation Corpus',\n",
       "  'homepage': 'https://github.com/MarkWuNLP/MultiTurnResponseSelection',\n",
       "  'description': 'We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. \\r\\n\\r\\n|      |Train|Val| Test         | \\r\\n| ------------- |:-------------:|:-------------:|:-------------:|\\r\\n| session-response pairs  | 1m|50k| 10k |\\r\\n| Avg. positive response per session     | 1|1| 1.18    | \\r\\n| Fless Kappa | N\\\\A|N\\\\A|0.41      | \\r\\n| Min turn per session | 3|3| 3      | \\r\\n| Max ture per session | 98|91|45    | \\r\\n| Average turn per session | 6.69|6.75|5.95    | \\r\\n| Average Word per utterance | 18.56|18.50|20.74   | \\r\\n\\r\\n\\r\\nThe test data contains 1000 dialogue context, and for each context we create 10 responses as candidates. We recruited three labelers to judge if a candidate is a proper response to the session. A proper response means the response can naturally reply to the message given the context. Each pair received three labels and the majority of the labels was taken as the final decision.\\r\\n\\r\\n<br>\\r\\nAs far as we known, this is the first human-labeled test set for retrieval-based chatbots. The entire corpus link https://www.dropbox.com/s/90t0qtji9ow20ca/DoubanConversaionCorpus.zip?dl=0',\n",
       "  'paper': {'title': 'Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots',\n",
       "   'url': 'https://paperswithcode.com/paper/sequential-matching-network-a-new'},\n",
       "  'introduced_date': '2016-12-06',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Conversational Response Selection',\n",
       "    'url': 'https://paperswithcode.com/task/conversational-response-selection'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['Douban', 'Douban Monti'],\n",
       "  'num_papers': 65,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/criteo',\n",
       "  'name': 'Criteo',\n",
       "  'full_name': 'Display Advertising Challenge',\n",
       "  'homepage': 'https://labs.criteo.com/2013/12/download-terabyte-click-logs/',\n",
       "  'description': '**Criteo** contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset.\\n\\nSource: [AMER: Automatic Behavior Modeling and Interaction Exploration in Recommender System](https://arxiv.org/abs/2006.05933)\\nImage Source: [https://www.kaggle.com/c/criteo-display-ad-challenge](https://www.kaggle.com/c/criteo-display-ad-challenge)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Click-Through Rate Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/click-through-rate-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Criteo'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': [{'url': 'https://github.com/licangseng/recommentetion',\n",
       "    'repo': 'https://github.com/licangseng/recommentetion',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ipinyou',\n",
       "  'name': 'iPinYou',\n",
       "  'full_name': 'iPinYou Global RTB Bidding Algorithm Competition Dataset',\n",
       "  'homepage': 'https://contest.ipinyou.com/',\n",
       "  'description': \"The **iPinYou** Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs.\\n\\nSource: [iPinYou Global RTB Bidding Algorithm Competition Dataset](https://contest.ipinyou.com/)\\nImage Source: [http://contest.ipinyou.com/ipinyou-dataset.pdf](http://contest.ipinyou.com/ipinyou-dataset.pdf)\",\n",
       "  'paper': {'title': 'iPinYou Global RTB Bidding Algorithm Competition Dataset',\n",
       "   'url': 'https://doi.org/10.1145/2648584.2648590'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Click-Through Rate Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/click-through-rate-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iPinYou'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': [{'url': 'https://github.com/tensorflow/recommenders/tree/v0.5.1',\n",
       "    'repo': 'https://github.com/tensorflow/recommenders',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-person-part',\n",
       "  'name': 'PASCAL-Part',\n",
       "  'full_name': 'PASCAL-Part',\n",
       "  'homepage': 'http://roozbehm.info/pascal-parts/pascal-parts.html',\n",
       "  'description': '**PASCAL-Part** is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object. For categories that do not have a consistent set of parts (e.g., boat), it provides the silhouette annotation. \\r\\n\\r\\nIt can also serve as a set for human semantic part segmentation: It contains multiple humans per image in unconstrained poses and occlusions (1,716 for training and 1,817 for testing). It provides careful pixel-wise annotations for six body parts (i.e., head, torso, upper/lower-arms, and upper-/lower-legs).\\r\\n\\r\\nSource: [The Ultimate Theory of Human Parsing](https://arxiv.org/abs/2001.06804)\\r\\nImage Source: [https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png](https://www.researchgate.net/profile/Zhedong_Zheng/publication/328123707/figure/fig4/AS:704683136016384@1545020960225/Qualitative-parsing-results-on-the-Pascal-Person-Part-dataset.png)',\n",
       "  'paper': {'title': 'Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts',\n",
       "   'url': 'https://paperswithcode.com/paper/detect-what-you-can-detecting-and'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Human Part Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/human-part-segmentation'},\n",
       "   {'task': 'Multi-Human Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/multi-human-parsing'},\n",
       "   {'task': 'Semantic Part Detection',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-part-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL-Part', 'PASCAL Part 2010 - Animals'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/citeseer',\n",
       "  'name': 'Citeseer',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://linqs.soe.ucsc.edu/data',\n",
       "  'description': 'The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words.\\r\\n\\r\\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)',\n",
       "  'paper': {'title': 'CiteSeer: An Automatic Citation Indexing System',\n",
       "   'url': 'https://doi.org/10.1145/276675.276685'},\n",
       "  'introduced_date': '1998-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Node Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/node-clustering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Citeseer (weighted evaluation)',\n",
       "   'Citeseer random partition',\n",
       "   'Citeseer Full-supervised',\n",
       "   'Citeseer (nonstandard variant)',\n",
       "   'Citeseer (biased evaluation)',\n",
       "   'Citeseer',\n",
       "   'CiteSeer with Public Split: fixed 5 nodes per class',\n",
       "   'CiteSeer with Public Split: fixed 20 nodes per class',\n",
       "   'CiteSeer (1%)',\n",
       "   'CiteSeer (0.5%)'],\n",
       "  'num_papers': 178,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.CiteseerGraphDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#citation',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cora',\n",
       "  'name': 'Cora',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://relational.fit.cvut.cz/dataset/CORA',\n",
       "  'description': 'The **Cora** dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\\r\\n\\r\\nSource: [https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)\\r\\nImage Source: [https://arxiv.org/abs/1611.08402](https://arxiv.org/abs/1611.08402)',\n",
       "  'paper': {'title': 'Automating the Construction of Internet Portals with Machine Learning',\n",
       "   'url': 'https://doi.org/10.1023/A:1009953814988'},\n",
       "  'introduced_date': '2000-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-classification'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'},\n",
       "   {'task': 'Node Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/node-clustering'},\n",
       "   {'task': 'Graph structure learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-structure-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cora (weighted evaluation)',\n",
       "   'Cora: fixed 5 node per class',\n",
       "   'Cora: fixed 20 node per class',\n",
       "   'Cora: fixed 10 node per class',\n",
       "   'Cora random partition',\n",
       "   'Cora Full-supervised',\n",
       "   'Cora (nonstandard variant)',\n",
       "   'Cora (biased evaluation)',\n",
       "   'Cora with Public Split: fixed 20 nodes per class',\n",
       "   'Cora (3%)',\n",
       "   'Cora (1%)',\n",
       "   'Cora (0.5%)',\n",
       "   'Cora'],\n",
       "  'num_papers': 278,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid',\n",
       "    'repo': 'https://github.com/pyg-team/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#citation-network-dataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#citation',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pubmed',\n",
       "  'name': 'Pubmed',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://linqs.soe.ucsc.edu/data',\n",
       "  'description': 'The **Pubmed** dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words.\\r\\n\\r\\nSource: [https://linqs.soe.ucsc.edu/data](https://linqs.soe.ucsc.edu/data)',\n",
       "  'paper': {'title': 'Collective Classification in Network Data',\n",
       "   'url': 'http://www.aaai.org/ojs/index.php/aimagazine/article/view/2157'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Node Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/node-clustering'},\n",
       "   {'task': 'Sentence Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Pubmed (weighted evaluation)',\n",
       "   'Pubmed random partition',\n",
       "   'Pubmed (nonstandard variant)',\n",
       "   'Pubmed (biased evaluation)',\n",
       "   'PubMed 20k RCT',\n",
       "   'Pubmed Full-supervised',\n",
       "   'Pubmed',\n",
       "   'PubMed with Public Split: fixed 20 nodes per class',\n",
       "   'PubMed (0.1%)',\n",
       "   'PubMed (0.05%)',\n",
       "   'PubMed (0.03%)'],\n",
       "  'num_papers': 617,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/pubmed',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.PubmedGraphDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#citation',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nell',\n",
       "  'name': 'NELL',\n",
       "  'full_name': 'Never Ending Language Learning',\n",
       "  'homepage': 'http://rtw.ml.cmu.edu/rtw/',\n",
       "  'description': '**NELL** is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs.\\r\\n\\r\\nSource: [A Survey on Knowledge Graphs: Representation, Acquisition and Applications](https://arxiv.org/abs/2002.00388)\\r\\nImage Source: [http://rtw.ml.cmu.edu/rtw/](http://rtw.ml.cmu.edu/rtw/)',\n",
       "  'paper': {'title': 'Toward an Architecture for Never-Ending Language Learning',\n",
       "   'url': 'http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1879'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['NELL'],\n",
       "  'num_papers': 113,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://huggingface.co/datasets/nell',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/blogcatalog',\n",
       "  'name': 'BlogCatalog',\n",
       "  'full_name': 'BlogCatalog',\n",
       "  'homepage': 'http://networkrepository.com/soc-BlogCatalog.php',\n",
       "  'description': '**BlogCatalog** is a graph dataset for a network of social relationships of bloggers listed in the BlogCatalog website. The network has 88,800 nodes and 2.1M edges.\\n\\nSource: [Graph Representation Learning: A Survey](https://arxiv.org/abs/1909.00958)\\nImage Source: [https://www.blogcatalog.com/](https://www.blogcatalog.com/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BlogCatalog'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wn18',\n",
       "  'name': 'WN18',\n",
       "  'full_name': 'WordNet18',\n",
       "  'homepage': 'https://everest.hds.utc.fr/doku.php?id=en:transe',\n",
       "  'description': 'The **WN18** dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue.\\r\\n\\r\\nSource: [http://nlpprogress.com/english/relation_prediction.html](http://nlpprogress.com/english/relation_prediction.html)',\n",
       "  'paper': {'title': 'Translating Embeddings for Modeling Multi-relational Data',\n",
       "   'url': 'https://paperswithcode.com/paper/translating-embeddings-for-modeling-multi'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Knowledge Graph Completion',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graph-completion'},\n",
       "   {'task': 'Ancestor-descendant prediction',\n",
       "    'url': 'https://paperswithcode.com/task/ancestor-descendant-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WN18', 'WN18RR', 'WN18 (filtered)'],\n",
       "  'num_papers': 279,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.WN18Dataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wordnet',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/scan2cad',\n",
       "  'name': 'Scan2CAD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/skanti/Scan2CAD',\n",
       "  'description': '**Scan2CAD** is an alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans. The top 3 annotated model classes are chairs, tables and cabinets which arises due to the nature of indoor scenes in ScanNet. The number of objects aligned per scene ranges from 1 to 40 with an average of 9.3.\\r\\n\\r\\nAdditionally, all ShapeNet CAD models used in the Scan2CAD dataset are annotated with their rotational symmetries: either none, 2-fold, 4-fold or infinite rotational symmetries around a canonical axis of the object.\\r\\n\\r\\nSource: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)\\r\\nImage Source: [Scan2CAD: Learning CAD Model Alignment in RGB-D Scans](https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb/)',\n",
       "  'paper': {'title': 'Scan2CAD: Learning CAD Model Alignment in RGB-D Scans',\n",
       "   'url': 'https://paperswithcode.com/paper/scan2cad-learning-cad-model-alignment-in-rgb'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', '3d meshes', 'Cad'],\n",
       "  'tasks': [{'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Scan2CAD'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://github.com/skanti/Scan2CAD',\n",
       "    'repo': 'https://github.com/skanti/Scan2CAD',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/utkface',\n",
       "  'name': 'UTKFace',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://susanqq.github.io/UTKFace/',\n",
       "  'description': 'The **UTKFace** dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc.\\r\\n\\r\\nSource: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)\\r\\nImage Source: [https://susanqq.github.io/UTKFace/](https://susanqq.github.io/UTKFace/)',\n",
       "  'paper': {'title': 'Age Progression/Regression by Conditional Adversarial Autoencoder',\n",
       "   'url': 'https://paperswithcode.com/paper/age-progressionregression-by-conditional'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UTKFace'],\n",
       "  'num_papers': 99,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/afad',\n",
       "  'name': 'AFAD',\n",
       "  'full_name': 'Asian Face Age Dataset',\n",
       "  'homepage': 'https://afad-dataset.github.io/',\n",
       "  'description': 'The Asian Face Age Dataset (AFAD) is a new dataset proposed for evaluating the performance of age estimation, which contains more than 160K facial images and the corresponding age and gender labels. This dataset is oriented to age estimation on Asian faces, so all the facial images are for Asian faces. It is noted that the AFAD is the biggest dataset for age estimation to date. It is well suited to evaluate how deep learning methods can be adopted for age estimation.',\n",
       "  'paper': {'title': 'Ordinal Regression With Multiple Output CNN for Age Estimation',\n",
       "   'url': 'https://paperswithcode.com/paper/ordinal-regression-with-multiple-output-cnn'},\n",
       "  'introduced_date': '2016-06-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cacd',\n",
       "  'name': 'CACD',\n",
       "  'full_name': 'Cross-Age Celebrity Dataset',\n",
       "  'homepage': 'https://bcsiriuschen.github.io/CARC/',\n",
       "  'description': 'The **Cross-Age Celebrity Dataset** (**CACD**) contains 163,446 images from 2,000 celebrities collected from the Internet. The images are collected from search engines using celebrity name and year (2004-2013) as keywords. Therefore, it is possible to estimate the ages of the celebrities on the images by simply subtract the birth year from the year of which the photo was taken.\\r\\n\\r\\nSource: [https://bcsiriuschen.github.io/CARC/](https://bcsiriuschen.github.io/CARC/)\\r\\nImage Source: [https://www.pkuml.org/resources/pku-vehicleid.html](https://www.pkuml.org/resources/pku-vehicleid.html)',\n",
       "  'paper': {'title': 'Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10599-4_49'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CACD'],\n",
       "  'num_papers': 51,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jigsaws',\n",
       "  'name': 'JIGSAWS',\n",
       "  'full_name': 'JHU-ISI Gesture and Skill Assessment Working Set',\n",
       "  'homepage': 'https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release',\n",
       "  'description': 'The **JHU-ISI Gesture and Skill Assessment Working Set** (**JIGSAWS**) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:\\r\\n\\r\\n* kinematic data: Cartesian positions, orientations, velocities, angular velocities and gripper angle describing the motion of the manipulators.\\r\\n* video data: stereo video captured from the endoscopic camera. Sample videos of the JIGSAWS tasks can be downloaded from the official webpage.\\r\\n* manual annotations including:\\r\\n* gesture (atomic surgical activity segment labels).\\r\\n* skill (global rating score using modified objective structured assessments of technical skills).\\r\\n* experimental setup: a standardized cross-validation experimental setup that can be used to evaluate automatic surgical gesture recognition and skill assessment methods.\\r\\n\\r\\nSource: [https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release](https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release)\\r\\nImage Source: [https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release](https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release)',\n",
       "  'paper': {'title': 'Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling',\n",
       "   'url': 'https://cirl.lcsr.jhu.edu/wp-content/uploads/2015/11/JIGSAWS.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'},\n",
       "   {'task': 'Surgical Skills Evaluation',\n",
       "    'url': 'https://paperswithcode.com/task/surgical-skills-evaluation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JIGSAWS'],\n",
       "  'num_papers': 62,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/compcars',\n",
       "  'name': 'CompCars',\n",
       "  'full_name': 'Comprehensive Cars',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/datasets/comp_cars/index.html',\n",
       "  'description': 'The **Comprehensive Cars (CompCars)** dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. \\r\\n\\r\\nThe dataset can be used for the tasks of:\\r\\n\\r\\n- Fine-grained classification\\r\\n- Attribute prediction\\r\\n- Car model verification\\r\\n\\r\\nThe dataset can be also used for other tasks such as image ranking, multi-task learning, and 3D reconstruction.',\n",
       "  'paper': {'title': 'A Large-Scale Car Dataset for Fine-Grained Categorization and Verification',\n",
       "   'url': 'https://paperswithcode.com/paper/a-large-scale-car-dataset-for-fine-grained'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CompCars'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rt-gene',\n",
       "  'name': 'RT-GENE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://zenodo.org/record/2529036',\n",
       "  'description': 'Presents a diverse eye-gaze dataset.\\r\\n\\r\\nSource: [RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments](/paper/rt-gene-real-time-eye-gaze-estimation-in)',\n",
       "  'paper': {'title': 'RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments',\n",
       "   'url': 'https://paperswithcode.com/paper/rt-gene-real-time-eye-gaze-estimation-in'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Gaze Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/gaze-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RT-GENE'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wn18rr',\n",
       "  'name': 'WN18RR',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/TimDettmers/ConvE',\n",
       "  'description': '**WN18RR** is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types.\\r\\n\\r\\nSource: [End-to-end Structure-Aware Convolutional Networks for Knowledge Base Completion](https://arxiv.org/abs/1811.04441)',\n",
       "  'paper': {'title': 'Convolutional 2D Knowledge Graph Embeddings',\n",
       "   'url': 'https://paperswithcode.com/paper/convolutional-2d-knowledge-graph-embeddings'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Knowledge Graph Completion',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graph-completion'},\n",
       "   {'task': 'Ancestor-descendant prediction',\n",
       "    'url': 'https://paperswithcode.com/task/ancestor-descendant-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WN18RR'],\n",
       "  'num_papers': 250,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/TimDettmers/ConvE',\n",
       "    'repo': 'https://github.com/TimDettmers/ConvE',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/t-less',\n",
       "  'name': 'T-LESS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cmp.felk.cvut.cz/t-less/',\n",
       "  'description': '**T-LESS** is a dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects.\\r\\n\\r\\nSource: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)\\r\\nImage Source: [http://cmp.felk.cvut.cz/t-less/](http://cmp.felk.cvut.cz/t-less/)',\n",
       "  'paper': {'title': 'T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects',\n",
       "   'url': 'https://paperswithcode.com/paper/t-less-an-rgb-d-dataset-for-6d-pose'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D', 'RGB-D'],\n",
       "  'tasks': [{'task': '6D Pose Estimation using RGB',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation'},\n",
       "   {'task': '6D Pose Estimation using RGBD',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-using-rgbd'}],\n",
       "  'languages': [],\n",
       "  'variants': ['T-LESS'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ace-2004',\n",
       "  'name': 'ACE 2004',\n",
       "  'full_name': 'ACE 2004 Multilingual Training Corpus',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2005T09',\n",
       "  'description': '**ACE 2004** Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program.\\r\\nThe objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic.\\r\\n\\r\\nSource: [https://catalog.ldc.upenn.edu/LDC2005T09](https://catalog.ldc.upenn.edu/LDC2005T09)',\n",
       "  'paper': {'title': 'Ace 2004 multilingual training corpus',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC2005T09'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Entity Disambiguation',\n",
       "    'url': 'https://paperswithcode.com/task/entity-disambiguation'},\n",
       "   {'task': 'Nested Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/nested-named-entity-recognition'},\n",
       "   {'task': 'Nested Mention Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/nested-mention-recognition'}],\n",
       "  'languages': ['English', 'Mandarin Chinese', 'Standard Arabic'],\n",
       "  'variants': ['ACE 2004', 'ACE2004'],\n",
       "  'num_papers': 33,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ace-2005',\n",
       "  'name': 'ACE 2005',\n",
       "  'full_name': 'ACE 2005 Multilingual Training Corpus',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2006T06',\n",
       "  'description': '**ACE 2005** Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC.\\n\\nSource: [https://catalog.ldc.upenn.edu/LDC2006T06](https://catalog.ldc.upenn.edu/LDC2006T06)\\nImage Source: [https://arxiv.org/pdf/1811.06031.pdf](https://arxiv.org/pdf/1811.06031.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Joint Entity and Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/joint-entity-and-relation-extraction'},\n",
       "   {'task': 'Nested Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/nested-named-entity-recognition'},\n",
       "   {'task': 'Nested Mention Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/nested-mention-recognition'}],\n",
       "  'languages': ['English', 'Mandarin Chinese', 'Standard Arabic'],\n",
       "  'variants': ['ACE 2005'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/genia',\n",
       "  'name': 'GENIA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.geniaproject.org/genia-corpus',\n",
       "  'description': 'The **GENIA** corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology.\\r\\n\\r\\nThe corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms “human”, “blood cells”, and “transcription factors”. The corpus has been annotated with various levels of linguistic and semantic information.\\r\\n\\r\\nThe primary categories of annotation in the GENIA corpus and the corresponding subcorpora are:\\r\\n\\r\\n* Part-of-Speech annotation\\r\\n* Constituency (phrase structure) syntactic annotation\\r\\n* Term annotation\\r\\n* Event annotation\\r\\n* Relation annotation\\r\\n* Coreference annotation\\r\\n\\r\\nSource: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)\\r\\nImage Source: [http://www.geniaproject.org/genia-corpus](http://www.geniaproject.org/genia-corpus)',\n",
       "  'paper': {'title': 'GENIA corpus - a semantically annotated corpus for bio-textmining',\n",
       "   'url': 'http://bioinformatics.oupjournals.org/cgi/content/abstract/19/suppl_1/i180?etoc'},\n",
       "  'introduced_date': '2003-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Dependency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/dependency-parsing'},\n",
       "   {'task': 'Event Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/event-extraction'},\n",
       "   {'task': 'Nested Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/nested-named-entity-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['GENIA', 'GENIA - LAS', 'GENIA - UAS', 'GENIA 2013'],\n",
       "  'num_papers': 76,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2',\n",
       "  'name': 'SemEval 2014 Task 4 Sub Task 2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://alt.qcri.org/semeval2014/task4/',\n",
       "  'description': 'Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.\\r\\n\\r\\n***Subtask 2: Aspect term polarity***\\r\\n\\r\\nFor a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).\\r\\n\\r\\nFor example:\\r\\n\\r\\n“I loved their fajitas” → {fajitas: positive}\\r\\n“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}\\r\\n“The fajitas are their first plate” → {fajitas: neutral}\\r\\n“The fajitas were great to taste, but not to see” → {fajitas: conflict}',\n",
       "  'paper': {'title': 'SemEval-2014 Task 4: Aspect Based Sentiment Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/semeval-2014-task-4-aspect-based-sentiment'},\n",
       "  'introduced_date': '2014-08-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Aspect-Based Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/aspect-based-sentiment-analysis'},\n",
       "   {'task': 'Aspect Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/aspect-extraction'},\n",
       "   {'task': 'Aspect-oriented  Opinion Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/aspect-oriented-opinion-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SemEval 2014 Task 4 Sub Task 2'],\n",
       "  'num_papers': 49,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/Charitarth/SemEval2014-Task4',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ohsumed',\n",
       "  'name': 'Ohsumed',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://disi.unitn.it/moschitti/corpora.htm',\n",
       "  'description': '**Ohsumed** includes medical abstracts from the MeSH categories of the year 1991. In [Joachims, 1997] were used the first 20,000 documents divided in 10,000 for training and 10,000 for testing. The specific task was to categorize the 23 cardiovascular diseases categories. After selecting the such category subset, the unique abstract number becomes 13,929 (6,286 for training and 7,643 for testing). As current computers can easily manage larger number of documents we make available all 34,389 cardiovascular diseases abstracts out of 50,216 medical abstracts contained in the year 1991.\\r\\n\\r\\nSource: [http://disi.unitn.it/moschitti/corpora.htm](http://disi.unitn.it/moschitti/corpora.htm)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Ohsumed'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mr',\n",
       "  'name': 'MR',\n",
       "  'full_name': 'MR Movie Reviews',\n",
       "  'homepage': 'http://www.cs.cornell.edu/people/pabo/movie-review-data/',\n",
       "  'description': '**MR** Movie Reviews is a dataset for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity.\\n\\nSource: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\\nImage Source: [https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png](https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MR'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sts-benchmark',\n",
       "  'name': 'STS Benchmark',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark',\n",
       "  'description': 'STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums.\\r\\n\\r\\nSource: [STS Benchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'}],\n",
       "  'languages': [],\n",
       "  'variants': ['STS Benchmark'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/weibo-ner',\n",
       "  'name': 'Weibo NER',\n",
       "  'full_name': 'Weibo NER',\n",
       "  'homepage': 'https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/Weibo',\n",
       "  'description': 'The **Weibo NER** dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo.\\r\\n\\r\\nSource: [Chinese NER Using Lattice LSTM](https://arxiv.org/abs/1805.02023)\\r\\nImage Source: [https://en.wikipedia.org/wiki/Sina_Weibo](https://en.wikipedia.org/wiki/Sina_Weibo)',\n",
       "  'paper': {'title': 'Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings',\n",
       "   'url': 'https://paperswithcode.com/paper/named-entity-recognition-for-chinese-social'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Chinese Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-named-entity-recognition'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['Weibo NER'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/weibo_ner',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/OYE93/Chinese-NLP-Corpus',\n",
       "    'repo': 'https://github.com/OYE93/Chinese-NLP-Corpus',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/resume-ner',\n",
       "  'name': 'Resume NER',\n",
       "  'full_name': 'Resume NER',\n",
       "  'homepage': 'https://arxiv.org/pdf/1805.02023.pdf',\n",
       "  'description': 'Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%.\\n\\nSource: [Query-Based Named Entity Recognition](https://arxiv.org/abs/1908.09138)\\nImage Source: [https://arxiv.org/pdf/1805.02023.pdf](https://arxiv.org/pdf/1805.02023.pdf)',\n",
       "  'paper': {'title': 'Chinese NER Using Lattice LSTM',\n",
       "   'url': 'https://paperswithcode.com/paper/chinese-ner-using-lattice-lstm'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Chinese Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-named-entity-recognition'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['Resume NER'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/reuters-21578',\n",
       "  'name': 'Reuters-21578',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html',\n",
       "  'description': 'The **Reuters-21578** dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words.\\r\\n\\r\\nSource: [Topic Model Based Multi-Label Classification from the Crowd](https://arxiv.org/abs/1604.00783)',\n",
       "  'paper': {'title': 'Reuters-21578',\n",
       "   'url': 'http://www.daviddlewis.com/resources/testcollections/reuters21578'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Multi-Label Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-text-classification'},\n",
       "   {'task': 'Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-classification'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Reuters-21578'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/reuters21578',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fce',\n",
       "  'name': 'FCE',\n",
       "  'full_name': 'First Certificate in English',\n",
       "  'homepage': 'https://ilexir.co.uk/datasets/index.html',\n",
       "  'description': 'The Cambridge Learner Corpus **First Certificate in English** (CLC **FCE**) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively.\\r\\n\\r\\nSource: [Compositional Sequence Labeling Models for Error Detection in Learner Writing](https://arxiv.org/abs/1607.06153)',\n",
       "  'paper': {'title': 'A New Dataset and Method for Automatically Grading ESOL Texts',\n",
       "   'url': 'https://www.aclweb.org/anthology/P11-1019/'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Grammatical Error Detection',\n",
       "    'url': 'https://paperswithcode.com/task/grammatical-error-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FCE'],\n",
       "  'num_papers': 119,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tacred',\n",
       "  'name': 'TACRED',\n",
       "  'full_name': 'The TAC Relation Extraction Dataset',\n",
       "  'homepage': 'https://nlp.stanford.edu/projects/tacred/',\n",
       "  'description': 'TACRED is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. These examples are created by combining available human annotations from the TAC KBP challenges and crowdsourcing.\\r\\n\\r\\nSource: https://nlp.stanford.edu/projects/tacred/',\n",
       "  'paper': {'title': 'Position-aware Attention and Supervised Data Improve Slot Filling',\n",
       "   'url': 'https://paperswithcode.com/paper/position-aware-attention-and-supervised-data'},\n",
       "  'introduced_date': '2017-09-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Relation Classification',\n",
       "    'url': 'https://paperswithcode.com/task/relation-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TACRED'],\n",
       "  'num_papers': 83,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/natural-questions',\n",
       "  'name': 'Natural Questions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ai.google.com/research/NaturalQuestions',\n",
       "  'description': 'The **Natural Questions** corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is “yes” or “no”, instead of a list of short spans.\\r\\n\\r\\nSource: [A BERT Baseline for the Natural Questions](https://arxiv.org/abs/1901.08634)\\r\\nImage Source: [https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question/](https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question/)',\n",
       "  'paper': {'title': 'Natural Questions: a Benchmark for Question Answering Research',\n",
       "   'url': 'https://paperswithcode.com/paper/natural-questions-a-benchmark-for-question'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'},\n",
       "   {'task': 'Question Generation',\n",
       "    'url': 'https://paperswithcode.com/task/question-generation'},\n",
       "   {'task': 'Passage Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/passage-retrieval'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['NQ (BEIR)',\n",
       "   'NQ',\n",
       "   'Natural Questions (short)',\n",
       "   'Natural Questions (long)',\n",
       "   'Natural Questions'],\n",
       "  'num_papers': 397,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/natural_questions',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#natural-questions',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/natural_questions',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mutag',\n",
       "  'name': 'MUTAG',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': 'In particular, **MUTAG** is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.\\r\\n\\r\\nSource: [Fast and Deep Graph Neural Networks](https://arxiv.org/abs/1911.08941)',\n",
       "  'paper': {'title': 'Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds',\n",
       "   'url': 'http://pubs.acs.org/doi/abs/10.1021/jm00106a046'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MUTAG'],\n",
       "  'num_papers': 154,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.MUTAGDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nci1',\n",
       "  'name': 'NCI1',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': 'The **NCI1** dataset comes from the cheminformatics domain, where each input graph is used as representation of a chemical compound: each vertex stands for an atom of the molecule, and edges between vertices represent bonds between atoms. This dataset is relative to anti-cancer screens where the chemicals are assessed as positive or negative to cell lung cancer. Each vertex has an input label representing the corresponding atom type, encoded by a one-hot-encoding scheme into a vector of 0/1 elements.\\r\\n\\r\\nSource: [Ring Reservoir Neural Networks for Graphs](https://arxiv.org/abs/2005.05294)',\n",
       "  'paper': {'title': 'Comparison of descriptor spaces for chemical compound retrieval and classification',\n",
       "   'url': 'https://doi.org/10.1109/ICDM.2006.39'},\n",
       "  'introduced_date': '2006-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NCI1', 'NCI-123'],\n",
       "  'num_papers': 145,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/proteins',\n",
       "  'name': 'PROTEINS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**PROTEINS** is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart.\\r\\n\\r\\nSource: [Fast and Deep Graph Neural Networks](https://arxiv.org/abs/1911.08941)',\n",
       "  'paper': {'title': 'Protein function prediction via graph kernels',\n",
       "   'url': 'https://doi.org/10.1093/bioinformatics/bti1007'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PROTEINS'],\n",
       "  'num_papers': 203,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/enzymes',\n",
       "  'name': 'ENZYMES',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/snap-stanford/GraphRNN/tree/master/dataset/ENZYMES',\n",
       "  'description': '**ENZYMES** is a dataset of 600 protein tertiary structures obtained from the BRENDA enzyme database. The ENZYMES dataset contains 6 enzymes.\\r\\n\\r\\nSource: [When Work Matters: Transforming Classical Network Structures to Graph CNN](https://arxiv.org/abs/1807.02653)',\n",
       "  'paper': {'title': 'Protein function prediction via graph kernels',\n",
       "   'url': 'https://doi.org/10.1093/bioinformatics/bti1007'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ENZYMES'],\n",
       "  'num_papers': 113,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']},\n",
       "   {'url': 'https://github.com/snap-stanford/GraphRNN',\n",
       "    'repo': 'https://github.com/snap-stanford/GraphRNN',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/collab',\n",
       "  'name': 'COLLAB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**COLLAB** is a scientific collaboration dataset. A graph corresponds to a researcher’s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher’s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2.\\r\\n\\r\\nSource: [1 Introduction](https://arxiv.org/abs/2006.11165)',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COLLAB'],\n",
       "  'num_papers': 149,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bc5cdr',\n",
       "  'name': 'BC5CDR',\n",
       "  'full_name': 'BioCreative V CDR corpus',\n",
       "  'homepage': 'https://www.ncbi.nlm.nih.gov/research/bionlp/Data/',\n",
       "  'description': '**BC5CDR** corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions.\\r\\n\\r\\nSource: [https://www.ncbi.nlm.nih.gov/research/bionlp/Data/](https://www.ncbi.nlm.nih.gov/research/bionlp/Data/)\\r\\nImage Source: [https://arxiv.org/pdf/1805.10586.pdf](https://arxiv.org/pdf/1805.10586.pdf)',\n",
       "  'paper': {'title': 'BioCreative V CDR task corpus: a resource for chemical disease relation extraction',\n",
       "   'url': 'https://doi.org/10.1093/database/baw068'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Weakly-Supervised Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-named-entity-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['BC5CDR', 'BC5CDR-disease', 'BC5CDR-chemical'],\n",
       "  'num_papers': 88,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jnlpba',\n",
       "  'name': 'JNLPBA',\n",
       "  'full_name': 'JNLPBA',\n",
       "  'homepage': 'https://metatext.io/datasets/jnlpba',\n",
       "  'description': '**JNLPBA** is a biomedical dataset that comes from the GENIA version 3.02 corpus (Kim et al., 2003). It was created with a controlled search on MEDLINE. From this search 2,000 abstracts were selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification. 36 terminal classes were used to annotate the GENIA corpus.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Token Classification',\n",
       "    'url': 'https://paperswithcode.com/task/token-classification'},\n",
       "   {'task': 'Medical Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/medical-named-entity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JNLPBA'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chemprot',\n",
       "  'name': 'ChemProt',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://biocreative.bioinformatics.udel.edu/news/corpora/chemprot-corpus-biocreative-vi/',\n",
       "  'description': '**ChemProt** consists of 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts and was used in the BioCreative VI text mining chemical-protein interactions shared task.\\r\\n\\r\\nSource: [Peng et al.](https://arxiv.org/pdf/1906.05474v2.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Biomedical'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ChemProt'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scierc',\n",
       "  'name': 'SciERC',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://nlp.cs.washington.edu/sciIE/',\n",
       "  'description': '**SciERC** dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.\\r\\n\\r\\nSource: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)\\r\\nImage Source: [http://nlp.cs.washington.edu/sciIE/](http://nlp.cs.washington.edu/sciIE/)',\n",
       "  'paper': {'title': 'Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-task-identification-of-entities'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Joint Entity and Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/joint-entity-and-relation-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SciERC'],\n",
       "  'num_papers': 59,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/paper-field',\n",
       "  'name': 'Paper Field',\n",
       "  'full_name': 'Paper Field',\n",
       "  'homepage': 'https://docs.microsoft.com/en-us/academic-services/graph/reference-data-schema',\n",
       "  'description': '**Paper Field** is built from the Microsoft Academic Graph and maps paper titles to one of 7 fields of study. Each field of study - geography, politics, economics, business, sociology, medicine, and psychology - has approximately 12K training examples.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Sentence Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Paper Field'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-context',\n",
       "  'name': 'PASCAL Context',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cs.stanford.edu/~roozbeh/pascal-context/',\n",
       "  'description': 'The **PASCAL Context** dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use.\\r\\n\\r\\nSource: [Image Segmentation Using Deep Learning:A Survey](https://arxiv.org/abs/2001.05566)\\r\\nImage Source: [https://cs.stanford.edu/~roozbeh/pascal-context/](https://cs.stanford.edu/~roozbeh/pascal-context/)',\n",
       "  'paper': {'title': 'The Role of Context for Object Detection and Semantic Segmentation in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/the-role-of-context-for-object-detection-and'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL Context'],\n",
       "  'num_papers': 156,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/scut-ctw1500',\n",
       "  'name': 'SCUT-CTW1500',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/Yuliang-Liu/Curve-Text-Detector',\n",
       "  'description': 'The **SCUT-CTW1500** dataset contains 1,500 images: 1,000 for training and 500 for testing. In particular, it provides 10,751 cropped text instance images, including 3,530 with curved text. The images are manually harvested from the Internet, image libraries such as Google Open-Image, or phone cameras. The dataset contains a lot of horizontal and multi-oriented text.\\r\\n\\r\\nSource: [Text Recognition in the Wild: A Survey](https://arxiv.org/abs/2005.03492)\\r\\nImage Source: [https://github.com/Yuliang-Liu/Curve-Text-Detector](https://github.com/Yuliang-Liu/Curve-Text-Detector)',\n",
       "  'paper': {'title': 'Detecting Curve Text in the Wild: New Dataset and New Solution',\n",
       "   'url': 'https://paperswithcode.com/paper/detecting-curve-text-in-the-wild-new-dataset'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'},\n",
       "   {'task': 'Curved Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/curved-text-detection'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['SCUT-CTW1500'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmocr/blob/main/docs/datasets.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmocr',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/Yuliang-Liu/Curve-Text-Detector',\n",
       "    'repo': 'https://github.com/Yuliang-Liu/Curve-Text-Detector',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ochuman',\n",
       "  'name': 'OCHuman',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/liruilong940607/OCHumanApi',\n",
       "  'description': 'This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, **OCHuman** is the most complex and challenging dataset related to human.\\n\\nSource: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)\\nImage Source: [https://github.com/liruilong940607/OCHumanApi](https://github.com/liruilong940607/OCHumanApi)',\n",
       "  'paper': {'title': 'Pose2Seg: Detection Free Human Instance Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/pose2seg-detection-free-human-instance'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '2D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/2d-human-pose-estimation'},\n",
       "   {'task': 'Keypoint Detection',\n",
       "    'url': 'https://paperswithcode.com/task/keypoint-detection'},\n",
       "   {'task': 'Human Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/human-instance-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OCHuman'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#ochuman',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/liruilong940607/OCHumanApi',\n",
       "    'repo': 'https://github.com/liruilong940607/OCHumanApi',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/yago3-10',\n",
       "  'name': 'YAGO3-10',\n",
       "  'full_name': 'Yet Another Great Ontology 3-10',\n",
       "  'homepage': '',\n",
       "  'description': 'YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations, and most of the triples describe attributes of persons such as citizenship, gender, and profession.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['YAGO3-10'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msu-mfsd',\n",
       "  'name': 'MSU-MFSD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://sites.google.com/site/huhanhomepage/download',\n",
       "  'description': 'The **MSU-MFSD** dataset contains 280 video recordings of genuine and attack faces. 35 individuals have participated in the development of this database with a total of 280 videos. Two kinds of cameras with different resolutions (720×480 and 640×480) were used to record the videos from the 35 individuals. For the real accesses, each individual has two video recordings captured with the Laptop cameras and Android, respectively. For the video attacks, two types of cameras, the iPhone and Canon cameras were used to capture high definition videos on each of the subject. The videos taken with Canon camera were then replayed on iPad Air screen to generate the HD replay attacks while the videos recorded by the iPhone mobile were replayed itself to generate the mobile replay attacks. Photo attacks were produced by printing the 35 subjects’ photos on A3 papers using HP colour printer. The recording videos with respect to the 35 individuals were divided into training (15 subjects with 120 videos) and testing (40 subjects with 160 videos) datasets, respectively.\\r\\n\\r\\nSource: [Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM Architecture](https://arxiv.org/abs/1901.05635)\\r\\n\\r\\nImage Source: [face anti-spoofing based on color texture analysis](https://arxiv.org/abs/1511.06316)',\n",
       "  'paper': {'title': 'Face Spoof Detection With Image Distortion Analysis',\n",
       "   'url': 'https://doi.org/10.1109/TIFS.2015.2400395'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Anti-Spoofing',\n",
       "    'url': 'https://paperswithcode.com/task/face-anti-spoofing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MSU-MFSD'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scicite',\n",
       "  'name': 'SciCite',\n",
       "  'full_name': 'SciCite',\n",
       "  'homepage': 'https://github.com/allenai/scicite',\n",
       "  'description': '**SciCite** is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC.\\n\\nSource: [Structural Scaffolds for Citation Intent Classification in Scientific Publications](https://arxiv.org/abs/1904.01608)\\nImage Source: [https://arxiv.org/pdf/1904.01608v2.pdf](https://arxiv.org/pdf/1904.01608v2.pdf)',\n",
       "  'paper': {'title': 'Structural Scaffolds for Citation Intent Classification in Scientific Publications',\n",
       "   'url': 'https://paperswithcode.com/paper/structural-scaffolds-for-citation-intent'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Sentence Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-classification'},\n",
       "   {'task': 'Citation Intent Classification',\n",
       "    'url': 'https://paperswithcode.com/task/citation-intent-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ScienceCite', 'SciCite'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/scicite',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/scicite',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/allenai/scicite',\n",
       "    'repo': 'https://github.com/allenai/scicite',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/panocontext',\n",
       "  'name': 'PanoContext',\n",
       "  'full_name': 'PanoContext',\n",
       "  'homepage': 'https://panocontext.cs.princeton.edu/',\n",
       "  'description': 'The **PanoContext** dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms.\\r\\n\\r\\nSource: [LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image](https://arxiv.org/abs/1803.08999)\\r\\nImage Source: [https://panocontext.cs.princeton.edu/paper.pdf](https://panocontext.cs.princeton.edu/paper.pdf)',\n",
       "  'paper': {'title': 'PanoContext: A Whole-Room 3D Context Model for Panoramic Scene Understanding',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10599-4_43'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Room Layouts From A Single RGB Panorama',\n",
       "    'url': 'https://paperswithcode.com/task/3d-room-layouts-from-a-single-rgb-panorama'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PanoContext'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/office-31',\n",
       "  'name': 'Office-31',\n",
       "  'full_name': 'Office Dataset',\n",
       "  'homepage': 'https://www.cc.gatech.edu/~judy/domainadapt/',\n",
       "  'description': 'The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288×2848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640×480) exhibit significant noise and color as well as white balance artifacts.\\r\\n\\r\\nSource: [Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors](https://arxiv.org/abs/1611.08195)\\r\\nImage Source: [https://www.researchgate.net/publication/310953258](https://www.researchgate.net/publication/310953258)',\n",
       "  'paper': {'title': 'Adapting Visual Category Models to New Domains',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-15561-1_16'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': '', 'url': 'https://paperswithcode.com/task/task'},\n",
       "   {'task': 'Brain Tumor Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/brain-tumor-segmentation'},\n",
       "   {'task': 'Multi-Source Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-source-unsupervised-domain-adaptation'},\n",
       "   {'task': 'Partial Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/partial-domain-adaptation'},\n",
       "   {'task': 'COVID-19 Diagnosis',\n",
       "    'url': 'https://paperswithcode.com/task/covid-19-detection'},\n",
       "   {'task': 'Universal Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/universal-domain-adaptation'},\n",
       "   {'task': 'Multi-target Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-target-domain-adaptation'},\n",
       "   {'task': 'Radiologist Binary Classification',\n",
       "    'url': 'https://paperswithcode.com/task/radiologist-binary-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Office-31', ''],\n",
       "  'num_papers': 380,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imageclef-da',\n",
       "  'name': 'ImageCLEF-DA',\n",
       "  'full_name': 'ImageCLEF-DA',\n",
       "  'homepage': 'https://www.imageclef.org/2014/adaptation',\n",
       "  'description': 'The **ImageCLEF-DA** dataset is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which contains three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P). For each domain, there are 12 categories and 50 images in each category.\\r\\n\\r\\nSource: [Domain-Symmetric Networks for Adversarial Domain Adaptation](https://arxiv.org/abs/1904.04663)\\r\\nImage Source: [https://www.imageclef.org/2014/adaptation](https://www.imageclef.org/2014/adaptation)',\n",
       "  'paper': {'title': 'Deep Transfer Learning with Joint Adaptation Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-transfer-learning-with-joint-adaptation'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Transfer Learning',\n",
       "    'url': 'https://paperswithcode.com/task/transfer-learning'},\n",
       "   {'task': 'Scene Graph Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-detection'},\n",
       "   {'task': 'Predicate Classification',\n",
       "    'url': 'https://paperswithcode.com/task/predicate-classification'},\n",
       "   {'task': 'Scene Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageCLEF-DA'],\n",
       "  'num_papers': 79,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/office-home',\n",
       "  'name': 'Office-Home',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.hemanthdv.org/officeHomeDataset.html',\n",
       "  'description': '**Office-Home** is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art – artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart – collection of clipart images; Product – images of objects without a background and Real-World – images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class.\\r\\n\\r\\nSource: [Multi-component Image Translation for Deep Domain Generalization](https://arxiv.org/abs/1812.08974)\\r\\n\\r\\nImage Source: [Wen et al](https://www.researchgate.net/publication/329023199_Exploiting_Local_Feature_Patterns_for_Unsupervised_Domain_Adaptation)',\n",
       "  'paper': {'title': 'Deep Hashing Network for Unsupervised Domain Adaptation',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-hashing-network-for-unsupervised-domain'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Transfer Learning',\n",
       "    'url': 'https://paperswithcode.com/task/transfer-learning'},\n",
       "   {'task': 'Multi-Source Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-source-unsupervised-domain-adaptation'},\n",
       "   {'task': 'Partial Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/partial-domain-adaptation'},\n",
       "   {'task': 'Universal Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/universal-domain-adaptation'},\n",
       "   {'task': 'Multi-target Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-target-domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Office-Home', 'Office-Home (RS-UT imbalance)'],\n",
       "  'num_papers': 432,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/office-home-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hpatches',\n",
       "  'name': 'HPatches',\n",
       "  'full_name': 'Homography-patches dataset',\n",
       "  'homepage': 'https://github.com/hpatches/hpatches-dataset',\n",
       "  'description': 'The **HPatches** is a recent dataset for local patch descriptor evaluation that consists of 116 sequences of 6 images with known homography. The dataset is split into two parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences with significant illumination change, both natural and artificial.\\r\\n\\r\\nSource: [RF-Net: An End-to-End Image Matching Network based on Receptive Field](https://arxiv.org/abs/1906.00604)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf](https://www.robots.ox.ac.uk/~vgg/publications/2017/Balntas17/balntas17.pdf)',\n",
       "  'paper': {'title': 'HPatches: A benchmark and evaluation of handcrafted and learned local descriptors',\n",
       "   'url': 'https://paperswithcode.com/paper/hpatches-a-benchmark-and-evaluation-of'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Dense Pixel Correspondence Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/dense-pixel-correspondence-estimation'},\n",
       "   {'task': 'Patch Matching',\n",
       "    'url': 'https://paperswithcode.com/task/patch-matching'},\n",
       "   {'task': 'Image Stitching',\n",
       "    'url': 'https://paperswithcode.com/task/image-stitching'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HPatches'],\n",
       "  'num_papers': 122,\n",
       "  'data_loaders': [{'url': 'https://github.com/hpatches/hpatches-dataset',\n",
       "    'repo': 'https://github.com/hpatches/hpatches-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/citypersons',\n",
       "  'name': 'CityPersons',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons',\n",
       "  'description': 'The **CityPersons** dataset is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average of the number of pedestrians in an image is 7. The visible-region and full-body annotations are provided.\\r\\n\\r\\nSource: [NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing](https://arxiv.org/abs/2003.12729)\\r\\nImage Source: [https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons](https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons)',\n",
       "  'paper': {'title': 'CityPersons: A Diverse Dataset for Pedestrian Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/citypersons-a-diverse-dataset-for-pedestrian'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pedestrian Detection',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CityPersons'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': [{'url': 'https://github.com/CharlesShang/Detectron-PYTORCH',\n",
       "    'repo': 'https://github.com/CharlesShang/Detectron-PYTORCH',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cremi',\n",
       "  'name': 'CREMI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cremi.org/',\n",
       "  'description': 'MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images.\\r\\n# About\\r\\nThe goal of this challenge is to evaluate algorithms for automatic reconstruction of neurons and neuronal connectivity from serial section electron microscopy data. The comparison is performed not only by evaluating the quality of neuron segmentations, but also by assessing the accuracy of detecting synapses and identifying synaptic partners. The challenge is carried out on three large and diverse datasets from adult Drosophila melanogaster brain tissue, comprising neuron segmentation ground truth and annotations for synaptic connections. A successful solution would demonstrate its efficiency and generalizability, and carry great potential to reduce the time spent on manual reconstruction of neural circuits in electron microscopy volumes.\\r\\n\\r\\n# Description\\r\\nWe provide three datasets, each consisting of two (5\\u2009μm)3 volumes (training and testing, each 1250\\u2009px\\u2009×\\u20091250\\u2009px\\u2009×\\u2009125\\u2009px) of serial section EM of the adult fly brain. Each volume has neuron and synapse labelings and annotations for pre- and post-synaptic partners.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Biomedical'],\n",
       "  'tasks': [{'task': 'Brain Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/brain-image-segmentation'},\n",
       "   {'task': '3D Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-instance-segmentation-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CREMI'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/cremi/cremi_python',\n",
       "    'repo': 'https://github.com/cremi/cremi_python',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/contactdb',\n",
       "  'name': 'ContactDB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/samarth-robo/contactdb_utils',\n",
       "  'description': '**ContactDB** is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images.\\r\\n\\r\\nSource: [https://arxiv.org/abs/1904.06830](https://arxiv.org/abs/1904.06830)\\r\\nImage Source: [https://github.com/samarth-robo/contactdb_utils](https://github.com/samarth-robo/contactdb_utils)',\n",
       "  'paper': {'title': 'ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging',\n",
       "   'url': 'https://paperswithcode.com/paper/contactdb-analyzing-and-predicting-grasp'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Grasp Contact Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/human-grasp-contact-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ContactDB'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': [{'url': 'https://github.com/samarth-robo/contactdb_utils',\n",
       "    'repo': 'https://github.com/samarth-robo/contactdb_utils',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/polyvore',\n",
       "  'name': 'Polyvore',\n",
       "  'full_name': 'Polyvore Outfits',\n",
       "  'homepage': 'https://github.com/xthan/polyvore-dataset',\n",
       "  'description': 'This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing.\\r\\n\\r\\nSource: [GitHub](https://github.com/xthan/polyvore-dataset)\\r\\nImage Source: [https://arxiv.org/pdf/1707.05691.pdf](https://arxiv.org/pdf/1707.05691.pdf)',\n",
       "  'paper': {'title': 'Learning Fashion Compatibility with Bidirectional LSTMs',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-fashion-compatibility-with'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Slot Filling',\n",
       "    'url': 'https://paperswithcode.com/task/slot-filling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Polyvore'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': [{'url': 'https://github.com/xthan/polyvore-dataset',\n",
       "    'repo': 'https://github.com/xthan/polyvore-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/comic2k',\n",
       "  'name': 'Comic2k',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://naoto0804.github.io/cross_domain_detection/',\n",
       "  'description': '**Comic2k** is a dataset used for cross-domain object detection which contains 2k comic images with image and instance-level annotations.\\nImage Source: [https://naoto0804.github.io/cross_domain_detection/](https://naoto0804.github.io/cross_domain_detection/)',\n",
       "  'paper': {'title': 'Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation',\n",
       "   'url': 'https://paperswithcode.com/paper/cross-domain-weakly-supervised-object'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Class-agnostic Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/class-agnostic-object-detection'},\n",
       "   {'task': 'Object Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/object-proposal-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Comic2k'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/peopleart',\n",
       "  'name': 'PeopleArt',\n",
       "  'full_name': 'PeopleArt',\n",
       "  'homepage': 'https://github.com/BathVisArtData/PeopleArt',\n",
       "  'description': 'People-Art is an object detection dataset which consists of people in 43 different styles. People contained in this dataset are quite different from those in common photographs. There are 42 categories of art styles and movements including Naturalism, Cubism, Socialist Realism, Impressionism, and Suprematism\\n\\nSource: [Point Linking Network for Object Detection](https://arxiv.org/abs/1706.03646)\\nImage Source: [https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597](https://www.researchgate.net/figure/Generalization-results-on-Picasso-and-People-Art-datasets-Joseph-Redmon-2016_fig12_328175597)',\n",
       "  'paper': {'title': 'Detecting People in Artwork with CNNs',\n",
       "   'url': 'https://paperswithcode.com/paper/detecting-people-in-artwork-with-cnns'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PeopleArt'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://github.com/BathVisArtData/PeopleArt',\n",
       "    'repo': 'https://github.com/BathVisArtData/PeopleArt',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/iconart',\n",
       "  'name': 'IconArt',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://zenodo.org/record/4737435',\n",
       "  'description': 'This dataset contains 5955 painting images (from WikiCommons) : a train set of 2978 images and a test set of 2977 images (for classification task). 1480 of the 2977 images are annotated with bounding boxes for 7 iconographic classes : ‘angel’,‘Child_Jesus’,‘crucifixion_of_Jesus’,‘Mary’,‘nudity’, ‘ruins’,‘Saint_Sebastien’.\\r\\n\\r\\nThe dataset IconArt dataset was introduced in the following paper : \"Weakly Supervised Object Detection in Artworks\" Gonthier et al. ECCV 2018 Workshop Computer Vision for Art Analysis - VISART 2018.\\r\\n\\r\\nhttps://wsoda.telecom-paristech.fr/ \\r\\nhttps://zenodo.org/record/4737435',\n",
       "  'paper': {'title': 'Weakly Supervised Object Detection in Artworks',\n",
       "   'url': 'https://paperswithcode.com/paper/weakly-supervised-object-detection-in'},\n",
       "  'introduced_date': '2018-10-05',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IconArt'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cofw',\n",
       "  'name': 'COFW',\n",
       "  'full_name': 'Caltech Occluded Faces in the Wild',\n",
       "  'homepage': 'http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset',\n",
       "  'description': 'The **Caltech Occluded Faces in the Wild** (**COFW**) dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones,\\u2028etc.). All images were hand annotated using the same 29 landmarks as in LFPW. Both the landmark positions as well as their occluded/unoccluded state were annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over 23.\\r\\n\\r\\nSource: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)\\r\\nImage Source: [http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset](http://www.vision.caltech.edu/xpburgos/ICCV13/#dataset)',\n",
       "  'paper': {'title': 'Robust Face Landmark Estimation under Occlusion',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2013.191'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COFW'],\n",
       "  'num_papers': 87,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_face_keypoint.md#cofw-dataset',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/rwth-phoenix-weather-2014',\n",
       "  'name': 'RWTH-PHOENIX-Weather 2014',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/',\n",
       "  'description': 'The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Sign Language Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/sign-language-recognition'}],\n",
       "  'languages': ['German'],\n",
       "  'variants': ['RWTH-PHOENIX-Weather 2014'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vrd',\n",
       "  'name': 'VRD',\n",
       "  'full_name': 'Visual Relationship Detection dataset',\n",
       "  'homepage': 'https://cs.stanford.edu/people/ranjaykrishna/vrd/',\n",
       "  'description': 'The Visual Relationship Dataset (**VRD**) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario.\\r\\n\\r\\nSource: [Compensating Supervision Incompleteness with Prior Knowledge in Semantic Image Interpretation](https://arxiv.org/abs/1910.00462)\\r\\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/vrd/](https://cs.stanford.edu/people/ranjaykrishna/vrd/)',\n",
       "  'paper': {'title': 'Visual Relationship Detection with Language Priors',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-relationship-detection-with-language'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Relationship Detection',\n",
       "    'url': 'https://paperswithcode.com/task/visual-relationship-detection'},\n",
       "   {'task': 'Scene Graph Generation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-generation'},\n",
       "   {'task': 'Scene Graph Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-graph-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VRD',\n",
       "   'VRD Predicate Detection',\n",
       "   'VRD Phrase Detection',\n",
       "   'VRD Relationship Detection'],\n",
       "  'num_papers': 120,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ppi',\n",
       "  'name': 'PPI',\n",
       "  'full_name': 'Protein-Protein Interactions (PPI)',\n",
       "  'homepage': 'http://snap.stanford.edu/graphsage/#datasets',\n",
       "  'description': 'protein roles—in terms of their cellular functions from\\r\\ngene ontology—in various protein-protein interaction (PPI) graphs, with each graph corresponding\\r\\nto a different human tissue [41]. positional gene sets are used, motif gene sets and immunological\\r\\nsignatures as features and gene ontology sets as labels (121 in total), collected from the Molecular\\r\\nSignatures Database [34]. The average graph contains 2373 nodes, with an average degree of 28.8.',\n",
       "  'paper': {'title': 'Inductive Representation Learning on Large Graphs',\n",
       "   'url': 'https://paperswithcode.com/paper/inductive-representation-learning-on-large'},\n",
       "  'introduced_date': '2017-06-07',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PPI'],\n",
       "  'num_papers': 208,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.PPIDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#ppi',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kuzushiji-mnist',\n",
       "  'name': 'Kuzushiji-MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/rois-codh/kmnist',\n",
       "  'description': 'Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images). Since MNIST restricts us to 10 classes, the authors chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST. Kuzushiji is a Japanese cursive writing style.\\r\\n\\r\\nSource: [Deep Learning for Classical Japanese Literature](/paper/deep-learning-for-classical-japanese)\\r\\nImage Source: [https://github.com/rois-codh/kmnist](https://github.com/rois-codh/kmnist)',\n",
       "  'paper': {'title': 'Deep Learning for Classical Japanese Literature',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-learning-for-classical-japanese'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': ['Japanese'],\n",
       "  'variants': ['Kuzushiji-MNIST'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/kmnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/rois-codh/kmnist',\n",
       "    'repo': 'https://github.com/rois-codh/kmnist',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/slashdot',\n",
       "  'name': 'Slashdot',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://snap.stanford.edu/data/soc-sign-Slashdot090221.html',\n",
       "  'description': 'The **Slashdot** dataset is a relational dataset obtained from Slashdot. Slashdot is a technology-related news website know for its specific user community. The website features user-submitted and editor-evaluated current primarily technology oriented news. In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. The network cotains friend/foe links between the users of Slashdot. The network was obtained in February 2009.\\r\\n\\r\\nSource: [http://snap.stanford.edu/data/soc-sign-Slashdot090221.html](http://snap.stanford.edu/data/soc-sign-Slashdot090221.html)',\n",
       "  'paper': {'title': 'Signed Networks in Social Media',\n",
       "   'url': 'https://dl.acm.org/doi/10.1145/1753326.1753532'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Multi-Label Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-text-classification'},\n",
       "   {'task': 'Link Sign Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-sign-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Slashdot'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/epinions',\n",
       "  'name': 'Epinions',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://snap.stanford.edu/data/soc-Epinions1.html',\n",
       "  'description': \"The **Epinions** dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user.\\nIt contains 75,879 nodes and 50,8837 edges.\\n\\nSource: [https://snap.stanford.edu/data/soc-Epinions1.html](https://snap.stanford.edu/data/soc-Epinions1.html)\",\n",
       "  'paper': {'title': 'Trust Management for the Semantic Web',\n",
       "   'url': 'https://doi.org/10.1007/978-3-540-39718-2_23'},\n",
       "  'introduced_date': '2003-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Link Sign Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-sign-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Epinions'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vot2016',\n",
       "  'name': 'VOT2016',\n",
       "  'full_name': 'VOT2016',\n",
       "  'homepage': 'https://www.votchallenge.net/vot2016/dataset.html',\n",
       "  'description': '**VOT2016** is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects.\\r\\n\\r\\nSource: [Video Saliency Detection by 3D Convolutional Neural Networks](https://arxiv.org/abs/1807.04514)\\r\\nImage Source: [https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png](https://www.researchgate.net/profile/Mohamed_Abdelpakey/publication/327850473/figure/fig3/AS:674547829338114@1537836143562/Visual-results-on-VOT2016-data-set-for-four-sequences.png)',\n",
       "  'paper': {'title': 'The Visual Object Tracking VOT2016 Challenge Results',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-48881-3_54'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VOT-2016', 'VOT2016'],\n",
       "  'num_papers': 102,\n",
       "  'data_loaders': [{'url': 'https://github.com/baoxinchen/siammask_e',\n",
       "    'repo': 'https://github.com/baoxinchen/siammask_e',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/culane',\n",
       "  'name': 'CULane',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://xingangpan.github.io/projects/CULane.html',\n",
       "  'description': '**CULane** is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. The dataset is divided into 88880 images for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories.\\r\\n\\r\\nSource: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)\\r\\nImage Source: [https://xingangpan.github.io/projects/CULane.html](https://xingangpan.github.io/projects/CULane.html)',\n",
       "  'paper': {'title': 'Spatial As Deep: Spatial CNN for Traffic Scene Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/spatial-as-deep-spatial-cnn-for-traffic-scene'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Lane Detection',\n",
       "    'url': 'https://paperswithcode.com/task/lane-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CULane'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/udacity',\n",
       "  'name': 'Udacity',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/udacity/self-driving-car/tree/master/datasets',\n",
       "  'description': 'The **Udacity** dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic.\\n\\nSource: [Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks](https://arxiv.org/abs/1811.02759)\\nImage Source: [https://www.researchgate.net/figure/Sample-from-the-Udacity-dataset-with-the-original-ground-truth-bounding-boxes-Note-that_fig3_345652980](https://www.researchgate.net/figure/Sample-from-the-Udacity-dataset-with-the-original-ground-truth-bounding-boxes-Note-that_fig3_345652980)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Steering Control',\n",
       "    'url': 'https://paperswithcode.com/task/steering-control'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Udacity'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/udacity/self-driving-car',\n",
       "    'repo': 'https://github.com/udacity/self-driving-car',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun360',\n",
       "  'name': 'SUN360',\n",
       "  'full_name': 'Scene UNderstanding 360° panorama',\n",
       "  'homepage': 'http://3dvision.princeton.edu/projects/2012/SUN360/',\n",
       "  'description': 'The goal of the **SUN360** panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene.\\r\\n\\r\\nSource: [Scene UNderstanding 360° panorama](https://vision.cs.princeton.edu/projects/2012/SUN360/data/)\\r\\nImage Source: [http://3dvision.princeton.edu/projects/2012/SUN360/](http://3dvision.princeton.edu/projects/2012/SUN360/)',\n",
       "  'paper': {'title': 'Recognizing scene viewpoint using panoramic place representation',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6247991'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Outdoor Light Source Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/light-source-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN360'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/acl-title-and-abstract-dataset',\n",
       "  'name': 'ACL Title and Abstract Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/EagleW/ACL_titles_abstracts_dataset',\n",
       "  'description': 'This dataset gathers 10,874 title and abstract pairs from the ACL Anthology Network (until 2016).\\r\\n\\r\\nThe structure of the data is as follows:\\r\\n-\\ttitle\\r\\n-\\tabstract\\r\\n-\\t\\\\newline\\r\\n\\r\\nThis dataset is used in our published paper:\\r\\nPaper Abstract Writing through Editing Mechanism\\r\\n\\r\\n## Citation\\r\\n```\\r\\n@inproceedings{wang-etal-2018-paper,\\r\\n    title = \"Paper Abstract Writing through Editing Mechanism\",\\r\\n    author = \"Wang, Qingyun  and\\r\\n      Zhou, Zhihao  and\\r\\n      Huang, Lifu  and\\r\\n      Whitehead, Spencer  and\\r\\n      Zhang, Boliang  and\\r\\n      Ji, Heng  and\\r\\n      Knight, Kevin\",\\r\\n    booktitle = \"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\\r\\n    month = jul,\\r\\n    year = \"2018\",\\r\\n    address = \"Melbourne, Australia\",\\r\\n    publisher = \"Association for Computational Linguistics\",\\r\\n    url = \"https://www.aclweb.org/anthology/P18-2042\",\\r\\n    doi = \"10.18653/v1/P18-2042\",\\r\\n    pages = \"260--265\",\\r\\n    abstract = \"We present a paper abstract writing system based on an attentive neural sequence-to-sequence model that can take a title as input and automatically generate an abstract. We design a novel Writing-editing Network that can attend to both the title and the previously generated abstract drafts and then iteratively revise and polish the abstract. With two series of Turing tests, where the human judges are asked to distinguish the system-generated abstracts from human-written ones, our system passes Turing tests by junior domain experts at a rate up to 30{\\\\%} and by non-expert at a rate up to 80{\\\\%}.\",\\r\\n}\\r\\n```',\n",
       "  'paper': {'title': 'Paper Abstract Writing through Editing Mechanism',\n",
       "   'url': 'https://paperswithcode.com/paper/paper-abstract-writing-through-editing'},\n",
       "  'introduced_date': '2018-05-15',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Paper generation',\n",
       "    'url': 'https://paperswithcode.com/task/paper-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ACL Title and Abstract Dataset'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/EagleW/ACL_titles_abstracts_dataset',\n",
       "    'repo': 'https://github.com/EagleW/ACL_titles_abstracts_dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikipedia-person-and-animal-dataset',\n",
       "  'name': 'Wikipedia Person and Animal Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://eaglew.github.io/dataset/narrating',\n",
       "  'description': 'This dataset gathers 428,748 person and 12,236 animal infobox with descriptions based on Wikipedia dump (2018/04/01) and Wikidata (2018/04/12).',\n",
       "  'paper': {'title': 'Describing a Knowledge Base',\n",
       "   'url': 'https://paperswithcode.com/paper/describing-a-knowledge-base'},\n",
       "  'introduced_date': '2018-09-06',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'},\n",
       "   {'task': 'Table-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/table-to-text-generation'},\n",
       "   {'task': 'KB-to-Language Generation',\n",
       "    'url': 'https://paperswithcode.com/task/kb-to-language-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Wikipedia Person and Animal Dataset'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vizwiz',\n",
       "  'name': 'VizWiz',\n",
       "  'full_name': 'VizWiz-VQA',\n",
       "  'homepage': 'https://vizwiz.org/tasks-and-datasets/vqa/',\n",
       "  'description': 'The **VizWiz**-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered.\\r\\n\\r\\nSource: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)\\r\\nImage Source: [https://vizwiz.org/tasks-and-datasets/vqa/](https://vizwiz.org/tasks-and-datasets/vqa/)',\n",
       "  'paper': {'title': 'VizWiz Grand Challenge: Answering Visual Questions from Blind People',\n",
       "   'url': 'https://paperswithcode.com/paper/vizwiz-grand-challenge-answering-visual'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/image-captioning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VizWiz 2020 test-dev',\n",
       "   'VizWiz 2020 test',\n",
       "   'VizWiz 2020 VQA',\n",
       "   'VizWiz 2020 Answerability',\n",
       "   'VizWiz 2018 Answerability',\n",
       "   'VizWiz 2018',\n",
       "   'VizWiz'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kt3dmoseg',\n",
       "  'name': 'KT3DMoSeg',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://alex-xun-xu.github.io/ProjectPage/CVPR_18/index.html',\n",
       "  'description': 'Please find more details of this dataset at https://alex-xun-xu.github.io/ProjectPage/CVPR_18/index.html\\r\\n\\r\\n3D motion segmentation has been the key problem in computer vision research due to the application in structure from motion and robotics. Traditional motion segmentation approaches are often evaluated on artificial dataset like Hopkins 155 [1] and its variants. Because the vanishing camera translation effect is often overlooked, these approaches would fail in real world scenes where camera is carrying out significant translation and scene has complex structure. We proposed the KT3DMoSeg to address the 3D motion segmentation problem in real world scenes. The KT3DMoSeg dataset was created upon the KITTI benchmark [2] by manually selecting 22 sequences and labelling each individual foreground object. We select sequence with more significant camera translation so camera mounted on moving cars are preferred. We are interested in the interplay of multiple motions, so clips with more than 3 motions are also chosen, as long as these moving objects contain enough features for forming motion hypotheses. 22 short clips, each with 10-20 frames, are chosen for evaluation. We extract dense trajectories from each sequence using [3] and prune out trajectories shorter than 5 frames.\\r\\n\\r\\nReference\\r\\n[1] R. Tron and R. Vidal. A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms. CVPR, 2007.\\r\\n[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research, 2013.\\r\\n[3] N. Sundaram, T. Brox, and K. Keutzer. Dense point trajectories by GPU-accelerated large displacement optical flow. In ECCV, 2010.',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2018-06-15',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Motion Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/motion-segmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['KT3DMoSeg'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hopkins155',\n",
       "  'name': 'Hopkins155',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.vision.jhu.edu/data/hopkins155/',\n",
       "  'description': 'The Hopkins 155 dataset consists of 156 video sequences of two or three motions. Each video sequence motion corresponds to a low-dimensional subspace. There are 39−550 data vectors drawn from two or three motions for each video sequence.\\r\\n\\r\\nSource: [Symmetric low-rank representation for subspace clustering](https://arxiv.org/abs/1410.8618)\\r\\nImage Source: [http://www.vision.jhu.edu/data/hopkins155/](http://www.vision.jhu.edu/data/hopkins155/)',\n",
       "  'paper': {'title': 'A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2007.382974'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Motion Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/motion-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Hopkins155'],\n",
       "  'num_papers': 87,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/s3dis',\n",
       "  'name': 'S3DIS',\n",
       "  'full_name': 'Stanford 3D Indoor Scene Dataset (S3DIS)',\n",
       "  'homepage': 'http://buildingparser.stanford.edu/dataset.html',\n",
       "  'description': 'The Stanford 3D Indoor Scene Dataset (**S3DIS**) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories.\\r\\n\\r\\nSource: [Grid-GCN for Fast and Scalable Point Cloud Learning](https://arxiv.org/abs/1912.02984)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-classified-scenes-in-S3DIS-dataset-left-with-groundtruth-right_fig2_328307943](https://www.researchgate.net/figure/Examples-of-classified-scenes-in-S3DIS-dataset-left-with-groundtruth-right_fig2_328307943)',\n",
       "  'paper': {'title': '3D Semantic Parsing of Large-Scale Indoor Spaces',\n",
       "   'url': 'https://paperswithcode.com/paper/3d-semantic-parsing-of-large-scale-indoor'},\n",
       "  'introduced_date': '2016-06-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': '3D Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-semantic-segmentation'},\n",
       "   {'task': '3D Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-instance-segmentation-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['S3DIS', 'S3DIS Area5'],\n",
       "  'num_papers': 209,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmdetection3d/blob/master/docs/data_preparation.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection3d',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/voxceleb1',\n",
       "  'name': 'VoxCeleb1',\n",
       "  'full_name': 'VoxCeleb1',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html',\n",
       "  'description': '**VoxCeleb1** is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube.',\n",
       "  'paper': {'title': 'VoxCeleb: a large-scale speaker identification dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/voxceleb-a-large-scale-speaker-identification'},\n",
       "  'introduced_date': '2017-06-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Talking Head Generation',\n",
       "    'url': 'https://paperswithcode.com/task/talking-head-generation'},\n",
       "   {'task': 'Video Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/video-reconstruction'},\n",
       "   {'task': 'Speaker Verification',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-verification'},\n",
       "   {'task': 'Speaker Identification',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-identification'},\n",
       "   {'task': 'Speaker Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VoxCeleb',\n",
       "   'VoxCeleb1 - 8-shot learning',\n",
       "   'VoxCeleb1 - 32-shot learning',\n",
       "   'VoxCeleb1 - 1-shot learning',\n",
       "   'VoxCeleb1'],\n",
       "  'num_papers': 334,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/voxceleb',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/moments-in-time',\n",
       "  'name': 'Moments in Time',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://moments.csail.mit.edu/',\n",
       "  'description': 'Moments in Time is a large-scale dataset for recognizing and understanding action in videos. The dataset includes a collection of one million labeled 3 second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene.',\n",
       "  'paper': {'title': 'Moments in Time Dataset: one million videos for event understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/moments-in-time-dataset-one-million-videos'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Multimodal Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Moments in Time', 'Moments in Time Dataset'],\n",
       "  'num_papers': 59,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/mit/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vqa-cp',\n",
       "  'name': 'VQA-CP',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.cc.gatech.edu/~aagrawal307/vqa-cp/',\n",
       "  'description': 'The **VQA-CP** dataset was constructed by reorganizing VQA v2 such that the correlation between the question type and correct answer differs in the training and test splits. For example, the most common answer to questions starting with What sport… is tennis in the training set, but skiing in the test set. A model that guesses an answer primarily from the question will perform poorly.\\n\\nSource: [Unshuffling Data for Improved Generalization](https://arxiv.org/abs/2002.11894)\\nImage Source: [https://arxiv.org/pdf/1712.00377.pdf](https://arxiv.org/pdf/1712.00377.pdf)',\n",
       "  'paper': {'title': \"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering\",\n",
       "   'url': 'https://paperswithcode.com/paper/dont-just-assume-look-and-answer-overcoming'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VQA-CP'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ljspeech',\n",
       "  'name': 'LJSpeech',\n",
       "  'full_name': 'The LJ Speech Dataset',\n",
       "  'homepage': 'https://keithito.com/LJ-Speech-Dataset/',\n",
       "  'description': 'This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.\\r\\n\\r\\nSource: [The LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)\\r\\nImage Source: [https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)\\r\\nAudio Source: [https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)',\n",
       "  'paper': {'title': 'The lj speech dataset',\n",
       "   'url': 'https://keithito.com/LJ-Speech-Dataset/'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Text-To-Speech Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-speech-synthesis'},\n",
       "   {'task': 'Speech Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/speech-synthesis'},\n",
       "   {'task': 'Resynthesis',\n",
       "    'url': 'https://paperswithcode.com/task/resynthesis'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['LJSpeech'],\n",
       "  'num_papers': 110,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/lj_speech',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/ljspeech',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/audio/stable/datasets.html#torchaudio.datasets.LJSPEECH',\n",
       "    'repo': 'https://github.com/pytorch/audio',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/qnli',\n",
       "  'name': 'QNLI',\n",
       "  'full_name': 'Question-answering NLI',\n",
       "  'homepage': 'https://gluebenchmark.com/',\n",
       "  'description': 'The **QNLI** (**Question-answering NLI**) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)',\n",
       "  'paper': {'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/glue-a-multi-task-benchmark-and-analysis'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Few-Shot NLI',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-nli'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['QNLI', 'QNLI (8 training examples per class)'],\n",
       "  'num_papers': 480,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rte',\n",
       "  'name': 'RTE',\n",
       "  'full_name': 'Recognizing Textual Entailment',\n",
       "  'homepage': 'https://aclweb.org/aclwiki/Recognizing_Textual_Entailment',\n",
       "  'description': 'The **Recognizing Textual Entailment (RTE)** datasets come from a series of textual entailment challenges. Data from RTE1, RTE2, RTE3 and RTE5 is combined. Examples are constructed based on news and Wikipedia text.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RTE'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mrpc',\n",
       "  'name': 'MRPC',\n",
       "  'full_name': 'Microsoft Research Paraphrase Corpus',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/download/details.aspx?id=52398',\n",
       "  'description': 'Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases).\\r\\n\\r\\nSource: [Exploiting Semantic Annotations and Q-Learning for Constructing an Efficient Hierarchy/Graph Texts Organization](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313059/)\\r\\nImage Source: [https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf)',\n",
       "  'paper': {'title': 'Automatically Constructing a Corpus of Sentential Paraphrases',\n",
       "   'url': 'https://www.aclweb.org/anthology/I05-5002/'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Paraphrase Identification',\n",
       "    'url': 'https://paperswithcode.com/task/paraphrase-identification'},\n",
       "   {'task': 'Semantic Textual Similarity within Bi-Encoder',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity-within-bi-encoder'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MRPC', 'MRPC Dev'],\n",
       "  'num_papers': 338,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/codah',\n",
       "  'name': 'CODAH',\n",
       "  'full_name': 'COmmonsense Dataset Adversarially-authored by Humans',\n",
       "  'homepage': 'https://github.com/Websail-NU/CODAH',\n",
       "  'description': 'The COmmonsense Dataset Adversarially-authored by Humans (**CODAH**) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. It contains 2801 questions in total, and uses 5-fold cross validation for evaluation.\\r\\n\\r\\nSource: [CODAH Dataset](https://github.com/Websail-NU/CODAH)\\r\\nImage Source: [https://www.aclweb.org/anthology/W19-2008.pdf](https://www.aclweb.org/anthology/W19-2008.pdf)',\n",
       "  'paper': {'title': 'CODAH: An Adversarially-Authored Question Answering Dataset for Common Sense',\n",
       "   'url': 'https://paperswithcode.com/paper/codah-an-adversarially-authored-question'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CODAH'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/codah',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/Websail-NU/CODAH',\n",
       "    'repo': 'https://github.com/Websail-NU/CODAH',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/crowdpose',\n",
       "  'name': 'CrowdPose',\n",
       "  'full_name': 'CrowdPose',\n",
       "  'homepage': 'https://github.com/Jeff-sjtu/CrowdPose',\n",
       "  'description': 'The **CrowdPose** dataset contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. The test set includes 8,000 images. The crowded images containing homes are extracted from MSCOCO, MPII and AI Challenger.\\r\\n\\r\\nSource: [Human Pose Estimation for Real-World Crowded Scenarios](https://arxiv.org/abs/1907.06922)\\r\\nImage Source: [https://github.com/Jeff-sjtu/CrowdPose](https://github.com/Jeff-sjtu/CrowdPose)',\n",
       "  'paper': {'title': 'CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/crowdpose-efficient-crowded-scenes-pose'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-person-pose-estimation'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['CrowdPose'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#crowdpose',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/Jeff-sjtu/CrowdPose',\n",
       "    'repo': 'https://github.com/Jeff-sjtu/CrowdPose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/memexqa',\n",
       "  'name': 'MemexQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://memexqa.cs.cmu.edu/',\n",
       "  'description': 'A large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers.\\r\\n\\r\\nSource: [MemexQA: Visual Memex Question Answering](/paper/memexqa-visual-memex-question-answering)',\n",
       "  'paper': {'title': 'MemexQA: Visual Memex Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/memexqa-visual-memex-question-answering'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Memex Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/memex-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MemexQA'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ecssd',\n",
       "  'name': 'ECSSD',\n",
       "  'full_name': 'Extended Complex Scene Saliency Dataset',\n",
       "  'homepage': 'https://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html',\n",
       "  'description': 'The **Extended Complex Scene Saliency Dataset** (**ECSSD**) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants.\\r\\n\\r\\nSource: [SAD: Saliency-based Defenses Against Adversarial Examples](https://arxiv.org/abs/2003.04820)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ECSSD'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hku-is',\n",
       "  'name': 'HKU-IS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://sites.google.com/site/ligb86/mdfsaliency/',\n",
       "  'description': '**HKU-IS** is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects.\\r\\n\\r\\nSource: [Deep Contrast Learning for Salient Object Detection](https://arxiv.org/abs/1603.01976)\\r\\nImage Source: [https://sites.google.com/site/ligb86/mdfsaliency/](https://sites.google.com/site/ligb86/mdfsaliency/)',\n",
       "  'paper': {'title': 'Visual Saliency Based on Multiscale Deep Features',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-saliency-based-on-multiscale-deep'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HKU-IS'],\n",
       "  'num_papers': 157,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-s',\n",
       "  'name': 'PASCAL-S',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cbs.ic.gatech.edu/salobj/',\n",
       "  'description': '**PASCAL-S** is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes.\\r\\n\\r\\nSource: [Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection](https://arxiv.org/abs/1909.04366)',\n",
       "  'paper': {'title': 'The Secrets of Salient Object Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/the-secrets-of-salient-object-segmentation'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL-S'],\n",
       "  'num_papers': 200,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dut-omron',\n",
       "  'name': 'DUT-OMRON',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://saliencydetection.net/dut-omron/',\n",
       "  'description': 'The **DUT-OMRON** dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background.\\r\\n\\r\\nSource: [Global Context-Aware Progressive Aggregation Network for Salient Object Detection](https://arxiv.org/abs/2003.00651)',\n",
       "  'paper': {'title': 'Saliency Detection via Graph-Based Manifold Ranking',\n",
       "   'url': 'https://paperswithcode.com/paper/saliency-detection-via-graph-based-manifold'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DUT-OMRON'],\n",
       "  'num_papers': 140,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msmt17',\n",
       "  'name': 'MSMT17',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.pkuvmc.com/publications/msmt17.html',\n",
       "  'description': 'MSMT17 is a multi-scene multi-time person re-identification dataset. The dataset consists of 180 hours of videos, captured by 12 outdoor cameras, 3 indoor cameras, and during 12 time slots. The videos cover a long period of time and present complex lighting variations, and it contains a large number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes.\\r\\n\\r\\nThe dataset is not available anymore.',\n",
       "  'paper': {'title': 'Person Transfer GAN to Bridge Domain Gap for Person Re-Identification',\n",
       "   'url': 'https://paperswithcode.com/paper/person-transfer-gan-to-bridge-domain-gap-for'},\n",
       "  'introduced_date': '2017-11-23',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Unsupervised Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-person-re-identification'},\n",
       "   {'task': 'Generalizable Person Re-identification',\n",
       "    'url': 'https://paperswithcode.com/task/generalizable-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MSMT17',\n",
       "   'Market-1501->MSMT17',\n",
       "   'DukeMTMC-reID->MSMT17',\n",
       "   'MSMT17->Market-1501'],\n",
       "  'num_papers': 149,\n",
       "  'data_loaders': [{'url': 'https://github.com/njhpro/hanareid',\n",
       "    'repo': 'https://github.com/njhpro/hanareid',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/usps',\n",
       "  'name': 'USPS',\n",
       "  'full_name': 'USPS',\n",
       "  'homepage': 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps',\n",
       "  'description': '**USPS** is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16×16 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles.\\r\\n\\r\\nSource: [Hallucinating Agnostic Images to Generalize Across Domains](https://arxiv.org/abs/1808.01102)\\r\\nImage Source: [https://ieeexplore.ieee.org/document/291440](https://ieeexplore.ieee.org/document/291440)',\n",
       "  'paper': {'title': 'A Database for Handwritten Text Recognition Research',\n",
       "   'url': 'https://doi.org/10.1109/34.291440'},\n",
       "  'introduced_date': '1994-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['USPS', 'MNIST-to-USPS', 'USPS-to-MNIST'],\n",
       "  'num_papers': 298,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.USPS',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sixray',\n",
       "  'name': 'SIXray',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/MeioJane/SIXray',\n",
       "  'description': 'The **SIXray** dataset is constructed by the Pattern Recognition and Intelligent System Development Laboratory, University of Chinese Academy of Sciences. It contains 1,059,231 X-ray images which are collected from some several subway stations. There are six common categories of prohibited items, namely, gun, knife, wrench, pliers, scissors and hammer. It has three subsets called SIXray10, SIXray100 and SIXray1000, There are image-level annotations provided by human security inspectors for the whole dataset. In addition the images in the test set are annotated with a bounding-box for each prohibited item to evaluate the performance of object localization.\\n\\nSource: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)\\nImage Source: [https://github.com/MeioJane/SIXray](https://github.com/MeioJane/SIXray)',\n",
       "  'paper': {'title': 'SIXray : A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images',\n",
       "   'url': 'https://paperswithcode.com/paper/sixray-a-large-scale-security-inspection-x'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SIXray'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://github.com/MeioJane/SIXray',\n",
       "    'repo': 'https://github.com/MeioJane/SIXray',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/django',\n",
       "  'name': 'Django',\n",
       "  'full_name': 'Django',\n",
       "  'homepage': 'https://github.com/odashi/ase15-django-dataset',\n",
       "  'description': 'The **Django** dataset is a dataset for code generation comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description.\\r\\n\\r\\nSource: [Latent Predictor Networks for Code Generation](https://arxiv.org/abs/1603.06744)\\r\\nImage Source: [https://github.com/microsoft/vscode-docs/issues/2696](https://github.com/microsoft/vscode-docs/issues/2696)',\n",
       "  'paper': {'title': 'Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)',\n",
       "   'url': 'https://doi.org/10.1109/ASE.2015.36'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Code Generation',\n",
       "    'url': 'https://paperswithcode.com/task/code-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Django'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': [{'url': 'https://github.com/odashi/ase15-django-dataset',\n",
       "    'repo': 'https://github.com/odashi/ase15-django-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pacs',\n",
       "  'name': 'PACS',\n",
       "  'full_name': 'Photo-Art-Cartoon-Sketch',\n",
       "  'homepage': 'https://domaingeneralization.github.io/#data',\n",
       "  'description': '**PACS** is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories.\\r\\n\\r\\nSource: [Deep Domain-Adversarial Image Generation for Domain Generalisation](https://arxiv.org/abs/2003.06054)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033](https://www.researchgate.net/figure/Sample-images-from-PACS-dataset-Each-row-represents-a-domain-and-each-column-represents_fig1_334695033)',\n",
       "  'paper': {'title': 'Deeper, Broader and Artier Domain Generalization',\n",
       "   'url': 'https://paperswithcode.com/paper/deeper-broader-and-artier-domain'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Annotated Code Search',\n",
       "    'url': 'https://paperswithcode.com/task/annotated-code-search'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PACS', 'PACS-StaQC-py', 'PACS-SO-DS', 'PACS-CoNaLa'],\n",
       "  'num_papers': 229,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/biogrid',\n",
       "  'name': 'BioGRID',\n",
       "  'full_name': 'Biological General Repository for Interaction Datasets',\n",
       "  'homepage': 'https://thebiogrid.org/',\n",
       "  'description': '**BioGRID** is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species.\\r\\n\\r\\nSource: [https://thebiogrid.org/](https://thebiogrid.org/)',\n",
       "  'paper': {'title': 'BioGRID: a general repository for interaction datasets',\n",
       "   'url': 'https://doi.org/10.1093/nar/gkj109'},\n",
       "  'introduced_date': '2006-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs', 'Biomedical'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Embedding',\n",
       "    'url': 'https://paperswithcode.com/task/graph-embedding'},\n",
       "   {'task': 'Gene Interaction Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/gene-interaction-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BioGRID', 'BioGRID(yeast)', 'BioGRID (human)'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-forest',\n",
       "  'name': 'Freiburg Forest',\n",
       "  'full_name': 'Freiburg Forest',\n",
       "  'homepage': 'http://deepscene.cs.uni-freiburg.de/',\n",
       "  'description': 'The **Freiburg Forest** dataset was collected using a Viona autonomous mobile robot platform equipped with cameras for capturing multi-spectral and multi-modal images. The dataset may be used for evaluation of different perception algorithms for segmentation, detection, classification, etc. All scenes were recorded at 20 Hz with a camera resolution of 1024x768 pixels. The data was collected on three different days to have enough variability in lighting conditions as shadows and sun angles play a crucial role in the quality of acquired images. The robot traversed about 4.7 km each day. The dataset creators provide manually annotated pixel-wise ground truth segmentation masks for 6 classes: Obstacle, Trail, Sky, Grass, Vegetation, and Void.\\r\\n\\r\\nSource: [http://deepscene.cs.uni-freiburg.de/](http://deepscene.cs.uni-freiburg.de/)\\r\\nImage Source: [http://deepscene.cs.uni-freiburg.de/](http://deepscene.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2021-09-15',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Unsupervised Image-To-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-to-image-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Forest', 'Freiburg Forest Dataset'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/snips',\n",
       "  'name': 'SNIPS',\n",
       "  'full_name': 'SNIPS Natural Language Understanding benchmark',\n",
       "  'homepage': 'https://github.com/sonos/nlu-benchmark',\n",
       "  'description': 'The **SNIPS** Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:\\r\\n\\r\\n* SearchCreativeWork (e.g. Find me the I, Robot television show),\\r\\n* GetWeather (e.g. Is it windy in Boston, MA right now?),\\r\\n* BookRestaurant (e.g. I want to book a highly rated restaurant in Paris tomorrow night),\\r\\n* PlayMusic (e.g. Play the last track from Beyoncé off Spotify),\\r\\n* AddToPlaylist (e.g. Add Diamonds to my roadtrip playlist),\\r\\n* RateBook (e.g. Give 6 stars to Of Mice and Men),\\r\\n* SearchScreeningEvent (e.g. Check the showtimes for Wonder Woman in Paris).\\r\\nThe training set contains of 13,084 utterances, the validation set and the test set contain 700 utterances each, with 100 queries per intent.\\r\\n\\r\\nSource: [https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/](https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken/)',\n",
       "  'paper': {'title': 'Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces',\n",
       "   'url': 'https://paperswithcode.com/paper/snips-voice-platform-an-embedded-spoken'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Intent Detection',\n",
       "    'url': 'https://paperswithcode.com/task/intent-detection'},\n",
       "   {'task': 'Open Intent Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/open-intent-discovery'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SNIPS'],\n",
       "  'num_papers': 141,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/snips_built_in_intents',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/sonos/nlu-benchmark',\n",
       "    'repo': 'https://github.com/sonos/nlu-benchmark',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nottingham',\n",
       "  'name': 'Nottingham',\n",
       "  'full_name': 'Nottingham',\n",
       "  'homepage': 'https://ifdo.ca/~seymour/nottingham/nottingham.html',\n",
       "  'description': 'The **Nottingham** Dataset is a collection of 1200 American and British folk songs.\\n\\nSource: [Rethinking Recurrent Latent Variable Model for Music Composition](https://arxiv.org/abs/1810.03226)\\nImage Source: [https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/](https://highnoongmt.wordpress.com/2018/10/02/going-to-use-the-nottingham-music-database/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Music Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/music-modeling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Nottingham'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cluttered-omniglot',\n",
       "  'name': 'Cluttered Omniglot',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/michaelisc/cluttered-omniglot',\n",
       "  'description': 'Dataset for one-shot segmentation.\\r\\n\\r\\nSource: [One-Shot Segmentation in Clutter](/paper/one-shot-segmentation-in-clutter)',\n",
       "  'paper': {'title': 'One-Shot Segmentation in Clutter',\n",
       "   'url': 'https://paperswithcode.com/paper/one-shot-segmentation-in-clutter'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'One-Shot Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cluttered Omniglot'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': [{'url': 'https://github.com/michaelisc/cluttered-omniglot',\n",
       "    'repo': 'https://github.com/michaelisc/cluttered-omniglot',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pku-mmd',\n",
       "  'name': 'PKU-MMD',\n",
       "  'full_name': 'PKU-MMD',\n",
       "  'homepage': 'https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html',\n",
       "  'description': 'The **PKU-MMD** dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view.\\r\\n\\r\\nSource: [Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation](https://arxiv.org/abs/1804.06055)\\r\\nImage Source: [https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html](https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html)',\n",
       "  'paper': {'title': 'PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/pku-mmd-a-large-scale-benchmark-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PKU-MMD'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ntu-rgb-d',\n",
       "  'name': 'NTU RGB+D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://rose1.ntu.edu.sg/datasets/actionrecognition.asp',\n",
       "  'description': '**NTU RGB+D** is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001–S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, −45∘,0∘, and +45∘. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45∘) for testing, and the other two cameras for training.\\r\\n\\r\\nSource: [Action Recognition for Depth Video using Multi-view Dynamic Images](https://arxiv.org/abs/1806.11269)',\n",
       "  'paper': {'title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/ntu-rgbd-a-large-scale-dataset-for-3d-human'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Human Interaction Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/human-interaction-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Human action generation',\n",
       "    'url': 'https://paperswithcode.com/task/human-action-generation'},\n",
       "   {'task': 'Pose Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/pose-prediction'},\n",
       "   {'task': 'Zero Shot Skeletal Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-skeletal-action-recognition'},\n",
       "   {'task': 'Generalized Zero Shot skeletal action recognition',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-skeletal-action'},\n",
       "   {'task': 'One-Shot 3D Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-3d-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NTU RGB+D', 'NTU RGB+D 120', 'Filtered NTU RGB+D'],\n",
       "  'num_papers': 251,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/birdsnap',\n",
       "  'name': 'Birdsnap',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://thomasberg.org/',\n",
       "  'description': '**Birdsnap** is a large bird dataset consisting of 49,829 images from 500 bird species with 47,386 images used for training and 2,443 images used for testing.\\r\\n\\r\\nSource: [Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks](https://arxiv.org/abs/1511.09209)\\r\\nImage Source: [http://thomasberg.org/](http://thomasberg.org/)',\n",
       "  'paper': {'title': 'Birdsnap: Large-scale Fine-grained Visual Categorization of Birds',\n",
       "   'url': 'https://paperswithcode.com/paper/birdsnap-large-scale-fine-grained-visual'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Birdsnap'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': [{'url': 'https://github.com/moskomule/sam.pytorch',\n",
       "    'repo': 'https://github.com/moskomule/sam.pytorch',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cola',\n",
       "  'name': 'CoLA',\n",
       "  'full_name': 'Corpus of Linguistic Acceptability',\n",
       "  'homepage': 'https://nyu-mll.github.io/CoLA/',\n",
       "  'description': 'The **Corpus of Linguistic Acceptability** (**CoLA**) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set.\\r\\n\\r\\nSource: [https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/)\\r\\nImage Source: [https://arxiv.org/pdf/1805.12471.pdf](https://arxiv.org/pdf/1805.12471.pdf)',\n",
       "  'paper': {'title': 'Neural Network Acceptability Judgments',\n",
       "   'url': 'https://paperswithcode.com/paper/neural-network-acceptability-judgments'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Linguistic Acceptability',\n",
       "    'url': 'https://paperswithcode.com/task/linguistic-acceptability'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CoLA', 'CoLA Dev'],\n",
       "  'num_papers': 295,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/astd',\n",
       "  'name': 'ASTD',\n",
       "  'full_name': 'Arabic Sentiment Tweets Dataset',\n",
       "  'homepage': 'https://github.com/mahmoudnabil/ASTD',\n",
       "  'description': 'Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed.\\r\\n\\r\\nSource: [ASTD: Arabic Sentiment Tweets Dataset](/paper/astd-arabic-sentiment-tweets-dataset)',\n",
       "  'paper': {'title': 'ASTD: Arabic Sentiment Tweets Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/astd-arabic-sentiment-tweets-dataset'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': ['Arabic'],\n",
       "  'variants': ['ASTD'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': [{'url': 'https://github.com/mahmoudnabil/ASTD',\n",
       "    'repo': 'https://github.com/mahmoudnabil/ASTD',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lsmdc',\n",
       "  'name': 'LSMDC',\n",
       "  'full_name': 'Large Scale Movie Description Challenge',\n",
       "  'homepage': 'https://sites.google.com/site/describingmovies/',\n",
       "  'description': 'This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets.\\r\\n\\r\\nSource: [Use What You Have: Video Retrieval Using Representations From Collaborative Experts](https://arxiv.org/abs/1907.13487)\\r\\nImage Source: [https://sites.google.com/site/describingmovies/](https://sites.google.com/site/describingmovies/)',\n",
       "  'paper': {'title': 'A Dataset for Movie Description',\n",
       "   'url': 'https://paperswithcode.com/paper/a-dataset-for-movie-description'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LSMDC'],\n",
       "  'num_papers': 55,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msr-vtt',\n",
       "  'name': 'MSR-VTT',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/',\n",
       "  'description': '**MSR-VTT** (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing.\\r\\n\\r\\nSource: [Learning to Discretely Compose Reasoning Module Networksfor Video Captioning](https://arxiv.org/abs/2007.09049)',\n",
       "  'paper': {'title': 'MSR-VTT: A Large Video Description Dataset for Bridging Video and Language',\n",
       "   'url': 'https://paperswithcode.com/paper/msr-vtt-a-large-video-description-dataset-for'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MSR-VTT', 'MSRVTT-QA', 'MSR-VTT-1kA'],\n",
       "  'num_papers': 200,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msvd',\n",
       "  'name': 'MSVD',\n",
       "  'full_name': 'Microsoft Research Video Description Corpus',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F',\n",
       "  'description': 'The **Microsoft Research Video Description Corpus** (**MSVD**) dataset consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data.\\r\\n\\r\\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F](https://www.microsoft.com/en-us/download/details.aspx?id=52422&from=https%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F)\\r\\nImage Source: [https://arxiv.org/pdf/1609.06782.pdf](https://arxiv.org/pdf/1609.06782.pdf)',\n",
       "  'paper': {'title': 'Collecting Highly Parallel Data for Paraphrase Evaluation',\n",
       "   'url': 'https://www.aclweb.org/anthology/P11-1020/'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MSVD', 'MSVD-QA'],\n",
       "  'num_papers': 136,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/didemo',\n",
       "  'name': 'DiDeMo',\n",
       "  'full_name': 'Distinct Describable Moments',\n",
       "  'homepage': 'https://github.com/LisaAnne/TemporalLanguageRelease',\n",
       "  'description': 'The **Distinct Describable Moments** (**DiDeMo**) dataset is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions. The videos are collected from Flickr and each video is trimmed to a maximum of 30 seconds. The videos in the dataset are divided into 5-second segments to reduce the complexity of annotation. The dataset is split into training, validation and test sets containing 8,395, 1,065 and 1,004 videos respectively. The dataset contains a total of 26,892 moments and one moment could be associated with descriptions from multiple annotators. The descriptions in DiDeMo dataset are detailed and contain camera movement, temporal transition indicators, and activities. Moreover, the descriptions in DiDeMo are verified so that each description refers to a single moment.\\r\\n\\r\\nSource: [Weakly Supervised Video Moment Retrieval From Text Queries](https://arxiv.org/abs/1904.03282)\\r\\nImage Source: [https://www.di.ens.fr/~miech/datasetviz/](https://www.di.ens.fr/~miech/datasetviz/)',\n",
       "  'paper': {'title': 'Localizing Moments in Video with Natural Language',\n",
       "   'url': 'https://paperswithcode.com/paper/localizing-moments-in-video-with-natural'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Natural Language Moment Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-moment-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DiDeMo'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': [{'url': 'https://github.com/LisaAnne/TemporalLanguageRelease',\n",
       "    'repo': 'https://github.com/LisaAnne/TemporalLanguageRelease',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mupots-3d',\n",
       "  'name': 'MuPoTS-3D',\n",
       "  'full_name': 'Multiperson Pose Test Set in 3DMulti-person Pose estimation Test Set in 3D',\n",
       "  'homepage': 'http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/',\n",
       "  'description': 'MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model.\\r\\n\\r\\nSource: [DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild](https://arxiv.org/abs/2008.09457)\\r\\nImage Source: [http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/](http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson/)',\n",
       "  'paper': {'title': 'Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB',\n",
       "   'url': 'https://paperswithcode.com/paper/single-shot-multi-person-3d-pose-estimation'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-multi-person-pose-estimation'},\n",
       "   {'task': '3D Multi-Person Pose Estimation (root-relative)',\n",
       "    'url': 'https://paperswithcode.com/task/3d-multi-person-pose-estimation-root-relative'},\n",
       "   {'task': '3D Multi-Person Pose Estimation (absolute)',\n",
       "    'url': 'https://paperswithcode.com/task/3d-multi-person-pose-estimation-absolute'},\n",
       "   {'task': 'Unsupervised 3D Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-3d-multi-person-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MuPoTS-3D'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/helsinki-prosody-corpus',\n",
       "  'name': 'Helsinki Prosody Corpus',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/Helsinki-NLP/prosody',\n",
       "  'description': \"The Helsinki Prosody Corpus is a dataset for predicting prosodic prominence from written text. The prosodic annotations are automatically generated, high quality prosodic for the 'clean' subsets of LibriTTS corpus (Zen et al., 2019), comprising of 262.5 hours of read speech from 1230 speakers. The transcribed sentences were aligned and then prosodically annotated with word-level acoustic prominence labels.\\r\\n\\r\\nSource: [Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations](https://arxiv.org/abs/1908.02262)\",\n",
       "  'paper': {'title': 'Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations',\n",
       "   'url': 'https://paperswithcode.com/paper/predicting-prosodic-prominence-from-text-with'},\n",
       "  'introduced_date': '2019-08-06',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Prosody Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/prosody-prediction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Helsinki Prosody Corpus'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/Helsinki-NLP/prosody',\n",
       "    'repo': 'https://github.com/Helsinki-NLP/prosody',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmca',\n",
       "  'name': 'WMCA',\n",
       "  'full_name': 'Wide Multi Channel Presentation Attack',\n",
       "  'homepage': 'https://www.idiap.ch/dataset/wmca',\n",
       "  'description': 'The Wide Multi Channel Presentation Attack (WMCA) database consists of 1941 short video recordings of both bonafide and presentation attacks from 72 different identities. The data is recorded from several channels including color, depth, infra-red, and thermal.\\r\\n\\r\\nAdditionally, the pulse reading data for bonafide recordings is also provided.\\r\\n\\r\\nPreprocessed images for some of the channels are also provided for part of the data used in the reference publication.\\r\\n\\r\\nThe WMCA database is produced at Idiap within the framework of “IARPA BATL” and “H2020 TESLA” projects and it is intended for investigation of presentation attack detection (PAD) methods for face recognition systems.',\n",
       "  'paper': {'title': 'Biometric Face Presentation Attack Detection with Multi-Channel Convolutional Neural Network',\n",
       "   'url': 'https://paperswithcode.com/paper/biometric-face-presentation-attack-detection-1'},\n",
       "  'introduced_date': '2019-09-19',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Face Presentation Attack Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-presentation-attack-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WMCA'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aquaint',\n",
       "  'name': 'AQUAINT',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2002T31',\n",
       "  'description': \"The **AQUAINT** Corpus consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST).\\n\\nSource: [Linguistic Data Consortium](https://catalog.ldc.upenn.edu/LDC2002T31)\\nImage Source: [https://catalog.ldc.upenn.edu/LDC2002T31](https://catalog.ldc.upenn.edu/LDC2002T31)\",\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Entity Disambiguation',\n",
       "    'url': 'https://paperswithcode.com/task/entity-disambiguation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AQUAINT'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mafl',\n",
       "  'name': 'MAFL',\n",
       "  'full_name': 'Multi-Attribute Facial Landmark',\n",
       "  'homepage': 'http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html',\n",
       "  'description': 'The **MAFL** dataset contains manually annotated facial landmark locations for 19,000 training and 1,000 test images.\\r\\n\\r\\nSource: [Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance](https://arxiv.org/abs/1806.06503)\\r\\nImage Source: [http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html](http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html)',\n",
       "  'paper': {'title': 'Facial Landmark Detection by Deep Multi-task Learning',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10599-4_7'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-facial-landmark-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MAFL'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/species-800',\n",
       "  'name': 'Species-800',\n",
       "  'full_name': 'Species-800',\n",
       "  'homepage': 'https://species.jensenlab.org/',\n",
       "  'description': '**Species-800** is a corpus for species entities, which is based on manually annotated abstracts. It comprises 800 PubMed abstracts that contain identified organism mentions. To increase the corpus taxonomic mention diversity the 800 abstracts were collected by selecting 100 abstracts from the following 8 categories: bacteriology, botany, entomology, medicine, mycology, protistology, virology and zoology. 800 has been annotated with a focus at the species level; however, higher taxa mentions (such as genera, families and orders) have also been considered.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Medical Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/medical-named-entity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Species-800'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/linnaeus',\n",
       "  'name': 'LINNAEUS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://linnaeus.sourceforge.net/',\n",
       "  'description': 'LINNAEUS is a general-purpose dictionary matching software, capable of processing multiple types of document formats in the biomedical domain (MEDLINE, PMC, BMC, OTMI, text, etc.). It can produce multiple types of output (XML, HTML, tab-separated-value file, or save to a database). It also contains methods for acting as a server (including load balancing across several servers), allowing clients to request matching over a network. A package with files for recognizing and identifying species names is available for LINNAEUS, showing 94% recall and 97% precision compared to LINNAEUS-species-corpus.\\r\\n\\r\\nSource: [LINNAEUS](http://linnaeus.sourceforge.net/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LINNAEUS'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/linnaeus',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nlvr',\n",
       "  'name': 'NLVR',\n",
       "  'full_name': 'Natural Language Visual Reasoningnatural language for visual reasoning',\n",
       "  'homepage': 'http://lil.nlp.cornell.edu/nlvr/',\n",
       "  'description': '**NLVR** contains 92,244 pairs of human-written English sentences grounded in synthetic images. Because the images are synthetically generated, this dataset can be used for semantic parsing.\\r\\n\\r\\nSource: [http://lil.nlp.cornell.edu/nlvr/](http://lil.nlp.cornell.edu/nlvr/)\\r\\nImage Source: [http://lil.nlp.cornell.edu/nlvr/](http://lil.nlp.cornell.edu/nlvr/)',\n",
       "  'paper': {'title': 'A Corpus of Natural Language for Visual Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/a-corpus-of-natural-language-for-visual'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/visual-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['NLVR', 'NLVR2 Dev', 'NLVR2 Test'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': [{'url': 'https://parl.ai/docs/tasks.html#nlvr',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/chestx-ray14',\n",
       "  'name': 'ChestX-ray14',\n",
       "  'full_name': 'ChestX-ray14',\n",
       "  'homepage': 'https://nihcc.app.box.com/v/ChestXray-NIHCC',\n",
       "  'description': '**ChestX-ray14** is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.\\r\\n\\r\\nSource: [https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610](https://nihcc.app.box.com/v/ChestXray-NIHCC/file/220660789610)\\r\\nImage Source: [https://nihcc.app.box.com/v/ChestXray-NIHCC](https://nihcc.app.box.com/v/ChestXray-NIHCC)',\n",
       "  'paper': {'title': 'ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases',\n",
       "   'url': 'https://paperswithcode.com/paper/chestx-ray8-hospital-scale-chest-x-ray'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Medical Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-generation'},\n",
       "   {'task': 'Pneumonia Detection',\n",
       "    'url': 'https://paperswithcode.com/task/pneumonia-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ChestX-ray14', 'ChestXray14 1024x1024'],\n",
       "  'num_papers': 122,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hico',\n",
       "  'name': 'HICO',\n",
       "  'full_name': 'Humans Interacting with Common Objects',\n",
       "  'homepage': 'http://www.umich.edu/~ywchao/hico/',\n",
       "  'description': '**HICO** is a benchmark for recognizing human-object interactions (HOI). \\r\\n\\r\\nKey features:\\r\\n\\r\\n- A diverse set of interactions with common object categories\\r\\n- A list of well-defined, sense-based HOI categories\\r\\n- An exhaustive labeling of co-occurring interactions with an object category in each image\\r\\n- The annotation of each HOI instance (i.e. a human and an object bounding box with an interaction class label) in all images\\r\\n\\r\\nSource: [HICO: A Benchmark for Recognizing Human-Object Interactions in Images](http://openaccess.thecvf.com/content_iccv_2015/papers/Chao_HICO_A_Benchmark_ICCV_2015_paper.pdf)',\n",
       "  'paper': {'title': 'HICO: A Benchmark for Recognizing Human-Object Interactions in Images',\n",
       "   'url': 'https://paperswithcode.com/paper/hico-a-benchmark-for-recognizing-human-object'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Human-Object Interaction Detection',\n",
       "    'url': 'https://paperswithcode.com/task/human-object-interaction-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HICO'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ade-corpus',\n",
       "  'name': 'Adverse Drug Events (ADE) Corpus',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://pubmed.ncbi.nlm.nih.gov/22554702/',\n",
       "  'description': 'Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports.\\r\\n\\r\\nA significant amount of information about drug-related safety issues such as adverse effects are published in medical case reports that can only be explored by human readers due to their unstructured nature. The work presented here aims at generating a systematically annotated corpus that can support the development and validation of methods for the automatic extraction of drug-related adverse effects from medical case reports. The documents are systematically double annotated in various rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate representative consensus annotations. In order to demonstrate an example use case scenario, the corpus was employed to train and validate models for the classification of informative against the non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by 10-fold cross-validation resulted in the F₁ score of 0.70 indicating a potential useful application of the corpus.',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Clinical Concept Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/clinical-concept-extraction'},\n",
       "   {'task': 'NER', 'url': 'https://paperswithcode.com/task/cg'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Adverse Drug Events (ADE) Corpus'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sports-1m',\n",
       "  'name': 'Sports-1M',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cs.stanford.edu/people/karpathy/deepvideo/',\n",
       "  'description': 'The **Sports-1M** dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class.\\r\\n\\r\\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)\\r\\n\\r\\nImage Source: [Computer Vision for Sports](https://www.researchgate.net/publication/316477606_Computer_vision_for_sports_Current_applications_and_research_topics)',\n",
       "  'paper': {'title': 'Large-Scale Video Classification with Convolutional Neural Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-video-classification-with-1'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Sports-1M'],\n",
       "  'num_papers': 145,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/youtube-8m',\n",
       "  'name': 'YouTube-8M',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://research.google.com/youtube8m/',\n",
       "  'description': 'The **YouTube-8M** dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc\\r\\n\\r\\nSource: [Audio-Visual Embedding for Cross-Modal Music Video Retrieval through Supervised Deep CCA](https://arxiv.org/abs/1908.03744)',\n",
       "  'paper': {'title': 'YouTube-8M: A Large-Scale Video Classification Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/youtube-8m-a-large-scale-video-classification'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video Classification',\n",
       "    'url': 'https://paperswithcode.com/task/video-classification'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['YouTube-8M'],\n",
       "  'num_papers': 107,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/something-something-v2',\n",
       "  'name': 'Something-Something V2',\n",
       "  'full_name': '20BN-Something-Something\\xa0Dataset V2',\n",
       "  'homepage': 'https://developer.qualcomm.com/software/ai-datasets/something-something',\n",
       "  'description': 'The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels.\\r\\n\\r\\n[Source](https://developer.qualcomm.com/software/ai-datasets/something-something)\\r\\n\\r\\n[Image Source](https://developer.qualcomm.com/software/ai-datasets/something-something)',\n",
       "  'paper': {'title': 'The \"something something\" video database for learning and evaluating visual common sense',\n",
       "   'url': 'https://paperswithcode.com/paper/the-something-something-video-database-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Classification',\n",
       "    'url': 'https://paperswithcode.com/task/video-classification'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'General Action Video Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/general-action-video-anomaly-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Something-Something V2'],\n",
       "  'num_papers': 111,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv2/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/jester',\n",
       "  'name': 'Jester',\n",
       "  'full_name': 'Jester dataset',\n",
       "  'homepage': 'http://eigentaste.berkeley.edu/dataset/',\n",
       "  'description': '6.5 million anonymous ratings of jokes by users of the **Jester** Joke Recommender System.\\r\\n\\r\\nSource: [Jester Datasets for Recommender Systems and Collaborative Filtering Research](http://eigentaste.berkeley.edu/dataset/)\\r\\nImage Source: [http://eigentaste.berkeley.edu/](http://eigentaste.berkeley.edu/)',\n",
       "  'paper': {'title': 'Eigentaste: A Constant Time Collaborative Filtering Algorithm',\n",
       "   'url': 'https://doi.org/10.1023/A:1011419012209'},\n",
       "  'introduced_date': '2001-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Hand Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Jester', 'Jester val', 'Jester test'],\n",
       "  'num_papers': 80,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jester/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/something-something-v1',\n",
       "  'name': 'Something-Something V1',\n",
       "  'full_name': '20BN-Something-Something\\xa0Dataset V1',\n",
       "  'homepage': 'https://20bn.com/datasets/something-something/v1',\n",
       "  'description': 'The 20BN-SOMETHING-SOMETHING dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 108,499 videos, with 86,017 in the training set, 11,522 in the validation set and 10,960 in the test set. There are 174 labels.\\r\\n\\r\\n⚠️ Attention: This is the outdated V1 of the dataset. V2 is available [here](https://paperswithcode.com/dataset/something-something-v2).\\r\\n\\r\\nSource: [https://20bn.com/datasets/something-something/v1](https://20bn.com/datasets/something-something/v1)\\r\\nImage Source: [https://20bn.com/datasets/something-something/v1](https://20bn.com/datasets/something-something/v1)',\n",
       "  'paper': {'title': 'The \"something something\" video database for learning and evaluating visual common sense',\n",
       "   'url': 'https://paperswithcode.com/paper/the-something-something-video-database-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Classification',\n",
       "    'url': 'https://paperswithcode.com/task/video-classification'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Something-Something V1'],\n",
       "  'num_papers': 86,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/sthv1/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hvu',\n",
       "  'name': 'HVU',\n",
       "  'full_name': 'Holistic Video Understanding',\n",
       "  'homepage': 'https://github.com/holistic-video-understanding/HVU-Dataset',\n",
       "  'description': 'HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios.\\r\\n\\r\\nSource: [Large Scale Holistic Video Understanding](/paper/holistic-large-scale-video-understanding)',\n",
       "  'paper': {'title': 'Large Scale Holistic Video Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/holistic-large-scale-video-understanding'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HVU'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/hvu/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/holistic-video-understanding/HVU-Dataset',\n",
       "    'repo': 'https://github.com/holistic-video-understanding/HVU-Dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ptc',\n",
       "  'name': 'PTC',\n",
       "  'full_name': 'Predictive Toxicology Challenge',\n",
       "  'homepage': 'https://relational.fit.cvut.cz/dataset/PTC',\n",
       "  'description': '**PTC** is a collection of 344 chemical compounds represented as graphs which report the carcinogenicity for rats. There are 19 node labels for each node.\\r\\n\\r\\nSource: [Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity](https://arxiv.org/abs/1904.01098)',\n",
       "  'paper': {'title': 'The predictive toxicology challenge',\n",
       "   'url': 'http://www.predictive-toxicology.org/ptc/Citedby:xxref-S2SS1p3'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PTC'],\n",
       "  'num_papers': 73,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ut-kinect',\n",
       "  'name': 'UT-Kinect',\n",
       "  'full_name': 'UTKinect-Action3D Dataset',\n",
       "  'homepage': 'https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html',\n",
       "  'description': 'The **UT-Kinect** dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s.\\r\\n\\r\\nSource: [https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)\\r\\nImage Source: [https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html](https://cvrc.ece.utexas.edu/KinectDatasets/HOJ3D.html)',\n",
       "  'paper': {'title': 'View invariant human action recognition using histograms of 3D joints',\n",
       "   'url': 'https://paperswithcode.com/paper/view-invariant-human-action-recognition-using'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Multimodal Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UT-Kinect'],\n",
       "  'num_papers': 66,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ipc-grounded',\n",
       "  'name': 'IPC-grounded',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': '',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IPC-grounded'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cad-120',\n",
       "  'name': 'CAD-120',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.re3data.org/repository/r3d100012216',\n",
       "  'description': 'The CAD-60 and **CAD-120** data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor. Being able to detect human activities is important for making personal assistant robots useful in performing assistive tasks. The CAD dataset comprises twelve different activities (composed of several sub-activities) performed by four people in different environments, such as a kitchen, a living room, and office, etc.\\r\\n\\r\\nSource: [https://www.re3data.org/repository/r3d100012216](https://www.re3data.org/repository/r3d100012216)\\r\\nImage Source: [https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041](https://www.researchgate.net/figure/The-CAD-120-dataset-A-Examples-of-high-level-activities-from-the-dataset-B-A_fig3_335424041)',\n",
       "  'paper': {'title': 'Learning Human Activities and Object Affordances from RGB-D Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-human-activities-and-object'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CAD-120'],\n",
       "  'num_papers': 48,\n",
       "  'data_loaders': [{'url': 'https://github.com/Shimingyi/MotioNet',\n",
       "    'repo': 'https://github.com/Shimingyi/MotioNet',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ntu-rgb-d-120',\n",
       "  'name': 'NTU RGB+D 120',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp',\n",
       "  'description': 'NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. \\r\\n\\r\\nSource: [NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding](https://arxiv.org/pdf/1905.04757v2.pdf)',\n",
       "  'paper': {'title': 'NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/ntu-rgbd-120-a-large-scale-benchmark-for-3d'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Human Interaction Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/human-interaction-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Human action generation',\n",
       "    'url': 'https://paperswithcode.com/task/human-action-generation'},\n",
       "   {'task': 'Zero Shot Skeletal Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-skeletal-action-recognition'},\n",
       "   {'task': 'Generalized Zero Shot skeletal action recognition',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-skeletal-action'},\n",
       "   {'task': 'One-Shot 3D Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-3d-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NTU RGB+D 120'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ck',\n",
       "  'name': 'CK+',\n",
       "  'full_name': 'Extended Cohn-Kanade dataset',\n",
       "  'homepage': 'http://www.jeffcohn.net/Resources/',\n",
       "  'description': 'The Extended Cohn-Kanade (**CK+**) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods.\\r\\n\\r\\nSource: [EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design for Real-time Facial Expression Recognition](https://arxiv.org/abs/2006.15759)',\n",
       "  'paper': {'title': 'The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2010.5543262'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': ['Korean'],\n",
       "  'variants': ['CK+'],\n",
       "  'num_papers': 170,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/youtube-vos',\n",
       "  'name': 'YouTube-VOS 2018 val',\n",
       "  'full_name': 'Youtube Video Object Segmentation',\n",
       "  'homepage': 'https://youtube-vos.org/',\n",
       "  'description': 'Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration.\\r\\n\\r\\nSource: [CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing](https://arxiv.org/abs/1910.00132)\\r\\nImage Source: [https://youtube-vos.org/](https://youtube-vos.org/)',\n",
       "  'paper': {'title': 'YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark',\n",
       "   'url': 'https://arxiv.org/pdf/1809.03327.pdf'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Semi-Supervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-video-object-segmentation'},\n",
       "   {'task': 'Video Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/video-inpainting'},\n",
       "   {'task': 'One-shot visual object segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-visual-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['YouTube-VOS 2018 val'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tabfact',\n",
       "  'name': 'TabFact',\n",
       "  'full_name': 'TabFact',\n",
       "  'homepage': 'https://tabfact.github.io/',\n",
       "  'description': '**TabFact** is a large-scale dataset which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. TabFact is the first dataset to evaluate language inference on structured data, which involves mixed reasoning skills in both symbolic and linguistic aspects. \\r\\n\\r\\nSource: [GitHub](https://github.com/wenhuchen/Table-Fact-Checking)',\n",
       "  'paper': {'title': 'TabFact: A Large-scale Dataset for Table-based Fact Verification',\n",
       "   'url': 'https://paperswithcode.com/paper/tabfact-a-large-scale-dataset-for-table-based'},\n",
       "  'introduced_date': '2019-09-05',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Table-based Fact Verification',\n",
       "    'url': 'https://paperswithcode.com/task/table-based-fact-verification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TabFact'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/tab_fact',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpi-inf-3dhp',\n",
       "  'name': 'MPI-INF-3DHP',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://gvv.mpi-inf.mpg.de/3dhp-dataset/',\n",
       "  'description': '**MPI-INF-3DHP** is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras.\\r\\n\\r\\nSource: [Anatomy-aware 3D Human Pose Estimation in Videos](https://arxiv.org/abs/2002.10322)\\r\\nImage Source: [https://arxiv.org/abs/1611.09813](https://arxiv.org/abs/1611.09813)',\n",
       "  'paper': {'title': 'Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision',\n",
       "   'url': 'https://paperswithcode.com/paper/monocular-3d-human-pose-estimation-in-the'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D'],\n",
       "  'tasks': [{'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'},\n",
       "   {'task': 'Pose Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/pose-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MPI-INF-3DHP'],\n",
       "  'num_papers': 133,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/3d_body_mesh.md#mpi-inf-3dhp',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/beijing-air-quality',\n",
       "  'name': 'Beijing Multi-Site Air-Quality Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data',\n",
       "  'description': 'This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA.',\n",
       "  'paper': {'title': 'Cautionary Tales on Air-Quality Improvement in Beijing',\n",
       "   'url': 'https://paperswithcode.com/paper/cautionary-tales-on-air-quality-improvement'},\n",
       "  'introduced_date': '2017-08-11',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Multivariate Time Series Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-imputation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Beijing Multi-Site Air-Quality Dataset'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/physionet-challenge-2012',\n",
       "  'name': 'PhysioNet Challenge 2012',\n",
       "  'full_name': 'PhysioNet Challenge 2012',\n",
       "  'homepage': 'https://polyp.grand-challenge.org/CVCClinicDB/',\n",
       "  'description': 'The **PhysioNet Challenge 2012** dataset is publicly available and contains the de-identified records of 8000 patients in Intensive Care Units (ICU). Each record consists of roughly 48 hours of multivariate time series data with up to 37 features recorded at various times from the patients during their stay such as respiratory rate, glucose etc.\\n\\nSource: [Multi-resolution Networks For Flexible Irregular Time Series Modeling (Multi-FIT)](https://arxiv.org/abs/1905.00125)\\nImage Source: [https://physionet.org/content/challenge-2016/1.0.0/](https://physionet.org/content/challenge-2016/1.0.0/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Multivariate Time Series Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-forecasting'},\n",
       "   {'task': 'Time Series Classification',\n",
       "    'url': 'https://paperswithcode.com/task/time-series-classification'},\n",
       "   {'task': 'Imputation', 'url': 'https://paperswithcode.com/task/imputation'},\n",
       "   {'task': 'Multivariate Time Series Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-imputation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PhysioNet Challenge 2012'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mujoco',\n",
       "  'name': 'MuJoCo',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.mujoco.org/',\n",
       "  'description': '**MuJoCo** (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods.',\n",
       "  'paper': {'title': 'MuJoCo: A physics engine for model-based control',\n",
       "   'url': 'https://paperswithcode.com/paper/mujoco-a-physics-engine-for-model-based'},\n",
       "  'introduced_date': '2012-10-07',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Multivariate Time Series Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-forecasting'},\n",
       "   {'task': 'Multivariate Time Series Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-imputation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MuJoCo'],\n",
       "  'num_papers': 815,\n",
       "  'data_loaders': [{'url': 'https://github.com/deepmind/mujoco',\n",
       "    'repo': 'https://github.com/deepmind/mujoco',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sunys',\n",
       "  'name': 'SunYs',\n",
       "  'full_name': 'Lungvesselct',\n",
       "  'homepage': '',\n",
       "  'description': '',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Pulmonary Artery–Vein Classification',\n",
       "    'url': 'https://paperswithcode.com/task/pulmonary-arteryvein-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SunYs'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wflw',\n",
       "  'name': 'WFLW',\n",
       "  'full_name': 'Wider Facial Landmarks in the Wild',\n",
       "  'homepage': 'https://wywu.github.io/projects/LAB/WFLW.html',\n",
       "  'description': 'The **Wider Facial Landmarks in the Wild** or **WFLW** database contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions.\\r\\n\\r\\nSource: [Deep Entwined Learning Head Pose and Face Alignment Inside an Attentional Cascade with Doubly-Conditional fusion](https://arxiv.org/abs/2004.06558)\\r\\nImage Source: [https://wywu.github.io/projects/LAB/WFLW.html](https://wywu.github.io/projects/LAB/WFLW.html)',\n",
       "  'paper': {'title': 'Look at Boundary: A Boundary-Aware Face Alignment Algorithm',\n",
       "   'url': 'https://paperswithcode.com/paper/look-at-boundary-a-boundary-aware-face'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WFLW'],\n",
       "  'num_papers': 64,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/wflw-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reds',\n",
       "  'name': 'REDS',\n",
       "  'full_name': 'REalistic and Diverse Scenes dataset\\nrealistic and dynamic scenes',\n",
       "  'homepage': 'https://seungjunnah.github.io/Datasets/reds.html',\n",
       "  'description': 'The realistic and dynamic scenes (**REDS**) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720×1,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively\\n\\nSource: [Video Super Resolution Based on Deep Learning: A comprehensive survey](https://arxiv.org/abs/2007.12928)\\nImage Source: [https://seungjunnah.github.io/Datasets/reds.html](https://seungjunnah.github.io/Datasets/reds.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Deblurring',\n",
       "    'url': 'https://paperswithcode.com/task/deblurring'}],\n",
       "  'languages': [],\n",
       "  'variants': ['REDS'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nuscenes',\n",
       "  'name': 'nuScenes',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.nuscenes.org/',\n",
       "  'description': 'The **nuScenes** dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360° coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers.\\r\\n\\r\\nSource: [PointPainting: Sequential Fusion for 3D Object Detection](https://arxiv.org/abs/1911.10150)',\n",
       "  'paper': {'title': 'nuScenes: A multimodal dataset for autonomous driving',\n",
       "   'url': 'https://paperswithcode.com/paper/nuscenes-a-multimodal-dataset-for-autonomous'},\n",
       "  'introduced_date': '2019-03-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud', 'LiDAR'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-prediction'},\n",
       "   {'task': '3D Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-semantic-segmentation'},\n",
       "   {'task': '3D Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/3d-multi-object-tracking'},\n",
       "   {'task': 'LIDAR Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/lidar-semantic-segmentation'},\n",
       "   {'task': 'Weather Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/weather-forecasting'},\n",
       "   {'task': \"Bird's-Eye View Semantic Segmentation\",\n",
       "    'url': 'https://paperswithcode.com/task/bird-s-eye-view-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['nuScenes', 'nuScenes-F', 'nuScenes-FB'],\n",
       "  'num_papers': 397,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection3d/blob/master/docs/data_preparation.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection3d',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sleep-edf',\n",
       "  'name': 'Sleep-EDF',\n",
       "  'full_name': 'Sleep-EDF Expanded',\n",
       "  'homepage': 'https://www.physionet.org/content/sleep-edfx/1.0.0/',\n",
       "  'description': 'The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers. Some records also contain respiration and body temperature. Corresponding hypnograms (sleep patterns) were manually scored by well-trained technicians according to the Rechtschaffen and Kales manual, and are also available.\\r\\n\\r\\nSource: [https://www.physionet.org/content/sleep-edfx/1.0.0/](https://www.physionet.org/content/sleep-edfx/1.0.0/)',\n",
       "  'paper': {'title': 'Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals',\n",
       "   'url': 'http://circ.ahajournals.org/content/101/23/e215.full'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Medical', 'EEG'],\n",
       "  'tasks': [{'task': 'Sleep Stage Detection',\n",
       "    'url': 'https://paperswithcode.com/task/sleep-stage-detection'},\n",
       "   {'task': 'Multimodal Sleep Stage Detection',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-sleep-stage-detection'},\n",
       "   {'task': 'Automatic Sleep Stage Classification',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-sleep-stage-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Sleep-EDF-SC', 'Sleep-EDF-ST', 'Sleep-EDF'],\n",
       "  'num_papers': 32,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/commonsenseqa',\n",
       "  'name': 'CommonsenseQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.tau-nlp.org/commonsenseqa',\n",
       "  'description': 'The **CommonsenseQA** is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each.\\r\\nThe dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):\\r\\n\\r\\n1. a crowd worker observes a source concept from ConceptNet (“River”) and three target concepts (“Waterfall”, “Bridge”, “Valley”) that are all related by the same ConceptNet relation (“AtLocation”),\\r\\n2. the worker authors three questions, one per target concept, such that only that particular target concept is the answer, while the other two distractor concepts are not, (“Where on a river can you hold a cup upright to catch water on a sunny day?”, “Where can I stand on a river to see water falling without getting wet?”, “I’m crossing the river, my feet are wet but my body is dry, where am I?”)\\r\\n3. for each question, another worker chooses one additional distractor from Concept Net (“pebble”, “stream”, “bank”), and the author another distractor (“mountain”, “bottom”, “island”) manually.\\r\\n\\r\\nSource: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)\\r\\nImage Source: [CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge](https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge/)',\n",
       "  'paper': {'title': 'CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge',\n",
       "   'url': 'https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CommonsenseQA'],\n",
       "  'num_papers': 128,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/commonsense_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#commonsenseqa',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/mc/dataset_readers/commonsenseqa/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/3dpw',\n",
       "  'name': '3DPW',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://virtualhumans.mpi-inf.mpg.de/3DPW',\n",
       "  'description': 'The **3D Poses in the Wild dataset** is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera.\\r\\n\\r\\nThe dataset includes:\\r\\n\\r\\n* 60 video sequences.\\r\\n* 2D pose annotations.\\r\\n* 3D poses obtained with the method introduced in the paper.\\r\\n* Camera poses for every frame in the sequences.\\r\\n* 3D body scans and 3D people models (re-poseable and re-shapeable). Each sequence contains its corresponding models.\\r\\n* 18 3D models in different clothing variations.\\r\\n\\r\\nSource: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)\\r\\nImage Source: [https://virtualhumans.mpi](http://virtualhumans.mpi-inf.mpg.de/3DPW)',\n",
       "  'paper': {'title': 'Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera',\n",
       "   'url': 'https://paperswithcode.com/paper/recovering-accurate-3d-human-pose-in-the-wild'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D'],\n",
       "  'tasks': [{'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'},\n",
       "   {'task': 'Human Pose Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/human-pose-forecasting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['3DPW'],\n",
       "  'num_papers': 136,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vot2019',\n",
       "  'name': 'VOT2019',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.votchallenge.net/vot2019/dataset.html',\n",
       "  'description': '**VOT2019** is a Visual Object Tracking benchmark for short-term tracking in RGB.\\n\\nSource: [https://www.votchallenge.net/vot2019/dataset.html](https://www.votchallenge.net/vot2019/dataset.html)\\nImage Source: [https://www.votchallenge.net/vot2019/dataset.html](https://www.votchallenge.net/vot2019/dataset.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VOT2019'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/musdb18',\n",
       "  'name': 'MUSDB18',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://sigsep.github.io/datasets/musdb.html',\n",
       "  'description': 'The **MUSDB18** is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems.\\r\\n\\r\\nThe dataset is split into training and test sets with 100 and 50 songs, respectively. All signals are stereophonic and encoded at 44.1kHz.\\r\\n\\r\\nSource: [https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)\\r\\nImage Source: [https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)',\n",
       "  'paper': {'title': 'The MUSDB18 corpus for music separation',\n",
       "   'url': 'https://doi.org/10.5281/zenodo.1117372'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'},\n",
       "   {'task': 'Audio Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/audio-source-separation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MUSDB18'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': [{'url': 'https://github.com/sigsep/open-unmix-pytorch',\n",
       "    'repo': 'https://github.com/sigsep/open-unmix-pytorch',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://sigsep.github.io/sigsep-mus-db/',\n",
       "    'repo': 'https://github.com/sigsep/sigsep-mus-db',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/boolq',\n",
       "  'name': 'BoolQ',\n",
       "  'full_name': 'Boolean Questions',\n",
       "  'homepage': 'https://github.com/google-research-datasets/boolean-questions',\n",
       "  'description': '**BoolQ** is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring – they are generated in unprompted and unconstrained settings.\\r\\nEach example is a triplet of (question, passage, answer), with the title of the page as optional additional context.\\r\\n\\r\\nQuestions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified and questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Only questions that were marked as having a yes/no answer are used, and each question is paired with the selected passage instead of the entire document.\\r\\n\\r\\nSource: [https://github.com/google-research-datasets/boolean-questions](https://github.com/google-research-datasets/boolean-questions)\\r\\nImage Source: [BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions](https://paperswithcode.com/paper/boolq-exploring-the-surprising-difficulty-of/)',\n",
       "  'paper': {'title': 'BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions',\n",
       "   'url': 'https://paperswithcode.com/paper/boolq-exploring-the-surprising-difficulty-of'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['BoolQ'],\n",
       "  'num_papers': 110,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/boolq',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/bool_q',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/google-research-datasets/boolean-questions',\n",
       "    'repo': 'https://github.com/google-research-datasets/boolean-questions',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/copa',\n",
       "  'name': 'COPA',\n",
       "  'full_name': 'Choice of Plausible Alternatives',\n",
       "  'homepage': 'https://people.ict.usc.edu/~gordon/copa.html',\n",
       "  'description': 'The Choice Of Plausible Alternatives (**COPA**) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%.\\r\\n\\r\\nSource: [Choice of Plausible Alternatives (COPA)](https://people.ict.usc.edu/~gordon/copa.html)\\r\\nImage Source: [https://people.ict.usc.edu/~gordon/copa.html](https://people.ict.usc.edu/~gordon/copa.html)',\n",
       "  'paper': {'title': 'Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning',\n",
       "   'url': 'http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['COPA'],\n",
       "  'num_papers': 111,\n",
       "  'data_loaders': [{'url': 'https://parl.ai/docs/tasks.html#choice-of-plausible-alternatives',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/record',\n",
       "  'name': 'ReCoRD',\n",
       "  'full_name': 'ReCoRD',\n",
       "  'homepage': 'https://sheng-z.github.io/ReCoRD-explorer/',\n",
       "  'description': \"**Reading Comprehension with Commonsense Reasoning Dataset** (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [ˈrɛkərd].\\r\\n\\r\\nImage Source: [Zhang et al](https://arxiv.org/pdf/1810.12885v1.pdf)\",\n",
       "  'paper': {'title': 'ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension',\n",
       "   'url': 'https://paperswithcode.com/paper/record-bridging-the-gap-between-human-and'},\n",
       "  'introduced_date': '2018-10-30',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ReCoRD'],\n",
       "  'num_papers': 57,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lidc-idri',\n",
       "  'name': 'LIDC-IDRI',\n",
       "  'full_name': 'LIDC-IDRI',\n",
       "  'homepage': 'https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI',\n",
       "  'description': 'The **LIDC-IDRI** dataset contains lesion annotations from four experienced thoracic radiologists. LIDC-IDRI contains 1,018 low-dose lung CTs from 1010 lung patients.\\r\\n\\r\\nSource: [A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans](https://arxiv.org/abs/1902.03233)\\r\\nImage Source: [https://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=IJACSA&SerialNo=15](https://thesai.org/Publications/ViewPaper?Volume=8&Issue=10&Code=IJACSA&SerialNo=15)',\n",
       "  'paper': {'title': 'The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans',\n",
       "   'url': 'http://dx.doi.org/10.1118/1.3528204'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Lung Nodule Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/lung-nodule-segmentation'},\n",
       "   {'task': 'Lung Nodule Classification',\n",
       "    'url': 'https://paperswithcode.com/task/lung-nodule-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LIDC-IDRI'],\n",
       "  'num_papers': 122,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/orl',\n",
       "  'name': 'ORL',\n",
       "  'full_name': 'Our Database of Faces',\n",
       "  'homepage': 'https://cam-orl.co.uk/facedatabase.html',\n",
       "  'description': 'The **ORL** Database of Faces contains 400 images from 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel.\\r\\n\\r\\nSource: [https://cam-orl.co.uk/facedatabase.html](https://cam-orl.co.uk/facedatabase.html)\\r\\nImage Source: [https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition](https://www.researchgate.net/publication/221786184_PCA_and_LDA_Based_Neural_Networks_for_Human_Face_Recognition)',\n",
       "  'paper': {'title': 'Parameterisation of a stochastic model for human face identification',\n",
       "   'url': 'https://doi.org/10.1109/ACV.1994.341300'},\n",
       "  'introduced_date': '1994-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Multi-view Subspace Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/multi-view-subspace-clustering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ORL'],\n",
       "  'num_papers': 105,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/egogesture',\n",
       "  'name': 'EgoGesture',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html',\n",
       "  'description': 'The **EgoGesture** dataset contains 2,081 RGB-D videos, 24,161 gesture samples and 2,953,224 frames from 50 distinct subjects.\\r\\n\\r\\nSource: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)\\r\\nImage Source: [http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html](http://www.nlpr.ia.ac.cn/iva/yfzhang/datasets/egogesture.html)',\n",
       "  'paper': {'title': 'EgoGesture: A New Dataset and Benchmark for Egocentric Hand Gesture Recognition',\n",
       "   'url': 'https://doi.org/10.1109/TMM.2018.2808769'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Hand Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['EgoGesture'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/street-scene',\n",
       "  'name': 'Street Scene',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.merl.com/demos/video-anomaly-detection',\n",
       "  'description': '**Street Scene** is a dataset for video anomaly detection. Street Scene consists of 46 training and 35 testing high resolution 1280×720 video sequences taken from a USB camera overlooking a scene of a two-lane street with bike lanes and pedestrian sidewalks during daytime. The dataset is challenging because of the variety of activity taking place such as cars driving, turning, stopping and parking; pedestrians walking, jogging and pushing strollers; and bikers riding in bike lanes. In addition the videos contain changing shadows, moving background such as a flag and trees blowing in the wind, and occlusions caused by trees and large vehicles. There are a total of 56,847 frames for training and 146,410 frames for testing, extracted from the original videos at 15 frames per second. The dataset contains a total of 205 naturally occurring anomalous events ranging from illegal activities such as jaywalking and illegal U-turns to simply those that do not occur in the training set such as pets being walked and a metermaid ticketing a car.\\n\\nSource: [A Survey of Single-SceneVideo Anomaly Detection](https://arxiv.org/abs/2004.05993)\\nImage Source: [https://www.merl.com/demos/video-anomaly-detection](https://www.merl.com/demos/video-anomaly-detection)',\n",
       "  'paper': {'title': 'Street Scene: A new dataset and evaluation protocol for video anomaly detection',\n",
       "   'url': 'https://paperswithcode.com/paper/street-scene-a-new-dataset-and-evaluation'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video-to-Video Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/video-to-video-synthesis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Street Scene'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/materials-project',\n",
       "  'name': 'Materials Project',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://materialsproject.org/',\n",
       "  'description': 'The **Materials Project** is a collection of chemical compounds labelled with different attributes.',\n",
       "  'paper': {'title': 'The Materials Project: A materials genome approach to accelerating materials innovation',\n",
       "   'url': 'http://link.aip.org/link/AMPADS/v1/i1/p011002/s1&Agg=doi'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Formation Energy',\n",
       "    'url': 'https://paperswithcode.com/task/formation-energy'},\n",
       "   {'task': 'Band Gap', 'url': 'https://paperswithcode.com/task/band-gap'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Materials Project'],\n",
       "  'num_papers': 41,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/semantic3d',\n",
       "  'name': 'Semantic3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.semantic3d.net/',\n",
       "  'description': '**Semantic3D** is a point cloud dataset of scanned outdoor scenes with over 3 billion points. It contains 15 training and 15 test scenes annotated with 8 class labels. This large labelled 3D point cloud data set of natural covers a range of diverse urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles to name just a few. The point clouds provided are scanned statically with state-of-the-art equipment and contain very fine details.\\r\\n\\r\\nSource: [Tangent Convolutions for Dense Prediction in 3D](https://arxiv.org/abs/1807.02443)\\r\\nImage Source: [http://www.semantic3d.net/](http://www.semantic3d.net/)',\n",
       "  'paper': {'title': 'Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/semantic3dnet-a-new-large-scale-point-cloud'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D', 'Point cloud'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Semantic3D'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/semantickitti',\n",
       "  'name': 'SemanticKITTI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.semantic-kitti.org/',\n",
       "  'description': '**SemanticKITTI** is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing.\\r\\n\\r\\nSource: [Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation](https://arxiv.org/abs/2008.01550)\\r\\nImage Source: [https://github.com/PRBonn/semantic-kitti-api](https://github.com/PRBonn/semantic-kitti-api)',\n",
       "  'paper': {'title': 'SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences',\n",
       "   'url': 'https://paperswithcode.com/paper/a-dataset-for-semantic-segmentation-of-point'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D', 'Point cloud'],\n",
       "  'tasks': [{'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': '3D Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-semantic-segmentation'},\n",
       "   {'task': 'LIDAR Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/lidar-semantic-segmentation'},\n",
       "   {'task': 'Real-Time 3D Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-3d-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SemanticKITTI'],\n",
       "  'num_papers': 164,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wine',\n",
       "  'name': 'Wine',\n",
       "  'full_name': 'Wine Data Set',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/Wine',\n",
       "  'description': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\\n\\nSource: [UCI Machine Learning Repository Wine Dataset](https://archive.ics.uci.edu/ml/datasets/Wine)\\nImage Source: [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)',\n",
       "  'paper': {'title': 'Endgame Analysis of Dou Shou Qi',\n",
       "   'url': 'https://paperswithcode.com/paper/endgame-analysis-of-dou-shou-qi'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'General Classification',\n",
       "    'url': 'https://paperswithcode.com/task/classification'},\n",
       "   {'task': 'Image/Document Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/imagedocument-clustering'},\n",
       "   {'task': 'Feature Importance',\n",
       "    'url': 'https://paperswithcode.com/task/feature-importance'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Wine'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jsb-chorales',\n",
       "  'name': 'JSB Chorales',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www-etud.iro.umontreal.ca/~boulanni/icml2012',\n",
       "  'description': 'The **JSB** chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the\\r\\n18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining\\r\\nthree voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively.',\n",
       "  'paper': {'title': 'Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription',\n",
       "   'url': 'https://paperswithcode.com/paper/modeling-temporal-dependencies-in-high'},\n",
       "  'introduced_date': '2012-06-27',\n",
       "  'warning': None,\n",
       "  'modalities': ['Midi', 'Music'],\n",
       "  'tasks': [{'task': 'Music Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/music-modeling'},\n",
       "   {'task': 'Music Generation',\n",
       "    'url': 'https://paperswithcode.com/task/music-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JSB Chorales'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tiny-imagenet',\n",
       "  'name': 'Tiny ImageNet',\n",
       "  'full_name': 'Tiny ImageNet',\n",
       "  'homepage': 'https://www.kaggle.com/c/tiny-imagenet',\n",
       "  'description': '**Tiny ImageNet** contains 100000 images of 200 classes (500 for each class) downsized to 64×64 colored images. Each class has 500 training images, 50 validation images and 50 test images.\\r\\n\\r\\nSource: [Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI](https://arxiv.org/abs/2007.06712)\\r\\nImage Source: [https://arxiv.org/pdf/1707.08819.pdf](https://arxiv.org/pdf/1707.08819.pdf)',\n",
       "  'paper': {'title': 'Tiny imagenet visual recognition challenge',\n",
       "   'url': 'https://tiny-imagenet.herokuapp.com'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Weakly-Supervised Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-localization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Tiny ImageNet',\n",
       "   'Tiny ImageNet Classification',\n",
       "   'Tiny-ImageNet'],\n",
       "  'num_papers': 300,\n",
       "  'data_loaders': [{'url': 'https://github.com/pranavphoenix/TinyImageNetLoader',\n",
       "    'repo': 'https://github.com/pranavphoenix/TinyImageNetLoader',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/afhq',\n",
       "  'name': 'AFHQ',\n",
       "  'full_name': 'Animal Faces-HQ',\n",
       "  'homepage': 'https://github.com/clovaai/stargan-v2',\n",
       "  'description': 'Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 × 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various\\r\\nbreeds (≥ eight) per each domain, AFHQ sets a more challenging image-to-image translation problem. \\r\\nAll images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort.\\r\\n\\r\\nSource: [StarGAN v2: Diverse Image Synthesis for Multiple Domains](https://arxiv.org/abs/1912.01865)',\n",
       "  'paper': {'title': 'StarGAN v2: Diverse Image Synthesis for Multiple Domains',\n",
       "   'url': 'https://paperswithcode.com/paper/stargan-v2-diverse-image-synthesis-for'},\n",
       "  'introduced_date': '2019-12-04',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Multimodal Unsupervised Image-To-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-unsupervised-image-to-image'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AFHQ Wild', 'AFHQ Dog', 'AFHQ Cat', 'AFHQV2', 'AFHQ'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': [{'url': 'https://github.com/clovaai/stargan-v2',\n",
       "    'repo': 'https://github.com/clovaai/stargan-v2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fss-1000',\n",
       "  'name': 'FSS-1000',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/HKUSTCV/FSS-1000',\n",
       "  'description': '**FSS-1000** is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc.\\n\\nSource: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)\\nImage Source: [https://github.com/HKUSTCV/FSS-1000](https://github.com/HKUSTCV/FSS-1000)',\n",
       "  'paper': {'title': 'FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/fss-1000-a-1000-class-dataset-for-few-shot'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Few-Shot Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FSS-1000'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': [{'url': 'https://github.com/HKUSTCV/FSS-1000',\n",
       "    'repo': 'https://github.com/HKUSTCV/FSS-1000',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reddit',\n",
       "  'name': 'Reddit',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://snap.stanford.edu/graphsage/',\n",
       "  'description': 'The **Reddit** dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or “subreddit”, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1706.02216.pdf](https://arxiv.org/pdf/1706.02216.pdf)\\r\\nImage Source: [https://minimaxir.com/2016/05/reddit-graph/](https://minimaxir.com/2016/05/reddit-graph/)',\n",
       "  'paper': {'title': 'Inductive Representation Learning on Large Graphs',\n",
       "   'url': 'https://paperswithcode.com/paper/inductive-representation-learning-on-large'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Sequence-to-sequence Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/sequence-to-sequence-language-modeling'},\n",
       "   {'task': 'Dialogue Generation',\n",
       "    'url': 'https://paperswithcode.com/task/dialogue-generation'},\n",
       "   {'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Reddit TIFU', 'Reddit (multi-ref)', 'REDDIT-B', 'Reddit'],\n",
       "  'num_papers': 286,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://huggingface.co/datasets/reddit',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.RedditDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/reddit',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://graphneural.network/datasets/#reddit',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/deepfashion',\n",
       "  'name': 'DeepFashion',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://liuziwei7.github.io/projects/DeepFashion.html',\n",
       "  'description': '**DeepFashion** is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos.\\r\\n\\r\\nSource: [A Benchmark for Inpainting of Clothing Images with Irregular Holes](https://arxiv.org/abs/2007.05080)',\n",
       "  'paper': {'title': 'DeepFashion: Powering Robust Clothes Recognition and Retrieval With Rich Annotations',\n",
       "   'url': 'https://paperswithcode.com/paper/deepfashion-powering-robust-clothes'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Virtual Try-on',\n",
       "    'url': 'https://paperswithcode.com/task/virtual-try-on'},\n",
       "   {'task': 'Pose Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/pose-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Deep-Fashion',\n",
       "   'DeepFashion',\n",
       "   'DeepFashion - Consumer-to-shop'],\n",
       "  'num_papers': 221,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_fashion_landmark.md#deepfashion',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fer2013',\n",
       "  'name': 'FER2013',\n",
       "  'full_name': 'Facial Expression Recognition 2013 Dataset',\n",
       "  'homepage': 'https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data',\n",
       "  'description': 'Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48×48, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images – 600, while other labels have nearly 5,000 samples each.\\r\\n\\r\\nSource: [Eavesdrop the Composition Proportion of Training Labels in Federated Learning](https://arxiv.org/abs/1910.06044)\\r\\nImage Source: [https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280](https://medium.com/@birdortyedi_23820/deep-learning-lab-episode-3-fer2013-c38f2e052280)',\n",
       "  'paper': {'title': 'Challenges in Representation Learning: A report on three machine learning contests',\n",
       "   'url': 'https://paperswithcode.com/paper/challenges-in-representation-learning-a'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FER2013'],\n",
       "  'num_papers': 91,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/fer2013-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pinterest',\n",
       "  'name': 'Pinterest',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://sites.google.com/site/xueatalphabeta/academic-projects',\n",
       "  'description': 'The **Pinterest** dataset contains more than 1 million images associated to Pinterest users’ who have “pinned” them.\\n\\nSource: [https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf](https://openaccess.thecvf.com/content_iccv_2015/papers/Geng_Learning_Image_and_ICCV_2015_paper.pdf)',\n",
       "  'paper': {'title': 'Learning Image and User Features for Recommendation in Social Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-image-and-user-features-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Pinterest'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lol',\n",
       "  'name': 'LOL',\n",
       "  'full_name': 'LOw-Light dataset',\n",
       "  'homepage': 'https://daooshee.github.io/BMVC2018website/',\n",
       "  'description': 'The **LOL** dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400×600.\\r\\n\\r\\nSource: [Unsupervised Real-world Low-light Image Enhancement with Decoupled Networks](https://arxiv.org/abs/2005.02818)\\r\\nImage Source: [https://daooshee.github.io/BMVC2018website/](https://daooshee.github.io/BMVC2018website/)',\n",
       "  'paper': {'title': 'Deep Retinex Decomposition for Low-Light Enhancement',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-retinex-decomposition-for-low-light'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Low-Light Image Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/low-light-image-enhancement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LOL'],\n",
       "  'num_papers': 64,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hrf',\n",
       "  'name': 'HRF',\n",
       "  'full_name': 'High-Resolution Fundus',\n",
       "  'homepage': 'https://www5.cs.fau.de/research/data/fundus-images/',\n",
       "  'description': 'The **HRF** dataset is a dataset for retinal vessel segmentation which comprises 45 images and is organized as 15 subsets. Each subset contains one healthy fundus image, one image of patient with diabetic retinopathy and one glaucoma image. The image sizes are 3,304 x 2,336, with a training/testing image split of 22/23.\\r\\n\\r\\nSource: [Connection Sensitive Attention U-NET for Accurate Retinal Vessel Segmentation](https://arxiv.org/abs/1903.05558)\\r\\nImage Source: [https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531](https://www.researchgate.net/figure/Examples-of-fundus-images-from-HRF-database-with-corresponding-hand-labelled-gold_fig1_260625531)',\n",
       "  'paper': {'title': 'Robust Vessel Segmentation in Fundus Images',\n",
       "   'url': 'https://doi.org/10.1155/2013/154860'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Retinal Vessel Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/retinal-vessel-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HRF'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dbrd',\n",
       "  'name': 'DBRD',\n",
       "  'full_name': 'Dutch Book Reviews Dataset',\n",
       "  'homepage': 'https://github.com/benjaminvdb/DBRD',\n",
       "  'description': 'The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels. It is greatly influenced by the Large Movie Review Dataset and intended as a benchmark for sentiment classification in Dutch. \\r\\n\\r\\nSource: [DBRD](https://github.com/benjaminvdb/DBRD)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': ['Dutch'],\n",
       "  'variants': ['DBRD'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/dbrd',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/benjaminvdb/DBRD',\n",
       "    'repo': 'https://github.com/benjaminvdb/DBRD',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/thyroid',\n",
       "  'name': 'Thyroid',\n",
       "  'full_name': 'Thyroid Disease',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/thyroid+disease',\n",
       "  'description': '**Thyroid** is a dataset for detection of thyroid diseases, in which patients diagnosed with hypothyroid or subnormal are anomalies against normal patients. It contains 2800 training data instance and 972 test instances, with 29 or so attributes.\\n\\nSource: [Deep Reinforcement Learning for Unknown Anomaly Detection](https://arxiv.org/abs/2009.06847)\\nImage Source: [https://www.researchgate.net/figure/Features-of-Thyroid-dataset_tbl1_285711967](https://www.researchgate.net/figure/Features-of-Thyroid-dataset_tbl1_285711967)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Thyroid'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/argoverse',\n",
       "  'name': 'Argoverse',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.argoverse.org/data.html',\n",
       "  'description': '**Argoverse** is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called “agent”, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap.\\r\\n\\r\\nSource: [Learning Lane Graph Representations for Motion Forecasting](https://arxiv.org/abs/2007.13732)\\r\\nImage Source: [https://arxiv.org/pdf/1911.02620.pdf](https://arxiv.org/pdf/1911.02620.pdf)',\n",
       "  'paper': {'title': 'Argoverse: 3D Tracking and Forecasting with Rich Maps',\n",
       "   'url': 'https://paperswithcode.com/paper/argoverse-3d-tracking-and-forecasting-with-1'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Point cloud', 'LiDAR'],\n",
       "  'tasks': [{'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Monocular Cross-View Road Scene Parsing(Road)',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-cross-view-road-scene-parsing-road'},\n",
       "   {'task': 'Motion Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/motion-forecasting'},\n",
       "   {'task': 'Monocular Cross-View Road Scene Parsing(Vehicle)',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-cross-view-road-scene-parsing'},\n",
       "   {'task': '3D Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Argoverse', 'Argoverse CVPR 2020'],\n",
       "  'num_papers': 130,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/clevr',\n",
       "  'name': 'CLEVR',\n",
       "  'full_name': 'Compositional Language and Elementary Visual Reasoning',\n",
       "  'homepage': 'https://cs.stanford.edu/people/jcjohns/clevr/',\n",
       "  'description': '**CLEVR** (**Compositional Language and Elementary Visual Reasoning**) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, A test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations.\\r\\n\\r\\nSource: [On transfer learning using a MAC model variant](https://arxiv.org/abs/1811.06529)\\r\\nImage Source: [Johnson et al](https://arxiv.org/pdf/1612.06890v1.pdf)',\n",
       "  'paper': {'title': 'CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/clevr-a-diagnostic-dataset-for-compositional'},\n",
       "  'introduced_date': '2016-12-20',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'},\n",
       "   {'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CLEVR', 'CLEVR-Dialog'],\n",
       "  'num_papers': 347,\n",
       "  'data_loaders': [{'url': 'https://parl.ai/docs/tasks.html#clevr',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/clevr',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tai-chi-hd',\n",
       "  'name': 'Tai-Chi-HD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/AliaksandrSiarohin/first-order-model',\n",
       "  'description': '**Thai-Chi-HD** is a high resolution dataset which can be used as reference benchmark for evaluating frameworks for image animation and video generation. It consists of cropped videos of full human bodies performing Tai Chi actions.\\r\\n\\r\\nImage source: [https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf](https://papers.nips.cc/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf)',\n",
       "  'paper': {'title': 'First Order Motion Model for Image Animation',\n",
       "   'url': 'https://paperswithcode.com/paper/first-order-motion-model-for-image-animation-1'},\n",
       "  'introduced_date': '2020-02-29',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/video-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tai-Chi-HD (256)', 'Tai-Chi-HD (512)'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/AliaksandrSiarohin/first-order-model',\n",
       "    'repo': 'https://github.com/AliaksandrSiarohin/first-order-model',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cmu-mosei',\n",
       "  'name': 'CMU-MOSEI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.amir-zadeh.com/datasets',\n",
       "  'description': 'CMU Multimodal Opinion Sentiment and Emotion Intensity (**CMU-MOSEI**) is the largest dataset of sentence level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics.\\r\\n\\r\\nSource: [https://www.amir-zadeh.com/datasets](https://www.amir-zadeh.com/datasets)\\r\\nImage Source: [https://www.amir-zadeh.com/datasets](https://www.amir-zadeh.com/datasets)',\n",
       "  'paper': {'title': 'Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph',\n",
       "   'url': 'https://paperswithcode.com/paper/multimodal-language-analysis-in-the-wild-cmu'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Multimodal Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-sentiment-analysis'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CMU-MOSEI'],\n",
       "  'num_papers': 68,\n",
       "  'data_loaders': [{'url': 'https://github.com/lobracost/MultimodalSDK_loader',\n",
       "    'repo': 'https://github.com/lobracost/MultimodalSDK_loader',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/affectnet',\n",
       "  'name': 'AffectNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://mohammadmahoor.com/affectnet/',\n",
       "  'description': '**AffectNet** is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.\\r\\n\\r\\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\\r\\nImage Source: [http://mohammadmahoor.com/affectnet/](http://mohammadmahoor.com/affectnet/)',\n",
       "  'paper': {'title': 'AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/affectnet-a-database-for-facial-expression'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'},\n",
       "   {'task': 'Valence Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/valence-estimation'},\n",
       "   {'task': 'Arousal Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/arousal-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AffectNet'],\n",
       "  'num_papers': 132,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fer',\n",
       "  'name': 'FER+',\n",
       "  'full_name': 'Face Expression Recognition Plus dataset',\n",
       "  'homepage': 'https://github.com/Microsoft/FERPlus',\n",
       "  'description': 'The **FER+** dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt.\\r\\n\\r\\nSource: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)\\r\\nImage Source: [https://github.com/Microsoft/FERPlus](https://github.com/Microsoft/FERPlus)',\n",
       "  'paper': {'title': 'Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution',\n",
       "   'url': 'https://paperswithcode.com/paper/training-deep-networks-for-facial-expression'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FER+', 'FERPlus'],\n",
       "  'num_papers': 51,\n",
       "  'data_loaders': [{'url': 'https://github.com/Microsoft/FERPlus',\n",
       "    'repo': 'https://github.com/Microsoft/FERPlus',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/commongen',\n",
       "  'name': 'CommonGen',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://inklab.usc.edu/CommonGen/',\n",
       "  'description': 'CommonGen is constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. \\r\\n\\r\\nSource: [CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning](/paper/commongen-a-constrained-text-generation)',\n",
       "  'paper': {'title': 'CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/commongen-a-constrained-text-generation'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CommonGen'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/common_gen',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/the-china-physiological-signal-challenge-2018',\n",
       "  'name': 'The China Physiological Signal Challenge 2018',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://2018.icbeb.org/Challenge.html',\n",
       "  'description': 'The China Physiological Signal Challenge 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include one normal ECG type and eight abnormal types.\\r\\n\\r\\nSource: [An Open Access Database for Evaluating the Algorithms of Electrocardiogram Rhythm and Morphology Abnormality Detection](/paper/an-open-access-database-for-evaluating-the)',\n",
       "  'paper': {'title': 'An Open Access Database for Evaluating the Algorithms of Electrocardiogram Rhythm and Morphology Abnormality Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/an-open-access-database-for-evaluating-the'},\n",
       "  'introduced_date': '2018-09-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Arrhythmia Detection',\n",
       "    'url': 'https://paperswithcode.com/task/arrhythmia-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['The China Physiological Signal Challenge 2018'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/university-1652',\n",
       "  'name': 'University-1652',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/layumi/University1652-Baseline',\n",
       "  'description': 'Contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. University-1652 is a drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. \\r\\n\\r\\nSource: [University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization](/paper/university-1652-a-multi-view-multi-source)',\n",
       "  'paper': {'title': 'University-1652: A Multi-view Multi-source Benchmark for Drone-based Geo-localization',\n",
       "   'url': 'https://paperswithcode.com/paper/university-1652-a-multi-view-multi-source'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Drone-view target localization',\n",
       "    'url': 'https://paperswithcode.com/task/drone-view-target-localization'},\n",
       "   {'task': 'Drone navigation',\n",
       "    'url': 'https://paperswithcode.com/task/drone-navigation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['University-1652'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/layumi/University1652-Baseline',\n",
       "    'repo': 'https://github.com/layumi/University1652-Baseline',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fquad',\n",
       "  'name': 'FQuAD',\n",
       "  'full_name': 'French Question Answering Dataset',\n",
       "  'homepage': 'https://fquad.illuin.tech/',\n",
       "  'description': 'A French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version.\\r\\n\\r\\nSource: [FQuAD: French Question Answering Dataset](/paper/fquad-french-question-answering-dataset)',\n",
       "  'paper': {'title': 'FQuAD: French Question Answering Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/fquad-french-question-answering-dataset'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['French'],\n",
       "  'variants': ['FQuAD'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/fquad',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hard',\n",
       "  'name': 'HARD',\n",
       "  'full_name': 'Hotel Arabic-Reviews Dataset',\n",
       "  'homepage': 'https://github.com/elnagara/HARD-Arabic-Dataset',\n",
       "  'description': 'The Hotel Arabic-Reviews Dataset (HARD) contains 93700 hotel reviews in Arabic language. The hotel reviews were collected from Booking.com website during June/July 2016. The reviews are expressed in Modern Standard Arabic as well as dialectal Arabic.\\r\\n\\r\\nSource: [HARD](https://github.com/elnagara/HARD-Arabic-Dataset)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': ['Arabic'],\n",
       "  'variants': ['HARD'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/hard',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/elnagara/HARD-Arabic-Dataset',\n",
       "    'repo': 'https://github.com/elnagara/HARD-Arabic-Dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/3dfaw',\n",
       "  'name': '3DFAW',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://mhug.disi.unitn.it/workshop/3dfaw/',\n",
       "  'description': '**3DFAW** contains 23k images with 66 3D face keypoint annotations.\\n\\nSource: [Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild](https://arxiv.org/abs/1911.11130)\\nImage Source: [http://mhug.disi.unitn.it/workshop/3dfaw/](http://mhug.disi.unitn.it/workshop/3dfaw/)',\n",
       "  'paper': {'title': 'The First 3D Face Alignment in the Wild (3DFAW) Challenge',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-48881-3_35'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': '3D Facial Landmark Localization',\n",
       "    'url': 'https://paperswithcode.com/task/3d-facial-landmark-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['3DFAW'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/breakfast',\n",
       "  'name': 'Breakfast',\n",
       "  'full_name': 'The Breakfast Actions Dataset',\n",
       "  'homepage': 'https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/',\n",
       "  'description': 'The **Breakfast** Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded “in the wild” as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings.\\r\\n\\r\\nSource: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)\\r\\nImage Source: [https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)',\n",
       "  'paper': {'title': 'The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities',\n",
       "   'url': 'https://paperswithcode.com/paper/the-language-of-actions-recovering-the-syntax'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Actions'],\n",
       "  'tasks': [{'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'},\n",
       "   {'task': 'Weakly Supervised Action Segmentation (Transcript)',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-action-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Breakfast'],\n",
       "  'num_papers': 79,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nell-995',\n",
       "  'name': 'NELL-995',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'NELL-995 KG Completion Dataset',\n",
       "  'paper': {'title': 'DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/deeppath-a-reinforcement-learning-method-for'},\n",
       "  'introduced_date': '2017-07-20',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NELL-995'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/composition-1k',\n",
       "  'name': 'Composition-1K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://arxiv.org/pdf/1703.03872v3.pdf',\n",
       "  'description': 'Composition-1K is a large-scale image matting dataset including 49300 training images and 1000 testing images.\\r\\n\\r\\nImage source: [https://arxiv.org/pdf/1703.03872v3.pdf](https://arxiv.org/pdf/1703.03872v3.pdf)',\n",
       "  'paper': {'title': 'Deep Image Matting',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-image-matting'},\n",
       "  'introduced_date': '2017-03-10',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Matting',\n",
       "    'url': 'https://paperswithcode.com/task/image-matting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Composition-1K'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kolektorsdd',\n",
       "  'name': 'KolektorSDD',\n",
       "  'full_name': 'Kolektor Surface-Defect Dataset',\n",
       "  'homepage': 'https://www.vicos.si/Downloads/KolektorSDD',\n",
       "  'description': 'The dataset is constructed from images of defective production items that were provided and annotated by [Kolektor Group d.o.o.](https://www.kolektordigital.com/en/advanced-visual-tecnologies). The images were captured in a controlled industrial environment in a real-world case.\\r\\n\\r\\nThe dataset consists of 399 images at 500 x ~1250 px in size.\\r\\n\\r\\nPlease cite our paper published in the Journal of Intelligent Manufacturing when using this dataset:\\r\\n\\r\\n```\\r\\n@article{Tabernik2019JIM,\\r\\n  author = {Tabernik, Domen and {\\\\v{S}}ela, Samo and Skvar{\\\\v{c}}, Jure and \\r\\n  Sko{\\\\v{c}}aj, Danijel},\\r\\n  journal = {Journal of Intelligent Manufacturing},\\r\\n  title = {{Segmentation-Based Deep-Learning Approach for Surface-Defect Detection}},\\r\\n  year = {2019},\\r\\n  month = {May},\\r\\n  day = {15},\\r\\n  issn={1572-8145},\\r\\n  doi={10.1007/s10845-019-01476-x}\\r\\n}\\r\\n```',\n",
       "  'paper': {'title': 'Segmentation-Based Deep-Learning Approach for Surface-Defect Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/segmentation-based-deep-learning-approach-for'},\n",
       "  'introduced_date': '2019-03-20',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'},\n",
       "   {'task': 'Defect Detection',\n",
       "    'url': 'https://paperswithcode.com/task/defect-detection'},\n",
       "   {'task': 'Weakly Supervised Defect Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-defect-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KolektorSDD'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aslg-pc12',\n",
       "  'name': 'ASLG-PC12',\n",
       "  'full_name': 'English-ASL Gloss Parallel Corpus 2012',\n",
       "  'homepage': 'https://achrafothman.net/site/english-asl-gloss-parallel-corpus-2012-aslg-pc12/',\n",
       "  'description': 'An artificial corpus built using grammatical dependencies rules due to the lack of resources for Sign Language.\\r\\n\\r\\nSource: [ASLG-PC12](https://achrafothman.net/site/english-asl-gloss-parallel-corpus-2012-aslg-pc12/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-03-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Sign Language Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/sign-language-recognition'},\n",
       "   {'task': 'Sign Language Translation',\n",
       "    'url': 'https://paperswithcode.com/task/sign-language-translation'},\n",
       "   {'task': 'Parallel Corpus Mining',\n",
       "    'url': 'https://paperswithcode.com/task/parallel-corpus-mining'}],\n",
       "  'languages': ['English', 'American Sign Language'],\n",
       "  'variants': ['ASLG-PC12'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/aslg_pc12',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cifar10-dvs',\n",
       "  'name': 'CIFAR10-DVS',\n",
       "  'full_name': 'CIFAR10-DVS',\n",
       "  'homepage': 'https://figshare.com/articles/CIFAR10-DVS_New/4724671/2',\n",
       "  'description': '**CIFAR10-DVS** is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128×128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera.\\n\\nSource: [Structure-Aware Network for Lane Marker Extraction with Dynamic Vision Sensor](https://arxiv.org/abs/2008.06204)\\nImage Source: [https://www.frontiersin.org/articles/10.3389/fnins.2017.00309/full](https://www.frontiersin.org/articles/10.3389/fnins.2017.00309/full)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Event data classification',\n",
       "    'url': 'https://paperswithcode.com/task/event-data-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CIFAR10-DVS'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-online-products',\n",
       "  'name': 'Stanford Online Products',\n",
       "  'full_name': 'Stanford Online Products',\n",
       "  'homepage': 'https://cvgl.stanford.edu/projects/lifted_struct/',\n",
       "  'description': '**Stanford Online Products** (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing\\r\\n\\r\\nSource: [Deep Metric Learning with Alternating Projections onto Feasible Sets](https://arxiv.org/abs/1907.07585)\\r\\nImage Source: [https://cvgl.stanford.edu/projects/lifted_struct/](https://cvgl.stanford.edu/projects/lifted_struct/)',\n",
       "  'paper': {'title': 'Deep Metric Learning via Lifted Structured Feature Embedding',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-metric-learning-via-lifted-structured'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Metric Learning',\n",
       "    'url': 'https://paperswithcode.com/task/metric-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford Online Products'],\n",
       "  'num_papers': 114,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/stanford_online_products',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ecoli',\n",
       "  'name': 'Ecoli',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/ecoli',\n",
       "  'description': 'The **Ecoli** dataset is a dataset for protein localization. It contains 336 E.coli proteins split into 8 different classes.',\n",
       "  'paper': {'title': 'A Probabilistic Classification System for Predicting the Cellular Localization Sites of Proteins',\n",
       "   'url': 'http://www.aaai.org/Library/ISMB/1996/ismb96-012.php'},\n",
       "  'introduced_date': '1996-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/imputation'},\n",
       "   {'task': 'Small Data Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/small-data'},\n",
       "   {'task': 'Outlier Detection',\n",
       "    'url': 'https://paperswithcode.com/task/outlier-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Ecoli'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/yeast',\n",
       "  'name': 'Yeast',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm',\n",
       "  'description': 'Yeast dataset consists of a protein-protein interaction network. Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology.\\r\\n\\r\\nSource: [http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm](http://vlado.fmf.uni-lj.si/pub/networks/data/bio/Yeast/Yeast.htm)',\n",
       "  'paper': {'title': 'Topological structure analysis of the protein-protein interaction network in budding yeast',\n",
       "   'url': 'http://www.imb-jena.de/jcb/ppi/PPI_PDF_free/bu2003.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs', 'Biology'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'},\n",
       "   {'task': 'Q-Learning',\n",
       "    'url': 'https://paperswithcode.com/task/q-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Yeast'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mot17',\n",
       "  'name': 'MOT17',\n",
       "  'full_name': 'Multiple Object Tracking 17',\n",
       "  'homepage': 'https://motchallenge.net/data/MOT17/',\n",
       "  'description': 'The **Multiple Object Tracking 17** (**MOT17**) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks.\\r\\n\\r\\nSource: [Deep Affinity Network for Multiple Object Tracking](https://arxiv.org/abs/1810.11780)\\r\\nImage Source: [https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502](https://www.researchgate.net/figure/Visualization-of-selected-sequences-from-the-MOT17-benchmark-dataset_fig4_337133502)',\n",
       "  'paper': {'title': 'MOT16: A Benchmark for Multi-Object Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/mot16-a-benchmark-for-multi-object-tracking'},\n",
       "  'introduced_date': '2016-03-02',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Online Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/online-multi-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MOT17'],\n",
       "  'num_papers': 112,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmtracking/blob/master/docs/dataset.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmtracking',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mot20',\n",
       "  'name': 'MOT20',\n",
       "  'full_name': 'MOT20',\n",
       "  'homepage': 'https://motchallenge.net/data/MOT20/',\n",
       "  'description': '**MOT20** is a dataset for multiple object tracking. The dataset contains 8 challenging video sequences (4 train, 4 test) in unconstrained environments, from crowded places such as train stations, town squares and a sports stadium.\\nImage Source: [https://motchallenge.net/vis/MOT20-04](https://motchallenge.net/vis/MOT20-04)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MOT20'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmtracking/blob/master/docs/dataset.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmtracking',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/semaine',\n",
       "  'name': 'SEMAINE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ibug.doc.ic.ac.uk/resources/semaine-database2/',\n",
       "  'description': 'The **SEMAINE** videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips.\\r\\n\\r\\nSource: [ROBUST MODELING OF EPISTEMIC MENTAL STATES](https://arxiv.org/abs/2005.13982)',\n",
       "  'paper': {'title': 'The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent',\n",
       "   'url': 'https://doi.org/10.1109/T-AFFC.2011.20'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D', 'Audio'],\n",
       "  'tasks': [{'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SEMAINE'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/room-to-room',\n",
       "  'name': 'R2R',\n",
       "  'full_name': 'Room-to-Room',\n",
       "  'homepage': 'https://bringmeaspoon.org/',\n",
       "  'description': 'R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI.\\r\\n\\r\\nSource: [Natural language interaction with robots](https://bringmeaspoon.org/)',\n",
       "  'paper': {'title': 'Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments',\n",
       "   'url': 'https://paperswithcode.com/paper/vision-and-language-navigation-interpreting'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts', 'Interactive'],\n",
       "  'tasks': [{'task': 'Visual Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/visual-navigation'},\n",
       "   {'task': 'Vision-Language Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/vision-language-navigation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Room2Room', 'R2R'],\n",
       "  'num_papers': 71,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scenenn',\n",
       "  'name': 'SceneNN',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://103.24.77.34/scenenn/home/',\n",
       "  'description': 'SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design.\\r\\nAll scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses.\\r\\n\\r\\nSource: [SceneNN: A Scene Meshes Dataset with aNNotations](http://103.24.77.34/scenenn/home/)\\r\\nImage Source: [http://103.24.77.34/scenenn/home/](http://103.24.77.34/scenenn/home/)',\n",
       "  'paper': {'title': 'SceneNN: A Scene Meshes Dataset with aNNotations',\n",
       "   'url': 'https://doi.org/10.1109/3DV.2016.18'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D', 'RGB-D'],\n",
       "  'tasks': [{'task': '3D Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-instance-segmentation-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SceneNN'],\n",
       "  'num_papers': 33,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/egtea',\n",
       "  'name': 'EGTEA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cbs.ic.gatech.edu/fpv/',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\n\\nProvide:\\n\\n* a high-level explanation of the dataset characteristics\\n* explain motivations and summary of its content\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'},\n",
       "   {'task': 'Egocentric Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/egocentric-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/gap',\n",
       "  'name': 'GAP',\n",
       "  'full_name': 'GAP Benchmark Suite',\n",
       "  'homepage': 'http://gap.cs.berkeley.edu/benchmark.html',\n",
       "  'description': \"**GAP** is a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing.\",\n",
       "  'paper': {'title': 'The GAP Benchmark Suite',\n",
       "   'url': 'https://paperswithcode.com/paper/the-gap-benchmark-suite'},\n",
       "  'introduced_date': '2015-08-14',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GAP'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/gap',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/gap',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/rotated-mnist',\n",
       "  'name': 'Rotated MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\n\\nProvide:\\n\\n* a high-level explanation of the dataset characteristics\\n* explain motivations and summary of its content\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'},\n",
       "   {'task': 'Rotated MNIST',\n",
       "    'url': 'https://paperswithcode.com/task/rotated-mnist'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/biped',\n",
       "  'name': 'BIPED',\n",
       "  'full_name': 'Barcelona Images for Perceptual Edge Detection',\n",
       "  'homepage': 'https://xavysp.github.io/MBIPED/',\n",
       "  'description': '#Details\\r\\nIt contains 250 outdoor images of 1280$\\\\times$720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked several times in order to correct possible mistakes or wrong edges by just one subject. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection—the subset for edge detection). The level of details of the edge level annotations in the BIPED’s images can be appreciated looking at the GT, see Figs above.\\r\\n\\r\\nBIPED dataset has 250 images in high definition. Thoses images are already split up for training and testing. 200 for training and 50 for testing.\\r\\n#Version\\r\\nThe current version is the second one.',\n",
       "  'paper': {'title': 'Dense Extreme Inception Network: Towards a Robust CNN Model for Edge Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/dense-extreme-inception-network-towards-a'},\n",
       "  'introduced_date': '2019-09-04',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BIPED'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': [{'url': 'https://github.com/xavysp/MBIPED',\n",
       "    'repo': 'https://github.com/xavysp/MBIPED',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/stereoset',\n",
       "  'name': 'StereoSet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/moinnadeem/stereoset',\n",
       "  'description': 'A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion.\\r\\n\\r\\nSource: [StereoSet: Measuring stereotypical bias in pretrained language models](/paper/stereoset-measuring-stereotypical-bias-in)',\n",
       "  'paper': {'title': 'StereoSet: Measuring stereotypical bias in pretrained language models',\n",
       "   'url': 'https://paperswithcode.com/paper/stereoset-measuring-stereotypical-bias-in'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Bias Detection',\n",
       "    'url': 'https://paperswithcode.com/task/bias-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['StereoSet'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/stereoset',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/moinnadeem/stereoset',\n",
       "    'repo': 'https://github.com/moinnadeem/StereoSet',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mit-states',\n",
       "  'name': 'MIT-States',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://web.mit.edu/phillipi/Public/states_and_transformations/index.html',\n",
       "  'description': 'The **MIT-States** dataset has 245 object classes, 115 attribute classes and ∼53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords.\\r\\n\\r\\nSource: [Attributes as Operators: Factorizing Unseen Attribute-Object Compositions](https://arxiv.org/abs/1803.09851)\\r\\nImage Source: [http://web.mit.edu/phillipi/Public/states_and_transformations/index.html](http://web.mit.edu/phillipi/Public/states_and_transformations/index.html)',\n",
       "  'paper': {'title': 'Discovering States and Transformations in Image Collections',\n",
       "   'url': 'https://paperswithcode.com/paper/discovering-states-and-transformations-in'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Retrieval with Multi-Modal Query',\n",
       "    'url': 'https://paperswithcode.com/task/multi-modal'},\n",
       "   {'task': 'Compositional Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/compositional-zero-shot-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MIT-States', 'MIT-States, generalized split'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/caltech-256',\n",
       "  'name': 'Caltech-256',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.vision.caltech.edu/Image_Datasets/Caltech256/',\n",
       "  'description': '**Caltech-256** is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset.\\r\\n\\r\\nSource: [Exploiting Non-Linear Redundancy for Neural Model Compression](https://arxiv.org/abs/2005.14070)\\r\\n\\r\\nImage Source: [ML4A](https://twitter.com/ml4a_/status/934796379171512322)',\n",
       "  'paper': {'title': 'Caltech-256 object category dataset',\n",
       "   'url': 'http://authors.library.caltech.edu/7694'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Semi-Supervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Caltech-256, 1024 Labels',\n",
       "   'Caltech-256',\n",
       "   'Caltech-256 5-way (1-shot)'],\n",
       "  'num_papers': 299,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scde',\n",
       "  'name': 'SCDE',\n",
       "  'full_name': 'SCDE',\n",
       "  'homepage': 'https://vgtomahawk.github.io/sced.html',\n",
       "  'description': '**SCDE** is a human-created sentence cloze dataset, collected from public school English examinations in China. The task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers.\\r\\n\\r\\nSource: [SCDE](https://vgtomahawk.github.io/sced.html)',\n",
       "  'paper': {'title': 'SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations',\n",
       "   'url': 'https://paperswithcode.com/paper/scde-sentence-cloze-dataset-with-high-quality'},\n",
       "  'introduced_date': '2020-04-27',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SCDE'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vatex',\n",
       "  'name': 'VATEX',\n",
       "  'full_name': 'Video And TEXt',\n",
       "  'homepage': 'https://eric-xw.github.io/vatex-website/index.html',\n",
       "  'description': '**VATEX** is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)\\r\\nImage Source: [https://arxiv.org/pdf/1904.03493.pdf](https://arxiv.org/pdf/1904.03493.pdf)',\n",
       "  'paper': {'title': 'VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research',\n",
       "   'url': 'https://paperswithcode.com/paper/vatex-a-large-scale-high-quality-multilingual'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['VATEX'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/viggo',\n",
       "  'name': 'ViGGO',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://nlds.soe.ucsc.edu/viggo',\n",
       "  'description': 'The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts.\\r\\n\\r\\nSource: [VIGGO](https://nlds.soe.ucsc.edu/viggo)',\n",
       "  'paper': {'title': 'ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation',\n",
       "   'url': 'https://paperswithcode.com/paper/viggo-a-video-game-corpus-for-data-to-text'},\n",
       "  'introduced_date': '2019-10-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ViGGO'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/istd',\n",
       "  'name': 'ISTD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/DeepInsight-PCALab/ST-CGAN',\n",
       "  'description': 'The Image Shadow Triplets dataset (**ISTD**) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image.\\r\\n\\r\\nSource: [ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal](https://arxiv.org/abs/1908.01323)\\r\\nImage Source: [Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal](https://paperswithcode.com/paper/stacked-conditional-generative-adversarial/)',\n",
       "  'paper': {'title': 'Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal',\n",
       "   'url': 'https://paperswithcode.com/paper/stacked-conditional-generative-adversarial'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Shadow Removal',\n",
       "    'url': 'https://paperswithcode.com/task/shadow-removal'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ISTD'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://github.com/DeepInsight-PCALab/ST-CGAN',\n",
       "    'repo': 'https://github.com/DeepInsight-PCALab/ST-CGAN',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lcqmc',\n",
       "  'name': 'LCQMC',\n",
       "  'full_name': 'Large-scale Chinese Question Matching Corpus',\n",
       "  'homepage': 'http://icrc.hitsz.edu.cn/info/1037/1146.htm',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\r\\n\\r\\nProvide:\\r\\n\\r\\n* a high-level explanation of the dataset characteristics\\r\\n* explain motivations and summary of its content\\r\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2018-08-20',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Chinese Sentence Pair Classification',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-sentence-pair-classification'},\n",
       "   {'task': 'Question Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/question-similarity'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': [],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ciao',\n",
       "  'name': 'Ciao',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm',\n",
       "  'description': 'The **Ciao** dataset contains rating information of users given to items, and also contain item category information. The data comes from the Epinions dataset.\\r\\n\\r\\nSource: [Collaborative Translational Metric Learning](https://arxiv.org/abs/1906.01637)',\n",
       "  'paper': {'title': 'mTrust: discerning multi-faceted trust in a connected world',\n",
       "   'url': 'https://doi.org/10.1145/2124295.2124309'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Ciao'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/thucnews',\n",
       "  'name': 'THUCNews',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\r\\n\\r\\nProvide:\\r\\n\\r\\n* a high-level explanation of the dataset characteristics\\r\\n* explain motivations and summary of its content\\r\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Chinese Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-document-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sick',\n",
       "  'name': 'SICK',\n",
       "  'full_name': 'Sentences Involving Compositional Knowledge',\n",
       "  'homepage': 'http://marcobaroni.org/composes/sick.html',\n",
       "  'description': 'The **Sentences Involving Compositional Knowledge** (**SICK**) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm.\\r\\n\\r\\nSource: [Multi-Label Transfer Learning for Multi-Relational Semantic Similarity](https://arxiv.org/abs/1805.12501)\\r\\nImage Source: [https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619](https://www.researchgate.net/figure/Example-of-SICK-dataset-sentence-expansion-process-14_fig1_344863619)',\n",
       "  'paper': {'title': 'A SICK cure for the evaluation of compositional distributional semantic models',\n",
       "   'url': 'https://paperswithcode.com/paper/a-sick-cure-for-the-evaluation-of'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'},\n",
       "   {'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Semantic Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-similarity'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SICK'],\n",
       "  'num_papers': 165,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/sick',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fb15k',\n",
       "  'name': 'FB15k',\n",
       "  'full_name': 'Freebase 15K',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/download/details.aspx?id=52312',\n",
       "  'description': 'The **FB15k** dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set.\\r\\n\\r\\nSource: [https://www.microsoft.com/en-us/download/details.aspx?id=52312](https://www.microsoft.com/en-us/download/details.aspx?id=52312)\\r\\nImage Source: [http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf](http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf)',\n",
       "  'paper': {'title': 'Translating Embeddings for Modeling Multi-relational Data',\n",
       "   'url': 'https://paperswithcode.com/paper/translating-embeddings-for-modeling-multi'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Knowledge Graphs',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graphs'},\n",
       "   {'task': 'Knowledge Graph Completion',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graph-completion'},\n",
       "   {'task': 'Knowledge Graph Embedding',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graph-embedding'}],\n",
       "  'languages': [],\n",
       "  'variants': [' FB15k', 'FB15k-237', 'FB15k', 'FB15k (filtered)'],\n",
       "  'num_papers': 359,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.FB15k237Dataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cjrc',\n",
       "  'name': 'CJRC',\n",
       "  'full_name': 'Chinese judicial reading comprehension',\n",
       "  'homepage': 'https://github.com/china-ai-law-challenge/CAIL2019',\n",
       "  'description': 'The Chinese judicial reading comprehension (CJRC) dataset contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. \\r\\n\\r\\nSource: [CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension](/paper/cjrc-a-reliable-human-annotated-benchmark)',\n",
       "  'paper': {'title': 'CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension',\n",
       "   'url': 'https://paperswithcode.com/paper/cjrc-a-reliable-human-annotated-benchmark'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Chinese Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-reading-comprehension'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['CJRC Dev', 'CJRC'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://github.com/china-ai-law-challenge/CAIL2019',\n",
       "    'repo': 'https://github.com/china-ai-law-challenge/CAIL2019',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/hyperlex',\n",
       "  'name': 'HyperLex',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://people.ds.cam.ac.uk/iv250/hyperlex.html',\n",
       "  'description': 'A dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. \\r\\n\\r\\nSource: [HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment](/paper/hyperlex-a-large-scale-evaluation-of-graded)',\n",
       "  'paper': {'title': 'HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment',\n",
       "   'url': 'https://paperswithcode.com/paper/hyperlex-a-large-scale-evaluation-of-graded'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Lexical Entailment',\n",
       "    'url': 'https://paperswithcode.com/task/lexical-entailment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HyperLex'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dblp',\n",
       "  'name': 'DBLP',\n",
       "  'full_name': 'Citation Network Dataset',\n",
       "  'homepage': 'https://www.aminer.org/citation',\n",
       "  'description': 'The **DBLP** is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title.\\r\\nThe data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc.\\r\\n\\r\\nSource: [https://www.aminer.org/citation](https://www.aminer.org/citation)',\n",
       "  'paper': {'title': 'ArnetMiner: extraction and mining of academic social networks',\n",
       "   'url': 'https://doi.org/10.1145/1401890.1402008'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'},\n",
       "   {'task': 'Node Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/node-clustering'},\n",
       "   {'task': 'Heterogeneous Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/heterogeneous-node-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DBLP (PACT) 14k', 'DBLP'],\n",
       "  'num_papers': 128,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/acm',\n",
       "  'name': 'ACM',\n",
       "  'full_name': 'Association for Computing Machinery\\nActive Contour Model\\nalgebraic collective model\\nand-Compare Module\\nActive Contour Models',\n",
       "  'homepage': 'https://github.com/Jhy1993/HAN',\n",
       "  'description': 'The **ACM** dataset contains papers published in KDD, SIGMOD, SIGCOMM, MobiCOMM, and VLDB and are divided into three classes (Database, Wireless Communication, Data Mining). An heterogeneous graph is constructed, which comprises 3025 papers, 5835 authors, and 56 subjects. Paper features correspond to elements of a bag-of-words represented of keywords.\\n\\nSource: [https://arxiv.org/pdf/1903.07293.pdf](https://arxiv.org/pdf/1903.07293.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ACM'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': [{'url': 'https://github.com/Jhy1993/HAN',\n",
       "    'repo': 'https://github.com/Jhy1993/HAN',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fnc-1',\n",
       "  'name': 'FNC-1',\n",
       "  'full_name': 'Fake News Challenge Stage 1',\n",
       "  'homepage': 'http://www.fakenewschallenge.org/',\n",
       "  'description': '**FNC-1** was designed as a stance detection dataset and it contains 75,385 labeled headline and article pairs. The pairs are labelled as either agree, disagree, discuss, and unrelated. Each headline in the dataset is phrased as a statement\\r\\n\\r\\nSource: [Investigating Rumor News Using Agreement-Aware Search](https://arxiv.org/abs/1802.07398)\\nImage Source: [http://www.fakenewschallenge.org/](http://www.fakenewschallenge.org/)',\n",
       "  'paper': {'title': 'The fake news challenge: Exploring how artificial intelligence technologies could be leveraged to combat fake news',\n",
       "   'url': 'http://www.fakenewschallenge.org/'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Fake News Detection',\n",
       "    'url': 'https://paperswithcode.com/task/fake-news-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FNC-1'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/gyafc',\n",
       "  'name': 'GYAFC',\n",
       "  'full_name': 'Grammarly’s Yahoo Answers Formality Corpus',\n",
       "  'homepage': 'https://github.com/raosudha89/GYAFC-corpus',\n",
       "  'description': 'Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs.\\r\\n\\r\\nYahoo Answers is a question answering forum, contains a large number of informal sentences and allows redistribution of data. The authors used the Yahoo Answers L6 corpus to create the GYAFC dataset of informal and formal sentence pairs. In order to ensure a uniform distribution of data, they removed sentences that are questions, contain URLs, and are shorter than 5 words or longer than 25. After these preprocessing steps, 40 million sentences remain. \\r\\n\\r\\nThe Yahoo Answers corpus consists of several different domains like Business, Entertainment & Music, Travel, Food, etc. Pavlick and Tetreault formality classifier (PT16) shows that the formality level varies significantly\\r\\nacross different genres. In order to control for this variation, the authors work with two specific domains that contain the most informal sentences and show results on training and testing within those categories. The authors use the formality classifier from PT16 to identify informal sentences and train this classifier on the Answers genre of the PT16 corpus\\r\\nwhich consists of nearly 5,000 randomly selected sentences from Yahoo Answers manually annotated on a scale of -3 (very informal) to 3 (very formal). They find that the domains of Entertainment & Music and Family & Relationships contain the most informal sentences and create the GYAFC dataset using these domains.\\r\\n\\r\\nSource: [Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer](https://arxiv.org/pdf/1803.06535v2.pdf)',\n",
       "  'paper': {'title': 'Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer',\n",
       "   'url': 'https://paperswithcode.com/paper/dear-sir-or-madam-may-i-introduce-the-gyafc'},\n",
       "  'introduced_date': '2018-03-17',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Unsupervised Text Style Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-text-style-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GYAFC'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': [{'url': 'https://github.com/raosudha89/GYAFC-corpus',\n",
       "    'repo': 'https://github.com/raosudha89/GYAFC-corpus',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/aids',\n",
       "  'name': 'AIDS',\n",
       "  'full_name': 'AIDS',\n",
       "  'homepage': 'http://networkrepository.com/AIDS.php',\n",
       "  'description': '**AIDS** is a graph dataset. It consists of 2000 graphs representing molecular compounds which are constructed from the AIDS Antiviral Screen Database of Active Compounds. It contains 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI.\\r\\n\\r\\nSource: [DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model](https://arxiv.org/abs/1712.03563)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222](https://www.researchgate.net/figure/Sample-component-in-AIDS-kernel-dataset-with-Graphwave-based-structural-role-colors-Here_fig1_338282222)',\n",
       "  'paper': {'title': 'IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning',\n",
       "   'url': 'https://doi.org/10.1007/978-3-540-89689-0_33'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AIDS'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sydney-urban-objects',\n",
       "  'name': 'Sydney Urban Objects',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml',\n",
       "  'description': 'This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees.\\n\\nIt was collected in order to test matching and classification algorithms. It aims to provide non-ideal sensing conditions that are representative of practical urban sensing systems, with a large variability in viewpoint and occlusion.\\n\\nSource: [http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)\\nImage Source: [http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml](http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud', 'LiDAR'],\n",
       "  'tasks': [{'task': '3D Point Cloud Classification',\n",
       "    'url': 'https://paperswithcode.com/task/3d-point-cloud-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Sydney Urban Objects'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/digits',\n",
       "  'name': 'Digits',\n",
       "  'full_name': 'Optical Recognition of Handwritten Digits',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits',\n",
       "  'description': 'The DIGITS dataset consists of 1797 8×8 grayscale images (1439 for training and 360 for testing) of handwritten digits.\\r\\n\\r\\nSource: [Differentially Private Variational Dropout](https://arxiv.org/abs/1712.02629)\\r\\nImage Source: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)',\n",
       "  'paper': {'title': 'Optical recognition of handwritten digits data set',\n",
       "   'url': 'https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Feature Importance',\n",
       "    'url': 'https://paperswithcode.com/task/feature-importance'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Optical Recognition of Handwritten Digits', 'Digits'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/optical-handwritten-digits-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mutagenicity',\n",
       "  'name': 'Mutagenicity',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**Mutagenicity** is a chemical compound dataset of drugs, which can be categorized into two classes: mutagen and non-mutagen.\\n\\nSource: [Hierarchical Graph Pooling with Structure Learning](https://arxiv.org/abs/1911.05954)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Mutagenicity'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sider',\n",
       "  'name': 'SIDER',\n",
       "  'full_name': 'SIDER',\n",
       "  'homepage': 'http://sideeffects.embl.de/',\n",
       "  'description': '**SIDER** contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug–target relations.\\n\\nSource: [Side Effect Resource](http://sideeffects.embl.de/)\\nImage Source: [http://sideeffects.embl.de/drugs/2756/](http://sideeffects.embl.de/drugs/2756/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Drug Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/drug-discovery'},\n",
       "   {'task': 'Molecular Property Prediction (1-shot))',\n",
       "    'url': 'https://paperswithcode.com/task/molecular-property-prediction-1-shot'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SIDER'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rcv1',\n",
       "  'name': 'RCV1',\n",
       "  'full_name': 'Reuters Corpus Volume 1',\n",
       "  'homepage': 'http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm',\n",
       "  'description': 'The **RCV1** dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions.\\r\\n\\r\\nSource: [Random Projections for Linear Support Vector Machines](https://arxiv.org/abs/1211.6085)\\r\\nImage Source: [https://www.nasdaq.com/publishers/reuters](https://www.nasdaq.com/publishers/reuters)',\n",
       "  'paper': {'title': 'RCV1: A New Benchmark Collection for Text Categorization Research',\n",
       "   'url': 'http://jmlr.org/papers/volume5/lewis04a/lewis04a.pdf'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Multi-Label Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-text-classification'},\n",
       "   {'task': 'Cross-Lingual Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-document-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RCV1-v2',\n",
       "   'Reuters RCV1/RCV2 English-to-German',\n",
       "   'Reuters RCV1/RCV2 German-to-English',\n",
       "   'RCV1'],\n",
       "  'num_papers': 272,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/crosstask',\n",
       "  'name': 'CrossTask',\n",
       "  'full_name': 'CrossTask',\n",
       "  'homepage': 'https://github.com/DmZhukov/CrossTask',\n",
       "  'description': \"**CrossTask** dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations.\\r\\n\\r\\nSource: [CrossTask](https://github.com/DmZhukov/CrossTask)\\r\\nImage Source: [https://arxiv.org/pdf/1903.08225v2.pdf](https://arxiv.org/pdf/1903.08225v2.pdf)\",\n",
       "  'paper': {'title': 'Cross-task weakly supervised learning from instructional videos',\n",
       "   'url': 'https://paperswithcode.com/paper/cross-task-weakly-supervised-learning-from'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CrossTask'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': [{'url': 'https://github.com/DmZhukov/CrossTask',\n",
       "    'repo': 'https://github.com/DmZhukov/CrossTask',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/youcook2',\n",
       "  'name': 'YouCook2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://youcook2.eecs.umich.edu/',\n",
       "  'description': '**YouCook2** is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world.\\r\\n\\r\\nSource: [http://youcook2.eecs.umich.edu/](http://youcook2.eecs.umich.edu/)\\r\\nImage Source: [https://competitions.codalab.org/competitions/20594](https://competitions.codalab.org/competitions/20594)',\n",
       "  'paper': {'title': 'Towards Automatic Learning of Procedures from Web Instructional Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/towards-automatic-learning-of-procedures-from'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'},\n",
       "   {'task': 'Dense Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/dense-video-captioning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['YouCook2'],\n",
       "  'num_papers': 68,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/faceforensics',\n",
       "  'name': 'FaceForensics',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://niessnerlab.org/projects/roessler2018faceforensics.html',\n",
       "  'description': 'FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:\\r\\n\\r\\n* Source-to-Target: where the authors reenact over 1000 videos with new facial expressions extracted from other videos, which e.g. can be used to train a classifier to detect fake images or videos.\\r\\n\\r\\n* Selfreenactment: where the authors use Face2Face to reenact the facial expressions of videos with their own facial expressions as input to get pairs of videos, which e.g. can be used to train supervised generative refinement models.',\n",
       "  'paper': {'title': 'FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces',\n",
       "   'url': 'https://paperswithcode.com/paper/faceforensics-a-large-scale-video-dataset-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'DeepFake Detection',\n",
       "    'url': 'https://paperswithcode.com/task/deepfake-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FaceForensics'],\n",
       "  'num_papers': 50,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stacked-mnist',\n",
       "  'name': 'Stacked MNIST',\n",
       "  'full_name': 'Stacked MNIST',\n",
       "  'homepage': '',\n",
       "  'description': 'The **Stacked MNIST** dataset is derived from the standard MNIST dataset with an increased number of discrete modes. 240,000 RGB images in the size of 32×32 are synthesized by stacking three random digit images from MNIST along the color channel, resulting in 1,000 explicit modes in a uniform distribution corresponding to the number of possible triples of digits.\\r\\n\\r\\nSource: [Inclusive GAN: Improving Data and Minority Coverage in Generative Models](https://arxiv.org/abs/2004.03355)\\r\\nImage Source: [https://arxiv.org/abs/1705.07761](https://arxiv.org/abs/1705.07761)',\n",
       "  'paper': {'title': 'Unrolled Generative Adversarial Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/unrolled-generative-adversarial-networks'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stacked MNIST'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/carpk',\n",
       "  'name': 'CARPK',\n",
       "  'full_name': 'car parking lot dataset',\n",
       "  'homepage': 'https://lafi.github.io/LPN/',\n",
       "  'description': 'The Car Parking Lot Dataset (**CARPK**) contains nearly 90,000 cars from 4 different parking lots collected by means of drone (PHANTOM 3 PROFESSIONAL). The images are collected with the drone-view at approximate 40 meters height. The image set is annotated by bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottom-right points. It is supporting object counting, object localizing, and further investigations with the annotation format in bounding boxes.\\r\\n\\r\\nSource: [https://lafi.github.io/LPN/](https://lafi.github.io/LPN/)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610](https://www.researchgate.net/figure/Sample-results-on-the-CARPK-dataset-Top-row-original-images-Bottom-row-predicted_fig4_328685610)',\n",
       "  'paper': {'title': 'Drone-based Object Counting by Spatially Regularized Regional Proposal Network',\n",
       "   'url': 'https://paperswithcode.com/paper/drone-based-object-counting-by-spatially'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Counting',\n",
       "    'url': 'https://paperswithcode.com/task/object-counting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CARPK'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pix3d',\n",
       "  'name': 'Pix3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://pix3d.csail.mit.edu/',\n",
       "  'description': 'The **Pix3D** dataset is a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc.\\r\\n\\r\\nSource: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)\\r\\nImage Source: [http://pix3d.csail.mit.edu/](http://pix3d.csail.mit.edu/)',\n",
       "  'paper': {'title': 'Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling',\n",
       "   'url': 'https://paperswithcode.com/paper/pix3d-dataset-and-methods-for-single-image-3d'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '3D Shape Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-shape-reconstruction'},\n",
       "   {'task': '3D Shape Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/3d-shape-modeling'},\n",
       "   {'task': '3D Shape Classification',\n",
       "    'url': 'https://paperswithcode.com/task/3d-shape-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Pix3D S2', 'Pix3D S1', 'Pix3D'],\n",
       "  'num_papers': 80,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cell',\n",
       "  'name': 'Cell',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/AltschulerWu-Lab/MuLANN',\n",
       "  'description': 'The CELL benchmark is made of fluorescence microscopy images of cell. \\r\\n\\r\\nSource: [Multi-Domain Adversarial Learning](/paper/multi-domain-adversarial-learning-1)\\r\\n\\r\\nImage Source: [https://arxiv.org/pdf/1903.09239v1.pdf](https://arxiv.org/pdf/1903.09239v1.pdf)',\n",
       "  'paper': {'title': 'Multi-Domain Adversarial Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-domain-adversarial-learning-1'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Nuclear Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/nuclear-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cell17', 'Cell', 'CellNet'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': [{'url': 'https://github.com/AltschulerWu-Lab/MuLANN',\n",
       "    'repo': 'https://github.com/AltschulerWu-Lab/MuLANN',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fbms',\n",
       "  'name': 'FBMS',\n",
       "  'full_name': 'Freiburg-Berkeley Motion Segmentation',\n",
       "  'homepage': 'https://lmb.informatik.uni-freiburg.de/resources/datasets/',\n",
       "  'description': 'The **Freiburg-Berkeley Motion Segmentation** Dataset (**FBMS**-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set.\\r\\n\\r\\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)\\r\\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/](https://lmb.informatik.uni-freiburg.de/resources/datasets/)',\n",
       "  'paper': {'title': 'Segmentation of Moving Objects by Long Term Video Analysis',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2013.242'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-segmentation'},\n",
       "   {'task': 'Video Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-salient-object-detection'},\n",
       "   {'task': 'Unsupervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FBMS-59', 'FBMS'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nvgesture-1',\n",
       "  'name': 'NVGesture',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://research.nvidia.com/publication/online-detection-and-classification-dynamic-hand-gestures-recurrent-3d-convolutional',\n",
       "  'description': 'The **NVGesture** dataset focuses on touchless driver controlling. It contains 1532 dynamic gestures fallen into 25 classes. It includes 1050 samples for training and 482 for testing. The videos are recorded with three modalities (RGB, depth, and infrared).\\n\\nSource: [Searching Multi-Rate and Multi-Modal Temporal Enhanced Networks for Gesture Recognition](https://arxiv.org/abs/2008.09412)\\nImage Source: [Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network](https://paperswithcode.com/paper/online-detection-and-classification-of/)',\n",
       "  'paper': {'title': 'Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network',\n",
       "   'url': 'https://paperswithcode.com/paper/online-detection-and-classification-of'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Hand Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': [' NVGesture', 'NVGesture'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun09',\n",
       "  'name': 'SUN09',\n",
       "  'full_name': 'SUN09',\n",
       "  'homepage': 'http://people.csail.mit.edu/myungjin/HContext.html',\n",
       "  'description': 'The **SUN09** dataset consists of 12,000 annotated images with more than 200 object categories. It consists of natural, indoor and outdoor images. Each image contains an average of 7 different annotated objects and the average occupancy of each object is 5% of image size. The frequencies of object categories follow a power law distribution.\\r\\n\\r\\nSource: [A Pooling Approach to Modelling Spatial Relations forImage Retrieval and Annotation](https://arxiv.org/abs/1411.5190)\\r\\nImage Source: [http://people.csail.mit.edu/myungjin/HContext.html](http://people.csail.mit.edu/myungjin/HContext.html)',\n",
       "  'paper': {'title': 'Exploiting hierarchical context on a large database of object categories',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2010.5540221'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Left Ventricle Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/left-ventricle-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN09'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/coin',\n",
       "  'name': 'COIN',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://coin-dataset.github.io/',\n",
       "  'description': 'The **COIN** dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments.\\r\\n\\r\\nSource: [COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis](/paper/coin-a-large-scale-dataset-for-comprehensive)',\n",
       "  'paper': {'title': 'COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/coin-a-large-scale-dataset-for-comprehensive'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-localization'},\n",
       "   {'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COIN'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kinetics-600',\n",
       "  'name': 'Kinetics-600',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://deepmind.com/research/open-source/kinetics',\n",
       "  'description': 'The **Kinetics-600** is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset.\\r\\n\\r\\nSource: [Learning to Localize Actions from Moments](https://arxiv.org/abs/2008.13705)\\r\\nImage Source: [https://towardsdatascience.com/downloading-the-kinetics-dataset-for-human-action-recognition-in-deep-learning-500c3d50f776](https://towardsdatascience.com/downloading-the-kinetics-dataset-for-human-action-recognition-in-deep-learning-500c3d50f776)',\n",
       "  'paper': {'title': 'A Short Note about Kinetics-600',\n",
       "   'url': 'https://paperswithcode.com/paper/a-short-note-about-kinetics-600'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'},\n",
       "   {'task': 'Video Generation',\n",
       "    'url': 'https://paperswithcode.com/task/video-generation'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Self-Supervised Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Kinetics-600 12 frames, 64x64',\n",
       "   'Kinetics-600 48 frames, 64x64',\n",
       "   'Kinetics-600 12 frames, 128x128',\n",
       "   'Kinetics-600'],\n",
       "  'num_papers': 64,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/audioset',\n",
       "  'name': 'AudioSet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://research.google.com/audioset/index.html',\n",
       "  'description': 'Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.\\r\\n\\r\\nSource: [Curriculum Audiovisual Learning](https://arxiv.org/abs/2001.09414)',\n",
       "  'paper': {'title': 'Audio Set: An ontology and human-labeled dataset for audio events',\n",
       "   'url': 'https://doi.org/10.1109/ICASSP.2017.7952261'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Audio Classification',\n",
       "    'url': 'https://paperswithcode.com/task/audio-classification'},\n",
       "   {'task': 'Audio Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/audio-source-separation'},\n",
       "   {'task': 'Audio Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/audio-tagging'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AudioSet'],\n",
       "  'num_papers': 282,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/diva-hisdb',\n",
       "  'name': 'DIVA-HisDB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://diuf.unifr.ch/main/hisdoc/diva-hisdb',\n",
       "  'description': 'The database consists of 150 annotated pages of three different medieval manuscripts with challenging layouts. Furthermore, we provide a layout analysis ground-truth which has been iterated on, reviewed, and refined by an expert in medieval studies.',\n",
       "  'paper': {'title': 'DIVA-HisDB: A Precisely Annotated Large Dataset of Challenging Medieval Manuscripts',\n",
       "   'url': 'https://paperswithcode.com/paper/diva-hisdb-a-precisely-annotated-large'},\n",
       "  'introduced_date': '2016-10-23',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Document Layout Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/document-layout-analysis'},\n",
       "   {'task': 'Text-Line Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/text-line-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DIVA-HisDB'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tdiuc',\n",
       "  'name': 'TDIUC',\n",
       "  'full_name': 'Task Directed Image Understanding Challenge',\n",
       "  'homepage': 'https://kushalkafle.com/projects/tdiuc.html',\n",
       "  'description': '**Task Directed Image Understanding Challenge** (**TDIUC**) dataset is a Visual Question Answering dataset which consists of 1.6M questions and 170K images sourced from MS COCO and the Visual Genome Dataset. The image-question pairs are split into 12 categories and 4 additional evaluation matrices which help evaluate models’ robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. The TDIUC dataset divides the VQA paradigm into 12 different task directed question types. These include questions that require a simpler task (e.g., object presence, color attribute) and more complex tasks (e.g., counting, positional reasoning). The dataset includes also an “Absurd” question category in which questions are irrelevant to the image contents to help balance the dataset.\\r\\n\\r\\nSource: [Question-Agnostic Attention for Visual Question Answering](https://arxiv.org/abs/1908.03289)\\r\\nImage Source: [https://kushalkafle.com/projects/tdiuc.html](https://kushalkafle.com/projects/tdiuc.html)',\n",
       "  'paper': {'title': 'An Analysis of Visual Question Answering Algorithms',\n",
       "   'url': 'https://paperswithcode.com/paper/an-analysis-of-visual-question-answering'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TDIUC'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mall',\n",
       "  'name': 'Mall',\n",
       "  'full_name': 'Mall Dataset',\n",
       "  'homepage': 'http://vision.cs.tut.fi/personal/kechen/codedata.html',\n",
       "  'description': 'The **Mall** is a dataset for crowd counting and profiling research. Its images are collected from publicly accessible webcam. It mainly includes 2,000 video frames, and the head position of every pedestrian in all frames is annotated. A total of more than 60,000 pedestrians are annotated in this dataset.\\r\\n\\r\\nSource: [Drone Based RGBT Vehicle Detection and Counting: A Challenge](https://arxiv.org/abs/2003.02437)\\r\\nImage Source: [http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf](http://www.bmva.org/bmvc/2012/BMVC/paper021/paper021.pdf)',\n",
       "  'paper': {'title': 'Feature Mining for Localised Crowd Counting',\n",
       "   'url': 'https://doi.org/10.5244/C.26.21'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Mall'],\n",
       "  'num_papers': 52,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/a3d',\n",
       "  'name': 'A3D',\n",
       "  'full_name': 'AnAn Accident Detection',\n",
       "  'homepage': 'https://github.com/MoonBlvd/tad-IROS2019',\n",
       "  'description': 'A new dataset of diverse traffic accidents.\\r\\n\\r\\nSource: [Unsupervised Traffic Accident Detection in First-Person Videos](/paper/unsupervised-traffic-accident-detection-in)',\n",
       "  'paper': {'title': 'Unsupervised Traffic Accident Detection in First-Person Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/unsupervised-traffic-accident-detection-in'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Traffic Accident Detection',\n",
       "    'url': 'https://paperswithcode.com/task/traffic-accident-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['A3D'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': [{'url': 'https://github.com/MoonBlvd/tad-IROS2019',\n",
       "    'repo': 'https://github.com/MoonBlvd/tad-IROS2019',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/frgc',\n",
       "  'name': 'FRGC',\n",
       "  'full_name': 'Face Recognition Grand Challenge',\n",
       "  'homepage': 'https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc',\n",
       "  'description': \"The data for **FRGC** consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor.\\r\\n\\r\\nSource: [https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc](https://www.nist.gov/programs-projects/face-recognition-grand-challenge-frgc)\\r\\nImage Source: [https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105](https://www.researchgate.net/figure/Example-of-images-in-FRGC-20-dataset-The-dataset-consist-of-controlled-images-a-c-as_fig10_285759105)\",\n",
       "  'paper': {'title': 'Overview of the Face Recognition Grand Challenge',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2005.268'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FRGC'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/har',\n",
       "  'name': 'HAR',\n",
       "  'full_name': 'Human Activity Recognition Using Smartphones',\n",
       "  'homepage': 'http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones',\n",
       "  'description': 'The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects.\\r\\n\\r\\nSource: [http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones](http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones)\\r\\nImage Source: [https://www.youtube.com/watch?v=XOEN9W05_4A](https://www.youtube.com/watch?v=XOEN9W05_4A)',\n",
       "  'paper': {'title': 'A Public Domain Dataset for Human Activity Recognition using Smartphones',\n",
       "   'url': 'http://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-84.pdf'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Time series'],\n",
       "  'tasks': [{'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Recognizing And Localizing Human Actions',\n",
       "    'url': 'https://paperswithcode.com/task/recognizing-and-localizing-human-actions'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HAR'],\n",
       "  'num_papers': 135,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mot15',\n",
       "  'name': 'MOT15',\n",
       "  'full_name': 'Multiple Object Tracking 15',\n",
       "  'homepage': 'https://motchallenge.net/results/2D_MOT_2015/',\n",
       "  'description': 'MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector.\\r\\n\\r\\nSource: [FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking](https://arxiv.org/abs/1904.04989)\\r\\nImage Source: [https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377](https://www.researchgate.net/figure/Exemplary-qualitative-tracking-results-for-the-MOT15-benchmark-dataset-a-d-are-from-a_fig1_340328377)',\n",
       "  'paper': {'title': 'MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/motchallenge-2015-towards-a-benchmark-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Online Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/online-multi-object-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['2DMOT15', '2D MOT 2015', 'MOT15'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmtracking/blob/master/docs/dataset.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmtracking',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/casia-mfsd',\n",
       "  'name': 'CASIA-MFSD',\n",
       "  'full_name': 'CASIA-MFSD',\n",
       "  'homepage': 'http://biometrics.idealtest.org/findTotalDbByMode.do?mode=Face',\n",
       "  'description': '**CASIA-MFSD** is a dataset for face anti-spoofing. It contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing.\\r\\n\\r\\nSource: [Improving Face Anti-Spoofing by 3D Virtual Synthesis](https://arxiv.org/abs/1901.00488)\\r\\nImage Source: [https://link.springer.com/referenceworkentry/10.1007%2F978-1-4899-7488-4_9067](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4899-7488-4_9067)',\n",
       "  'paper': {'title': 'A face antispoofing database with diverse attacks',\n",
       "   'url': 'https://doi.org/10.1109/ICB.2012.6199754'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Anti-Spoofing',\n",
       "    'url': 'https://paperswithcode.com/task/face-anti-spoofing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CASIA-MFSD'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/replay-attack',\n",
       "  'name': 'Replay-Attack',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.idiap.ch/dataset/replayattack',\n",
       "  'description': 'The **Replay-Attack** Database for face spoofing consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds.\\r\\n\\r\\nSource: [https://www.idiap.ch/dataset/replayattack](https://www.idiap.ch/dataset/replayattack)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-images-from-the-PRINT-ATTACK-1-and-REPLAY-ATTACK-12-databases-Top-and-bottom_fig1_330400888](https://www.researchgate.net/figure/Sample-images-from-the-PRINT-ATTACK-1-and-REPLAY-ATTACK-12-databases-Top-and-bottom_fig1_330400888)',\n",
       "  'paper': {'title': 'On the effectiveness of local binary patterns in face anti-spoofing',\n",
       "   'url': 'https://dl.gi.de/20.500.12116/18295'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Anti-Spoofing',\n",
       "    'url': 'https://paperswithcode.com/task/face-anti-spoofing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Replay-Attack'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': [{'url': 'https://paperswithcode.com/dataset/replay-attack',\n",
       "    'repo': 'https://github.com/SoftwareGift/FeatherNets_Face-Anti-spoofing-Attack-Detection-Challenge-CVPR2019',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/delicious',\n",
       "  'name': 'Delicious',\n",
       "  'full_name': 'Delicious',\n",
       "  'homepage': 'http://mlkd.csd.auth.gr/multilabel.html',\n",
       "  'description': '**Delicious** : This data set contains tagged web pages retrieved from the website delicious.com.\\n\\nSource: [Text segmentation on multilabel documents: A distant-supervised approach](https://arxiv.org/abs/1904.06730)\\nImage Source: [http://mlkd.csd.auth.gr/multilabel.html](http://mlkd.csd.auth.gr/multilabel.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Delicious'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wechat',\n",
       "  'name': 'WeChat',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/yaqingwang/WeFEND-AAAI20',\n",
       "  'description': 'The **WeChat** dataset for fake news detection contains more than 20k news labelled as fake news or not.',\n",
       "  'paper': {'title': 'Weak Supervision for Fake News Detection via Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/weak-supervision-for-fake-news-detection-via'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WeChat'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://github.com/yaqingwang/WeFEND-AAAI20',\n",
       "    'repo': 'https://github.com/yaqingwang/WeFEND-AAAI20',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/raf-db',\n",
       "  'name': 'RAF-DB',\n",
       "  'full_name': 'Real-world Affective Faces',\n",
       "  'homepage': 'http://www.whdeng.cn/raf/model1.html',\n",
       "  'description': \"The **Real-world Affective Faces** Database (**RAF-DB**) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc.\\r\\n\\r\\nSource: [Landmark Guidance Independent Spatio-channel Attention and Complementary Context Information based Facial Expression Recognition](https://arxiv.org/abs/2007.10298)\\r\\nImage Source: [http://www.whdeng.cn/raf/model1.html](http://www.whdeng.cn/raf/model1.html)\",\n",
       "  'paper': {'title': 'Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/reliable-crowdsourcing-and-deep-locality'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Real-World Affective Faces', 'RAF-DB'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ferg',\n",
       "  'name': 'FERG',\n",
       "  'full_name': 'Facial Expression Research Group Database',\n",
       "  'homepage': 'http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html',\n",
       "  'description': '**FERG** is a database of cartoon characters with annotated facial expressions containing 55,769 annotated face images of six characters. The images for each character are grouped into 7 types of cardinal expressions, viz. anger, disgust, fear, joy, neutral, sadness and surprise.\\n\\nSource: [VGAN-Based Image Representation Learningfor Privacy-Preserving Facial Expression Recognition](https://arxiv.org/abs/1803.07100)\\nImage Source: [http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html](http://grail.cs.washington.edu/projects/deepexpr/ferg-2d-db.html)',\n",
       "  'paper': {'title': 'Modeling Stylized Character Expressions via Deep Learning',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-54184-6_9'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Facial Expression Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/facial-expression-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FERG'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/coco-text',\n",
       "  'name': 'COCO-Text',\n",
       "  'full_name': 'COCO-Text',\n",
       "  'homepage': 'https://bgshih.github.io/cocotext/',\n",
       "  'description': 'The **COCO-Text** dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text.\\r\\n\\r\\nSource: [Improving Text Proposals for Scene Images with Fully Convolutional Networks](https://arxiv.org/abs/1702.05089)\\r\\nImage Source: [https://vision.cornell.edu/se3/coco-text-2/](https://vision.cornell.edu/se3/coco-text-2/)',\n",
       "  'paper': {'title': 'COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images',\n",
       "   'url': 'https://paperswithcode.com/paper/coco-text-dataset-and-benchmark-for-text'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['COCO-Text'],\n",
       "  'num_papers': 52,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmocr/blob/main/docs/datasets.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmocr',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/discofuse',\n",
       "  'name': 'DiscoFuse',\n",
       "  'full_name': 'DiscoFuse',\n",
       "  'homepage': 'https://github.com/google-research-datasets/discofuse',\n",
       "  'description': 'DiscoFuse was created by applying a rule-based splitting method on two corpora -\\r\\nsports articles crawled from the Web, and Wikipedia. See the paper for a detailed\\r\\ndescription of the dataset generation process and evaluation.\\r\\n\\r\\nDiscoFuse has two parts with 44,177,443 and 16,642,323 examples sourced from Sports articles and Wikipedia, respectively.\\r\\n\\r\\nFor each part, a random split is provided to train (98% of the examples), development (1%) and test (1%) sets. In addition, as the original data distribution is highly skewed (see details in the paper), a balanced version for each part is also provided.\\r\\n\\r\\nSource: [Google Research](https://github.com/google-research-datasets/discofuse)',\n",
       "  'paper': {'title': 'DiscoFuse: A Large-Scale Dataset for Discourse-Based Sentence Fusion',\n",
       "   'url': 'https://paperswithcode.com/paper/discofuse-a-large-scale-dataset-for-discourse'},\n",
       "  'introduced_date': '2019-02-27',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Sentence Fusion',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-fusion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DiscoFuse'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/discofuse',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/google-research-datasets/discofuse',\n",
       "    'repo': 'https://github.com/google-research-datasets/discofuse',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/figer',\n",
       "  'name': 'FIGER',\n",
       "  'full_name': 'Fine-Grained Entity Recognition',\n",
       "  'homepage': 'http://xiaoling.github.io/figer/',\n",
       "  'description': 'The **FIGER** dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as *person/doctor*, *art/written_work* and *building/hotel*. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually.\\r\\n\\r\\nSource: [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152](http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152)',\n",
       "  'paper': {'title': 'Fine-Grained Entity Recognition',\n",
       "   'url': 'http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/view/5152'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Entity Linking',\n",
       "    'url': 'https://paperswithcode.com/task/entity-linking'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FIGER'],\n",
       "  'num_papers': 81,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cuhk-sysu',\n",
       "  'name': 'CUHK-SYSU',\n",
       "  'full_name': 'CUHK-SYSU Person Search Dataset',\n",
       "  'homepage': 'http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html',\n",
       "  'description': 'The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery.\\r\\n\\r\\nSource: [http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)\\r\\nImage Source: [http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)',\n",
       "  'paper': {'title': 'Joint Detection and Identification Feature Learning for Person Search',\n",
       "   'url': 'https://paperswithcode.com/paper/joint-detection-and-identification-feature'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Person Search',\n",
       "    'url': 'https://paperswithcode.com/task/person-search'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUHK-SYSU'],\n",
       "  'num_papers': 56,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chairs',\n",
       "  'name': 'Chairs',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.di.ens.fr/willow/research/seeing3Dchairs/',\n",
       "  'description': 'The **Chairs** dataset contains rendered images of around 1000 different three-dimensional chair models.\\r\\n\\r\\nSource: [Adversarial Disentanglement with Grouped Observations](https://arxiv.org/abs/2001.04761)\\r\\nImage Source: [https://www.di.ens.fr/willow/research/seeing3Dchairs/](https://www.di.ens.fr/willow/research/seeing3Dchairs/)',\n",
       "  'paper': {'title': 'Seeing 3D Chairs: Exemplar Part-based 2D-3D Alignment using a Large Dataset of CAD Models',\n",
       "   'url': 'https://paperswithcode.com/paper/seeing-3d-chairs-exemplar-part-based-2d-3d'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Sketch-Based Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/sketch-based-image-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Chairs'],\n",
       "  'num_papers': 105,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/zinc',\n",
       "  'name': 'ZINC',\n",
       "  'full_name': 'ZINC',\n",
       "  'homepage': 'http://zinc15.docking.org/',\n",
       "  'description': '**ZINC** is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs.\\r\\n\\r\\nSource: [ZINC15](http://zinc15.docking.org/)\\r\\nImage Source: [https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.5b00559)',\n",
       "  'paper': {'title': 'ZINC: A Free Tool to Discover Chemistry for Biology',\n",
       "   'url': 'https://doi.org/10.1021/ci3001277'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Interactive'],\n",
       "  'tasks': [{'task': 'Graph Regression',\n",
       "    'url': 'https://paperswithcode.com/task/graph-regression'},\n",
       "   {'task': 'Graph Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/graph-ranking'},\n",
       "   {'task': 'Molecular Graph Generation',\n",
       "    'url': 'https://paperswithcode.com/task/molecular-graph-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ZINC', 'ZINC 100k', 'ZINC-500k'],\n",
       "  'num_papers': 106,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/qed',\n",
       "  'name': 'QED',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/google-research-datasets/QED',\n",
       "  'description': '**QED** is a linguistically principled framework for explanations in question answering. Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps:\\nsentence selection := identification of a sentence implying an answer to the question\\nreferential equality := identification of noun phrases in the question and the answer sentence that refer to the same thing\\npredicate entailment := confirmation that the predicate in the sentence entails the predicate in the question once referential equalities are abstracted away.\\nThe QED dataset is an expert-annotated dataset of QED explanations build upon a subset of the Google Natural Questions dataset.\\n\\nSource: [https://github.com/google-research-datasets/QED](https://github.com/google-research-datasets/QED)\\nImage Source: [https://github.com/google-research-datasets/QED](https://github.com/google-research-datasets/QED)',\n",
       "  'paper': {'title': 'QED: A Framework and Dataset for Explanations in Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/qed-a-framework-and-dataset-for-explanations'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Drug Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/drug-discovery'}],\n",
       "  'languages': [],\n",
       "  'variants': ['QED'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/qed',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/google-research-datasets/QED',\n",
       "    'repo': 'https://github.com/google-research-datasets/QED',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mef',\n",
       "  'name': 'MEF',\n",
       "  'full_name': 'Multi-exposure image fusion',\n",
       "  'homepage': '',\n",
       "  'description': 'Multi-exposure image fusion (MEF) is considered\\r\\nan effective quality enhancement technique widely adopted in\\r\\nconsumer electronics, but little work has been dedicated to the\\r\\nperceptual quality assessment of multi-exposure fused images.\\r\\nIn this paper, we first build an MEF database and carry\\r\\nout a subjective user study to evaluate the quality of images\\r\\ngenerated by different MEF algorithms. There are several useful\\r\\nfindings. First, considerable agreement has been observed among\\r\\nhuman subjects on the quality of MEF images. Second, no single\\r\\nstate-of-the-art MEF algorithm produces the best quality for\\r\\nall test images. Third, the existing objective quality models for\\r\\ngeneral image fusion are very limited in predicting perceived\\r\\nquality of MEF images. Motivated by the lack of appropriate\\r\\nobjective models, we propose a novel objective image quality\\r\\nassessment (IQA) algorithm for MEF images based on the\\r\\nprinciple of the structural similarity approach and a novel\\r\\nmeasure of patch structural consistency. Our experimental results\\r\\non the subjective database show that the proposed model well\\r\\ncorrelates with subjective judgments and significantly outperforms the existing IQA models for general image fusion. Finally,\\r\\nwe demonstrate the potential application of the proposed model\\r\\nby automatically tuning the parameters of MEF algorithms',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Low-Light Image Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/low-light-image-enhancement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MEF'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dicm',\n",
       "  'name': 'DICM',\n",
       "  'full_name': 'DICM',\n",
       "  'homepage': None,\n",
       "  'description': '**DICM** is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras.\\n\\nSource: [Deep Retinex Decomposition for Low-Light Enhancement](https://arxiv.org/abs/1808.04560)\\nImage Source: [GLADNet: Low-Light Enhancement Network with Global Awareness](https://ieeexplore.ieee.org/document/8373911)',\n",
       "  'paper': {'title': 'Contrast enhancement based on layered difference representation',\n",
       "   'url': 'https://doi.org/10.1109/ICIP.2012.6467022'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Low-Light Image Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/low-light-image-enhancement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DICM'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/guesswhat',\n",
       "  'name': 'GuessWhat?!',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/GuessWhatGame/guesswhat',\n",
       "  'description': '**GuessWhat?!** is a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images.\\r\\n\\r\\nGuessWhat?! is a cooperative two-player game in which\\r\\nboth players see the picture of a rich visual scene with several objects. One player – the oracle – is randomly assigned\\r\\nan object (which could be a person) in the scene. This object is not known by the other player – the questioner –\\r\\nwhose goal it is to locate the hidden object. To do so, the\\r\\nquestioner can ask a series of yes-no questions which are\\r\\nanswered by the oracle.\\r\\n\\r\\nSource: [https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through](https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through)\\r\\nImage Source: [Vries et al](https://arxiv.org/pdf/1611.08481v2.pdf)',\n",
       "  'paper': {'title': 'GuessWhat?! Visual object discovery through multi-modal dialogue',\n",
       "   'url': 'https://paperswithcode.com/paper/guesswhat-visual-object-discovery-through'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['GuessWhat?!'],\n",
       "  'num_papers': 57,\n",
       "  'data_loaders': [{'url': 'https://github.com/GuessWhatGame/guesswhat',\n",
       "    'repo': 'https://github.com/GuessWhatGame/guesswhat',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/objectnet',\n",
       "  'name': 'ObjectNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://objectnet.dev/',\n",
       "  'description': '**ObjectNet** is a test set of images collected directly using crowd-sourcing. ObjectNet is unique as the objects are captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance. There are 50,000 images in the test set which controls for rotation, background and viewpoint. There are 313 object classes with 113 overlapping ImageNet.\\r\\n\\r\\nSource: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)\\r\\nImage Source: [https://objectnet.dev/](https://objectnet.dev/)',\n",
       "  'paper': {'title': 'ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models',\n",
       "   'url': 'https://paperswithcode.com/paper/objectnet-a-large-scale-bias-controlled'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Zero-Shot Transfer Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-transfer-image-classification'},\n",
       "   {'task': 'Unsupervised Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ObjectNet', 'ObjectNet (Bounding Box)'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/activitynet-captions',\n",
       "  'name': 'ActivityNet Captions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cs.stanford.edu/people/ranjaykrishna/densevid/',\n",
       "  'description': 'The **ActivityNet Captions** dataset is built on ActivityNet v1.3 which includes 20k YouTube untrimmed videos with 100k caption annotations. The videos are 120 seconds long on average. Most of the videos contain over 3 annotated events with corresponding start/end time and human-written sentences, which contain 13.5 words on average. The number of videos in train/validation/test split is 10024/4926/5044, respectively.\\r\\n\\r\\nSource: [Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning](https://arxiv.org/abs/1804.00100)\\r\\nImage Source: [https://cs.stanford.edu/people/ranjaykrishna/densevid/](https://cs.stanford.edu/people/ranjaykrishna/densevid/)',\n",
       "  'paper': {'title': 'Dense-Captioning Events in Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/dense-captioning-events-in-videos'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'},\n",
       "   {'task': 'Natural Language Moment Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-moment-retrieval'},\n",
       "   {'task': 'Temporal Action Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-action-proposal-generation'},\n",
       "   {'task': 'Dense Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/dense-video-captioning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ActivityNet Captions'],\n",
       "  'num_papers': 111,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/smallnorb',\n",
       "  'name': 'smallNORB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/',\n",
       "  'description': 'The **smallNORB** dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees).\\r\\nThe training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).\\r\\n\\r\\nSource: [https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/](https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/)\\r\\nImage Source: [https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10](https://www.kaggle.com/nepuerto/the-small-norb-dataset-v10)',\n",
       "  'paper': {'title': 'Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting',\n",
       "   'url': 'http://doi.ieeecomputersociety.org/10.1109/CVPR.2004.144'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['smallNORB'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/smallnorb',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/docred',\n",
       "  'name': 'DocRED',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/thunlp/DocRED',\n",
       "  'description': '**DocRED** (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data.\\r\\n\\r\\nDocRED contains 132,375 entities and 56,354 relational facts annotated on 5,053 Wikipedia documents. In addition to the human-annotated data, the dataset provides large-scale distantly supervised data over 101,873 documents.\\r\\n\\r\\nSource: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)\\r\\nImage Source: [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation/)',\n",
       "  'paper': {'title': 'DocRED: A Large-Scale Document-Level Relation Extraction Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/docred-a-large-scale-document-level-relation'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Joint Entity and Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/joint-entity-and-relation-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DocRED'],\n",
       "  'num_papers': 66,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/docred',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/thunlp/DocRED',\n",
       "    'repo': 'https://github.com/thunlp/DocRED',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/imaterialist',\n",
       "  'name': 'iMaterialist',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/visipedia/imat_fashion_comp',\n",
       "  'description': 'Constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes.\\r\\n\\r\\nSource: [The iMaterialist Fashion Attribute Dataset](/paper/the-imaterialist-fashion-attribute-dataset)',\n",
       "  'paper': {'title': 'The iMaterialist Fashion Attribute Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/the-imaterialist-fashion-attribute-dataset'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Complimentary Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/complimentary-image-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iMaterialist'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': [{'url': 'https://github.com/visipedia/imat_fashion_comp',\n",
       "    'repo': 'https://github.com/visipedia/imat_fashion_comp',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet-c',\n",
       "  'name': 'ImageNet-C',\n",
       "  'full_name': 'ImageNet-C',\n",
       "  'homepage': 'https://zenodo.org/record/2235448',\n",
       "  'description': '**ImageNet-C** is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set.\\r\\n\\r\\nSource: [Selective Brain Damage: Measuring the Disparate Impact of Model Pruning](https://arxiv.org/abs/1911.05248)\\r\\nImage Source: [https://arxiv.org/pdf/1807.01697.pdf](https://arxiv.org/pdf/1807.01697.pdf)',\n",
       "  'paper': {'title': 'Benchmarking Neural Network Robustness to Common Corruptions and Perturbations',\n",
       "   'url': 'https://paperswithcode.com/paper/benchmarking-neural-network-robustness-to-2'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Adversarial Robustness',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-robustness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageNet-C'],\n",
       "  'num_papers': 210,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet-a',\n",
       "  'name': 'ImageNet-A',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/hendrycks/natural-adv-examples',\n",
       "  'description': 'The **ImageNet-A** dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models.\\r\\n\\r\\nSource: [On Robustness and Transferability of Convolutional Neural Networks](https://arxiv.org/abs/2007.08558)\\r\\nImage Source: [https://github.com/hendrycks/natural-adv-examples](https://github.com/hendrycks/natural-adv-examples)',\n",
       "  'paper': {'title': 'Natural Adversarial Examples',\n",
       "   'url': 'https://paperswithcode.com/paper/natural-adversarial-examples'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Zero-Shot Transfer Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-transfer-image-classification'},\n",
       "   {'task': 'Adversarial Robustness',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-robustness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageNet-A'],\n",
       "  'num_papers': 83,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/imagenet_a',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/hendrycks/natural-adv-examples',\n",
       "    'repo': 'https://github.com/hendrycks/natural-adv-examples',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/biosses',\n",
       "  'name': 'BIOSSES',\n",
       "  'full_name': 'Biomedical Semantic Similarity Estimation System',\n",
       "  'homepage': 'https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html',\n",
       "  'description': 'The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the \"[TAC2 Biomedical Summarization Track Training Data Set](https://tac.nist.gov/2014/BiomedSumm/)\" .\\r\\n\\r\\nThe sentence pairs were evaluated by five different human experts that judged their similarity and gave scores in a range [0-4]. Our guideline was prepared based on SemEval 2012 Task 6 Guideline.\\r\\n\\r\\nImage source: [BIOSSES](https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html)',\n",
       "  'paper': {'title': 'BIOSSES: A Semantic Sentence Similarity Estimation System for the Biomedical Domain',\n",
       "   'url': 'https://paperswithcode.com/paper/biosses-a-semantic-sentence-similarity'},\n",
       "  'introduced_date': '2017-07-15',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Semantic Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-similarity'},\n",
       "   {'task': 'Sentence Embeddings For Biomedical Texts',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-embeddings-for-biomedical-texts'},\n",
       "   {'task': 'Sentence Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-similarity'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BIOSSES'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/biosses',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mednli',\n",
       "  'name': 'MedNLI',\n",
       "  'full_name': 'Medical Natural Language Inference',\n",
       "  'homepage': 'https://physionet.org/content/mednli/1.0.0/',\n",
       "  'description': 'The **MedNLI** dataset consists of the sentence pairs developed by Physicians from the Past Medical History section of MIMIC-III clinical notes annotated for Definitely True, Maybe True and Definitely False. The dataset contains 11,232 training, 1,395 development and 1,422 test instances. This provides a natural language inference task (NLI) grounded in the medical history of patients.\\n\\nSource: [MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning](https://arxiv.org/abs/2004.10220)\\nImage Source: [https://arxiv.org/abs/1904.02181](https://arxiv.org/abs/1904.02181)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MedNLI'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ucf-qnrf',\n",
       "  'name': 'UCF-QNRF',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.crcv.ucf.edu/data/ucf-qnrf/',\n",
       "  'description': 'The **UCF-QNRF** dataset is a crowd counting dataset and it contains large diversity both in scenes, as well as in background types. It consists of 1535 images high-resolution images from Flickr, Web Search and Hajj footage. The number of people (i.e., the count) varies from 50 to 12,000 across images.\\r\\n\\r\\nSource: [Understanding the impact of mistakes on background regions in crowd counting](https://arxiv.org/abs/2003.13759)\\r\\nImage Source: [https://www.crcv.ucf.edu/data/ucf-qnrf/](https://www.crcv.ucf.edu/data/ucf-qnrf/)',\n",
       "  'paper': {'title': 'Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds',\n",
       "   'url': 'https://paperswithcode.com/paper/composition-loss-for-counting-density-map'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Crowd Counting',\n",
       "    'url': 'https://paperswithcode.com/task/crowd-counting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCF-QNRF'],\n",
       "  'num_papers': 109,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/widerperson',\n",
       "  'name': 'WiderPerson',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/',\n",
       "  'description': 'WiderPerson contains a total of 13,382 images with 399,786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild.\\r\\n\\r\\nSource: [WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild](/paper/widerperson-a-diverse-dataset-for-dense)\\r\\nImage Source: [http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/](http://www.cbsr.ia.ac.cn/users/sfzhang/WiderPerson/)',\n",
       "  'paper': {'title': 'WiderPerson: A Diverse Dataset for Dense Pedestrian Detection in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/widerperson-a-diverse-dataset-for-dense'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WiderPerson'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cid',\n",
       "  'name': 'CID',\n",
       "  'full_name': 'Campus Image Dataset',\n",
       "  'homepage': 'https://github.com/505030475/ExtremeLowLight',\n",
       "  'description': 'The **CID** (**Campus Image Dataset**) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst.\\n\\nSource: [https://github.com/505030475/ExtremeLowLight](https://github.com/505030475/ExtremeLowLight)',\n",
       "  'paper': {'title': 'Learning an Adaptive Model for Extreme Low-light Raw Image Processing',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-an-adaptive-model-for-extreme-low'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CID'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/505030475/ExtremeLowLight',\n",
       "    'repo': 'https://github.com/505030475/ExtremeLowLight',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lener-br',\n",
       "  'name': 'LeNER-Br',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cic.unb.br/~teodecampos/LeNER-Br/',\n",
       "  'description': 'LeNER-Br is a dataset for named entity recognition (NER) in Brazilian Legal Text.',\n",
       "  'paper': {'title': 'LeNER-Br: a Dataset for Named Entity Recognition in Brazilian Legal Text',\n",
       "   'url': 'https://paperswithcode.com/paper/lener-br-a-dataset-for-named-entity'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Token Classification',\n",
       "    'url': 'https://paperswithcode.com/task/token-classification'},\n",
       "   {'task': 'Fill Mask', 'url': 'https://paperswithcode.com/task/fill-mask'}],\n",
       "  'languages': ['Portuguese'],\n",
       "  'variants': ['LeNER-Br',\n",
       "   'lener_br',\n",
       "   'pierreguillou/lener_br_finetuning_language_model'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/pierreguillou/lener_br_finetuning_language_model',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/lener_br',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/davis',\n",
       "  'name': 'DAVIS',\n",
       "  'full_name': 'Densely Annotated VIdeo Segmentation',\n",
       "  'homepage': 'https://davischallenge.org/',\n",
       "  'description': 'The Densely Annotation Video Segmentation dataset (**DAVIS**) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation.\\r\\n\\r\\nSource: [TENet: Triple Excitation Network for Video Salient Object Detection](https://arxiv.org/abs/2007.09943)',\n",
       "  'paper': {'title': 'A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/a-benchmark-dataset-and-evaluation'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Video Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/video-denoising'},\n",
       "   {'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'},\n",
       "   {'task': 'Video Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-detection'},\n",
       "   {'task': 'Video Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/video-inpainting'},\n",
       "   {'task': 'Interactive Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-video-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DAVIS sigma50',\n",
       "   'DAVIS sigma40',\n",
       "   'DAVIS sigma30',\n",
       "   'DAVIS sigma20',\n",
       "   'DAVIS sigma10',\n",
       "   'DAVIS 2017',\n",
       "   'DAVIS'],\n",
       "  'num_papers': 361,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/davis',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vist',\n",
       "  'name': 'VIST',\n",
       "  'full_name': 'Visual Storytelling',\n",
       "  'homepage': 'https://visionandlanguage.net/VIST/dataset.html',\n",
       "  'description': 'The **Visual Storytelling** Dataset (**VIST**) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person.\\r\\n\\r\\nSource: [Stories for Images-in-Sequence by using Visual and Narrative Components This research was partially funded by Pendulibrium and the Faculty of computer science and engineering, Ss. Cyril and Methodius University in Skopje.](https://arxiv.org/abs/1805.05622)\\r\\nImage Source: [https://arxiv.org/pdf/1604.03968.pdf](https://arxiv.org/pdf/1604.03968.pdf)',\n",
       "  'paper': {'title': 'Visual Storytelling',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-storytelling'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Storytelling',\n",
       "    'url': 'https://paperswithcode.com/task/visual-storytelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['VIST'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dtd',\n",
       "  'name': 'DTD',\n",
       "  'full_name': 'Describable Textures Dataset',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/dtd/',\n",
       "  'description': 'The **Describable Textures Dataset** (**DTD**) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures.\\r\\n\\r\\nSource: [Where is the Fake? Patch-Wise Supervised GANs for Texture Inpainting](https://arxiv.org/abs/1911.02274)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/dtd/](https://www.robots.ox.ac.uk/~vgg/data/dtd/)',\n",
       "  'paper': {'title': 'Describing Textures in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/describing-textures-in-the-wild'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DTD'],\n",
       "  'num_papers': 204,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/dtd',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/adience',\n",
       "  'name': 'Adience',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://talhassner.github.io/home/projects/Adience/Adience-data.html',\n",
       "  'description': 'The **Adience** dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few.\\r\\n\\r\\nSource: [Understanding and Comparing Deep Neural Networksfor Age and Gender Classification](https://arxiv.org/abs/1708.07689)\\r\\nImage Source: [https://talhassner.github.io/home/projects/Adience/Adience-data.html](https://talhassner.github.io/home/projects/Adience/Adience-data.html)',\n",
       "  'paper': {'title': 'Age and Gender Estimation of Unfiltered Faces',\n",
       "   'url': 'https://doi.org/10.1109/TIFS.2014.2359646'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Age And Gender Classification',\n",
       "    'url': 'https://paperswithcode.com/task/age-and-gender-classification'},\n",
       "   {'task': 'Face Quality Assessement',\n",
       "    'url': 'https://paperswithcode.com/task/face-quality-assessement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Adience',\n",
       "   'Adience Age',\n",
       "   'Adience Gender',\n",
       "   'Adience (Online Open Set)'],\n",
       "  'num_papers': 85,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/adience-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/matterport3d',\n",
       "  'name': 'Matterport3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://niessner.github.io/Matterport/',\n",
       "  'description': 'The **Matterport3D** dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation.\\r\\n\\r\\nSource: [Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention](https://arxiv.org/abs/1812.04155)',\n",
       "  'paper': {'title': 'Matterport3D: Learning from RGB-D Data in Indoor Environments',\n",
       "   'url': 'https://paperswithcode.com/paper/matterport3d-learning-from-rgb-d-data-in'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Depth Completion',\n",
       "    'url': 'https://paperswithcode.com/task/depth-completion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Matterport3D'],\n",
       "  'num_papers': 182,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/totto',\n",
       "  'name': 'ToTTo',\n",
       "  'full_name': 'ToTTo',\n",
       "  'homepage': 'https://github.com/google-research-datasets/totto',\n",
       "  'description': 'ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.\\r\\n\\r\\nDuring the dataset creation process, tables from English Wikipedia are matched with (noisy) descriptions. Each table cell mentioned in the description is highlighted and the descriptions are iteratively cleaned and corrected to faithfully reflect the content of the highlighted cells.\\r\\n\\r\\nSource: [Google Research Datasets](https://github.com/google-research-datasets/totto)',\n",
       "  'paper': {'title': 'ToTTo: A Controlled Table-To-Text Generation Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/totto-a-controlled-table-to-text-generation'},\n",
       "  'introduced_date': '2020-04-29',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ToTTo'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/totto',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/google-research-datasets/totto',\n",
       "    'repo': 'https://github.com/google-research-datasets/ToTTo',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pcam',\n",
       "  'name': 'PCam',\n",
       "  'full_name': 'PatchCamelyon',\n",
       "  'homepage': 'https://github.com/basveeling/pcam',\n",
       "  'description': '**PatchCamelyon** is an image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than ImageNet, trainable on a single GPU.',\n",
       "  'paper': {'title': 'Rotation Equivariant CNNs for Digital Pathology',\n",
       "   'url': 'https://paperswithcode.com/paper/rotation-equivariant-cnns-for-digital'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Breast Tumour Classification',\n",
       "    'url': 'https://paperswithcode.com/task/breast-tumour-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PCam'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/patch_camelyon',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/basveeling/pcam',\n",
       "    'repo': 'https://github.com/basveeling/pcam',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kumar',\n",
       "  'name': 'Kumar',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://monuseg.grand-challenge.org/Data/',\n",
       "  'description': 'The **Kumar** dataset contains 30 1,000×1,000 image tiles from seven organs (6 breast, 6 liver, 6 kidney, 6 prostate, 2 bladder, 2 colon and 2 stomach) of The Cancer Genome Atlas (TCGA) database acquired at 40× magnification. Within each image, the boundary of each nucleus is fully annotated.\\r\\n\\r\\nSource: [Dense Steerable Filter CNNs for Exploiting Rotational Symmetry in Histology Images](https://arxiv.org/abs/2004.03037)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Interactive'],\n",
       "  'tasks': [{'task': 'Multi-tissue Nucleus Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-tissue-nucleus-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Kumar'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hellaswag',\n",
       "  'name': 'HellaSwag',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://rowanzellers.com/hellaswag/',\n",
       "  'description': 'HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy).',\n",
       "  'paper': {'title': 'HellaSwag: Can a Machine Really Finish Your Sentence?',\n",
       "   'url': 'https://paperswithcode.com/paper/hellaswag-can-a-machine-really-finish-your'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Sentence Completion',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-completion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HellaSwag'],\n",
       "  'num_papers': 55,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/hellaswag',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/hellaswag',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lambada',\n",
       "  'name': 'LAMBADA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://zenodo.org/record/2630551#.YFJVaWT7S_w',\n",
       "  'description': 'The **LAMBADA** (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions.\\r\\n\\r\\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\\r\\nImage Source: [https://arxiv.org/pdf/1606.06031.pdf](https://arxiv.org/pdf/1606.06031.pdf)',\n",
       "  'paper': {'title': 'The LAMBADA dataset: Word prediction requiring a broad discourse context',\n",
       "   'url': 'https://paperswithcode.com/paper/the-lambada-dataset-word-prediction-requiring'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['LAMBADA'],\n",
       "  'num_papers': 55,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/lambada',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/lambada',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/piqa',\n",
       "  'name': 'PIQA',\n",
       "  'full_name': 'Physical Interaction: Question Answering',\n",
       "  'homepage': 'https://yonatanbisk.com/piqa/',\n",
       "  'description': 'PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP. \\r\\n\\r\\nSource: [PIQA](https://yonatanbisk.com/piqa/)',\n",
       "  'paper': {'title': 'PIQA: Reasoning about Physical Commonsense in Natural Language',\n",
       "   'url': 'https://paperswithcode.com/paper/piqa-reasoning-about-physical-commonsense-in'},\n",
       "  'introduced_date': '2019-11-26',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PIQA'],\n",
       "  'num_papers': 49,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/piqa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/corypaik/prost',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/piqa',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/mc/dataset_readers/piqa/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/openbookqa',\n",
       "  'name': 'OpenBookQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://allenai.org/data/open-book-qa',\n",
       "  'description': '**OpenBookQA** is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small “book” of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm.\\r\\nAdditionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID.\\r\\n\\r\\nSource: [https://allenai.org/data/open-book-qa](https://allenai.org/data/open-book-qa)\\r\\nImage Source: [https://arxiv.org/pdf/1809.02789.pdf](https://arxiv.org/pdf/1809.02789.pdf)',\n",
       "  'paper': {'title': 'Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/can-a-suit-of-armor-conduct-electricity-a-new'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['OpenBookQA'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/openbookqa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/openbookqa',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wsc',\n",
       "  'name': 'WSC',\n",
       "  'full_name': 'Winograd Schema Challenge',\n",
       "  'homepage': 'https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html',\n",
       "  'description': 'The **Winograd Schema Challenge** was introduced both as an alternative to the Turing Test and as a test of a system’s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes.\\r\\n\\r\\nThe original Winograd Schema Challenge dataset consisted of 100 Winograd schemas constructed manually by AI experts. As of 2020 there are 285 examples available; however, the last 12 examples were only added recently. To ensure consistency with earlier models, several authors often prefer to report the performance on the first 273 examples only. These datasets are usually referred to as **WSC**285 and WSC273, respectively.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/2004.13831.pdf](https://arxiv.org/pdf/2004.13831.pdf)\\r\\nImage Source: [https://arxiv.org/pdf/1907.11983.pdf](https://arxiv.org/pdf/1907.11983.pdf)',\n",
       "  'paper': {'title': 'The Winograd Schema Challenge',\n",
       "   'url': 'http://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'},\n",
       "   {'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Winograd Schema Challenge', 'WSC'],\n",
       "  'num_papers': 161,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/winograd_wsc',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wsc273',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/arxiv',\n",
       "  'name': 'arXiv',\n",
       "  'full_name': 'Arxiv HEP-TH (high energy physics theory) citation graph',\n",
       "  'homepage': 'https://snap.stanford.edu/data/cit-HepTh.html',\n",
       "  'description': '**Arxiv HEP-TH (high energy physics theory) citation graph** is from the e-print **arXiv** and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.\\nThe data covers papers in the period from January 1993 to April 2003 (124 months).\\n\\nSource: [https://snap.stanford.edu/data/cit-HepTh.html](https://snap.stanford.edu/data/cit-HepTh.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/document-summarization'},\n",
       "   {'task': 'Extended Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/extended-summarization'},\n",
       "   {'task': 'Topic Models',\n",
       "    'url': 'https://paperswithcode.com/task/topic-models'},\n",
       "   {'task': 'Clique Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/clique-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['arXiv-AstroPh 3-clique',\n",
       "   'arXiv-GrQc 4-clique',\n",
       "   'arXiv',\n",
       "   'arXiv-Long Val',\n",
       "   'arXiv-Long Test'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/levir-cd',\n",
       "  'name': 'LEVIR-CD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://justchenhao.github.io/LEVIR/',\n",
       "  'description': 'LEVIR-CD is a new large-scale remote sensing building Change Detection dataset. The introduced dataset would be a new benchmark for evaluating change detection (CD) algorithms, especially those based on deep learning.\\r\\n\\r\\nLEVIR-CD consists of 637 very high-resolution (VHR, 0.5m/pixel) Google Earth (GE) image patch pairs with a size of 1024 × 1024 pixels. These bitemporal images with time span of 5 to 14 years have significant land-use changes, especially the construction growth. LEVIR-CD covers various types of buildings, such as villa residences, tall apartments, small garages and large warehouses. Here, we focus on building-related changes, including the building growth (the change from soil/grass/hardened ground or building under construction to new build-up regions) and the building decline. These bitemporal images are annotated by remote sensing image interpretation experts using binary labels (1 for change and 0 for unchanged). Each sample in our dataset is annotated by one annotator and then double-checked by another to produce high-quality annotations. The fully annotated LEVIR-CD contains a total of 31,333 individual change-building instances.',\n",
       "  'paper': {'title': 'A Spatial-Temporal Attention-Based Method and a New Dataset for Remote Sensing Image Change Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/a-spatial-temporal-attention-based-method-and'},\n",
       "  'introduced_date': '2020-05-22',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Change Detection',\n",
       "    'url': 'https://paperswithcode.com/task/change-detection'},\n",
       "   {'task': 'Building change detection for remote sensing images',\n",
       "    'url': 'https://paperswithcode.com/task/building-change-detection-for-remote-sensing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LEVIR-CD'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fever',\n",
       "  'name': 'FEVER',\n",
       "  'full_name': 'Fact Extraction and VERification',\n",
       "  'homepage': 'https://fever.ai/resources.html',\n",
       "  'description': 'FEVER is a publicly available dataset for fact extraction and verification against textual sources.\\r\\n\\r\\nIt consists of 185,445 claims manually verified against the introductory sections of Wikipedia pages and classified as SUPPORTED, REFUTED or NOTENOUGHINFO. For the first two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim.\\r\\n\\r\\nThe claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The verification of each claim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was\\r\\nextracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.\\r\\n\\r\\nSource: [FEVER: a large-scale dataset for Fact Extraction and VERification](https://arxiv.org/pdf/1803.05355v3.pdf)',\n",
       "  'paper': {'title': 'FEVER: a large-scale dataset for Fact Extraction and VERification',\n",
       "   'url': 'https://paperswithcode.com/paper/fever-a-large-scale-dataset-for-fact'},\n",
       "  'introduced_date': '2018-03-14',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Fact Checking',\n",
       "    'url': 'https://paperswithcode.com/task/fact-checking'},\n",
       "   {'task': 'Fact Verification',\n",
       "    'url': 'https://paperswithcode.com/task/fact-verification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FEVER', 'FEVER (BEIR)'],\n",
       "  'num_papers': 203,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/fever',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/meld',\n",
       "  'name': 'MELD',\n",
       "  'full_name': 'Multimodal EmotionLines Dataset',\n",
       "  'homepage': 'https://affective-meld.github.io/',\n",
       "  'description': '**Multimodal EmotionLines Dataset** (**MELD**) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance.\\r\\n\\r\\nSource: [https://affective-meld.github.io/](https://affective-meld.github.io/)\\r\\nImage Source: [https://affective-meld.github.io/](https://affective-meld.github.io/)',\n",
       "  'paper': {'title': 'MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations',\n",
       "   'url': 'https://paperswithcode.com/paper/meld-a-multimodal-multi-party-dataset-for'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MELD'],\n",
       "  'num_papers': 75,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/emorynlp',\n",
       "  'name': 'EmoryNLP',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/emorynlp/character-mining',\n",
       "  'description': 'EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)’s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.',\n",
       "  'paper': {'title': 'Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/emotion-detection-on-tv-show-transcripts-with'},\n",
       "  'introduced_date': '2017-08-14',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['EmoryNLP'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': [{'url': 'https://github.com/emorynlp/character-mining',\n",
       "    'repo': 'https://github.com/emorynlp/character-mining',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/4d-light-field-dataset',\n",
       "  'name': '4D Light Field Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://lightfield-analysis.uni-konstanz.de/',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\n\\nProvide:\\n\\n* a high-level explanation of the dataset characteristics\\n* explain motivations and summary of its content\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/virtual-kitti-2',\n",
       "  'name': 'Virtual KITTI 2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds',\n",
       "  'description': 'Virtual KITTI 2 is an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15◦). For each sequence we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset’s capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.',\n",
       "  'paper': {'title': 'Virtual KITTI 2',\n",
       "   'url': 'https://paperswithcode.com/paper/virtual-kitti-2'},\n",
       "  'introduced_date': '2020-01-29',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Monocular 3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-3d-object-detection'},\n",
       "   {'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Visual Odometry',\n",
       "    'url': 'https://paperswithcode.com/task/visual-odometry'},\n",
       "   {'task': 'Simultaneous Localization and Mapping',\n",
       "    'url': 'https://paperswithcode.com/task/simultaneous-localization-and-mapping'},\n",
       "   {'task': 'Stereo Matching',\n",
       "    'url': 'https://paperswithcode.com/task/stereo-matching-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Virtual KITTI 2'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/whamr',\n",
       "  'name': 'WHAMR!',\n",
       "  'full_name': 'WHAM! with synthetic reverberated sources',\n",
       "  'homepage': 'https://wham.whisper.ai/',\n",
       "  'description': '**WHAMR!** is a dataset for noisy and reverberant speech separation. It extends [WHAM!](/dataset/wham) by introducing synthetic reverberation to the\\r\\nspeech sources in addition to the existing noise. Room impulse responses were generated and convolved using `pyroomacoustics`. Reverberation times were chosen to approximate domestic and classroom environments (expected to be similar to the restaurants and coffee shops where the WHAM! noise was collected), and\\r\\nfurther classified as high, medium, and low reverberation based on a\\r\\nqualitative assessment of the mixture’s noise recording.',\n",
       "  'paper': {'title': 'WHAMR!: Noisy and Reverberant Single-Channel Speech Separation',\n",
       "   'url': 'https://paperswithcode.com/paper/whamr-noisy-and-reverberant-single-channel'},\n",
       "  'introduced_date': '2019-10-22',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Speech'],\n",
       "  'tasks': [{'task': 'Speech Separation',\n",
       "    'url': 'https://paperswithcode.com/task/speech-separation'},\n",
       "   {'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'},\n",
       "   {'task': 'Audio Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/audio-source-separation'},\n",
       "   {'task': 'Speech Dereverberation',\n",
       "    'url': 'https://paperswithcode.com/task/speech-dereverberation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WHAMR!'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/demand',\n",
       "  'name': 'DEMAND',\n",
       "  'full_name': 'Diverse Environments Multi-channel Acoustic Noise Database',\n",
       "  'homepage': 'https://zenodo.org/record/1227121',\n",
       "  'description': 'The **DEMAND** (Diverse Environments Multichannel Acoustic Noise Database) provides a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm.\\n\\nSource: [DEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments](https://zenodo.org/record/1227121)\\nImage Source: [https://asa.scitation.org/doi/pdf/10.1121/1.4799597](https://asa.scitation.org/doi/pdf/10.1121/1.4799597)',\n",
       "  'paper': {'title': 'The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings',\n",
       "   'url': 'https://asa.scitation.org/doi/abs/10.1121/1.4799597'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DEMAND'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/buff',\n",
       "  'name': 'BUFF',\n",
       "  'full_name': 'Bodies Under Flowing Fashion',\n",
       "  'homepage': 'http://buff.is.tue.mpg.de/',\n",
       "  'description': '**BUFF** consists of 5 subjects, 3 male and 2 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit.\\r\\nThey perform 3 different motions i) hips ii) tilt_twist_left iii) shoulders_mill.\\r\\n\\r\\nSource: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)\\r\\nImage Source: [http://buff.is.tue.mpg.de/](http://buff.is.tue.mpg.de/)',\n",
       "  'paper': {'title': 'Detailed, accurate, human shape estimation from clothed 3D scan sequences',\n",
       "   'url': 'https://paperswithcode.com/paper/detailed-accurate-human-shape-estimation-from'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Object Reconstruction From A Single Image',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-reconstruction-from-a-single-image'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BUFF'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/taskonomy',\n",
       "  'name': 'Taskonomy',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://taskonomy.stanford.edu/',\n",
       "  'description': 'Taskonomy provides a large and high-quality dataset of varied indoor scenes.\\r\\n\\r\\n- Complete pixel-level geometric information via aligned meshes.\\r\\n- Semantic information via knowledge distillation from ImageNet, MS COCO, and MIT Places.\\r\\n- Globally consistent camera poses. Complete camera intrinsics.\\r\\n- High-definition images.\\r\\n- 3x times big as ImageNet.\\r\\n\\r\\nSource: [Taskonomy](http://taskonomy.stanford.edu/)\\r\\nImage Source: [http://taskonomy.stanford.edu/](http://taskonomy.stanford.edu/)',\n",
       "  'paper': {'title': 'Taskonomy: Disentangling Task Transfer Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/taskonomy-disentangling-task-transfer'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Surface Normals Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/surface-normals-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Taskonomy'],\n",
       "  'num_papers': 70,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/abalone',\n",
       "  'name': 'Abalone',\n",
       "  'full_name': 'Abalone',\n",
       "  'homepage': 'http://archive.ics.uci.edu/ml/datasets/Abalone',\n",
       "  'description': 'Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.\\n\\nSource: [UCL Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Abalone)\\nImage Source: [http://archive.ics.uci.edu/ml/datasets/Abalone](http://archive.ics.uci.edu/ml/datasets/Abalone)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Core set discovery',\n",
       "    'url': 'https://paperswithcode.com/task/core-set-discovery'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Abalone'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/letter',\n",
       "  'name': 'Letter',\n",
       "  'full_name': 'Letter Recognition Data Set',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/Letter+Recognition',\n",
       "  'description': 'Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.\\r\\n\\r\\nSource: [UCL Machine Learning Repository Letter Recognition](https://archive.ics.uci.edu/ml/datasets/Letter+Recognition)\\r\\nImage Source: [http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf](http://www.cs.uu.nl/docs/vakken/mpr/Frey-Slate.pdf)',\n",
       "  'paper': {'title': 'Letter Recognition Using Holland-Style Adaptive Classifiers',\n",
       "   'url': 'https://doi.org/10.1007/BF00114162'},\n",
       "  'introduced_date': '1991-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Core set discovery',\n",
       "    'url': 'https://paperswithcode.com/task/core-set-discovery'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['LetterA-J', 'Letter'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/electricity',\n",
       "  'name': 'Electricity',\n",
       "  'full_name': 'Individual household electric power consumption Data Set',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption',\n",
       "  'description': 'Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available.\\r\\n\\r\\nData Set Information:\\r\\n\\r\\nThis archive contains 2075259 measurements gathered in a house located in Sceaux (7km of Paris, France) between December 2006 and November 2010 (47 months).\\r\\nNotes:\\r\\n1.(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.\\r\\n2.The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\\r\\n\\r\\n\\r\\nAttribute Information:\\r\\n\\r\\n1.date: Date in format dd/mm/yyyy\\r\\n2.time: time in format hh:mm:ss\\r\\n3.global_active_power: household global minute-averaged active power (in kilowatt)\\r\\n4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\\r\\n5.voltage: minute-averaged voltage (in volt)\\r\\n6.global_intensity: household global minute-averaged current intensity (in ampere)\\r\\n7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\\r\\n8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\\r\\n9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\\r\\n\\r\\nWe suggest the following pseudo-APA reference format for referring to this repository:\\r\\n\\r\\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\\r\\n\\r\\nHere is a BiBTeX citation as well:\\r\\n\\r\\n@misc{Dua:2019 ,\\r\\nauthor = \"Dua, Dheeru and Graff, Casey\",\\r\\nyear = \"2017\",\\r\\ntitle = \"{UCI} Machine Learning Repository\",\\r\\nurl = \"http://archive.ics.uci.edu/ml\",\\r\\ninstitution = \"University of California, Irvine, School of Information and Computer Sciences\" }',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2021-12-13',\n",
       "  'warning': None,\n",
       "  'modalities': ['Time series'],\n",
       "  'tasks': [{'task': 'Core set discovery',\n",
       "    'url': 'https://paperswithcode.com/task/core-set-discovery'},\n",
       "   {'task': 'Multivariate Time Series Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-imputation'},\n",
       "   {'task': 'Univariate Time Series Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/univariate-time-series-forecasting'},\n",
       "   {'task': 'Time Series',\n",
       "    'url': 'https://paperswithcode.com/task/time-series'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Electricity'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nethack-learning-environment',\n",
       "  'name': 'NetHack Learning Environment',\n",
       "  'full_name': 'NetHack Learning Environment',\n",
       "  'homepage': 'https://github.com/facebookresearch/nle',\n",
       "  'description': 'The **NetHack Learning Environment** (NLE) is a Reinforcement Learning environment based on NetHack 3.6.6. It is designed to provide a standard reinforcement learning interface to the game, and comes with tasks that function as a first step to evaluate agents on this new environment.\\nNetHack is one of the oldest and arguably most impactful videogames in history, as well as being one of the hardest roguelikes currently being played by humans. It is procedurally generated, rich in entities and dynamics, and overall an extremely challenging environment for current state-of-the-art RL agents, while being much cheaper to run compared to other challenging testbeds. Through NLE, the authors wish to establish NetHack as one of the next challenges for research in decision making and machine learning.\\n\\nSource: [https://github.com/facebookresearch/nle](https://github.com/facebookresearch/nle)\\nImage Source: [https://github.com/facebookresearch/nle](https://github.com/facebookresearch/nle)',\n",
       "  'paper': {'title': 'The NetHack Learning Environment',\n",
       "   'url': 'https://paperswithcode.com/paper/the-nethack-learning-environment'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Environment'],\n",
       "  'tasks': [{'task': 'NetHack Score',\n",
       "    'url': 'https://paperswithcode.com/task/score'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NetHack Learning Environment'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://github.com/facebookresearch/nle',\n",
       "    'repo': 'https://github.com/facebookresearch/nle',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kvasir-seg',\n",
       "  'name': 'Kvasir-SEG',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://datasets.simula.no/kvasir-seg/.',\n",
       "  'description': 'Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. \\r\\n\\r\\nSource: [Kvasir-SEG: A Segmented Polyp Dataset](/paper/kvasir-seg-a-segmented-polyp-dataset)\\r\\nImage Source: [https://datasets.simula.no/kvasir-seg/](https://datasets.simula.no/kvasir-seg/)',\n",
       "  'paper': {'title': 'Kvasir-SEG: A Segmented Polyp Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/kvasir-seg-a-segmented-polyp-dataset'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Biomedical', 'Medical'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'},\n",
       "   {'task': 'Colorectal Polyps Characterization',\n",
       "    'url': 'https://paperswithcode.com/task/colorectal-polyps-characterization'},\n",
       "   {'task': 'Real-Time Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-object-detection'},\n",
       "   {'task': 'Colorectal Gland Segmentation:',\n",
       "    'url': 'https://paperswithcode.com/task/colorectal-gland-segmentation'},\n",
       "   {'task': 'Polyp Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/polyp-segmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Kvasir-SEG'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/2018-data-science-bowl',\n",
       "  'name': '2018 Data Science Bowl',\n",
       "  'full_name': '2018 Data Science Bowl Find the nuclei in divergent images to advance medical discovery',\n",
       "  'homepage': 'https://www.kaggle.com/c/data-science-bowl-2018/overview',\n",
       "  'description': \"This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.\\r\\n\\r\\nEach image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders:\\r\\n\\r\\nimages contains the image file.\\r\\nmasks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).\\r\\nThe second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details.\\r\\n\\r\\nAs with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set.\",\n",
       "  'paper': {'title': 'UNet++: A Nested U-Net Architecture for Medical Image Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/unet-a-nested-u-net-architecture-for-medical'},\n",
       "  'introduced_date': '2018-07-18',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Nuclear Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/nuclear-segmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['2018 Data Science Bowl'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cvc-clinicdb',\n",
       "  'name': 'CVC-ClinicDB',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://polyp.grand-challenge.org/CVCClinicDB/',\n",
       "  'description': '**CVC-ClinicDB** is an open-access dataset of 612 images with a resolution of 384×288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos.\\n\\nSource: [ResUNet++: An Advanced Architecture for Medical Image Segmentation](https://arxiv.org/abs/1911.07067)\\nImage Source: [https://polyp.grand-challenge.org/CVCClinicDB/](https://polyp.grand-challenge.org/CVCClinicDB/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CVC-ClinicDB'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cat2000',\n",
       "  'name': 'CAT2000',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://saliency.mit.edu/results_cat2000.html',\n",
       "  'description': 'Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings.\\r\\n\\r\\nSource: [CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research](https://arxiv.org/pdf/1505.03581v1.pdf)',\n",
       "  'paper': {'title': 'CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research',\n",
       "   'url': 'https://paperswithcode.com/paper/cat2000-a-large-scale-fixation-dataset-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CAT2000'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fixatons',\n",
       "  'name': 'FixaTons',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/dariozanca/FixaTons',\n",
       "  'description': 'FixaTons is a large collection of datasets human scanpaths (temporally ordered sequences of fixations) and saliency maps. \\r\\n\\r\\nSource: [FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity](/paper/fixatons-a-collection-of-human-fixations)',\n",
       "  'paper': {'title': 'FixaTons: A collection of Human Fixations Datasets and Metrics for Scanpath Similarity',\n",
       "   'url': 'https://paperswithcode.com/paper/fixatons-a-collection-of-human-fixations'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Scanpath prediction',\n",
       "    'url': 'https://paperswithcode.com/task/scanpath-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FixaTons'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/dariozanca/FixaTons',\n",
       "    'repo': 'https://github.com/dariozanca/FixaTons',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet-r',\n",
       "  'name': 'ImageNet-R',\n",
       "  'full_name': 'ImageNet-Rendition',\n",
       "  'homepage': 'https://github.com/hendrycks/imagenet-r',\n",
       "  'description': 'ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes.\\r\\n\\r\\nImageNet-R has renditions of 200 ImageNet classes resulting in 30,000 images.\\r\\n\\r\\nSource: [The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](/paper/the-many-faces-of-robustness-a-critical)',\n",
       "  'paper': {'title': 'The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization',\n",
       "   'url': 'https://paperswithcode.com/paper/the-many-faces-of-robustness-a-critical'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Zero-Shot Transfer Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-transfer-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageNet-R'],\n",
       "  'num_papers': 63,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/imagenet_r',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/hendrycks/imagenet-r',\n",
       "    'repo': 'https://github.com/hendrycks/imagenet-r',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/20-newsgroups',\n",
       "  'name': '20 Newsgroups',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://qwone.com/~jason/20Newsgroups/',\n",
       "  'description': 'The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Text Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/text-clustering'},\n",
       "   {'task': 'Topic Models',\n",
       "    'url': 'https://paperswithcode.com/task/topic-models'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['20 Newsgroups'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hacs',\n",
       "  'name': 'HACS',\n",
       "  'full_name': 'Human Action Clips and Segments',\n",
       "  'homepage': 'http://hacs.csail.mit.edu/',\n",
       "  'description': 'HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively.\\r\\n\\r\\nAuthors split the collection into training, validation and testing sets of size 1.4M, 50K and 50K clips, which are sampled\\r\\nfrom 492K, 6K and 6K videos, respectively.',\n",
       "  'paper': {'title': 'HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization',\n",
       "   'url': 'https://paperswithcode.com/paper/hacs-human-action-clips-and-segments-dataset'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-localization'},\n",
       "   {'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HACS'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kinetics-700',\n",
       "  'name': 'Kinetics-700',\n",
       "  'full_name': 'Kinetics-700',\n",
       "  'homepage': 'https://deepmind.com/research/open-source/kinetics',\n",
       "  'description': 'Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds.',\n",
       "  'paper': {'title': 'A Short Note on the Kinetics-700 Human Action Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/a-short-note-on-the-kinetics-700-human-action'},\n",
       "  'introduced_date': '2019-07-15',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Semantic Object Interaction Classification',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-object-interaction-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Kinetics-700'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/completion3d',\n",
       "  'name': 'Completion3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://completion3d.stanford.edu/',\n",
       "  'description': 'The Completion3D benchmark is a dataset for evaluating state-of-the-art 3D Object Point Cloud Completion methods. Ggiven a partial 3D object point cloud the goal is to infer a complete 3D point cloud for the object.\\r\\n\\r\\nSource: [TopNet: Structural Point Cloud Decoder](/paper/topnet-structural-point-cloud-decoder)',\n",
       "  'paper': {'title': 'TopNet: Structural Point Cloud Decoder',\n",
       "   'url': 'https://paperswithcode.com/paper/topnet-structural-point-cloud-decoder'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Point cloud'],\n",
       "  'tasks': [{'task': 'Point Cloud Completion',\n",
       "    'url': 'https://paperswithcode.com/task/point-cloud-completion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Completion3D'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/qmnist',\n",
       "  'name': 'QMNIST',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/facebookresearch/qmnist',\n",
       "  'description': 'The exact pre-processing steps used to construct the MNIST dataset have long been lost. This leaves us with no reliable way to associate its characters with the ID of the writer and little hope to recover the full MNIST testing set that had 60K images but was never released. The official MNIST testing set only contains 10K randomly sampled images and is often considered too small to provide meaningful confidence intervals.\\nThe **QMNIST** dataset was generated from the original data found in the NIST Special Database 19 with the goal to match the MNIST preprocessing as closely as possible.\\nQMNIST is licensed under the BSD-style license.\\n\\nSource: [https://github.com/facebookresearch/qmnist](https://github.com/facebookresearch/qmnist)\\nImage Source: [https://github.com/facebookresearch/qmnist](https://github.com/facebookresearch/qmnist)',\n",
       "  'paper': {'title': 'Cold Case: The Lost MNIST Digits',\n",
       "   'url': 'https://paperswithcode.com/paper/cold-case-the-lost-mnist-digits'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['QMNIST'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.QMNIST',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/facebookresearch/qmnist',\n",
       "    'repo': 'https://github.com/facebookresearch/qmnist',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/rocstories',\n",
       "  'name': 'ROCStories',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cs.rochester.edu/nlp/rocstories/',\n",
       "  'description': '**ROCStories** is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871.\\r\\n\\r\\nSource: [Incorporating Structured Commonsense Knowledge in Story Completion](https://arxiv.org/abs/1811.00625)\\r\\nImage Source: [https://cs.rochester.edu/nlp/rocstories/](https://cs.rochester.edu/nlp/rocstories/)',\n",
       "  'paper': {'title': 'A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories',\n",
       "   'url': 'https://paperswithcode.com/paper/a-corpus-and-cloze-evaluation-for-deeper'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Emotion Classification',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Story Cloze Test', 'ROCStories'],\n",
       "  'num_papers': 118,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/story_cloze',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/epillid',\n",
       "  'name': 'ePillID',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/usuyama/ePillID-benchmark',\n",
       "  'description': '**ePillID** is a benchmark for developing and evaluating computer vision models for pill identification. The ePillID benchmark is designed as a low-shot fine-grained benchmark, reflecting real-world challenges for developing image-based pill identification systems.\\nThe characteristics of the ePillID benchmark include:\\n* Reference and consumer images: The reference images are taken with controlled lighting and backgrounds, and with professional equipment. The consumer images are taken with real-world settings including different lighting, backgrounds, and equipment. For most of the pills, one image per side (two images per pill type) is available from the NIH Pillbox dataset.\\n* Low-shot and fine-grained setting: 13k images representing 9804 appearance classes (two sides for 4902 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting.\\n\\nSource: [https://github.com/usuyama/ePillID-benchmark](https://github.com/usuyama/ePillID-benchmark)\\nImage Source: [https://github.com/usuyama/ePillID-benchmark](https://github.com/usuyama/ePillID-benchmark)',\n",
       "  'paper': {'title': 'ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill Identification',\n",
       "   'url': 'https://paperswithcode.com/paper/epillid-dataset-a-low-shot-fine-grained'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pill Classification (Both Sides)',\n",
       "    'url': 'https://paperswithcode.com/task/pill-classification-both-sides'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ePillID'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/usuyama/ePillID-benchmark',\n",
       "    'repo': 'https://github.com/usuyama/ePillID-benchmark',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/codesearchnet',\n",
       "  'name': 'CodeSearchNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/github/CodeSearchNet',\n",
       "  'description': 'The **CodeSearchNet** Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes:\\r\\n* Six million methods overall\\r\\n* Two million of which have associated documentation (docstrings, JavaDoc, and more)\\r\\n* Metadata that indicates the original location (repository or line number, for example) where the data was found\\r\\n\\r\\nSource: [https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/](https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/)',\n",
       "  'paper': {'title': 'CodeSearchNet Challenge: Evaluating the State of Semantic Code Search',\n",
       "   'url': 'https://paperswithcode.com/paper/codesearchnet-challenge-evaluating-the-state'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Source Code Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/code-summarization'},\n",
       "   {'task': 'Code Search',\n",
       "    'url': 'https://paperswithcode.com/task/code-search'},\n",
       "   {'task': 'Code Documentation Generation',\n",
       "    'url': 'https://paperswithcode.com/task/code-documentation-generation'},\n",
       "   {'task': 'Method name prediction',\n",
       "    'url': 'https://paperswithcode.com/task/method-name-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CodeSearchNet - Ruby',\n",
       "   'CodeSearchNet - Php',\n",
       "   'CodeSearchNet - JavaScript',\n",
       "   'CodeSearchNet - Java',\n",
       "   'CodeSearchNet - Go',\n",
       "   'CodeSearchNet - Python',\n",
       "   'CodeSearchNet'],\n",
       "  'num_papers': 70,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/code_search_net',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/github/CodeSearchNet',\n",
       "    'repo': 'https://github.com/github/CodeSearchNet',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikitablequestions',\n",
       "  'name': 'WikiTableQuestions',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ppasupat.github.io/WikiTableQuestions/',\n",
       "  'description': '**WikiTableQuestions** is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions.\\r\\n\\r\\nSource: [Explaining Queries over Web Tables to Non-Experts](https://arxiv.org/abs/1808.04614)\\r\\nImage Source: [https://ppasupat.github.io/WikiTableQuestions/](https://ppasupat.github.io/WikiTableQuestions/)',\n",
       "  'paper': {'title': 'Compositional Semantic Parsing on Semi-Structured Tables',\n",
       "   'url': 'https://paperswithcode.com/paper/compositional-semantic-parsing-on-semi'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tabular'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiTableQuestions'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/wiki_table_questions',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/avid',\n",
       "  'name': 'AViD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/piergiaj/AViD',\n",
       "  'description': 'Is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries.\\r\\n\\r\\nSource: [AViD Dataset: Anonymized Videos from Diverse Countries](/paper/avid-dataset-anonymized-videos-from-diverse)',\n",
       "  'paper': {'title': 'AViD Dataset: Anonymized Videos from Diverse Countries',\n",
       "   'url': 'https://paperswithcode.com/paper/avid-dataset-anonymized-videos-from-diverse'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AViD'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://github.com/piergiaj/AViD',\n",
       "    'repo': 'https://github.com/piergiaj/AViD',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mtl-aqa',\n",
       "  'name': 'MTL-AQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/ParitoshParmar/MTL-AQA',\n",
       "  'description': 'A new multitask action quality assessment (AQA) dataset, the largest to date, comprising of more than 1600 diving samples; contains detailed annotations for  fine-grained action recognition, commentary generation, and estimating the AQA score. Videos from multiple angles provided wherever available.\\r\\n\\r\\nSource: [What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment](/paper/what-and-how-well-you-performed-a-multitask)',\n",
       "  'paper': {'title': 'What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment',\n",
       "   'url': 'https://paperswithcode.com/paper/what-and-how-well-you-performed-a-multitask'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/video-captioning'},\n",
       "   {'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Action Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/action-quality-assessment'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'},\n",
       "   {'task': 'Dense Video Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/dense-video-captioning'},\n",
       "   {'task': 'Action Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/action-understanding'},\n",
       "   {'task': 'Fine-Grained Visual Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-visual-recognition'},\n",
       "   {'task': 'Fine-Grained Visual Categorization',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-visual-categorization'},\n",
       "   {'task': 'Multiview Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multiview-learning'},\n",
       "   {'task': 'Fine-grained Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MTL-AQA'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': [{'url': 'https://github.com/ParitoshParmar/MTL-AQA',\n",
       "    'repo': 'https://github.com/ParitoshParmar/MTL-AQA',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/aqa-7',\n",
       "  'name': 'AQA-7',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://rtis.oit.unlv.edu/datasets.html',\n",
       "  'description': 'Consists of 1106 action samples from seven actions with quality scores as measured by expert human judges.\\r\\n\\r\\nSource: [Action Quality Assessment Across Multiple Actions](/paper/action-quality-assessment-across-multiple)',\n",
       "  'paper': {'title': 'Action Quality Assessment Across Multiple Actions',\n",
       "   'url': 'https://paperswithcode.com/paper/action-quality-assessment-across-multiple'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Audio'],\n",
       "  'tasks': [{'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Action Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/action-quality-assessment'},\n",
       "   {'task': 'Skills Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/skills-assessment'},\n",
       "   {'task': 'Skills Evaluation',\n",
       "    'url': 'https://paperswithcode.com/task/skills-evaluation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AQA-7'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/agenda',\n",
       "  'name': 'AGENDA',\n",
       "  'full_name': 'Abstract GENeration DAtaset',\n",
       "  'homepage': 'https://github.com/rikdz/GraphWriter',\n",
       "  'description': 'Abstract GENeration DAtaset (AGENDA) is a dataset of knowledge graphs paired with scientific abstracts. The dataset consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences.\\r\\n\\r\\nSource: [Text Generation from Knowledge Graphs with Graph Transformers](https://arxiv.org/pdf/1904.02342v2.pdf)',\n",
       "  'paper': {'title': 'Text Generation from Knowledge Graphs with Graph Transformers',\n",
       "   'url': 'https://paperswithcode.com/paper/text-generation-from-knowledge-graphs-with'},\n",
       "  'introduced_date': '2019-04-04',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'KG-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/kg-to-text'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AGENDA'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/gopro',\n",
       "  'name': 'GoPro',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://seungjunnah.github.io/Datasets/gopro',\n",
       "  'description': 'The **GoPro** dataset for deblurring consists of 3,214 blurred images with the size of 1,280×720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth shapr image that are obtained by a high-speed camera.\\r\\n\\r\\nSource: [Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networksfor Non-Uniform Single Image Deblurring](https://arxiv.org/abs/1903.10157)\\r\\nImage Source: [Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf)',\n",
       "  'paper': {'title': 'Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-multi-scale-convolutional-neural-network'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Deblurring',\n",
       "    'url': 'https://paperswithcode.com/task/deblurring'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GoPro linear subset', 'GoPro'],\n",
       "  'num_papers': 139,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/amz-computers',\n",
       "  'name': 'AMZ Computers',\n",
       "  'full_name': 'amazon_electronics_computers',\n",
       "  'homepage': '',\n",
       "  'description': 'AMZ Computers is a co-purchase graph extracted from Amazon, where nodes represent products, edges represent the co-purchased relations of products, and features are bag-of-words vectors extracted from product reviews.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AMZ Computers'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/svg-icons8',\n",
       "  'name': 'SVG-Icons8',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/alexandre01/deepsvg',\n",
       "  'description': 'A new large-scale dataset along with an open-source library for SVG manipulation.\\r\\n\\r\\nSource: [DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation](/paper/deepsvg-a-hierarchical-generative-network-for)',\n",
       "  'paper': {'title': 'DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation',\n",
       "   'url': 'https://paperswithcode.com/paper/deepsvg-a-hierarchical-generative-network-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Vector Graphics Animation',\n",
       "    'url': 'https://paperswithcode.com/task/vector-graphics-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SVG-Icons8'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/alexandre01/deepsvg',\n",
       "    'repo': 'https://github.com/alexandre01/deepsvg',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/k2hpd',\n",
       "  'name': 'K2HPD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.sysu-hcp.net/kinect2-human-pose-dataset-k2hpd/',\n",
       "  'description': 'Includes 100K depth images under challenging scenarios.\\r\\n\\r\\nSource: [Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning](/paper/human-pose-estimation-from-depth-images-via)',\n",
       "  'paper': {'title': 'Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/human-pose-estimation-from-depth-images-via'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'},\n",
       "   {'task': '3D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['K2HPD'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/binarized-mnist',\n",
       "  'name': 'Binarized MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.dmi.usherb.ca/~larocheh/mlpython/_modules/datasets/binarized_mnist.html',\n",
       "  'description': 'A binarized version of MNIST.\\r\\n\\r\\nSource: [Binarized MNIST](http://www.dmi.usherb.ca/~larocheh/mlpython/_modules/datasets/binarized_mnist.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Binarized MNIST'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/binarized_mnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/camo',\n",
       "  'name': 'CAMO',\n",
       "  'full_name': 'Camouflaged Object',\n",
       "  'homepage': 'https://sites.google.com/view/ltnghia/research/camo',\n",
       "  'description': 'Camouflaged Object (CAMO) dataset specifically designed for the task of camouflaged object segmentation. We focus on two categories, i.e., naturally camouflaged objects and artificially camouflaged objects, which usually correspond to animals and humans in the real world, respectively. Camouflaged object images consists of 1250 images (1000 images for the training set and 250 images for the testing set). Non-camouflaged object images are collected from the MS-COCO dataset (1000 images for the training set and 250 images for the testing set). CAMO has objectness mask ground-truth.',\n",
       "  'paper': {'title': 'Anabranch Network for Camouflaged Object Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/anabranch-network-for-camouflaged-object-1'},\n",
       "  'introduced_date': '2021-05-20',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Camouflaged Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/camouflaged-object-segmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CAMO'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lrw-1000',\n",
       "  'name': 'CAS-VSR-W1k (LRW-1000)',\n",
       "  'full_name': 'CAS-VSR-W1k (LRW-1000)',\n",
       "  'homepage': 'https://vipl.ict.ac.cn/en/view_database.php?id=13',\n",
       "  'description': '*LRW-1000 has been renamed as CAS-VSR-W1k.** It is a naturally-distributed large-scale benchmark for word-level lipreading in the wild, including 1000 classes with about 718,018 video samples from more than 2000 individual speakers. There are more than 1,000,000 Chinese character instances in total. Each class corresponds to the syllables of a Mandarin word which is composed by one or several Chinese characters. This dataset aims to cover a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications.\\r\\n\\r\\nSource: [VIPL](https://vipl.ict.ac.cn/en/view_database.php?id=13)\\r\\nImage Source: [https://arxiv.org/pdf/1810.06990v6.pdf](https://arxiv.org/pdf/1810.06990v6.pdf)',\n",
       "  'paper': {'title': 'LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/lrw-1000-a-naturally-distributed-large-scale'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Visual Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/visual-speech-recognition'},\n",
       "   {'task': 'Lipreading', 'url': 'https://paperswithcode.com/task/lipreading'},\n",
       "   {'task': 'Lip Reading',\n",
       "    'url': 'https://paperswithcode.com/task/lip-reading'},\n",
       "   {'task': 'Audio-Visual Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/audio-visual-speech-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CAS-VSR-W1k (LRW-1000)'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lrs2',\n",
       "  'name': 'LRS2',\n",
       "  'full_name': 'Lip Reading Sentences 2',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html',\n",
       "  'description': 'The Oxford-BBC **Lip Reading Sentences 2** (**LRS2**) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances.\\r\\n\\r\\nSource: [Audio-visual Recognition of Overlapped speech for the LRS2 dataset](https://arxiv.org/abs/2001.01656)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)',\n",
       "  'paper': {'title': 'Lip Reading Sentences in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/lip-reading-sentences-in-the-wild'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Visual Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/visual-speech-recognition'},\n",
       "   {'task': 'Lipreading', 'url': 'https://paperswithcode.com/task/lipreading'},\n",
       "   {'task': 'Unconstrained Lip-synchronization',\n",
       "    'url': 'https://paperswithcode.com/task/lip-sync'},\n",
       "   {'task': 'Visual Keyword Spotting',\n",
       "    'url': 'https://paperswithcode.com/task/visual-keyword-spotting'},\n",
       "   {'task': 'Image Manipulation',\n",
       "    'url': 'https://paperswithcode.com/task/image-manipulation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LRS2'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/moving-mnist',\n",
       "  'name': 'Moving MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cs.toronto.edu/~nitish/unsupervised_video/',\n",
       "  'description': 'The **Moving MNIST** dataset contains 10,000 video sequences, each consisting of 20 frames. In each video sequence, two digits move independently around the frame, which has a spatial resolution of 64×64 pixels. The digits frequently intersect with each other and bounce off the edges of the frame\\r\\n\\r\\nSource: [Mutual Suppression Network for Video Prediction using Disentangled Features](https://arxiv.org/abs/1804.04810)\\r\\nImage Source: [http://www.cs.toronto.edu/~nitish/unsupervised_video/](http://www.cs.toronto.edu/~nitish/unsupervised_video/)',\n",
       "  'paper': {'title': 'Unsupervised Learning of Video Representations using LSTMs',\n",
       "   'url': 'https://paperswithcode.com/paper/unsupervised-learning-of-video'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Moving MNIST'],\n",
       "  'num_papers': 134,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/moving_mnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sprites',\n",
       "  'name': 'Sprites',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/YingzhenLi/Sprites',\n",
       "  'description': 'The **Sprites** dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon.\\r\\n\\r\\nSource: [Challenges in Disentangling Independent Factors of Variation](https://arxiv.org/abs/1711.02245)',\n",
       "  'paper': {'title': 'Deep Visual Analogy-Making',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-visual-analogy-making'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/imputation'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Colored dSprites', 'Sprites'],\n",
       "  'num_papers': 50,\n",
       "  'data_loaders': [{'url': 'https://github.com/YingzhenLi/Sprites',\n",
       "    'repo': 'https://github.com/YingzhenLi/Sprites',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bigpatent',\n",
       "  'name': 'BigPatent',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://evasharma.github.io/bigpatent/',\n",
       "  'description': 'Consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries.\\r\\n\\r\\nSource: [BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization](https://arxiv.org/pdf/1906.03741v1.pdf)',\n",
       "  'paper': {'title': 'BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization',\n",
       "   'url': 'https://paperswithcode.com/paper/bigpatent-a-large-scale-dataset-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BigPatent'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/big_patent',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/big_patent',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/now-benchmark',\n",
       "  'name': 'NoW Benchmark',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ringnet.is.tue.mpg.de/challenge.html',\n",
       "  'description': 'The goal of this benchmark is to introduce a standard evaluation metric to measure the accuracy and robustness of 3D face reconstruction methods under variations in viewing angle, lighting, and common occlusions. \\r\\n\\r\\nThe dataset contains 2054 2D images of 100 subjects, captured with an iPhone X, and a separate 3D head scan for each subject. This head scan serves as ground truth for the evaluation. The subjects are selected to contain variations in age, BMI, and sex (55 female, 45 male).',\n",
       "  'paper': {'title': 'Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-regress-3d-face-shape-and'},\n",
       "  'introduced_date': '2019-05-16',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3d meshes'],\n",
       "  'tasks': [{'task': '3D Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NoW Benchmark'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://github.com/soubhiksanyal/now_evaluation',\n",
       "    'repo': 'https://github.com/soubhiksanyal/now_evaluation',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikihow',\n",
       "  'name': 'WikiHow',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/mahnazkoupaee/WikiHow-Dataset',\n",
       "  'description': '**WikiHow** is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles.\\r\\n\\r\\nSource: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)\\r\\nImage Source: [WikiHow: A Large Scale Text Summarization Dataset](https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization/)',\n",
       "  'paper': {'title': 'WikiHow: A Large Scale Text Summarization Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/wikihow-a-large-scale-text-summarization'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WikiHow'],\n",
       "  'num_papers': 49,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wikihow',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wikihow',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/mahnazkoupaee/WikiHow-Dataset',\n",
       "    'repo': 'https://github.com/mahnazkoupaee/WikiHow-Dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/horse-10',\n",
       "  'name': 'Horse-10',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://horse10.deeplabcut.org',\n",
       "  'description': '**Horse-10** is an animal pose estimation dataset. It comprises 30 diverse Thoroughbred horses, for which 22 body parts were labeled by an expert in *8,114* frames (animal pose estimation). Horses have various coat colors and the “in-the-wild” aspect of the collected data at various Thoroughbred yearling sales and farms added additional complexity.  The authors introduce Horse-C to contrast the domain shift inherent in the Horse-10 dataset with domain shift induced by common image corruptions.',\n",
       "  'paper': {'title': 'Pretraining boosts out-of-domain robustness for pose estimation',\n",
       "   'url': 'https://paperswithcode.com/paper/pretraining-boosts-out-of-domain-robustness'},\n",
       "  'introduced_date': '2019-09-24',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Animal Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/animal-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Horse-10'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_animal_keypoint.md#horse-10',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/freihand',\n",
       "  'name': 'FreiHAND',\n",
       "  'full_name': 'FreiHAND',\n",
       "  'homepage': 'https://lmb.informatik.uni-freiburg.de/projects/freihand/',\n",
       "  'description': '**FreiHAND** is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples.\\r\\n\\r\\nSource: [Knowledge as Priors: Cross-Modal Knowledge Generalizationfor Datasets without Superior Knowledge](https://arxiv.org/abs/2004.00176)\\r\\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html)',\n",
       "  'paper': {'title': 'FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape from Single RGB Images',\n",
       "   'url': 'https://paperswithcode.com/paper/freihand-a-dataset-for-markerless-capture-of'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FreiHAND'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_hand_keypoint.md#freihand-dataset',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/domainnet',\n",
       "  'name': 'DomainNet',\n",
       "  'full_name': 'DomainNet',\n",
       "  'homepage': 'http://ai.bu.edu/M3SDA/',\n",
       "  'description': '**DomainNet** is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game “Quick Draw!”.\\r\\n\\r\\nSource: [What is being transferred in transfer learning?](https://arxiv.org/abs/2008.11687)\\r\\nImage Source: [http://ai.bu.edu/M3SDA/](http://ai.bu.edu/M3SDA/)',\n",
       "  'paper': {'title': 'Moment Matching for Multi-Source Domain Adaptation',\n",
       "   'url': 'https://paperswithcode.com/paper/moment-matching-for-multi-source-domain'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Multi-Source Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-source-unsupervised-domain-adaptation'},\n",
       "   {'task': 'Partial Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/partial-domain-adaptation'},\n",
       "   {'task': 'Universal Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/universal-domain-adaptation'},\n",
       "   {'task': 'Multi-target Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-target-domain-adaptation'},\n",
       "   {'task': 'Zero-Shot Learning + Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning-domain-generalization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DomainNet'],\n",
       "  'num_papers': 190,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/skeleton-mimetics',\n",
       "  'name': 'Skeleton-Mimetics',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/skelemoa/quovadis/tree/master/skeleton-mimetics',\n",
       "  'description': 'A dataset derived from the recently introduced Mimetics dataset.\\r\\n\\r\\nSource: [Quo Vadis, Skeleton Action Recognition ?](/paper/quo-vadis-skeleton-action-recognition)',\n",
       "  'paper': {'title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "   'url': 'https://paperswithcode.com/paper/quo-vadis-skeleton-action-recognition'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Skeleton-Mimetics'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/skelemoa/quovadis',\n",
       "    'repo': 'https://github.com/skelemoa/quovadis',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/universal-dependencies',\n",
       "  'name': 'Universal Dependencies',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://universaldependencies.org/',\n",
       "  'description': 'The **Universal Dependencies** (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels.\\r\\n\\r\\nSource: [Evaluating Contextualized Embeddings on 54 Languagesin POS Tagging, Lemmatization and Dependency Parsing](https://arxiv.org/abs/1908.07448)\\r\\nImage Source: [https://universaldependencies.org/introduction.html](https://universaldependencies.org/introduction.html)',\n",
       "  'paper': {'title': 'Universal Dependencies v1: A Multilingual Treebank Collection',\n",
       "   'url': 'https://paperswithcode.com/paper/universal-dependencies-v1-a-multilingual'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Dependency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/dependency-parsing'},\n",
       "   {'task': 'Part-Of-Speech Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/part-of-speech-tagging'},\n",
       "   {'task': 'Language Identification',\n",
       "    'url': 'https://paperswithcode.com/task/language-identification'},\n",
       "   {'task': 'Cross-lingual zero-shot dependency parsing',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-zero-shot-dependency-parsing'},\n",
       "   {'task': 'Cross-Lingual POS Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-pos-tagging'}],\n",
       "  'languages': ['English',\n",
       "   'French',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Italian',\n",
       "   'Chinese',\n",
       "   'Japanese',\n",
       "   'Russian',\n",
       "   'Portuguese',\n",
       "   'Afrikaans',\n",
       "   'Akkadian',\n",
       "   'Akuntsu',\n",
       "   'Albanian',\n",
       "   'Amharic',\n",
       "   'Ancient Greek',\n",
       "   'Apurinã',\n",
       "   'Arabic',\n",
       "   'Armenian',\n",
       "   'Assyrian Neo-Aramaic',\n",
       "   'Bambara',\n",
       "   'Basque',\n",
       "   'Belarusian',\n",
       "   'Bhojpuri',\n",
       "   'Breton',\n",
       "   'Bulgarian',\n",
       "   'Catalan',\n",
       "   'Chukot',\n",
       "   'Church Slavic',\n",
       "   'Coptic',\n",
       "   'Croatian',\n",
       "   'Czech',\n",
       "   'Danish',\n",
       "   'Dutch',\n",
       "   'Erzya',\n",
       "   'Estonian',\n",
       "   'Faroese',\n",
       "   'Finnish',\n",
       "   'Galician',\n",
       "   'Gothic',\n",
       "   'Hebrew',\n",
       "   'Hindi',\n",
       "   'Hungarian',\n",
       "   'Icelandic',\n",
       "   'Indonesian',\n",
       "   'Irish',\n",
       "   'Karelian',\n",
       "   'Kazakh',\n",
       "   'Khunsari',\n",
       "   'Komi-Permyak',\n",
       "   'Komi-Zyrian',\n",
       "   'Korean',\n",
       "   'Latin',\n",
       "   'Latvian',\n",
       "   'Literary Chinese',\n",
       "   'Lithuanian',\n",
       "   'Livvi',\n",
       "   'Maltese',\n",
       "   'Manx',\n",
       "   'Marathi',\n",
       "   'Mbyá Guaraní',\n",
       "   'Modern Greek',\n",
       "   'Moksha',\n",
       "   'Mundurukú',\n",
       "   'Nayini',\n",
       "   'Nigerian Pidgin',\n",
       "   'Northern Kurdish',\n",
       "   'Northern Sami',\n",
       "   'Norwegian',\n",
       "   'Old French',\n",
       "   'Old Russian',\n",
       "   'Old Turkish',\n",
       "   'Persian',\n",
       "   'Polish',\n",
       "   'Romanian',\n",
       "   'Russia Buriat',\n",
       "   'Sanskrit',\n",
       "   'Scottish Gaelic',\n",
       "   'Serbian',\n",
       "   'Skolt Sami',\n",
       "   'Slovak',\n",
       "   'Slovenian',\n",
       "   'Soi',\n",
       "   'South Levantine Arabic',\n",
       "   'Swedish',\n",
       "   'Swedish Sign Language',\n",
       "   'Swiss German',\n",
       "   'Tagalog',\n",
       "   'Tamil',\n",
       "   'Telugu',\n",
       "   'Thai',\n",
       "   'Tupinambá',\n",
       "   'Turkish',\n",
       "   'Uighur',\n",
       "   'Ukrainian',\n",
       "   'Upper Sorbian',\n",
       "   'Urdu',\n",
       "   'Vietnamese',\n",
       "   'Warlpiri',\n",
       "   'Welsh',\n",
       "   'Wolof',\n",
       "   'Yoruba',\n",
       "   'Yue Chinese'],\n",
       "  'variants': ['UD',\n",
       "   'Universal Dependencies',\n",
       "   'Universal Dependency Treebank',\n",
       "   'Universal Dependencies v2.8'],\n",
       "  'num_papers': 428,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/universal_dependencies',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/structured_prediction/dataset_readers/universal_dependencies/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tallyqa',\n",
       "  'name': 'TallyQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.manojacharya.com/tallyqa.html',\n",
       "  'description': 'TallyQA is a large-scale dataset for open-ended counting.\\r\\n\\r\\nSource: [TallyQA: Answering Complex Counting Questions](/paper/tallyqa-answering-complex-counting-questions)',\n",
       "  'paper': {'title': 'TallyQA: Answering Complex Counting Questions',\n",
       "   'url': 'https://paperswithcode.com/paper/tallyqa-answering-complex-counting-questions'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TallyQA'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/crisismmd',\n",
       "  'name': 'CrisisMMD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://crisisnlp.qcri.org/crisismmd.html',\n",
       "  'description': 'CrisisMMD is a large multi-modal dataset collected from Twitter during different natural disasters. It consists of several thousands of manually annotated tweets and images collected during seven major natural disasters including earthquakes, hurricanes, wildfires, and floods that happened in the year 2017 across different parts of the World. The provided datasets include three types of annotations.',\n",
       "  'paper': {'title': 'CrisisMMD: Multimodal Twitter Datasets from Natural Disasters',\n",
       "   'url': 'https://paperswithcode.com/paper/crisismmd-multimodal-twitter-datasets-from'},\n",
       "  'introduced_date': '2018-05-02',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Informativeness',\n",
       "    'url': 'https://paperswithcode.com/task/informativeness'},\n",
       "   {'task': 'Disaster Response',\n",
       "    'url': 'https://paperswithcode.com/task/disaster-response'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CrisisMMD'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/uava',\n",
       "  'name': 'UAVA',\n",
       "  'full_name': 'UAV Assistant',\n",
       "  'homepage': 'https://vcl3d.github.io/UAVA/',\n",
       "  'description': 'The UAVA,<i>UAV-Assistant</i>, dataset is specifically designed for fostering applications which consider UAVs and humans as cooperative agents.\\r\\nWe employ a real-world 3D scanned dataset (<a href=\"https://niessner.github.io/Matterport/\">Matterport3D</a>), physically-based rendering, a gamiﬁed simulator for realistic drone navigation trajectory collection, to generate realistic multimodal data both from the user’s exocentric view of the drone, as well as the drone’s egocentric view.',\n",
       "  'paper': {'title': 'DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss',\n",
       "   'url': 'https://paperswithcode.com/paper/dronepose-photorealistic-uav-assistant'},\n",
       "  'introduced_date': '2020-08-20',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D', '6D', 'Tracking'],\n",
       "  'tasks': [{'task': 'Drone Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/drone-pose-estimation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['UAVA'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': [{'url': 'https://github.com/VCL3D/UAVA',\n",
       "    'repo': 'https://github.com/VCL3D/UAVA',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cmu-panoptic',\n",
       "  'name': 'CMU Panoptic',\n",
       "  'full_name': 'CMU Panoptic Dataset',\n",
       "  'homepage': 'http://domedb.perception.cs.cmu.edu/',\n",
       "  'description': '**CMU Panoptic** is a large scale dataset providing 3D pose annotations for multiple people engaging social activities. It contains 65 videos with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters.\\r\\n\\r\\nSource: [Single-Stage Multi-Person Pose Machines](https://arxiv.org/abs/1908.09220)\\r\\nImage Source: [http://domedb.perception.cs.cmu.edu/](http://domedb.perception.cs.cmu.edu/)',\n",
       "  'paper': {'title': 'Panoptic Studio: A Massively Multiview System for Social Motion Capture',\n",
       "   'url': 'https://paperswithcode.com/paper/panoptic-studio-a-massively-multiview-system-1'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': '3D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-pose-estimation'},\n",
       "   {'task': '3D Human Pose Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CMU Panoptic'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_hand_keypoint.md#cmu-panoptic-handdb',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/set5',\n",
       "  'name': 'Set5',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html',\n",
       "  'description': 'The **Set5** dataset is a dataset consisting of 5 images (“baby”, “bird”, “butterfly”, “head”, “woman”) commonly used for testing performance of Image Super-Resolution models.\\r\\nImage Source: [http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html)',\n",
       "  'paper': {'title': 'Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding',\n",
       "   'url': 'https://doi.org/10.5244/C.26.135'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Compressive Sensing',\n",
       "    'url': 'https://paperswithcode.com/task/compressive-sensing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Set5-2x',\n",
       "   'Set5 - 8x upscaling',\n",
       "   'Set5 - 4x upscaling',\n",
       "   'Set5 - 3x upscaling',\n",
       "   'Set5 - 2x upscaling',\n",
       "   'Set5'],\n",
       "  'num_papers': 258,\n",
       "  'data_loaders': [{'url': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'repo': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/contactpose',\n",
       "  'name': 'ContactPose',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/facebookresearch/ContactPose',\n",
       "  'description': 'ContactPose is a dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. \\r\\n\\r\\nSource: [ContactPose: A Dataset of Grasps with Object Contact and Hand Pose](/paper/contactpose-a-dataset-of-grasps-with-object)',\n",
       "  'paper': {'title': 'ContactPose: A Dataset of Grasps with Object Contact and Hand Pose',\n",
       "   'url': 'https://paperswithcode.com/paper/contactpose-a-dataset-of-grasps-with-object'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Grasp Contact Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/human-grasp-contact-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ContactPose'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://github.com/facebookresearch/ContactPose',\n",
       "    'repo': 'https://github.com/facebookresearch/ContactPose',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dhf1k',\n",
       "  'name': 'DHF1K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/wenguanwang/DHF1K',\n",
       "  'description': '**DHF1K** is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server.\\r\\n\\r\\nSource: [ViP: Video Platform for PyTorch](https://arxiv.org/abs/1910.02793)\\r\\nImage Source: [https://arxiv.org/pdf/1801.07424.pdf](https://arxiv.org/pdf/1801.07424.pdf)',\n",
       "  'paper': {'title': 'Revisiting Video Saliency: A Large-scale Benchmark and a New Model',\n",
       "   'url': 'https://paperswithcode.com/paper/revisiting-video-saliency-a-large-scale'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Video Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-saliency-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DHF1K'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': [{'url': 'https://github.com/wenguanwang/DHF1K',\n",
       "    'repo': 'https://github.com/wenguanwang/DHF1K',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/how2',\n",
       "  'name': 'How2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://srvk.github.io/how2-dataset/',\n",
       "  'description': 'The **How2** dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations.\\r\\n\\r\\nSource: [exploring multiview correlations in open-domain videos](https://arxiv.org/abs/1811.08890)',\n",
       "  'paper': {'title': 'How2: A Large-scale Dataset for Multimodal Language Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/how2-a-large-scale-dataset-for-multimodal'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Multimodal Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-abstractive-text-summarization'}],\n",
       "  'languages': ['English', 'Portuguese'],\n",
       "  'variants': ['How2', 'How2 300h'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/asset',\n",
       "  'name': 'ASSET',\n",
       "  'full_name': 'ASSET',\n",
       "  'homepage': 'https://github.com/facebookresearch/asset',\n",
       "  'description': 'ASSET is a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.\\r\\n\\r\\nSource: [ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations](https://arxiv.org/pdf/2005.00481v1.pdf)',\n",
       "  'paper': {'title': 'ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations',\n",
       "   'url': 'https://paperswithcode.com/paper/asset-a-dataset-for-tuning-and-evaluation-of'},\n",
       "  'introduced_date': '2020-05-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Simplification',\n",
       "    'url': 'https://paperswithcode.com/task/text-simplification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ASSET'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/asset',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/facebookresearch/asset',\n",
       "    'repo': 'https://github.com/facebookresearch/asset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/turkcorpus',\n",
       "  'name': 'TurkCorpus',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/cocoxu/simplification/tree/master/data/turkcorpus/GEM',\n",
       "  'description': 'TurkCorpus, a dataset with 2,359 original sentences from English Wikipedia, each with 8 manual reference simplifications.\\r\\nThe dataset is divided into two subsets: 2,000 sentences for validation and 359 for testing of sentence simplification models.',\n",
       "  'paper': {'title': 'Optimizing Statistical Machine Translation for Text Simplification',\n",
       "   'url': 'https://paperswithcode.com/paper/optimizing-statistical-machine-translation'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Simplification',\n",
       "    'url': 'https://paperswithcode.com/task/text-simplification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TurkCorpus'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://github.com/cocoxu/simplification',\n",
       "    'repo': 'https://github.com/cocoxu/simplification',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mlfp',\n",
       "  'name': 'MLFP',\n",
       "  'full_name': 'Multispectral Latex Mask based Video Face Presentation Attack',\n",
       "  'homepage': 'http://iab-rubric.org/resources/mlfp.html',\n",
       "  'description': 'The **MLFP** dataset consists of face presentation attacks captured with seven 3D latex masks and three 2D print attacks. The dataset contains videos captured from color, thermal and infrared channels.\\n\\nSource: [Learning One Class Representations for Face Presentation Attack Detection using Multi-channel Convolutional Neural Networks](https://arxiv.org/abs/2007.11457)\\nImage Source: [http://iab-rubric.org/papers/2017_cvprw_18.pdf](http://iab-rubric.org/papers/2017_cvprw_18.pdf)',\n",
       "  'paper': {'title': 'Face Presentation Attack with Latex Masks in Multispectral Videos',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2017.40'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Anti-Spoofing',\n",
       "    'url': 'https://paperswithcode.com/task/face-anti-spoofing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MLFP'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/conll',\n",
       "  'name': 'CoNLL++',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/ZihanWangKi/CrossWeigh',\n",
       "  'description': 'CoNLL++ is a corrected version of the CoNLL03 NER dataset where 5.38% of the test sentences have been fixed.\\r\\n\\r\\nSource: [CrossWeigh: Training Named Entity Tagger from Imperfect Annotations](/paper/crossweigh-training-named-entity-tagger-from)',\n",
       "  'paper': {'title': 'CrossWeigh: Training Named Entity Tagger from Imperfect Annotations',\n",
       "   'url': 'https://paperswithcode.com/paper/crossweigh-training-named-entity-tagger-from'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Weakly-Supervised Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-named-entity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CoNLL03', 'CoNLL++'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/conllpp',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/ncats/EpiSet4NER',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/ZihanWangKi/CrossWeigh',\n",
       "    'repo': 'https://github.com/ZihanWangKi/CrossWeigh',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/soc',\n",
       "  'name': 'SOC',\n",
       "  'full_name': 'Salient Objects in Clutter',\n",
       "  'homepage': 'http://dpfan.net/SOCBenchmark/',\n",
       "  'description': 'SOC (Salient Objects in Clutter) is a dataset for Salient Object Detection (SOD). It includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes.',\n",
       "  'paper': {'title': 'Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground',\n",
       "   'url': 'https://paperswithcode.com/paper/salient-objects-in-clutter-bringing-salient'},\n",
       "  'introduced_date': '2018-03-16',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SOC'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cosal2015',\n",
       "  'name': 'CoSal2015',\n",
       "  'full_name': 'CoSal2015',\n",
       "  'homepage': None,\n",
       "  'description': 'Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection.\\n\\nSource: [Adaptive Graph Convolutional Network with Attention Graph Clustering for Co-saliency Detection](https://arxiv.org/abs/2003.06167)\\nImage Source: [https://arxiv.org/pdf/1604.07090.pdf](https://arxiv.org/pdf/1604.07090.pdf)',\n",
       "  'paper': {'title': 'Detection of Co-salient Objects by Looking Deep and Wide',\n",
       "   'url': 'https://doi.org/10.1007/s11263-016-0907-4'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Co-Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/co-saliency-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CoSal2015'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sip',\n",
       "  'name': 'SIP',\n",
       "  'full_name': 'Salient Person',\n",
       "  'homepage': 'http://dpfan.net/sipdataset/',\n",
       "  'description': 'The **Salient Person** dataset (**SIP**) contains 929 salient person samples with different poses and illumination conditions.\\n\\nSource: [Accurate RGB-D Salient Object Detection via Collaborative Learning](https://arxiv.org/abs/2007.11782)\\nImage Source: [https://arxiv.org/pdf/1907.06781.pdf](https://arxiv.org/pdf/1907.06781.pdf)',\n",
       "  'paper': {'title': 'Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks',\n",
       "   'url': 'https://paperswithcode.com/paper/rethinking-rgb-d-salient-object-detection'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB-D Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/rgb-d-salient-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SIP'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nju2k',\n",
       "  'name': 'NJU2K',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://drive.google.com/open?id=1R1O2dWr6HqpTOiDn6hZxUWTesOSJteQo',\n",
       "  'description': '**NJU2K** is a large RGB-D dataset containing 1,985 image pairs. The stereo images were collected from the Internet and 3D movies, while photographs were taken by a Fuji W3 camera.\\n\\nSource: [Bifurcated Backbone Strategy for RGB-D Salient Object Detection](https://arxiv.org/abs/2007.02713)\\nImage Source: [Depth saliency based on anisotropic center-surround difference](https://doi.org/10.1109/ICIP.2014.7025222)',\n",
       "  'paper': {'title': 'Depth saliency based on anisotropic center-surround difference',\n",
       "   'url': 'https://doi.org/10.1109/ICIP.2014.7025222'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB-D Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/rgb-d-salient-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NJU2K'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nlpr',\n",
       "  'name': 'NLPR',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://sites.google.com/site/rgbdsaliency/home',\n",
       "  'description': 'The **NLPR** dataset for salient object detection consists of 1,000 image pairs captured by a standard Microsoft Kinect with a resolution of 640×480. The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets).\\r\\n\\r\\nSource: [Bifurcated Backbone Strategy for RGB-D Salient Object Detection](https://arxiv.org/abs/2007.02713)\\r\\nImage Source: [https://sites.google.com/site/rgbdsaliency/dataset](https://sites.google.com/site/rgbdsaliency/dataset)',\n",
       "  'paper': {'title': 'RGBD Salient Object Detection: A Benchmark and Algorithms',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-10578-9_7'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB-D Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/rgb-d-salient-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NLPR'],\n",
       "  'num_papers': 72,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/des',\n",
       "  'name': 'DES',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\n\\nProvide:\\n\\n* a high-level explanation of the dataset characteristics\\n* explain motivations and summary of its content\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'RGB-D Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/rgb-d-salient-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lfsd',\n",
       "  'name': 'LFSD',\n",
       "  'full_name': 'Light Field Saliency Database',\n",
       "  'homepage': 'https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/',\n",
       "  'description': 'The **Light Field Saliency Database** (**LFSD**) contains 100 light fields with 360×360 spatial resolution. A rough focal stack and an all-focus image are provided for each light field. The images in this dataset usually have one salient foreground object and a background with good color contrast.\\r\\n\\r\\nSource: [Light Field Saliency Detection with Deep Convolutional Networks](https://arxiv.org/abs/1906.08331)\\r\\nImage Source: [https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/](https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/)',\n",
       "  'paper': {'title': 'Saliency Detection on Light Field',\n",
       "   'url': 'https://paperswithcode.com/paper/saliency-detection-on-light-field'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB-D Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/rgb-d-salient-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LFSD'],\n",
       "  'num_papers': 58,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cam2bev',\n",
       "  'name': 'Cam2BEV',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/ika-rwth-aachen/Cam2BEV',\n",
       "  'description': 'The [dataset](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data) contains two subsets of synthetic, semantically segmented road-scene images, which have been created for developing and applying the methodology described in the paper **\"A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View\"** ([IEEE Xplore](https://ieeexplore.ieee.org/document/9294462), [arXiv](http://arxiv.org/abs/2005.04078), [YouTube](https://www.youtube.com/watch?v=TzXuwt56a0E))\\r\\n\\r\\nThe dataset can be used through the official code implementation of the Cam2BEV methodology described on [Github](https://github.com/ika-rwth-aachen/Cam2BEV).\\r\\n\\r\\n\\r\\n| Dataset | # Training Samples | # Validation Samples | # Vehicle Cameras | # Semantic Classes | Contained Images (examples) |\\r\\n| --- | --- | --- | --- | --- | --- |\\r\\n| [Dataset 1](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/2_F): 360° Surround | 33199 | 3731 | 4 (front, rear, left, right) | 30 (CityScapes) | [front camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/front.png), [rear camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/rear.png), [left camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/left.png), [right camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/right.png), [bird\\'s eye view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/bev.png), [bird\\'s eye view incl. occlusion](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples bev+occlusion.png), [homography view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/1_FRLR/examples/homography.png) |\\r\\n| [Dataset 2](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/tree/master/2_F): Front Camera only | 32246 | 3172 | 1 (front) | 30 (CityScapes) | [front camera](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/front.png), [bird\\'s eye view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/bev.png), [bird\\'s eye view incl. occlusion](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/bev+occlusion.png), [homography view](https://gitlab.ika.rwth-aachen.de/cam2bev/cam2bev-data/-/raw/master/2_F/examples/homography.png) |',\n",
       "  'paper': {'title': \"A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's Eye View\",\n",
       "   'url': 'https://paperswithcode.com/paper/a-sim2real-deep-learning-approach-for-the'},\n",
       "  'introduced_date': '2020-05-08',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Cross-View Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/cross-view-image-to-image-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cam2BEV'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/ika-rwth-aachen/Cam2BEV',\n",
       "    'repo': 'https://github.com/ika-rwth-aachen/Cam2BEV',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sstem',\n",
       "  'name': 'ssTEM',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/unidesigner/groundtruth-drosophila-vnc/archive/master.zip',\n",
       "  'description': 'We provide two image stacks where each contains 20 sections from serial section Transmission Electron Microscopy (ssTEM) of the Drosophila melanogaster third instar larva ventral nerve cord. Both stacks measure approx. 4.7 x 4.7 x 1 microns with a resolution of 4.6 x 4.6 nm/pixel and section thickness of 45-50 nm.\\r\\n\\r\\nIn addition to the raw image data, we provide for the first stack a dense labeling of neuron membranes (including orientation and junction), mitochondria, synapses and glia/extracellular space. The first stack serves as a training dataset, and a second stack of the same dimension can be used as a test dataset.\\r\\n\\r\\nSource: [https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713](https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713)\\r\\n\\r\\nImage source:  [https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713](https://figshare.com/articles/dataset/Segmented_anisotropic_ssTEM_dataset_of_neural_tissue/856713)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ssTEM'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/veri-776',\n",
       "  'name': 'VeRi-776',\n",
       "  'full_name': 'VeRi-776',\n",
       "  'homepage': 'https://vehiclereid.github.io/VeRi/',\n",
       "  'description': '**VeRi-776** is a vehicle re-identification dataset which contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow. The dataset contains bounding boxes, types, colors and brands.\\r\\n\\r\\nSource: [VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification](https://arxiv.org/abs/2004.06305)\\r\\nImage Source: [https://vehiclereid.github.io/VeRi/](https://vehiclereid.github.io/VeRi/)',\n",
       "  'paper': {'title': 'A Deep Learning-Based Approach to Progressive Vehicle Re-identification for Urban Surveillance',\n",
       "   'url': 'https://doi.org/10.1007/978-3-319-46475-6_53'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Vehicle Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/vehicle-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VeRi-776'],\n",
       "  'num_papers': 48,\n",
       "  'data_loaders': [{'url': 'https://github.com/michuanhaohao/reid-strong-baseline',\n",
       "    'repo': 'https://github.com/michuanhaohao/reid-strong-baseline',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/unsw-nb15',\n",
       "  'name': 'UNSW-NB15',\n",
       "  'full_name': 'UNSQ-NB15',\n",
       "  'homepage': 'https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/',\n",
       "  'description': '**UNSW-NB15** is a network intrusion dataset. It contains nine different attacks, includes DoS, worms, Backdoors, and Fuzzers. The dataset contains raw network packets. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal.\\r\\n\\r\\nSource: [Evaluation of Adversarial Training on Different Types of Neural Networks in Deep Learning-based IDSs](https://arxiv.org/abs/2007.04472)\\r\\nImage Source: [https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/)\\r\\nPaper: [UNSW-NB15: a comprehensive data set for network intrusion detection systems](https://doi.org/10.1109/MilCIS.2015.7348942)',\n",
       "  'paper': {'title': 'UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)',\n",
       "   'url': 'https://doi.org/10.1109/MilCIS.2015.7348942'},\n",
       "  'introduced_date': '2015-11-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Tabular'],\n",
       "  'tasks': [{'task': 'Intrusion Detection',\n",
       "    'url': 'https://paperswithcode.com/task/intrusion-detection'},\n",
       "   {'task': 'Network Intrusion Detection',\n",
       "    'url': 'https://paperswithcode.com/task/network-intrusion-detection'}],\n",
       "  'languages': ['Russian'],\n",
       "  'variants': ['UNSW-NB15'],\n",
       "  'num_papers': 59,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/farstail',\n",
       "  'name': 'FarsTail',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/dml-qom/FarsTail',\n",
       "  'description': 'Natural Language Inference (NLI), also called Textual Entailment, is an important task in NLP with the goal of determining the inference relationship between a premise p and a hypothesis h. It is a three-class problem, where each pair (p, h) is assigned to one of these classes: \"ENTAILMENT\" if the hypothesis can be inferred from the premise, \"CONTRADICTION\" if the hypothesis contradicts the premise, and \"NEUTRAL\" if none of the above holds. There are large datasets such as SNLI, MNLI, and SciTail for NLI in English, but there are few datasets for poor-data languages like Persian. Persian (Farsi) language is a pluricentric language spoken by around 110 million people in countries like Iran, Afghanistan, and Tajikistan. **FarsTail** is the first relatively large-scale Persian dataset for NLI task. A total of 10,367 samples are generated from a collection of 3,539 multiple-choice questions. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances, respectively.\\r\\n\\r\\nSource: [https://github.com/dml-qom/FarsTail](https://github.com/dml-qom/FarsTail)\\r\\nImage Source: [https://github.com/dml-qom/FarsTail](https://github.com/dml-qom/FarsTail)',\n",
       "  'paper': {'title': 'FarsTail: A Persian Natural Language Inference Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/farstail-a-persian-natural-language-inference'},\n",
       "  'introduced_date': '2020-09-18',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': ['Persian'],\n",
       "  'variants': ['FarsTail'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://github.com/dml-qom/FarsTail',\n",
       "    'repo': 'https://github.com/dml-qom/FarsTail',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cuhk-pedes',\n",
       "  'name': 'CUHK-PEDES',\n",
       "  'full_name': 'CUHK-PEDES',\n",
       "  'homepage': 'https://github.com/layumi/Image-Text-Embedding/tree/master/dataset/CUHK-PEDES-prepare',\n",
       "  'description': 'The **CUHK-PEDES** dataset is a caption-annotated pedestrian dataset. It contains 40,206 images over 13,003 persons. Images are collected from five existing person re-identification datasets, CUHK03, Market-1501, SSM, VIPER, and CUHK01 while each image is annotated with 2 text descriptions by crowd-sourcing workers. Sentences incorporate rich details about person appearances, actions, poses.\\r\\n\\r\\nSource: [MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination](https://arxiv.org/abs/2010.00947)\\r\\nImage Source: [https://www.researchgate.net/figure/Image-samples-in-three-datasets-For-MSCOCO-and-Flickr30k-dataset-we-view-every-image_fig2_321095980](https://www.researchgate.net/figure/Image-samples-in-three-datasets-For-MSCOCO-and-Flickr30k-dataset-we-view-every-image_fig2_321095980)',\n",
       "  'paper': {'title': 'Person Search with Natural Language Description',\n",
       "   'url': 'https://paperswithcode.com/paper/person-search-with-natural-language'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Text based Person Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/nlp-based-person-retrival'},\n",
       "   {'task': 'NLP based Person Retrival',\n",
       "    'url': 'https://paperswithcode.com/task/nlp-based-person-retrival-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CUHK-PEDES'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': [{'url': 'https://github.com/layumi/Image-Text-Embedding',\n",
       "    'repo': 'https://github.com/layumi/Image-Text-Embedding',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/and-dataset',\n",
       "  'name': 'AND Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/mshaikh2/HDL_Forensics',\n",
       "  'description': 'The **AND Dataset** contains 13700 handwritten samples and 15 corresponding expert examined features for each sample. The dataset is released for public use and the methods can be extended to provide explanations on other verification tasks like face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification.\\r\\n\\r\\nSource: [Explanation based Handwriting Verification](https://paperswithcode.com/paper/explanation-based-handwriting-verification)',\n",
       "  'paper': {'title': 'Explanation based Handwriting Verification',\n",
       "   'url': 'https://paperswithcode.com/paper/explanation-based-handwriting-verification'},\n",
       "  'introduced_date': '2019-08-14',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Handwriting Verification',\n",
       "    'url': 'https://paperswithcode.com/task/handwriting-verification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['AND Dataset'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/mshaikh2/HDL_Forensics',\n",
       "    'repo': 'https://github.com/mshaikh2/HDL_Forensics',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/multi-dsprites',\n",
       "  'name': 'Multi-dSprites',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/deepmind/multi_object_datasets',\n",
       "  'description': '',\n",
       "  'paper': {'title': 'MONet: Unsupervised Scene Decomposition and Representation',\n",
       "   'url': 'https://paperswithcode.com/paper/monet-unsupervised-scene-decomposition-and'},\n",
       "  'introduced_date': '2019-01-22',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Unsupervised Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-object-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Multi-dSprites'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/object-discovery',\n",
       "  'name': 'Object Discovery',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://people.csail.mit.edu/mrub/ObjectDiscovery/',\n",
       "  'description': 'The **Object Discovery** dataset was collected by downloading images from Internet for airplane, car and horse. It is significantly larger and thus, diverse in terms of viewpoints, texture, color etc\\r\\n\\r\\nSource: [One shot Joint Colocalization & Cosegmentation](https://arxiv.org/abs/1705.06000)\\r\\nImage Source: [http://people.csail.mit.edu/mrub/ObjectDiscovery/](http://people.csail.mit.edu/mrub/ObjectDiscovery/)',\n",
       "  'paper': {'title': 'Unsupervised Joint Object Discovery and Segmentation in Internet Images',\n",
       "   'url': 'https://paperswithcode.com/paper/unsupervised-joint-object-discovery-and'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Single-object discovery',\n",
       "    'url': 'https://paperswithcode.com/task/single-object-discovery'},\n",
       "   {'task': 'Single-object colocalization',\n",
       "    'url': 'https://paperswithcode.com/task/single-object-colocalization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Object Discovery'],\n",
       "  'num_papers': 45,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/open-entity-1',\n",
       "  'name': 'Open Entity',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cs.utexas.edu/~eunsol/html_pages/open_entity.html',\n",
       "  'description': 'The **Open Entity** dataset is a collection of about 6,000 sentences with fine-grained entity types annotations. The entity types are free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Sentences were sampled from Gigaword, OntoNotes and web articles. On average each sentence has 5 labels.\\r\\n\\r\\nSource: [Ultra-Fine Entity Typing](https://paperswithcode.com/paper/ultra-fine-entity-typing/)\\r\\nImage Source: [Ultra-Fine Entity Typing](https://paperswithcode.com/paper/ultra-fine-entity-typing/)',\n",
       "  'paper': {'title': 'Ultra-Fine Entity Typing',\n",
       "   'url': 'https://paperswithcode.com/paper/ultra-fine-entity-typing'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Entity Typing',\n",
       "    'url': 'https://paperswithcode.com/task/entity-typing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': [' Open Entity', 'Open Entity'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rite',\n",
       "  'name': 'RITE',\n",
       "  'full_name': 'Retinal Images vessel Tree Extraction',\n",
       "  'homepage': 'https://medicine.uiowa.edu/eye/rite-dataset',\n",
       "  'description': 'The RITE (Retinal Images vessel Tree Extraction) is a database that enables comparative studies on segmentation or classification of arteries and veins on retinal fundus images, which is established based on the public available DRIVE database (Digital Retinal Images for Vessel Extraction).\\r\\n\\r\\nRITE contains 40 sets of images, equally separated into a training subset and a test subset, the same as DRIVE. The two subsets are built from the corresponding two subsets in DRIVE. For each set, there is a fundus photograph, a vessel reference standard, and a Arteries/Veins (A/V) reference standard. \\r\\n\\r\\n* The fundus photograph is inherited from DRIVE. \\r\\n* For the training set, the vessel reference standard is a modified version of 1st_manual from DRIVE. \\r\\n* For the test set, the vessel reference standard is 2nd_manual from DRIVE. \\r\\n* For the A/V reference standard, four types of vessels are labelled using four colors based on the vessel reference standard. \\r\\n* Arteries are labelled in red; veins are labelled in blue; the overlapping of arteries and veins are labelled in green; the vessels which are uncertain are labelled in white. \\r\\n* The fundus photograph is in tif format. And the vessel reference standard and the A/V reference standard are in png format.  \\r\\n\\r\\nThe dataset is described in more detail in our paper, which you will cite if you use the dataset in any way: \\r\\n\\r\\n Hu Q, Abràmoff MD, Garvin MK. Automated separation of binary overlapping trees in low-contrast color retinal images. Med Image Comput Comput Assist Interv. 2013;16(Pt 2):436-43. PubMed PMID: 24579170 https://doi.org/10.1007/978-3-642-40763-5_54',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RITE'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/contract-discovery',\n",
       "  'name': 'Contract Discovery',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/applicaai/contract-discovery',\n",
       "  'description': 'A new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts.\\r\\n\\r\\nSource: [Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines](/paper/searching-for-legal-clauses-by-analogy-few)',\n",
       "  'paper': {'title': 'Contract Discovery: Dataset and a Few-Shot Semantic Retrieval Challenge with Competitive Baselines',\n",
       "   'url': 'https://paperswithcode.com/paper/searching-for-legal-clauses-by-analogy-few'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Semantic Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Contract Discovery'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/applicaai/contract-discovery',\n",
       "    'repo': 'https://github.com/applicaai/contract-discovery',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ui-prmd',\n",
       "  'name': 'UI-PRMD',\n",
       "  'full_name': 'University of Idaho – Physical Rehabilitation Movement Dataset',\n",
       "  'homepage': 'https://www.webpages.uidaho.edu/ui-prmd/',\n",
       "  'description': 'UI-PRMD is a data set of movements related to common exercises performed by patients in physical therapy and rehabilitation programs. The data set consists of 10 rehabilitation exercises. A sample of 10 healthy individuals repeated each exercise 10 times in front of two sensory systems for motion capturing: a Vicon optical tracker, and a Kinect camera. The data is presented as positions and angles of the body joints in the skeletal models provided by the Vicon and Kinect mocap systems.',\n",
       "  'paper': {'title': 'A Deep Learning Framework for Assessing Physical Rehabilitation Exercises',\n",
       "   'url': 'https://paperswithcode.com/paper/a-deep-learning-framework-for-assessing'},\n",
       "  'introduced_date': '2019-01-29',\n",
       "  'warning': None,\n",
       "  'modalities': ['Biomedical', 'Time series', 'Actions'],\n",
       "  'tasks': [{'task': 'Action Quality Assessment',\n",
       "    'url': 'https://paperswithcode.com/task/action-quality-assessment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UI-PRMD'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tvr',\n",
       "  'name': 'TVR',\n",
       "  'full_name': 'TV show Retrieval',\n",
       "  'homepage': 'https://github.com/jayleicn/TVRetrieval',\n",
       "  'description': 'A new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. \\r\\n\\r\\nSource: [TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval](/paper/tvr-a-large-scale-dataset-for-video-subtitle)',\n",
       "  'paper': {'title': 'TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval',\n",
       "   'url': 'https://paperswithcode.com/paper/tvr-a-large-scale-dataset-for-video-subtitle'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Video Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-retrieval'},\n",
       "   {'task': 'Video Corpus Moment Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/video-corpus-moment-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TVR'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': [{'url': 'https://github.com/jayleicn/TVRetrieval',\n",
       "    'repo': 'https://github.com/jayleicn/TVRetrieval',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tvqa',\n",
       "  'name': 'TVQA',\n",
       "  'full_name': 'TVQA',\n",
       "  'homepage': 'http://tvqa.cs.unc.edu/',\n",
       "  'description': \"The **TVQA** dataset is a large-scale vido dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer.\\r\\n\\r\\nSource: [Two-stream Spatiotemporal Feature for Video QA Task](https://arxiv.org/abs/1907.05006)\\r\\nImage Source: [https://arxiv.org/abs/1809.01696](https://arxiv.org/abs/1809.01696)\",\n",
       "  'paper': {'title': 'TVQA: Localized, Compositional Video Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/tvqa-localized-compositional-video-question'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts'],\n",
       "  'tasks': [{'task': 'Video Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/video-question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TVQA'],\n",
       "  'num_papers': 71,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dialogre',\n",
       "  'name': 'DialogRE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://dataset.org/dialogre/',\n",
       "  'description': '**DialogRE** is the first human-annotated dialogue-based relation extraction dataset, containing 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy Friends. The are annotations for all occurrences of 36 possible relation types that exist between an argument pair in a dialogue. DialogRE is available in English and Chinese.\\r\\n\\r\\nSource: [DialogRE](https://dataset.org/dialogre/)',\n",
       "  'paper': {'title': 'Dialogue-Based Relation Extraction',\n",
       "   'url': 'https://paperswithcode.com/paper/dialogue-based-relation-extraction'},\n",
       "  'introduced_date': '2020-04-17',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Dialog Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/dialog-relation-extraction'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['DialogRE'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/dialog_re',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tweebank',\n",
       "  'name': 'Tweebank',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/Oneplus/Tweebank',\n",
       "  'description': 'Briefly describe the dataset. Provide:\\r\\n\\r\\n* a high-level explanation of the dataset characteristics\\r\\n* explain motivations and summary of its content\\r\\n* potential use cases of the dataset\\r\\n\\r\\nIf the description or image is from a different paper, please refer to it as follows:\\r\\nSource: [title](url)\\r\\nImage Source: [title](url)',\n",
       "  'paper': {'title': 'Parsing Tweets into Universal Dependencies',\n",
       "   'url': 'https://paperswithcode.com/paper/parsing-tweets-into-universal-dependencies'},\n",
       "  'introduced_date': '2018-04-23',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Part-Of-Speech Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/part-of-speech-tagging'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tweebank'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scanobjectnn',\n",
       "  'name': 'ScanObjectNN',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://hkust-vgd.github.io/scanobjectnn/',\n",
       "  'description': '**ScanObjectNN** is a newly published real-world dataset comprising of 2902 3D objects in 15 categories. It is a challenging point cloud classification datasets due to the background, missing parts and deformations.\\r\\n\\r\\nSource: [A Self Contour-based Rotation and Translation-Invariant Transformation for Point Clouds Recognition](https://arxiv.org/abs/2009.06903)\\r\\nImage Source: [https://hkust-vgd.github.io/scanobjectnn/](https://hkust-vgd.github.io/scanobjectnn/)',\n",
       "  'paper': {'title': 'Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data',\n",
       "   'url': 'https://paperswithcode.com/paper/revisiting-point-cloud-classification-a-new'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud'],\n",
       "  'tasks': [{'task': '3D Point Cloud Classification',\n",
       "    'url': 'https://paperswithcode.com/task/3d-point-cloud-classification'},\n",
       "   {'task': '3D Point Cloud Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-point-cloud-data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ScanObjectNN'],\n",
       "  'num_papers': 68,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/coma',\n",
       "  'name': 'COMA',\n",
       "  'full_name': 'COMA',\n",
       "  'homepage': 'https://coma.is.tue.mpg.de/',\n",
       "  'description': 'CoMA contains 17,794 meshes of the human face in various expressions\\r\\n\\r\\nSource: [DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects](https://arxiv.org/abs/1905.10290)\\r\\nImage Source: [https://coma.is.tue.mpg.de/](https://coma.is.tue.mpg.de/)',\n",
       "  'paper': {'title': 'Generating 3D faces using Convolutional Mesh Autoencoders',\n",
       "   'url': 'https://paperswithcode.com/paper/generating-3d-faces-using-convolutional-mesh'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Interactive', '3d meshes'],\n",
       "  'tasks': [{'task': 'Graph Representation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-representation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['COMA'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/anuragranj/coma',\n",
       "    'repo': 'https://github.com/anuragranj/coma',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/told-br',\n",
       "  'name': 'ToLD-Br',\n",
       "  'full_name': 'Toxic Language Detection for Brazilian Portuguese',\n",
       "  'homepage': 'https://github.com/JAugusto97/ToLD-Br',\n",
       "  'description': 'The **Toxic Language Detection for Brazilian Portuguese** (**ToLD-Br**) is a dataset with tweets in Brazilian Portuguese annotated according to different toxic aspects.\\n\\nSource: [https://github.com/JAugusto97/ToLD-Br](https://github.com/JAugusto97/ToLD-Br)',\n",
       "  'paper': {'title': 'Toxic Language Detection in Social Media for Brazilian Portuguese: New Dataset and Multilingual Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/toxic-language-detection-in-social-media-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Hate Speech Detection',\n",
       "    'url': 'https://paperswithcode.com/task/hate-speech-detection'}],\n",
       "  'languages': ['Portuguese'],\n",
       "  'variants': ['ToLD-Br'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/told-br',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/JAugusto97/ToLD-Br',\n",
       "    'repo': 'https://github.com/JAugusto97/ToLD-Br',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mimic-cxr',\n",
       "  'name': 'MIMIC-CXR',\n",
       "  'full_name': 'MIMIC-CXR',\n",
       "  'homepage': 'https://physionet.org/content/mimic-cxr/2.0.0/',\n",
       "  'description': '**MIMIC-CXR** from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA.\\n\\nSource: [Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification](https://arxiv.org/abs/1909.01940)\\nImage Source: [https://arxiv.org/abs/1901.07042](https://arxiv.org/abs/1901.07042)',\n",
       "  'paper': {'title': 'MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs',\n",
       "   'url': 'https://paperswithcode.com/paper/mimic-cxr-a-large-publicly-available-database'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MIMIC-CXR'],\n",
       "  'num_papers': 35,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chexpert',\n",
       "  'name': 'CheXpert',\n",
       "  'full_name': 'CheXpert',\n",
       "  'homepage': 'https://stanfordmlgroup.github.io/competitions/chexpert/',\n",
       "  'description': 'The **CheXpert** dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets.\\r\\n\\r\\nSource: [Deep Mining External Imperfect Data for Chest X-ray Disease Screening](https://arxiv.org/abs/2006.03796)\\r\\nImage Source: [https://stanfordmlgroup.github.io/competitions/chexpert/](https://stanfordmlgroup.github.io/competitions/chexpert/)',\n",
       "  'paper': {'title': 'CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison',\n",
       "   'url': 'https://paperswithcode.com/paper/chexpert-a-large-chest-radiograph-dataset'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CheXpert'],\n",
       "  'num_papers': 211,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/chexpert',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/deepfix',\n",
       "  'name': 'DeepFix',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://bitbucket.org/iiscseal/deepfix',\n",
       "  'description': '**DeepFix** consists of a program repair dataset (fix compiler errors in C programs). It enables research around automatically fixing programming errors using deep learning.',\n",
       "  'paper': {'title': 'DeepFix: Fixing Common C Language Errors by Deep Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/deepfix-fixing-common-c-language-errors-by'},\n",
       "  'introduced_date': '2017-02-04',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Program Repair',\n",
       "    'url': 'https://paperswithcode.com/task/program-repair'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DeepFix'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': [{'url': 'https://bitbucket.org/iiscseal/deepfix',\n",
       "    'repo': 'https://bitbucket.org/iiscseal/deepfix',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/duc-2004',\n",
       "  'name': 'DUC 2004',\n",
       "  'full_name': 'DUC 2004',\n",
       "  'homepage': 'https://duc.nist.gov/duc2004/',\n",
       "  'description': 'The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents.\\n\\nSource: [Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction](https://arxiv.org/abs/2005.01791)\\nImage Source: [https://duc.nist.gov/duc2004/](https://duc.nist.gov/duc2004/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Extractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/extractive-document-summarization'},\n",
       "   {'task': 'Multi-Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/multi-document-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DUC 2004 Task 1', 'DUC 2004'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cl-scisumm',\n",
       "  'name': 'CL-SciSumm',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/WING-NUS/scisumm-corpus',\n",
       "  'description': '',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Scientific Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/scientific-article-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CL-SciSumm'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/WING-NUS/scisumm-corpus',\n",
       "    'repo': 'https://github.com/WING-NUS/scisumm-corpus',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ut-interaction',\n",
       "  'name': 'UT-Interaction',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html',\n",
       "  'description': 'The **UT-Interaction** dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, resulting in 8 executions of human activities per video on average. Several participants with more than 15 different clothing conditions appear in the videos. The videos are taken with the resolution of 720*480, 30fps, and the height of a person in the video is about 200 pixels.\\r\\n\\r\\nSource: [https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)\\r\\nImage Source: [https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html](https://cvrc.ece.utexas.edu/SDHA2010/Human_Interaction.html)',\n",
       "  'paper': {'title': 'UT-Interaction Dataset, ICPR contest on Semantic Description of Human Activities (SDHA)',\n",
       "   'url': 'http://cvrc.ece.utexas.edu/SDHA2010/Human˙Interaction.html'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Human Interaction Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/human-interaction-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UT-Interaction'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/avsd',\n",
       "  'name': 'AVSD',\n",
       "  'full_name': 'Audio-Visual Scene-Aware Dialog',\n",
       "  'homepage': 'http://workshop.colips.org/dstc7/call.html',\n",
       "  'description': 'The Audio Visual Scene-Aware Dialog (**AVSD**) dataset, or DSTC7 Track 3, is a audio-visual dataset for dialogue understanding. The goal with the dataset and track was to design systems to generate responses in a dialog about a video, given the dialog history and audio-visual content of the video.\\n\\nSource: [The Eighth Dialog System Technology Challenge](https://arxiv.org/abs/1911.06394)\\nImage Source: [http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf](http://workshop.colips.org/dstc7/papers/DSTC7_Task_3_overview_paper.pdf)',\n",
       "  'paper': {'title': 'Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7',\n",
       "   'url': 'https://paperswithcode.com/paper/audio-visual-scene-aware-dialog-avsd'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Scene-Aware Dialogue',\n",
       "    'url': 'https://paperswithcode.com/task/scene-aware-dialogue'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['AVSD'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/eqasc',\n",
       "  'name': 'eQASC',\n",
       "  'full_name': 'eQASC',\n",
       "  'homepage': 'https://allenai.org/data/eqasc',\n",
       "  'description': 'This dataset contains 98k 2-hop explanations for questions in the QASC dataset, with annotations indicating if they are valid (~25k) or invalid (~73k) explanations.\\r\\n\\r\\nThis repository addresses the current lack of training data for distinguish valid multihop explanations from invalid, by providing three new datasets. The main one, eQASC, contains 98k explanation annotations for the multihop question answering dataset [QASC](https://allenai.org/data/qasc), and is the first that annotates multiple candidate explanations for each answer.\\r\\n\\r\\nThe second dataset, eQASC-perturbed, is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the [OBQA dataset](https://allenai.org/data/open-book-qa) to test generalization of models trained on eQASC.\\r\\n\\r\\nSource: [Allen Institute for AI](https://allenai.org/data/eqasc)',\n",
       "  'paper': {'title': 'Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-explain-datasets-and-models-for'},\n",
       "  'introduced_date': '2020-10-07',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Reasoning Chain Explanations',\n",
       "    'url': 'https://paperswithcode.com/task/reasoning-chain-explanations'}],\n",
       "  'languages': [],\n",
       "  'variants': ['eQASC'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet-lt',\n",
       "  'name': 'ImageNet-LT',\n",
       "  'full_name': 'ImageNet Long-Tailed',\n",
       "  'homepage': 'https://liuziwei7.github.io/projects/LongTail.html',\n",
       "  'description': '**ImageNet Long-Tailed** is a subset of [/dataset/imagenet](ImageNet) dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set.\\r\\n\\r\\nSource: [Large-Scale Long-Tailed Recognition in an Open World](https://arxiv.org/pdf/1904.05160v2.pdf)',\n",
       "  'paper': {'title': 'Large-Scale Long-Tailed Recognition in an Open World',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-long-tailed-recognition-in-an'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'},\n",
       "   {'task': 'Long-tail learning with class descriptors',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning-with-class-descriptors'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageNet-LT', 'ImageNet-LT-d'],\n",
       "  'num_papers': 81,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/places-lt',\n",
       "  'name': 'Places-LT',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://liuziwei7.github.io/projects/LongTail.html',\n",
       "  'description': '**Places-LT** has an imbalanced training set with 62,500 images for 365 classes from Places-2. The class frequencies follow a natural power law distribution with a maximum number of 4,980 images per class and a minimum number of 5 images per class. The validation and testing sets are balanced and contain 20 and 100 images per class respectively.\\r\\n\\r\\nSource: [Long-Tailed Recognition Using Class-Balanced Experts](https://arxiv.org/abs/2004.03706)',\n",
       "  'paper': {'title': 'Large-Scale Long-Tailed Recognition in an Open World',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-long-tailed-recognition-in-an'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Long-tail Learning',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Places-LT'],\n",
       "  'num_papers': 33,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/salinas',\n",
       "  'name': 'Salinas',\n",
       "  'full_name': 'Salinas Scene',\n",
       "  'homepage': 'http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes',\n",
       "  'description': '**Salinas Scene** is a hyperspectral dataset collected by the 224-band AVIRIS sensor over Salinas Valley, California, and is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. 20 water absorption bands were discarder: [108-112], [154-167], 224. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. Salinas groundtruth contains 16 classes.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Hyperspectral images'],\n",
       "  'tasks': [{'task': 'Hyperspectral Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/hyperspectral-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Salinas Scene', 'Salinas'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rst-dt',\n",
       "  'name': 'RST-DT',\n",
       "  'full_name': 'RST Discourse Treebank',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2002T07',\n",
       "  'description': \"The Rhetorical Structure Theory (RST) Discourse Treebank consists of 385 Wall Street Journal articles\\r\\nfrom the Penn Treebank annotated with discourse structure in the RST framework along with\\r\\nhuman-generated extracts and abstracts associated with the source documents.\\r\\n\\r\\nIn the RST framework (Mann and Thompson, 1988), a text's discourse structure can be\\r\\nrepresented as a tree in four aspects:\\r\\n\\r\\n(1) the leaves correspond to text fragments called elementary discourse units (the mininal discourse units);\\r\\n(2) the internal nodes of the tree correspond to contiguous text spans;\\r\\n(3) each node is characterized by its nuclearity, or essential unit of information; and\\r\\n(4) each node is also characterized by a rhetorical relation between two or more non-overlapping, adjacent text spans. \\r\\n\\r\\nData\\r\\n\\r\\nThe data in this release is divided into a training set (347 documents) and a test set (38 documents).\\r\\nAll annotations were produced using a discourse annotation tool that can be downloaded from http://www.isi.edu/~marcu/discourse.\",\n",
       "  'paper': None,\n",
       "  'introduced_date': '2002-02-21',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Discourse Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/discourse-parsing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RST-DT'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sqa',\n",
       "  'name': 'SQA',\n",
       "  'full_name': 'SequentialQA',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/download/details.aspx?id=54253',\n",
       "  'description': 'The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.\\r\\n\\r\\nSource: [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253)',\n",
       "  'paper': {'title': 'Search-based Neural Structured Learning for Sequential Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/search-based-neural-structured-learning-for'},\n",
       "  'introduced_date': '2017-07-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SQA'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/bc4chemd',\n",
       "  'name': 'BC4CHEMD',\n",
       "  'full_name': 'BioCreative IV Chemical compound and drug name recognition',\n",
       "  'homepage': 'https://biocreative.bioinformatics.udel.edu/resources/biocreative-iv/chemdner-corpus/',\n",
       "  'description': 'Introduced by Krallinger et al. in [The CHEMDNER corpus of chemicals and drugs and its annotation principles](https://jcheminf.biomedcentral.com/articles/10.1186/1758-2946-7-S1-S2)\\r\\n\\r\\n**BC4CHEMD** is a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'NER', 'url': 'https://paperswithcode.com/task/cg'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['BC4CHEMD'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sherliic',\n",
       "  'name': 'SherLIiC',\n",
       "  'full_name': 'SherLIiC',\n",
       "  'homepage': 'https://github.com/mnschmit/SherLIiC',\n",
       "  'description': 'SherLIiC is a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types.\\r\\n\\r\\nSource: [SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference](https://arxiv.org/pdf/1906.01393v1.pdf)',\n",
       "  'paper': {'title': 'SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for Evaluating Natural Language Inference',\n",
       "   'url': 'https://paperswithcode.com/paper/sherliic-a-typed-event-focused-lexical'},\n",
       "  'introduced_date': '2019-06-04',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Few-Shot NLI',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-nli'},\n",
       "   {'task': 'Lexical Entailment',\n",
       "    'url': 'https://paperswithcode.com/task/lexical-entailment'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SherLIiC'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': [{'url': 'https://github.com/mnschmit/SherLIiC',\n",
       "    'repo': 'https://github.com/mnschmit/SherLIiC',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/xcopa',\n",
       "  'name': 'XCOPA',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/cambridgeltl/xcopa',\n",
       "  'description': 'The Cross-lingual Choice of Plausible Alternatives (**XCOPA**) dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across languages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages.\\n\\nSource: [https://github.com/cambridgeltl/xcopa](https://github.com/cambridgeltl/xcopa)',\n",
       "  'paper': {'title': 'XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/xcopa-a-multilingual-dataset-for-causal'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Cross-Lingual Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['XCOPA'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/xcopa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/cambridgeltl/xcopa',\n",
       "    'repo': 'https://github.com/cambridgeltl/xcopa',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/debatesum',\n",
       "  'name': 'DebateSum',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/Hellisotherpeople/DebateSum',\n",
       "  'description': '**DebateSum** consists of 187328 debate documents, arguments (also can be thought of as abstractive summaries, or queries), word-level extractive summaries, citations, and associated metadata organized by topic-year. This data is ready for analysis by NLP systems.\\n\\nSource: [https://github.com/Hellisotherpeople/DebateSum](https://github.com/Hellisotherpeople/DebateSum)',\n",
       "  'paper': {'title': 'DebateSum: A large-scale argument mining and summarization dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/debatesum-a-large-scale-argument-mining-and'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Extractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/extractive-document-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DebateSum'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/Hellisotherpeople/DebateSum',\n",
       "    'repo': 'https://github.com/Hellisotherpeople/DebateSum',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/isaid',\n",
       "  'name': 'iSAID',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://captain-whu.github.io/iSAID/index.html',\n",
       "  'description': 'iSAID contains 655,451 object instances for 15 categories across 2,806 high-resolution images. The images of iSAID is the same as the DOTA-v1.0 dataset, which are manily collected from the Google Earth, some are taken by satellite JL-1, the others are taken by satellite GF-2 of the China Centre for Resources Satellite Data and Application.\\r\\n\\r\\nSource: [iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images](/paper/isaid-a-large-scale-dataset-for-instance)\\r\\nImage Source: [iSAID](https://captain-whu.github.io/iSAID/index.html)',\n",
       "  'paper': {'title': 'iSAID: A Large-scale Dataset for Instance Segmentation in Aerial Images',\n",
       "   'url': 'https://paperswithcode.com/paper/isaid-a-large-scale-dataset-for-instance'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Small Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/small-object-detection'},\n",
       "   {'task': 'Object Detection In Aerial Images',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection-in-aerial-images'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iSAID'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rudas',\n",
       "  'name': 'RuDaS',\n",
       "  'full_name': 'Synthetic Datasets for Rule Learning',\n",
       "  'homepage': 'https://github.com/IBM/RuDaS',\n",
       "  'description': 'Logical rules are a popular knowledge representation language in many domains. Recently, neural networks have been proposed to support the complex rule induction process. However, we argue that existing datasets and evaluation approaches are lacking in various dimensions; for example, different kinds of rules or dependencies between rules are neglected. Moreover, for the development of neural approaches, we need large amounts of data to learn from and adequate, approximate evaluation measures. In this paper, we provide a tool for generating diverse datasets and for evaluating neural rule learning systems, including novel performance metrics.',\n",
       "  'paper': {'title': 'RuDaS: Synthetic Datasets for Rule Learning and Evaluation Tools',\n",
       "   'url': 'https://paperswithcode.com/paper/rudas-synthetic-datasets-for-rule-learning'},\n",
       "  'introduced_date': '2019-09-16',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Inductive logic programming',\n",
       "    'url': 'https://paperswithcode.com/task/inductive-logic-programming'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RuDaS'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/reclor',\n",
       "  'name': 'ReClor',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://whyu.me/reclor/',\n",
       "  'description': 'Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. **ReClor** is a dataset extracted from logical reasoning questions of standardized graduate admission examinations.\\r\\n\\r\\nSource: [ReClor](https://whyu.me/reclor/)',\n",
       "  'paper': {'title': 'ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/reclor-a-reading-comprehension-dataset-1'},\n",
       "  'introduced_date': '2020-02-11',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Machine Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/machine-reading-comprehension'},\n",
       "   {'task': 'Logical Reasoning Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/logical-reasoning-question-ansering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ReClor'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/reclor',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sen12ms-cr',\n",
       "  'name': 'SEN12MS-CR',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://mediatum.ub.tum.de/1554803',\n",
       "  'description': 'Curates a large novel data set for training new cloud removal approaches and evaluate on two recently proposed performance metrics of image quality and diversity.\\r\\n\\r\\nSource: [Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery](/paper/multi-sensor-data-fusion-for-cloud-removal-in)',\n",
       "  'paper': {'title': 'Multi-Sensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-sensor-data-fusion-for-cloud-removal-in'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Cloud Removal',\n",
       "    'url': 'https://paperswithcode.com/task/cloud-removal'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SEN12MS-CR'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/convai2',\n",
       "  'name': 'ConvAI2',\n",
       "  'full_name': 'Conversational Intelligence Challenge 2',\n",
       "  'homepage': 'https://parl.ai/projects/convai2/',\n",
       "  'description': 'The **ConvAI2** NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers.\\r\\n\\r\\nTo avoid modeling that takes advantage of trivial word overlap, additional rewritten sets of the same train and test personas were crowdsourced, with related sentences that are rephrases, generalizations or specializations, rendering the task much more challenging. For example “I just got my nails done” is revised as “I love to pamper myself on a regular basis” and “I am on a diet now” is revised as “I need to lose weight.”\\r\\n\\r\\nThe training, validation and hidden test sets consists of 17,878, 1,000 and 1,015 dialogues, respectively.\\r\\n\\r\\nSource: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)\\r\\nImage Source: [The Second Conversational Intelligence Challenge (ConvAI2)](https://paperswithcode.com/paper/the-second-conversational-intelligence/)',\n",
       "  'paper': {'title': 'The Second Conversational Intelligence Challenge (ConvAI2)',\n",
       "   'url': 'https://paperswithcode.com/paper/the-second-conversational-intelligence'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Dialog'],\n",
       "  'tasks': [{'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ConvAI2'],\n",
       "  'num_papers': 58,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/conv_ai_2',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/empatheticdialogues',\n",
       "  'name': 'EmpatheticDialogues',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/facebookresearch/EmpatheticDialogues',\n",
       "  'description': 'The **EmpatheticDialogues** dataset is a large-scale multi-turn empathetic dialogue dataset collected on the Amazon Mechanical Turk, containing 24,850 one-to-one open-domain conversations. Each conversation was obtained by pairing two crowd-workers: a speaker and a listener. The speaker is asked to talk about the personal emotional feelings. The listener infers the underlying emotion through what the speaker says and responds empathetically. The dataset provides 32 evenly distributed emotion labels.\\r\\n\\r\\nSource: [Empathetic Dialogue Generation viaKnowledge Enhancing and Emotion Dependency Modeling](https://arxiv.org/abs/2009.09708)\\r\\nImage Source: [Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset](https://arxiv.org/pdf/1811.00207.pdf)',\n",
       "  'paper': {'title': 'Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/i-know-the-feeling-learning-to-converse-with'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Dialog'],\n",
       "  'tasks': [{'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'},\n",
       "   {'task': 'Empathetic Response Generation',\n",
       "    'url': 'https://paperswithcode.com/task/empathetic-response-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['EmpatheticDialogues'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/empathetic_dialogues',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#empathetic-dialogues',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/facebookresearch/EmpatheticDialogues',\n",
       "    'repo': 'https://github.com/facebookresearch/EmpatheticDialogues',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wizard-of-wikipedia',\n",
       "  'name': 'Wizard of Wikipedia',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://parl.ai/projects/wizard_of_wikipedia/',\n",
       "  'description': '**Wizard of Wikipedia** is a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. It is used to train and evaluate dialogue systems for knowledgeable open dialogue with clear grounding',\n",
       "  'paper': {'title': 'Wizard of Wikipedia: Knowledge-Powered Conversational agents',\n",
       "   'url': 'https://paperswithcode.com/paper/wizard-of-wikipedia-knowledge-powered'},\n",
       "  'introduced_date': '2018-11-03',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Wizard of Wikipedia'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/phm-100',\n",
       "  'name': 'PPM-100',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/ZHKKKe/PPM',\n",
       "  'description': 'PPM is a portrait matting benchmark with the following characteristics:\\r\\n\\r\\n- Fine Annotation - All images are labeled and checked carefully.\\r\\n- Natural Background - All images use the original background without replacement.\\r\\n- Rich Diversity - The images cover full/half body and various postures.\\r\\n- High Resolution - The resolution of images is between 1080p and 4k.\\r\\n\\r\\nDataset is created by authors of real-time matting model MODNet to measure performance for matting task.',\n",
       "  'paper': {'title': 'MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition',\n",
       "   'url': 'https://paperswithcode.com/paper/is-a-green-screen-really-necessary-for-real'},\n",
       "  'introduced_date': '2020-11-24',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Image Matting',\n",
       "    'url': 'https://paperswithcode.com/task/image-matting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PPM-100'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/ppm-100-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/goemotions',\n",
       "  'name': 'GoEmotions',\n",
       "  'full_name': 'GoEmotions',\n",
       "  'homepage': 'https://github.com/google-research/google-research/tree/master/goemotions',\n",
       "  'description': '**GoEmotions** is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.\\r\\n\\r\\n- Number of examples: 58,009.\\r\\n- Number of labels: 27 + Neutral.\\r\\n- Maximum sequence length in training and evaluation datasets: 30.\\r\\n\\r\\nOn top of the raw data, the dataset also includes a version filtered based on reter-agreement, which contains a train/test/validation split:\\r\\n\\r\\n- Size of training dataset: 43,410.\\r\\n- Size of test dataset: 5,427.\\r\\n- Size of validation dataset: 5,426.\\r\\n\\r\\nThe emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.\\r\\n\\r\\nSource: [Google Research](https://github.com/google-research/google-research/tree/master/goemotions)',\n",
       "  'paper': {'title': 'GoEmotions: A Dataset of Fine-Grained Emotions',\n",
       "   'url': 'https://paperswithcode.com/paper/goemotions-a-dataset-of-fine-grained-emotions'},\n",
       "  'introduced_date': '2020-05-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Emotion Classification',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GoEmotions'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': [{'url': 'https://github.com/google-research/google-research',\n",
       "    'repo': 'https://github.com/google-research/google-research',\n",
       "    'frameworks': ['tf']},\n",
       "   {'url': 'https://huggingface.co/datasets/go_emotions',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/goemotions',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/recipenlg',\n",
       "  'name': 'RecipeNLG',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://recipenlg.cs.put.poznan.pl/',\n",
       "  'description': '',\n",
       "  'paper': {'title': 'RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation',\n",
       "   'url': 'https://paperswithcode.com/paper/recipenlg-a-cooking-recipes-dataset-for-semi'},\n",
       "  'introduced_date': '2020-12-15',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Recipe Generation',\n",
       "    'url': 'https://paperswithcode.com/task/recipe-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RecipeNLG'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/recipe_nlg',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dronedeploy',\n",
       "  'name': 'DroneDeploy',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/dronedeploy/dd-ml-segmentation-benchmark',\n",
       "  'description': 'From DroneDeploy:\\r\\n\\r\\nWe’ve collected a dataset of aerial orthomosaics and elevation images. These have been annotated into 6 different classes: Ground, Water, Vegetation, Cars, Clutter, and Buildings. The resolution of the images is approximately 10cm per pixel which gives them a great level of detail. We’re looking forward to making more data available and encourage more research into the impact this imagery can have in furthering safety, conservation, and efficiency.\\r\\n\\r\\nImage source: [https://arxiv.org/pdf/2012.02024v1.pdf](https://arxiv.org/pdf/2012.02024v1.pdf)\\r\\n\\r\\nSource: [DroneDeploy Segmentation Benchmark Challenge](https://www.dronedeploy.com/blog/dronedeploy-segmentation-benchmark-challenge/)\\r\\nImage Source: [title](url)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DroneDeploy'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fastmri',\n",
       "  'name': 'fastMRI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://fastmri.med.nyu.edu/',\n",
       "  'description': 'The **fastMRI** dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets.\\r\\nThe deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.\\r\\n**Knee MRI**: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression.\\r\\n**Brain MRI**: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent.\\r\\n\\r\\nSource: [https://fastmri.med.nyu.edu/](https://fastmri.med.nyu.edu/)\\r\\nImage Source: [https://fastmri.med.nyu.edu/](https://fastmri.med.nyu.edu/)',\n",
       "  'paper': {'title': 'fastMRI: An Open Dataset and Benchmarks for Accelerated MRI',\n",
       "   'url': 'https://paperswithcode.com/paper/fastmri-an-open-dataset-and-benchmarks-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical', 'MRI'],\n",
       "  'tasks': [{'task': 'MRI Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/mri-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['fastMRI',\n",
       "   'fastMRI Knee 4x',\n",
       "   'fastMRI Knee 8x',\n",
       "   'fastMRI Brain 4x',\n",
       "   'fastMRI Brain 8x'],\n",
       "  'num_papers': 126,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/who',\n",
       "  'name': 'WHO-COVID19 Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://covid19.who.int/',\n",
       "  'description': 'COVID19 Data from the World Health Organization',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'COVID-19 Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/covid-19-modelling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WHO-COVID19 Dataset'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mams',\n",
       "  'name': 'MAMS',\n",
       "  'full_name': 'Multi Aspect Multi-Sentiment',\n",
       "  'homepage': 'https://github.com/siat-nlp/MAMS-for-ABSA',\n",
       "  'description': 'MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA).\\r\\n\\r\\nSource: [MAMS](https://github.com/siat-nlp/MAMS-for-ABSA)',\n",
       "  'paper': {'title': 'A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/a-challenge-dataset-and-effective-models-for'},\n",
       "  'introduced_date': '2019-11-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Aspect-Based Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/aspect-based-sentiment-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MAMS'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://github.com/siat-nlp/MAMS-for-ABSA',\n",
       "    'repo': 'https://github.com/siat-nlp/MAMS-for-ABSA',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ddrel',\n",
       "  'name': 'DDRel',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/JiaQiSJTU/DialogueRelationClassification',\n",
       "  'description': '**DDRel** is a dataset for interpersonal relation classification in dyadic dialogues. It consists of 6,300 dyadic dialogue sessions between 694 pairs of speakers with 53,126 utterances in total. It is constructed by crawling movie scripts from IMSDb and annotating the relation labels for each session according to 13 pre-defines relationships.\\n\\nSource: [https://github.com/JiaQiSJTU/DialogueRelationClassification](https://github.com/JiaQiSJTU/DialogueRelationClassification)',\n",
       "  'paper': {'title': 'DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues',\n",
       "   'url': 'https://paperswithcode.com/paper/ddrel-a-new-dataset-for-interpersonal'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Dialog Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/dialog-relation-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DDRel'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': [{'url': 'https://github.com/JiaQiSJTU/DialogueRelationClassification',\n",
       "    'repo': 'https://github.com/JiaQiSJTU/DialogueRelationClassification',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sysu-mm01',\n",
       "  'name': 'SYSU-MM01',\n",
       "  'full_name': 'SYSU-MM01',\n",
       "  'homepage': 'https://github.com/wuancong/SYSU-MM01',\n",
       "  'description': 'The **SYSU-MM01** is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery.\\r\\n\\r\\nSource: [An Efficient Framework for Visible-Infrared Cross Modality Person Re-Identification](https://arxiv.org/abs/1907.06498)\\r\\nImage Source: [https://github.com/wuancong/SYSU-MM01](https://github.com/wuancong/SYSU-MM01)',\n",
       "  'paper': {'title': 'RGB-Infrared Cross-Modality Person Re-Identification',\n",
       "   'url': 'https://paperswithcode.com/paper/rgb-infrared-cross-modality-person-re'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Cross-Modal  Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/cross-view-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SYSU-MM01'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': [{'url': 'https://github.com/wuancong/SYSU-MM01',\n",
       "    'repo': 'https://github.com/wuancong/SYSU-MM01',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/musicnet',\n",
       "  'name': 'MusicNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://zenodo.org/record/5120004#.Yhxr0-jMJBA',\n",
       "  'description': \"MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results.\\r\\n\\r\\nSource: [MusicNet](https://homes.cs.washington.edu/~thickstn/musicnet.html)\",\n",
       "  'paper': {'title': 'Learning Features of Music from Scratch',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-features-of-music-from-scratch'},\n",
       "  'introduced_date': '2016-11-29',\n",
       "  'warning': None,\n",
       "  'modalities': ['Music'],\n",
       "  'tasks': [{'task': 'Music Transcription',\n",
       "    'url': 'https://paperswithcode.com/task/music-transcription'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MusicNet'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hatexplain',\n",
       "  'name': 'HateXplain',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/punyajoy/HateXplain',\n",
       "  'description': 'Covers multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based.\\r\\n\\r\\nSource: [HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection](/paper/hatexplain-a-benchmark-dataset-for)',\n",
       "  'paper': {'title': 'HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/hatexplain-a-benchmark-dataset-for'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Hate Speech Detection',\n",
       "    'url': 'https://paperswithcode.com/task/hate-speech-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HateXplain'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/hatexplain',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/punyajoy/HateXplain',\n",
       "    'repo': 'https://github.com/punyajoy/HateXplain',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reccon',\n",
       "  'name': 'RECCON',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/declare-lab/RECCON',\n",
       "  'description': 'RECCON is a dataset for the task of recognizing emotion cause in conversations.\\r\\n\\r\\nSource: [Recognizing Emotion Cause in Conversations](https://arxiv.org/pdf/2012.11820.pdf)',\n",
       "  'paper': {'title': 'Recognizing Emotion Cause in Conversations',\n",
       "   'url': 'https://paperswithcode.com/paper/recognizing-emotion-cause-in-conversations-1'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Recognizing Emotion Cause in Conversations',\n",
       "    'url': 'https://paperswithcode.com/task/recognizing-emotion-cause-in-conversations'},\n",
       "   {'task': 'Causal Emotion Entailment',\n",
       "    'url': 'https://paperswithcode.com/task/causal-emotion-entailment'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RECCON'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': [{'url': 'https://github.com/declare-lab/RECCON',\n",
       "    'repo': 'https://github.com/declare-lab/RECCON',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/regdb',\n",
       "  'name': 'RegDB',\n",
       "  'full_name': 'Dongguk Body-based Person Recognition Database (DBPerson-Recog-DB1)',\n",
       "  'homepage': 'http://dm.dongguk.edu/link.html',\n",
       "  'description': 'RegDB is used for Visible-Infrared Re-ID which handles the cross-modality matching between the daytime visible and night-time infrared images. The dataset contains images of 412 people. It includes 10 color and 10 thermal images for each person.',\n",
       "  'paper': {'title': 'Deep Learning for Person Re-identification: A Survey and Outlook',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-learning-for-person-re-identification-a'},\n",
       "  'introduced_date': '2020-01-13',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Cross-Modal  Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/cross-view-person-re-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RegDB'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': [{'url': 'https://github.com/sunpeil/reid',\n",
       "    'repo': 'https://github.com/sunpeil/reid',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kennedy-space-center',\n",
       "  'name': 'Kennedy Space Center',\n",
       "  'full_name': 'Kennedy Space Center',\n",
       "  'homepage': 'http://www.csr.utexas.edu/projects/rs/hrs/classify.html',\n",
       "  'description': \"**Kennedy Space Center** is a dataset for the classification of wetland vegetation at the Kennedy Space Center, Florida using hyperspectral imagery. Hyperspectral data were acquired over KSC on March 23, 1996 using JPL's Airborne Visible/Infrared Imaging Spectrometer.\",\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Hyperspectral images'],\n",
       "  'tasks': [{'task': 'Hyperspectral Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/hyperspectral-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Kennedy Space Center'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vlcs',\n",
       "  'name': 'VLCS',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Click to add a brief description of the dataset (Markdown and LaTeX enabled).\\n\\nProvide:\\n\\n* a high-level explanation of the dataset characteristics\\n* explain motivations and summary of its content\\n* potential use cases of the dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'}],\n",
       "  'languages': [],\n",
       "  'variants': [],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cholec80',\n",
       "  'name': 'Cholec80',\n",
       "  'full_name': 'Surgical Workflow Dataset',\n",
       "  'homepage': 'http://camma.u-strasbg.fr/datasets',\n",
       "  'description': 'Cholec80 is an endoscopic video dataset containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in Strasbourg hospital, France. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, a tool is defined as present in an image if at least half of the tool tip is visible.\\r\\n\\r\\nSource: [EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos](/paper/endonet-a-deep-architecture-for-recognition)[https://arxiv.org/pdf/1602.03012.pdf]',\n",
       "  'paper': {'title': 'EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/endonet-a-deep-architecture-for-recognition'},\n",
       "  'introduced_date': '2016-02-09',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Medical'],\n",
       "  'tasks': [{'task': 'Surgical tool detection',\n",
       "    'url': 'https://paperswithcode.com/task/surgical-tool-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Cholec80'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multi-pie',\n",
       "  'name': 'Multi-PIE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html',\n",
       "  'description': 'The **Multi-PIE** (Multi Pose, Illumination, Expressions) dataset consists of face images of 337 subjects taken under different pose, illumination and expressions. The pose range contains 15 discrete views, capturing a face profile-to-profile. Illumination changes were modeled using 19 flashlights located in different places of the room.\\r\\n\\r\\nSource: [Hybrid VAE: Improving Deep Generative Models using Partial Observations](https://arxiv.org/abs/1711.11566)',\n",
       "  'paper': {'title': 'Multi-PIE',\n",
       "   'url': 'https://doi.org/10.1109/AFGR.2008.4813399'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Single-Image Portrait Relighting',\n",
       "    'url': 'https://paperswithcode.com/task/single-image-portrait-relighting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Multi-PIE'],\n",
       "  'num_papers': 266,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/the-pile',\n",
       "  'name': 'The Pile',\n",
       "  'full_name': 'The Pile',\n",
       "  'homepage': 'https://pile.eleuther.ai/',\n",
       "  'description': 'The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.\\r\\n\\r\\nDatasheet: [Datasheet for the Pile](https://paperswithcode.com/paper/datasheet-for-the-pile)',\n",
       "  'paper': {'title': 'The Pile: An 800GB Dataset of Diverse Text for Language Modeling',\n",
       "   'url': 'https://paperswithcode.com/paper/the-pile-an-800gb-dataset-of-diverse-text-for'},\n",
       "  'introduced_date': '2020-12-31',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['The Pile'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ecb',\n",
       "  'name': 'ECB+',\n",
       "  'full_name': 'extension to the EventCorefBank',\n",
       "  'homepage': 'http://www.newsreader-project.eu/results/data/the-ecb-corpus/',\n",
       "  'description': 'The ECB+ corpus is an extension to the EventCorefBank (ECB, Bejan and Harabagiu, 2010). A newly added corpus component consists of 502 documents that belong to the 43 topics of the ECB but that describe different seminal events than those already captured in the ECB. All corpus texts were found through Google Search and were annotated with mentions of events and their times, locations, human and non-human participants as well as with within- and cross-document event and entity coreference information. The 2012 version of annotation of the ECB corpus (Lee et al., 2012) was used as a starting point for re-annotation of the ECB according to the ECB+ annotation guideline.\\r\\n\\r\\nThe major differences with respect to the 2012 version of annotation of the ECB are:\\r\\n\\r\\n(a) five event components are annotated in text:\\r\\n\\r\\n    actions (annotation tags starting with ACTION and NEG)\\r\\n    times (annotation tags starting with TIME)\\r\\n    locations (annotation tags starting with LOC)\\r\\n    human participants (annotation tags starting with HUMAN)\\r\\n    non-human participants (annotation tags starting with NON_HUMAN)\\r\\n\\r\\n(b) specific action classes and entity subtypes are distinguished for each of the five main event components resulting in a total tagset of 30 annotation tags based on ACE annotation guidelines (LDC 2008), TimeML (Pustejovsky et al., 2003 and Sauri et al., 2005)\\r\\n(c) intra- and cross-document coreference relations between mentions of the five event components were established:\\r\\n\\r\\n    INTRA_DOC_COREF tag captures within document coreference chains that do not participate in cross-document relations; within document coreference was annotated by means of the CAT tool (Bartalesi et al., 2012)\\r\\n    CROSS_DOC_COREF tag indicates cross-document coreference relations created in the CROMER tool (Girardi et al., 2014); all coreference branches refer by means of relation target IDs to the so called TAG_DESCRIPTORS, pointing to human friendly instance names (assigned by coders) and also to instance_id-s\\r\\n\\r\\n(d) events are annotated from an “event-centric” perspective, i.e. annotation tags are assigned depending on the role a mention plays in an event (for more information see ECB+ references).',\n",
       "  'paper': {'title': 'Using a sledgehammer to crack a nut? Lexical diversity and event coreference resolution',\n",
       "   'url': 'https://paperswithcode.com/paper/using-a-sledgehammer-to-crack-a-nut-lexical'},\n",
       "  'introduced_date': '2014-05-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'},\n",
       "   {'task': 'Event Cross-Document Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/event-cross-document-coreference-resolution'},\n",
       "   {'task': 'Entity Cross-Document Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/entity-cross-document-coreference-resolution'},\n",
       "   {'task': 'Event Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/event-coreference-resolution'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ECB+ test', 'ECB+'],\n",
       "  'num_papers': 58,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/ecb',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/s2orc',\n",
       "  'name': 'S2ORC',\n",
       "  'full_name': 'S2ORC',\n",
       "  'homepage': 'https://allenai.org/data/s2orc',\n",
       "  'description': 'A large corpus of 81.1M English-language academic papers spanning many academic disciplines. Rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.\\r\\n\\r\\nSource: [Allen Institute for AI](https://allenai.org/data/s2orc)',\n",
       "  'paper': {'title': 'S2ORC: The Semantic Scholar Open Research Corpus',\n",
       "   'url': 'https://paperswithcode.com/paper/gorc-a-large-contextual-citation-graph-of'},\n",
       "  'introduced_date': '2019-11-07',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Citation Recommendation',\n",
       "    'url': 'https://paperswithcode.com/task/citation-recommendation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['S2ORC'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/s2orc',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/webedit',\n",
       "  'name': 'WebEdit',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/isomap/factedit',\n",
       "  'description': 'Fact-based Text Editing dataset based on WebNLG dataset.',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tabular'],\n",
       "  'tasks': [{'task': 'Fact-based Text Editing',\n",
       "    'url': 'https://paperswithcode.com/task/fact-based-text-editing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['WebEdit'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/rotoedit',\n",
       "  'name': 'RotoEdit',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/isomap/factedit',\n",
       "  'description': 'Fact-based Text Editing dataset based on RotoWire dataset',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tabular'],\n",
       "  'tasks': [{'task': 'Fact-based Text Editing',\n",
       "    'url': 'https://paperswithcode.com/task/fact-based-text-editing'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['RotoEdit'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/webvision-database',\n",
       "  'name': 'WebVision',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/webvision/dataset2017.html',\n",
       "  'description': 'The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. \\r\\n\\r\\nThe same 1,000 concepts as the ILSVRC 2012 dataset are used for querying images, such that a bunch of existing approaches can be directly investigated and compared to the models trained from the ILSVRC 2012 dataset, and also makes it possible to study the dataset bias issue in the large scale scenario. The textual information accompanied with those images (e.g., caption, user tags, or description) are also provided as additional meta information. A validation set contains 50,000 images (50 images per category) is provided to facilitate the algorithmic development.',\n",
       "  'paper': {'title': 'WebVision Database: Visual Learning and Understanding from Web Data',\n",
       "   'url': 'https://paperswithcode.com/paper/webvision-database-visual-learning-and'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Learning with noisy labels',\n",
       "    'url': 'https://paperswithcode.com/task/learning-with-noisy-labels'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WebVision-1000', 'WebVision', 'mini WebVision 1.0'],\n",
       "  'num_papers': 96,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nasa-worldview',\n",
       "  'name': 'NASA Worldview',\n",
       "  'full_name': 'Understanding Clouds from Satellite Images',\n",
       "  'homepage': 'https://www.kaggle.com/c/understanding_cloud_organization/data',\n",
       "  'description': 'In this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names: Fish, Flower, Gravel, Sugar. For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four.\\r\\n\\r\\nThe images were downloaded from NASA Worldview. Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.\\r\\n\\r\\nThe labels were created in a crowd-sourcing activity at the Max-Planck-Institite for Meteorology in Hamburg, Germany, and the Laboratoire de météorologie dynamique in Paris, France. A team of 68 scientists identified areas of cloud patterns in each image, and each images was labeled by approximately 3 different scientists. Ground truth was determined by the union of the areas marked by all labelers for that image, after removing any black band area from the areas.\\r\\n\\r\\nThe segment for each cloud formation label for an image is encoded into a single row, even if there are several non-contiguous areas of the same formation in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank. You can read more about the encoding standard on the Evaluation page.\\r\\n\\r\\nFiles\\r\\ntrain.csv - the run length encoded segmentations for each image-label pair in the train_images\\r\\ntrain_images.zip - folder of training images\\r\\ntest_images.zip - folder of test images; your task is to predict the segmentations masks of each of the 4 cloud types (labels) for each image. IMPORTANT: Your prediction masks should be scaled down to 350 x 525 px.\\r\\nsample_submission.csv - a sample submission file in the correct format',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Satellite Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/satellite-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NASA Worldview'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cdd-dataset-season-varying',\n",
       "  'name': 'CDD Dataset (season-varying)',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'Source: [CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS](https://pdfs.semanticscholar.org/ae15/e5ccccaaff44ab542003386349ef1d3b7511.pdf)',\n",
       "  'paper': {'title': 'CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS',\n",
       "   'url': 'https://paperswithcode.com/paper/change-detection-in-remote-sensing-images'},\n",
       "  'introduced_date': '2018-06-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Change detection for remote sensing images',\n",
       "    'url': 'https://paperswithcode.com/task/change-detection-for-remote-sensing-images'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CDD Dataset (season-varying)'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': [{'url': 'https://github.com/yangL-H/tensorflow',\n",
       "    'repo': 'https://github.com/yangL-H/tensorflow',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/labelme',\n",
       "  'name': 'LabelMe',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://labelme.csail.mit.edu/Release3.0/index.php',\n",
       "  'description': '**LabelMe** database is a large collection of images with ground truth labels for object detection and recognition. The annotations come from two different sources, including the LabelMe online annotation tool.\\r\\n\\r\\nSource: [LabelMe: A Database and Web-Based Tool for Image Annotation](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)\\r\\nImage Source: [Russell et al](https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf)',\n",
       "  'paper': {'title': 'LabelMe: A Database and Web-Based Tool for Image Annotation',\n",
       "   'url': 'https://people.csail.mit.edu/brussell/research/AIM-2005-025-new.pdf'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LabelMe'],\n",
       "  'num_papers': 134,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ict-3dhp',\n",
       "  'name': 'ICT-3DHP',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ieeexplore.ieee.org/document/6247980',\n",
       "  'description': 'ICT-3DHP is collected using the Microsoft Kinect sensor and contains RGB images and depth maps of about 14k frames, divided in 10 sequences. The image resolution is 640 × 480 pixels. An hardware sensor (Polhemus Fastrack) is exploited to generate the ground truth annotation. The device is placed on a white cap worn by each subject, visible in both RGB and depth frames.\\r\\n\\r\\nSource: [POSEidon: Face-from-Depth for Driver Pose Estimation](https://arxiv.org/pdf/1611.10195v3.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/face-reconstruction'},\n",
       "   {'task': 'Head Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/head-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ICT-3DHP'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/eth',\n",
       "  'name': 'ETH',\n",
       "  'full_name': 'ETH Pedestrian',\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/aess/dataset/',\n",
       "  'description': '**ETH** is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS.\\r\\n\\r\\nSource: [Scale-aware Fast R-CNN for Pedestrian Detection](https://arxiv.org/abs/1510.08160)\\r\\nImage Source: [https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3](https://medium.com/@zhenqinghu/pedestrian-detection-on-eth-data-set-with-faster-r-cnn-19d0a906f1d3)',\n",
       "  'paper': {'title': 'Depth and Appearance for Mobile Scene Analysis',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2007.4409092'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Person Re-Identification',\n",
       "    'url': 'https://paperswithcode.com/task/person-re-identification'},\n",
       "   {'task': 'Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-prediction'},\n",
       "   {'task': 'Pedestrian Detection',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-detection'},\n",
       "   {'task': 'Multi-future Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/multi-future-trajectory-prediction'},\n",
       "   {'task': 'Multi Future Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/multi-future-trajectory-prediction-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ETH BIWI Walking Pedestrians dataset', 'ETH/UCY', 'ETH'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ucy',\n",
       "  'name': 'UCY',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data.html',\n",
       "  'description': 'The **UCY** dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (Δt=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view.\\r\\n\\r\\nSource: [Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data](https://arxiv.org/abs/2001.03093)\\r\\nImage Source: [http://trajnet.stanford.edu/data.php?n=1](http://trajnet.stanford.edu/data.php?n=1)',\n",
       "  'paper': {'title': 'Crowds by Example',\n",
       "   'url': 'https://doi.org/10.1111/j.1467-8659.2007.01089.x'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-prediction'},\n",
       "   {'task': 'Trajectory Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-forecasting'},\n",
       "   {'task': 'motion prediction',\n",
       "    'url': 'https://paperswithcode.com/task/motion-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCY'],\n",
       "  'num_papers': 107,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/iam',\n",
       "  'name': 'IAM',\n",
       "  'full_name': 'IAM Handwriting',\n",
       "  'homepage': 'https://fki.tic.heia-fr.ch/databases/iam-handwriting-database',\n",
       "  'description': 'The **IAM** database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels.\\r\\n\\r\\nSource: [Measuring Human Perception to Improve Handwritten Document Transcription](https://arxiv.org/abs/1904.03734)\\r\\nImage Source: [https://fki.tic.heia-fr.ch/databases/iam-handwriting-database](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database)',\n",
       "  'paper': {'title': 'The IAM-database: an English sentence database for offline handwriting recognition',\n",
       "   'url': 'https://doi.org/10.1007/s100320200071'},\n",
       "  'introduced_date': '2002-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Optical Character Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/optical-character-recognition'},\n",
       "   {'task': 'Handwritten Text Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/handwritten-text-recognition'},\n",
       "   {'task': 'Handwriting Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/handwriting-recognition'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['IAM'],\n",
       "  'num_papers': 111,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/webkb',\n",
       "  'name': 'WebKB',\n",
       "  'full_name': 'WebKB',\n",
       "  'homepage': 'http://www.cs.cmu.edu/~webkb/',\n",
       "  'description': '**WebKB** is a dataset that includes web pages from computer science departments of various universities. 4,518 web pages are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project). Additionally there is Other miscellanea category that is not comparable to the rest.\\r\\n\\r\\nSource: [Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation](https://arxiv.org/abs/1606.04429)',\n",
       "  'paper': {'title': 'Learning to Extract Symbolic Knowledge from the World Wide Web',\n",
       "   'url': 'http://www.aaai.org/Library/AAAI/1998/aaai98-072.php'},\n",
       "  'introduced_date': '1998-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Imputation', 'url': 'https://paperswithcode.com/task/imputation'},\n",
       "   {'task': 'Relational Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/relational-reasoning'},\n",
       "   {'task': 'Bayesian Inference',\n",
       "    'url': 'https://paperswithcode.com/task/bayesian-inference'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WebKB', 'Cornell', 'Texas', 'Wisconsin'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/memetracker',\n",
       "  'name': 'MemeTracker',\n",
       "  'full_name': 'MemeTracker',\n",
       "  'homepage': 'https://snap.stanford.edu/data/memetracker9.html',\n",
       "  'description': 'The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites.\\n\\nSource: [Marked Temporal Dynamics Modeling based on Recurrent Neural Network](https://arxiv.org/abs/1701.03918)\\nImage Source: [http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html](http://blog.fabric.ch/index.php?/archives/292-Memetracker-Tracking-News-Phrases-over-the-Web.html)',\n",
       "  'paper': {'title': 'Meme-tracking and the dynamics of the news cycle',\n",
       "   'url': 'https://doi.org/10.1145/1557019.1557077'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Combinatorial Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/combinatorial-optimization'},\n",
       "   {'task': 'Point Processes',\n",
       "    'url': 'https://paperswithcode.com/task/point-processes'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MemeTracker'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/english-web-treebank',\n",
       "  'name': 'English Web Treebank',\n",
       "  'full_name': 'English Web Treebank',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2012T13',\n",
       "  'description': '**English Web Treebank** is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated.\\r\\n\\r\\nSource: [https://catalog.ldc.upenn.edu/LDC2012T13](https://catalog.ldc.upenn.edu/LDC2012T13)',\n",
       "  'paper': {'title': 'English web treebank',\n",
       "   'url': 'http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=L%DC2012T13'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Dependency Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/dependency-parsing'},\n",
       "   {'task': 'Part-Of-Speech Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/part-of-speech-tagging'},\n",
       "   {'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['English Web Treebank'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/silhouettes',\n",
       "  'name': 'Silhouettes',\n",
       "  'full_name': 'CalTech 101 Silhouettes',\n",
       "  'homepage': 'https://people.cs.umass.edu/~marlin/data.shtml',\n",
       "  'description': 'The Caltech 101 **Silhouettes** dataset consists of 4,100 training samples, 2,264 validation samples and 2,307 test samples. The datast is based on CalTech 101 image annotations. Each image in the CalTech 101 data set includes a high-quality polygon outline of the primary object in the scene. To create the **CalTech 101 Silhouettes** data set, the authors center and scale each outline and render it on a DxD pixel image-plane. The outline is rendered as a filled, black polygon on a white background. Many object classes exhibit silhouettes that have distinctive class-specific features. A relatively small number of classes like soccer ball, pizza, stop sign, and yin-yang are indistinguishable based on shape, but have been left-in in the data.\\r\\n\\r\\nSource: [1 Introduction](https://arxiv.org/abs/1506.04557)\\r\\nImage Source: [https://people.cs.umass.edu/~marlin/data.shtml](https://people.cs.umass.edu/~marlin/data.shtml)',\n",
       "  'paper': {'title': 'Inductive Principles for Restricted Boltzmann Machine Learning',\n",
       "   'url': 'http://proceedings.mlr.press/v9/marlin10a.html'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Variational Inference',\n",
       "    'url': 'https://paperswithcode.com/task/variational-inference'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Silhouettes'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/eth-sfm',\n",
       "  'name': 'ETH SfM',\n",
       "  'full_name': 'ETH Structure-from-Motion',\n",
       "  'homepage': 'https://cvg.ethz.ch/research/symmetries-in-sfm/',\n",
       "  'description': 'The **ETH SfM** (structure-from-motion) dataset is a dataset for 3D Reconstruction. The benchmark investigates how different methods perform in terms of building a 3D model from a set of available 2D images.\\n\\nSource: [SOSNet: Second Order Similarity Regularization forLocal Descriptor Learning](https://arxiv.org/abs/1904.05019)\\nImage Source: [https://cvg.ethz.ch/research/symmetries-in-sfm/](https://cvg.ethz.ch/research/symmetries-in-sfm/)',\n",
       "  'paper': {'title': 'Comparative Evaluation of Hand-Crafted and Learned Local Features',\n",
       "   'url': 'https://paperswithcode.com/paper/comparative-evaluation-of-hand-crafted-and-1'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D'],\n",
       "  'tasks': [{'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Patch Matching',\n",
       "    'url': 'https://paperswithcode.com/task/patch-matching'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ETH SfM'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inria-person',\n",
       "  'name': 'INRIA Person',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://pascal.inrialpes.fr/data/human/',\n",
       "  'description': 'The **INRIA Person** dataset is a dataset of images of persons used for pedestrian detection. It consists of 614 person detections for training and 288 for testing.\\r\\n\\r\\nSource: [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/)\\r\\nImage Source: [https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181](https://www.researchgate.net/figure/Some-human-examples-of-the-INRIA-person-dataset-4-Though-the-examples-are-aligned_fig2_224135181)',\n",
       "  'paper': {'title': 'Histograms of Oriented Gradients for Human Detection',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2005.177'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Pedestrian Detection',\n",
       "    'url': 'https://paperswithcode.com/task/pedestrian-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['INRIA Person'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inria-horse',\n",
       "  'name': 'INRIA-Horse',\n",
       "  'full_name': 'INRIA-Horse',\n",
       "  'homepage': 'http://calvin-vision.net/datasets/inria-horses/',\n",
       "  'description': 'The **INRIA-Horse** dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction.\\n\\nSource: [Dynamical And-Or Graph Learning for Object Shape Modeling and Detection](https://arxiv.org/abs/1502.00741)\\nImage Source: [http://calvin-vision.net/datasets/inria-horses/](http://calvin-vision.net/datasets/inria-horses/)',\n",
       "  'paper': {'title': 'Scale-Invariant Shape Features for Recognition of Object Categories',\n",
       "   'url': 'http://doi.ieeecomputersociety.org/10.1109/CVPR.2004.218'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['INRIA-Horse'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inria-aerial-image-labeling',\n",
       "  'name': 'INRIA Aerial Image Labeling',\n",
       "  'full_name': 'INRIA Aerial Image Labeling',\n",
       "  'homepage': 'https://project.inria.fr/aerialimagelabeling/',\n",
       "  'description': 'The **INRIA Aerial Image Labeling** dataset is comprised of 360 RGB tiles of 5000×5000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints.\\r\\n\\r\\nSource: [Distance transform regression for spatially-aware deep semantic segmentation](https://arxiv.org/abs/1909.01671)\\r\\nImage Source: [https://project.inria.fr/aerialimagelabeling/](https://project.inria.fr/aerialimagelabeling/)',\n",
       "  'paper': {'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/very-deep-convolutional-networks-for-large'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Incremental Learning',\n",
       "    'url': 'https://paperswithcode.com/task/incremental-learning'},\n",
       "   {'task': 'Model Compression',\n",
       "    'url': 'https://paperswithcode.com/task/model-compression'}],\n",
       "  'languages': [],\n",
       "  'variants': ['INRIA Aerial Image Labeling'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/office-caltech-10',\n",
       "  'name': 'Office-Caltech-10',\n",
       "  'full_name': 'Office-Caltech-10',\n",
       "  'homepage': 'https://people.eecs.berkeley.edu/~jhoffman/domainadapt/',\n",
       "  'description': '**Office-Caltech-10** a standard benchmark for domain adaptation, which consists of Office 10 and Caltech 10 datasets. It contains the 10 overlapping categories between the Office dataset and Caltech256 dataset. SURF BoW historgram features, vector quantized to 800 dimensions are also available for this dataset.\\n\\nSource: [Impact of ImageNet Model Selection on Domain Adaptation](https://arxiv.org/abs/2002.02559)\\nImage Source: [https://arxiv.org/abs/1409.5241](https://arxiv.org/abs/1409.5241)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Office-Caltech-10'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/poser',\n",
       "  'name': 'Poser',\n",
       "  'full_name': 'Poser',\n",
       "  'homepage': 'http://groups.csail.mit.edu/vision/datasets/poser/',\n",
       "  'description': 'The **Poser** dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package.\\n\\nSource: [Overlapping Cover Local Regression Machines](https://arxiv.org/abs/1701.01218)\\nImage Source: [https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487](https://www.researchgate.net/figure/Test-data-used-in-the-user-study-Left-the-pose-pictures-shown-to-the-user-Middle-the_fig17_221847487)',\n",
       "  'paper': {'title': 'Recovering 3D Human Pose from Monocular Images',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2006.21'},\n",
       "  'introduced_date': '2006-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Gaussian Processes',\n",
       "    'url': 'https://paperswithcode.com/task/gaussian-processes'},\n",
       "   {'task': 'GPR', 'url': 'https://paperswithcode.com/task/gpr'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Poser'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ukp',\n",
       "  'name': 'UKP',\n",
       "  'full_name': 'UKP Argument Annotated Essays',\n",
       "  'homepage': 'https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp',\n",
       "  'description': 'The **UKP** Argument Annotated Essays corpus consists of argument annotated persuasive essays including annotations of argument components and argumentative relations.\\n\\nSource: [https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp](https://www.informatik.tu-darmstadt.de/ukp/research_6/data/argumentation_mining_1/argument_annotated_essays/index.en.jsp)\\nImage Source: [https://www.aclweb.org/anthology/C14-1142.pdf](https://www.aclweb.org/anthology/C14-1142.pdf)',\n",
       "  'paper': {'title': 'Annotating Argument Components and Relations in Persuasive Essays',\n",
       "   'url': 'https://paperswithcode.com/paper/annotating-argument-components-and-relations'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Argument Mining',\n",
       "    'url': 'https://paperswithcode.com/task/argument-mining'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UKP'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/eth-biwi-walking-pedestrians',\n",
       "  'name': 'ETH BIWI Walking Pedestrians',\n",
       "  'full_name': 'ETH BIWI Walking Pedestrians',\n",
       "  'homepage': 'https://icu.ee.ethz.ch/research/datsets.html',\n",
       "  'description': 'The BIWI Walking Pedestrians dataset consists of walking pedestrians in busy scenarios from a birds eye view.\\n\\nSource: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)\\nImage Source: [https://icu.ee.ethz.ch/research/datsets.html](https://icu.ee.ethz.ch/research/datsets.html)',\n",
       "  'paper': {'title': \"You'll never walk alone: Modeling social behavior for multi-target tracking\",\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2009.5459260'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['ETH BIWI Walking Pedestrians'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wasabi',\n",
       "  'name': 'WASABI',\n",
       "  'full_name': 'WASABI',\n",
       "  'homepage': 'https://github.com/micbuffa/WasabiDataset',\n",
       "  'description': 'The **WASABI** Song Corpus is a large corpus of songs enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis.\\r\\nMore specifically, given that lyrics encode an important part of the semantics of a song, the authors focus on the description of the methods they proposed to extract relevant information from the lyrics, such as their structure segmentation, their topics, the explicitness of the lyrics content, the salient passages of a song and the emotions conveyed.\\r\\nThe corpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at different levels with the output of the above mentioned methods. Such corpus labels and the provided methods can be exploited by music search engines and music professionals (e.g. journalists, radio presenters) to better handle large collections of lyrics, allowing an intelligent browsing, categorization and segmentation recommendation of songs.\\r\\n\\r\\nSource: [https://github.com/micbuffa/WasabiDataset](https://github.com/micbuffa/WasabiDataset)\\nImage Source: [https://arxiv.org/abs/1912.02477](https://arxiv.org/abs/1912.02477)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': ['English', 'French'],\n",
       "  'variants': ['WASABI'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': [{'url': 'https://github.com/micbuffa/WasabiDataset',\n",
       "    'repo': 'https://github.com/micbuffa/WasabiDataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/multilingual-reuters',\n",
       "  'name': 'Multilingual Reuters',\n",
       "  'full_name': 'Multilingual Reuters Collection',\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/Reuters+RCV1+RCV2+Multilingual,+Multiview+Text+Categorization+Test+collection',\n",
       "  'description': 'The **Multilingual Reuters** Collection dataset comprises over 11,000 articles from six classes in five languages, i.e., English (E), French (F), German (G), Italian (I), and Spanish (S).\\n\\nSource: [Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network](https://arxiv.org/abs/2008.02714)\\nImage Source: [https://papers.nips.cc/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf](https://papers.nips.cc/paper/2009/file/f79921bbae40a577928b76d2fc3edc2a-Paper.pdf)',\n",
       "  'paper': {'title': 'Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-from-multiple-partially-observed'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Text Categorization',\n",
       "    'url': 'https://paperswithcode.com/task/text-categorization'}],\n",
       "  'languages': ['English', 'French', 'Spanish', 'German', 'Italian'],\n",
       "  'variants': ['Multilingual Reuters'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pan-chiphoto',\n",
       "  'name': 'Pan+ChiPhoto',\n",
       "  'full_name': 'Pan+ChiPhoto',\n",
       "  'homepage': None,\n",
       "  'description': '**Pan+ChiPhoto** dataset is a Chinese character dataset. It is built by the combination of two datasets: ChiPhoto and Pan_Chinese_Character dataset. The images in this dataset are mainly captured at outdoors in Beijing and Shanghai, China, which involve various scenes like signs, boards, advertisements, banners, objects with texts printed on their surfaces.\\n\\nSource: [Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs](https://arxiv.org/abs/1907.05577)\\nImage Source: [https://www.researchgate.net/publication/318679069_Multi-order_Co-occurrence_Activations_Encoded_with_Fisher_Vector_for_Scene_Character_Recognition](https://www.researchgate.net/publication/318679069_Multi-order_Co-occurrence_Activations_Encoded_with_Fisher_Vector_for_Scene_Character_Recognition)',\n",
       "  'paper': {'title': 'Multilingual scene character recognition with co-occurrence of histogram of oriented gradients',\n",
       "   'url': 'https://doi.org/10.1016/j.patcog.2015.07.009'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['Pan+ChiPhoto'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/isi-bengali-character',\n",
       "  'name': 'ISI_Bengali_Character',\n",
       "  'full_name': 'ISI_Bengali_Character',\n",
       "  'homepage': 'https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html',\n",
       "  'description': 'The **ISI_Bengali_Character** dataset contains 158 classes of Bengali numerals, characters or their parts. 19,530 Bengali character samples are available. Most of the images in the dataset are synthesized.\\n\\nSource: [Boosting Scene Character Recognition by Learning Canonical Forms of Glyphs](https://arxiv.org/abs/1907.05577)\\nImage Source: [https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html](https://www.isical.ac.in/~ujjwal/download/SegmentedSceneCharacter.html)',\n",
       "  'paper': {'title': 'Multilingual scene character recognition with co-occurrence of histogram of oriented gradients',\n",
       "   'url': 'https://doi.org/10.1016/j.patcog.2015.07.009'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [],\n",
       "  'languages': ['Bengali'],\n",
       "  'variants': ['ISI_Bengali_Character'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/florentine',\n",
       "  'name': 'Florentine',\n",
       "  'full_name': 'Florentine',\n",
       "  'homepage': None,\n",
       "  'description': 'The **Florentine** dataset is a dataset of facial gestures which contains facial clips from 160 subjects (both male and female), where gestures were artificially generated according to a specific request, or genuinely given due to a shown stimulus. 1032 clips were captured for posed expressions and 1745 clips for induced facial expressions amounting to a total of 2777 video clips. Genuine facial expressions were induced in subjects using visual stimuli, i.e. videos selected randomly from a bank of Youtube videos to generate a specific emotion.\\n\\nSource: [Deep video gesture recognition using illumination invariants](https://arxiv.org/abs/1603.06531)\\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-faces/](https://www.micc.unifi.it/resources/datasets/florence-3d-faces/)',\n",
       "  'paper': {'title': 'Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild',\n",
       "   'url': 'https://doi.org/10.1145/2663204.2666274'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/gesture-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Florentine'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inria-dlfd',\n",
       "  'name': 'INRIA DLFD',\n",
       "  'full_name': 'INRIA Dense Light Field',\n",
       "  'homepage': 'http://clim.inria.fr/Datasets/InriaSynLF/index.html',\n",
       "  'description': 'The **INRIA Dense Light Field** Dataset (DLFD) is a dataset for testing depth estimation methods in a light field. DLFD contains 39 scenes with disparity range [-4,4] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.\\n\\nSource: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)\\nImage Source: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)',\n",
       "  'paper': {'title': 'A Framework for Learning Depth From a Flexible Subset of Dense and Sparse Light Field Views',\n",
       "   'url': 'https://doi.org/10.1109/TIP.2019.2923323'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/super-resolution'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['INRIA DLFD'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/inria-slfd',\n",
       "  'name': 'INRIA SLFD',\n",
       "  'full_name': 'INRIA Sparse Light Field',\n",
       "  'homepage': 'http://clim.inria.fr/Datasets/InriaSynLF/index.html',\n",
       "  'description': 'The INRIA Sprse Light Field Dataset (SLFD) is a dataset for testing depth estimation methods in a light field. SLFD contains 53 scenes with disparity range [-20,20] pixels. The light fields are of spatial resolution 512 x 512 and angular resolution 9 x 9.\\n\\nSource: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)\\nImage Source: [http://clim.inria.fr/Datasets/InriaSynLF/index.html](http://clim.inria.fr/Datasets/InriaSynLF/index.html)',\n",
       "  'paper': {'title': 'A Framework for Learning Depth From a Flexible Subset of Dense and Sparse Light Field Views',\n",
       "   'url': 'https://doi.org/10.1109/TIP.2019.2923323'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['INRIA SLFD'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aids-antiviral-screen',\n",
       "  'name': 'AIDS Antiviral Screen',\n",
       "  'full_name': 'AIDS Antiviral Screen',\n",
       "  'homepage': 'https://wiki.nci.nih.gov/display/ncidtpdata/aids+antiviral+screen+data',\n",
       "  'description': 'The **AIDS Antiviral Screen** dataset is a dataset of screens checking tens of thousands of compounds for evidence of anti-HIV activity. The available screen results are chemical graph-structured data of these various compounds.\\n\\nSource: [Graph Neural Processes Towards Bayesian Graph Neural Networks](https://arxiv.org/abs/1902.10042)',\n",
       "  'paper': {'title': 'IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning',\n",
       "   'url': 'https://doi.org/10.1007/978-3-540-89689-0_33'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Imputation', 'url': 'https://paperswithcode.com/task/imputation'},\n",
       "   {'task': 'Graph Embedding',\n",
       "    'url': 'https://paperswithcode.com/task/graph-embedding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AIDS Antiviral Screen'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/retinal-microsurgery',\n",
       "  'name': 'Retinal Microsurgery',\n",
       "  'full_name': 'Retinal Microsurgery',\n",
       "  'homepage': 'https://sites.google.com/site/sznitr/code-and-datasets/rmd_form',\n",
       "  'description': 'The **Retinal Microsurgery** dataset is a dataset for surgical instrument tracking. It consists of 18 in-vivo sequences, each with 200 frames of resolution 1920 × 1080 pixels. The dataset is further classified into four instrument-dependent subsets. The annotated tool joints are n=3 and semantic classes c=2 (tool and background).\\n\\nSource: [Concurrent Segmentation and Localization for Tracking of Surgical Instruments](https://arxiv.org/abs/1703.10701)\\nImage Source: [https://sites.google.com/site/sznitr/research/retinalmicrosurgery](https://sites.google.com/site/sznitr/research/retinalmicrosurgery)',\n",
       "  'paper': {'title': 'Real-time localization of articulated surgical instruments in retinal microsurgery',\n",
       "   'url': 'https://doi.org/10.1016/j.media.2016.05.003'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Motion Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/motion-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Retinal Microsurgery'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/daimler-monocular-pedestrian-detection',\n",
       "  'name': 'Daimler Monocular Pedestrian Detection',\n",
       "  'full_name': 'Daimler Monocular Pedestrian Detection',\n",
       "  'homepage': 'http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html',\n",
       "  'description': 'The **Daimler Monocular Pedestrian Detection** dataset is a dataset for pedestrian detection in urban environments. The training set contains 15560 pedestrian samples (image cut-outs at 48×96 resolution) and 6744 additional full images without pedestrians for extracting negative samples. The test set contains an independent sequence with more than 21790 images and 56492 pedestrian labels (fully visible or partially occluded), captured from a vehicle during a 27 min driving through the urban traffic.\\n\\nSource: [A Large Scale Urban Surveillance Video Dataset for Multiple-Object Tracking and Behavior Analysis](https://arxiv.org/abs/1904.11784)\\nImage Source: [http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html](http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_mono_ped__detection_be.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Interactive'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Daimler Monocular Pedestrian Detection'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ethz-shape',\n",
       "  'name': 'ETHZ-Shape',\n",
       "  'full_name': 'ETHZ-Shape',\n",
       "  'homepage': 'http://calvin-vision.net/datasets/ethz-shape-classes/',\n",
       "  'description': 'The ETHZ Shape dataset contains images of five diverse shape-based classes, collected from Flickr and Google Images. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The authors deliberately selected several images where the object comprises only a rather small portion of the image, and made an effort to include objects appearing at a wide range of scales. The objects are mostly unoccluded and are all taken from approximately the same viewpoint (the side).\\n\\nSource: [http://calvin-vision.net/datasets/ethz-shape-classes/](http://calvin-vision.net/datasets/ethz-shape-classes/)\\nImage Source: [http://calvin-vision.net/datasets/ethz-shape-classes/](http://calvin-vision.net/datasets/ethz-shape-classes/)',\n",
       "  'paper': {'title': 'Groups of Adjacent Contour Segments for Object Detection',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2007.1144'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Graph Learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ETHZ-Shape'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/l-bird',\n",
       "  'name': 'L-Bird',\n",
       "  'full_name': 'Large-Bird',\n",
       "  'homepage': None,\n",
       "  'description': 'The **L-Bird** (**Large-Bird**) dataset contains nearly 4.8 million images which are obtained by searching images of a total of 10,982 bird species from the Internet.\\n\\nSource: [Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data](https://arxiv.org/abs/1807.10916)\\nImage Source: [https://arxiv.org/pdf/1511.06789.pdf](https://arxiv.org/pdf/1511.06789.pdf)',\n",
       "  'paper': {'title': 'The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/the-unreasonable-effectiveness-of-noisy-data'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Meta-Learning',\n",
       "    'url': 'https://paperswithcode.com/task/meta-learning'},\n",
       "   {'task': 'Active Learning',\n",
       "    'url': 'https://paperswithcode.com/task/active-learning'},\n",
       "   {'task': 'Fine-Grained Visual Categorization',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-visual-categorization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['L-Bird'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/extended-bbc-pose',\n",
       "  'name': 'Extended BBC Pose',\n",
       "  'full_name': 'Extended BBC Pose',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation',\n",
       "  'description': \"**Extended BBC Pose** is a pose estimation dataset which extends the BBC Pose dataset with 72 additional training videos. Combined with the original BBC TV dataset, the dataset contains 92 videos (82 training, 5 validation and 5 testing), i.e. around 7 million frames. The frames of the new 72 videos are automatically assigned joint locations (used as ground truth for training) with the tracker of Charles et al. IJCV'13.\\n\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/](https://www.robots.ox.ac.uk/~vgg/data/pose/)\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/pose/](https://www.robots.ox.ac.uk/~vgg/data/pose/)\",\n",
       "  'paper': None,\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Extended BBC Pose'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/short-bbc-pose',\n",
       "  'name': 'Short BBC Pose',\n",
       "  'full_name': 'Short BBC Pose',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/pose/',\n",
       "  'description': '**Short BBC Pose** contains five one-hour-long videos with sign language signers each with different sleeve length (in contrast to the BBC pose and Extended BBC Pose, which only contain signers with moderately long sleeves). Each of the five videos has 200 test frames (which have been manually annotated with joint locations), amounting to 1,000 test frames in total. Test frames were selected by the authors to contain a diverse range of poses.\\n\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation)\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/publications/2013/Charles13/charles13.pdf](https://www.robots.ox.ac.uk/~vgg/publications/2013/Charles13/charles13.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Short BBC Pose'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chalearn-pose',\n",
       "  'name': 'ChaLearn Pose',\n",
       "  'full_name': 'ChaLearn Pose',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation',\n",
       "  'description': \"**ChaLearn Pose** is a subset of the ChaLearn 2013 Multi-modal gesture dataset from Escalera et al. ICMI'13, which contains 23 hours of Kinect data of 27 persons performing 20 Italian gestures. The data includes RGB, depth, foreground segmentations and full body skeletons. In this dataset, both the training and testing labels are noisy (from Kinect).\\n\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation](https://www.robots.ox.ac.uk/~vgg/data/pose/index.html#citation)\\nImage Source: [http://sunai.uoc.edu/chalearnLAP/](http://sunai.uoc.edu/chalearnLAP/)\",\n",
       "  'paper': {'title': 'Multi-modal Gesture Recognition Challenge 2013: Dataset and Results',\n",
       "   'url': 'http://refbase.cvc.uab.es/files/EGB2013.pdf'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': ['Italian'],\n",
       "  'variants': ['ChaLearn Pose'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/voxceleb2',\n",
       "  'name': 'VoxCeleb2',\n",
       "  'full_name': 'VoxCeleb2',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/voxceleb/',\n",
       "  'description': '**VoxCeleb2** is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected ‘in the wild’, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example – visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets.\\r\\n\\r\\nSource: [VoxCeleb2: Deep Speaker Recognition](https://arxiv.org/pdf/1806.05622v2.pdf)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/voxceleb/](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)',\n",
       "  'paper': {'title': 'VoxCeleb2: Deep Speaker Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/voxceleb2-deep-speaker-recognition'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Talking Head Generation',\n",
       "    'url': 'https://paperswithcode.com/task/talking-head-generation'},\n",
       "   {'task': 'Speaker Verification',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-verification'}],\n",
       "  'languages': ['Multilingual'],\n",
       "  'variants': ['VoxCeleb2 - 1-shot learning',\n",
       "   'VoxCeleb2 - 8-shot learning',\n",
       "   'VoxCeleb2 - 32-shot learning',\n",
       "   'VoxCeleb2'],\n",
       "  'num_papers': 230,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vctk',\n",
       "  'name': 'VCTK',\n",
       "  'full_name': 'CSTR VCTK Corpus',\n",
       "  'homepage': 'https://datashare.is.ed.ac.uk/handle/10283/2651',\n",
       "  'description': 'This CSTR **VCTK** Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed.\\r\\n\\r\\nSource: [CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)](https://datashare.is.ed.ac.uk/handle/10283/3443)',\n",
       "  'paper': {'title': 'CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit',\n",
       "   'url': 'http://dx.doi.org/10.7488/ds/1994'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Audio Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/audio-super-resolution'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Voice Bank corpus (VCTK)', 'VCTK Multi-Speaker', 'VCTK'],\n",
       "  'num_papers': 156,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/vctk',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/vctk',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/audio/stable/datasets.html#torchaudio.datasets.VCTK',\n",
       "    'repo': 'https://github.com/pytorch/audio',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dirha',\n",
       "  'name': 'DIRHA',\n",
       "  'full_name': 'Distant-speech Interaction for Robust Home Applications',\n",
       "  'homepage': 'http://dirha.fbk.eu/DIRHA_English',\n",
       "  'description': '**DIRHA**-English is a multi-microphone database composed of real and simulated sequences of 1-minute. The overall corpus is composed of different types of sequences including: 1) Phonetically-rich sentences; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands).\\r\\nThe sequences are available for both UK and US English at 48 kHz. The DIRHA-English dataset offers the possibility to work with a very large number of microphone channels, to use of microphone arrays having different characteristics and to work considering different speech recognition tasks (e.g., phone-loop, keyword spotting, ASR with small and very large language models).\\r\\n\\r\\nSource: [The DIRHA-English Corpus](http://dirha.fbk.eu/DIRHA_English)\\r\\nImage Source: [https://arxiv.org/pdf/1710.02560v1.pdf](https://arxiv.org/pdf/1710.02560v1.pdf)',\n",
       "  'paper': {'title': 'The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments',\n",
       "   'url': 'https://paperswithcode.com/paper/the-dirha-english-corpus-and-related-tasks'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Distant Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/distant-speech-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DIRHA English WSJ', 'DIRHA'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/voxforge',\n",
       "  'name': 'VoxForge',\n",
       "  'full_name': 'VoxForge',\n",
       "  'homepage': 'http://www.voxforge.org/home',\n",
       "  'description': '**VoxForge** is an open speech dataset that was set up to collect transcribed speech for use with Free and Open Source Speech Recognition Engines (on Linux, Windows and Mac).\\nImage Source: [http://www.voxforge.org/home](http://www.voxforge.org/home)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio', 'Speech'],\n",
       "  'tasks': [{'task': 'Spoken language identification',\n",
       "    'url': 'https://paperswithcode.com/task/spoken-language-identification'},\n",
       "   {'task': 'Keyword Spotting',\n",
       "    'url': 'https://paperswithcode.com/task/keyword-spotting'},\n",
       "   {'task': 'Accented Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/accented-speech-recognition'}],\n",
       "  'languages': ['English',\n",
       "   'French',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Italian',\n",
       "   'Japanese',\n",
       "   'Russian'],\n",
       "  'variants': ['VoxForge American-Canadian',\n",
       "   'VoxForge Commonwealth',\n",
       "   'VoxForge European',\n",
       "   'VoxForge Indian',\n",
       "   'VoxForge'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/voxforge',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/penn-action',\n",
       "  'name': 'Penn Action',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://dreamdragon.github.io/PennAction/',\n",
       "  'description': 'The **Penn Action** Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence.\\r\\n\\r\\nSource: [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)\\r\\nImage Source: [http://dreamdragon.github.io/PennAction/](http://dreamdragon.github.io/PennAction/)',\n",
       "  'paper': {'title': 'From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2013.280'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Video Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/video-alignment'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UPenn Action', 'Penn Action'],\n",
       "  'num_papers': 77,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tvsum-1',\n",
       "  'name': 'TVSum',\n",
       "  'full_name': 'TVSum: Summarizing web videos using titles',\n",
       "  'homepage': 'https://github.com/yalesong/tvsum/blob/master/README.md',\n",
       "  'description': '# TVSum Dataset\\r\\nTitle-based Video Summarization (TVSum) dataset used in our CVPR 2015 paper **\"TVSum: Summarizing web videos using titles.\"**\\r\\n\\r\\n\\r\\n\\r\\n## Overview\\r\\nTitle-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video). The video and annotation data permits an automatic evaluation of various video summarization techniques, without having to conduct (expensive) user study.\\r\\n\\r\\nThe videos, collected from YouTube, comes with the [Creative Commons CC-BY (v3.0) license](https://support.google.com/youtube/answer/2797468?hl=en). We release both the video files and their URLs. The shot-level importance scores are annotated via Amazon Mechanical Turk -- each video was annotated by 20 crowd-workers. The dataset has been reviewed to conform to Yahoo\\'s data protection standards, including strict controls on privacy.\\r\\n\\r\\n## Task\\r\\nThe primary task of the dataset is video summarization, where the goal is to create a short, meaningful summary of a given video. The summary may contain a few shots that capture the highlights of a video and are non-redundant. Although the task is inherently subjective, we carefully curated the dataset and annotated it so that the evaluation is done in an objective way. (We have a reasonably high degree of inter-rater reliability, with the Cronbach’s alpha of 0.81.)\\r\\n\\r\\n## Evaluations\\r\\nLet’s say we’ve generated a 15 second-long summary of video “[Will a cat eat dog food?](https://www.youtube.com/watch?v=-esJrBWj2d8)”, shown below:',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/video-summarization'},\n",
       "   {'task': 'Unsupervised Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-summarization'},\n",
       "   {'task': 'Supervised Video Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/supervised-video-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['TvSum', 'TVSum'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://github.com/yalesong/tvsum',\n",
       "    'repo': 'https://github.com/yalesong/tvsum',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/flic',\n",
       "  'name': 'FLIC',\n",
       "  'full_name': 'Frames Labelled in Cinema',\n",
       "  'homepage': 'https://bensapp.github.io/flic-dataset.html',\n",
       "  'description': 'The **FLIC** dataset contains 5003 images from popular Hollywood movies. The images were obtained by running a state-of-the-art person detector on every tenth frame of 30 movies. People detected with high confidence (roughly 20K candidates) were then sent to the crowdsourcing marketplace Amazon Mechanical Turk to obtain ground truth labelling. Each image was annotated by five Turkers to label 10 upper body joints. The median-of-five labelling was taken in each image to be robust to outlier annotation. Finally, images were rejected manually by if the person was occluded or severely non-frontal.\\r\\n\\r\\nSource: [https://bensapp.github.io/flic-dataset.html](https://bensapp.github.io/flic-dataset.html)\\r\\nImage Source: [https://www.tensorflow.org/datasets/catalog/flic](https://www.tensorflow.org/datasets/catalog/flic)',\n",
       "  'paper': {'title': 'MODEC: Multimodal Decomposable Models for Human Pose Estimation',\n",
       "   'url': 'https://paperswithcode.com/paper/modec-multimodal-decomposable-models-for'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FLIC Elbows', 'FLIC Wrists', 'FLIC'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/flic',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikiart',\n",
       "  'name': 'WikiArt',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/cs-chan/ArtGAN/blob/master/WikiArt%20Dataset/README.md',\n",
       "  'description': '**WikiArt** contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing.\\r\\n\\r\\nSource: [Adding New Tasks to a Single Network with Weight Transformations using Binary Masks](https://arxiv.org/abs/1805.11119)\\r\\nImage Source: [https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e](https://towardsdatascience.com/the-non-treachery-of-dataset-df1f6cbe577e)',\n",
       "  'paper': {'title': 'Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-classification-of-fine-art'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Wikiart (Fine-grained 6 Tasks)', 'WikiArt'],\n",
       "  'num_papers': 32,\n",
       "  'data_loaders': [{'url': 'https://github.com/cs-chan/ArtGAN',\n",
       "    'repo': 'https://github.com/cs-chan/ArtGAN',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sim10k',\n",
       "  'name': 'Sim10k',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://fcav.engin.umich.edu/projects/driving-in-the-matrix',\n",
       "  'description': 'SIM10k is a synthetic dataset containing 10,000 images, which is rendered from the video game Grand Theft Auto V (GTA5).\\r\\n\\r\\nSource: [Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation](https://arxiv.org/abs/2003.10275)\\r\\nImage Source: [https://arxiv.org/pdf/1610.01983.pdf](https://arxiv.org/pdf/1610.01983.pdf)',\n",
       "  'paper': {'title': 'Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?',\n",
       "   'url': 'https://paperswithcode.com/paper/driving-in-the-matrix-can-virtual-worlds'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SIM10K to Cityscapes', 'SIM10K to BDD100K', 'Sim10k'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/eyediap',\n",
       "  'name': 'EYEDIAP',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.idiap.ch/dataset/eyediap',\n",
       "  'description': 'The **EYEDIAP** dataset is a dataset for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras. The recording methodology was designed by systematically including, and isolating, most of the variables which affect the remote gaze estimation algorithms:\\r\\n\\r\\n* Head pose variations.\\r\\n* Person variation.\\r\\n* Changes in ambient and sensing condition.\\r\\n* Types of target: screen or 3D object.\\r\\n\\r\\nSource: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)\\r\\nImage Source: [https://www.idiap.ch/dataset/eyediap](https://www.idiap.ch/dataset/eyediap)',\n",
       "  'paper': {'title': 'EYEDIAP: a database for the development and evaluation of gaze estimation algorithms from RGB and RGB-D cameras',\n",
       "   'url': 'https://doi.org/10.1145/2578153.2578190'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Gaze Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/gaze-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['EYEDIAP (screen target)',\n",
       "   'EYEDIAP (floating target)',\n",
       "   'EYEDIAP'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/g3d',\n",
       "  'name': 'G3D',\n",
       "  'full_name': 'Gaming 3D Dataset',\n",
       "  'homepage': 'http://velastin.dynu.com/G3D/G3D.html',\n",
       "  'description': 'The Gaming 3D Dataset (**G3D**) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: “punch right”, “punch left”, “kick right”, “kick left”, “defend”, “golf swing”, “tennis swing forehand”, “tennis swing backhand”, “tennis serve”, “throw bowling ball”, “aim and fire gun”, “walk”, “run”, “jump”, “climb”, “crouch”, “steer a car”, “wave”, “flap” and “clap”.\\r\\n\\r\\nSource: [Skeleton Based Action Recognition Using Translation-Scale Invariant Image Mapping And Multi-Scale Deep CNN](https://arxiv.org/abs/1704.05645)\\r\\nImage Source: [G3D: A gaming action dataset and real time action recognition evaluation framework](https://doi.org/10.1109/CVPRW.2012.6239175)',\n",
       "  'paper': {'title': 'G3D: A gaming action dataset and real time action recognition evaluation framework',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2012.6239175'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D'],\n",
       "  'tasks': [{'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Pose Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/pose-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Gaming 3D (G3D)', 'G3D'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/o-haze-1',\n",
       "  'name': 'O-HAZE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/',\n",
       "  'description': 'The O-Haze dataset contains 35 hazy images (size 2833×4657 pixels) for training. It has 5 hazy images for validation along with their corresponding ground truth images.\\r\\n\\r\\nSource: [Single image dehazing for a variety of haze scenarios using back projected pyramid network](https://arxiv.org/abs/2008.06713)\\r\\nImage Source: [https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/](https://data.vision.ee.ethz.ch/cvl/ntire18//o-haze/)',\n",
       "  'paper': {'title': 'O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images',\n",
       "   'url': 'https://paperswithcode.com/paper/o-haze-a-dehazing-benchmark-with-real-hazy'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Dehazing',\n",
       "    'url': 'https://paperswithcode.com/task/image-dehazing'},\n",
       "   {'task': 'Single Image Dehazing',\n",
       "    'url': 'https://paperswithcode.com/task/single-image-dehazing'},\n",
       "   {'task': 'SSIM', 'url': 'https://paperswithcode.com/task/ssim'}],\n",
       "  'languages': [],\n",
       "  'variants': ['O-Haze', 'O-HAZE'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/umist-1',\n",
       "  'name': 'UMIST',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://eprints.lincoln.ac.uk/id/eprint/16081/',\n",
       "  'description': 'The Sheffield (previously **UMIST**) Face Database consists of 564 images of 20 individuals (mixed race/gender/appearance). Each individual is shown in a range of poses from profile to frontal views – each in a separate directory labelled 1a, 1b, … 1t and images are numbered consecutively as they were taken. The files are all in PGM format, approximately 220 x 220 pixels with 256-bit grey-scale.\\n\\nSource: [https://www.visioneng.org.uk/datasets/](https://www.visioneng.org.uk/datasets/)\\nImage Source: [https://www.visioneng.org.uk/datasets/](https://www.visioneng.org.uk/datasets/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UMist', 'UMIST'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cvusa-1',\n",
       "  'name': 'CVUSA',\n",
       "  'full_name': 'Cross-View USA',\n",
       "  'homepage': 'http://mvrl.cs.uky.edu/datasets/cvusa/',\n",
       "  'description': 'A large dataset containing millions of pairs of ground-level and aerial/satellite images from across the United States.\\r\\n\\r\\nSource: [http://mvrl.cs.uky.edu/datasets/cvusa/](http://mvrl.cs.uky.edu/datasets/cvusa/)\\r\\nImage Source: [https://arxiv.org/pdf/1612.02709.pdf](https://arxiv.org/pdf/1612.02709.pdf)',\n",
       "  'paper': {'title': 'Wide-Area Image Geolocalization with Aerial Reference Imagery',\n",
       "   'url': 'https://paperswithcode.com/paper/wide-area-image-geolocalization-with-aerial'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Cross-View Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/cross-view-image-to-image-translation'},\n",
       "   {'task': 'Metric Learning',\n",
       "    'url': 'https://paperswithcode.com/task/metric-learning'},\n",
       "   {'task': 'Image-Based Localization',\n",
       "    'url': 'https://paperswithcode.com/task/image-based-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['cvusa', 'CVUSA'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fc100',\n",
       "  'name': 'FC100',\n",
       "  'full_name': 'Fewshot-CIFAR100',\n",
       "  'homepage': 'https://github.com/ElementAI/TADAM',\n",
       "  'description': 'The **FC100** dataset (**Fewshot-CIFAR100**) is a newly split dataset based on CIFAR-100 for few-shot learning. It contains 20 high-level categories which are divided into 12, 4, 4 categories for training, validation and test. There are 60, 20, 20 low-level classes in the corresponding split containing 600 images of size 32 × 32 per class. Smaller image size makes it more challenging for few-shot learning.\\r\\n\\r\\nSource: [Prototype Rectification for Few-Shot Learning](https://arxiv.org/abs/1911.10713)',\n",
       "  'paper': {'title': 'TADAM: Task dependent adaptive metric for improved few-shot learning',\n",
       "   'url': 'https://paperswithcode.com/paper/tadam-task-dependent-adaptive-metric-for'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Fewshot-CIFAR100 - 5-Shot Learning',\n",
       "   'Fewshot-CIFAR100 - 10-Shot Learning',\n",
       "   'Fewshot-CIFAR100 - 1-Shot Learning',\n",
       "   'FC100 5-way (10-shot)',\n",
       "   'FC100',\n",
       "   'FC100 5-way (5-shot)',\n",
       "   'FC100 5-way (1-shot)'],\n",
       "  'num_papers': 89,\n",
       "  'data_loaders': [{'url': 'https://github.com/ElementAI/TADAM',\n",
       "    'repo': 'https://github.com/ElementAI/TADAM',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-5i',\n",
       "  'name': 'PASCAL-5i',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/DeepTrial/pascal-5',\n",
       "  'description': '**PASCAL-5i** is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training.\\r\\n\\r\\nSource: [AMP: Adaptive Masked Proxies for Few-Shot Segmentation](https://arxiv.org/abs/1902.11123)\\r\\nImage Source: [https://arxiv.org/pdf/1709.03410.pdf](https://arxiv.org/pdf/1709.03410.pdf)',\n",
       "  'paper': {'title': 'One-Shot Learning for Semantic Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/one-shot-learning-for-semantic-segmentation'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Few-Shot Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Pascal5i', 'PASCAL-5i'],\n",
       "  'num_papers': 71,\n",
       "  'data_loaders': [{'url': 'https://github.com/DeepTrial/pascal-5',\n",
       "    'repo': 'https://github.com/DeepTrial/pascal-5',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/trajnet-1',\n",
       "  'name': 'TrajNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://trajnet.stanford.edu/',\n",
       "  'description': 'The **TrajNet** Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t−7,t−6,…,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,…,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird’s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird’s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird’s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants).\\r\\n\\r\\nSource: [Transformer Networks for Trajectory Forecasting](https://arxiv.org/abs/2003.08111)\\r\\nImage Source: [http://trajnet.stanford.edu/](http://trajnet.stanford.edu/)',\n",
       "  'paper': {'title': 'An Evaluation of Trajectory Prediction Approaches and Notes on the TrajNet Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/an-evaluation-of-trajectory-prediction'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-prediction'},\n",
       "   {'task': 'Trajectory Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-forecasting'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TrajNet++', 'TrajNet'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/set12',\n",
       "  'name': 'Set12',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': '**Set12** is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256×256.\\r\\n\\r\\nSource: [Designing and Training of A Dual CNN for Image Denoising](https://arxiv.org/abs/2007.03951)\\r\\nImage Source: [https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598](https://www.researchgate.net/figure/12-images-from-Set12-dataset_fig11_338424598)',\n",
       "  'paper': {'title': 'Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising',\n",
       "   'url': 'https://paperswithcode.com/paper/beyond-a-gaussian-denoiser-residual-learning'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Grayscale Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/grayscale-image-denoising'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Set12',\n",
       "   'Set12 sigma70',\n",
       "   'Set12 sigma50',\n",
       "   'Set12 sigma30',\n",
       "   'Set12 sigma25',\n",
       "   'Set12 sigma15'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/totalcapture',\n",
       "  'name': 'TotalCapture',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cvssp.org/data/totalcapture/',\n",
       "  'description': 'The **TotalCapture** dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions.\\r\\n\\r\\nSource: [Semantic Estimation of 3D Body Shape and Pose using Minimal Cameras](https://arxiv.org/abs/1908.03030)\\r\\nImage Source: [https://cvssp.org/data/totalcapture/](https://cvssp.org/data/totalcapture/)',\n",
       "  'paper': {'title': 'Total capture: 3D human pose estimation fusing video and inertial sensors',\n",
       "   'url': 'https://paperswithcode.com/paper/total-capture-3d-human-pose-estimation-fusing'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'},\n",
       "   {'task': '3D Absolute Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-absolute-human-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Total Capture', 'TotalCapture'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/i-haze-1',\n",
       "  'name': 'I-HAZE',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/',\n",
       "  'description': 'The I-Haze dataset contains 25 indoor hazy images (size 2833×4657 pixels) training. It has 5 hazy images for validation along with their corresponding ground truth images.\\n\\nSource: [Single image dehazing for a variety of haze scenarios using back projected pyramid network](https://arxiv.org/abs/2008.06713)\\nImage Source: [https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/](https://data.vision.ee.ethz.ch/cvl/ntire18//i-haze/)',\n",
       "  'paper': {'title': 'I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images',\n",
       "   'url': 'https://paperswithcode.com/paper/i-haze-a-dehazing-benchmark-with-real-hazy'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Dehazing',\n",
       "    'url': 'https://paperswithcode.com/task/image-dehazing'},\n",
       "   {'task': 'Single Image Dehazing',\n",
       "    'url': 'https://paperswithcode.com/task/single-image-dehazing'},\n",
       "   {'task': 'SSIM', 'url': 'https://paperswithcode.com/task/ssim'}],\n",
       "  'languages': [],\n",
       "  'variants': ['I-Haze', 'I-HAZE'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/seed-1',\n",
       "  'name': 'SEED',\n",
       "  'full_name': 'SJTU Emotion EEG Dataset',\n",
       "  'homepage': 'http://bcmi.sjtu.edu.cn/home/seed/index.html',\n",
       "  'description': \"The **SEED** dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones.\\r\\n\\r\\nSource: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)\\r\\nImage Source: [http://bcmi.sjtu.edu.cn/home/seed/index.html](http://bcmi.sjtu.edu.cn/home/seed/index.html)\",\n",
       "  'paper': {'title': 'Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks',\n",
       "   'url': 'https://paperswithcode.com/paper/investigating-critical-frequency-bands-and'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['EEG'],\n",
       "  'tasks': [{'task': 'Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition'},\n",
       "   {'task': 'EEG', 'url': 'https://paperswithcode.com/task/eeg'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SEED-IV', '\\u3000SEED', 'SEED'],\n",
       "  'num_papers': 45,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/shrec',\n",
       "  'name': 'SHREC',\n",
       "  'full_name': 'SHape REtrieval Contest',\n",
       "  'homepage': 'http://tosca.cs.technion.ac.il/book/shrec.html',\n",
       "  'description': 'The **SHREC** dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset.\\n\\nSource: [Exploiting Recurrent Neural Networks and Leap Motion Controller for Sign Language and Semaphoric Gesture Recognition](https://arxiv.org/abs/1803.10435)\\nImage Source: [http://tosca.cs.technion.ac.il/book/shrec.html](http://tosca.cs.technion.ac.il/book/shrec.html)',\n",
       "  'paper': {'title': 'A comparison of 3D shape retrieval methods based on a large-scale benchmark supporting multimodal queries',\n",
       "   'url': 'https://doi.org/10.1016/j.cviu.2014.10.006'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/gesture-recognition'},\n",
       "   {'task': 'Hand Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': '3D Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-recognition'},\n",
       "   {'task': 'Point Cloud Super Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/point-cloud-super-resolution'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DHG-28',\n",
       "   'DHG-14',\n",
       "   'SHREC11, Split16-4',\n",
       "   'SHREC',\n",
       "   'SHREC 2017',\n",
       "   'SHREC15',\n",
       "   'SHREC 2017 track on 3D Hand Gesture Recognition'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/florence3d',\n",
       "  'name': 'Florence3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/',\n",
       "  'description': 'The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples.\\r\\n\\r\\nSource: [https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/](https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/)\\r\\nImage Source: [https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/](https://www.micc.unifi.it/resources/datasets/florence-3d-actions-dataset/)',\n",
       "  'paper': {'title': 'Recognizing Actions from Depth Cameras as Weakly Aligned Multi-part Bag-of-Poses',\n",
       "   'url': 'https://doi.org/10.1109/CVPRW.2013.77'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': '3D Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-action-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Florence 3D', 'Florence3D'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/snap',\n",
       "  'name': 'SNAP',\n",
       "  'full_name': 'Standford Large Network Dataset Collection',\n",
       "  'homepage': 'https://snap.stanford.edu/data/',\n",
       "  'description': '**SNAP** is a collection of large network datasets. It includes graphs representing social networks, citation networks, web graphs, online communities, online reviews and more.\\r\\n\\r\\n[Social networks](http://snap.stanford.edu/data/#socnets) : online social networks, edges represent interactions between people\\r\\n\\r\\n[Networks with ground-truth communities](http://snap.stanford.edu/data/#communities) : ground-truth network communities in social and information networks\\r\\n\\r\\n[Communication networks](http://snap.stanford.edu/data/#email) : email communication networks with edges representing communication\\r\\n\\r\\n[Citation networks](http://snap.stanford.edu/data/#citnets) : nodes represent papers, edges represent citations\\r\\n\\r\\n[Collaboration networks](http://snap.stanford.edu/data/#canets) : nodes represent scientists, edges represent collaborations (co-authoring a paper)\\r\\n\\r\\n[Web graphs](http://snap.stanford.edu/data/#web) : nodes represent webpages and edges are hyperlinks\\r\\n\\r\\n[Amazon networks](http://snap.stanford.edu/data/#amazon) : nodes represent products and edges link commonly co-purchased products\\r\\n\\r\\n[Internet networks](http://snap.stanford.edu/data/#p2p) : nodes represent computers and edges communication\\r\\n\\r\\n[Road networks](http://snap.stanford.edu/data/#road) : nodes represent intersections and edges roads connecting the intersections\\r\\n\\r\\n[Autonomous systems](http://snap.stanford.edu/data/#as) : graphs of the internet\\r\\n\\r\\n[Signed networks](http://snap.stanford.edu/data/#signnets) : networks with positive and negative edges (friend/foe, trust/distrust)\\r\\n\\r\\n[Location-based online social networks](http://snap.stanford.edu/data/#locnet) : social networks with geographic check-ins\\r\\n\\r\\n[Wikipedia networks, articles, and metadata](http://snap.stanford.edu/data/#wikipedia) : talk, editing, voting, and article data from Wikipedia\\r\\n\\r\\n[Temporal networks](http://snap.stanford.edu/data/#temporal) : networks where edges have timestamps\\r\\n\\r\\n[Twitter and Memetracker](http://snap.stanford.edu/data/#twitter) : memetracker phrases, links and 467 million Tweets\\r\\n\\r\\n[Online communities](http://snap.stanford.edu/data/#onlinecoms) : data from online communities such as Reddit and Flickr\\r\\n\\r\\n[Online reviews](http://snap.stanford.edu/data/#reviews) : data from online review systems such as BeerAdvocate and Amazon\\r\\n\\r\\n[User actions](http://snap.stanford.edu/data/#actions) : actions of users on social platforms.\\r\\n\\r\\n[Face-to-face communication networks](http://snap.stanford.edu/data/#face2face) : networks of face-to-face (non-online) interactions\\r\\n\\r\\n[Graph classification datasets](http://snap.stanford.edu/data/#disjointgraphs) : disjoint graphs from different classes\\r\\n\\r\\nImage Source: [https://snap.stanford.edu/data/](https://snap.stanford.edu/data/)',\n",
       "  'paper': {'title': 'SNAP Datasets: Stanford large network dataset collection',\n",
       "   'url': 'http://snap.stanford.edu/data'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SNAP'],\n",
       "  'num_papers': 102,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bioasq',\n",
       "  'name': 'BioASQ',\n",
       "  'full_name': 'Biomedical Semantic Indexing and Question Answering',\n",
       "  'homepage': 'http://participants-area.bioasq.org/datasets/',\n",
       "  'description': '**BioASQ** is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets).\\r\\n\\r\\nSource: [Transferability of Natural Language Inference to Biomedical Question Answering](https://arxiv.org/abs/2007.00217)\\r\\nImage Source: [http://participants-area.bioasq.org/datasets/](http://participants-area.bioasq.org/datasets/)',\n",
       "  'paper': {'title': 'An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition',\n",
       "   'url': 'https://doi.org/10.1186/s12859-015-0564-6'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Biomedical Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/biomedical-information-retrieval'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BioASQ', 'BioASQ (BEIR)'],\n",
       "  'num_papers': 99,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/string',\n",
       "  'name': 'STRING',\n",
       "  'full_name': 'STRING',\n",
       "  'homepage': 'https://string-db.org/',\n",
       "  'description': '**STRING** is a collection of protein-protein interaction (PPI) networks.',\n",
       "  'paper': {'title': 'STRING v10: protein-protein interaction networks, integrated over the tree of life',\n",
       "   'url': 'https://doi.org/10.1093/nar/gku1003'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['STRING'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/openwebtext',\n",
       "  'name': 'OpenWebText',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://skylion007.github.io/OpenWebTextCorpus/',\n",
       "  'description': '**OpenWebText** is an open-source recreation of the [WebText](/dataset/webtext) corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).\\r\\n\\r\\nSource: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)',\n",
       "  'paper': {'title': 'OpenWebText corpus',\n",
       "   'url': 'http://Skylion007.github.io/OpenWebTextCorpus'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['OpenWebText'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/openwebtext',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/foursquare',\n",
       "  'name': 'Foursquare',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://sites.google.com/site/yangdingqi/home/foursquare-dataset',\n",
       "  'description': 'The **Foursquare** dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories).\\nAnother subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins.\\n\\nSource: [https://sites.google.com/site/yangdingqi/home/foursquare-dataset](https://sites.google.com/site/yangdingqi/home/foursquare-dataset)',\n",
       "  'paper': {'title': 'Participatory Cultural Mapping Based on Collective Behavior Data in Location-Based Social Networks',\n",
       "   'url': 'https://doi.org/10.1145/2814575'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Crime Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/crime-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Foursquare'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/peerread',\n",
       "  'name': 'PeerRead',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/allenai/PeerRead',\n",
       "  'description': 'PearRead is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers.\\n\\nSource: [https://github.com/allenai/PeerRead](https://github.com/allenai/PeerRead)',\n",
       "  'paper': {'title': 'A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications',\n",
       "   'url': 'https://paperswithcode.com/paper/a-dataset-of-peer-reviews-peerread-collection'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['PeerRead'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/peer_read',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/allenai/PeerRead',\n",
       "    'repo': 'https://github.com/allenai/PeerRead',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/kinship',\n",
       "  'name': 'Kinship',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://archive.ics.uci.edu/ml/datasets/kinship',\n",
       "  'description': 'This relational database consists of 24 unique names in two families (they have equivalent structures).\\n\\nSource: [https://archive.ics.uci.edu/ml/datasets/kinship](https://archive.ics.uci.edu/ml/datasets/kinship)',\n",
       "  'paper': {'title': 'Learning distributed representations of concepts',\n",
       "   'url': 'https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.408.7684&rep=rep1&type=pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Knowledge Graphs',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graphs'},\n",
       "   {'task': 'Automated Theorem Proving',\n",
       "    'url': 'https://paperswithcode.com/task/automated-theorem-proving'},\n",
       "   {'task': 'Inductive logic programming',\n",
       "    'url': 'https://paperswithcode.com/task/inductive-logic-programming'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Kinship'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mindboggle',\n",
       "  'name': 'Mindboggle',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://mindboggle.info/data.html',\n",
       "  'description': '**Mindboggle** is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 32 manually labeled parcels.\\n\\nSource: [Graph Convolutions on Spectral Embeddings: Learning of Cortical Surface Data](https://arxiv.org/abs/1803.10336)\\nImage Source: [https://mindboggle.info/data.html](https://mindboggle.info/data.html)',\n",
       "  'paper': {'title': 'Mindboggling morphometry of human brains',\n",
       "   'url': 'https://doi.org/10.1371/journal.pcbi.1005350'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs', '3D', 'Medical'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Graph Matching',\n",
       "    'url': 'https://paperswithcode.com/task/graph-matching'},\n",
       "   {'task': 'Graph Learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Mindboggle'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/learning-to-rank-challenge',\n",
       "  'name': 'Learning to Rank Challenge',\n",
       "  'full_name': 'Yahoo! Learning to Rank Challenge',\n",
       "  'homepage': 'https://webscope.sandbox.yahoo.com/catalog.php?datatype=c',\n",
       "  'description': 'The Yahoo! **Learning to Rank Challenge** dataset consists of 709,877 documents encoded in 700 features and sampled from query logs of the Yahoo! search engine, spanning 29,921 queries.\\r\\n\\r\\nSource: [Ranking for Relevance and Display Preferencesin Complex Presentation Layouts](https://arxiv.org/abs/1805.02404)',\n",
       "  'paper': {'title': 'Yahoo! Learning to Rank Challenge Overview',\n",
       "   'url': 'http://proceedings.mlr.press/v14/chapelle11a.html'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tabular'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Learning-To-Rank',\n",
       "    'url': 'https://paperswithcode.com/task/learning-to-rank'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Learning to Rank Challenge'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/linux',\n",
       "  'name': 'Linux',\n",
       "  'full_name': 'Linux Program Dependence Graphs',\n",
       "  'homepage': None,\n",
       "  'description': 'The LINUX dataset consists of 48,747 Program Dependence Graphs (PDG) generated from the **Linux** kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements\\n\\nSource: [Convolutional Set Matching for Graph Similarity](https://arxiv.org/abs/1810.10866)',\n",
       "  'paper': {'title': 'An Efficient Graph Indexing Method',\n",
       "   'url': 'https://doi.org/10.1109/ICDE.2012.28'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Graph Matching',\n",
       "    'url': 'https://paperswithcode.com/task/graph-matching'},\n",
       "   {'task': 'Graph Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/graph-similarity'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Linux'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aminer',\n",
       "  'name': 'AMiner',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.aminer.org/data',\n",
       "  'description': 'The **AMiner** Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others.',\n",
       "  'paper': {'title': 'ArnetMiner: extraction and mining of academic social networks',\n",
       "   'url': 'https://doi.org/10.1145/1401890.1402008'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AMiner'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/email-eu',\n",
       "  'name': 'Email-EU',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.cs.cornell.edu/~arb/data/email-Eu/',\n",
       "  'description': 'EmailEU is a directed temporal network constructed from email exchanges in a large European research institution for a 803-day period. It contains 986 email addresses as nodes and 332,334 emails as edges with timestamps. There are 42 ground truth departments in the dataset.\\n\\nSource: [gl2vec: Learning Feature Representation Using Graphlets for Directed Networks](https://arxiv.org/abs/1812.05473)',\n",
       "  'paper': {'title': 'Local Higher-Order Graph Clustering',\n",
       "   'url': 'https://doi.org/10.1145/3097983.3098069'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Email-EU'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imdb-binary',\n",
       "  'name': 'IMDB-BINARY',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**IMDB-BINARY** is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres.\\r\\n\\r\\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Graph Representation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-representation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IMDb-B', 'IMDB-BINARY'],\n",
       "  'num_papers': 169,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ncbi-disease-1',\n",
       "  'name': 'NCBI Disease',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/',\n",
       "  'description': 'The **NCBI Disease** corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM.\\r\\n\\r\\nSource: [A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization](https://arxiv.org/abs/1812.06081)',\n",
       "  'paper': {'title': 'NCBI disease corpus: A resource for disease name recognition and concept normalization',\n",
       "   'url': 'http://dx.doi.org/10.1016/j.jbi.2013.12.006'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NCBI-disease', 'NCBI Disease'],\n",
       "  'num_papers': 69,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/ncbi_disease',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/arxiv-astro-ph',\n",
       "  'name': 'arXiv Astro-Ph',\n",
       "  'full_name': 'arXiv Astro Physics',\n",
       "  'homepage': 'https://snap.stanford.edu/data/ca-AstroPh.html',\n",
       "  'description': 'Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\\n\\nSource: [https://snap.stanford.edu/data/ca-AstroPh.html](https://snap.stanford.edu/data/ca-AstroPh.html)',\n",
       "  'paper': {'title': 'SNAP Datasets: Stanford large network dataset collection',\n",
       "   'url': 'http://snap.stanford.edu/data'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Clique Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/clique-prediction'},\n",
       "   {'task': 'Network Embedding',\n",
       "    'url': 'https://paperswithcode.com/task/network-embedding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['arXiv-AstroPh 2-clique',\n",
       "   'arXiv-AstroPh 4-clique',\n",
       "   'arXiv Astro-Ph'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mslr-web10k',\n",
       "  'name': 'MSLR-WEB10K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/project/mslr/',\n",
       "  'description': 'The **MSLR-WEB10K** dataset consists of 10,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair. It is a subset of the MSLR-WEB30K dataset.\\r\\n\\r\\nSource: [Dueling Bandits with Qualitative Feedback](https://arxiv.org/abs/1809.05274)',\n",
       "  'paper': {'title': 'Introducing LETOR 4.0 Datasets',\n",
       "   'url': 'https://paperswithcode.com/paper/13062597'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Ranking'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Learning-To-Rank',\n",
       "    'url': 'https://paperswithcode.com/task/learning-to-rank'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MSLR-WEB10K'],\n",
       "  'num_papers': 22,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/beeradvocate',\n",
       "  'name': 'BeerAdvocate',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://snap.stanford.edu/data/web-BeerAdvocate.html',\n",
       "  'description': 'BeerAdvocate is a dataset that consists of beer reviews from beeradvocate. The data span a period of more than 10 years, including all ~1.5 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review.\\r\\n\\r\\nSource: [https://snap.stanford.edu/data/web-BeerAdvocate.html](https://snap.stanford.edu/data/web-BeerAdvocate.html)',\n",
       "  'paper': {'title': 'From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews',\n",
       "   'url': 'https://paperswithcode.com/paper/from-amateurs-to-connoisseurs-modeling-the'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['BeerAdvocate'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/epinion',\n",
       "  'name': 'Epinion',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm',\n",
       "  'description': 'The **Epinion**s dataset is trust network dataset. For each user, it contains his profile, his ratings and his trust relations. For each rating, it has the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating.\\n\\nSource: [https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm](https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm)',\n",
       "  'paper': {'title': 'eTrust: understanding trust evolution in an online world',\n",
       "   'url': 'https://doi.org/10.1145/2339530.2339574'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Outlier Detection',\n",
       "    'url': 'https://paperswithcode.com/task/outlier-detection'},\n",
       "   {'task': 'Change Point Detection',\n",
       "    'url': 'https://paperswithcode.com/task/change-point-detection'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Epinions-Extend', 'Epinion'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-light-field',\n",
       "  'name': 'Stanford Light Field',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://lightfield.stanford.edu/',\n",
       "  'description': 'The **Stanford Light Field** Archive is a collection of several light fields for research in computer graphics and vision.\\r\\n\\r\\nSource: [http://lightfield.stanford.edu/](http://lightfield.stanford.edu/)\\r\\nImage Source: [http://lightfield.stanford.edu/](http://lightfield.stanford.edu/)',\n",
       "  'paper': {'title': 'The (New) Stanford Light Field Archive',\n",
       "   'url': 'http://lightfield.stanford.edu/'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/super-resolution'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford Light Field'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/arxiv-gr-qc',\n",
       "  'name': 'Arxiv GR-QC',\n",
       "  'full_name': 'General Relativity and Quantum Cosmology collaboration network',\n",
       "  'homepage': 'https://snap.stanford.edu/data/ca-GrQc.html',\n",
       "  'description': '**Arxiv GR-QC** (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes.\\n\\nSource: [https://snap.stanford.edu/data/ca-GrQc.html](https://snap.stanford.edu/data/ca-GrQc.html)',\n",
       "  'paper': {'title': 'Graph evolution: Densification and shrinking diameters',\n",
       "   'url': 'https://doi.org/10.1145/1217299.1217301'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Clique Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/clique-prediction'},\n",
       "   {'task': 'Active Learning',\n",
       "    'url': 'https://paperswithcode.com/task/active-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['arXiv-GrQc 2-clique', 'arXiv-GrQc 3-clique', 'Arxiv GR-QC'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/orkut',\n",
       "  'name': 'Orkut',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://snap.stanford.edu/data/com-Orkut.html',\n",
       "  'description': '**Orkut** is a social network dataset consisting of friendship social network and ground-truth communities from Orkut.com on-line social network where users form friendship each other.\\r\\n\\r\\nEach connected component in a group is regarded as a separate ground-truth community. The ground-truth communities which have less than 3 nodes are removed. The dataset also provides the top 5,000 communities with highest quality and the largest connected component of the network.\\r\\n\\r\\nSource: [https://snap.stanford.edu/data/com-Orkut.html](https://snap.stanford.edu/data/com-Orkut.html)',\n",
       "  'paper': {'title': 'Defining and Evaluating Network Communities based on Ground-truth',\n",
       "   'url': 'https://paperswithcode.com/paper/defining-and-evaluating-network-communities'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Community Detection',\n",
       "    'url': 'https://paperswithcode.com/task/community-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Orkut'],\n",
       "  'num_papers': 57,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/friendster',\n",
       "  'name': 'Friendster',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://snap.stanford.edu/data/com-Friendster.html',\n",
       "  'description': '**Friendster** is an on-line gaming network. Before re-launching as a game website, Friendster was a social networking site where users can form friendship edge each other. Friendster social network also allows users form a group which other members can then join. The Friendster dataset consist of ground-truth communities (based on user-defined groups) and the social network from induced subgraph of the nodes that either belong to at least one community or are connected to other nodes that belong to at least one community.\\r\\n\\r\\nSource: [https://snap.stanford.edu/data/com-Friendster.html](https://snap.stanford.edu/data/com-Friendster.html)',\n",
       "  'paper': {'title': 'Defining and Evaluating Network Communities based on Ground-truth',\n",
       "   'url': 'https://paperswithcode.com/paper/defining-and-evaluating-network-communities'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Node Classification',\n",
       "    'url': 'https://paperswithcode.com/task/node-classification'},\n",
       "   {'task': 'Graph Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-clustering'},\n",
       "   {'task': 'Graph Sampling',\n",
       "    'url': 'https://paperswithcode.com/task/graph-sampling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Friendster'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mq2008',\n",
       "  'name': 'MQ2008',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/',\n",
       "  'description': 'The **MQ2008** dataset is a dataset for Learning to Rank. It contains 800 queries with labelled documents.',\n",
       "  'paper': {'title': 'Introducing LETOR 4.0 Datasets',\n",
       "   'url': 'https://paperswithcode.com/paper/13062597'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Ranking'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Document Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/document-ranking'},\n",
       "   {'task': 'Learning-To-Rank',\n",
       "    'url': 'https://paperswithcode.com/task/learning-to-rank'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MQ2008'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imdb-multi',\n",
       "  'name': 'IMDB-MULTI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**IMDB-MULTI** is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi.\\r\\n\\r\\nSource: [Learning metrics for persistence-based summaries and applications for graph classification](https://arxiv.org/abs/1904.12189)',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-classification'},\n",
       "   {'task': 'Graph Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/graph-similarity'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IMDb-M', 'IMDB-MULTI'],\n",
       "  'num_papers': 150,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reddit-12k',\n",
       "  'name': 'REDDIT-12K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': 'Reddit12k contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community.\\r\\n\\r\\nSource: [Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity](https://arxiv.org/abs/1904.01098)',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Quantization',\n",
       "    'url': 'https://paperswithcode.com/task/quantization'},\n",
       "   {'task': 'Topological Data Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/topological-data-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['REDDIT-12K'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reddit-binary',\n",
       "  'name': 'REDDIT-BINARY',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': '**REDDIT-BINARY** consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other’s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community.\\r\\n\\r\\nSource: [A simple yet effective baseline for non-attributed graph classification](https://arxiv.org/abs/1811.03508)',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Graph Representation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/graph-representation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['REDDIT-BINARY'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mq2007',\n",
       "  'name': 'MQ2007',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/',\n",
       "  'description': 'The **MQ2007** dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are “relevant”, “partially relevant”, and “not relevant”.\\r\\n\\r\\nSource: [ARSM GRADIENT ESTIMATOR FOR SUPERVISED LEARNING TO RANK](https://arxiv.org/abs/1911.00465)',\n",
       "  'paper': {'title': 'Introducing LETOR 4.0 Datasets',\n",
       "   'url': 'https://paperswithcode.com/paper/13062597'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Ranking'],\n",
       "  'tasks': [{'task': 'Meta-Learning',\n",
       "    'url': 'https://paperswithcode.com/task/meta-learning'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Learning-To-Rank',\n",
       "    'url': 'https://paperswithcode.com/task/learning-to-rank'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MQ2007'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/amazon-fine-foods',\n",
       "  'name': 'Amazon Fine Foods',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://snap.stanford.edu/data/web-FineFoods.html',\n",
       "  'description': 'Amazon Fine Foods is a dataset that consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review.\\r\\n\\r\\nSource: [https://snap.stanford.edu/data/web-FineFoods.html](https://snap.stanford.edu/data/web-FineFoods.html)',\n",
       "  'paper': {'title': 'From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews',\n",
       "   'url': 'https://paperswithcode.com/paper/from-amateurs-to-connoisseurs-modeling-the'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'},\n",
       "   {'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Amazon Fine Foods'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/reddit-5k',\n",
       "  'name': 'REDDIT-5K',\n",
       "  'full_name': 'REDDIT-MULTI-5K',\n",
       "  'homepage': 'https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets',\n",
       "  'description': 'Reddit-5K is a relational dataset extracted from Reddit.',\n",
       "  'paper': {'title': 'Deep Graph Kernels',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-graph-kernels'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Classification',\n",
       "    'url': 'https://paperswithcode.com/task/graph-classification'},\n",
       "   {'task': 'Topological Data Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/topological-data-analysis'}],\n",
       "  'languages': [],\n",
       "  'variants': ['REDDIT-MULTI-5k', 'REDDIT-5K'],\n",
       "  'num_papers': 56,\n",
       "  'data_loaders': [{'url': 'https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.TUDataset',\n",
       "    'repo': 'https://github.com/dmlc/dgl',\n",
       "    'frameworks': ['pytorch', 'tf', 'mxnet']},\n",
       "   {'url': 'https://graphneural.network/datasets/#tudataset',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lastfm-asia',\n",
       "  'name': 'LastFM Asia',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/benedekrozemberczki/FEATHER',\n",
       "  'description': 'A social network of LastFM users which was collected from the public API in March 2020. Nodes are LastFM users from Asian countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is multinomial node classification - one has to predict the location of users. This target feature was derived from the country field for each user.',\n",
       "  'paper': {'title': 'Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models',\n",
       "   'url': 'https://paperswithcode.com/paper/characteristic-functions-on-graphs-birds-of-a'},\n",
       "  'introduced_date': '2020-05-16',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Graph Embedding',\n",
       "    'url': 'https://paperswithcode.com/task/graph-embedding'},\n",
       "   {'task': 'Graph Sampling',\n",
       "    'url': 'https://paperswithcode.com/task/graph-sampling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LastFM Asia'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/emnist',\n",
       "  'name': 'EMNIST',\n",
       "  'full_name': 'Extended MNIST',\n",
       "  'homepage': 'https://www.nist.gov/itl/products-and-services/emnist-dataset',\n",
       "  'description': '**EMNIST** (extended MNIST) has 4 times more data than [MNIST](/dataset/mnist). It is a set of handwritten digits with a 28 x 28 format.\\r\\n\\r\\nSource: [Domain Discrepancy Measure for Complex Models in Unsupervised Domain Adaptation](https://arxiv.org/abs/1901.10654)\\r\\nImage Source: [https://arxiv.org/pdf/1803.01900.pdf](https://arxiv.org/pdf/1803.01900.pdf)',\n",
       "  'paper': {'title': 'EMNIST: an extension of MNIST to handwritten letters',\n",
       "   'url': 'https://paperswithcode.com/paper/emnist-an-extension-of-mnist-to-handwritten'},\n",
       "  'introduced_date': '2017-02-17',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['EMNIST-Letters', 'EMNIST-Digits', 'EMNIST-Balanced', 'EMNIST'],\n",
       "  'num_papers': 155,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.EMNIST',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://docs.activeloop.ai/datasets',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/emnist',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'repo': 'https://gitlab.com/afagarap/pt-datasets',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/arcade-learning-environment',\n",
       "  'name': 'Arcade Learning Environment',\n",
       "  'full_name': 'Arcade Learning Environment',\n",
       "  'homepage': 'https://github.com/mgbellemare/Arcade-Learning-Environment',\n",
       "  'description': 'The **Arcade Learning Environment** (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design.\\r\\n\\r\\nSource: [https://github.com/mgbellemare/Arcade-Learning-Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)\\r\\nImage Source: [https://github.com/muupan/async-rl/blob/master/README.md](https://github.com/muupan/async-rl/blob/master/README.md)',\n",
       "  'paper': {'title': 'The Arcade Learning Environment: An Evaluation Platform for General Agents',\n",
       "   'url': 'https://paperswithcode.com/paper/the-arcade-learning-environment-an-evaluation'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Atari Games',\n",
       "    'url': 'https://paperswithcode.com/task/atari-games'},\n",
       "   {'task': \"Montezuma's Revenge\",\n",
       "    'url': 'https://paperswithcode.com/task/montezumas-revenge'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Atari 2600 Alien',\n",
       "   'Atari 2600 Amidar',\n",
       "   'Atari 2600 Berzerk',\n",
       "   'Atari 2600 Bowling',\n",
       "   'Atari 2600 Gopher',\n",
       "   'Atari 2600 HERO',\n",
       "   'Atari 2600 Krull',\n",
       "   'Atari 2600 Phoenix',\n",
       "   'Atari 2600 Pong',\n",
       "   'Atari 2600 Pooyan',\n",
       "   'Atari 2600 Q*Bert',\n",
       "   'Atari 2600 Skiing',\n",
       "   'Atari 2600 Tennis',\n",
       "   'Atari 2600 Venture',\n",
       "   'Atari 2600 Zaxxon',\n",
       "   'Atari-57',\n",
       "   'Atari 2600 Assault',\n",
       "   'Atari 2600 Asteroids',\n",
       "   'Atari 2600 Atlantis',\n",
       "   'Atari 2600 Bank Heist',\n",
       "   'Atari 2600 Battle Zone',\n",
       "   'Atari 2600 Beam Rider',\n",
       "   'Atari 2600 Carnival',\n",
       "   'Atari 2600 Centipede',\n",
       "   'Atari 2600 Chopper Command',\n",
       "   'Atari 2600 Crazy Climber',\n",
       "   'Atari 2600 Demon Attack',\n",
       "   'Atari 2600 Double Dunk',\n",
       "   'Atari 2600 Elevator Action',\n",
       "   'Atari 2600 Fishing Derby',\n",
       "   'Atari 2600 Freeway',\n",
       "   'Atari 2600 Frostbite',\n",
       "   'Atari 2600 Gravitar',\n",
       "   'Atari 2600 Ice Hockey',\n",
       "   'Atari 2600 James Bond',\n",
       "   'Atari 2600 Journey Escape',\n",
       "   'Atari 2600 Kangaroo',\n",
       "   'Atari 2600 Kung-Fu Master',\n",
       "   \"Atari 2600 Montezuma's Revenge\",\n",
       "   'Atari 2600 Ms. Pacman',\n",
       "   'Atari 2600 Name This Game',\n",
       "   'Atari 2600 Pitfall!',\n",
       "   'Atari 2600 Private Eye',\n",
       "   'Atari 2600 River Raid',\n",
       "   'Atari 2600 Road Runner',\n",
       "   'Atari 2600 Robotank',\n",
       "   'Atari 2600 Seaquest',\n",
       "   'Atari 2600 Space Invaders',\n",
       "   'Atari 2600 Surround',\n",
       "   'Atari 2600 Time Pilot',\n",
       "   'Atari 2600 Tutankham',\n",
       "   'Atari 2600 Up and Down',\n",
       "   'Atari 2600 Video Pinball',\n",
       "   'Atari 2600 Wizard of Wor',\n",
       "   'Atari 2600 Yars Revenge',\n",
       "   'Arcade Learning Environment',\n",
       "   'Atari 2600 Breakout'],\n",
       "  'num_papers': 282,\n",
       "  'data_loaders': [{'url': 'https://github.com/mgbellemare/Arcade-Learning-Environment',\n",
       "    'repo': 'https://github.com/mgbellemare/Arcade-Learning-Environment',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/medleydb',\n",
       "  'name': 'MedleyDB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://medleydb.weebly.com/',\n",
       "  'description': \"**MedleyDB**, is a dataset of annotated, royalty-free multitrack recordings. It was curated primarily to support research on melody extraction. For each song melody f₀ annotations are provided as well as instrument activations for evaluating automatic instrument recognition. The original dataset consists of 122 multitrack songs out of which 108 include melody annotations.\\r\\n\\r\\nThe songs in MedleyDB were obtained from the following sources:\\r\\n\\r\\n* Independent Artists (30 songs)\\r\\n* NYU's Dolan Recording Studio (32 songs)\\r\\n* Weathervane Music (25 songs)\\r\\n* Music Delta (35 songs)\\r\\n\\r\\nMedleyDB contains songs of a variety of musical genres: Singer/Songwriter, Classical, Rock, World/Folk, Fusion, Jazz, Pop, Musical Theatre, Rap. For each song three types of audio content are given: a mix, stems, and raw audio. All types of audio files are .wav files with a sample rate of 44.1 kHz and a bit depth of 16.\\r\\n\\r\\nSource: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\\r\\nImage Source: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\\r\\nAudio Source: [https://zenodo.org/record/1438309](https://zenodo.org/record/1438309)\",\n",
       "  'paper': {'title': 'MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research',\n",
       "   'url': 'http://www.terasoft.com.tw/conf/ismir2014/proceedings/T028_322_Paper.pdf'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MedleyDB'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/medleydb-2-0',\n",
       "  'name': 'MedleyDB 2.0',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://medleydb.weebly.com/',\n",
       "  'description': '**MedleyDB 2.0** is a superset of the MedleyDB – a dataset of annotated, royalty-free multitrack recordings. The second iteration of the dataset includes 74 new multitrack recordings resulting in 194 songs in total.\\n\\nSource: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\\nImage Source: [https://medleydb.weebly.com/](https://medleydb.weebly.com/)\\nAudio Source: [https://zenodo.org/record/1438309](https://zenodo.org/record/1438309)',\n",
       "  'paper': {'title': 'MedleyDB 2.0: New Data and a System for Sustainable Data Collection',\n",
       "   'url': 'https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/08/bittner-medleydb.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Audio Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/audio-super-resolution'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MedleyDB 2.0'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mir-1k',\n",
       "  'name': 'MIR-1K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://mirlab.org/dataset/public/',\n",
       "  'description': '**MIR-1K** (Multimedia Information Retrieval lab, 1000 song clips) is a dataset designed for singing voice separation. It contains:\\r\\n\\r\\n* 1000 song clips with the music accompaniment and the singing voice recorded as left and right channels, respectively,\\r\\n* Manual annotations of pitch contours in semitone, indices and types for unvoiced frames, lyrics, and vocal/non-vocal segments,\\r\\n* The speech recordings of the lyrics by the same person who sang the songs.\\r\\n\\r\\nThe duration of each clip ranges from 4 to 13 seconds, and the total length of the dataset is 133 minutes. These clips are extracted from 110 karaoke songs which contain a mixture track and a music accompaniment track. These songs are freely selected from 5000 Chinese pop songs and sung by researchers from MIR lab (8 females and 11 males). Most of the singers are amateur and do not have professional music training.\\r\\n\\r\\nSource: [https://sites.google.com/site/unvoicedsoundseparation/mir-1k](https://sites.google.com/site/unvoicedsoundseparation/mir-1k)\\r\\nAudio Source: [https://sites.google.com/site/unvoicedsoundseparation/sounddemosforjournal](https://sites.google.com/site/unvoicedsoundseparation/sounddemosforjournal)',\n",
       "  'paper': {'title': 'On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset',\n",
       "   'url': 'https://doi.org/10.1109/TASL.2009.2026503'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Speech Separation',\n",
       "    'url': 'https://paperswithcode.com/task/speech-separation'},\n",
       "   {'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['MIR-1K'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/magnatagatune',\n",
       "  'name': 'MagnaTagATune',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset',\n",
       "  'description': '**MagnaTagATune** dataset contains 25,863 music clips. Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists. The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. Each audio clip is supplied with a vector of binary annotations of 188 tags. These annotations are obtained by humans playing the two-player online TagATune game. In this game, the two players are either presented with the same or a different audio clip. Subsequently, they are asked to come up with tags for their specific audio clip. Afterward, players view each other’s tags and are asked to decide whether they were presented the same audio clip. Tags are only assigned when more than two players agreed. The annotations include tags like ’singer’, ’no singer’, ’violin’, ’drums’, ’classical’, ’jazz’. The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag. There are 16 parts, and researchers comonnly use parts 1-12 for training, part 13 for validation and parts 14-16 for testing.\\r\\n\\r\\nSource: [Brains on Beats](https://arxiv.org/abs/1606.02627)\\r\\nAudio Source: [http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset](http://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)',\n",
       "  'paper': {'title': 'Evaluation of Algorithms Using Games: The Case of Music Tagging',\n",
       "   'url': 'http://ismir2009.ismir.net/proceedings/OS5-5.pdf'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Music Auto-Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/music-auto-tagging'},\n",
       "   {'task': 'Music Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/music-tagging'},\n",
       "   {'task': 'Music Classification',\n",
       "    'url': 'https://paperswithcode.com/task/music-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MagnaTagATune'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lakh-midi-dataset',\n",
       "  'name': 'Lakh MIDI Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://colinraffel.com/projects/lmd/',\n",
       "  'description': 'The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level.\\r\\n\\r\\nLMD-full denotes the whole dataset. LMD-matched is the subset of LMD-full that consists of MIDI files matched with the Million Song Dataset entries. LMD-aligned contains all the files of LMD-matched, aligned to preview MP3s from the Million Song Dataset.\\r\\n\\r\\nA lakh is a unit of measure used in the Indian number system which signifies 100,000.\\r\\n\\r\\nSource: [https://colinraffel.com/projects/lmd/](https://colinraffel.com/projects/lmd/)\\r\\nAudio Source: [https://colinraffel.com/projects/lmd/](https://colinraffel.com/projects/lmd/)',\n",
       "  'paper': {'title': 'Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching',\n",
       "   'url': 'http://colinraffel.com/publications/thesis.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Lyrics'],\n",
       "  'tasks': [{'task': 'Music Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/music-modeling'},\n",
       "   {'task': 'Music Generation',\n",
       "    'url': 'https://paperswithcode.com/task/music-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Lakh MIDI Dataset'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ikala',\n",
       "  'name': 'iKala',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://mac.citi.sinica.edu.tw/ikala/',\n",
       "  'description': 'The **iKala** dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided.\\r\\n\\r\\nThis dataset is not available anymore.\\r\\n\\r\\nSource: [http://mac.citi.sinica.edu.tw/ikala/](http://mac.citi.sinica.edu.tw/ikala/)',\n",
       "  'paper': {'title': 'Vocal activity informed singing voice separation with the iKala dataset',\n",
       "   'url': 'https://doi.org/10.1109/ICASSP.2015.7178063'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Lyrics'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Style Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/style-transfer'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['iKala'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cal500',\n",
       "  'name': 'CAL500',\n",
       "  'full_name': 'Computer Audition Lab 500',\n",
       "  'homepage': 'http://calab1.ucsd.edu/~datasets/',\n",
       "  'description': '**CAL500** (**Computer Audition Lab 500**) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:\\r\\n\\r\\n* 29 instruments were annotated as present in the song or not,\\r\\n* 22 vocal characteristics were annotated as relevant to the singer or not,\\r\\n* 36 genres,\\r\\n* 18 emotions were rated on a scale from one to three (e.g., ``not happy\", ``neutral\", ``happy\"),\\r\\n* 15 song concepts describing the acoustic qualities of the song, artist and recording (e.g., tempo, energy, sound quality),\\r\\n* 15 usage terms (e.g., \"I would listen to this song while driving, sleeping, etc.\").\\r\\n\\r\\nSource: [http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt](http://calab1.ucsd.edu/~datasets/cal500/details_cal500.txt)\\r\\nAudio Source: [http://calab1.ucsd.edu/~datasets/cal500/cal500data/](http://calab1.ucsd.edu/~datasets/cal500/cal500data/)',\n",
       "  'paper': {'title': 'Semantic Annotation and Retrieval of Music and Sound Effects',\n",
       "   'url': 'https://doi.org/10.1109/TASL.2007.913750'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Tabular'],\n",
       "  'tasks': [{'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Matrix Completion',\n",
       "    'url': 'https://paperswithcode.com/task/matrix-completion'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CAL500'],\n",
       "  'num_papers': 18,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/urmp',\n",
       "  'name': 'URMP',\n",
       "  'full_name': 'University of Rochester Multi-Modal Musical Performance',\n",
       "  'homepage': 'http://www2.ece.rochester.edu/projects/air/projects/URMP.html',\n",
       "  'description': '**URMP** (**University of Rochester Multi-Modal Musical Performance**) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces.\\n\\nSource: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)\\nImage Source: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)\\nAudio Source: [http://www2.ece.rochester.edu/projects/air/projects/URMP.html](http://www2.ece.rochester.edu/projects/air/projects/URMP.html)',\n",
       "  'paper': {'title': 'Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications',\n",
       "   'url': 'https://doi.org/10.1109/TMM.2018.2856090'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Audio'],\n",
       "  'tasks': [{'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'},\n",
       "   {'task': 'Music Generation',\n",
       "    'url': 'https://paperswithcode.com/task/music-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['URMP'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fma',\n",
       "  'name': 'FMA',\n",
       "  'full_name': 'Free Music Archive',\n",
       "  'homepage': 'https://github.com/mdeff/fma',\n",
       "  'description': 'The **Free Music Archive** (**FMA**) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies.\\r\\n\\r\\nThere are four subsets defined by the authors:\\r\\n\\r\\n* Full: the complete dataset,\\r\\n* Large: the full dataset with audio limited to 30 seconds clips extracted from the middle of the tracks (or entire track if shorter than 30 seconds),\\r\\n* Medium: a selection of 25,000 30s clips having a single root genre,\\r\\n* Small: a balanced subset containing 8,000 30s clips with 1,000 clips per one of 8 root genres.\\r\\n\\r\\nThe official split into training, validation and test sets (80/10/10) uses stratified sampling to preserve the percentage of tracks per genre. Songs of the same artists are part of one set only.\\r\\n\\r\\nSource: [FMA: A Dataset For Music Analysis](https://arxiv.org/pdf/1612.01840.pdf)\\r\\nAudio Source: [https://github.com/mdeff/fma](https://github.com/mdeff/fma)',\n",
       "  'paper': {'title': 'FMA: A Dataset For Music Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/fma-a-dataset-for-music-analysis'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Genre classification',\n",
       "    'url': 'https://paperswithcode.com/task/genre-classification'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Free Music Archive', 'FMA'],\n",
       "  'num_papers': 55,\n",
       "  'data_loaders': [{'url': 'https://github.com/mdeff/fma',\n",
       "    'repo': 'https://github.com/mdeff/fma',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ccmixter',\n",
       "  'name': 'CCMixter',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://members.loria.fr/ALiutkus/kam/',\n",
       "  'description': '**CCMixter** is a singing voice separation dataset consisting of 50 full-length stereo tracks from [ccMixter](www.ccmixter.org) featuring many different musical genres. For each song there are three WAV files available: the background music, the voice signal, and their sum.\\n\\nSource: [Kernel Additive Models for Source Separation](https://doi.org/10.1109/TSP.2014.2332434)\\nAudio Source: [https://members.loria.fr/ALiutkus/kam/](https://members.loria.fr/ALiutkus/kam/)',\n",
       "  'paper': {'title': 'Kernel Additive Models for Source Separation',\n",
       "   'url': 'https://doi.org/10.1109/TSP.2014.2332434'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['CCMixter'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/goodsounds',\n",
       "  'name': 'GoodSounds',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.upf.edu/web/mtg/good-sounds',\n",
       "  'description': '**GoodSounds** dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (“good-sound”, “bad”, “scale-good”, “scale-bad”).\\n\\nSource: [A real-time system for measuring sound goodness in instrumental sounds](http://mtg.upf.edu/node/3197)\\nImage Source: [A real-time system for measuring sound goodness in instrumental sounds](http://mtg.upf.edu/node/3197)\\nAudio Source: [https://zenodo.org/record/820937](https://zenodo.org/record/820937)',\n",
       "  'paper': {'title': 'A real-time system for measuring sound goodness in instrumental sounds',\n",
       "   'url': 'http://mtg.upf.edu/node/3197'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'},\n",
       "   {'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GoodSounds'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/jamendo-corpus',\n",
       "  'name': 'Jamendo Corpus',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://zenodo.org/record/2585988',\n",
       "  'description': 'The **Jamendo Corpus** is a voice detection dataset consisting of 93 songs with Creative Commons license from the [Jamendo](http://www.jamendo.com/) free music sharing website. Segments of each song are annotated as “voice” (sung or spoken) or “no-voice”. The songs constitute a total of about 6 hours of music. The files are all from different artists and represent various genres from mainstream commercial music. The Jamendo audio files are coded in stereo Vorbis OGG 44.1kHz with 112KB/s bitrate. The original split contains 61, 16 and 16 songs in training, validation and testing set, respectively.\\n\\nSource: [Vocal detection in music with support vector machines](https://perso.telecom-paristech.fr/grichard/Publications/Icassp08_ramona.pdf)\\nAudio Source: [https://zenodo.org/record/2585988](https://zenodo.org/record/2585988)',\n",
       "  'paper': {'title': 'Vocal detection in music with support vector machines',\n",
       "   'url': 'https://perso.telecom-paristech.fr/grichard/Publications/Icassp08_ramona.pdf'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': ['English', 'French'],\n",
       "  'variants': ['Jamendo Corpus'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/foredeck',\n",
       "  'name': 'ForeDeCk',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://fsudataset.com/',\n",
       "  'description': '**ForeDeCk** is a time series database compiled at the National Technical University of Athens that contains 900,000 continuous time series, built from multiple, diverse and publicly accessible sources. ForeDeCk emphasizes business forecasting applications, including series from relevant domains such as industries, services, tourism, imports & exports, demographics, education, labor & wage, government, households, bonds, stocks, insurances, loans, real estate, transportation, and natural resources & environment.\\n\\nSource: [Are forecasting competitions data representative of the reality?](https://www.sciencedirect.com/science/article/abs/pii/S0169207019300159)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Time series'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['ForeDeCk'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/m4',\n",
       "  'name': 'M4',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/Mcompetitions/M4-methods',\n",
       "  'description': 'The **M4** dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones.\\r\\n\\r\\nThe M4 dataset was created by selecting a random sample of 100,000 time series from the ForeDeCk database. The selected series were then scaled to prevent negative observations and values lower than 10, thus avoiding possible problems when calculating various error measures. The scaling was performed by simply adding a constant to the series so that their minimum value was equal to 10 (29 occurrences across the whole dataset). In addition, any information that could possibly lead to the identification of the original series was removed so as to ensure the objectivity of the results. This included the starting dates of the series, which did not become available to the participants until the M4 had ended.\\r\\n\\r\\nSource: [The M4 competition: Results, findings, conclusion and way forward](https://doi.org/10.1016/j.ijforecast.2018.06.001)\\r\\nImage Source: [Are forecasting competitions data representative of the reality?](https://www.sciencedirect.com/science/article/abs/pii/S0169207019300159)',\n",
       "  'paper': {'title': 'The M4 competition: Results, findings, conclusion and way forward',\n",
       "   'url': 'https://doi.org/10.1016/j.ijforecast.2018.06.001'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Time series'],\n",
       "  'tasks': [{'task': 'Meta-Learning',\n",
       "    'url': 'https://paperswithcode.com/task/meta-learning'},\n",
       "   {'task': 'Time Series Forecasting',\n",
       "    'url': 'https://paperswithcode.com/task/time-series-forecasting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['M4'],\n",
       "  'num_papers': 50,\n",
       "  'data_loaders': [{'url': 'https://github.com/Mcompetitions/M4-methods',\n",
       "    'repo': 'https://github.com/Mcompetitions/M4-methods',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/musdb18-hq',\n",
       "  'name': 'MUSDB18-HQ',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://sigsep.github.io/datasets/musdb.html',\n",
       "  'description': '**MUSDB18-HQ** is a high-quality version of the MUSDB18 music tracks dataset. The high-quality dataset consists of the same 150 songs, but instead of MP4 files (compressed with Advanced Audio Coding encoder at 256kbps, with bandwidth limited to 16kHz), the songs are provided as raw WAV files.\\nImage Source: [https://sigsep.github.io/datasets/musdb.html](https://sigsep.github.io/datasets/musdb.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MUSDB18-HQ'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/slakh2100',\n",
       "  'name': 'Slakh2100',\n",
       "  'full_name': 'Synthesized Lakh Dataset',\n",
       "  'homepage': 'http://www.slakh.com/',\n",
       "  'description': 'The Synthesized Lakh (Slakh) Dataset is a dataset for audio source separation that is synthesized from the Lakh MIDI Dataset v0.1 using professional-grade sample-based virtual instruments. This first release of Slakh, called **Slakh2100**, contains 2100 automatically mixed tracks and accompanying MIDI files synthesized using a professional-grade sampling engine. The tracks in Slakh2100 are split into training (1500 tracks), validation (375 tracks), and test (225 tracks) subsets, totaling 145 hours of mixtures.\\n\\nSource: [http://www.slakh.com/](http://www.slakh.com/)\\nImage Source: [http://www.slakh.com/](http://www.slakh.com/)\\nAudio Source: [http://www.slakh.com/](http://www.slakh.com/)',\n",
       "  'paper': {'title': 'Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity',\n",
       "   'url': 'https://paperswithcode.com/paper/cutting-music-source-separation-some-slakh-a'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Midi'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Slakh2100'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/guitarset',\n",
       "  'name': 'GuitarSet',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://guitarset.weebly.com/',\n",
       "  'description': '**GuitarSet** is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:\\n\\n* 6 players,\\n* 2 versions: comping and soloing,\\n* 5 styles: Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk,\\n* 3 progressions: 12 Bar Blues, Autumn Leaves, and Pachelbel Canon,\\n* 2 tempi: slow and fast.\\n\\nEach excerpt is annotated with 6 pitch contour and midi note annotations (one per string), 2 chord annotations (instructed and performed), beat and tempo annotations.\\n\\nSource: [https://guitarset.weebly.com/](https://guitarset.weebly.com/)\\nAudio Source: [https://zenodo.org/record/3371780](https://zenodo.org/record/3371780)',\n",
       "  'paper': {'title': 'GuitarSet: A Dataset for Guitar Transcription',\n",
       "   'url': 'https://guitarset.weebly.com/uploads/1/2/1/6/121620128/xi_ismir_2018.pdf'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Contrastive Learning',\n",
       "    'url': 'https://paperswithcode.com/task/contrastive-learning'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GuitarSet'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mixing-secrets',\n",
       "  'name': 'Mixing Secrets',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://musicinformatics.gatech.edu/conferences/mixing-secrets-a-multi-track-dataset-for-instrument-recognition/',\n",
       "  'description': '**Mixing Secrets** is an instrument recognition dataset containing 258 multi-track recordings sourced from the [Mixing Secrets for The Small Studio]( https://www.cambridge-mt.com/ms/mtk/) website. The dataset was labelled to be consistent with MedleyDB format.\\n\\nSource: [Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music](None)\\nImage Source: [Mixing secrets: a multi-track dataset for instrument recognition in polyphonic music](None)\\nAudio Source: [https://multitracksearch.cambridge-mt.com/ms-mtk-search.htm](https://multitracksearch.cambridge-mt.com/ms-mtk-search.htm)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Mixing Secrets'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/openmic-2018',\n",
       "  'name': 'OpenMIC-2018',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://zenodo.org/record/1432913',\n",
       "  'description': '**OpenMIC-2018** is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the [Free Music Archive](http://freemusicarchive.org/). Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform.\\n\\nSource: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\\nImage Source: [OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition](http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf)\\nAudio Source: [https://zenodo.org/record/1432913](https://zenodo.org/record/1432913)',\n",
       "  'paper': {'title': 'OpenMIC-2018: An Open Data-set for Multiple Instrument Recognition',\n",
       "   'url': 'http://ismir2018.ircam.fr/doc/pdfs/248_Paper.pdf'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Audio Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/audio-source-separation'},\n",
       "   {'task': 'Instrument Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/instrument-recognition'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OpenMIC-2018'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cal500exp',\n",
       "  'name': 'CAL500exp',\n",
       "  'full_name': 'CAL500 Expansion',\n",
       "  'homepage': 'http://slam.iis.sinica.edu.tw/demo/CAL500exp/',\n",
       "  'description': 'The **CAL500 Expansion** (**CAL500exp**) dataset is an enriched version of the CAL500 music information retrieval dataset. CAL500exp is designed to facilitate music auto-tagging in a smaller temporal scale. The dataset consists of the same songs split into 3,223 acoustically homogenous segments of 3 to 16 seconds. The tag labels are annotated in the segment level instead of track level. The annotations were obtained from annotators with strong music background.\\n\\nSource: [Towards time-varying music auto-tagging based on CAL500 expansion](https://doi.org/10.1109/ICME.2014.6890290)\\nImage Source: [Towards time-varying music auto-tagging based on CAL500 expansion](https://doi.org/10.1109/ICME.2014.6890290)\\nAudio Source: [http://calab1.ucsd.edu/~datasets/cal500/cal500data/](http://calab1.ucsd.edu/~datasets/cal500/cal500data/)',\n",
       "  'paper': {'title': 'Towards time-varying music auto-tagging based on CAL500 expansion',\n",
       "   'url': 'https://doi.org/10.1109/ICME.2014.6890290'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['CAL500exp'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cal10k',\n",
       "  'name': 'CAL10K',\n",
       "  'full_name': 'Computer Audition Lab 10000',\n",
       "  'homepage': 'http://calab1.ucsd.edu/~datasets/',\n",
       "  'description': 'The **CAL10K** dataset (introduced as Swat10k) contains 10,870 songs that are weakly-labelled using a tag vocabulary of 475 acoustic tags and 153 genre tags. The tags have all been harvested from [Pandora’s](https://www.pandora.com/) website and result from song annotations performed by expert musicologists involved with the Music Genome Project.\\n\\nSource: [Exploring automatic music annotation with “acoustically-objectiv” tags](http://modelai.gettysburg.edu/2012/music/docs/Tingle_Autotag_MIR10.pdf)',\n",
       "  'paper': {'title': 'Exploring automatic music annotation with “acoustically-objectiv” tags',\n",
       "   'url': 'http://modelai.gettysburg.edu/2012/music/docs/Tingle_Autotag_MIR10.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Quantization',\n",
       "    'url': 'https://paperswithcode.com/task/quantization'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CAL10K'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/musescore',\n",
       "  'name': 'MuseScore',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://biboamy.github.io/streaming-demo/main_site/',\n",
       "  'description': 'The **MuseScore** dataset is a collection of 344,166 audio and MIDI pairs downloaded from [MuseScore](https://musescore.org/) website. The audio is usually synthesized by the MuseScore synthesizer. The audio clips have diverse musical genres and are about two mins long on average.\\r\\n\\r\\nDue to copyright issues the dataset is not publicly available, but can be collected and processed with the provided source code.\\r\\n\\r\\nSource: [Multitask learning for frame-level instrument recognition](https://arxiv.org/pdf/1811.01143.pdf)\\r\\nImage Source: [https://biboamy.github.io/instrument-demo/demo.html](https://biboamy.github.io/instrument-demo/demo.html)\\r\\nAudio Source: [Somewhere over the rainbow](https://biboamy.github.io/instrument-demo/demo.html)',\n",
       "  'paper': {'title': 'Multitask learning for frame-level instrument recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/multitask-learning-for-frame-level-instrument'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [{'task': 'Music Source Separation',\n",
       "    'url': 'https://paperswithcode.com/task/music-source-separation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MuseScore'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mtg-jamendo',\n",
       "  'name': 'MTG-Jamendo',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://mtg.github.io/mtg-jamendo-dataset/',\n",
       "  'description': 'The **MTG-Jamendo** dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format.\\n\\nA subset of the dataset is used in the Emotion and Theme Recognition in Music Task within MediaEval 2019.\\n\\nSource: [https://mtg.github.io/mtg-jamendo-dataset/](https://mtg.github.io/mtg-jamendo-dataset/)\\nAudio Source: [https://essentia.upf.edu/datasets/mtg-jamendo/raw_30s/audio/](https://essentia.upf.edu/datasets/mtg-jamendo/raw_30s/audio/)',\n",
       "  'paper': {'title': 'The MTG-Jamendo Dataset for Automatic Music Tagging',\n",
       "   'url': 'http://hdl.handle.net/10230/42015'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Music'],\n",
       "  'tasks': [{'task': 'Scene Classification',\n",
       "    'url': 'https://paperswithcode.com/task/scene-classification'},\n",
       "   {'task': 'Acoustic Scene Classification',\n",
       "    'url': 'https://paperswithcode.com/task/acoustic-scene-classification'},\n",
       "   {'task': 'Music Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/music-information-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MTG-Jamendo'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/libricount',\n",
       "  'name': 'LibriCount',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://faroit.com/#libricount',\n",
       "  'description': '**LibriCount** is a synthetic dataset for speaker count estimation. The dataset contains a simulated cocktail party environment of 0 to 10 speakers, mixed with 0dB SNR from random utterances of different speakers from the LibriSpeech `CleanTest` dataset. All recordings are of 5s durations, and all speakers are active for the most part of the recording.\\n\\nSource: [https://faroit.com/#libricount](https://faroit.com/#libricount)\\nImage Source: [https://faroit.com/#libricount](https://faroit.com/#libricount)\\nAudio Source: [https://zenodo.org/record/1216072](https://zenodo.org/record/1216072)',\n",
       "  'paper': {'title': 'LibriCount, a dataset for speaker count estimation (Version v1.0.0)',\n",
       "   'url': 'http://doi.org/10.5281/zenodo.1216072'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['LibriCount'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multiwoz',\n",
       "  'name': 'MultiWOZ',\n",
       "  'full_name': 'Multi-domain Wizard-of-Oz',\n",
       "  'homepage': 'https://github.com/budzianowski/multiwoz',\n",
       "  'description': 'The **Multi-domain Wizard-of-Oz** (**MultiWOZ**) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police.\\r\\n\\r\\nSource: [Contents](https://arxiv.org/abs/1905.07687)\\r\\n\\r\\nImage Source: [Zhang et al](https://www.researchgate.net/figure/Example-of-the-difference-between-dialogue-state-annotation-in-MultiWOZ-21-and-MultiWOZ_fig2_343022084)',\n",
       "  'paper': {'title': 'MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling',\n",
       "   'url': 'https://paperswithcode.com/paper/multiwoz-a-large-scale-multi-domain-wizard-of'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'},\n",
       "   {'task': 'Multi-domain Dialogue State Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-domain-dialogue-state-tracking'},\n",
       "   {'task': 'End-To-End Dialogue Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/end-to-end-dialogue-modelling'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MULTIWOZ 2.4',\n",
       "   'MULTIWOZ 2.3',\n",
       "   'MULTIWOZ 2.2',\n",
       "   'MULTIWOZ 2.0',\n",
       "   'MultiWOZ',\n",
       "   'MULTIWOZ 2.1'],\n",
       "  'num_papers': 193,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/multi_woz_v22',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#multiwoz-2.0',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/budzianowski/multiwoz',\n",
       "    'repo': 'https://github.com/budzianowski/multiwoz',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/reverb-challenge',\n",
       "  'name': 'ReVerb Challenge',\n",
       "  'full_name': 'REverberant Voice Enhancement and Recognition Benchmark',\n",
       "  'homepage': 'https://reverb2014.dereverberation.com/index.html',\n",
       "  'description': 'The REVERB (**REverberant Voice Enhancement and Recognition Benchmark**) challenge is a benchmark for evaluation of automatic speech recognition techniques. The challenge assumes the scenario of capturing utterances spoken by a single stationary distant-talking speaker with 1-channe, 2-channel or 8-channel microphone-arrays in reverberant meeting rooms. It features both real recordings and simulated data.\\r\\n\\r\\nThe challenge constis of speech enhancement and automatic speech recognition tasks in reverberant environments. The speech enhancement challenge task consists of enhancing noisy reverberant speech with single-/multi-channel speech enhancement techniques, and evaluating the enhanced data in terms of objective and subjective evaluation metrics. The automatic speech recognition challenge task consists of improving the recognition accuracy of the same reverberant speech. The background noise is mostly stationary and the signal-to-noise ratio is modest.\\r\\n\\r\\nSource: [https://reverb2014.dereverberation.com/index.html](https://reverb2014.dereverberation.com/index.html)',\n",
       "  'paper': {'title': 'The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech',\n",
       "   'url': 'https://ieeexplore.ieee.org/document/6701894'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Speech'],\n",
       "  'tasks': [{'task': 'Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-recognition'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'},\n",
       "   {'task': 'Distant Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/distant-speech-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Reverb', 'ReVerb Challenge'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/persona-chat-1',\n",
       "  'name': 'PERSONA-CHAT',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/personachat',\n",
       "  'description': 'The **PERSONA-CHAT** dataset contains multi-turn dialogues conditioned on personas. The dataset consists of 8939 complete dialogues for training, 1000 for validation, and 968 for testing. Each dialogue was performed between two crowd-source workers assuming artificial personas (described by 3 to 5 profile sentences, such as “I like to ski”, “I am an artist”, “I eat sardines for breakfast daily”). There are 955 possible personas for training, 100 for validation, and 100 for testing. Additionally, a version of revised persona descriptions are also provided by rephrasing, generalizing, or specializing the original ones.\\r\\n\\r\\nSource: [Dually Interactive Matching Network for Personalized Response Selection in Retrieval-Based Chatbots](https://arxiv.org/abs/1908.05859)\\r\\nImage Source: [https://arxiv.org/pdf/1801.07243.pdf](https://arxiv.org/pdf/1801.07243.pdf)',\n",
       "  'paper': {'title': 'Personalizing Dialogue Agents: I have a dog, do you have pets too?',\n",
       "   'url': 'https://paperswithcode.com/paper/personalizing-dialogue-agents-i-have-a-dog-do'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Dialogue Generation',\n",
       "    'url': 'https://paperswithcode.com/task/dialogue-generation'},\n",
       "   {'task': 'Chatbot', 'url': 'https://paperswithcode.com/task/chatbot'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Persona-Chat', 'PERSONA-CHAT'],\n",
       "  'num_papers': 196,\n",
       "  'data_loaders': [{'url': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#persona-chat',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpqa-opinion-corpus',\n",
       "  'name': 'MPQA Opinion Corpus',\n",
       "  'full_name': 'Multi-Perspective Question Answering',\n",
       "  'homepage': 'https://mpqa.cs.pitt.edu/',\n",
       "  'description': 'The **MPQA Opinion Corpus** contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).\\r\\n\\r\\nSource: [http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf](http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf)\\r\\nImage Source: [https://mpqa.cs.pitt.edu/](https://mpqa.cs.pitt.edu/)',\n",
       "  'paper': {'title': 'Annotating Expressions of Opinions and Emotions in Language',\n",
       "   'url': 'https://doi.org/10.1007/s10579-005-7880-9'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'},\n",
       "   {'task': 'Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/document-classification'},\n",
       "   {'task': 'Keyword Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/keyword-extraction'},\n",
       "   {'task': 'Fine-Grained Opinion Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-opinion-analysis'},\n",
       "   {'task': 'Opinion Mining',\n",
       "    'url': 'https://paperswithcode.com/task/opinion-mining'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MPQA', 'MPQA Opinion Corpus'],\n",
       "  'num_papers': 215,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/drop',\n",
       "  'name': 'DROP',\n",
       "  'full_name': 'Discrete Reasoning Over Paragraphs',\n",
       "  'homepage': 'https://allennlp.org/drop',\n",
       "  'description': '**Discrete Reasoning Over Paragraphs** **DROP** is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set.\\r\\n\\r\\nSource: [https://allennlp.org/drop](https://allennlp.org/drop)\\r\\nImage Source: [DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs](https://paperswithcode.com/paper/drop-a-reading-comprehension-benchmark/)',\n",
       "  'paper': {'title': 'DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs',\n",
       "   'url': 'https://paperswithcode.com/paper/drop-a-reading-comprehension-benchmark'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['DROP Test', 'DROP'],\n",
       "  'num_papers': 132,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/drop',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/drop',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/rc/dataset_readers/drop/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/new-york-times-annotated-corpus',\n",
       "  'name': 'New York Times Annotated Corpus',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2008T19',\n",
       "  'description': 'The **New York Times Annotated Corpus** contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:\\r\\n\\r\\n- Over 1.8 million articles (excluding wire services articles that appeared during the covered period).\\r\\n- Over 650,000 article summaries written by library scientists.\\r\\n- Over 1,500,000 articles manually tagged by library scientists with tags drawn from a normalized indexing vocabulary of people, organizations, locations and topic descriptors.\\r\\n- Over 275,000 algorithmically-tagged articles that have been hand verified by the online production staff at nytimes.com.\\r\\nAs part of the New York Times\\' indexing procedures, most articles are manually summarized and tagged by a staff of library scientists. This collection contains over 650,000 article-summary pairs which may prove to be useful in the development and evaluation of algorithms for automated document summarization. Also, over 1.5 million documents have at least one tag. Articles are tagged for persons, places, organizations, titles and topics using a controlled vocabulary that is applied consistently across articles. For instance if one article mentions \"Bill Clinton\" and another refers to \"President William Jefferson Clinton\", both articles will be tagged with \"CLINTON, BILL\".\\r\\n\\r\\nSource: [https://catalog.ldc.upenn.edu/LDC2008T19](https://catalog.ldc.upenn.edu/LDC2008T19)',\n",
       "  'paper': {'title': 'The New York Times Annotated Corpus',\n",
       "   'url': 'http://catalog.ldc.upenn.edu/LDC2008T19'},\n",
       "  'introduced_date': '2008-10-17',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'},\n",
       "   {'task': 'Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/document-summarization'},\n",
       "   {'task': 'Relationship Extraction (Distant Supervised)',\n",
       "    'url': 'https://paperswithcode.com/task/relationship-extraction-distant-supervised'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['New York Times Corpus', 'New York Times Annotated Corpus'],\n",
       "  'num_papers': 39,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/visdial',\n",
       "  'name': 'VisDial',\n",
       "  'full_name': 'Visual Dialog',\n",
       "  'homepage': 'https://visualdialog.org/',\n",
       "  'description': '**Visual Dialog** (**VisDial**) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a ‘questioner’ and the other person acted as an ‘answerer’. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to “imagine the scene better”. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max.\\r\\n\\r\\n**VisDial v1.0** contains 123K dialogues on MS COCO (2017 training set) for training split, 2K dialogues with validation images for validation split and 8K dialogues on test set for test-standard set. The previously released v0.5 and v0.9 versions of VisDial dataset (corresponding to older splits of MS COCO) are considered deprecated.\\r\\n\\r\\nSource: [Granular Multimodal Attention Networks for Visual Dialog](https://arxiv.org/abs/1910.05728)\\r\\nImage Source: [https://arxiv.org/pdf/1611.08669.pdf](https://arxiv.org/pdf/1611.08669.pdf)',\n",
       "  'paper': {'title': 'Visual Dialog',\n",
       "   'url': 'https://paperswithcode.com/paper/visual-dialog'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts', 'Dialog'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'},\n",
       "   {'task': 'Visual Dialog',\n",
       "    'url': 'https://paperswithcode.com/task/visual-dialogue'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Visual Dialog v1.0',\n",
       "   'Visual Dialog v0.9',\n",
       "   'VisDial v1.0 test-std',\n",
       "   'VisDial v0.9 val',\n",
       "   'Visual Dialog v1.0 test-std',\n",
       "   'Visual Dialog  v0.9',\n",
       "   'VisDial'],\n",
       "  'num_papers': 94,\n",
       "  'data_loaders': [{'url': 'https://parl.ai/docs/tasks.html#visdial',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/amr-bank',\n",
       "  'name': 'AMR Bank',\n",
       "  'full_name': 'Abstract Meaning Representation',\n",
       "  'homepage': 'https://amr.isi.edu/',\n",
       "  'description': 'The **AMR Bank** is a set of English sentences paired with simple, readable semantic representations. Version 3.0 released in 2020 consists of 59,255 sentences.\\r\\n\\r\\nEach AMR is a single rooted, directed graph. AMRs include PropBank semantic roles, within-sentence coreference, named entities and types, modality, negation, questions, quantities, and so on.\\r\\n\\r\\nThe image presents an AMR of a sample sentence “The boy wants to go”.\\r\\n\\r\\nSource: [https://amr.isi.edu/](https://amr.isi.edu/)\\r\\nImage Source: [https://amr.isi.edu/language.html](https://amr.isi.edu/language.html)',\n",
       "  'paper': {'title': 'Abstract Meaning Representation for Sembanking',\n",
       "   'url': 'https://paperswithcode.com/paper/abstract-meaning-representation-for'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['AMR (english, MRP 2020)', 'AMR Bank'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2016',\n",
       "  'name': 'WMT 2016',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.statmt.org/wmt16/index.html',\n",
       "  'description': '**WMT 2016** is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation.\\r\\n\\r\\nThe conference featured ten shared tasks:\\r\\n\\r\\n- a news translation task,\\r\\n- an IT domain translation task,\\r\\n- a biomedical translation task,\\r\\n- an automatic post-editing task,\\r\\n- a metrics task (assess MT quality given reference translation).\\r\\n- a quality estimation task (assess MT quality without access to any reference),\\r\\n- a tuning task (optimize a given MT system),\\r\\n- a pronoun translation task,\\r\\n- a bilingual document alignment task,\\r\\n- a multimodal translation task.\\r\\n\\r\\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)',\n",
       "  'paper': {'title': 'Findings of the 2016 Conference on Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Translation deu-eng',\n",
       "    'url': 'https://paperswithcode.com/task/translation-deu-eng'},\n",
       "   {'task': 'Sequence-to-sequence Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/sequence-to-sequence-language-modeling'},\n",
       "   {'task': 'Translation eng-deu',\n",
       "    'url': 'https://paperswithcode.com/task/translation-eng-deu'},\n",
       "   {'task': 'Unsupervised Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-machine-translation'}],\n",
       "  'languages': ['English',\n",
       "   'French',\n",
       "   'German',\n",
       "   'Russian',\n",
       "   'Czech',\n",
       "   'Finnish',\n",
       "   'Romanian'],\n",
       "  'variants': ['newstest2016-ende eng-deu',\n",
       "   'newstest2016-deen deu-eng',\n",
       "   'newstest2016-ende',\n",
       "   'newstest2016-deen',\n",
       "   'wmt16',\n",
       "   'WMT2016 En-Ro',\n",
       "   'WMT 2016',\n",
       "   'WMT2016 Russian-English',\n",
       "   'WMT2016 Finnish-English',\n",
       "   'WMT2016 English-Russian',\n",
       "   'WMT2016 English-French',\n",
       "   'WMT2016 English-Czech',\n",
       "   'WMT2016 Czech-English',\n",
       "   'WMT2016 Romanian-English',\n",
       "   'WMT2016 German-English',\n",
       "   'WMT2016 English-Romanian',\n",
       "   'WMT2016 English-German',\n",
       "   'WMT2016 English--Romanian'],\n",
       "  'num_papers': 117,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wmt16',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wmt16_translate',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2016-news',\n",
       "  'name': 'WMT 2016 News',\n",
       "  'full_name': 'WMT 2016 News Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt16/index.html',\n",
       "  'description': 'News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators.\\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\\nSome training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10⁹ corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl.\\n\\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\\nImage Source: [https://www.aclweb.org/anthology/W16-2301.pdf](https://www.aclweb.org/anthology/W16-2301.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2016 Conference on Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Parallel'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Unsupervised Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-machine-translation'},\n",
       "   {'task': 'Word Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/word-alignment'},\n",
       "   {'task': 'Automatic Post-Editing',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-post-editing'}],\n",
       "  'languages': ['English',\n",
       "   'German',\n",
       "   'Russian',\n",
       "   'Czech',\n",
       "   'Finnish',\n",
       "   'Romanian',\n",
       "   'Turkish'],\n",
       "  'variants': ['WMT2016 Russian-English',\n",
       "   'WMT2016 Romanian-English',\n",
       "   'WMT2016 Finnish-English',\n",
       "   'WMT2016 English-Russian',\n",
       "   'WMT2016 English--Romanian',\n",
       "   'WMT2016 English-Romanian',\n",
       "   'WMT 2016 News'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2016-it',\n",
       "  'name': 'WMT 2016 IT',\n",
       "  'full_name': 'WMT 2016 IT Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt16/index.html',\n",
       "  'description': 'The IT Translation Task is a shared task introduced in the First Conference on Machine Translation. Compared to WMT 2016 News, this task brought several novelties to WMT:\\r\\n\\r\\n*  4 out of the 7 langauges of the IT task are new in WMT,\\r\\n* adaptation to the IT domain with its specifics such as frequent named entities (mostly menu items, names of products and companies) and technical jargon,\\r\\n* adaptation to translation of answers in helpdesk service setting (many of the sentences are instructions with imperative verbs, which is very rare in the News translation task).\\r\\n\\r\\nThe test set consisted of 1000 answers from the Batch 3 of the QTLeap Corpus. The in-domain training data contained 2000 answers from the Batches 1 and 2 and also localization files from several open-source projects (LibreOffice, KDE, VLC) and bilingual dictionaries of IT-related terms extracted from Wikipedia. The out-of-domain training data contained all the corpora from the WMT 2016 News, plus PaCo2-EuEn Basque-English corpus and SETimes with Bulgarian-English parallel sentences. “Constrained” systems were restricted to use only these training data provided by the organizers.\\r\\n\\r\\nThe task was evaluated on the following language pairs:\\r\\n\\r\\n* English → Bulgarian\\r\\n* English → Czech\\r\\n* English → German\\r\\n* English → Spanish\\r\\n* English → Basque\\r\\n* English → Dutch\\r\\n* English → Portuguese\\r\\n\\r\\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)\\r\\nImage Source: [Bojar et al](https://www.aclweb.org/anthology/W16-2301.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2016 Conference on Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Parallel'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Multimodal Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-machine-translation'},\n",
       "   {'task': 'Automatic Post-Editing',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-post-editing'}],\n",
       "  'languages': ['English',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Basque',\n",
       "   'Bulgarian',\n",
       "   'Czech',\n",
       "   'Dutch',\n",
       "   'Portuguse'],\n",
       "  'variants': ['WMT 2016 IT'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2016-biomedical',\n",
       "  'name': 'WMT 2016 Biomedical',\n",
       "  'full_name': 'WMT 2016 Biomedical Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt16/index.html',\n",
       "  'description': 'The Biomedical Translation Shared Task was first introduced at the First Conference of Machine Translation. The task aims to evaluate systems for the translation of biomedical titles and abstracts from scientific publications. The data includes three language pairs (English ↔ Portuguese, English  ↔ Spanish, English  ↔ French) and two sub-domains of biological sciences and health sciences.\\n\\nThe training data consists mainly of the Scielo corpus, a parallel collection of scientific publications composed of either titles, abstracts or title and abstracts which were retrieved from the Scielo database. For the Scielo corpus, a parallel documents are provided for all language pairs in the two sub-domains, except for the English  ↔ French, where only health was considered, as there were inadequate parallel documents available for biology in that pair. The training data was aligned using the GMA alignment tool. Additionally, a corpus of parallel titles from MEDLINEⓇ for all three language pairs were provided as well as monolingual documents for the four languages, retrieved from the Scielo database. These consist of documents in the Scielo database which have no corresponding document in another language.\\n\\nThe test set consisted of 500 documents (title and abstract) for each of the two directions of each language pair. None of the test documents was included in the training data and there is no overlap of documents between the test sets for any language pair, translation direction and sub-domain.\\n\\nSource: [http://www.statmt.org/wmt16/index.html](http://www.statmt.org/wmt16/index.html)\\nImage Source: [https://www.aclweb.org/anthology/W16-2301.pdf](https://www.aclweb.org/anthology/W16-2301.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2016 Conference on Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Multimodal Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-machine-translation'},\n",
       "   {'task': 'Automatic Post-Editing',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-post-editing'}],\n",
       "  'languages': ['English', 'French', 'Spanish', 'Portuguese'],\n",
       "  'variants': ['WMT 2016 Biomedical'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/xsum',\n",
       "  'name': 'XSum',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset',\n",
       "  'description': 'The Extreme Summarization (**XSum**) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question “What is the article about?”. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively.\\n\\nSource: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)\\nImage Source: [https://arxiv.org/pdf/1808.08745.pdf](https://arxiv.org/pdf/1808.08745.pdf)',\n",
       "  'paper': {'title': \"Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization\",\n",
       "   'url': 'https://paperswithcode.com/paper/dont-give-me-the-details-just-the-summary'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Sequence-to-sequence Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/sequence-to-sequence-language-modeling'},\n",
       "   {'task': 'Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/document-summarization'},\n",
       "   {'task': 'Extreme Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/extreme-summarization'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['BBC XSum', 'XSum', 'X-Sum'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/xsum',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/xsum',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/EdinburghNLP/XSum',\n",
       "    'repo': 'https://github.com/EdinburghNLP/XSum',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2014',\n",
       "  'name': 'WMT 2014',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.statmt.org/wmt14/index.html',\n",
       "  'description': '**WMT 2014** is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:\\r\\n\\r\\n* a news translation task,\\r\\n* a quality estimation task,\\r\\n* a metrics task,\\r\\n* a medical text translation task.\\r\\n\\r\\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2014 Workshop on Statistical Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2014-workshop-on-statistical'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Translation deu-eng',\n",
       "    'url': 'https://paperswithcode.com/task/translation-deu-eng'},\n",
       "   {'task': 'Sequence-to-sequence Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/sequence-to-sequence-language-modeling'},\n",
       "   {'task': 'Translation eng-deu',\n",
       "    'url': 'https://paperswithcode.com/task/translation-eng-deu'},\n",
       "   {'task': 'Unsupervised Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-machine-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['newstest2014-deen eng-deu',\n",
       "   'newstest2014-deen deu-eng',\n",
       "   'newstest2014-deen',\n",
       "   'wmt14',\n",
       "   'WMT 2014',\n",
       "   'WMT2014 German-English',\n",
       "   'WMT2014 English-Czech',\n",
       "   'WMT 2014 EN-FR',\n",
       "   'WMT 2014 EN-DE',\n",
       "   'WMT2014 English-German',\n",
       "   'WMT2014 French-English',\n",
       "   'WMT2014 English-French'],\n",
       "  'num_papers': 186,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wmt14',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#wmt',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wmt14_translate',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2014-medical',\n",
       "  'name': 'WMT 2014 Medical',\n",
       "  'full_name': 'WMT 2014 Medical Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt14/index.html',\n",
       "  'description': 'The Medical Translation Task of WMT 2014 addresses the problem of domain-specific and genre-specific machine translation. The task is split into two subtasks: summary translation, focused on translation of sentences from summaries of medical articles, and query translation, focused on translation of queries entered by users into medical information search engines. Both subtasks included translation between English and Czech, German, and French, in both directions.\\n\\nSource: [https://www.aclweb.org/anthology/W14-3302.pdf](https://www.aclweb.org/anthology/W14-3302.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2014 Workshop on Statistical Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2014-workshop-on-statistical'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Medical'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'}],\n",
       "  'languages': ['English', 'French', 'German', 'Czech'],\n",
       "  'variants': ['WMT 2014 Medical'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2015',\n",
       "  'name': 'WMT 2015',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.statmt.org/wmt15/index.html',\n",
       "  'description': '**WMT 2015** is a collection of datasets used in shared tasks of the Tenth Workshop on Statistical Machine Translation. The workshop featured five tasks:\\r\\n\\r\\n* a news translation task,\\r\\n* a metrics task,\\r\\n* a tuning task,\\r\\n* a quality estimation task,\\r\\n* an automatic post-editing task.\\r\\n\\r\\nSource: [https://www.aclweb.org/anthology/W15-3001.pdf](https://www.aclweb.org/anthology/W15-3001.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2015 Workshop on Statistical Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2015-workshop-on-statistical'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Translation deu-eng',\n",
       "    'url': 'https://paperswithcode.com/task/translation-deu-eng'},\n",
       "   {'task': 'Translation eng-deu',\n",
       "    'url': 'https://paperswithcode.com/task/translation-eng-deu'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WMT2015 English-German',\n",
       "   'WMT2015 English-Russian',\n",
       "   'WMT 2015',\n",
       "   'newstest2015-deen',\n",
       "   'newstest2015-ende',\n",
       "   'newstest2015-deen deu-eng',\n",
       "   'newstest2015-ende eng-deu'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wmt15',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wmt15_translate',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2015-news',\n",
       "  'name': 'WMT 2015 News',\n",
       "  'full_name': 'WMT 2015 News Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt15/index.html',\n",
       "  'description': 'News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, French, Russian) and additional 1500 sentences from each of the 5 languages translated to English. The sentences are taken from newspaper articles for each language pair, except for French, where the test set was drawn from user-generated comments on the news articles (from Guardian and Le Monde). The translation was done by professional translators.\\n\\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\\nSome training corpora were identical from WMT 2014 (Europarl, United Nations, French-English 10⁹ corpus, CzEng, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (News Commentary, monolingual news data). Additionally, the Finnish Europarl and Finnish-English Wikipedia Headline corpus were added.\\n\\nSource: [https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/](https://paperswithcode.com/paper/findings-of-the-2016-conference-on-machine/)\\nImage Source: [httpshttps://www.aclweb.org/anthology/W15-3001.pdf](httpshttps://www.aclweb.org/anthology/W15-3001.pdf)',\n",
       "  'paper': {'title': 'Findings of the 2015 Workshop on Statistical Machine Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2015-workshop-on-statistical'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Parallel'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Automatic Post-Editing',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-post-editing'}],\n",
       "  'languages': ['English', 'French', 'German', 'Russian', 'Czech', 'Finnish'],\n",
       "  'variants': ['WMT 2015 News'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/shapes-1',\n",
       "  'name': 'SHAPES',\n",
       "  'full_name': 'Swarm Heuristics based Adaptive and Penalized Estimation of Splines',\n",
       "  'homepage': 'https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset',\n",
       "  'description': '**SHAPES** is a dataset of synthetic images designed to benchmark systems for understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, e.g. Is there a red shape above a circle? or Is a red shape blue?. Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases.\\r\\n\\r\\nEach image is a 30×30 RGB image depicting a 3×3 grid of objects. Each object is characterized by shape (circle, square, triangle), colour (red, green, blue) and size (small, big).\\r\\n\\r\\nSource: [Visual Question Answering: A Survey of Methods and Datasets](https://arxiv.org/abs/1607.05910)\\r\\nImage Source: [https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset](https://github.com/ronghanghu/n2nmn#train-and-evaluate-on-the-shapes-dataset)',\n",
       "  'paper': {'title': 'Neural Module Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/neural-module-networks'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Time Series Classification',\n",
       "    'url': 'https://paperswithcode.com/task/time-series-classification'},\n",
       "   {'task': 'Visual Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/visual-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SHAPES'],\n",
       "  'num_papers': 80,\n",
       "  'data_loaders': [{'url': 'https://github.com/ronghanghu/n2nmn',\n",
       "    'repo': 'https://github.com/ronghanghu/n2nmn',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ags-corpus',\n",
       "  'name': 'AG’s Corpus',\n",
       "  'full_name': \"AG's corpus of news articlesNews\",\n",
       "  'homepage': 'http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html',\n",
       "  'description': \"Antonio Gulli’s corpus of news articles is a collection of more than 1 million news articles. The articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004.\\r\\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non - commercial activity.\\r\\n\\r\\nA subset of this corpus, AG News, consisting of the 4 largest classes is a popular topic classification dataset.\\r\\n\\r\\nSource: [AG's corpus of news articles](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\",\n",
       "  'paper': {'title': \"AG's corpus of news articles\",\n",
       "   'url': 'http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['AG’s Corpus'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/quasar-s',\n",
       "  'name': 'QUASAR-S',\n",
       "  'full_name': 'QUestion Answering by Search And Reading – Stack Overflow',\n",
       "  'homepage': 'http://curtis.ml.cmu.edu/datasets/quasar/',\n",
       "  'description': '**QUASAR-S** is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 37,362 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The answer to each question is restricted to be another software entity, from an output vocabulary of 4874 entities.\\n\\nSource: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)\\nImage Source: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)',\n",
       "  'paper': {'title': 'Quasar: Datasets for Question Answering by Search and Reading',\n",
       "   'url': 'https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['QUASAR-S'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/quasar-t',\n",
       "  'name': 'QUASAR-T',\n",
       "  'full_name': 'QUestion Answering by Search And Reading – Trivia',\n",
       "  'homepage': 'https://github.com/bdhingra/quasar',\n",
       "  'description': '**QUASAR-T** is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 43,013 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. The answers to these questions are free-form spans of text, though most are noun phrases.\\r\\n\\r\\nSource: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)\\r\\nImage Source: [Quasar: Datasets for Question Answering by Search and Reading](https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by/)',\n",
       "  'paper': {'title': 'Quasar: Datasets for Question Answering by Search and Reading',\n",
       "   'url': 'https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Quasar', 'Quasart-T', 'QUASAR-T'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': [{'url': 'https://github.com/bdhingra/quasar',\n",
       "    'repo': 'https://github.com/bdhingra/quasar',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mldoc',\n",
       "  'name': 'MLDoc',\n",
       "  'full_name': 'Multilingual Document Classification Corpus',\n",
       "  'homepage': 'https://github.com/facebookresearch/MLDoc',\n",
       "  'description': '**Multilingual Document Classification Corpus** (**MLDoc**) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:\\r\\n\\r\\n* uniform class coverage: same number of examples for each class and language,\\r\\n* official train / development / test split: for each language a training data of different sizes (1K, 2K, 5K and 10K stories), a development (1K) and a test corpus (4K) are provided (with exception of Spanish and Russian with 9458 and 5216 training documents respectively.\\r\\n\\r\\nSource: [A Corpus for Multilingual Document Classification in Eight Languages](https://paperswithcode.com/paper/a-corpus-for-multilingual-document/)',\n",
       "  'paper': {'title': 'A Corpus for Multilingual Document Classification in Eight Languages',\n",
       "   'url': 'https://paperswithcode.com/paper/a-corpus-for-multilingual-document'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Cross-Lingual Document Classification',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-document-classification'},\n",
       "   {'task': 'Cross-Lingual Sentiment Classification',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-sentiment-classification'}],\n",
       "  'languages': ['English',\n",
       "   'French',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Italian',\n",
       "   'Chinese',\n",
       "   'Multilingual',\n",
       "   'Japanese',\n",
       "   'Russian'],\n",
       "  'variants': ['MLDoc',\n",
       "   'MLDoc Zero-Shot German-to-French',\n",
       "   'MLDoc Zero-Shot English-to-Spanish',\n",
       "   'MLDoc Zero-Shot English-to-Russian',\n",
       "   'MLDoc Zero-Shot English-to-Japanese',\n",
       "   'MLDoc Zero-Shot English-to-Italian',\n",
       "   'MLDoc Zero-Shot English-to-German',\n",
       "   'MLDoc Zero-Shot English-to-French',\n",
       "   'MLDoc Zero-Shot English-to-Chinese'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://github.com/facebookresearch/MLDoc',\n",
       "    'repo': 'https://github.com/facebookresearch/MLDoc',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2018',\n",
       "  'name': 'WMT 2018',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.statmt.org/wmt18/',\n",
       "  'description': '**WMT 2018** is a collection of datasets used in shared tasks of the Third Conference on Machine Translation. The conference builds on a series of twelve previous annual workshops and conferences on Statistical Machine Translation.\\r\\n\\r\\nThe conference featured ten shared tasks:\\r\\n\\r\\n- a news translation task,\\r\\n- a biomedical translation task,\\r\\n- a multimodal machine translation task,\\r\\n- a metrics task,\\r\\n- a quality estimation task,\\r\\n- an automatic post-editing task,\\r\\n- a parallel corpus filtering task.\\r\\n\\r\\nSource: [http://www.statmt.org/wmt18/](http://www.statmt.org/wmt18/)',\n",
       "  'paper': {'title': 'Findings of the 2018 Conference on Machine Translation (WMT18)',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2018-conference-on-machine'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Translation deu-eng',\n",
       "    'url': 'https://paperswithcode.com/task/translation-deu-eng'},\n",
       "   {'task': 'Translation eng-deu',\n",
       "    'url': 'https://paperswithcode.com/task/translation-eng-deu'}],\n",
       "  'languages': ['English', 'Estonian', 'Finnish'],\n",
       "  'variants': ['WMT 2018 English-Estonian',\n",
       "   'WMT 2018 English-Finnish',\n",
       "   'WMT 2018 Estonian-English',\n",
       "   'WMT 2018 Finnish-English',\n",
       "   'WMT 2018',\n",
       "   'newstest2018-deen',\n",
       "   'newstest2018-ende',\n",
       "   'newstest2018-deen deu-eng',\n",
       "   'newstest2018-ende eng-deu'],\n",
       "  'num_papers': 32,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wmt18',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wmt-2018-news',\n",
       "  'name': 'WMT 2018 News',\n",
       "  'full_name': 'WMT 2018 News Translation Task',\n",
       "  'homepage': 'http://www.statmt.org/wmt18/translation-task.html',\n",
       "  'description': 'News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Chinese, Czech, Estonian, German, Finnish, Russian, Turkish) and additional 1500 sentences from each of the 7 languages translated to English. The sentences were selected from dozens of news websites and translated by professional translators.\\r\\n\\r\\nThe training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning.\\r\\nSome training corpora were identical from WMT 2017 (Europarl, Common Crawl, SETIMES2, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (United Nations, CzEng v1.7, News Commentary v13, monolingual news data).\\r\\nAdditionally, the EU Press Release parallel corpus for German, Finnish and Estonian was added.\\r\\n\\r\\nSource: [https://www.statmt.org/wmt18/translation-task.html](https://www.statmt.org/wmt18/translation-task.html)',\n",
       "  'paper': {'title': 'Findings of the 2018 Conference on Machine Translation (WMT18)',\n",
       "   'url': 'https://paperswithcode.com/paper/findings-of-the-2018-conference-on-machine'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Automatic Post-Editing',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-post-editing'}],\n",
       "  'languages': ['English',\n",
       "   'German',\n",
       "   'Chinese',\n",
       "   'Russian',\n",
       "   'Czech',\n",
       "   'Estonian',\n",
       "   'Finnish',\n",
       "   'Turkish'],\n",
       "  'variants': ['WMT 2018 News'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/wmt18_translate',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/arxivpapers',\n",
       "  'name': 'ArxivPapers',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/paperswithcode/axcell',\n",
       "  'description': 'The **ArxivPapers** dataset is an unlabelled collection of over 104K papers related to machine learning and published on arXiv.org between 2007–2020. The dataset includes around 94K papers (for which LaTeX source code is available) in a structured form in which paper is split into a title, abstract, sections, paragraphs and references. Additionally, the dataset contains over 277K tables extracted from the LaTeX papers.\\r\\n\\r\\nDue to the papers license the dataset is published as a metadata and open-source pipeline that can be used to obtain and convert the papers. \\r\\n\\r\\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\\r\\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)',\n",
       "  'paper': {'title': 'AxCell: Automatic Extraction of Results from Machine Learning Papers',\n",
       "   'url': 'https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tables'],\n",
       "  'tasks': [{'task': 'Scientific Results Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/scientific-results-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ArxivPapers'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/paperswithcode/axcell',\n",
       "    'repo': 'https://github.com/paperswithcode/axcell',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/segmentedtables',\n",
       "  'name': 'SegmentedTables',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/paperswithcode/axcell',\n",
       "  'description': 'The **SegmentedTables** dataset is a collection of almost 2,000 tables extracted from 352 machine learning papers. Each table consists of rich text content, layout and caption. Tables are annotated with types (leaderboard, ablation, irrelevant) and cells of relevant tables are annotated with semantic roles (such as “paper model”, “competing model”, “dataset”, “metric”).\\n\\nDue to the license of source papers the dataset is published as a metadata, all annotations and open-source pipeline that can be used to extract the tables.\\n\\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)',\n",
       "  'paper': {'title': 'AxCell: Automatic Extraction of Results from Machine Learning Papers',\n",
       "   'url': 'https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tables'],\n",
       "  'tasks': [{'task': 'Scientific Results Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/scientific-results-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['SegmentedTables'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/paperswithcode/axcell',\n",
       "    'repo': 'https://github.com/paperswithcode/axcell',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/linkedresults',\n",
       "  'name': 'LinkedResults',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/paperswithcode/axcell',\n",
       "  'description': 'The **LinkedResults** dataset contains around 1,600 results capturing performance of machine learning models from tables of 239 papers. All tables come from a subset of SegmentedTables dataset. Each result is a tuple of form (task, dataset, metric name, metric value) and is linked to a particular table, row and cell it originates from.\\r\\n\\r\\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)',\n",
       "  'paper': {'title': 'AxCell: Automatic Extraction of Results from Machine Learning Papers',\n",
       "   'url': 'https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from'},\n",
       "  'introduced_date': '2020-04-29',\n",
       "  'warning': None,\n",
       "  'modalities': ['Tabular'],\n",
       "  'tasks': [{'task': 'Scientific Results Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/scientific-results-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['LinkedResults'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/paperswithcode/axcell',\n",
       "    'repo': 'https://github.com/paperswithcode/axcell',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pwc-leaderboards',\n",
       "  'name': 'PWC Leaderboards',\n",
       "  'full_name': 'Papers with Code Leaderboards',\n",
       "  'homepage': 'https://github.com/paperswithcode/axcell',\n",
       "  'description': 'The **Papers with Code Leaderboards** dataset is a collection of over 5,000 results capturing performance of machine learning models. Each result is a tuple of form (task, dataset, metric name, metric value). The data was collected using the Papers with Code review interface.\\n\\nSource: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)\\nImage Source: [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from)',\n",
       "  'paper': {'title': 'AxCell: Automatic Extraction of Results from Machine Learning Papers',\n",
       "   'url': 'https://paperswithcode.com/paper/axcell-automatic-extraction-of-results-from'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Tabular'],\n",
       "  'tasks': [{'task': 'Scientific Results Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/scientific-results-extraction'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['PWC Leaderboards (restricted)', 'PWC Leaderboards'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/paperswithcode/axcell',\n",
       "    'repo': 'https://github.com/paperswithcode/axcell',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sku110k',\n",
       "  'name': 'SKU110K',\n",
       "  'full_name': 'SKU110K',\n",
       "  'homepage': 'https://github.com/eg4000/SKU110K_CVPR19',\n",
       "  'description': 'The Sku110k dataset provides 11,762 images with more than 1.7 million annotated bounding boxes captured in densely packed scenarios, including 8,233 images for training, 588 images for validation, and 2,941 images for testing. There are around 1,733,678 instances in total. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [−15∘, 15∘].\\n\\nSource: [Rethinking Object Detection in Retail Stores](https://arxiv.org/abs/2003.08230)\\nImage Source: [https://github.com/eg4000/SKU110K_CVPR19](https://github.com/eg4000/SKU110K_CVPR19)',\n",
       "  'paper': {'title': 'Precise Detection in Densely Packed Scenes',\n",
       "   'url': 'https://paperswithcode.com/paper/precise-detection-in-densely-packed-scenes'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'One-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-object-detection'},\n",
       "   {'task': 'Dense Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/dense-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SKU-110K', 'SKU110K'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': [{'url': 'https://github.com/eg4000/SKU110K_CVPR19',\n",
       "    'repo': 'https://github.com/eg4000/SKU110K_CVPR19',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ubiris-v2',\n",
       "  'name': 'UBIRIS.v2',\n",
       "  'full_name': 'UBIRIS.v2',\n",
       "  'homepage': 'http://iris.di.ubi.pt/ubiris2.html',\n",
       "  'description': 'The **UBIRIS.v2** iris dataset contains 11,102 iris images from 261 subjects with 10 images each subject. The images were captured under unconstrained conditions (at-a-distance, on-the-move and on the visible wavelength), with realistic noise factors.\\n\\nSource: [Constrained Design of Deep Iris Networks](https://arxiv.org/abs/1905.09481)\\nImage Source: [https://arxiv.org/pdf/1905.09481.pdf](https://arxiv.org/pdf/1905.09481.pdf)',\n",
       "  'paper': {'title': 'The UBIRIS.v2: A database of visible wavelength images captured on-the-move and at-a-distance',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2009.66'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Iris Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/iris-segmentation'},\n",
       "   {'task': 'Iris Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/iris-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UBIRIS.v2'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/viva',\n",
       "  'name': 'VIVA',\n",
       "  'full_name': 'Vision for Intelligent Vehicles and Applications',\n",
       "  'homepage': 'http://cvrr.ucsd.edu/vivachallenge/',\n",
       "  'description': 'The **VIVA** challenge’s dataset is a multimodal dynamic hand gesture dataset specifically designed with difficult settings of cluttered background, volatile illumination, and frequent occlusion for studying natural human activities in real-world driving settings. This dataset was captured using a Microsoft Kinect device, and contains 885 intensity and depth video sequences of 19 different dynamic hand gestures performed by 8 subjects inside a vehicle.\\n\\nSource: [Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture Recognition](https://arxiv.org/abs/2001.05833)\\nImage Source: [http://www.site.uottawa.ca/research/viva/projects/hand_detection/index.html](http://www.site.uottawa.ca/research/viva/projects/hand_detection/index.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Hand Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition'},\n",
       "   {'task': 'Hand-Gesture Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/hand-gesture-recognition-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VIVA Hand Gestures Dataset', 'VIVA'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/itop',\n",
       "  'name': 'ITOP',\n",
       "  'full_name': 'Invariant-Top View Dataset',\n",
       "  'homepage': 'https://zenodo.org/record/3932973',\n",
       "  'description': 'The **ITOP** dataset consists of 40K training and 10K testing depth images for each of the front-view and top-view tracks. This dataset contains depth images with 20 actors who perform 15 sequences each and is recorded by two Asus Xtion Pro cameras. The ground-truth of this dataset is the 3D coordinates of 15 body joints.\\n\\nSource: [V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation from a Single Depth Map](https://arxiv.org/abs/1711.07399)\\nImage Source: [https://www.youtube.com/watch?v=4gPI-GOf9wg](https://www.youtube.com/watch?v=4gPI-GOf9wg)',\n",
       "  'paper': {'title': 'Towards Viewpoint Invariant 3D Human Pose Estimation',\n",
       "   'url': 'https://paperswithcode.com/paper/towards-viewpoint-invariant-3d-human-pose'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': [' ITOP front-view', 'ITOP top-view', 'ITOP front-view', 'ITOP'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dayton',\n",
       "  'name': 'Dayton',\n",
       "  'full_name': 'Dayton',\n",
       "  'homepage': 'https://github.com/kregmi/cross-view-image-synthesis/blob/master/README.md',\n",
       "  'description': 'The **Dayton** dataset is a dataset for ground-to-aerial (or aerial-to-ground) image translation, or cross-view image synthesis. It contains images of road views and aerial views of roads. There are 76,048 images in total and the train/test split is 55,000/21,048. The images in the original dataset have 354×354 resolution.\\n\\nSource: [Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation](https://arxiv.org/abs/2002.01048)\\nImage Source: [https://arxiv.org/abs/1912.06112](https://arxiv.org/abs/1912.06112)',\n",
       "  'paper': {'title': 'Localizing and Orienting Street Views Using Overhead Imagery',\n",
       "   'url': 'https://paperswithcode.com/paper/localizing-and-orienting-street-views-using'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Cross-View Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/cross-view-image-to-image-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Dayton (64×64) - aerial-to-ground',\n",
       "   'Dayton (64x64) - ground-to-aerial',\n",
       "   'Dayton (256×256) - aerial-to-ground',\n",
       "   'Dayton (256×256) - ground-to-aerial',\n",
       "   'Dayton'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://github.com/kregmi/cross-view-image-synthesis',\n",
       "    'repo': 'https://github.com/kregmi/cross-view-image-synthesis',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/aolp',\n",
       "  'name': 'AOLP',\n",
       "  'full_name': 'Application-oriented License Plate',\n",
       "  'homepage': 'http://aolpr.ntust.edu.tw/lab/index.html',\n",
       "  'description': 'The application-oriented license plate (**AOLP**) benchmark database has 2049 images of Taiwan license plates. This database is categorized into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples. AC refers to the cases that a vehicle passes a fixed passage with a lower speed or full stop. This is the easiest situation. The images are captured under different illuminations and different weather conditions. LE refers to the cases that a vehicle violates traffic laws and is captured by roadside camera. The background are really cluttered, with road sign and multiple plates in one image. RP refers to the cases that the camera is held on a patrolling vehicle, and the images are taken with arbitrary viewpoints and distances.\\n\\nSource: [Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs](https://arxiv.org/abs/1601.05610)\\nImage Source: [http://aolpr.ntust.edu.tw/lab/index.html](http://aolpr.ntust.edu.tw/lab/index.html)',\n",
       "  'paper': {'title': 'Application-Oriented License Plate Recognition',\n",
       "   'url': 'https://doi.org/10.1109/TVT.2012.2226218'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'License Plate Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/license-plate-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AOLP-RP', 'AOLP'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/set11',\n",
       "  'name': 'Set11',\n",
       "  'full_name': 'Set11',\n",
       "  'homepage': None,\n",
       "  'description': '**Set11** is a dataset of 11 grayscale images. It is a dataset used for image reconstruction and image compression.\\n\\nSource: [ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing](https://arxiv.org/abs/1706.07929)\\nImage Source: [https://arxiv.org/pdf/1706.07929.pdf](https://arxiv.org/pdf/1706.07929.pdf)',\n",
       "  'paper': {'title': 'ReconNet: Non-Iterative Reconstruction of Images From Compressively Sensed Measurements',\n",
       "   'url': 'https://paperswithcode.com/paper/reconnet-non-iterative-reconstruction-of-1'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Compressive Sensing',\n",
       "    'url': 'https://paperswithcode.com/task/compressive-sensing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Set11 cs=50%', 'Set11'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/salicon',\n",
       "  'name': 'SALICON',\n",
       "  'full_name': 'Salicency in Context',\n",
       "  'homepage': 'http://salicon.net/',\n",
       "  'description': 'The SALIency in CONtext (**SALICON**) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO.\\r\\nThe ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded.\\r\\nThe training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze.\\r\\nThe testing data contains only the image and resolution fields.\\r\\n\\r\\nSource: [DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations](https://arxiv.org/abs/1510.02927)\\r\\nImage Source: [http://salicon.net/explore/](http://salicon.net/explore/)',\n",
       "  'paper': {'title': 'SALICON: Saliency in Context',\n",
       "   'url': 'https://paperswithcode.com/paper/salicon-saliency-in-context'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Few-Shot Transfer Learning for Saliency Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-prediction-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SALICON->WebpageSaliency - 1-shot',\n",
       "   'SALICON->WebpageSaliency - 5-shot ',\n",
       "   'SALICON->WebpageSaliency - 10-shot ',\n",
       "   'SALICON->WebpageSaliency - EUB',\n",
       "   'SALICON'],\n",
       "  'num_papers': 119,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/grid',\n",
       "  'name': 'GRID Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html',\n",
       "  'description': 'The QMUL underGround Re-IDentification (**GRID**) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution.\\r\\n\\r\\nSource: [https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html](https://personal.ie.cuhk.edu.hk/~ccloy/downloads_qmul_underground_reid.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Speech Separation',\n",
       "    'url': 'https://paperswithcode.com/task/speech-separation'},\n",
       "   {'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'},\n",
       "   {'task': 'Speaker-Specific Lip to Speech Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/speaker-specific-lip-to-speech-synthesis'},\n",
       "   {'task': 'Lipreading', 'url': 'https://paperswithcode.com/task/lipreading'},\n",
       "   {'task': 'Lip Reading',\n",
       "    'url': 'https://paperswithcode.com/task/lip-reading'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GRID corpus (mixed-speech)', 'GRID Dataset'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/flickr30k-entities',\n",
       "  'name': 'Flickr30K Entities',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://bryanplummer.com/Flickr30kEntities/',\n",
       "  'description': 'The **Flickr30K Entities** dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image.\\r\\n\\r\\nSource: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)\\r\\nImage Source: [http://bryanplummer.com/Flickr30kEntities/](http://bryanplummer.com/Flickr30kEntities/)',\n",
       "  'paper': {'title': 'Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models',\n",
       "   'url': 'https://paperswithcode.com/paper/flickr30k-entities-collecting-region-to'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Phrase Grounding',\n",
       "    'url': 'https://paperswithcode.com/task/phrase-grounding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Flickr30k Entities Dev',\n",
       "   'Flickr30k Entities Test',\n",
       "   'Flickr30K Entities'],\n",
       "  'num_papers': 77,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fgvc-aircraft-1',\n",
       "  'name': 'FGVC-Aircraft',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/',\n",
       "  'description': 'FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label.\\r\\nAircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:\\r\\n\\r\\n* Model, e.g. Boeing 737-76J. Since certain models are nearly visually indistinguishable, this level is not used in the evaluation.\\r\\n* Variant, e.g. Boeing 737-700. A variant collapses all the models that are visually indistinguishable into one class. The dataset comprises 102 different variants.\\r\\n* Family, e.g. Boeing 737. The dataset comprises 70 different families.\\r\\n* Manufacturer, e.g. Boeing. The dataset comprises 41 different manufacturers.\\r\\nThe data is divided into three equally-sized training, validation and test subsets.\\r\\n\\r\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/](https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/)\\r\\nImage Source: [Fine-Grained Visual Classification of Aircraft](https://arxiv.org/abs/1306.5151)',\n",
       "  'paper': {'title': 'Fine-Grained Visual Classification of Aircraft',\n",
       "   'url': 'https://paperswithcode.com/paper/fine-grained-visual-classification-of'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Fine-Grained Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/fine-grained-image-classification'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FGVC Aircraft', 'FGVC-Aircraft'],\n",
       "  'num_papers': 151,\n",
       "  'data_loaders': [{'url': 'https://github.com/phi-3517/aircraft',\n",
       "    'repo': 'https://github.com/phi-3517/aircraft',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/duts',\n",
       "  'name': 'DUTS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://saliencydetection.net/duts/',\n",
       "  'description': '**DUTS** is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects.\\r\\n\\r\\nSource: [http://saliencydetection.net/duts/](http://saliencydetection.net/duts/)\\r\\nImage Source: [https://ieeexplore.ieee.org/document/8099887](https://ieeexplore.ieee.org/document/8099887)',\n",
       "  'paper': {'title': 'Learning to Detect Salient Objects With Image-Level Supervision',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-detect-salient-objects-with-image'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'RGB Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/salient-object-detection-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DUTS-test', 'DUTS-TE', 'DUTS'],\n",
       "  'num_papers': 145,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lip',\n",
       "  'name': 'LIP',\n",
       "  'full_name': 'Look into Person',\n",
       "  'homepage': 'http://sysu-hcp.net/lip/',\n",
       "  'description': 'The **LIP** (**Look into Person**) dataset is a large-scale dataset focusing on semantic understanding of a person. It contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels and 2D human poses with 16 key points. The images are collected from real-world scenarios and the subjects appear with challenging poses and view, heavy occlusions, various appearances and low resolution.\\r\\n\\r\\nSource: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)\\r\\nImage Source: [http://sysu-hcp.net/lip/](http://sysu-hcp.net/lip/)',\n",
       "  'paper': {'title': 'Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing',\n",
       "   'url': 'https://paperswithcode.com/paper/look-into-person-self-supervised-structure'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LIP val', 'LIP'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/apolloscape-1',\n",
       "  'name': 'ApolloScape',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://apolloscape.auto/',\n",
       "  'description': '**ApolloScape** is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D.\\r\\n\\r\\nSource: [A2D2: Audi Autonomous Driving Dataset](https://arxiv.org/abs/2004.06320)\\r\\nImage Source: [https://arxiv.org/pdf/1803.06184.pdf](https://arxiv.org/pdf/1803.06184.pdf)',\n",
       "  'paper': {'title': 'The ApolloScape Open Dataset for Autonomous Driving and its Application',\n",
       "   'url': 'https://paperswithcode.com/paper/the-apolloscape-open-dataset-for-autonomous'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Trajectory Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/trajectory-prediction'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'},\n",
       "   {'task': 'Motion Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/motion-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ApolloScape'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/posetrack',\n",
       "  'name': 'PoseTrack',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://posetrack.net/',\n",
       "  'description': 'The **PoseTrack** dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance.\\r\\n\\r\\nSource: [Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208)\\r\\nImage Source: [https://posetrack.net/](https://posetrack.net/)',\n",
       "  'paper': {'title': 'PoseTrack: A Benchmark for Human Pose Estimation and Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/posetrack-a-benchmark-for-human-pose'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Multi-Person Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/multi-person-pose-estimation'},\n",
       "   {'task': 'Pose Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/pose-tracking'},\n",
       "   {'task': 'Multi-Person Pose Estimation and Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-person-pose-estimation-and-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PoseTrack2017', 'PoseTrack2018', 'PoseTrack'],\n",
       "  'num_papers': 59,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/2d_body_keypoint.md#posetrack18',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/icvl-hand-posture',\n",
       "  'name': 'ICVL Hand Posture',\n",
       "  'full_name': 'ICVL Hand Posture Dataset',\n",
       "  'homepage': 'https://labicvl.github.io/hand.html',\n",
       "  'description': 'The ICVL dataset is a hand pose estimation dataset that consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame.\\n\\nSource: [AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation](https://arxiv.org/abs/2007.09590)\\nImage Source: [Tang et al.; Latent Regression Forest: Structured Estimation of 3D Hand Poses](https://alykhantejani.github.io/pdfs/LRF_PAMI_DRAFT.pdf)',\n",
       "  'paper': {'title': 'Latent Regression Forest: Structured Estimation of 3D Articulated Hand Posture',\n",
       "   'url': 'https://paperswithcode.com/paper/latent-regression-forest-structured'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'},\n",
       "   {'task': '3D Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ICVL Hand Posture'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': [{'url': 'https://github.com/zhangboshen/A2J',\n",
       "    'repo': 'https://github.com/zhangboshen/A2J',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/segtrack-v2-1',\n",
       "  'name': 'SegTrack-v2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html',\n",
       "  'description': 'SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video.\\r\\n\\r\\nSource: [https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html](https://web.engr.oregonstate.edu/~lif/SegTrack2/dataset.html)\\r\\nImage Source: [https://www.researchgate.net/publication/325842926_Semantic_Video_Segmentation_A_Review_on_Recent_Approaches](https://www.researchgate.net/publication/325842926_Semantic_Video_Segmentation_A_Review_on_Recent_Approaches)',\n",
       "  'paper': {'title': 'Video Segmentation by Tracking Many Figure-Ground Segments',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2013.273'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-object-segmentation'},\n",
       "   {'task': 'Video Salient Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/video-salient-object-detection'},\n",
       "   {'task': 'Unsupervised Video Object Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-video-object-segmentation'},\n",
       "   {'task': 'Video Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-segmentation'},\n",
       "   {'task': 'Video Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SegTrack v2', 'SegTrack-v2'],\n",
       "  'num_papers': 83,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/foggy-cityscapes',\n",
       "  'name': 'Foggy Cityscapes',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/',\n",
       "  'description': '**Foggy Cityscapes** is a synthetic foggy dataset which simulates fog on real scenes. Each foggy image is rendered with a clear image and depth map from Cityscapes. Thus the annotations and data split in Foggy Cityscapes are inherited from Cityscapes.\\r\\n\\r\\nSource: [Exploring Object Relation in Mean Teacher for Cross-Domain Detection](https://arxiv.org/abs/1904.11245)\\r\\nImage Source: [http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/](http://people.ee.ethz.ch/~csakarid/SFSU_synthetic/)',\n",
       "  'paper': {'title': 'Semantic Foggy Scene Understanding with Synthetic Data',\n",
       "   'url': 'https://paperswithcode.com/paper/semantic-foggy-scene-understanding-with'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cityscapes to Foggy Cityscapes',\n",
       "   'Cityscapes-to-Foggy Cityscapes',\n",
       "   'Foggy Cityscapes'],\n",
       "  'num_papers': 108,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vimeo90k-1',\n",
       "  'name': 'Vimeo90K',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://toflow.csail.mit.edu/',\n",
       "  'description': 'The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.\\r\\n\\r\\nSource: [https://arxiv.org/pdf/1711.09078.pdf](https://arxiv.org/pdf/1711.09078.pdf)\\r\\nImage Source: [http://toflow.csail.mit.edu/](http://toflow.csail.mit.edu/)',\n",
       "  'paper': {'title': 'Video Enhancement with Task-Oriented Flow',\n",
       "   'url': 'https://paperswithcode.com/paper/video-enhancement-with-task-oriented-flow'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/super-resolution'},\n",
       "   {'task': 'Video Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/video-super-resolution'},\n",
       "   {'task': 'Optical Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/optical-flow-estimation'},\n",
       "   {'task': 'Video Frame Interpolation',\n",
       "    'url': 'https://paperswithcode.com/task/video-frame-interpolation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Vimeo90K'],\n",
       "  'num_papers': 84,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpiigaze',\n",
       "  'name': 'MPIIGaze',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild',\n",
       "  'description': '**MPIIGaze** is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination.\\r\\n\\r\\nSource: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild)\\r\\nImage Source: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild)',\n",
       "  'paper': {'title': 'Appearance-Based Gaze Estimation in the Wild',\n",
       "   'url': 'https://paperswithcode.com/paper/appearance-based-gaze-estimation-in-the-wild'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Gaze Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/gaze-estimation'},\n",
       "   {'task': 'Style Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/style-transfer'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MPII Gaze', 'MPIIGaze'],\n",
       "  'num_papers': 53,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/referitgame',\n",
       "  'name': 'ReferItGame',\n",
       "  'full_name': 'ReferItGame',\n",
       "  'homepage': 'http://tamaraberg.com/referitgame/',\n",
       "  'description': 'The ReferIt dataset contains 130,525 expressions for referring to 96,654 objects in 19,894 images of natural scenes.\\r\\n\\r\\nSource: [BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up Visual Processing with Referring Expressions](https://arxiv.org/abs/2003.12739)\\r\\nImage Source: [http://tamaraberg.com/referitgame/](http://tamaraberg.com/referitgame/)',\n",
       "  'paper': {'title': 'ReferItGame: Referring to Objects in Photographs of Natural Scenes',\n",
       "   'url': 'https://paperswithcode.com/paper/referitgame-referring-to-objects-in'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Image Captioning',\n",
       "    'url': 'https://paperswithcode.com/task/image-captioning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ReferItGame'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multithumos',\n",
       "  'name': 'MultiTHUMOS',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://ai.stanford.edu/~syyeung/everymoment.html',\n",
       "  'description': \"The **MultiTHUMOS** dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video.\\r\\n\\r\\nSource: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)\\r\\nImage Source: [http://ai.stanford.edu/~syyeung/everymoment.html](http://ai.stanford.edu/~syyeung/everymoment.html)\",\n",
       "  'paper': {'title': 'Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos',\n",
       "   'url': 'https://paperswithcode.com/paper/every-moment-counts-dense-detailed-labeling'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Detection',\n",
       "    'url': 'https://paperswithcode.com/task/action-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Multi-THUMOS', 'MultiTHUMOS'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/crowdhuman',\n",
       "  'name': 'CrowdHuman',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.crowdhuman.org/',\n",
       "  'description': '**CrowdHuman** is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10× boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with ∼340k person and ∼99k ignore region annotations in the CrowdHuman training subset.\\r\\n\\r\\nSource: [SADet: Learning An Efficient and Accurate Pedestrian Detector](https://arxiv.org/abs/2007.13119)\\r\\nImage Source: [http://www.crowdhuman.org/](http://www.crowdhuman.org/)',\n",
       "  'paper': {'title': 'CrowdHuman: A Benchmark for Detecting Human in a Crowd',\n",
       "   'url': 'https://paperswithcode.com/paper/crowdhuman-a-benchmark-for-detecting-human-in'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['CrowdHuman (full body)', 'CrowdHuman'],\n",
       "  'num_papers': 63,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msrdailyactivity3d',\n",
       "  'name': 'MSRDailyActivity3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://research.microsoft.com/∼zliu/ActionRecoRsrc',\n",
       "  'description': '**DailyActivity3D** dataset is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, sit down. If possible, each subject performs an activity in two different poses: “sitting on sofa” and “standing”. The total number of the activity samples is 320.\\r\\nThis dataset is designed to cover human’s daily activities in the living room. When the performer stands close to the sofa or sits on the sofa, the 3D joint positions extracted by the skeleton tracker are very noisy. Moreover, most of the activities involve the humans-object interactions. Thus this dataset is more challenging.\\r\\n\\r\\nSource: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf)\\r\\nImage Source: [https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition](https://www.researchgate.net/publication/308001852_Automatic_Learning_of_Articulated_Skeletons_Based_on_Mean_of_3D_Joints_for_Efficient_Action_Recognition)',\n",
       "  'paper': {'title': 'Mining actionlet ensemble for action recognition with depth cameras',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2012.6247813'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Multimodal Activity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/multimodal-activity-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MSR Daily Activity3D',\n",
       "   'MSR Daily Activity3D dataset',\n",
       "   'MSRDailyActivity3D'],\n",
       "  'num_papers': 38,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mcmaster',\n",
       "  'name': 'McMaster',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www4.comp.polyu.edu.hk/~cslzhang/CDM_Dataset.htm',\n",
       "  'description': 'The **McMaster** dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500×500.\\r\\n\\r\\nSource: [FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising](https://arxiv.org/abs/1710.04026)\\r\\nImage Source: [https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf](https://www4.comp.polyu.edu.hk/~cslzhang/paper/LMMSEdemosaicing.pdf)',\n",
       "  'paper': {'title': 'Color demosaicking by local directional interpolation and nonlocal adaptive thresholding',\n",
       "   'url': 'https://doi.org/10.1117/1.3600632'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Joint Demosaicing and Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/joint-demosaicing-and-denoising'}],\n",
       "  'languages': [],\n",
       "  'variants': ['McMaster',\n",
       "   'McMaster sigma75',\n",
       "   'McMaster sigma50',\n",
       "   'McMaster sigma35',\n",
       "   'McMaster sigma25',\n",
       "   'McMaster sigma15'],\n",
       "  'num_papers': 35,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sketch',\n",
       "  'name': 'Sketch',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/',\n",
       "  'description': 'The **Sketch** dataset contains over 20,000 sketches evenly distributed over 250 object categories.\\r\\n\\r\\nSource: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)\\r\\nImage Source: [http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/](http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/)',\n",
       "  'paper': {'title': 'How do humans sketch objects?',\n",
       "   'url': 'https://doi.org/10.1145/2185520.2185540'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Sketch (Fine-grained 6 Tasks)', 'Sketch'],\n",
       "  'num_papers': 153,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wireframe',\n",
       "  'name': 'Wireframe',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/huangkuns/wireframe',\n",
       "  'description': 'The **Wireframe** dataset consists of 5,462 images (5,000 for training, 462 for test) of indoor and outdoor man-made scenes.\\r\\n\\r\\nSource: [MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line Segment Detection](https://arxiv.org/abs/2001.01788)\\r\\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf)',\n",
       "  'paper': {'title': 'Learning to Parse Wireframes in Images of Man-Made Environments',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-parse-wireframes-in-images-of-man-1'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Line Segment Detection',\n",
       "    'url': 'https://paperswithcode.com/task/line-segment-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['wireframe dataset', 'Wireframe'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': [{'url': 'https://github.com/huangkuns/wireframe',\n",
       "    'repo': 'https://github.com/huangkuns/wireframe',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mnist-m',\n",
       "  'name': 'MNIST-M',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://yaroslav.ganin.net/',\n",
       "  'description': '**MNIST-M** is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images.\\r\\n\\r\\nSource: [A Review of Single-Source Deep Unsupervised Visual Domain Adaptation](https://arxiv.org/abs/2009.00155)\\r\\nImage Source: [https://arxiv.org/pdf/1505.07818v4.pdf](https://arxiv.org/pdf/1505.07818v4.pdf)',\n",
       "  'paper': {'title': 'Domain-Adversarial Training of Neural Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/domain-adversarial-training-of-neural'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MNIST-to-MNIST-M', 'MNIST-M'],\n",
       "  'num_papers': 136,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/spider-1',\n",
       "  'name': 'SPIDER',\n",
       "  'full_name': 'SPIDER',\n",
       "  'homepage': 'https://yale-lily.github.io/spider',\n",
       "  'description': 'Spider is a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. The goal of the Spider challenge is to develop natural language interfaces to cross-domain databases. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. In Spider 1.0, different complex SQL queries and databases appear in train and test sets. To do well on it, systems must generalize well to not only new SQL queries but also new database schemas.\\r\\n\\r\\nSource: [Spider](https://yale-lily.github.io/spider)\\r\\nImage Source: [https://yale-lily.github.io/spider](https://yale-lily.github.io/spider)',\n",
       "  'paper': {'title': 'Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task',\n",
       "   'url': 'https://paperswithcode.com/paper/spider-a-large-scale-human-labeled-dataset'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Text-To-Sql',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-sql'}],\n",
       "  'languages': [],\n",
       "  'variants': ['spider', 'SPIDER'],\n",
       "  'num_papers': 60,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/spider',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/tieredimagenet',\n",
       "  'name': 'tieredImageNet',\n",
       "  'full_name': 'tieredImageNet',\n",
       "  'homepage': 'https://github.com/yaoyao-liu/tiered-imagenet-tools',\n",
       "  'description': 'The **tieredImageNet** dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes.\\r\\n\\r\\nSource: [tieredImageNet](https://github.com/yaoyao-liu/tiered-imagenet-tools)\\r\\nImage Source: [https://arxiv.org/pdf/1803.00676.pdf](https://arxiv.org/pdf/1803.00676.pdf)',\n",
       "  'paper': {'title': 'Meta-Learning for Semi-Supervised Few-Shot Classification',\n",
       "   'url': 'https://paperswithcode.com/paper/meta-learning-for-semi-supervised-few-shot'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tiered ImageNet 5-way (1-shot)',\n",
       "   'Tiered ImageNet 5-way (5-shot)',\n",
       "   'tieredImageNet',\n",
       "   'Tiered ImageNet 10-way (1-shot)',\n",
       "   'Tiered ImageNet 10-way (5-shot)'],\n",
       "  'num_papers': 201,\n",
       "  'data_loaders': [{'url': 'https://github.com/yaoyao-liu/tiered-imagenet-tools',\n",
       "    'repo': 'https://github.com/yaoyao-liu/tiered-imagenet-tools',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/apy',\n",
       "  'name': 'aPY',\n",
       "  'full_name': 'Attribute Pascal and Yahoo',\n",
       "  'homepage': 'https://vision.cs.uiuc.edu/attributes/',\n",
       "  'description': '**aPY** is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, …, zebra).\\r\\n\\r\\nSource: [From Classical to Generalized Zero-Shot Learning: a Simple Adaptation Process](https://arxiv.org/abs/1809.10120)\\r\\nImage Source: [https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf](https://www.cs.cmu.edu/~afarhadi/papers/Attributes.pdf)',\n",
       "  'paper': {'title': 'Describing objects by their attributes',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2009.5206772'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['aPY - 0-Shot', 'aPY'],\n",
       "  'num_papers': 123,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/visda-2017',\n",
       "  'name': 'VisDA-2017',\n",
       "  'full_name': 'VisDA-2017',\n",
       "  'homepage': 'http://ai.bu.edu/visda-2017/',\n",
       "  'description': '**VisDA-2017** is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO..\\r\\n\\r\\nSource: [Gradually Vanishing Bridge for Adversarial Domain Adaptation](https://arxiv.org/abs/2003.13183)\\r\\nImage Source: [http://ai.bu.edu/visda-2017/](http://ai.bu.edu/visda-2017/)',\n",
       "  'paper': {'title': 'VisDA: The Visual Domain Adaptation Challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/visda-the-visual-domain-adaptation-challenge'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Partial Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/partial-domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VisDA2017', 'VisDA-2017'],\n",
       "  'num_papers': 117,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pdbbind-1',\n",
       "  'name': 'PDBBind',\n",
       "  'full_name': 'PDBBind',\n",
       "  'homepage': 'http://www.pdbbind.org.cn/',\n",
       "  'description': 'The **PDBBind** database provides a comprehensive collection of structures of protein-ligand complexes and their binding affinity data. The original experimental data in Protein Data Bank (PDB) are selected to PDBBind database based on certain quality requirements and curated for applications.\\n\\nSource: [Representability of algebraic topology for biomolecules in machine learning based scoring and virtual screening](https://arxiv.org/abs/1708.08135)\\nImage Source: [http://www.pdbbind.org.cn/browse.php](http://www.pdbbind.org.cn/browse.php)',\n",
       "  'paper': {'title': 'PDB-wide collection of binding data: current status of the PDBbind database',\n",
       "   'url': 'https://doi.org/10.1093/bioinformatics/btu626'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Drug Discovery',\n",
       "    'url': 'https://paperswithcode.com/task/drug-discovery'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PDBbind', 'PDBBind'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenet-32',\n",
       "  'name': 'ImageNet-32',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/PatrykChrabaszcz/Imagenet32_Scripts',\n",
       "  'description': 'Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels.\\r\\n\\r\\nSource: [Self-supervised Knowledge Distillation Using Singular Value Decomposition](https://arxiv.org/abs/1807.06819)\\r\\nImage Source: [https://arxiv.org/pdf/1707.08819v3.pdf](https://arxiv.org/pdf/1707.08819v3.pdf)',\n",
       "  'paper': {'title': 'A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets',\n",
       "   'url': 'https://paperswithcode.com/paper/a-downsampled-variant-of-imagenet-as-an'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Compression',\n",
       "    'url': 'https://paperswithcode.com/task/image-compression'},\n",
       "   {'task': 'Sparse Learning',\n",
       "    'url': 'https://paperswithcode.com/task/sparse-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ImageNet32', 'ImageNet-32'],\n",
       "  'num_papers': 61,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/imagenet_resized',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/PatrykChrabaszcz/Imagenet32_Scripts',\n",
       "    'repo': 'https://github.com/PatrykChrabaszcz/Imagenet32_Scripts',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mvtecad',\n",
       "  'name': 'MVTecAD',\n",
       "  'full_name': 'MVTEC ANOMALY DETECTION DATASET',\n",
       "  'homepage': 'https://www.mvtec.com/company/research/datasets/mvtec-ad/',\n",
       "  'description': 'MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects.\\r\\n\\r\\nThere are two common metrics: Detection AUROC and Segmentation (or pixelwise) AUROC\\r\\n\\r\\nDetection (or, classification) methods output single float (anomaly score) per input test image. \\r\\n\\r\\nSegmentation methods output anomaly probability for each pixel. \\r\\n\"To assess segmentation performance, we evaluate the relative per-region overlap of the segmentation with the ground truth. To get an additional performance measure that is independent of the determined threshold, we compute the area under the receiver operating characteristic curve (ROC AUC). We define the true positive rate as the percentage of pixels that were correctly classified as anomalous\" [1]\\r\\nLater segmentation metric was improved to balance regions with small and large area, see PRO-AUC and other in [2]\\r\\nSource: [MVTEC ANOMALY DETECTION DATASET](https://www.mvtec.com/company/research/datasets/mvtec-ad/)\\r\\nImage Source: [https://www.mvtec.com/company/research/datasets/mvtec-ad/](https://www.mvtec.com/company/research/datasets/mvtec-ad/)\\r\\n[1] Paul Bergmann et al, \"MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection\"\\r\\n[2] [Bergmann, P., Batzner, K., Fauser, M. et al. The MVTec Anomaly Detection Dataset: A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection. Int J Comput Vis (2021). https://doi.org/10.1007/s11263-020-01400-4](https://link.springer.com/article/10.1007/s11263-020-01400-4)',\n",
       "  'paper': {'title': 'MVTec AD -- A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection',\n",
       "   'url': 'https://paperswithcode.com/paper/mvtec-ad-a-comprehensive-real-world-dataset'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': '', 'url': 'https://paperswithcode.com/task/task'},\n",
       "   {'task': 'Outlier Detection',\n",
       "    'url': 'https://paperswithcode.com/task/outlier-detection'},\n",
       "   {'task': 'Unsupervised Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-anomaly-detection'},\n",
       "   {'task': 'Few Shot Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-anomaly-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MVTec AD', 'MVTecAD'],\n",
       "  'num_papers': 80,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kvasir',\n",
       "  'name': 'Kvasir',\n",
       "  'full_name': 'The Kvasir Dataset',\n",
       "  'homepage': 'https://datasets.simula.no/kvasir/',\n",
       "  'description': 'The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class.\\r\\n\\r\\nSource: [Two-Stream Deep Feature Modelling for Automated Video Endoscopy Data Analysis](https://arxiv.org/abs/2007.05914)\\r\\nImage Source: [https://datasets.simula.no/kvasir/](https://datasets.simula.no/kvasir/)',\n",
       "  'paper': {'title': 'KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection',\n",
       "   'url': 'https://doi.org/10.1145/3083187.3083212'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Medical Image Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/medical-image-segmentation'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'},\n",
       "   {'task': 'Colorectal Polyps Characterization',\n",
       "    'url': 'https://paperswithcode.com/task/colorectal-polyps-characterization'},\n",
       "   {'task': 'Real-Time Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-object-detection'},\n",
       "   {'task': 'Colorectal Gland Segmentation:',\n",
       "    'url': 'https://paperswithcode.com/task/colorectal-gland-segmentation'},\n",
       "   {'task': 'Polyp Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/polyp-segmentation'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['Kvasir-SEG', 'Kvasir'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/syn2real',\n",
       "  'name': 'Syn2Real',\n",
       "  'full_name': 'Syn2Real',\n",
       "  'homepage': 'https://ai.bu.edu/syn2real/',\n",
       "  'description': '**Syn2Real**, a synthetic-to-real visual domain adaptation benchmark meant to encourage further development of robust domain transfer methods. The goal is to train a model on a synthetic \"source\" domain and then update it so that its performance improves on a real \"target\" domain, without using any target annotations. It includes three tasks, illustrated in figures above: the more traditional closed-set classification task with a known set of categories; the less studied open-set classification task with unknown object categories in the target domain; and the object detection task, which involves localizing instances of objects by predicting their bounding boxes and corresponding class labels.\\r\\n\\r\\nSource: [Syn2Real](https://ai.bu.edu/syn2real/)\\r\\nImage Source: [https://ai.bu.edu/syn2real/](https://ai.bu.edu/syn2real/)',\n",
       "  'paper': {'title': 'Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation',\n",
       "   'url': 'https://paperswithcode.com/paper/syn2real-a-new-benchmark-forsynthetic-to-real'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Synthetic-to-Real Translation',\n",
       "    'url': 'https://paperswithcode.com/task/synthetic-to-real-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Syn2Real-C', 'Syn2Real'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/anli',\n",
       "  'name': 'ANLI',\n",
       "  'full_name': 'Adversarial NLI',\n",
       "  'homepage': 'https://github.com/facebookresearch/anli',\n",
       "  'description': 'The Adversarial Natural Language Inference (**ANLI**, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa.\\r\\n\\r\\nSource: [The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/abs/2002.07972)\\r\\nImage Source: [https://arxiv.org/pdf/1910.14599.pdf](https://arxiv.org/pdf/1910.14599.pdf)',\n",
       "  'paper': {'title': 'Adversarial NLI: A New Benchmark for Natural Language Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/adversarial-nli-a-new-benchmark-for-natural'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Natural Language Inference',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-inference'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ANLI test', 'ANLI'],\n",
       "  'num_papers': 85,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/anli',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#adversarial-natural-language-inference-anli-corpus',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/anli',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/facebookresearch/anli',\n",
       "    'repo': 'https://github.com/facebookresearch/anli',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cityscapes',\n",
       "  'name': 'Cityscapes',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cityscapes-dataset.com/dataset-overview/',\n",
       "  'description': '**Cityscapes** is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background.\\r\\n\\r\\nSource: [A Review on Deep Learning Techniques Applied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\\r\\nImage Source: [https://www.cityscapes-dataset.com/dataset-overview/](https://www.cityscapes-dataset.com/dataset-overview/)',\n",
       "  'paper': {'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/the-cityscapes-dataset-for-semantic-urban'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Scene Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/scene-parsing'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Semi-Supervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-semantic-segmentation'},\n",
       "   {'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'},\n",
       "   {'task': 'Panoptic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/panoptic-segmentation'},\n",
       "   {'task': 'Edge Detection',\n",
       "    'url': 'https://paperswithcode.com/task/edge-detection'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'},\n",
       "   {'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Real-time Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-instance-segmentation'},\n",
       "   {'task': 'Video Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/video-semantic-segmentation'},\n",
       "   {'task': 'Robust Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/robust-object-detection'},\n",
       "   {'task': 'Domain 11-5',\n",
       "    'url': 'https://paperswithcode.com/task/domain-11-5'},\n",
       "   {'task': 'Domain 11-1',\n",
       "    'url': 'https://paperswithcode.com/task/domain-11-1'},\n",
       "   {'task': 'Domain 1-1',\n",
       "    'url': 'https://paperswithcode.com/task/domain-1-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cityscapes 10% labeled',\n",
       "   'Cityscapes',\n",
       "   'Cityscapes-5K 256x512',\n",
       "   'Cityscapes-25K 256x512',\n",
       "   'Cityscapes val',\n",
       "   'Cityscapes test',\n",
       "   'Cityscapes Photo-to-Labels',\n",
       "   'Cityscapes Labels-to-Photo',\n",
       "   'Cityscapes 50% labeled',\n",
       "   'Cityscapes 25% labeled',\n",
       "   'Cityscapes 12.5% labeled',\n",
       "   'Cityscapes 100 samples labeled'],\n",
       "  'num_papers': 1958,\n",
       "  'data_loaders': [{'url': 'https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-cityscapes',\n",
       "    'repo': 'https://github.com/facebookresearch/detectron2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.Cityscapes',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/cityscapes',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'repo': 'https://github.com/facebookresearch/MaskFormer',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-voc',\n",
       "  'name': 'PASCAL VOC',\n",
       "  'full_name': 'PASCAL Visual Object Classes Challenge',\n",
       "  'homepage': 'http://host.robots.ox.ac.uk/pascal/VOC/',\n",
       "  'description': 'The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The **PASCAL VOC** dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.\\r\\n\\r\\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\\r\\nImage Source: [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/images/sheep_06.jpg)',\n",
       "  'paper': {'title': 'Location-aware Single Image Reflection Removal',\n",
       "   'url': 'https://paperswithcode.com/paper/location-aware-single-image-reflection'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'},\n",
       "   {'task': 'Semi-Supervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semi-supervised-semantic-segmentation'},\n",
       "   {'task': 'Weakly Supervised Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-object-detection'},\n",
       "   {'task': 'Interactive Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/interactive-segmentation'},\n",
       "   {'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Unsupervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-semantic-segmentation'},\n",
       "   {'task': 'Single-object discovery',\n",
       "    'url': 'https://paperswithcode.com/task/single-object-discovery'},\n",
       "   {'task': 'Class-agnostic Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/class-agnostic-object-detection'},\n",
       "   {'task': 'Object Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/object-proposal-generation'},\n",
       "   {'task': 'Single-object colocalization',\n",
       "    'url': 'https://paperswithcode.com/task/single-object-colocalization'},\n",
       "   {'task': 'Open World Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/open-world-object-detection'},\n",
       "   {'task': 'Weakly-Supervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-semantic-segmentation'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Object Counting',\n",
       "    'url': 'https://paperswithcode.com/task/object-counting'},\n",
       "   {'task': 'Zero-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-object-detection'},\n",
       "   {'task': 'Real-Time Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-object-detection'},\n",
       "   {'task': 'Weakly-supervised instance segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-instance-segmentation'},\n",
       "   {'task': 'Robust Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/robust-object-detection'},\n",
       "   {'task': 'Multi-object discovery',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-discovery'},\n",
       "   {'task': 'Knowledge Distillation',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-distillation'},\n",
       "   {'task': 'Graph Matching',\n",
       "    'url': 'https://paperswithcode.com/task/graph-matching'},\n",
       "   {'task': 'One-Shot Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/one-shot-object-detection'},\n",
       "   {'task': 'Multi-object colocalization',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-colocalization'},\n",
       "   {'task': 'Overlapped 19-1',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-19-1'},\n",
       "   {'task': 'Overlapped 15-5',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-15-5'},\n",
       "   {'task': 'Overlapped 15-1',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-15-1'},\n",
       "   {'task': 'Overlapped 10-1',\n",
       "    'url': 'https://paperswithcode.com/task/overlapped-10-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VOC12',\n",
       "   'PASCAL VOC 2012',\n",
       "   'PASCAL VOC',\n",
       "   'Pascal VOC 2012 5% labeled',\n",
       "   'Pascal VOC 2012 2% labeled',\n",
       "   'Pascal VOC 2012 12.5% labeled',\n",
       "   'Pascal VOC 2012 1% labeled',\n",
       "   'Pascal VOC 2007 count-test',\n",
       "   'PASCAL VOC 2012, 60 proposals per image',\n",
       "   'PASCAL VOC 2012 val',\n",
       "   'PASCAL VOC 2012 test',\n",
       "   'PASCAL VOC 2012 25% labeled',\n",
       "   'PASCAL VOC 2011 test',\n",
       "   'PASCAL VOC 2011',\n",
       "   'PASCAL VOC 2007'],\n",
       "  'num_papers': 204,\n",
       "  'data_loaders': [{'url': 'https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-pascal-voc',\n",
       "    'repo': 'https://github.com/facebookresearch/detectron2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.VOCSegmentation',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vgg-face-1',\n",
       "  'name': 'VGG Face',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/vgg_face/',\n",
       "  'description': 'The **VGG Face** dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images.\\r\\n\\r\\nSource: [https://www.robots.ox.ac.uk/~vgg/data/vgg_face/](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/)\\r\\nImage Source: [http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf](http://www.bmva.org/bmvc/2015/papers/paper041/paper041.pdf)',\n",
       "  'paper': {'title': 'Deep Face Recognition',\n",
       "   'url': 'https://doi.org/10.5244/C.29.41'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VGG Face'],\n",
       "  'num_papers': 74,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/librispeech',\n",
       "  'name': 'LibriSpeech',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.openslr.org/12',\n",
       "  'description': 'The **LibriSpeech** corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.\\r\\n\\r\\nSource: [State-of-the-art Speech Recognition using Multi-stream Self-attention with Dilated 1D Convolutions](https://arxiv.org/abs/1910.00716)',\n",
       "  'paper': {'title': 'Librispeech: An ASR corpus based on public domain audio books',\n",
       "   'url': 'https://doi.org/10.1109/ICASSP.2015.7178964'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Audio', 'Speech'],\n",
       "  'tasks': [{'task': 'Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-recognition'},\n",
       "   {'task': 'Speech Enhancement',\n",
       "    'url': 'https://paperswithcode.com/task/speech-enhancement'},\n",
       "   {'task': 'Automatic Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/automatic-speech-recognition'},\n",
       "   {'task': 'Resynthesis',\n",
       "    'url': 'https://paperswithcode.com/task/resynthesis'}],\n",
       "  'languages': ['Hindi'],\n",
       "  'variants': ['LibriSpeech test-clean',\n",
       "   'LibriSpeech test-other',\n",
       "   'LibriSpeechDuplicate',\n",
       "   'LibriSpeechLibri-Light test-othertest-other',\n",
       "   'LibriSpeech',\n",
       "   'LibriSpeech ASR'],\n",
       "  'num_papers': 943,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/librispeech_asr',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/librispeech',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/audio/stable/datasets.html#torchaudio.datasets.LIBRISPEECH',\n",
       "    'repo': 'https://github.com/pytorch/audio',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://gitlab.com/jaco-assistant/corcua',\n",
       "    'repo': 'https://gitlab.com/jaco-assistant/corcua',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/casia-webface',\n",
       "  'name': 'CASIA-WebFace',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'The **CASIA-WebFace** dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web.\\r\\n\\r\\nSource: [On Hallucinating Context and Background Pixels from a Face Mask using Multi-scale GANs](https://arxiv.org/abs/1811.07104)',\n",
       "  'paper': {'title': 'Learning Face Representation from Scratch',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-face-representation-from-scratch'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Metric Learning',\n",
       "    'url': 'https://paperswithcode.com/task/metric-learning'},\n",
       "   {'task': 'Facial Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/facial-inpainting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WebFace', 'WebFace - 8x upscaling', 'CASIA-WebFace'],\n",
       "  'num_papers': 277,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/set14',\n",
       "  'name': 'Set14',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "  'description': 'The **Set14** dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models.\\r\\nImage Source: [https://www.ece.rice.edu/~wakin/images/](https://www.ece.rice.edu/~wakin/images/)',\n",
       "  'paper': {'title': 'On Single Image Scale-Up Using Sparse-Representations',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-27413-8_47'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Set14 - 4x upscaling',\n",
       "   'Set14 - 2x upscaling',\n",
       "   'Set14 - 3x upscaling',\n",
       "   'Set14 - 8x upscaling',\n",
       "   'Set14'],\n",
       "  'num_papers': 289,\n",
       "  'data_loaders': [{'url': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "    'repo': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "    'frameworks': ['none']},\n",
       "   {'url': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'repo': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ms-celeb-1m',\n",
       "  'name': 'MS-Celeb-1M',\n",
       "  'full_name': '',\n",
       "  'homepage': None,\n",
       "  'description': 'The **MS-Celeb-1M** dataset is a large-scale face recognition dataset consists of 100K identities, and each identity has about 100 facial images. The original identity labels are obtained automatically from webpages.\\r\\n\\r\\n**NOTE**: This dataset [is currently inactive](https://exposing.ai/msceleb/). \\r\\n\\r\\nSource: [Learning to Cluster Faces on an Affinity Graph](https://arxiv.org/abs/1904.02749)\\r\\nImage Source: [MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition](https://arxiv.org/abs/1607.08221)',\n",
       "  'paper': {'title': 'MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/ms-celeb-1m-a-dataset-and-benchmark-for-large'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Verification',\n",
       "    'url': 'https://paperswithcode.com/task/face-verification'},\n",
       "   {'task': 'Face Identification',\n",
       "    'url': 'https://paperswithcode.com/task/face-identification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MS-Celeb-1M'],\n",
       "  'num_papers': 186,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/uci-machine-learning-repository',\n",
       "  'name': 'UCI Machine Learning Repository',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://archive.ics.uci.edu/ml/datasets.php',\n",
       "  'description': '**UCI Machine Learning Repository** is a collection of over 550 datasets.',\n",
       "  'paper': {'title': 'Endgame Analysis of Dou Shou Qi',\n",
       "   'url': 'https://paperswithcode.com/paper/endgame-analysis-of-dou-shou-qi'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Core set discovery',\n",
       "    'url': 'https://paperswithcode.com/task/core-set-discovery'},\n",
       "   {'task': 'Multivariate Time Series Imputation',\n",
       "    'url': 'https://paperswithcode.com/task/multivariate-time-series-imputation'},\n",
       "   {'task': 'Synthetic Data Generation',\n",
       "    'url': 'https://paperswithcode.com/task/synthetic-data-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCI localization data',\n",
       "   'UCI POWER',\n",
       "   'UCI MINIBOONE',\n",
       "   'UCI HEPMASS',\n",
       "   'UCI GAS',\n",
       "   'UCI Epileptic Seizure Recognition',\n",
       "   'UCI Machine Learning Repository'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/synthia',\n",
       "  'name': 'SYNTHIA',\n",
       "  'full_name': 'SYNTHetic Collection of Imagery and Annotations',\n",
       "  'homepage': 'https://synthia-dataset.net/',\n",
       "  'description': 'The **SYNTHIA** dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 × 960.\\r\\n\\r\\nSource: [Orientation-aware Semantic Segmentation on Icosahedron Spheres](https://arxiv.org/abs/1907.12849)\\r\\nImage Source: [https://synthia-dataset.net/](https://synthia-dataset.net/)',\n",
       "  'paper': {'title': 'The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes',\n",
       "   'url': 'https://paperswithcode.com/paper/the-synthia-dataset-a-large-collection-of'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Image-to-Image Translation',\n",
       "    'url': 'https://paperswithcode.com/task/image-to-image-translation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'},\n",
       "   {'task': 'Novel View Synthesis',\n",
       "    'url': 'https://paperswithcode.com/task/novel-view-synthesis'},\n",
       "   {'task': 'Synthetic-to-Real Translation',\n",
       "    'url': 'https://paperswithcode.com/task/synthetic-to-real-translation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SYNTHIA Fall-to-Winter',\n",
       "   'SYNTHIA-to-Cityscapes',\n",
       "   'Synthia Novel View Synthesis',\n",
       "   'SYNTHIA-CVPR’16',\n",
       "   'SYNTHIA'],\n",
       "  'num_papers': 307,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nyuv2',\n",
       "  'name': 'NYUv2',\n",
       "  'full_name': 'NYU-Depth V2',\n",
       "  'homepage': 'https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html',\n",
       "  'description': 'The **NYU-Depth V2** data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:\\r\\n\\r\\n* 1449 densely labeled pairs of aligned RGB and depth images\\r\\n* 464 new scenes taken from 3 cities\\r\\n* 407,024 new unlabeled frames\\r\\n* Each object is labeled with a class and an instance number.\\r\\nThe dataset has several components:\\r\\n* Labeled: A subset of the video data accompanied by dense multi-class labels. This data has also been preprocessed to fill in missing depth labels.\\r\\n* Raw: The raw RGB, depth and accelerometer data as provided by the Kinect.\\r\\n* Toolbox: Useful functions for manipulating the data and labels.\\r\\n\\r\\nSource: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)\\r\\nImage Source: [https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)',\n",
       "  'paper': {'title': 'Indoor Segmentation and Support Inference from RGBD Images',\n",
       "   'url': 'http://arxiv.org/pdf/1301.3572.pdf'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['RGB-D'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Monocular Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-depth-estimation'},\n",
       "   {'task': 'Depth Completion',\n",
       "    'url': 'https://paperswithcode.com/task/depth-completion'},\n",
       "   {'task': 'Scene Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-segmentation'},\n",
       "   {'task': 'Real-Time Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-semantic-segmentation'},\n",
       "   {'task': 'Surface Normals Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/surface-normals-estimation'},\n",
       "   {'task': 'Plane Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/plane-instance-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NYU-Depth V2',\n",
       "   'NYU Depth v2',\n",
       "   'NYU-Depth V2 Surface Normals',\n",
       "   'NYUv2'],\n",
       "  'num_papers': 446,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/nyu_depth_v2',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/urban100',\n",
       "  'name': 'Urban100',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "  'description': 'The **Urban100** dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models.\\r\\nImage Source: [http://vllab.ucmerced.edu/wlai24/LapSRN/](http://vllab.ucmerced.edu/wlai24/LapSRN/)',\n",
       "  'paper': {'title': 'Single Image Super-Resolution From Transformed Self-Exemplars',\n",
       "   'url': 'https://paperswithcode.com/paper/single-image-super-resolution-from'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Grayscale Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/grayscale-image-denoising'},\n",
       "   {'task': 'Compressive Sensing',\n",
       "    'url': 'https://paperswithcode.com/task/compressive-sensing'},\n",
       "   {'task': 'Joint Demosaicing and Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/joint-demosaicing-and-denoising'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Urban100',\n",
       "   'Urban100 sigma70',\n",
       "   'Urban100 sigma50',\n",
       "   'Urban100 sigma30',\n",
       "   'Urban100 sigma25',\n",
       "   'Urban100 sigma15',\n",
       "   'urban100 sigma15',\n",
       "   'Urban100 sigma10',\n",
       "   'Urban100 - 8x upscaling',\n",
       "   'Urban100 - 4x upscaling',\n",
       "   'Urban100 - 3x upscaling',\n",
       "   'Urban100 - 2x upscaling',\n",
       "   'Urban100 - 16x upscaling'],\n",
       "  'num_papers': 277,\n",
       "  'data_loaders': [{'url': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "    'repo': 'https://github.com/jbhuang0604/SelfExSR',\n",
       "    'frameworks': ['none']},\n",
       "   {'url': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'repo': 'https://github.com/eugenesiow/super-image-data',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vggface2-1',\n",
       "  'name': 'VGGFace2',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/vgg_face/',\n",
       "  'description': 'The **VGGFace2** dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity. The dataset is divided into two splits, one for the training and one for test. The latter contains around 170000 images divided into 500 identities while all the other images belong to the remaining 8631 classes available for training. While constructing the datasets, the authors focused their efforts on reaching a very low label noise and a high pose and age diversity thus, making the VGGFace2 dataset a suitable choice to train state-of-the-art deep learning models on face-related tasks. The images of the training set have an average resolution of 137x180 pixels, with less than 1% at a resolution below 32 pixels (considering the shortest side).\\r\\n\\r\\n**CAUTION**: Authors note that the distribution of identities in the VGG-Face dataset may not be representative of the global human population. Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.\\r\\n\\r\\nSource: [Cross-Resolution Learning for Face Recognition](https://arxiv.org/abs/1912.02851)',\n",
       "  'paper': {'title': 'VGGFace2: A dataset for recognising faces across pose and age',\n",
       "   'url': 'https://paperswithcode.com/paper/vggface2-a-dataset-for-recognising-faces'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Facial Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/facial-inpainting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VggFace2',\n",
       "   'VGGFace2 (2.3M)',\n",
       "   'VggFace2 - 8x upscaling',\n",
       "   'VGGFace2'],\n",
       "  'num_papers': 332,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/vgg_face2',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal3d-2',\n",
       "  'name': 'PASCAL3D+',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://cvgl.stanford.edu/projects/pascal3d.html',\n",
       "  'description': 'The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.\\r\\n\\r\\nSource: [Convolutional Models for Joint Object Categorization and Pose Estimation](https://arxiv.org/abs/1511.05175)\\r\\nImage Source: [Beyond PASCAL: A benchmark for 3D object detection in the wild](https://doi.org/10.1109/WACV.2014.6836101)',\n",
       "  'paper': {'title': 'Beyond PASCAL: A benchmark for 3D object detection in the wild',\n",
       "   'url': 'https://doi.org/10.1109/WACV.2014.6836101'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Keypoint Detection',\n",
       "    'url': 'https://paperswithcode.com/task/keypoint-detection'},\n",
       "   {'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Viewpoint Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/viewpoint-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': [' Pascal3D+', 'PASCAL3D+'],\n",
       "  'num_papers': 181,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun-rgb-d',\n",
       "  'name': 'SUN RGB-D',\n",
       "  'full_name': 'SUN RGB-D',\n",
       "  'homepage': 'https://rgbd.cs.princeton.edu/',\n",
       "  'description': 'The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively.\\r\\n\\r\\nSource: [Mix and match networks: multi-domain alignment for unpaired image-to-image translation](https://arxiv.org/abs/1903.04294)\\r\\nImage Source: [https://rgbd.cs.princeton.edu/](https://rgbd.cs.princeton.edu/)',\n",
       "  'paper': {'title': 'SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite',\n",
       "   'url': 'https://paperswithcode.com/paper/sun-rgb-d-a-rgb-d-scene-understanding'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Interactive', 'Point cloud', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': '3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-detection'},\n",
       "   {'task': 'Monocular 3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-3d-object-detection'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'},\n",
       "   {'task': 'Scene Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-segmentation'},\n",
       "   {'task': 'Room Layout Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/room-layout-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN-RGBD', 'SUN-RGBD val', 'SUN RGB-D'],\n",
       "  'num_papers': 239,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection3d/blob/master/docs/data_preparation.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection3d',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/suncg',\n",
       "  'name': 'SUNCG',\n",
       "  'full_name': 'SUNCG',\n",
       "  'homepage': 'https://sscnet.cs.princeton.edu/',\n",
       "  'description': '**SUNCG** is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations.\\r\\n\\r\\nThe dataset is currently not available.\\r\\n\\r\\nSource: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)\\r\\nImage Source: [https://sscnet.cs.princeton.edu/](https://sscnet.cs.princeton.edu/)',\n",
       "  'paper': {'title': 'Semantic Scene Completion from a Single Depth Image',\n",
       "   'url': 'https://paperswithcode.com/paper/semantic-scene-completion-from-a-single-depth'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Visual Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/visual-navigation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUNCG'],\n",
       "  'num_papers': 162,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/places205',\n",
       "  'name': 'Places205',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://places.csail.mit.edu/downloadData.html',\n",
       "  'description': 'The **Places205** dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images).\\r\\n\\r\\nSource: [Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs](https://arxiv.org/abs/1610.01119)\\r\\nImage Source: [http://places.csail.mit.edu/browser.html](http://places.csail.mit.edu/browser.html)',\n",
       "  'paper': {'title': 'Learning Deep Features for Scene Recognition using Places Database',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-deep-features-for-scene-recognition'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Places205', 'Places'],\n",
       "  'num_papers': 433,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/places205-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/modelnet',\n",
       "  'name': 'ModelNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://modelnet.cs.princeton.edu/',\n",
       "  'description': 'The **ModelNet**40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere.\\r\\n\\r\\nSource: [Geometric Feedback Network for Point Cloud Classification](https://arxiv.org/abs/1911.12885)',\n",
       "  'paper': {'title': '3D ShapeNets: A Deep Representation for Volumetric Shapes',\n",
       "   'url': 'https://paperswithcode.com/paper/3d-shapenets-a-deep-representation-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['3D', 'Point cloud'],\n",
       "  'tasks': [{'task': '3D Point Cloud Classification',\n",
       "    'url': 'https://paperswithcode.com/task/3d-point-cloud-classification'},\n",
       "   {'task': '3D Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-recognition'},\n",
       "   {'task': '3D Object Classification',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-classification'},\n",
       "   {'task': '3D Point Cloud Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-point-cloud-data-augmentation'},\n",
       "   {'task': '3D Object Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-retrieval'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['ModelNet40', 'ModelNet10', 'ModelNet'],\n",
       "  'num_papers': 757,\n",
       "  'data_loaders': [{'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://graphneural.network/datasets/#modelnet',\n",
       "    'repo': 'https://github.com/danielegrattarola/spektral',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/yago',\n",
       "  'name': 'YAGO',\n",
       "  'full_name': 'Yet Another Great Ontology',\n",
       "  'homepage': 'https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/',\n",
       "  'description': '**Yet Another Great Ontology** (**YAGO**) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each.\\r\\n\\r\\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\\r\\nImage Source: [https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/)',\n",
       "  'paper': {'title': 'Yago: a core of semantic knowledge',\n",
       "   'url': 'https://doi.org/10.1145/1242572.1242667'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'Time-interval Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/time-interval-prediction'},\n",
       "   {'task': 'Triple Classification',\n",
       "    'url': 'https://paperswithcode.com/task/triple-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['YAGO3-10', 'YAGO', 'Yago11k', 'YAGO15k', 'YAGO39K', 'YAGO37'],\n",
       "  'num_papers': 244,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/wider-face-1',\n",
       "  'name': 'WIDER FACE',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://shuoyang1213.me/WIDERFACE/',\n",
       "  'description': 'The **WIDER FACE** dataset contains 32,203 images and labels 393,703 faces with a high degree of variability in scale, pose and occlusion. The database is split into training (40%), validation (10%) and testing (50%) set. Besides, the images are divided into three levels (Easy ⊆ Medium ⊆ Hard) according to the difficulties of the detection. The images and annotations of training and validation set are available online, while the annotations of testing set are not released and the results are sent to the database server for receiving the precision-recall curves.\\r\\n\\r\\nSource: [S{}^{3}FD: Single Shot Scale-invariant Face Detector](https://arxiv.org/abs/1708.05237)',\n",
       "  'paper': {'title': 'WIDER FACE: A Face Detection Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/wider-face-a-face-detection-benchmark'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-detection'}],\n",
       "  'languages': ['Indonesian'],\n",
       "  'variants': ['WIDER Face (Easy)',\n",
       "   'WIDER Face (Medium)',\n",
       "   'WIDER Face (Hard)',\n",
       "   'WIDER FACE'],\n",
       "  'num_papers': 171,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://huggingface.co/datasets/wider_face',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/wider_face',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mpi-sintel',\n",
       "  'name': 'MPI Sintel',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://sintel.is.tue.mpg.de/',\n",
       "  'description': 'MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024×436 pixels and 8-bit per channel.\\r\\n\\r\\nSource: [Fast Disparity Estimation using Dense Networks*](https://arxiv.org/abs/1805.07499)',\n",
       "  'paper': {'title': 'A Naturalistic Open Source Movie for Optical Flow Evaluation',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-33783-3_44'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Stereo'],\n",
       "  'tasks': [{'task': 'Optical Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/optical-flow-estimation'},\n",
       "   {'task': 'Style Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/style-transfer'},\n",
       "   {'task': 'Intrinsic Image Decomposition',\n",
       "    'url': 'https://paperswithcode.com/task/intrinsic-image-decomposition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Sintel-final - unsupervised',\n",
       "   'Sintel-final',\n",
       "   'Sintel-clean',\n",
       "   'Sintel Final unsupervised',\n",
       "   'Sintel Clean unsupervised',\n",
       "   'MPI Sintel'],\n",
       "  'num_papers': 125,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/helen',\n",
       "  'name': 'Helen',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.ifp.illinois.edu/~vuongle2/helen/',\n",
       "  'description': 'The HELEN dataset is composed of 2330 face images of 400×400 pixels with labeled facial components generated through manually-annotated contours along eyes, eyebrows, nose, lips and jawline.\\r\\n\\r\\nSource: [Face Parsing via a Fully-Convolutional Continuous CRF Neural Network](https://arxiv.org/abs/1708.03736)\\r\\nImage Source: [http://www.ifp.illinois.edu/~vuongle2/helen/](http://www.ifp.illinois.edu/~vuongle2/helen/)',\n",
       "  'paper': {'title': 'Interactive Facial Feature Localization',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-33712-3_49'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Helen'],\n",
       "  'num_papers': 170,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/framenet',\n",
       "  'name': 'FrameNet',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://framenet.icsi.berkeley.edu/fndrupal/',\n",
       "  'description': '**FrameNet** is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word.\\r\\n\\r\\nSource: [Retrofitting Distributional Embeddings to Knowledge Graphswith Functional Relations](https://arxiv.org/abs/1708.00112)',\n",
       "  'paper': {'title': 'The Berkeley FrameNet Project',\n",
       "   'url': 'https://www.aclweb.org/anthology/P98-1013/'},\n",
       "  'introduced_date': '1998-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Graphs'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Semantic Role Labeling',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-role-labeling'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['FrameNet'],\n",
       "  'num_papers': 376,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lsun',\n",
       "  'name': 'LSUN',\n",
       "  'full_name': 'Large-scale Scene UNderstanding Challenge',\n",
       "  'homepage': 'https://www.yf.io/p/lsun',\n",
       "  'description': 'The Large-scale Scene Understanding (**LSUN**) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category.\\r\\n\\r\\nSource: [Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs](https://arxiv.org/abs/1610.01119)\\r\\nImage Source: [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)',\n",
       "  'paper': {'title': 'LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop',\n",
       "   'url': 'https://paperswithcode.com/paper/lsun-construction-of-a-large-scale-image'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LSUN Roon Layout',\n",
       "   'LSUN Bedroom 128 x 128',\n",
       "   'LSUN Bedroom',\n",
       "   'LSUN',\n",
       "   'LSUN Horse 256 x 256',\n",
       "   'LSUN Churches 256 x 256',\n",
       "   'LSUN Cat 256 x 256',\n",
       "   'LSUN Car 512 x 384',\n",
       "   'LSUN Car 256 x 256',\n",
       "   'LSUN Bedroom 64 x 64',\n",
       "   'LSUN Bedroom 256 x 256'],\n",
       "  'num_papers': 450,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.LSUN',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/lsun',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lfpw',\n",
       "  'name': 'LFPW',\n",
       "  'full_name': 'Labeled Face Parts in the Wild',\n",
       "  'homepage': 'https://neerajkumar.org/databases/lfpw/',\n",
       "  'description': 'The **Labeled Face Parts in-the-Wild** (**LFPW**) consists of 1,432 faces from images downloaded from the web using simple text queries on sites such as google.com, flickr.com, and yahoo.com.   Each image was labeled by three MTurk workers, and 29 fiducial points, shown below, are included in dataset.\\r\\n\\r\\nSource: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)\\r\\nImage Source: [https://neerajkumar.org/databases/lfpw/](https://neerajkumar.org/databases/lfpw/)',\n",
       "  'paper': {'title': 'Localizing Parts of Faces Using a Consensus of Exemplars',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2011.5995602'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Face Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/face-alignment'},\n",
       "   {'task': 'Facial Landmark Detection',\n",
       "    'url': 'https://paperswithcode.com/task/facial-landmark-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LFPW'],\n",
       "  'num_papers': 119,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/carla',\n",
       "  'name': 'CARLA',\n",
       "  'full_name': 'Car Learning to Act',\n",
       "  'homepage': 'https://carla.org/',\n",
       "  'description': '**CARLA** (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation).\\r\\n\\r\\nSource: [Synthetic Data for Deep Learning](https://arxiv.org/abs/1909.11512)',\n",
       "  'paper': {'title': 'CARLA: An Open Urban Driving Simulator',\n",
       "   'url': 'https://paperswithcode.com/paper/carla-an-open-urban-driving-simulator'},\n",
       "  'introduced_date': '2017-11-10',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Autonomous Vehicles',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-vehicles'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CARLA', 'CARLA Leaderboard'],\n",
       "  'num_papers': 462,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/otb',\n",
       "  'name': 'OTB',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html',\n",
       "  'description': 'Object Tracking Benchmark (**OTB**) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. [OTB-2013](otb-2013) dataset contains 51 sequences and the [OTB-2015](otb-2015) dataset contains all 100 sequences of the OTB dataset.\\r\\n\\r\\nSource: [Deep Meta Learning for Real-Time Target-Aware Visual Tracking](https://arxiv.org/abs/1712.09153)',\n",
       "  'paper': {'title': 'Object Tracking Benchmark',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2014.2388226'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Tracking'],\n",
       "  'tasks': [{'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OTB-2015', 'OTB-2013', 'OTB-50', 'OTB'],\n",
       "  'num_papers': 324,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/places365',\n",
       "  'name': 'Places365',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://places2.csail.mit.edu/',\n",
       "  'description': 'The **Places365** dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes).\\r\\n\\r\\nSource: [Semantic-Aware Scene Recognition](https://arxiv.org/abs/1909.02410)\\r\\nImage Source: [Places](http://places2.csail.mit.edu/index.html)',\n",
       "  'paper': {'title': 'Semantic-Aware Scene Recognition',\n",
       "   'url': 'https://paperswithcode.com/paper/semantic-aware-scene-recognition'},\n",
       "  'introduced_date': '2019-09-05',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'},\n",
       "   {'task': 'Scene Classification',\n",
       "    'url': 'https://paperswithcode.com/task/scene-classification'},\n",
       "   {'task': 'Image Outpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-outpainting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Places365-Standard', 'Places365'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': [{'url': 'https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.Places365',\n",
       "    'repo': 'https://github.com/pytorch/vision',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/places365_small',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/extended-yale-b-1',\n",
       "  'name': 'Extended Yale B',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html',\n",
       "  'description': 'The **Extended Yale B** database contains 2414 frontal-face images with size 192×168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions.\\r\\n\\r\\nSource: [Learning Locality-Constrained Collaborative Representation for Robust Face Recognition](https://arxiv.org/abs/1210.1316)\\r\\nImage Source: [http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)',\n",
       "  'paper': {'title': 'From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose',\n",
       "   'url': 'https://doi.org/10.1109/34.927464'},\n",
       "  'introduced_date': '2001-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/image-clustering'},\n",
       "   {'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Dictionary Learning',\n",
       "    'url': 'https://paperswithcode.com/task/dictionary-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Extended Yale-B', 'Extended Yale B'],\n",
       "  'num_papers': 171,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imdb-movie-reviews',\n",
       "  'name': 'IMDb Movie Reviews',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "  'description': 'The **IMDb Movie Reviews** dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data.\\r\\n\\r\\nSource: [http://nlpprogress.com/english/sentiment_analysis.html](http://nlpprogress.com/english/sentiment_analysis.html)\\r\\nImage Source: [Maas et al](https://www.aclweb.org/anthology/P11-1015/)',\n",
       "  'paper': {'title': 'Learning Word Vectors for Sentiment Analysis',\n",
       "   'url': 'https://www.aclweb.org/anthology/P11-1015/'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Tabular'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Sentiment Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/sentiment-analysis'},\n",
       "   {'task': 'Link Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/link-prediction'},\n",
       "   {'task': 'SQL Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/sql-parsing'},\n",
       "   {'task': 'Node Clustering',\n",
       "    'url': 'https://paperswithcode.com/task/node-clustering'},\n",
       "   {'task': 'Graph Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/graph-similarity'}],\n",
       "  'languages': [],\n",
       "  'variants': ['IMDb', 'IMDb Movie Reviews', 'User and product information'],\n",
       "  'num_papers': 923,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/imdb',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/imdb_reviews',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://pytorch.org/text/stable/datasets.html#torchtext.datasets.IMDB',\n",
       "    'repo': 'https://github.com/pytorch/text',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bookcorpus',\n",
       "  'name': 'BookCorpus',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/soskek/bookcorpus',\n",
       "  'description': '**BookCorpus** is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).\\r\\n\\r\\nSource: [Temporal Event Knowledge Acquisition via Identifying Narratives](https://arxiv.org/abs/1805.10956)',\n",
       "  'paper': {'title': 'Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books',\n",
       "   'url': 'https://paperswithcode.com/paper/aligning-books-and-movies-towards-story-like'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BookCorpus'],\n",
       "  'num_papers': 190,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/bookcorpus',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://huggingface.co/datasets/bookcorpusopen',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/soskek/bookcorpus',\n",
       "    'repo': 'https://github.com/soskek/bookcorpus',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/facewarehouse',\n",
       "  'name': 'FaceWarehouse',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://kunzhou.net/zjugaps/facewarehouse/',\n",
       "  'description': '**FaceWarehouse** is a 3D facial expression database that provides the facial geometry of 150 subjects, covering a wide range of ages and ethnic backgrounds.\\r\\n\\r\\nSource: [3D Face Reconstruction with Geometry Details from a Single Image](https://arxiv.org/abs/1702.05619)',\n",
       "  'paper': {'title': 'FaceWarehouse: A 3D Facial Expression Database for Visual Computing',\n",
       "   'url': 'https://doi.org/10.1109/TVCG.2013.249'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': '3D Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-reconstruction'},\n",
       "   {'task': 'Face Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/face-reconstruction'},\n",
       "   {'task': 'Face Model',\n",
       "    'url': 'https://paperswithcode.com/task/face-model'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FaceWarehouse'],\n",
       "  'num_papers': 71,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/lsp',\n",
       "  'name': 'LSP',\n",
       "  'full_name': 'Leeds Sports Pose',\n",
       "  'homepage': 'https://dbcollection.readthedocs.io/en/latest/datasets/leeds_sports_pose_extended.html',\n",
       "  'description': 'The **Leeds Sports Pose** (**LSP**) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training.\\r\\n\\r\\nSource: [Deep Deformation Network for Object Landmark Localization](https://arxiv.org/abs/1605.01014)\\r\\n\\r\\nImage: [Sumer et al](https://www.researchgate.net/figure/Pose-estimation-results-in-Leeds-Sports-Pose-dataset-First-images-are-from-test-set-with_fig3_322058596)',\n",
       "  'paper': {'title': 'Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation',\n",
       "   'url': 'https://doi.org/10.5244/C.24.12'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'},\n",
       "   {'task': '3D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Leeds Sports Poses', ' Leeds Sports Pose', 'LSP'],\n",
       "  'num_papers': 160,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmpose/blob/master/docs/tasks/3d_body_mesh.md#lsp',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/linemod-1',\n",
       "  'name': 'LINEMOD',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://bop.felk.cvut.cz/datasets/',\n",
       "  'description': '**LINEMOD** is an RGB+D dataset, which has become a de facto standard benchmark for 6D pose estimation. The dataset contains poorly textured objects in a cluttered scene. The dataset contains 15 object sequences. The images in each object sequence contain multiple objects, however, only one object is annotated with the ground-truth class label, bounding box, and 6D pose. The camera intrinsic matrix is also provided with the dataset.\\r\\n\\r\\nSource: [Deep-6DPose: Recovering 6D Object Pose from a Single RGB Image](https://arxiv.org/abs/1802.10367)',\n",
       "  'paper': {'title': 'Model Based Training, Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-37331-2_42'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': '6D Pose Estimation using RGB',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation'},\n",
       "   {'task': '6D Pose Estimation using RGBD',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-using-rgbd'},\n",
       "   {'task': '6D Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['LineMOD',\n",
       "   'Occlusion LineMOD',\n",
       "   'Synth Objects-to-LINEMOD',\n",
       "   'LINEMOD'],\n",
       "  'num_papers': 145,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kth',\n",
       "  'name': 'KTH',\n",
       "  'full_name': 'KTH Action dataset',\n",
       "  'homepage': 'https://www.csc.kth.se/cvap/actions/',\n",
       "  'description': 'The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the **KTH** Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors.\\r\\n\\r\\nSource: [Review of Action Recognition and Detection Methods](https://arxiv.org/abs/1610.06906)',\n",
       "  'paper': {'title': 'Recognizing Human Actions: A Local SVM Approach',\n",
       "   'url': 'https://doi.org/10.1109/ICPR.2004.1334462'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Video Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/video-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KTH'],\n",
       "  'num_papers': 196,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/places',\n",
       "  'name': 'Places',\n",
       "  'full_name': 'Places',\n",
       "  'homepage': 'http://places.csail.mit.edu/',\n",
       "  'description': 'The **Places** dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category.\\r\\n\\r\\nSource: [Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey](https://arxiv.org/abs/1902.06162)\\r\\nImage Source: [http://places.csail.mit.edu/browser.html](http://places.csail.mit.edu/browser.html)',\n",
       "  'paper': {'title': 'Places: A 10 Million Image Database for Scene Recognition',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2017.2723009'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Inpainting',\n",
       "    'url': 'https://paperswithcode.com/task/image-inpainting'},\n",
       "   {'task': 'Scene Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-recognition'},\n",
       "   {'task': 'Uncropping',\n",
       "    'url': 'https://paperswithcode.com/task/uncropping'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Places2', 'Places2 val', 'Places205', 'Places'],\n",
       "  'num_papers': 558,\n",
       "  'data_loaders': [{'url': 'https://docs.activeloop.ai/datasets/places205-dataset',\n",
       "    'repo': 'https://github.com/activeloopai/Hub',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mocap',\n",
       "  'name': 'MoCap',\n",
       "  'full_name': 'CMU Graphics Lab Motion Capture Database',\n",
       "  'homepage': 'http://mocap.cs.cmu.edu/',\n",
       "  'description': \"Collection of various motion capture recordings (walking, dancing, sports, and others) performed by over 140 subjects. The database contains free motions which you can download and use. There is a zip file of all asf/amc's on the FAQs page.\\n\\nSource: [https://www.re3data.org/repository/r3d100012183](https://www.re3data.org/repository/r3d100012183)\",\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['MoCap'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kit-whole-body-human-motion',\n",
       "  'name': 'KIT Whole-Body Human Motion',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://motion-database.humanoids.kit.edu/',\n",
       "  'description': 'The **KIT Whole-Body Human Motion** Database is a large-scale dataset of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. Captured subject-specific motion is normalized regarding the subject’s height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in this database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, See the paper for procedures and techniques used for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject–object relations.\\r\\n\\r\\nSource: [https://motion-database.humanoids.kit.edu/](https://motion-database.humanoids.kit.edu/)\\r\\nImage Source: [https://motion-database.humanoids.kit.edu/](https://motion-database.humanoids.kit.edu/)',\n",
       "  'paper': {'title': 'The KIT whole-body human motion database',\n",
       "   'url': 'https://doi.org/10.1109/ICAR.2015.7251476'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Multi-Label Classification',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-classification'},\n",
       "   {'task': 'Robotic Grasping',\n",
       "    'url': 'https://paperswithcode.com/task/robotic-grasping'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KIT Whole-Body Human Motion'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/meta-dataset',\n",
       "  'name': 'Meta-Dataset',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://github.com/google-research/meta-dataset',\n",
       "  'description': 'The **Meta-Dataset** benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: \\r\\n\\r\\n* ILSVRC-2012 (the ImageNet dataset, consisting of natural images with 1000 categories)\\r\\n* Omniglot (hand-written characters, 1623 classes)\\r\\n* Aircraft (dataset of aircraft images, 100 classes)\\r\\n* CUB-200-2011 (dataset of Birds, 200 classes)\\r\\n* Describable Textures (different kinds of texture images with 43 categories)\\r\\n* Quick Draw (black and white sketches of 345 different categories)\\r\\n* Fungi (a large dataset of mushrooms with 1500 categories)\\r\\n* VGG Flower (dataset of flower images with 102 categories), \\r\\n* Traffic Signs (German traffic sign images with 43 classes)\\r\\n* MSCOCO (images collected from Flickr, 80 classes). \\r\\n\\r\\nAll datasets except Traffic signs and MSCOCO have a training, validation and test split (proportioned roughly into 70%, 15%, 15%). The datasets Traffic Signs and MSCOCO are reserved for testing only.\\r\\n\\r\\nSource: [Optimized Generic Feature Learning for Few-shot Classification across Domains](https://arxiv.org/abs/2001.07926)\\r\\nImage Source: [Triantafillou et al](https://arxiv.org/pdf/1903.03096.pdf)',\n",
       "  'paper': {'title': 'Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples',\n",
       "   'url': 'https://paperswithcode.com/paper/meta-dataset-a-dataset-of-datasets-for'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Meta-Learning',\n",
       "    'url': 'https://paperswithcode.com/task/meta-learning'},\n",
       "   {'task': 'Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Meta-Dataset', 'Meta-Dataset Rank'],\n",
       "  'num_papers': 58,\n",
       "  'data_loaders': [{'url': 'https://github.com/google-research/meta-dataset',\n",
       "    'repo': 'https://github.com/google-research/meta-dataset',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/usf',\n",
       "  'name': 'USF',\n",
       "  'full_name': 'Human ID Gait Challenge Dataset',\n",
       "  'homepage': None,\n",
       "  'description': 'The **USF** **Human ID Gait Challenge Dataset** is a dataset of videos for gait recognition. It has videos from 122 subjects in up to 32 possible combinations of variations in factors.\\n\\nSource: [http://www.eng.usf.edu/cvprg/Gait_Data.html](http://www.eng.usf.edu/cvprg/Gait_Data.html)',\n",
       "  'paper': {'title': 'The HumanID Gait Challenge Problem: Data Sets, Performance, and Analysis',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2005.39'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Age Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/age-estimation'},\n",
       "   {'task': 'Gait Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/gait-recognition'},\n",
       "   {'task': 'Rectification',\n",
       "    'url': 'https://paperswithcode.com/task/rectification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['USF'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/birdsong',\n",
       "  'name': 'BirdSong',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': 'The **BirdSong** dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings.',\n",
       "  'paper': {'title': 'Rank-loss support instance machines for MIML instance annotation',\n",
       "   'url': 'https://doi.org/10.1145/2339530.2339616'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/denoising'},\n",
       "   {'task': 'Graph Matching',\n",
       "    'url': 'https://paperswithcode.com/task/graph-matching'},\n",
       "   {'task': 'Multi-Label Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-label-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BirdSong'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/oxford5k',\n",
       "  'name': 'Oxford5k',\n",
       "  'full_name': 'Oxford Buildings',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/',\n",
       "  'description': 'Oxford5K is the **Oxford Buildings** Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark.\\r\\n\\r\\nSource: [Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN 1 Corresponding Author](https://arxiv.org/abs/1907.05793)\\r\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/](https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/)',\n",
       "  'paper': {'title': 'Object retrieval with large vocabularies and fast spatial matching',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2007.383172'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Content-Based Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/content-based-image-retrieval'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Oxford5k'],\n",
       "  'num_papers': 126,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cbsd68',\n",
       "  'name': 'CBSD68',\n",
       "  'full_name': 'Color BSD68',\n",
       "  'homepage': 'https://github.com/clausmichele/CBSD68-dataset',\n",
       "  'description': '**Color BSD68** dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images.\\r\\n\\r\\nSource: [https://github.com/clausmichele/CBSD68-dataset](https://github.com/clausmichele/CBSD68-dataset)',\n",
       "  'paper': {'title': 'A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics',\n",
       "   'url': 'http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=937655'},\n",
       "  'introduced_date': '2001-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/denoising'},\n",
       "   {'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Image Restoration',\n",
       "    'url': 'https://paperswithcode.com/task/image-restoration'},\n",
       "   {'task': 'Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/image-denoising'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CBSD68 sigma70',\n",
       "   'CBSD68 sigma65',\n",
       "   'CBSD68 sigma60',\n",
       "   'CBSD68 sigma55',\n",
       "   'CBSD68 sigma5',\n",
       "   'CBSD68 sigma45',\n",
       "   'CBSD68 sigma40',\n",
       "   'CBSD68 sigma30',\n",
       "   'CBSD68 sigma20',\n",
       "   'CBSD68 sigma10',\n",
       "   'CBSD68 sigma75',\n",
       "   'CBSD68 sigma50',\n",
       "   'CBSD68 sigma35',\n",
       "   'CBSD68 sigma25',\n",
       "   'CBSD68 sigma15',\n",
       "   'CBSD68'],\n",
       "  'num_papers': 48,\n",
       "  'data_loaders': [{'url': 'https://github.com/clausmichele/CBSD68-dataset',\n",
       "    'repo': 'https://github.com/clausmichele/CBSD68-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/scribblesup',\n",
       "  'name': 'ScribbleSup',\n",
       "  'full_name': 'PASCAL-Scribble Dataset',\n",
       "  'homepage': 'https://jifengdai.org/downloads/scribble_sup/',\n",
       "  'description': 'The **PASCAL-Scribble Dataset** is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set.\\r\\nIn the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated.\\r\\n\\r\\nSource: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)\\r\\nImage Source: [https://jifengdai.org/downloads/scribble_sup/](https://jifengdai.org/downloads/scribble_sup/)',\n",
       "  'paper': {'title': 'ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation',\n",
       "   'url': 'https://paperswithcode.com/paper/scribblesup-scribble-supervised-convolutional'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Weakly-Supervised Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ScribbleSup'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-background',\n",
       "  'name': 'Stanford Background',\n",
       "  'full_name': 'Standford Background Dataset',\n",
       "  'homepage': 'http://dags.stanford.edu/projects/scenedataset.html',\n",
       "  'description': 'The **Stanford Background** dataset contains 715 RGB images and the corresponding label images. Images are approximately 240×320 pixels in size and pixels are classified into eight different categories\\r\\n\\r\\nSource: [Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation](https://arxiv.org/abs/1605.01368)\\r\\nImage Source: [http://dags.stanford.edu/projects/scenedataset.html](http://dags.stanford.edu/projects/scenedataset.html)',\n",
       "  'paper': {'title': 'Decomposing a scene into geometric and semantically consistent regions',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2009.5459211'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Scene Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/scene-parsing'},\n",
       "   {'task': 'Scene Labeling',\n",
       "    'url': 'https://paperswithcode.com/task/scene-labeling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford Background'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/new-college',\n",
       "  'name': 'New College',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~mobile/MOOS/wiki/pmwiki.php',\n",
       "  'description': 'The **New College** Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore.\\r\\n\\r\\nSource: [https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html](https://www.ros.org/news/2010/07/new-college-dataset-parser-for-ros.html)',\n",
       "  'paper': {'title': 'The New College Vision and Laser Data Set',\n",
       "   'url': 'https://doi.org/10.1177/0278364909103911'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Visual Odometry',\n",
       "    'url': 'https://paperswithcode.com/task/visual-odometry'},\n",
       "   {'task': 'Loop Closure Detection',\n",
       "    'url': 'https://paperswithcode.com/task/loop-closure-detection'},\n",
       "   {'task': 'Simultaneous Localization and Mapping',\n",
       "    'url': 'https://paperswithcode.com/task/simultaneous-localization-and-mapping'}],\n",
       "  'languages': [],\n",
       "  'variants': ['New College'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/malf',\n",
       "  'name': 'MALF',\n",
       "  'full_name': 'Multi-Attribute Labelled Faces',\n",
       "  'homepage': 'http://www.cbsr.ia.ac.cn/faceevaluation/',\n",
       "  'description': 'The **MALF** dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation.\\n\\nSource: [Pushing the Limits of Unconstrained Face Detection:a Challenge Dataset and Baseline Results](https://arxiv.org/abs/1804.10275)\\nImage Source: [http://www.cbsr.ia.ac.cn/faceevaluation/](http://www.cbsr.ia.ac.cn/faceevaluation/)',\n",
       "  'paper': {'title': 'Fine-grained evaluation on face detection in the wild',\n",
       "   'url': 'https://doi.org/10.1109/FG.2015.7163158'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Face Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-detection'},\n",
       "   {'task': 'Robust Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/robust-face-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MALF'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/oxford-affine',\n",
       "  'name': 'Oxford-Affine',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/affine/',\n",
       "  'description': 'The **Oxford-Affine** dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies.\\n\\nSource: [A Large Dataset for Improving Patch Matching](https://arxiv.org/abs/1801.01466)\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/data/affine/](https://www.robots.ox.ac.uk/~vgg/data/affine/)',\n",
       "  'paper': {'title': 'A Comparison of Affine Region Detectors',\n",
       "   'url': 'https://doi.org/10.1007/s11263-005-3848-x'},\n",
       "  'introduced_date': '2005-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': '3D Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/3d-reconstruction'},\n",
       "   {'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Oxford-Affine'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/oxford105k',\n",
       "  'name': 'Oxford105k',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/',\n",
       "  'description': '**Oxford105k** is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale.\\r\\n\\r\\nSource: [Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors Extended Version](https://arxiv.org/abs/1504.03285)',\n",
       "  'paper': {'title': 'Object retrieval with large vocabularies and fast spatial matching',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2007.383172'},\n",
       "  'introduced_date': '2007-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Instance Search',\n",
       "    'url': 'https://paperswithcode.com/task/instance-search'},\n",
       "   {'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Oxford105k'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/dispscenes',\n",
       "  'name': 'DispScenes',\n",
       "  'full_name': None,\n",
       "  'homepage': None,\n",
       "  'description': 'The **DispScenes** dataset was created to address the specific problem of disparate image matching. The image pairs in all the datasets exhibit high levels of variation in illumination and viewpoint and also contain instances of occlusion. The DispScenes dataset provides manual ground truth keypoint correspondences for all images.\\n\\nSource: [Matching Disparate Image Pairs Using Shape-Aware ConvNets](https://arxiv.org/abs/1811.09889)',\n",
       "  'paper': {'title': 'Deep Spectral Correspondence for Matching Disparate Image Pairs',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-spectral-correspondence-for-matching'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Graph Matching',\n",
       "    'url': 'https://paperswithcode.com/task/graph-matching'},\n",
       "   {'task': 'Matching Disparate Images',\n",
       "    'url': 'https://paperswithcode.com/task/matching-disparate-images'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DispScenes'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/retrieval-sfm',\n",
       "  'name': 'Retrieval-SfM',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://cmp.felk.cvut.cz/cnnimageretrieval/',\n",
       "  'description': 'The Retrieval-SFM dataset is used for instance image retrieval. The dataset contains 28559 images from 713 locations in the world. Each image has a label indicating the location it belongs to. Most locations are famous man-made architectures such as palaces and towers, which are relatively static and positively contribute to visual place recognition. The training dataset contains various perceptual changes including variations in viewing angles, occlusions and illumination conditions, etc.\\n\\nSource: [Localizing Discriminative Visual Landmarks for Place Recognition](https://arxiv.org/abs/1904.06635)',\n",
       "  'paper': {'title': 'CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples',\n",
       "   'url': 'https://paperswithcode.com/paper/cnn-image-retrieval-learns-from-bow'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Visual Place Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/visual-place-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Retrieval-SfM'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/vgg-cell',\n",
       "  'name': 'VGG Cell',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html',\n",
       "  'description': 'The **VGG Cell** dataset (made up entirely of synthetic images) is the main public benchmark used to compare cell counting techniques.\\n\\nSource: [People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting](https://arxiv.org/abs/1711.05586)\\nImage Source: [https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html](https://www.robots.ox.ac.uk/~vgg/research/counting/index_org.html)',\n",
       "  'paper': {'title': 'Microscopy cell counting and detection with fully convolutional regression networks',\n",
       "   'url': 'https://doi.org/10.1080/21681163.2016.1149104'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Medical'],\n",
       "  'tasks': [{'task': 'Object Localization',\n",
       "    'url': 'https://paperswithcode.com/task/object-localization'},\n",
       "   {'task': 'Object Counting',\n",
       "    'url': 'https://paperswithcode.com/task/object-counting'}],\n",
       "  'languages': [],\n",
       "  'variants': ['VGG Cell'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tiny-images',\n",
       "  'name': 'Tiny Images',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://groups.csail.mit.edu/vision/TinyImages/',\n",
       "  'description': 'The image dataset TinyImages contains 80 million images of size 32×32 collected from the Internet, crawling the words in WordNet. \\r\\n\\r\\n**The authors have decided to withdraw it because it contains offensive content, and have asked the community to stop using it.**\\r\\n\\r\\nSource: [Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments](https://arxiv.org/abs/1502.03032)',\n",
       "  'paper': {'title': '80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2008.128'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Image Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/image-retrieval'},\n",
       "   {'task': 'Quantization',\n",
       "    'url': 'https://paperswithcode.com/task/quantization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Tiny Images'],\n",
       "  'num_papers': 86,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/permuted-mnist',\n",
       "  'name': 'Permuted MNIST',\n",
       "  'full_name': '',\n",
       "  'homepage': '',\n",
       "  'description': '**Permuted MNIST** is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels.\\r\\n\\r\\nSource: [Lifelong Learning with Dynamically Expandable Networks](https://arxiv.org/abs/1708.01547)\\r\\nImage Source: [https://arxiv.org/pdf/1810.12488.pdf](https://arxiv.org/pdf/1810.12488.pdf)',\n",
       "  'paper': {'title': 'An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks',\n",
       "   'url': 'https://paperswithcode.com/paper/an-empirical-investigation-of-catastrophic'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/continual-learning'},\n",
       "   {'task': 'Incremental Learning',\n",
       "    'url': 'https://paperswithcode.com/task/incremental-learning'},\n",
       "   {'task': 'Domain-IL Continual Learning',\n",
       "    'url': 'https://paperswithcode.com/task/domain-il-continual-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Permuted MNIST'],\n",
       "  'num_papers': 86,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mnist-8m',\n",
       "  'name': 'MNIST-8M',\n",
       "  'full_name': 'Infinite MNIST',\n",
       "  'homepage': 'https://leon.bottou.org/projects/infimnist',\n",
       "  'description': 'MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset.\\r\\n\\r\\nSource: [Scalable and Sustainable Deep Learningvia Randomized Hashing](https://arxiv.org/abs/1602.08194)',\n",
       "  'paper': {'title': 'Training invariant support vector machines using selective sampling',\n",
       "   'url': 'http://leon.bottou.org/papers/loosli-canu-bottou-2006'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Active Learning',\n",
       "    'url': 'https://paperswithcode.com/task/active-learning'},\n",
       "   {'task': 'Distributed Computing',\n",
       "    'url': 'https://paperswithcode.com/task/distributed-computing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MNIST-8M'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun3d',\n",
       "  'name': 'SUN3D',\n",
       "  'full_name': 'SUN3D',\n",
       "  'homepage': 'http://sun3d.cs.princeton.edu/',\n",
       "  'description': '**SUN3D** contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day.\\r\\n\\r\\nSource: [A Review on Deep Learning TechniquesApplied to Semantic Segmentation](https://arxiv.org/abs/1704.06857)\\r\\nImage Source: [http://sun3d.cs.princeton.edu/](http://sun3d.cs.princeton.edu/)',\n",
       "  'paper': {'title': 'SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2013.458'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', '3D', 'Point cloud', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN3D'],\n",
       "  'num_papers': 78,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tum-rgb-d',\n",
       "  'name': 'TUM RGB-D',\n",
       "  'full_name': 'TUM RGB-D',\n",
       "  'homepage': 'https://vision.in.tum.de/data/datasets/rgbd-dataset',\n",
       "  'description': '**TUM RGB-D** is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz).\\r\\n\\r\\nSource: [https://vision.in.tum.de/data/datasets/rgbd-dataset](https://vision.in.tum.de/data/datasets/rgbd-dataset)\\r\\nImage Source: [https://vision.in.tum.de/research/rgb-d_sensors_kinect](https://vision.in.tum.de/research/rgb-d_sensors_kinect)',\n",
       "  'paper': {'title': 'A benchmark for the evaluation of RGB-D SLAM systems',\n",
       "   'url': 'https://doi.org/10.1109/IROS.2012.6385773'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Visual Odometry',\n",
       "    'url': 'https://paperswithcode.com/task/visual-odometry'},\n",
       "   {'task': 'Simultaneous Localization and Mapping',\n",
       "    'url': 'https://paperswithcode.com/task/simultaneous-localization-and-mapping'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TUM RGB-D'],\n",
       "  'num_papers': 110,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scenenet',\n",
       "  'name': 'SceneNet',\n",
       "  'full_name': 'SceneNet',\n",
       "  'homepage': 'https://robotvault.bitbucket.io/',\n",
       "  'description': '**SceneNet** is a dataset of labelled synthetic indoor scenes. There are several labeled indoor scenes, including:\\r\\n\\r\\n- 11 Bedroom scenes with 428 objects\\r\\n- 15 Office scenes with 1,203 objects\\r\\n- 11 Kitchen scenes with 797 objects\\r\\n- 10 Living Room scenes with 715 objects\\r\\n- 10 Bathrooms with 556 objects\\r\\n\\r\\nSource: [https://robotvault.bitbucket.io/](https://robotvault.bitbucket.io/)\\r\\nImage Source: [https://robotvault.bitbucket.io/big_scene.html](https://robotvault.bitbucket.io/big_scene.html)',\n",
       "  'paper': {'title': 'SceneNet: Understanding Real World Indoor Scenes With Synthetic Data',\n",
       "   'url': 'https://paperswithcode.com/paper/scenenet-understanding-real-world-indoor'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '3D'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SceneNet'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/scenenet-rgb-d',\n",
       "  'name': 'SceneNet RGB-D',\n",
       "  'full_name': 'SceneNet RGB-D',\n",
       "  'homepage': 'https://robotvault.bitbucket.io/scenenet-rgbd.html',\n",
       "  'description': 'SceneNet-RGBD is a synthetic dataset containing large-scale photorealistic renderings of indoor scene trajectories with pixel-level annotations. Random sampling permits virtually unlimited scene configurations, and the dataset creators provide a set of 5M rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses. Each layout also has random lighting, camera trajectories, and textures. The scale of this dataset is well suited for pre-training data-driven computer vision techniques from scratch with RGB-D inputs, which previously has been limited by relatively small labelled datasets in NYUv2 and SUN RGB-D. It also provides a basis for investigating 3D scene labelling tasks by providing perfect camera poses and depth data as proxy for a SLAM system.\\n\\nSource: [ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation](https://arxiv.org/abs/1911.11789)\\nImage Source: [https://robotvault.bitbucket.io/scenenet-rgbd.html](https://robotvault.bitbucket.io/scenenet-rgbd.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['SceneNet RGB-D'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sun-attribute',\n",
       "  'name': 'SUN Attribute',\n",
       "  'full_name': 'SUN Attribute',\n",
       "  'homepage': 'https://cs.brown.edu/~gmpatter/sunattributes.html',\n",
       "  'description': 'The **SUN Attribute** dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition.\\r\\n\\r\\nSource: [Zero-Shot Learning with Multi-Battery Factor Analysis](https://arxiv.org/abs/1606.09349)\\r\\nImage Source: [https://cs.brown.edu/~gmpatter/sunattributes.html](https://cs.brown.edu/~gmpatter/sunattributes.html)',\n",
       "  'paper': {'title': 'The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding',\n",
       "   'url': 'https://doi.org/10.1007/s11263-013-0695-z'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SUN Attribute'],\n",
       "  'num_papers': 27,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/isun',\n",
       "  'name': 'iSUN',\n",
       "  'full_name': 'iSUN',\n",
       "  'homepage': 'http://turkergaze.cs.princeton.edu/',\n",
       "  'description': '**iSUN** is a ground truth of gaze traces on images from the SUN dataset. The collection is partitioned into 6,000 images for training, 926 for validation and 2,000 for test.\\r\\n\\r\\nSource: [End-to-end Convolutional Network for Saliency Prediction](https://arxiv.org/abs/1507.01422)\\r\\nImage Source: [http://turkergaze.cs.princeton.edu/](http://turkergaze.cs.princeton.edu/)',\n",
       "  'paper': {'title': 'TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking',\n",
       "   'url': 'https://paperswithcode.com/paper/turkergaze-crowdsourcing-saliency-with-webcam'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Out-of-Distribution Detection',\n",
       "    'url': 'https://paperswithcode.com/task/out-of-distribution-detection'},\n",
       "   {'task': 'Saliency Detection',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-detection'},\n",
       "   {'task': 'Saliency Prediction',\n",
       "    'url': 'https://paperswithcode.com/task/saliency-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['iSUN'],\n",
       "  'num_papers': 37,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/bms-26',\n",
       "  'name': 'BMS-26',\n",
       "  'full_name': 'Berkeley Motion Segmentation',\n",
       "  'homepage': 'https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html',\n",
       "  'description': 'The  **Berkeley Motion Segmentation** Dataset (**BMS-26**) is a dataset for motion segmentation, which consists of 26 video sequences with pixel-accurate segmentation annotation of moving objects. A total of 189 frames is annotated. 12 of the sequences are taken from the Hopkins 155 dataset and new annotation is added.\\n\\nSource: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)\\nImage Source: [https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html](https://lmb.informatik.uni-freiburg.de/resources/datasets/moseg.en.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['BMS-26'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-groceries',\n",
       "  'name': 'Freiburg Groceries',\n",
       "  'full_name': 'Freiburg Groceries',\n",
       "  'homepage': 'http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/',\n",
       "  'description': '**Freiburg Groceries** is a groceries classification dataset consisting of 5000 images of size 256x256, divided into 25 categories. It has imbalanced class sizes ranging from 97 to 370 images per class. Images were taken in various aspect ratios and padded to squares.\\n\\nSource: [XNAS: Neural Architecture Search with Expert Advice](https://arxiv.org/abs/1906.08031)\\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_groceries_dataset/)',\n",
       "  'paper': {'title': 'The Freiburg Groceries Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/the-freiburg-groceries-dataset'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Neural Architecture Search',\n",
       "    'url': 'https://paperswithcode.com/task/architecture-search'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Groceries'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-spatial-relations',\n",
       "  'name': 'Freiburg Spatial Relations',\n",
       "  'full_name': 'Freiburg Spatial Relations',\n",
       "  'homepage': 'http://spatialrelations.cs.uni-freiburg.de/',\n",
       "  'description': 'The **Freiburg Spatial Relations** dataset features 546 scenes each containing two out of 25 household objects. The depicted spatial relations can roughly be described as on top, on top on the corner, inside, inside and inclined, next to, and inclined. The dataset contains the 25 object models as textured .obj and .dae files, a low resolution .dae version for visualization in rviz, a scene description file containing the translation and rotation of the objects for each scene, a file with labels for each scene, the 15 splits used for cross validation, and a bash script to convert the models to pointclouds.\\n\\nSource: [http://spatialrelations.cs.uni-freiburg.de/](http://spatialrelations.cs.uni-freiburg.de/)\\nImage Source: [http://spatialrelations.cs.uni-freiburg.de/](http://spatialrelations.cs.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'Metric Learning for Generalizing Spatial Relations to New Objects',\n",
       "   'url': 'https://paperswithcode.com/paper/metric-learning-for-generalizing-spatial'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Point cloud'],\n",
       "  'tasks': [{'task': 'Metric Learning',\n",
       "    'url': 'https://paperswithcode.com/task/metric-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Spatial Relations'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-street-crossing',\n",
       "  'name': 'Freiburg Street Crossing',\n",
       "  'full_name': 'Freiburg Street Crossing',\n",
       "  'homepage': 'http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/',\n",
       "  'description': 'The **Freiburg Street Crossing** dataset consists of data collected from three different street crossings in Freiburg, Germany; ; two of which were traffic light regulated intersections and one a zebra crossing without traffic lights. The data can be used to train agents to cross roads autonomously.\\n\\nSource: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)\\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)',\n",
       "  'paper': {'title': 'Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing',\n",
       "   'url': 'https://paperswithcode.com/paper/multimodal-interaction-aware-motion'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'motion prediction',\n",
       "    'url': 'https://paperswithcode.com/task/motion-prediction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Street Crossing'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-campus-3d-scan',\n",
       "  'name': 'Freiburg Campus 3D Scan',\n",
       "  'full_name': 'Freiburg Campus 3D Scan',\n",
       "  'homepage': 'http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/',\n",
       "  'description': 'The **Freiburg Campus 3D Scan** dataset consists of 3D area maps from the Freiburg campus that were scanned with 3D lasers. Areas include corridors, the outdoor campus, and some of the colleges and buildings.\\n\\nSource: [http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/](http://aisdatasets.informatik.uni-freiburg.de/streetcrossing/)\\nImage Source: [http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/](http://ais.informatik.uni-freiburg.de/projects/datasets/octomap/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Campus 3D Scan'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/plant-centroids',\n",
       "  'name': 'Plant Centroids',\n",
       "  'full_name': 'Plant Centroids',\n",
       "  'homepage': 'http://plantcentroids.cs.uni-freiburg.de/',\n",
       "  'description': '**Plant Centroids** is a dataset for stem emerging points (SEP) detection in RGB and NIR image data. The dataset is meant to aid the construction of agricultural robots, where detecting SEPs is an important perception task (to position weeding or fertilizing tools at the plant’s center and finding natural landmarks in the field environment). The dataset contains annotations for ~2000 image sets with a broad variance of plant species and growth stages.\\n\\nSource: [http://plantcentroids.cs.uni-freiburg.de/](http://plantcentroids.cs.uni-freiburg.de/)\\nImage Source: [http://plantcentroids.cs.uni-freiburg.de/](http://plantcentroids.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Plant Centroids'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-across-seasons',\n",
       "  'name': 'Freiburg Across Seasons',\n",
       "  'full_name': 'Freiburg Across Seasons',\n",
       "  'homepage': 'http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/',\n",
       "  'description': '**Freiburg Across Seasons** captures long-term perceptual changes across a span of 3 years. Image sequences were recorded with a forward facing bumblebee stereo camera mounted on a car. During summer, the camera was mounted outside the car where as during winters the camera was inside the car. The image sequences are recorded at relatively low frame rates of 1Hz and 4Hz. All the images have a resolution of 1024 × 768 (width×height) and are JPEG compressed. In total, there are ground truth matchings for 8,133 images for localization based on GPS position.\\n\\nSource: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/)\\nImage Source: [http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/](http://aisdatasets.informatik.uni-freiburg.de/freiburg_across_seasons/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Across Seasons'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-terrains',\n",
       "  'name': 'Freiburg Terrains',\n",
       "  'full_name': 'Freiburg Terrains',\n",
       "  'homepage': 'http://deepterrain.cs.uni-freiburg.de/',\n",
       "  'description': '**Freiburg Terrains** consist of three parts: 3.7 hours of audio recordings of the microphone pointed at the robot wheels. It also contains 24K RGB images from the camera mounted on top of the robot. The dataset creators also provide the SLAM poses for each data collection run. The dataset can be used for terrain classification which is useful for agent navigation tasks.\\n\\nSource: [http://deepterrain.cs.uni-freiburg.de/](http://deepterrain.cs.uni-freiburg.de/)\\nImage Source: [http://deepterrain.cs.uni-freiburg.de/](http://deepterrain.cs.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/self-supervised-visual-terrain-classification'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Audio'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Terrains'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-block-tasks',\n",
       "  'name': 'Freiburg Block Tasks',\n",
       "  'full_name': 'Freiburg Block Tasks',\n",
       "  'homepage': 'http://robotskills.cs.uni-freiburg.de/',\n",
       "  'description': '**Freiburg Block Tasks** is a dataset for robot skill learning. It consists of two datasets.\\r\\nThe first data set consisted of three simulated robot tasks: stacking (A), color pushing (B) and color stacking (C). The data set contains 300 multi-view demonstration videos per task. The tasks are simulated with PyBullet. Of these 300 demonstrations, 150 represent unsuccessful executions of the different tasks. The authors found it helpful to add unsuccessful demonstrations in the training of the embedding to enable training RL agents on it. Without fake examples, the distances in the embedding space for states not seen during training might be noisy. The test set contains the manipulation of blocks. Within the validation set, the blocks are replaced by cylinders of different colors.\\r\\nThe second data set includes real-world human executions of the simulated robot tasks (A, B and C), as well as demonstrations for a task where one has to first separate blocks in order to stack them (D). For each task, there are 60 multi-view demonstration videos, corresponding to 24 minutes of interaction. In contrast to the simulated data set, the real demonstrations contain no unsuccessful executions and are of varying length. The test set contains blocks of unseen sizes and textures, as well as unknown backgrounds.\\r\\n\\r\\nSource: [http://robotskills.cs.uni-freiburg.de/](http://robotskills.cs.uni-freiburg.de/)\\nImage Source: [http://robotskills.cs.uni-freiburg.de/](http://robotskills.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Block Tasks'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cityscapes-motion',\n",
       "  'name': 'Cityscapes-Motion',\n",
       "  'full_name': 'Cityscapes-Motion',\n",
       "  'homepage': 'http://deepmotion.cs.uni-freiburg.de/',\n",
       "  'description': 'The **Cityscapes-Motion** dataset is a supplement to the semantic annotations provided by the Cityscapes dataset, containing 2975 training images and 500 validation images. The dataset creators provide manually annotated motion labels for the category of cars. The images are of resolution 2048×1024 pixels. The task to learn is not just semantic segmentation but also the motion status of the objects.\\n\\nSource: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)\\nImage Source: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Cityscapes-Motion'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kitti-motion',\n",
       "  'name': 'KITTI-Motion',\n",
       "  'full_name': 'KITTI-Motion',\n",
       "  'homepage': 'http://deepmotion.cs.uni-freiburg.de/',\n",
       "  'description': 'The **KITTI-Motion** dataset contains pixel-wise semantic class labels and moving object annotations for 255 images taken from the KITTI Raw dataset. The images are of resolution 1280×384 pixels and contain scenes of freeways, residential areas and inner-cities. The task is not just to semantically segment objects but also to identify their motion status.\\n\\nSource: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)\\nImage Source: [http://deepmotion.cs.uni-freiburg.de/](http://deepmotion.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['KITTI-Motion'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mobilityaids',\n",
       "  'name': 'MobilityAids',\n",
       "  'full_name': 'MobilityAids',\n",
       "  'homepage': 'http://mobility-aids.informatik.uni-freiburg.de/',\n",
       "  'description': '**MobilityAids** is a dataset for perception of people and their mobility aids. The annotated dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. In total the hospital dataset has over 17, 000 annotated RGB-D images, containing people categorized according to the mobility aids they use. The images were collected in the facilities of the Faculty of Engineering of the University of Freiburg and in a hospital in Frankfurt.\\n\\nSource: [http://mobility-aids.informatik.uni-freiburg.de/](http://mobility-aids.informatik.uni-freiburg.de/)\\nImage Source: [http://mobility-aids.informatik.uni-freiburg.de/](http://mobility-aids.informatik.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'Deep Detection of People and their Mobility Aids for a Hospital Robot',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-detection-of-people-and-their-mobility'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Region Proposal',\n",
       "    'url': 'https://paperswithcode.com/task/region-proposal'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MobilityAids'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/robotpush',\n",
       "  'name': 'RobotPush',\n",
       "  'full_name': 'RobotPush',\n",
       "  'homepage': 'http://robotpush.cs.uni-freiburg.de/',\n",
       "  'description': '**RobotPush** is a dataset for object singulation – the task of separating cluttered objects through physical interaction. The dataset contains 3456 training images with labels and 1024 validation images with labels. It consists of simulated and real-world data collected from a PR2 robot that equipped with a Kinect 2 camera. The dataset also contains ground truth instance segmentation masks for 110 images in the test set.\\r\\n\\r\\nSource: [http://robotpush.cs.uni-freiburg.de/](http://robotpush.cs.uni-freiburg.de/)\\nImage Source: [http://robotpush.cs.uni-freiburg.de/](http://robotpush.cs.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'Learning to Singulate Objects using a Push Proposal Network',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-to-singulate-objects-using-a-push'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': 'Optical Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/optical-flow-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RobotPush'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/deeploccross',\n",
       "  'name': 'DeepLocCross',\n",
       "  'full_name': 'DeepLocCross',\n",
       "  'homepage': 'http://deeploc.cs.uni-freiburg.de/',\n",
       "  'description': '**DeepLocCross** is a localization dataset that contains RGB-D stereo images captured at 1280 x 720 pixels at a rate of 20 Hz. The ground-truth pose labels are generated using a LiDAR-based SLAM system. In addition to the 6-DoF localization poses of the robot, the dataset additionally contains tracked detections of the observable dynamic objects. Each tracked object is identified using a unique track ID, spatial coordinates, velocity and orientation angle. Furthermore, as the dataset contains multiple pedestrian crossings, labels at each intersection indicating its safety for crossing are provided.\\r\\nThis dataset consists of seven training sequences with a total of 2264 images, and three testing sequences with a total of 930 images. The dynamic nature of the surrounding environment at which the dataset was captured renders the tasks of localization and visual odometry estimation extremely challenging due to the varying weather conditions, presence of shadows and motion blur caused by the movement of the robot platform. Furthermore, the presence of multiple dynamic objects often results in partial and full occlusions to the informative regions of the image. Moreover, the presence of repeated structures render the pose estimation task more challenging. Overall this dataset covers a wide range of perception related tasks such as loop closure detection, semantic segmentation, visual odometry estimation, global localization, scene flow estimation and behavior prediction.\\r\\n\\r\\nSource: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)\\nImage Source: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry',\n",
       "   'url': 'https://paperswithcode.com/paper/vlocnet-deep-multitask-learning-for-semantic'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'},\n",
       "   {'task': 'Visual Localization',\n",
       "    'url': 'https://paperswithcode.com/task/visual-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DeepLocCross'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/deeploc',\n",
       "  'name': 'DeepLoc',\n",
       "  'full_name': 'DeepLoc',\n",
       "  'homepage': 'http://deeploc.cs.uni-freiburg.de/',\n",
       "  'description': '**DeepLoc** is a large-scale urban outdoor localization dataset. The dataset is currently comprised of one scene spanning an area of 110 x 130 m, that a robot traverses multiple times with different driving patterns. The dataset creators use a LiDAR-based SLAM system with sub-centimeter and sub-degree accuracy to compute the pose labels that provided as groundtruth. Poses in the dataset are approximately spaced by 0.5 m which is twice as dense as other relocalization datasets.\\r\\n\\r\\nFurthermore, for each image the dataset creators provide pixel-wise semantic segmentation annotations for ten categories: Background, Sky, Road, Sidewalk, Grass, Vegetation, Building, Poles & Fences, Dynamic and Void. The dataset is divided into a train and test splits such that the train set comprises seven loops with alternating driving styles amounting to 2737 images, while the test set comprises three loops with a total of 1173 images. The dataset also contains global GPS/INS data and LiDAR measurements.\\r\\n\\r\\nThis dataset can be very challenging for vision based applications such as global localization, camera relocalization, semantic segmentation, visual odometry and loop closure detection, as it contains substantial lighting, weather changes, repeating structures, reflective and transparent glass buildings.\\r\\n\\r\\nSource: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)\\nImage Source: [http://deeploc.cs.uni-freiburg.de/](http://deeploc.cs.uni-freiburg.de/)',\n",
       "  'paper': {'title': 'VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry',\n",
       "   'url': 'https://paperswithcode.com/paper/vlocnet-deep-multitask-learning-for-semantic'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'},\n",
       "   {'task': 'Visual Localization',\n",
       "    'url': 'https://paperswithcode.com/task/visual-localization'},\n",
       "   {'task': 'Camera Relocalization',\n",
       "    'url': 'https://paperswithcode.com/task/camera-relocalization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DeepLoc'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-lighting-adaptable-map-tracking',\n",
       "  'name': 'Freiburg Lighting Adaptable Map Tracking',\n",
       "  'full_name': 'Freiburg Lighting Adaptable Map Tracking',\n",
       "  'homepage': 'http://tracklam.informatik.uni-freiburg.de/',\n",
       "  'description': '**Freiburg Lighting Adaptable Map Tracking** is a dataset for camera trajectory estimation. The dataset consists of two subdatasets, each consisting of a Lighting Adaptable Map and three camera trajectories recorded under varying lighting conditions. The map meshes are stored in PLY format with custom properties and elements. The trajectories contain synchronized RGB-D images, exposure times and gains, ground-truth light settings and camera poses, as well as the camera tracking results presented in the paper.\\n\\nSource: [http://tracklam.informatik.uni-freiburg.de/](http://tracklam.informatik.uni-freiburg.de/)\\nImage Source: [http://tracklam.informatik.uni-freiburg.de/](http://tracklam.informatik.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Lighting Adaptable Map Tracking'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-poking',\n",
       "  'name': 'Freiburg Poking',\n",
       "  'full_name': 'Freiburg Poking',\n",
       "  'homepage': 'http://hind4sight.cs.uni-freiburg.de/',\n",
       "  'description': 'The **Freiburg Poking** dataset is a dataset for learning intuitive physics from physical interaction. It consists of 40K of interaction data with a KUKA LBR iiwa manipulator and a fixed Azure Kinect RGB-D camera. The dataset creators built an arena of styrofoam with walls for preventing objects from falling down. At any given time there were 3-7 objects randomly chosen from a set of 34 distinct objects present on the arena. The objects differed from each other in shape, appearance, material, mass and friction.\\n\\nSource: [http://hind4sight.cs.uni-freiburg.de/](http://hind4sight.cs.uni-freiburg.de/)\\nImage Source: [http://hind4sight.cs.uni-freiburg.de/](http://hind4sight.cs.uni-freiburg.de/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg Poking'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/7-scenes',\n",
       "  'name': '7-Scenes',\n",
       "  'full_name': '7-Scenes',\n",
       "  'homepage': 'https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/',\n",
       "  'description': 'The **7-Scenes** dataset is a collection of tracked RGB-D camera frames. The dataset may be used for evaluation of methods for different applications such as dense tracking and mapping and relocalization techniques.\\nAll scenes were recorded from a handheld Kinect RGB-D camera at 640×480 resolution. The dataset creators use an implementation of the KinectFusion system to obtain the ‘ground truth’ camera tracks, and a dense 3D model. Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets.\\n\\nSource: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)\\nImage Source: [https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['7-Scenes'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cross-dataset-testbed',\n",
       "  'name': 'Cross-Dataset Testbed',\n",
       "  'full_name': 'Cross-Dataset Testbed',\n",
       "  'homepage': 'https://sites.google.com/site/crossdataset/',\n",
       "  'description': 'The Cross-dataset Testbed is a Decaf7 based cross-dataset image classification dataset, which contains 40 categories of images from 3 domains: 3,847 images in Caltech256, 4,000 images in ImageNet, and 2,626 images for SUN. In total there are 10,473 images of 40 categories from these three domains.\\n\\nSource: [Probability Weighted Compact Feature for Domain Adaptive Retrieval](https://arxiv.org/abs/2003.03293)\\nImage Source: [https://sites.google.com/site/crossdataset/](https://sites.google.com/site/crossdataset/)',\n",
       "  'paper': {'title': 'A Testbed for Cross-Dataset Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/a-testbed-for-cross-dataset-analysis'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Unsupervised Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/unsupervised-domain-adaptation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Cross-Dataset Testbed'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/washington-rgb-d',\n",
       "  'name': 'Washington RGB-D',\n",
       "  'full_name': 'Washington RGB-D',\n",
       "  'homepage': 'https://rgbd-dataset.cs.washington.edu/',\n",
       "  'description': '**Washington RGB-D** is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating.\\r\\n\\r\\nSource: [Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work](https://arxiv.org/abs/1702.08513)\\r\\nImage Source: [https://rgbd-dataset.cs.washington.edu/](https://rgbd-dataset.cs.washington.edu/)',\n",
       "  'paper': {'title': 'A large-scale hierarchical multi-view RGB-D object dataset',\n",
       "   'url': 'https://doi.org/10.1109/ICRA.2011.5980382'},\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/object-recognition'},\n",
       "   {'task': '3D Object Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/3d-object-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Washington RGB-D'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/tum-kitchen',\n",
       "  'name': 'TUM Kitchen',\n",
       "  'full_name': 'TUM Kitchen',\n",
       "  'homepage': 'https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data',\n",
       "  'description': 'The **TUM Kitchen** dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384×288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately.\\n\\nSource: [Temporal Human Action Segmentation via Dynamic Clustering](https://arxiv.org/abs/1803.05790)\\nImage Source: [https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data](https://ias.in.tum.de/dokuwiki/software/kitchen-activity-data)',\n",
       "  'paper': {'title': 'The TUM Kitchen Data Set of everyday manipulation activities for motion tracking and action recognition',\n",
       "   'url': 'https://doi.org/10.1109/ICCVW.2009.5457583'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TUM Kitchen'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/hic',\n",
       "  'name': 'HIC',\n",
       "  'full_name': 'Hands in Action',\n",
       "  'homepage': 'http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/',\n",
       "  'description': 'The Hands in action dataset (**HIC**) dataset has RGB-D sequences of hands interacting with objects.\\n\\nSource: [Learning joint reconstruction of hands and manipulated objects](https://arxiv.org/abs/1904.05767)\\nImage Source: [http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/](http://files.is.tue.mpg.de/dtzionas/Hand-Object-Capture/)',\n",
       "  'paper': {'title': 'Capturing Hands in Action using Discriminative Salient Points and Physics Simulation',\n",
       "   'url': 'https://paperswithcode.com/paper/capturing-hands-in-action-using'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'},\n",
       "   {'task': 'Hand Joint Reconstruction',\n",
       "    'url': 'https://paperswithcode.com/task/hand-joint-reconstruction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HIC'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/george-washington',\n",
       "  'name': 'George Washington',\n",
       "  'full_name': 'George Washington',\n",
       "  'homepage': 'https://github.com/cwiep/gw-online-dataset',\n",
       "  'description': 'The **George Washington** dataset contains 20 pages of letters written by George Washington and his associates in 1755 and thereby categorized into historical collection. The images are annotated at word level and contain approximately 5,000 words.\\r\\n\\r\\nSource: [HWNet v2: An Efficient Word Image Representation for Handwritten Documents.](https://arxiv.org/abs/1802.06194)\\r\\nImage Source: [https://www.loc.gov/resource/mgw1a.002/?sp=2](https://www.loc.gov/resource/mgw1a.002/?sp=2)',\n",
       "  'paper': {'title': 'Lexicon-free handwritten word spotting using character HMMs',\n",
       "   'url': 'https://doi.org/10.1016/j.patrec.2011.09.009'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Information Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/information-retrieval'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['George Washington'],\n",
       "  'num_papers': 12,\n",
       "  'data_loaders': [{'url': 'https://github.com/cwiep/gw-online-dataset',\n",
       "    'repo': 'https://github.com/cwiep/gw-online-dataset',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/watch-n-patch',\n",
       "  'name': 'Watch-n-Patch',\n",
       "  'full_name': 'Watch-n-Patch',\n",
       "  'homepage': '',\n",
       "  'description': 'The **Watch-n-Patch** dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations.\\r\\n\\r\\nSource: [Head Detection with Depth Images in the Wild](https://arxiv.org/abs/1707.06786)\\r\\nImage Source: [https://openaccess.thecvf.com/content_cvpr_2015/papers/Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2015/papers/Wu_Watch-n-Patch_Unsupervised_Understanding_2015_CVPR_paper.pdf)',\n",
       "  'paper': {'title': 'Watch-n-Patch: Unsupervised Understanding of Actions and Relations',\n",
       "   'url': 'https://paperswithcode.com/paper/watch-n-patch-unsupervised-understanding-of'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Interactive'],\n",
       "  'tasks': [{'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Human-Object Interaction Detection',\n",
       "    'url': 'https://paperswithcode.com/task/human-object-interaction-detection'},\n",
       "   {'task': 'Action Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/action-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Watch-n-Patch'],\n",
       "  'num_papers': 10,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/parzival',\n",
       "  'name': 'Parzival',\n",
       "  'full_name': 'Parzival',\n",
       "  'homepage': 'https://diuf.unifr.ch/main/hisdoc/divadia',\n",
       "  'description': 'The **Parzival** dataset consists of 47 pages by three writers. These pages were taken from a medieval German manuscript from the 13th century that contains the epic poem Parzival by Wolfram von Eschenbach. The image size is 2000 x 3000 pixels. 24 pages are selected as training set; 14 pages are selected as test set; 2 pages are selected as validation set.\\n\\nSource: [https://diuf.unifr.ch/main/hisdoc/divadia](https://diuf.unifr.ch/main/hisdoc/divadia)\\nImage Source: [https://diuf.unifr.ch/main/hisdoc/divadia](https://diuf.unifr.ch/main/hisdoc/divadia)',\n",
       "  'paper': {'title': 'Language Model Integration for the Recognition of Handwritten Medieval Documents',\n",
       "   'url': 'https://doi.org/10.1109/ICDAR.2009.17'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [],\n",
       "  'languages': ['German'],\n",
       "  'variants': ['Parzival'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cdtb',\n",
       "  'name': 'CDTB',\n",
       "  'full_name': 'Color-and-Depth Tracking',\n",
       "  'homepage': 'https://www.vicos.si/Projects/CDTB',\n",
       "  'description': '\\n\\nSource: [https://www.vicos.si/Projects/CDTB 4.2 State-of-the-art Comparison A TH CTB (color-and-depth visual object tracking) dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.](https://www.vicos.si/Projects/CDTB 4.2 State-of-the-art Comparison A TH CTB (color-and-depth visual object tracking) dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.)\\nImage Source: [https://www.vicos.si/Projects/CDTB](https://www.vicos.si/Projects/CDTB)',\n",
       "  'paper': {'title': 'CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark',\n",
       "   'url': 'https://paperswithcode.com/paper/cdtb-a-color-and-depth-visual-object-tracking'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Visual Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-object-tracking'},\n",
       "   {'task': 'Visual Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/visual-tracking'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CDTB'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/egodexter',\n",
       "  'name': 'EgoDexter',\n",
       "  'full_name': 'EgoDexter',\n",
       "  'homepage': 'https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm',\n",
       "  'description': 'The **EgoDexter** dataset provides both 2D and 3D pose annotations for 4 testing video sequences with 3190 frames. The videos are recorded with body-mounted camera from egocentric viewpoints and contain cluttered backgrounds, fast camera motion, and complex interactions with various objects. Fingertip positions were manually annotated for 1485 out of 3190 frames.\\n\\nSource: [Hand Pose Estimation via Latent 2.5D Heatmap Regression](https://arxiv.org/abs/1804.09534)\\nImage Source: [https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/EgoDexter.htm)',\n",
       "  'paper': {'title': 'Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor',\n",
       "   'url': 'https://paperswithcode.com/paper/real-time-hand-tracking-under-occlusion-from'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'},\n",
       "   {'task': '3D Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['EgoDexter'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/synthhands',\n",
       "  'name': 'SynthHands',\n",
       "  'full_name': 'SynthHands',\n",
       "  'homepage': 'https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm',\n",
       "  'description': 'The **SynthHands** dataset is a dataset for hand pose estimation which consists of real captured hand motion retargeted to a virtual hand with natural backgrounds and interactions with different objects. The dataset contains data for male and female hands, both with and without interaction with objects. While the hand and foreground object are synthtically generated using Unity, the motion was obtained from real performances as described in the accompanying paper. In addition, real object textures and background images (depth and color) were used. Ground truth 3D positions are provided for 21 keypoints of the hand.\\n\\nSource: [Egocentric 6-DoF Tracking of Small Handheld Objects](https://arxiv.org/abs/1804.05870)\\nImage Source: [https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm](https://handtracker.mpi-inf.mpg.de/projects/OccludedHands/SynthHands.htm)',\n",
       "  'paper': {'title': 'Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor',\n",
       "   'url': 'https://paperswithcode.com/paper/real-time-hand-tracking-under-occlusion-from'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos', 'RGB-D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'},\n",
       "   {'task': '3D Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SynthHands'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/washington-rgb-d-scenes-v2',\n",
       "  'name': 'Washington RGB-D Scenes v2',\n",
       "  'full_name': 'Washington RGB-D Scenes v2',\n",
       "  'homepage': 'https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/',\n",
       "  'description': 'The RGB-D Scenes Dataset v2 consists of 14 scenes containing furniture (chair, coffee table, sofa, table) and a subset of the objects in the RGB-D Object Dataset (bowls, caps, cereal boxes, coffee mugs, and soda cans). Each scene is a point cloud created by aligning a set of video frames using Patch Volumes Mapping.\\r\\n\\r\\nSource: [https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/](https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/)\\nImage Source: [https://arxiv.org/abs/1904.02530](https://arxiv.org/abs/1904.02530)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Point cloud', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Washington RGB-D Scenes v2'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/washington-rgb-d-scenes',\n",
       "  'name': 'Washington RGB-D Scenes',\n",
       "  'full_name': 'Washington RGB-D Scenes',\n",
       "  'homepage': 'https://rgbd-dataset.cs.washington.edu/dataset/',\n",
       "  'description': 'The RGB-D Scenes Dataset contains 8 scenes annotated with objects that belong to the Washington RGB-D Object Dataset. Each scene is a single video sequence consisting of multiple RGB-D frames.\\n\\nSource: [https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/](https://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/)\\nImage Source: [https://arxiv.org/abs/1904.02530](https://arxiv.org/abs/1904.02530)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Point cloud', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Washington RGB-D Scenes'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mannequinchallenge',\n",
       "  'name': 'MannequinChallenge',\n",
       "  'full_name': 'MannequinChallenge',\n",
       "  'homepage': 'https://google.github.io/mannequinchallenge/www/index.html',\n",
       "  'description': 'The **MannequinChallenge** Dataset (MQC) provides in-the-wild videos of people in static poses while a hand-held camera pans around the scene. The dataset consists of three splits for training, validation and testing.\\r\\n\\r\\nSource: [Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild](https://arxiv.org/abs/2003.07581)\\r\\nImage Source: [https://google.github.io/mannequinchallenge/www/index.html](https://google.github.io/mannequinchallenge/www/index.html)',\n",
       "  'paper': {'title': 'Learning the Depths of Moving People by Watching Frozen People',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-the-depths-of-moving-people-by'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Monocular Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MannequinChallenge'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/freiburg-rgb-d-people',\n",
       "  'name': 'Freiburg RGB-D People',\n",
       "  'full_name': 'Freiburg RGB-D People',\n",
       "  'homepage': 'http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html',\n",
       "  'description': 'The **Freiburg RGB-D People** dataset contains 3000+ RGB-D frames acquired in a university hall from three vertically mounted Kinect sensors. The data contains mostly upright walking and standing persons seen from different orientations and with different levels of occlusions.\\n\\nSource: [http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html](http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)\\nImage Source: [http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html](http://www2.informatik.uni-freiburg.de/~spinello/RGBD-dataset.html)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2011-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Freiburg RGB-D People'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/fraunhofer-ipa-bin-picking',\n",
       "  'name': 'Fraunhofer IPA Bin-Picking',\n",
       "  'full_name': 'Fraunhofer IPA Bin-Picking',\n",
       "  'homepage': 'https://www.bin-picking.ai/en/dataset.html',\n",
       "  'description': 'The **Fraunhofer IPA Bin-Picking** dataset is a large-scale dataset comprising both simulated and real-world scenes for various objects (potentially having symmetries) and is fully annotated with 6D poses. A pyhsics simulation is used to create scenes of many parts in bulk by dropping objects in a random position and orientation above a bin. Additionally, this dataset extends the Siléane dataset by providing more samples. This allows to e.g. train deep neural networks and benchmark the performance on the public Siléane dataset\\n\\nSource: [https://www.bin-picking.ai/en/dataset.html](https://www.bin-picking.ai/en/dataset.html)\\nImage Source: [https://arxiv.org/abs/1912.12125](https://arxiv.org/abs/1912.12125)',\n",
       "  'paper': {'title': 'Large-scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-6d-object-pose-estimation-dataset'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', '6D'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Instance Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/instance-segmentation'},\n",
       "   {'task': '6D Pose Estimation using RGB',\n",
       "    'url': 'https://paperswithcode.com/task/6d-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Fraunhofer IPA Bin-Picking'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/pavis-rgb-d',\n",
       "  'name': 'PAVIS RGB-D',\n",
       "  'full_name': 'PAVIS RGB-D',\n",
       "  'homepage': 'https://pavis.iit.it/datasets/rgbd-id',\n",
       "  'description': '**PAVIS RGB-D** is a dataset for person re-identification using depth information. The main motivation is that techniques such as  SDALF fail when the individuals change their clothing, therefore they cannot be used for long-term video surveillance. Depth information is the solution to deal with this problem because it stays constant for a longer period of time. The dataset is composed by four different groups of data collected using the Kinect. The first group of data has been obtained by recording 79 people with a frontal view, walking slowly, avoiding occlusions and with stretched arms (\"Collaborative\"). This happened in an indoor scenario, where the people were at least 2 meters away from the camera. The second (\"Walking1\") and third (\"Walking2\") groups of data are composed by frontal recordings of the same 79 people walking normally while entering the lab where they normally work. The fourth group (\"Backwards\") is a back view recording of the people walking away from the lab.\\r\\nThe dataset creators provide 5 synchronized information for each person: 1) a set of 5 RGB images, 2) the foreground masks, 3) the skeletons, 4) the 3d mesh (ply), 5) the estimated floor.\\r\\n\\r\\nSource: [https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset](https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset)\\r\\nImage Source: [https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset](https://www.iit.it/research/lines/pattern-analysis-and-computer-vision/pavis-datasets/534-rgb-d-person-re-identification-dataset)',\n",
       "  'paper': {'title': 'Re-identification with RGB-D Sensors',\n",
       "   'url': 'https://doi.org/10.1007/978-3-642-33863-2_43'},\n",
       "  'introduced_date': '2012-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'RGB-D'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['PAVIS RGB-D'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/couples-therapy',\n",
       "  'name': 'Couples Therapy',\n",
       "  'full_name': 'Couples Therapy Corpus',\n",
       "  'homepage': None,\n",
       "  'description': 'The **Couples Therapy** corpus contains audio, video recordings and manual transcriptions of conversations between 134 real-life couples attending marital therapy. In each session, one person selected a topic that was discussed over 10 minutes with the spouse. At the end of the session, both speakers were rated separately on 33 “behavior codes” by multiple annotators based on the Couples Interaction and Social Support Rating Systems. Each behavior was rated on a Likert scale from 1, indicating absence, to 9, indicating strong presence. A session-level rating was obtained for each speaker by averaging the annotator ratings. This process was repeated for the spouse, resulting in 2 sessions per couple at a time. The total number of sessions per couple varied between 2 and 6.\\n\\nSource: [Modeling Interpersonal Influence of Verbal Behaviorin Couples Therapy Dyadic Interactions](https://arxiv.org/abs/1805.09436)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Texts', 'Audio'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Couples Therapy'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/raider',\n",
       "  'name': 'Raider',\n",
       "  'full_name': 'Raider',\n",
       "  'homepage': 'https://github.com/HaxbyLab/raiders_data',\n",
       "  'description': 'The **Raider** dataset collects fMRI recordings of 1000 voxels from the ventral temporal cortex, for 10 healthy adult participants passively watching the full-length movie “Raiders of the Lost Ark”.\\n\\nSource: [Time-Resolved fMRI Shared Response Model using Gaussian Process Factor Analysis](https://arxiv.org/abs/2006.05572)\\nImage Source: [https://arxiv.org/abs/1909.12537](https://arxiv.org/abs/1909.12537)',\n",
       "  'paper': {'title': 'A common, high-dimensional model of the representational space in human ventral temporal cortex',\n",
       "   'url': 'http://dx.doi.org/10.1016/j.neuron.2011.08.026'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Medical', 'fMRI'],\n",
       "  'tasks': [{'task': 'Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/denoising'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Raider'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://github.com/HaxbyLab/raiders_data',\n",
       "    'repo': 'https://github.com/HaxbyLab/raiders_data',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/vizdoom',\n",
       "  'name': 'VizDoom',\n",
       "  'full_name': 'VizDoom',\n",
       "  'homepage': 'http://vizdoom.cs.put.edu.pl/',\n",
       "  'description': 'ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players’ actions are collected or runs in a constant frame rate without waiting.\\r\\n\\r\\nSource: [Arena: a toolkit for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1907.09467)\\r\\nImage Source: [https://github.com/mwydmuch/ViZDoom](https://github.com/mwydmuch/ViZDoom)',\n",
       "  'paper': {'title': 'ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/vizdoom-a-doom-based-ai-research-platform-for'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Scene Generation',\n",
       "    'url': 'https://paperswithcode.com/task/scene-generation'},\n",
       "   {'task': 'Game of Doom',\n",
       "    'url': 'https://paperswithcode.com/task/game-of-doom'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Q-Learning', 'url': 'https://paperswithcode.com/task/q-learning'},\n",
       "   {'task': 'Starcraft', 'url': 'https://paperswithcode.com/task/starcraft'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ViZDoom Basic Scenario', 'VizDoom'],\n",
       "  'num_papers': 114,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/starcraft-ii-learning-environment',\n",
       "  'name': 'StarCraft II Learning Environment',\n",
       "  'full_name': 'StarCraft II Learning Environment',\n",
       "  'homepage': 'https://github.com/deepmind/pysc2',\n",
       "  'description': 'The **StarCraft II Learning Environment** (S2LE) is a reinforcement learning environment based on the game StarCraft II. The environment consists of three sub-components: a Linux StarCraft II binary, the StarCraft II API and PySC2. The StarCraft II API allows programmatic control of StarCraft II. It can be used to start a game, get observations, take actions, and review replays. PyC2 is a Python environment that wraps the StarCraft II API to ease the interaction between Python reinforcement learning agents and StarCraft II. It defines an action and observation specification, and includes a random agent and a handful of rule-based agents as examples. It also includes some mini-games as challenges and visualization tools to understand what the agent can see and do.\\n\\nSource: [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)\\nImage Source: [https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)',\n",
       "  'paper': {'title': 'StarCraft II: A New Challenge for Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/starcraft-ii-a-new-challenge-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Multi-agent Reinforcement Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-agent-reinforcement-learning'},\n",
       "   {'task': 'Starcraft II',\n",
       "    'url': 'https://paperswithcode.com/task/starcraft-ii'},\n",
       "   {'task': 'Starcraft', 'url': 'https://paperswithcode.com/task/starcraft'}],\n",
       "  'languages': [],\n",
       "  'variants': ['StarCraft II Learning Environment'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': [{'url': 'https://github.com/deepmind/pysc2',\n",
       "    'repo': 'https://github.com/deepmind/pysc2',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ai2-thor',\n",
       "  'name': 'AI2-THOR',\n",
       "  'full_name': 'AI2-THOR',\n",
       "  'homepage': 'https://ai2thor.allenai.org/',\n",
       "  'description': 'AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with.\\r\\n\\r\\nSource: [Learning Object Relation Graph andTentative Policy for Visual Navigation](https://arxiv.org/abs/2007.11018)\\r\\nImage Source: [https://ai2thor.allenai.org/](https://ai2thor.allenai.org/)',\n",
       "  'paper': {'title': 'AI2-THOR: An Interactive 3D Environment for Visual AI',\n",
       "   'url': 'https://paperswithcode.com/paper/ai2-thor-an-interactive-3d-environment-for'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Visual Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/visual-navigation'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AI2-THOR'],\n",
       "  'num_papers': 99,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/torcs',\n",
       "  'name': 'TORCS',\n",
       "  'full_name': 'The Open Racing Car Simulator',\n",
       "  'homepage': 'https://sourceforge.net/projects/torcs/',\n",
       "  'description': '**TORCS** (**The Open Racing Car Simulator**) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception.\\r\\n\\r\\nSource: [MADRaS : Multi Agent Driving Simulator](https://arxiv.org/abs/2010.00993)\\r\\nImage Source: [https://sourceforge.net/projects/torcs/](https://sourceforge.net/projects/torcs/)',\n",
       "  'paper': {'title': 'Torcs, the open racing car simulator',\n",
       "   'url': 'http://torcs.sourceforge.net'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['TORCS'],\n",
       "  'num_papers': 75,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/deepmind-control-suite',\n",
       "  'name': 'DeepMind Control Suite',\n",
       "  'full_name': 'DeepMind Control Suite',\n",
       "  'homepage': 'https://github.com/deepmind/dm_control',\n",
       "  'description': 'The **DeepMind Control Suite** (DMCS) is a set of simulated continuous control environments with a standardized structure and interpretable rewards. The tasks are written and powered by the MuJoCo physics engine, making them easy to identify. Control Suite tasks include Pendulum, Acrobot, Cart-pole, Cart-k-pole, Ball in cup, Point-mass, Reacher, Finger, Hooper, Fish, Cheetah, Walker, Manipulator, Manipulator extra, Stacker, Swimmer, Humanoid, Humanoid_CMU and LQR.\\r\\n\\r\\nSource: [Unsupervised Learning of Object Structure and Dynamics from Videos](https://arxiv.org/abs/1906.07889)\\r\\nImage Source: [https://arxiv.org/abs/1801.00690](https://arxiv.org/abs/1801.00690)',\n",
       "  'paper': {'title': 'DeepMind Control Suite',\n",
       "   'url': 'https://paperswithcode.com/paper/deepmind-control-suite'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Continuous Control',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control'},\n",
       "   {'task': 'Continuous Control (100k environment steps)',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control-100k-environment-steps'},\n",
       "   {'task': 'Continuous Control (500k environment steps)',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control-500k-environment-steps'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DeepMind Walker Walk (Images)',\n",
       "   'DeepMind Finger Spin (Images)',\n",
       "   'DeepMind Cup Catch (Images)',\n",
       "   'DeepMind Cheetah Run (Images)',\n",
       "   'DeepMind Cartpole Swingup (Images)',\n",
       "   'DeepMind Cartpole Balance (Images)',\n",
       "   'DeepMind Control Suite'],\n",
       "  'num_papers': 169,\n",
       "  'data_loaders': [{'url': 'https://github.com/deepmind/dm_control',\n",
       "    'repo': 'https://github.com/deepmind/dm_control',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/gvgai',\n",
       "  'name': 'GVGAI',\n",
       "  'full_name': 'General Video Game AI',\n",
       "  'homepage': 'http://www.gvgai.net/',\n",
       "  'description': 'The **General Video Game AI** (**GVGAI**) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed.\\r\\n\\r\\nSource: [Rolling Horizon Evolutionary Algorithms for General Video Game Playing](https://arxiv.org/abs/2003.12331)\\r\\nImage Source: [http://www.gvgai.net/](http://www.gvgai.net/)',\n",
       "  'paper': {'title': 'General Video Game AI: Competition, Challenges and Opportunities',\n",
       "   'url': 'http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11853'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Fairness', 'url': 'https://paperswithcode.com/task/fairness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GVGAI'],\n",
       "  'num_papers': 30,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stardata',\n",
       "  'name': 'StarData',\n",
       "  'full_name': 'StarData',\n",
       "  'homepage': 'https://github.com/TorchCraft/StarData',\n",
       "  'description': '**StarData** is a StarCraft: Brood War replay dataset, with 65,646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second.\\r\\n\\r\\nSource: [https://github.com/TorchCraft/StarData](https://github.com/TorchCraft/StarData)\\r\\nImage Source: [https://www.youtube.com/watch?app=desktop&v=vBjgww8jDgw](https://www.youtube.com/watch?app=desktop&v=vBjgww8jDgw)',\n",
       "  'paper': {'title': 'STARDATA: A StarCraft AI Research Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/stardata-a-starcraft-ai-research-dataset'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Replay data', 'Actions'],\n",
       "  'tasks': [{'task': 'Real-Time Strategy Games',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-strategy-games'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'},\n",
       "   {'task': 'Starcraft', 'url': 'https://paperswithcode.com/task/starcraft'}],\n",
       "  'languages': [],\n",
       "  'variants': ['StarData'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/TorchCraft/StarData',\n",
       "    'repo': 'https://github.com/TorchCraft/StarData',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/atari-head',\n",
       "  'name': 'Atari-HEAD',\n",
       "  'full_name': None,\n",
       "  'homepage': 'https://zenodo.org/record/2587121',\n",
       "  'description': '**Atari-HEAD** is a dataset of human actions and eye movements recorded while playing Atari videos games. For every game frame, its corresponding image frame, the human keystroke action, the reaction time to make that action, the gaze positions, and immediate reward returned by the environment were recorded. The gaze data was recorded using an EyeLink 1000 eye tracker at 1000Hz. The human subjects are amateur players who are familiar with the games. The human subjects were only allowed to play for 15 minutes and were required to rest for at least 15 minutes before the next trial. Data was collected from 4 subjects, 16 games, 175 15-minute trials, and a total of 2.97 million frames/demonstrations.\\n\\nSource: [https://zenodo.org/record/2587121](https://zenodo.org/record/2587121)\\nImage Source: [https://arxiv.org/abs/1903.06754](https://arxiv.org/abs/1903.06754)',\n",
       "  'paper': {'title': 'Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/atari-head-atari-human-eye-tracking-and'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Actions', 'Tracking'],\n",
       "  'tasks': [{'task': 'Atari Games',\n",
       "    'url': 'https://paperswithcode.com/task/atari-games'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Atari-HEAD'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mario-ai',\n",
       "  'name': 'Mario AI',\n",
       "  'full_name': 'Mario AI',\n",
       "  'homepage': 'https://github.com/zerg000000/mario-ai',\n",
       "  'description': '**Mario AI** was a benchmark environment for reinforcement learning. The gameplay in Mario AI, as in the original Nintendo’s version, consists in moving the controlled character, namely Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over cliffs to get past them. Mario can be in one of three states: Small, Big (can kill enemies by jumping onto them), and Fire (can shoot fireballs).\\n\\nSource: [https://github.com/zerg000000/mario-ai](https://github.com/zerg000000/mario-ai)\\nImage Source: [http://julian.togelius.com/Karakovskiy2012The.pdf](http://julian.togelius.com/Karakovskiy2012The.pdf)',\n",
       "  'paper': {'title': 'The 2009 Mario AI Competition',\n",
       "   'url': 'https://doi.org/10.1109/CEC.2010.5586133'},\n",
       "  'introduced_date': '2010-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'SNES Games',\n",
       "    'url': 'https://paperswithcode.com/task/snes-games'},\n",
       "   {'task': 'Real-Time Strategy Games',\n",
       "    'url': 'https://paperswithcode.com/task/real-time-strategy-games'},\n",
       "   {'task': 'Starcraft', 'url': 'https://paperswithcode.com/task/starcraft'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Mario AI'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': [{'url': 'https://github.com/zerg000000/mario-ai',\n",
       "    'repo': 'https://github.com/zerg000000/mario-ai',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/d4rl',\n",
       "  'name': 'D4RL',\n",
       "  'full_name': 'D4RL',\n",
       "  'homepage': 'https://sites.google.com/view/d4rl/home',\n",
       "  'description': '**D4RL** is a collection of environments for offline reinforcement learning. These environments include Maze2D, AntMaze, Adroit, Gym, Flow, FrankKitchen and CARLA.\\r\\n\\r\\nSource: [https://sites.google.com/view/d4rl/home](https://sites.google.com/view/d4rl/home)\\r\\nImage Source: [https://sites.google.com/view/d4rl/home](https://sites.google.com/view/d4rl/home)',\n",
       "  'paper': {'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/datasets-for-data-driven-reinforcement'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Continuous Control',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Offline RL',\n",
       "    'url': 'https://paperswithcode.com/task/offline-rl'}],\n",
       "  'languages': [],\n",
       "  'variants': ['D4RL'],\n",
       "  'num_papers': 112,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/d4rl_mujoco_ant',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/d4rl_mujoco_halfcheetah',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/d4rl_mujoco_hopper',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/d4rl_mujoco_walker2d',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/atariari',\n",
       "  'name': 'AtariARI',\n",
       "  'full_name': 'Atari Annotated RAM Interface',\n",
       "  'homepage': 'https://github.com/mila-iqia/atari-representation-learning',\n",
       "  'description': 'The **AtariARI** (**Atari Annotated RAM Interface**) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces.\\n\\nSource: [https://arxiv.org/pdf/1906.08226.pdf](https://arxiv.org/pdf/1906.08226.pdf)\\nImage Source: [https://github.com/mila-iqia/atari-representation-learning](https://github.com/mila-iqia/atari-representation-learning)',\n",
       "  'paper': {'title': 'Unsupervised State Representation Learning in Atari',\n",
       "   'url': 'https://paperswithcode.com/paper/unsupervised-state-representation-learning-in'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Environment'],\n",
       "  'tasks': [{'task': 'Atari Games',\n",
       "    'url': 'https://paperswithcode.com/task/atari-games'},\n",
       "   {'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AtariARI'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/mila-iqia/atari-representation-learning',\n",
       "    'repo': 'https://github.com/mila-iqia/atari-representation-learning',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/lani',\n",
       "  'name': 'Lani',\n",
       "  'full_name': None,\n",
       "  'homepage': None,\n",
       "  'description': 'LANI is a 3D navigation environment and corpus, where an agent navigates between landmarks. **Lani** contains 27,965 crowd-sourced instructions for navigation in an open environment. Each datapoint includes an instruction, a human-annotated ground-truth demonstration trajectory, and an environment with various landmarks and lakes. The dataset train/dev/test split is 19,758/4,135/4,072. Each environment specification defines placement of 6–13 landmarks within a square grass field of size 50m×50m.\\n\\nSource: [Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction](https://arxiv.org/abs/1811.04179)\\nImage Source: [https://arxiv.org/pdf/1809.00786.pdf](https://arxiv.org/pdf/1809.00786.pdf)',\n",
       "  'paper': {'title': 'Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction',\n",
       "   'url': 'https://paperswithcode.com/paper/mapping-instructions-to-actions-in-3d'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Environment'],\n",
       "  'tasks': [{'task': 'Continuous Control',\n",
       "    'url': 'https://paperswithcode.com/task/continuous-control'},\n",
       "   {'task': 'Starcraft II',\n",
       "    'url': 'https://paperswithcode.com/task/starcraft-ii'},\n",
       "   {'task': 'Starcraft', 'url': 'https://paperswithcode.com/task/starcraft'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Lani'],\n",
       "  'num_papers': 4,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chalet',\n",
       "  'name': 'CHALET',\n",
       "  'full_name': 'Cornell House Agent Learning Environment',\n",
       "  'homepage': 'https://github.com/lil-lab/chalet',\n",
       "  'description': '**CHALET** is a 3D house simulator with support for navigation and manipulation. Unlike existing systems, CHALET supports both a wide range of object manipulation, as well as supporting complex environemnt layouts consisting of multiple rooms. The range of object manipulations includes the ability to pick up and place objects, toggle the state of objects like taps or televesions, open or close containers, and insert or remove objects from these containers. In addition, the simulator comes with 58 rooms that can be combined to create houses, including 10 default house layouts. CHALET is therefore suitable for setting up challenging environments for various AI tasks that require complex language understanding and planning, such as navigation, manipulation, instruction following, and interactive question answering.\\r\\n\\r\\nSource: [https://github.com/lil-lab/chalet](https://github.com/lil-lab/chalet)\\r\\nImage Source: [https://arxiv.org/pdf/1809.00786.pdf](https://arxiv.org/pdf/1809.00786.pdf)',\n",
       "  'paper': {'title': 'CHALET: Cornell House Agent Learning Environment',\n",
       "   'url': 'https://paperswithcode.com/paper/chalet-cornell-house-agent-learning'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Environment'],\n",
       "  'tasks': [{'task': 'Vision and Language Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/vision-and-language-navigation'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'},\n",
       "   {'task': 'Imitation Learning',\n",
       "    'url': 'https://paperswithcode.com/task/imitation-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['CHALET'],\n",
       "  'num_papers': 9,\n",
       "  'data_loaders': [{'url': 'https://github.com/lil-lab/chalet',\n",
       "    'repo': 'https://github.com/lil-lab/chalet',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/griddly',\n",
       "  'name': 'Griddly',\n",
       "  'full_name': 'Griddly',\n",
       "  'homepage': 'https://griddly.readthedocs.io/en/latest',\n",
       "  'description': '**Griddly** is an environment for grid-world based research.  Griddly provides a highly optimized game state and rendering engine with a flexible high-level interface for configuring environments. Not only does Griddly offer simple interfaces for single, multi-player and RTS games, but also multiple methods of rendering, configurable partial observability and interfaces for procedural content generation.\\r\\n\\r\\nSource: [https://griddly.readthedocs.io/en/latest/about/introduction.html](https://griddly.readthedocs.io/en/latest/about/introduction.html)\\r\\nImage Source: [https://griddly.readthedocs.io/en/latest/](https://griddly.readthedocs.io/en/latest/)',\n",
       "  'paper': {'title': 'Griddly: A platform for AI research in games',\n",
       "   'url': 'https://paperswithcode.com/paper/griddly-a-platform-for-ai-research-in-games'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Environment'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['Griddly'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nombank',\n",
       "  'name': 'NomBank',\n",
       "  'full_name': 'NomBank',\n",
       "  'homepage': 'https://nlp.cs.nyu.edu/meyers/NomBank.html',\n",
       "  'description': '**NomBank** is an annotation project at New York University that is related to the PropBank project at the University of Colorado.  The goal is to mark the sets of arguments that cooccur with nouns in the PropBank Corpus (the Wall Street Journal Corpus of the Penn Treebank), just as PropBank records such information for verbs.  As a side effect of the annotation process, the authors are producing a number of other resources including various dictionaries, as well as PropBank style lexical entries called frame files. These resources help the user label the various arguments and adjuncts of the head nouns with roles (sets of argument labels for each sense of each noun).  NYU and U of Colorado are making a coordinated effort to insure that, when possible, role definitions are consistent across parts of speech. For example, PropBank\\'s frame file for the verb \"decide\" was used in the annotation of the noun \"decision\".\\r\\n\\r\\nSource: [Nombank](https://nlp.cs.nyu.edu/meyers/NomBank.html)\\r\\nImage Source: [https://nlp.cs.nyu.edu/meyers/NomBank.html](https://nlp.cs.nyu.edu/meyers/NomBank.html)',\n",
       "  'paper': {'title': 'The NomBank Project: An Interim Report',\n",
       "   'url': 'https://www.aclweb.org/anthology/W04-2705/'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Semantic Role Labeling',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-role-labeling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NomBank'],\n",
       "  'num_papers': 45,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/qa-srl',\n",
       "  'name': 'QA-SRL',\n",
       "  'full_name': 'QA-SRL',\n",
       "  'homepage': 'http://qasrl.org/',\n",
       "  'description': '**QA-SRL** was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences.\\r\\n\\r\\nSource: [Zero-Shot Relation Extraction via Reading Comprehension](https://arxiv.org/abs/1706.04115)\\r\\nImage Source: [http://browse.qasrl.org/](http://browse.qasrl.org/)',\n",
       "  'paper': {'title': 'Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language',\n",
       "   'url': 'https://paperswithcode.com/paper/question-answer-driven-semantic-role-labeling'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Open Information Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/open-information-extraction'},\n",
       "   {'task': 'Semantic Role Labeling',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-role-labeling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['QA-SRL'],\n",
       "  'num_papers': 33,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/qa_srl',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#qa-srl-semantic-role-labeling',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sparc',\n",
       "  'name': 'SParC',\n",
       "  'full_name': 'Semantic Parsing in Context',\n",
       "  'homepage': 'https://github.com/taoyds/sparc',\n",
       "  'description': '**SParC** is a large-scale dataset for complex, cross-domain, and context-dependent (multi-turn) semantic parsing and text-to-SQL task (interactive natural language interfaces for relational databases).\\r\\n\\r\\nSource: [https://github.com/taoyds/sparc](https://github.com/taoyds/sparc)\\r\\nImage Source: [https://arxiv.org/pdf/1906.02285.pdf](https://arxiv.org/pdf/1906.02285.pdf)',\n",
       "  'paper': {'title': 'SParC: Cross-Domain Semantic Parsing in Context',\n",
       "   'url': 'https://paperswithcode.com/paper/sparc-cross-domain-semantic-parsing-in'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Text-To-Sql',\n",
       "    'url': 'https://paperswithcode.com/task/text-to-sql'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SParC'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': [{'url': 'https://github.com/taoyds/sparc',\n",
       "    'repo': 'https://github.com/taoyds/sparc',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/conll-2002',\n",
       "  'name': 'CoNLL 2002',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.clips.uantwerpen.be/conll2002/ner/',\n",
       "  'description': 'The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task.\\r\\n\\r\\nSource: [CoNLL 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\\r\\nImage Source: [https://www.aclweb.org/anthology/W02-2024.pdf](https://www.aclweb.org/anthology/W02-2024.pdf)',\n",
       "  'paper': {'title': 'Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition',\n",
       "   'url': 'https://www.aclweb.org/anthology/W02-2024/'},\n",
       "  'introduced_date': '2002-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Token Classification',\n",
       "    'url': 'https://paperswithcode.com/task/token-classification'},\n",
       "   {'task': 'Part-Of-Speech Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/part-of-speech-tagging'},\n",
       "   {'task': 'Cross-Lingual Transfer',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-transfer'}],\n",
       "  'languages': ['English', 'Spanish', 'German', 'Chinese', 'Dutch'],\n",
       "  'variants': ['CoNLL 2002 (Spanish)',\n",
       "   'CoNLL 2002 (Dutch)',\n",
       "   'CoNLL 2002',\n",
       "   'conll2002'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/conll2002',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/panlex',\n",
       "  'name': 'Panlex',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://panlex.org/source-list/',\n",
       "  'description': 'PanLex translates words in thousands of languages. Its database is panlingual (emphasizes coverage of every language) and lexical (focuses on words, not sentences).\\r\\n\\r\\nSource: [PANLEX](https://panlex.org/)\\r\\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf)',\n",
       "  'paper': {'title': 'PanLex: Building a Resource for Panlingual Lexical Translation',\n",
       "   'url': 'https://paperswithcode.com/paper/panlex-building-a-resource-for-panlingual'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English', 'Spanish'],\n",
       "  'variants': ['Panlex'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/mcscript',\n",
       "  'name': 'MCScript',\n",
       "  'full_name': 'MCScript',\n",
       "  'homepage': 'http://www.sfb1102.uni-saarland.de/?page_id=2582',\n",
       "  'description': '**MCScript** is used as the official dataset of SemEval2018 Task11. This dataset constructs a collection of text passages about daily life activities and a series of questions referring to each passage, and each question is equipped with two answer choices. The MCScript comprises 9731, 1411, and 2797 questions in training, development, and test set respectively.\\r\\n\\r\\nSource: [Multi-Perspective Fusion Network for Commonsense Reading Comprehension](https://arxiv.org/abs/1901.02257)\\r\\nImage Source: [https://arxiv.org/pdf/1803.05223.pdf](https://arxiv.org/pdf/1803.05223.pdf)',\n",
       "  'paper': {'title': 'MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge',\n",
       "   'url': 'https://paperswithcode.com/paper/mcscript-a-novel-dataset-for-assessing'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Natural Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MCScript'],\n",
       "  'num_papers': 20,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kp20k',\n",
       "  'name': 'KP20k',\n",
       "  'full_name': 'KP20k',\n",
       "  'homepage': 'https://github.com/memray/seq2seq-keyphrase',\n",
       "  'description': '**KP20k** is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing.\\r\\n\\r\\nSource: [Keyphrase Prediction With Pre-trained Language Model](https://arxiv.org/abs/2004.10462)\\r\\nImage Source: [https://arxiv.org/pdf/1704.06879.pdf](https://arxiv.org/pdf/1704.06879.pdf)',\n",
       "  'paper': {'title': 'Deep Keyphrase Generation',\n",
       "   'url': 'https://paperswithcode.com/paper/deep-keyphrase-generation'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Phrase Ranking',\n",
       "    'url': 'https://paperswithcode.com/task/phrase-ranking'},\n",
       "   {'task': 'Keyphrase Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/keyphrase-extraction'},\n",
       "   {'task': 'Phrase Tagging',\n",
       "    'url': 'https://paperswithcode.com/task/phrase-tagging'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['KP20k'],\n",
       "  'num_papers': 54,\n",
       "  'data_loaders': [{'url': 'https://github.com/memray/seq2seq-keyphrase',\n",
       "    'repo': 'https://github.com/memray/seq2seq-keyphrase',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/semantic-scholar',\n",
       "  'name': 'Semantic Scholar',\n",
       "  'full_name': 'Semantic Scholar',\n",
       "  'homepage': 'https://allenai.org/data/open-research-corpus',\n",
       "  'description': 'The **Semantic Scholar** corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps).\\r\\n\\r\\nSource: [Learning Dynamic Author Representations with Temporal Language Models](https://arxiv.org/abs/1909.04985)\\r\\nImage Source: [http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/](http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/)',\n",
       "  'paper': {'title': 'Construction of the Literature Graph in Semantic Scholar',\n",
       "   'url': 'https://paperswithcode.com/paper/construction-of-the-literature-graph-in'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Knowledge Graphs',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graphs'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Semantic Scholar'],\n",
       "  'num_papers': 44,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/evalution',\n",
       "  'name': 'EVALution',\n",
       "  'full_name': 'EVALution',\n",
       "  'homepage': 'https://github.com/esantus/EVALution',\n",
       "  'description': '**EVALution** dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives.\\r\\n\\r\\nSource: [Network Features Based Co-hyponymy Detection](https://arxiv.org/abs/1802.04609)\\r\\nImage Source: [https://www.aclweb.org/anthology/W15-4208.pdf](https://www.aclweb.org/anthology/W15-4208.pdf)',\n",
       "  'paper': {'title': 'EVALution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models',\n",
       "   'url': 'https://paperswithcode.com/paper/evalution-10-an-evolving-semantic-dataset-for'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Lexical Entailment',\n",
       "    'url': 'https://paperswithcode.com/task/lexical-entailment'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['EVALution'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': [{'url': 'https://github.com/esantus/EVALution',\n",
       "    'repo': 'https://github.com/esantus/EVALution',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/senseval-2-1',\n",
       "  'name': 'Senseval-2',\n",
       "  'full_name': 'Senseval-2',\n",
       "  'homepage': 'https://web.eecs.umich.edu/~mihalcea/senseval/',\n",
       "  'description': 'There are now many computer programs for automatically determining the sense of a word in context (Word Sense Disambiguation or WSD).  The purpose of SENSEVAL is to evaluate the strengths and weaknesses of such programs with respect to different words, different varieties of language, and different languages.\\r\\n\\r\\nSource: [SENSEVAL](http://www.hipposmond.com/senseval2/)\\r\\nImage Source: [http://www.itri.brighton.ac.uk/events/senseval/](http://www.itri.brighton.ac.uk/events/senseval/)',\n",
       "  'paper': {'title': 'SENSEVAL-2: Overview',\n",
       "   'url': 'https://www.aclweb.org/anthology/S01-1001/'},\n",
       "  'introduced_date': '2001-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Word Sense Disambiguation',\n",
       "    'url': 'https://paperswithcode.com/task/word-sense-disambiguation'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Senseval-2'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/robocup',\n",
       "  'name': 'RoboCup',\n",
       "  'full_name': 'RoboCup',\n",
       "  'homepage': 'https://www.robocup.org/',\n",
       "  'description': '**RoboCup** is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts.\\r\\n\\r\\nSource: [Robots as Actors in a Film: No War, A Robot Story](https://arxiv.org/abs/1910.12294)\\r\\nImage Source: [https://www.robocup.org/](https://www.robocup.org/)',\n",
       "  'paper': {'title': 'Learning to sportscast: a test of grounded language acquisition',\n",
       "   'url': 'https://doi.org/10.1145/1390156.1390173'},\n",
       "  'introduced_date': '2008-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Data-to-Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/data-to-text-generation'},\n",
       "   {'task': 'Knowledge Graphs',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graphs'}],\n",
       "  'languages': [],\n",
       "  'variants': ['RoboCup'],\n",
       "  'num_papers': 17,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sharc',\n",
       "  'name': 'ShARC',\n",
       "  'full_name': 'Shaping Answers with Rules through Conversation',\n",
       "  'homepage': 'https://sharc-data.github.io/',\n",
       "  'description': '**ShARC** is a Conversational Question Answering dataset focussing on question answering from texts containing rules.\\r\\n\\r\\nSource: [ShARC](https://sharc-data.github.io/data.html)\\r\\nImage Source: [https://arxiv.org/abs/1809.01494](https://arxiv.org/abs/1809.01494)',\n",
       "  'paper': {'title': 'Interpretation of Natural Language Rules in Conversational Machine Reading',\n",
       "   'url': 'https://paperswithcode.com/paper/interpretation-of-natural-language-rules-in'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'}],\n",
       "  'languages': [],\n",
       "  'variants': ['ShARC'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/sharc',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/social-iqa',\n",
       "  'name': 'Social IQA',\n",
       "  'full_name': 'Social Interaction QA',\n",
       "  'homepage': 'https://leaderboard.allenai.org/socialiqa/submissions/public',\n",
       "  'description': '**Social Interaction QA**, a new question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people’s actions and their social implications. For example, given an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do this?\", humans can easily infer that Jesse wanted \"to see their favorite performer\" or \"to enjoy the music\", and not \"to see what\\'s happening inside\" or \"to see if it works\". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models’ abilities to reason about the social implications of everyday events and situations.\\r\\n\\r\\nSource: [Social IQA](https://leaderboard.allenai.org/socialiqa/submissions/get-started)\\r\\nImage Source: [https://arxiv.org/pdf/1904.09728.pdf](https://arxiv.org/pdf/1904.09728.pdf)',\n",
       "  'paper': {'title': 'Social IQa: Commonsense Reasoning about Social Interactions',\n",
       "   'url': 'https://paperswithcode.com/paper/social-iqa-commonsense-reasoning-about-social'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Social IQA'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/social_i_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/olid',\n",
       "  'name': 'OLID',\n",
       "  'full_name': 'Offensive Language Identification Dataset',\n",
       "  'homepage': 'https://scholar.harvard.edu/malmasi/olid',\n",
       "  'description': 'The **OLID** is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019.\\r\\n\\r\\nSource: [Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection](https://arxiv.org/abs/2004.13432)\\r\\nImage Source: [https://arxiv.org/pdf/1902.09666.pdf](https://arxiv.org/pdf/1902.09666.pdf)',\n",
       "  'paper': {'title': 'Predicting the Type and Target of Offensive Posts in Social Media',\n",
       "   'url': 'https://paperswithcode.com/paper/predicting-the-type-and-target-of-offensive'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Language Identification',\n",
       "    'url': 'https://paperswithcode.com/task/language-identification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['OLID'],\n",
       "  'num_papers': 91,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/multi-news',\n",
       "  'name': 'Multi-News',\n",
       "  'full_name': 'Multi-News',\n",
       "  'homepage': 'https://github.com/Alex-Fabbri/Multi-News',\n",
       "  'description': '**Multi-News**, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited.\\r\\n\\r\\nSource: [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749.pdf)\\r\\nImage Source: [https://arxiv.org/pdf/1906.01749.pdf](https://arxiv.org/pdf/1906.01749.pdf)',\n",
       "  'paper': {'title': 'Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-news-a-large-scale-multi-document'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/text-summarization'},\n",
       "   {'task': 'Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/document-summarization'},\n",
       "   {'task': 'Multi-Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/multi-document-summarization'},\n",
       "   {'task': 'Cross-Document Language Modeling',\n",
       "    'url': 'https://paperswithcode.com/task/cross-document-language-modeling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MultiNews val', 'MultiNews test', 'Multi-News'],\n",
       "  'num_papers': 45,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/multi_news',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/multi_news',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/Alex-Fabbri/Multi-News',\n",
       "    'repo': 'https://github.com/Alex-Fabbri/Multi-News',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/cloth',\n",
       "  'name': 'CLOTH',\n",
       "  'full_name': 'CLOze test by TeacHers',\n",
       "  'homepage': 'https://www.cs.cmu.edu/~glai1/data/cloth/',\n",
       "  'description': 'The Cloze Test by Teachers (**CLOTH**) benchmark is a collection of nearly 100,000 4-way multiple-choice cloze-style questions from middle- and high school-level English language exams, where the answer fills a blank in a given text. Each question is labeled with a type of deep reasoning it involves, where the four possible types are grammar, short-term reasoning, matching/paraphrasing, and long-term reasoning, i.e., reasoning over multiple sentences\\n\\nSource: [Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches](https://arxiv.org/abs/1904.01172)\\nImage Source: [https://arxiv.org/pdf/1711.03225.pdf](https://arxiv.org/pdf/1711.03225.pdf)',\n",
       "  'paper': {'title': 'Large-scale Cloze Test Dataset Created by Teachers',\n",
       "   'url': 'https://paperswithcode.com/paper/large-scale-cloze-test-dataset-created-by'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CLOTH'],\n",
       "  'num_papers': 10,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cosmosqa',\n",
       "  'name': 'CosmosQA',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://wilburone.github.io/cosmos/',\n",
       "  'description': 'CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people’s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context.\\r\\n\\r\\nSource: [Teaching Pretrained Models with Commonsense Reasoning: A Preliminary KB-Based Approach](https://arxiv.org/abs/1909.09743)\\r\\nImage Source: [https://arxiv.org/pdf/1909.00277.pdf](https://arxiv.org/pdf/1909.00277.pdf)',\n",
       "  'paper': {'title': 'Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/cosmos-qa-machine-reading-comprehension-with'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Natural Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/natural-language-understanding'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['CosmosQA'],\n",
       "  'num_papers': 43,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/cosmos_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/cosmos_qa',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/winobias',\n",
       "  'name': 'WinoBias',\n",
       "  'full_name': 'WinoBias',\n",
       "  'homepage': 'https://uclanlp.github.io/corefBias/overview',\n",
       "  'description': '**WinoBias** contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.\\r\\n\\r\\nSource: [WinoBias](https://uclanlp.github.io/corefBias/overview)\\r\\nImage Source: [https://uclanlp.github.io/corefBias/overview](https://uclanlp.github.io/corefBias/overview)',\n",
       "  'paper': {'title': 'Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods',\n",
       "   'url': 'https://paperswithcode.com/paper/gender-bias-in-coreference-resolution-1'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Coreference Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/coreference-resolution'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'},\n",
       "   {'task': 'Fairness', 'url': 'https://paperswithcode.com/task/fairness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WinoBias'],\n",
       "  'num_papers': 49,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/wino_bias',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://docs.allennlp.org/models/main/models/coref/dataset_readers/winobias/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/spades',\n",
       "  'name': 'Spades',\n",
       "  'full_name': 'Semantic PArsing of DEclarative Sentences',\n",
       "  'homepage': 'https://github.com/sivareddyg/graph-parser',\n",
       "  'description': 'Datasets **Spades** contains 93,319 questions derived from clueweb09 sentences. Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs.\\n\\nSource: [Learning an Executable Neural Semantic Parser](https://arxiv.org/abs/1711.05066)\\nImage Source: [https://github.com/sivareddyg/graph-parser/blob/master/data/spades/results/graphparser-ccg-supervised-dev.txt](https://github.com/sivareddyg/graph-parser/blob/master/data/spades/results/graphparser-ccg-supervised-dev.txt)',\n",
       "  'paper': {'title': 'Evaluating Induced CCG Parsers on Grounded Semantic Parsing',\n",
       "   'url': 'https://paperswithcode.com/paper/evaluating-induced-ccg-parsers-on-grounded'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Semantic Parsing',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-parsing'},\n",
       "   {'task': 'Slot Filling',\n",
       "    'url': 'https://paperswithcode.com/task/slot-filling'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Spades'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/sivareddyg/graph-parser',\n",
       "    'repo': 'https://github.com/sivareddyg/graph-parser',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/wikisum',\n",
       "  'name': 'WikiSum',\n",
       "  'full_name': 'WikiSum',\n",
       "  'homepage': 'https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum',\n",
       "  'description': '**WikiSum** is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively.\\r\\n\\r\\nSource: [Generating Wikipedia by Summarizing Long Sequences](https://arxiv.org/pdf/1801.10198.pdf)\\r\\nImage Source: [https://arxiv.org/pdf/1801.10198.pdf](https://arxiv.org/pdf/1801.10198.pdf)',\n",
       "  'paper': {'title': 'Generating Wikipedia by Summarizing Long Sequences',\n",
       "   'url': 'https://paperswithcode.com/paper/generating-wikipedia-by-summarizing-long'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Abstractive Text Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/abstractive-text-summarization'},\n",
       "   {'task': 'Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/document-summarization'},\n",
       "   {'task': 'Multi-Document Summarization',\n",
       "    'url': 'https://paperswithcode.com/task/multi-document-summarization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['WikiSum'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://github.com/tensorflow/tensor2tensor',\n",
       "    'repo': 'https://github.com/tensorflow/tensor2tensor',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/drcd',\n",
       "  'name': 'DRCD',\n",
       "  'full_name': 'Delta Reading Comprehension Dataset',\n",
       "  'homepage': 'https://github.com/DRCKnowledgeTeam/DRCD',\n",
       "  'description': 'Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators.\\r\\n\\r\\nSource: [https://github.com/DRCKnowledgeTeam/DRCD](https://github.com/DRCKnowledgeTeam/DRCD)\\r\\nImage Source: [https://arxiv.org/pdf/1806.00920.pdf](https://arxiv.org/pdf/1806.00920.pdf)',\n",
       "  'paper': {'title': 'DRCD: a Chinese Machine Reading Comprehension Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/drcd-a-chinese-machine-reading-comprehension'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Chinese Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-reading-comprehension'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Reading Comprehension (Zero-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-zero-shot'},\n",
       "   {'task': 'Reading Comprehension (One-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-one-shot'},\n",
       "   {'task': 'Reading Comprehension (Few-Shot)',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension-few-shot'},\n",
       "   {'task': 'Machine Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/machine-reading-comprehension'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['DRCD (Traditional Chinese) Dev',\n",
       "   'DRCD (Traditional Chinese)',\n",
       "   'DRCD'],\n",
       "  'num_papers': 36,\n",
       "  'data_loaders': [{'url': 'https://github.com/DRCKnowledgeTeam/DRCD',\n",
       "    'repo': 'https://github.com/DRCKnowledgeTeam/DRCD',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/emotionlines',\n",
       "  'name': 'EmotionLines',\n",
       "  'full_name': 'EmotionLines',\n",
       "  'homepage': 'http://doraemon.iis.sinica.edu.tw/emotionlines/download.html',\n",
       "  'description': '**EmotionLines** contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman’s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral.\\r\\n\\r\\nSource: [Bridging Dialogue Generation and Facial Expression Synthesis](https://arxiv.org/abs/1905.11240)\\r\\nImage Source: [https://arxiv.org/pdf/1802.08379.pdf](https://arxiv.org/pdf/1802.08379.pdf)',\n",
       "  'paper': {'title': 'EmotionLines: An Emotion Corpus of Multi-Party Conversations',\n",
       "   'url': 'https://paperswithcode.com/paper/emotionlines-an-emotion-corpus-of-multi-party'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition'},\n",
       "   {'task': 'Emotion Recognition in Conversation',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition-in-conversation'},\n",
       "   {'task': 'Emotion Classification',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-classification'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['EmotionPush', 'EmotionLines'],\n",
       "  'num_papers': 31,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/chinese-gigaword',\n",
       "  'name': 'Chinese Gigaword',\n",
       "  'full_name': 'Chinese Gigaword',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC2011T13',\n",
       "  'description': '**Chinese Gigaword** corpus consists of 2.2M of headline-document pairs of news stories covering over 284 months from two Chinese newspapers, namely the Xinhua News Agency of China (XIN) and the Central News Agency of Taiwan (CNA).\\r\\n\\r\\nSource: [Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification](https://arxiv.org/abs/1709.05475)\\r\\nImage Source: [https://catalog.ldc.upenn.edu/desc/addenda/LDC2011T13.jpg](https://catalog.ldc.upenn.edu/desc/addenda/LDC2011T13.jpg)',\n",
       "  'paper': {'title': 'LDC Catalog No.: LDC2003T09, ISBN',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC2011T13'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'},\n",
       "   {'task': 'Decipherment',\n",
       "    'url': 'https://paperswithcode.com/task/decipherment'}],\n",
       "  'languages': ['Chinese'],\n",
       "  'variants': ['Chinese Gigaword'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/celex',\n",
       "  'name': 'CELEX',\n",
       "  'full_name': 'CELEX',\n",
       "  'homepage': 'https://catalog.ldc.upenn.edu/LDC96L14',\n",
       "  'description': '**CELEX** database comprises three different searchable lexical databases, Dutch, English and German. The lexical data contained in each database is divided into five categories: orthography, phonology, morphology, syntax (word class) and word frequency.\\r\\n\\r\\nSource: [Polysemy and Brevity versus Frequency in Language](https://arxiv.org/abs/1904.00812)\\r\\nImage Source: [https://www.aclweb.org/anthology/W17-7619.pdf](https://www.aclweb.org/anthology/W17-7619.pdf)',\n",
       "  'paper': {'title': 'The CELEX lexical database',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC96L14'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Multi-Task Learning',\n",
       "    'url': 'https://paperswithcode.com/task/multi-task-learning'},\n",
       "   {'task': 'Morphological Analysis',\n",
       "    'url': 'https://paperswithcode.com/task/morphological-analysis'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English', 'German', 'Dutch'],\n",
       "  'variants': ['CELEX'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/must-c',\n",
       "  'name': 'MuST-C',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://ict.fbk.eu/must-c/',\n",
       "  'description': '**MuST-C** currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split.\\r\\n\\r\\nSource: [One-to-Many Multilingual End-to-End Speech Translation](https://arxiv.org/abs/1910.03320)\\r\\nImage Source: [https://ict.fbk.eu/must-c/](https://ict.fbk.eu/must-c/)',\n",
       "  'paper': {'title': 'MuST-C: a Multilingual Speech Translation Corpus',\n",
       "   'url': 'https://paperswithcode.com/paper/must-c-a-multilingual-speech-translation'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Speech Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/speech-recognition'},\n",
       "   {'task': 'Speech-to-Text Translation',\n",
       "    'url': 'https://paperswithcode.com/task/speech-to-text-translation'},\n",
       "   {'task': 'Data Augmentation',\n",
       "    'url': 'https://paperswithcode.com/task/data-augmentation'}],\n",
       "  'languages': ['French',\n",
       "   'Spanish',\n",
       "   'German',\n",
       "   'Italian',\n",
       "   'Chinese',\n",
       "   'Russian',\n",
       "   'Portuguese',\n",
       "   'Arabic',\n",
       "   'Czech',\n",
       "   'Dutch',\n",
       "   'Persian',\n",
       "   'Romanian',\n",
       "   'Turkish',\n",
       "   'Vietnamese'],\n",
       "  'variants': ['MuST-C EN->DE', 'MuST-C'],\n",
       "  'num_papers': 95,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/who-did-what',\n",
       "  'name': 'Who-did-What',\n",
       "  'full_name': 'Who did What',\n",
       "  'homepage': 'https://tticnlp.github.io/who_did_what/',\n",
       "  'description': '**Who-did-What** collects its corpus from news and provides options for questions similar to CBT. Each question is formed from two independent articles: an article is treated as context to be read and a separate article on the same event is used to form the query.\\r\\n\\r\\nSource: [ChID: A Large-scale Chinese IDiom Dataset for Cloze Test](https://arxiv.org/abs/1906.01265)\\r\\nImage Source: [https://tticnlp.github.io/who_did_what/sample.html](https://tticnlp.github.io/who_did_what/sample.html)',\n",
       "  'paper': {'title': 'Who did What: A Large-Scale Person-Centered Cloze Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/who-did-what-a-large-scale-person-centered'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Machine Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/machine-reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Who-did-What'],\n",
       "  'num_papers': 14,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/metaqa',\n",
       "  'name': 'MetaQA',\n",
       "  'full_name': 'MoviE Text Audio QA',\n",
       "  'homepage': 'https://github.com/yuyuz/MetaQA',\n",
       "  'description': 'The **MetaQA** dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries.\\r\\n\\r\\nSource: [https://arxiv.org/abs/1907.08176](https://arxiv.org/abs/1907.08176)\\r\\nImage Source: [https://github.com/yuyuz/MetaQA](https://github.com/yuyuz/MetaQA)',\n",
       "  'paper': {'title': 'Variational Reasoning for Question Answering with Knowledge Graph',\n",
       "   'url': 'https://paperswithcode.com/paper/variational-reasoning-for-question-answering'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Knowledge Graphs',\n",
       "    'url': 'https://paperswithcode.com/task/knowledge-graphs'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MetaQA'],\n",
       "  'num_papers': 33,\n",
       "  'data_loaders': [{'url': 'https://github.com/yuyuz/MetaQA',\n",
       "    'repo': 'https://github.com/yuyuz/MetaQA',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/fakenewsnet',\n",
       "  'name': 'FakeNewsNet',\n",
       "  'full_name': 'FakeNewsNet',\n",
       "  'homepage': 'https://github.com/KaiDMML/FakeNewsNet',\n",
       "  'description': '**FakeNewsNet** is collected from two fact-checking websites: GossipCop and PolitiFact containing news contents with labels annotated by professional journalists and experts, along with social context information.\\r\\n\\r\\nSource: [Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News](https://arxiv.org/abs/2004.01732)\\r\\nImage Source: [https://arxiv.org/pdf/1809.01286.pdf](https://arxiv.org/pdf/1809.01286.pdf)',\n",
       "  'paper': {'title': 'FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media',\n",
       "   'url': 'https://paperswithcode.com/paper/fakenewsnet-a-data-repository-with-news'},\n",
       "  'introduced_date': '2018-09-05',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Fake News Detection',\n",
       "    'url': 'https://paperswithcode.com/task/fake-news-detection'},\n",
       "   {'task': 'Misinformation',\n",
       "    'url': 'https://paperswithcode.com/task/misinformation'},\n",
       "   {'task': 'News Generation',\n",
       "    'url': 'https://paperswithcode.com/task/news-generation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['FakeNewsNet'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': [{'url': 'https://github.com/KaiDMML/FakeNewsNet',\n",
       "    'repo': 'https://github.com/KaiDMML/FakeNewsNet',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/sts-2014',\n",
       "  'name': 'STS 2014',\n",
       "  'full_name': 'STS 2014',\n",
       "  'homepage': 'https://alt.qcri.org/semeval2014/task10/',\n",
       "  'description': 'STS-2014 is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes.\\r\\n\\r\\nSource: [Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering](https://arxiv.org/abs/1806.04330)\\r\\nImage Source: [https://www.aclweb.org/anthology/S14-2010.pdf](https://www.aclweb.org/anthology/S14-2010.pdf)',\n",
       "  'paper': {'title': 'SemEval-2014 Task 10: Multilingual Semantic Textual Similarity',\n",
       "   'url': 'https://paperswithcode.com/paper/semeval-2014-task-10-multilingual-semantic'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Semantic Textual Similarity',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-textual-similarity'},\n",
       "   {'task': 'Sentence Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-embeddings'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English', 'Spanish'],\n",
       "  'variants': ['STS 2014'],\n",
       "  'num_papers': 23,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/media',\n",
       "  'name': 'MEDIA',\n",
       "  'full_name': 'MEDIA',\n",
       "  'homepage': 'http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf',\n",
       "  'description': 'The **MEDIA** French corpus is dedicated to semantic extraction from speech in a context of human/machine dialogues. The corpus has manual transcription and conceptual annotation of dialogues from 250 speakers. It is split into the following three parts : (1) the training set (720 dialogues, 12K sentences), (2) the development set (79 dialogues, 1.3K sentences, and (3) the test set (200 dialogues, 3K sentences).\\n\\nSource: [Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems](https://arxiv.org/abs/2002.06012)\\nImage Source: [http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf](http://www.lrec-conf.org/proceedings/lrec2004/pdf/356.pdf)',\n",
       "  'paper': {'title': 'The French MEDIA/EVALDA Project: the Evaluation of the Understanding Capability of Spoken Language Dialogue Systems',\n",
       "   'url': 'http://www.lrec-conf.org/proceedings/lrec2004/summaries/356.htm'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Audio'],\n",
       "  'tasks': [{'task': 'Named Entity Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/named-entity-recognition-ner'},\n",
       "   {'task': 'Spoken Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/spoken-language-understanding'},\n",
       "   {'task': 'Slot Filling',\n",
       "    'url': 'https://paperswithcode.com/task/slot-filling'}],\n",
       "  'languages': ['English', 'French'],\n",
       "  'variants': ['MEDIA'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/aspec',\n",
       "  'name': 'ASPEC',\n",
       "  'full_name': 'Asian Scientific Paper Excerpt Corpus',\n",
       "  'homepage': 'http://lotus.kuee.kyoto-u.ac.jp/ASPEC/',\n",
       "  'description': '**ASPEC**, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010.\\r\\n\\r\\nSource: [ASPEC](http://lotus.kuee.kyoto-u.ac.jp/ASPEC/)\\r\\nImage Source: [https://www.aclweb.org/anthology/L16-1350.pdf](https://www.aclweb.org/anthology/L16-1350.pdf)',\n",
       "  'paper': {'title': 'ASPEC: Asian Scientific Paper Excerpt Corpus',\n",
       "   'url': 'https://paperswithcode.com/paper/aspec-asian-scientific-paper-excerpt-corpus'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Domain Adaptation',\n",
       "    'url': 'https://paperswithcode.com/task/domain-adaptation'},\n",
       "   {'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Low-Resource Neural Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/low-resource-neural-machine-translation'}],\n",
       "  'languages': ['English', 'Chinese', 'Japanese'],\n",
       "  'variants': ['ASPEC'],\n",
       "  'num_papers': 77,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/omics',\n",
       "  'name': 'OMICS',\n",
       "  'full_name': 'Open Mind Indoor Common Sense',\n",
       "  'homepage': 'https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf',\n",
       "  'description': '**OMICS** is an extensive collection of knowledge for indoor service robots gathered from internet users. Currently, it contains 48 tables capturing different sorts of knowledge. Each tuple of the Help table maps a user desire to a task that may meet the desire (e.g., ⟨ “feel thirsty”, “by offering drink” ⟩). Each tuple of the Tasks/Steps table decomposes a task into several steps (e.g., ⟨ “serve a drink”, 0. “get a glass”, 1. “get a bottle”, 2. “fill class from bottle”, 3. “give class to person” ⟩). Given this, OMICS offers useful knowledge about hierarchism of naturalistic instructions, where a high-level user request (e.g., “serve a drink”) can be reduced to lower-level tasks (e.g., “get a glass”, ⋯). Another feature of OMICS is that elements of any tuple in an OMICS table are semantically related according to a predefined template. This facilitates the semantic interpretation of the OMICS tuples.\\n\\nSource: [Understanding User Instructions by Utilizing Open Knowledge for Service Robots](https://arxiv.org/abs/1606.02877)\\nImage Source: [https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf](https://www.aaai.org/Papers/AAAI/2004/AAAI04-096.pdf)',\n",
       "  'paper': {'title': 'Common Sense Data Acquisition for Indoor Mobile Robots',\n",
       "   'url': 'http://www.aaai.org/Library/AAAI/2004/aaai04-096.php'},\n",
       "  'introduced_date': '2004-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'},\n",
       "   {'task': 'Decision Making',\n",
       "    'url': 'https://paperswithcode.com/task/decision-making'}],\n",
       "  'languages': [],\n",
       "  'variants': ['OMICS'],\n",
       "  'num_papers': 6,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/quasar-1',\n",
       "  'name': 'QUASAR',\n",
       "  'full_name': 'QUestion Answering by Search And Reading',\n",
       "  'homepage': 'https://github.com/bdhingra/quasar',\n",
       "  'description': 'The Question Answering by Search And Reading (**QUASAR**) is a large-scale dataset consisting of [QUASAR-S](quasar-s) and [QUASAR-T](quasar-t). Each of these datasets is built to focus on evaluating systems devised to understand a natural language query, a large corpus of texts and to extract an answer to the question from the corpus. Specifically, QUASAR-S comprises 37,012 fill-in-the-gaps questions that are collected from the popular website Stack Overflow using entity tags. The QUASAR-T dataset contains 43,012 open-domain questions collected from various internet sources. The candidate documents for each question in this dataset are retrieved from an Apache Lucene based search engine built on top of the ClueWeb09 dataset.\\r\\n\\r\\nSource: [MRNN: A Multi-Resolution Neural Network with Duplex Attention for Document Retrieval in the Context of Question Answering](https://arxiv.org/abs/1911.00964)\\r\\nImage Source: [https://arxiv.org/pdf/1707.03904.pdf](https://arxiv.org/pdf/1707.03904.pdf)',\n",
       "  'paper': {'title': 'Quasar: Datasets for Question Answering by Search and Reading',\n",
       "   'url': 'https://paperswithcode.com/paper/quasar-datasets-for-question-answering-by'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Open-Domain Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/open-domain-question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Quasar', 'QUASAR'],\n",
       "  'num_papers': 34,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/sagnikrayc/quasar',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'https://github.com/bdhingra/quasar',\n",
       "    'repo': 'https://github.com/bdhingra/quasar',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/dialogue-state-tracking-challenge',\n",
       "  'name': 'Dialogue State Tracking Challenge',\n",
       "  'full_name': 'Dialogue State Tracking Challenge',\n",
       "  'homepage': 'https://github.com/matthen/dstc',\n",
       "  'description': \"The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog.\\r\\nIn these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period.\\r\\n\\r\\nThe corpus was collected using Amazon Mechanical Turk, and consists of dialogs in two domains: restaurant information, and tourist information. Tourist information subsumes restaurant information, and includes bars, cafés etc. as well as multiple new slots. There were two rounds of evaluation using this data:\\r\\n\\r\\nDSTC 2 released a large number of training dialogs related to restaurant search. Compared to DSTC (which was in the bus timetables domain), DSTC 2 introduces changing user goals, tracking 'requested slots' as well as the new restaurants domain. Results from DSTC 2 were presented at SIGDIAL 2014.\\r\\nDSTC 3 addressed the problem of adaption to a new domain - tourist information. DSTC 3 releases a small amount of labelled data in the tourist information domain; participants will use this data plus the restaurant data from DSTC 2 for training.\\r\\nDialogs used for training are fully labelled; user transcriptions, user dialog-act semantics and dialog state are all annotated. (This corpus therefore is also suitable for studies in Spoken Language Understanding.)\\r\\n\\r\\nSource: [https://github.com/matthen/dstc](https://github.com/matthen/dstc)\\r\\nImage Source: [https://www.aclweb.org/anthology/W13-4065.pdf](https://www.aclweb.org/anthology/W13-4065.pdf)\",\n",
       "  'paper': {'title': 'The Dialog State Tracking Challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/the-dialog-state-tracking-challenge'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts', 'Dialog'],\n",
       "  'tasks': [{'task': 'Spoken Language Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/spoken-language-understanding'},\n",
       "   {'task': 'Deblurring', 'url': 'https://paperswithcode.com/task/deblurring'},\n",
       "   {'task': 'Dialogue State Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/dialogue-state-tracking'},\n",
       "   {'task': 'Spoken Dialogue Systems',\n",
       "    'url': 'https://paperswithcode.com/task/spoken-dialogue-systems'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Second dialogue state tracking challenge',\n",
       "   'Dialogue State Tracking Challenge'],\n",
       "  'num_papers': 26,\n",
       "  'data_loaders': [{'url': 'https://github.com/matthen/dstc',\n",
       "    'repo': 'https://github.com/matthen/dstc',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/isear',\n",
       "  'name': 'ISEAR',\n",
       "  'full_name': 'International Survey on Emotion Antecedents and Reactions',\n",
       "  'homepage': 'https://www.unige.ch/cisa/research/materials-and-online-research/research-material/',\n",
       "  'description': 'Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the **ISEAR** project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents.\\r\\n\\r\\nSource: [https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)\\r\\nImage Source: [https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)',\n",
       "  'paper': {'title': 'Evidence for universality and cultural variation of differential emotion response patterning',\n",
       "   'url': 'https://doi.org/10.1037/0022-3514.67.1.55'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Emotion Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-recognition'},\n",
       "   {'task': 'Emotion Classification',\n",
       "    'url': 'https://paperswithcode.com/task/emotion-classification'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ISEAR'],\n",
       "  'num_papers': 29,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/cmrc',\n",
       "  'name': 'CMRC',\n",
       "  'full_name': 'Chinese Machine Reading Comprehension 2018',\n",
       "  'homepage': 'https://github.com/ymcui/cmrc2018',\n",
       "  'description': 'CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues.\\r\\n\\r\\nSource: [A Span-Extraction Dataset for Chinese Machine Reading Comprehension](https://www.aclweb.org/anthology/D19-1600.pdf)\\r\\nImage Source: [https://www.aclweb.org/anthology/D19-1600.pdf](https://www.aclweb.org/anthology/D19-1600.pdf)',\n",
       "  'paper': {'title': 'A Span-Extraction Dataset for Chinese Machine Reading Comprehension',\n",
       "   'url': 'https://paperswithcode.com/paper/a-span-extraction-dataset-for-chinese-machine'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Chinese Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/chinese-reading-comprehension'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'},\n",
       "   {'task': 'Machine Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/machine-reading-comprehension'}],\n",
       "  'languages': ['English', 'Chinese'],\n",
       "  'variants': ['CMRC 2018 (Simplified Chinese)', 'CMRC'],\n",
       "  'num_papers': 42,\n",
       "  'data_loaders': [{'url': 'https://github.com/ymcui/cmrc2018',\n",
       "    'repo': 'https://github.com/ymcui/cmrc2018',\n",
       "    'frameworks': ['tf']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pubmed-rct',\n",
       "  'name': 'PubMed RCT',\n",
       "  'full_name': 'PubMed 200k RCT',\n",
       "  'homepage': 'https://github.com/Franck-Dernoncourt/pubmed-rct',\n",
       "  'description': '**PubMed 200k RCT** is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.\\r\\n\\r\\nSource: [GitHub](https://github.com/Franck-Dernoncourt/pubmed-rct)\\nImage Source: [https://arxiv.org/pdf/1710.06071.pdf](https://arxiv.org/pdf/1710.06071.pdf)',\n",
       "  'paper': {'title': 'PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts',\n",
       "   'url': 'https://paperswithcode.com/paper/pubmed-200k-rct-a-dataset-for-sequential'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Classification',\n",
       "    'url': 'https://paperswithcode.com/task/text-classification'},\n",
       "   {'task': 'Sentence Classification',\n",
       "    'url': 'https://paperswithcode.com/task/sentence-classification'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PubMed RCT'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': [{'url': 'https://github.com/Franck-Dernoncourt/pubmed-rct',\n",
       "    'repo': 'https://github.com/Franck-Dernoncourt/pubmed-rct',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/nsides',\n",
       "  'name': 'NSIDES',\n",
       "  'full_name': 'Offsides and Twosides (NSIDES v0.1)',\n",
       "  'homepage': 'http://tatonettilab.org/offsides/',\n",
       "  'description': 'Drug side effects and drug-drug interactions were mined from publicly available data. Offsides is a database of drug side-effects that were found, but are not listed on the official FDA label. Twosides is the only comprehensive database drug-drug-effect relationships. Over 3,300 drugs and 63,000 combinations connected to millions of potential adverse reactions.\\n\\nSource: [http://tatonettilab.org/offsides/](http://tatonettilab.org/offsides/)\\nImage Source: [http://doi.org/10.1126/scitranslmed.3003377](http://doi.org/10.1126/scitranslmed.3003377)',\n",
       "  'paper': {'title': 'Data-driven prediction of drug effects and interactions',\n",
       "   'url': 'http://doi.org/10.1126/scitranslmed.3003377'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['NSIDES'],\n",
       "  'num_papers': 0,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/ddi',\n",
       "  'name': 'DDI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://www.cs.york.ac.uk/semeval-2013/task9/',\n",
       "  'description': 'The **DDI**Extraction 2013 task relies on the DDI corpus which contains MedLine abstracts on drug-drug interactions as well as documents describing drug-drug interactions from the DrugBank database.\\r\\n\\r\\nSource: [DDIExtraction 2013](https://www.cs.york.ac.uk/semeval-2013/task9/)\\r\\nImage Source: [https://www.aclweb.org/anthology/S13-2056.pdf](https://www.aclweb.org/anthology/S13-2056.pdf)',\n",
       "  'paper': {'title': 'Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)',\n",
       "   'url': 'https://www.aclweb.org/anthology/S13-2056.pdf'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/relation-extraction'},\n",
       "   {'task': 'Medical Relation Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/medical-relation-extraction'},\n",
       "   {'task': 'Drug–drug Interaction Extraction',\n",
       "    'url': 'https://paperswithcode.com/task/drug-drug-interaction-extraction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['DDI extraction 2013 corpus', 'DDI'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/stylized-imagenet',\n",
       "  'name': 'Stylized ImageNet',\n",
       "  'full_name': 'Stylized ImageNet',\n",
       "  'homepage': 'https://github.com/rgeirhos/Stylized-ImageNet',\n",
       "  'description': 'The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures.\\r\\n\\r\\nSource: [Adversarial Examples Improve Image Recognition](https://arxiv.org/abs/1911.09665)\\r\\nImage Source: [https://github.com/rgeirhos/Stylized-ImageNet](https://github.com/rgeirhos/Stylized-ImageNet)',\n",
       "  'paper': {'title': 'ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness',\n",
       "   'url': 'https://paperswithcode.com/paper/imagenet-trained-cnns-are-biased-towards'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/image-classification'},\n",
       "   {'task': 'Domain Generalization',\n",
       "    'url': 'https://paperswithcode.com/task/domain-generalization'},\n",
       "   {'task': 'Adversarial Robustness',\n",
       "    'url': 'https://paperswithcode.com/task/adversarial-robustness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stylized ImageNet'],\n",
       "  'num_papers': 62,\n",
       "  'data_loaders': [{'url': 'https://github.com/rgeirhos/Stylized-ImageNet',\n",
       "    'repo': 'https://github.com/rgeirhos/Stylized-ImageNet',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/mutual',\n",
       "  'name': 'MuTual',\n",
       "  'full_name': 'MuTual',\n",
       "  'homepage': 'https://github.com/Nealcly/MuTual',\n",
       "  'description': '**MuTual** is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction.\\n\\nSource: [https://github.com/Nealcly/MuTual](https://github.com/Nealcly/MuTual)\\nImage Source: [https://github.com/Nealcly/MuTual](https://github.com/Nealcly/MuTual)',\n",
       "  'paper': {'title': 'MuTual: A Dataset for Multi-Turn Dialogue Reasoning',\n",
       "   'url': 'https://paperswithcode.com/paper/mutual-a-dataset-for-multi-turn-dialogue'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Text Generation',\n",
       "    'url': 'https://paperswithcode.com/task/text-generation'},\n",
       "   {'task': 'Task-Oriented Dialogue Systems',\n",
       "    'url': 'https://paperswithcode.com/task/task-oriented-dialogue-systems'},\n",
       "   {'task': 'Machine Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/machine-reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['MuTual'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': [{'url': 'https://github.com/Nealcly/MuTual',\n",
       "    'repo': 'https://github.com/Nealcly/MuTual',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/crim13',\n",
       "  'name': 'CRIM13',\n",
       "  'full_name': 'Caltech Resident-Intruder Mouse 13',\n",
       "  'homepage': 'https://pdollar.github.io/research.html',\n",
       "  'description': 'The Caltech Resident-Intruder Mouse dataset (**CRIM13**) consists of 237x2 videos (recorded with synchronized top and side view) of pairs of mice engaging in social behavior, catalogued into thirteen different actions. Each video lasts ~10min, for a total of 88 hours of video and 8 million frames. A team of behavior experts annotated each video on a frame-by-frame basis for a state-of-the-art study of the neurophysiological mechanisms involved in aggression and courtship in mice.\\n\\nSource: [https://pdollar.github.io/research.html](https://pdollar.github.io/research.html)\\nImage Source: [https://authors.library.caltech.edu/104600/1/2020.07.26.222299v1.full.pdf](https://authors.library.caltech.edu/104600/1/2020.07.26.222299v1.full.pdf)',\n",
       "  'paper': None,\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['CRIM13'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagewoof',\n",
       "  'name': 'Imagewoof',\n",
       "  'full_name': 'Imagewoof',\n",
       "  'homepage': 'https://github.com/fastai/imagenette',\n",
       "  'description': '**Imagewoof** is a subset of 10 dog breed classes from Imagenet. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog.\\n\\nSource: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette)\\nImage Source: [https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c](https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c)',\n",
       "  'paper': {'title': 'fastai: A Layered API for Deep Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/fastai-a-layered-api-for-deep-learning'},\n",
       "  'introduced_date': '2020-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['Imagewoof'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/fastai/imagenette',\n",
       "    'repo': 'https://github.com/fastai/imagenette',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/imagenette',\n",
       "  'name': 'Imagenette',\n",
       "  'full_name': 'Imagenette',\n",
       "  'homepage': 'https://github.com/fastai/imagenette',\n",
       "  'description': '**Imagenette** is a subset of 10 easily classified classes from Imagenet (bench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\\n\\nSource: [https://github.com/fastai/imagenette](https://github.com/fastai/imagenette)\\nImage Source: [https://docs.fast.ai/tutorial.imagenette.html](https://docs.fast.ai/tutorial.imagenette.html)',\n",
       "  'paper': {'title': 'fastai: A Layered API for Deep Learning',\n",
       "   'url': 'https://paperswithcode.com/paper/fastai-a-layered-api-for-deep-learning'},\n",
       "  'introduced_date': '2020-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': ['English', 'French'],\n",
       "  'variants': ['Imagenette'],\n",
       "  'num_papers': 5,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/imagenette',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://github.com/fastai/imagenette',\n",
       "    'repo': 'https://github.com/fastai/imagenette',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/stanford-ecm',\n",
       "  'name': 'Stanford-ECM',\n",
       "  'full_name': 'Stanford-ECM',\n",
       "  'homepage': 'http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html',\n",
       "  'description': '**Stanford-ECM** is an egocentric multimodal dataset which comprises about 27 hours of egocentric video augmented with heart rate and acceleration data. The lengths of the individual videos cover a diverse range from 3 minutes to about 51 minutes in length. A mobile phone was used to collect egocentric video at 720x1280 resolution and 30 fps, as well as triaxial acceleration at 30Hz. The mobile phone was equipped with a wide-angle lens, so that the horizontal field of view was enlarged from 45 degrees to about 64 degrees. A wrist-worn heart rate sensor was used to capture the heart rate every 5 seconds. The phone and heart rate monitor was time-synchronized through Bluetooth, and all data was stored in the phone’s storage. Piecewise cubic polynomial interpolation was used to fill in any gaps in heart rate data. Finally, data was aligned to the millisecond level at 30 Hz.\\n\\nSource: [http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html](http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html)\\nImage Source: [http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html](http://ai.stanford.edu/~syyeung/ecm_dataset/egocentric_multimodal.html)',\n",
       "  'paper': {'title': 'Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals',\n",
       "   'url': 'https://paperswithcode.com/paper/jointly-learning-energy-expenditures-and'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'Audio'],\n",
       "  'tasks': [{'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'},\n",
       "   {'task': 'Video Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/video-understanding'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Stanford-ECM'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/bsd',\n",
       "  'name': 'BSD',\n",
       "  'full_name': 'Berkeley Segmentation Dataset',\n",
       "  'homepage': 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/',\n",
       "  'description': '**BSD** is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.\\r\\n\\r\\nSource: [A Deep Journey into Super-resolution: A Survey](https://arxiv.org/abs/1904.07523)\\r\\nImage Source: [https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015](https://www.slideshare.net/jbhuang/single-image-super-resolution-from-transformed-selfexemplars-cvpr-2015)',\n",
       "  'paper': {'title': 'A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics',\n",
       "   'url': 'http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=937655'},\n",
       "  'introduced_date': '2001-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'},\n",
       "   {'task': 'Color Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/color-image-denoising'},\n",
       "   {'task': 'Grayscale Image Denoising',\n",
       "    'url': 'https://paperswithcode.com/task/grayscale-image-denoising'},\n",
       "   {'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Salt-And-Pepper Noise Removal',\n",
       "    'url': 'https://paperswithcode.com/task/salt-and-pepper-noise-removal'},\n",
       "   {'task': 'Compressive Sensing',\n",
       "    'url': 'https://paperswithcode.com/task/compressive-sensing'}],\n",
       "  'languages': [],\n",
       "  'variants': ['BSD68 sigma65',\n",
       "   'BSD68 sigma60',\n",
       "   'BSD68 sigma55',\n",
       "   'BSD68 sigma45',\n",
       "   'BSD68 sigma40',\n",
       "   'BSD68 sigma20',\n",
       "   'BSD68 CS=50%',\n",
       "   'BSDS300',\n",
       "   'BSDS100 - 8x upscaling',\n",
       "   'BSDS100 - 4x upscaling',\n",
       "   'BSDS100 - 2x upscaling',\n",
       "   'BSD68 sigma75',\n",
       "   'BSD68 sigma70',\n",
       "   'BSD68 sigma50',\n",
       "   'BSD68 sigma5',\n",
       "   'BSD68 sigma35',\n",
       "   'BSD68 sigma30',\n",
       "   'BSD68 sigma25',\n",
       "   'BSD68 sigma15',\n",
       "   'BSD68 sigma10',\n",
       "   'BSD200 sigma70',\n",
       "   'BSD200 sigma50',\n",
       "   'BSD200 sigma30',\n",
       "   'BSD200 sigma10',\n",
       "   'BSD200 - 2x upscaling',\n",
       "   'BSD',\n",
       "   'BSD300 sigma70',\n",
       "   'BSD300 sigma50',\n",
       "   'BSD300 sigma30',\n",
       "   'BSD300 Noise Level 70%',\n",
       "   'BSD300 Noise Level 50%',\n",
       "   'BSD300 Noise Level 30%',\n",
       "   'BSD100 - 8x upscaling',\n",
       "   'BSD100 - 4x upscaling',\n",
       "   'BSD100 - 3x upscaling',\n",
       "   'BSD100 - 2x upscaling',\n",
       "   'BSD100 - 16x upscaling'],\n",
       "  'num_papers': 418,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/thumos14-1',\n",
       "  'name': 'THUMOS14',\n",
       "  'full_name': 'THUMOS 2014',\n",
       "  'homepage': 'http://crcv.ucf.edu/THUMOS14/home.html',\n",
       "  'description': 'The **THUMOS14** dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively.\\r\\n\\r\\nSource: [Learning to Localize Actions from Moments](https://arxiv.org/abs/2008.13705)\\r\\nImage Source: [http://crcv.ucf.edu/THUMOS14/](http://crcv.ucf.edu/THUMOS14/)',\n",
       "  'paper': {'title': 'THUMOS challenge: Action recognition with a large number of classes',\n",
       "   'url': 'http://www.thumos.info/'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos'},\n",
       "   {'task': 'Action Classification',\n",
       "    'url': 'https://paperswithcode.com/task/action-classification'},\n",
       "   {'task': 'Action Recognition In Videos',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition-in-videos-2'},\n",
       "   {'task': 'Weakly Supervised Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-action-localization'},\n",
       "   {'task': 'Temporal Action Proposal Generation',\n",
       "    'url': 'https://paperswithcode.com/task/temporal-action-proposal-generation'},\n",
       "   {'task': 'Few Shot Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-temporal-action-localization'},\n",
       "   {'task': 'Weakly-supervised Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-temporal-action'},\n",
       "   {'task': 'Weakly Supervised Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/weakly-supervised-temporal-action-1'}],\n",
       "  'languages': [],\n",
       "  'variants': [\"THUMOS' 14\",\n",
       "   'THUMOS 2014',\n",
       "   'THUMOS’14',\n",
       "   \"THUMOS'14\",\n",
       "   'THUMOS14'],\n",
       "  'num_papers': 205,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/thumos14/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/msra-hand',\n",
       "  'name': 'MSRA Hand',\n",
       "  'full_name': 'MSRA Hand',\n",
       "  'homepage': 'https://jimmysuen.github.io/',\n",
       "  'description': \"**MSRA Hand**s is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively.\\nThe camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each *.bin file stores the depth pixel values in row scanning order, which are 320*240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag.\\njoint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip.\\nThe corresponding *.jpg file is just for visualization of depth and ground truth joints.\\n\\nSource: [https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt](https://jimmysuen.github.io/txt/cvpr14_MSRAHandTrackingDB_readme.txt)\\nImage Source: [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Qian_Realtime_and_Robust_2014_CVPR_paper.pdf)\",\n",
       "  'paper': {'title': 'Realtime and Robust Hand Tracking from Depth',\n",
       "   'url': 'https://paperswithcode.com/paper/realtime-and-robust-hand-tracking-from-depth'},\n",
       "  'introduced_date': '2014-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Stochastic Optimization',\n",
       "    'url': 'https://paperswithcode.com/task/stochastic-optimization'},\n",
       "   {'task': 'Hand Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/hand-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['MSRA Hands', 'MSRA Hand'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/msra10k',\n",
       "  'name': 'MSRA10K',\n",
       "  'full_name': 'MSRA10K Salient Object Database',\n",
       "  'homepage': 'https://mmcheng.net/msra10k/',\n",
       "  'description': '**MSRA10K** is a dataset for salient object detection that contains 10,000 images with pixel-level saliency labeling for 10K images from the MSRA salient object detection dataset. The original MRSA database provides salient object annotation in terms of bounding boxes provided by 3-9 users.\\n\\nSource: [https://mmcheng.net/msra10k/](https://mmcheng.net/msra10k/)\\nImage Source: [https://mmcheng.net/msra10k/](https://mmcheng.net/msra10k/)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [],\n",
       "  'languages': [],\n",
       "  'variants': ['MSRA10K'],\n",
       "  'num_papers': 1,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/babi-1',\n",
       "  'name': 'bAbI',\n",
       "  'full_name': 'bAbI',\n",
       "  'homepage': 'https://research.fb.com/downloads/babi/',\n",
       "  'description': 'The **bAbI** dataset is a textual QA benchmark composed of 20 different tasks. Each task is designed to test a different reasoning skill, such as deduction, induction, and coreference resolution. Some of the tasks need relational reasoning, for instance, to compare the size of different entities. Each sample is composed of a question, an answer, and a set of facts. There are two versions of the dataset, referring to different dataset sizes: bAbI-1k and bAbI-10k. The bAbI-10k version of the dataset consists of 10,000 training samples per task.\\r\\n\\r\\nSource: [Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module](https://arxiv.org/abs/1805.09354)\\r\\nImage Source: [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)',\n",
       "  'paper': {'title': 'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks',\n",
       "   'url': 'https://paperswithcode.com/paper/towards-ai-complete-question-answering-a-set'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Language Modelling',\n",
       "    'url': 'https://paperswithcode.com/task/language-modelling'},\n",
       "   {'task': 'Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/question-answering'},\n",
       "   {'task': 'Reading Comprehension',\n",
       "    'url': 'https://paperswithcode.com/task/reading-comprehension'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['bAbi', 'bAbI', 'catbAbI QA-mode', 'catbAbI LM-mode'],\n",
       "  'num_papers': 176,\n",
       "  'data_loaders': [{'url': 'https://huggingface.co/datasets/babi_qa',\n",
       "    'repo': 'https://github.com/huggingface/datasets',\n",
       "    'frameworks': ['tf', 'pytorch', 'jax']},\n",
       "   {'url': 'http://docs.allennlp.org/main/api/data/dataset_readers/babi/',\n",
       "    'repo': 'https://github.com/allenai/allennlp',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#babi-1k',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://parl.ai/docs/tasks.html#babi-10k',\n",
       "    'repo': 'https://github.com/facebookresearch/ParlAI',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/jhmdb',\n",
       "  'name': 'JHMDB',\n",
       "  'full_name': 'Joint-annotated Human Motion Data Base',\n",
       "  'homepage': 'http://jhmdb.is.tue.mpg.de/',\n",
       "  'description': '**JHMDB** is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality).\\r\\n\\r\\nSource: [Unsupervised Deep Metric Learning via Orthogonality based Probabilistic Loss](https://arxiv.org/abs/2008.09880)\\r\\nImage Source: [https://arxiv.org/pdf/1712.06316.pdf](https://arxiv.org/pdf/1712.06316.pdf)',\n",
       "  'paper': {'title': 'Towards Understanding Action Recognition',\n",
       "   'url': 'https://doi.org/10.1109/ICCV.2013.396'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Temporal Action Localization',\n",
       "    'url': 'https://paperswithcode.com/task/action-recognition'},\n",
       "   {'task': 'Skeleton Based Action Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/skeleton-based-action-recognition'},\n",
       "   {'task': 'Referring Expression Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/referring-expression-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['JHMDB',\n",
       "   'J-HMBD Early Action',\n",
       "   'JHMDB Pose Tracking',\n",
       "   'JHMDB (2D poses only)',\n",
       "   'J-HMDB',\n",
       "   'J-HMDB-21'],\n",
       "  'num_papers': 179,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmaction2/blob/master/tools/data/jhmdb/README.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmaction2',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmpose',\n",
       "    'repo': 'https://github.com/open-mmlab/mmpose',\n",
       "    'frameworks': []}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/ucf-cc-50-1',\n",
       "  'name': 'UCF-CC-50',\n",
       "  'full_name': 'UCF-CC-50',\n",
       "  'homepage': 'https://www.crcv.ucf.edu/data/ucf-cc-50/',\n",
       "  'description': '**UCF-CC-50** is a dataset for crowd counting and consists of images of extremely dense crowds. It has 50 images with 63,974 head center annotations in total. The head counts range between 94 and 4,543 per image. The small dataset size and large variance make this a very challenging counting dataset.\\r\\n\\r\\nSource: [Active Crowd Counting with Limited Supervision](https://arxiv.org/abs/2007.06334)',\n",
       "  'paper': {'title': 'Multi-source Multi-scale Counting in Extremely Dense Crowd Images',\n",
       "   'url': 'https://paperswithcode.com/paper/multi-source-multi-scale-counting-in'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Density Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/density-estimation'},\n",
       "   {'task': 'Crowd Counting',\n",
       "    'url': 'https://paperswithcode.com/task/crowd-counting'},\n",
       "   {'task': 'Human Detection',\n",
       "    'url': 'https://paperswithcode.com/task/human-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['UCF-CC-50'],\n",
       "  'num_papers': 21,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/surreal-1',\n",
       "  'name': 'SURREAL',\n",
       "  'full_name': 'Synthetic Humans for REAL Tasks',\n",
       "  'homepage': 'https://www.di.ens.fr/willow/research/surreal/data/',\n",
       "  'description': '**SURREAL** (Synthetic hUmans foR REAL tasks) is a large-scale person dataset that generates photorealistic synthetic images with labeling for human part segmentation and depth estimation, producing 6.5M frames in 67.5K short clips (about 100 frames each) of 2.6K action sequences with 145 different synthetic subjects. To ensure realism, the synthetic bodies are created using the SMPL body model, whose parameters are fit by the MoSh method given raw 3D MoCap marker data.\\r\\n\\r\\nSource: [Synthetic Data for Deep Learning](https://arxiv.org/abs/1909.11512)\\r\\nImage Source: [https://www.di.ens.fr/willow/research/surreal/data/](https://www.di.ens.fr/willow/research/surreal/data/)',\n",
       "  'paper': {'title': 'Learning from Synthetic Humans',\n",
       "   'url': 'https://paperswithcode.com/paper/learning-from-synthetic-humans'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Videos', 'RGB Video'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': '3D Human Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-human-pose-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Surreal', 'SURREAL'],\n",
       "  'num_papers': 105,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/awa2-1',\n",
       "  'name': 'AwA2',\n",
       "  'full_name': 'Animals with Attributes 2',\n",
       "  'homepage': 'https://cvml.ist.ac.at/AwA2/',\n",
       "  'description': '**Animals with Attributes 2** (**AwA2**) is a dataset for benchmarking transfer-learning algorithms, such as attribute base classification and zero-shot learning. AwA2 is a drop-in replacement of original Animals with Attributes (AwA) dataset, with more images released for each category. Specifically, AwA2 consists of in total 37322 images distributed in 50 animal categories. The AwA2 also provides a category-attribute matrix, which contains an 85-dim attribute vector (e.g., color, stripe, furry, size, and habitat) for each category.\\r\\n\\r\\nSource: [Learning from Noisy Web Data with Category-level Supervision](https://arxiv.org/abs/1803.03857)\\nImage Source: [https://arxiv.org/pdf/1604.00326.pdf](https://arxiv.org/pdf/1604.00326.pdf)',\n",
       "  'paper': {'title': 'Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad and the Ugly',\n",
       "   'url': 'https://paperswithcode.com/paper/zero-shot-learning-a-comprehensive-evaluation'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Generalized Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-few-shot-learning'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AWA2 - 0-Shot', 'AwA2'],\n",
       "  'num_papers': 134,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/awa-1',\n",
       "  'name': 'AwA',\n",
       "  'full_name': 'Animals with Attributes',\n",
       "  'homepage': 'https://cvml.ist.ac.at/AwA/',\n",
       "  'description': \"**Animals with Attributes** (**AwA**) was a dataset for benchmarking transfer-learning algorithms, in particular attribute base classification. It consisted of 30475 images of 50 animals classes with six pre-extracted feature representations for each image. The animals classes are aligned with Osherson's classical class/attribute matrix, thereby providing 85 numeric attribute values for each class. Using the shared attributes, it is possible to transfer information between different classes.\\r\\nThe Animals with Attributes dataset was suspended. Its images are not available anymore because of copyright restrictions. A drop-in replacement, Animals with Attributes 2, is available instead.\\r\\n\\r\\nSource: [Transductive Multi-view Zero-Shot Learning](https://arxiv.org/abs/1501.04560)\\r\\nImage Source: [https://cvml.ist.ac.at/AwA/](https://cvml.ist.ac.at/AwA/)\",\n",
       "  'paper': {'title': 'Learning to detect unseen object classes by between-class attribute transfer',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.2009.5206594'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/zero-shot-learning'},\n",
       "   {'task': 'Few-Shot Image Classification',\n",
       "    'url': 'https://paperswithcode.com/task/few-shot-image-classification'},\n",
       "   {'task': 'Generalized Few-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-few-shot-learning'},\n",
       "   {'task': 'Generalized Zero-Shot Learning',\n",
       "    'url': 'https://paperswithcode.com/task/generalized-zero-shot-learning'},\n",
       "   {'task': 'Long-tail learning with class descriptors',\n",
       "    'url': 'https://paperswithcode.com/task/long-tail-learning-with-class-descriptors'}],\n",
       "  'languages': [],\n",
       "  'variants': ['AWA1 - 0-Shot', 'AWA - 0-Shot', 'AwA', 'AwA2', 'AWA-LT'],\n",
       "  'num_papers': 222,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/arc',\n",
       "  'name': 'ARC',\n",
       "  'full_name': 'AI2 Reasoning Challenge',\n",
       "  'homepage': 'https://allenai.org/data/arc',\n",
       "  'description': 'The AI2’s Reasoning Challenge (**ARC**) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages.\\r\\n\\r\\nSource: [Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering](https://arxiv.org/abs/1911.07176)\\r\\nImage Source: [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)',\n",
       "  'paper': {'title': 'Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge',\n",
       "   'url': 'https://paperswithcode.com/paper/think-you-have-solved-question-answering-try'},\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Common Sense Reasoning',\n",
       "    'url': 'https://paperswithcode.com/task/common-sense-reasoning'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ARC (Easy)', 'ARC (Challenge)', 'ARC'],\n",
       "  'num_papers': 82,\n",
       "  'data_loaders': [{'url': 'https://www.tensorflow.org/datasets/catalog/arc',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/ai2_arc',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']},\n",
       "   {'url': 'https://www.tensorflow.org/datasets/catalog/ai2_arc_with_ir',\n",
       "    'repo': 'https://github.com/tensorflow/datasets',\n",
       "    'frameworks': ['tf', 'jax']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/pascal-voc-2011',\n",
       "  'name': 'PASCAL VOC 2011',\n",
       "  'full_name': 'PASCAL VOC 2011',\n",
       "  'homepage': 'http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html',\n",
       "  'description': '**PASCAL VOC 2011** is an image segmentation dataset. It contains around 2,223 images for training, consisting of 5,034 objects. Testing consists of 1,111 images with 2,028 objects. In total there are over 5,000 precisely segmented objects for training.\\r\\n\\r\\nSource: [Scene Parsing with Integration of Parametric and Non-parametric Models](https://arxiv.org/abs/1604.05848)\\r\\nImage Source: [http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html](http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2011/index.html)',\n",
       "  'paper': {'title': 'The PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results',\n",
       "   'url': 'http://www.pascal-network.org/challenges/VOC/voc2011/workshop/index.html'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['PASCAL VOC 2011 test', 'PASCAL VOC 2011'],\n",
       "  'num_papers': 19,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmdetection/blob/master/docs/1_exist_data_model.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmdetection',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html',\n",
       "    'repo': 'https://github.com/rusty1s/pytorch_geometric',\n",
       "    'frameworks': ['pytorch']},\n",
       "   {'url': 'https://github.com/open-mmlab/mmsegmentation/blob/master/docs/dataset_prepare.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmsegmentation',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/2d-3d-s',\n",
       "  'name': '2D-3D-S',\n",
       "  'full_name': '2D-3D-Semantic',\n",
       "  'homepage': 'https://github.com/alexsax/2D-3D-Semantics',\n",
       "  'description': 'The **2D-3D-S** dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360° equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.\\r\\n\\r\\nSource: [https://github.com/alexsax/2D-3D-Semantics](https://github.com/alexsax/2D-3D-Semantics)\\r\\nImage Source: [https://github.com/alexsax/2D-3D-Semantics](https://github.com/alexsax/2D-3D-Semantics)',\n",
       "  'paper': {'title': 'Joint 2D-3D-Semantic Data for Indoor Scene Understanding',\n",
       "   'url': 'https://paperswithcode.com/paper/joint-2d-3d-semantic-data-for-indoor-scene'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': '3D Face Animation',\n",
       "    'url': 'https://paperswithcode.com/task/3d-face-animation'},\n",
       "   {'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Visual Navigation',\n",
       "    'url': 'https://paperswithcode.com/task/visual-navigation'},\n",
       "   {'task': 'Self-Supervised Learning',\n",
       "    'url': 'https://paperswithcode.com/task/self-supervised-learning'}],\n",
       "  'languages': [],\n",
       "  'variants': ['2D-3D-S'],\n",
       "  'num_papers': 40,\n",
       "  'data_loaders': [{'url': 'https://github.com/alexsax/2D-3D-Semantics',\n",
       "    'repo': 'https://github.com/alexsax/2D-3D-Semantics',\n",
       "    'frameworks': ['none']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/color-feret',\n",
       "  'name': 'Color FERET',\n",
       "  'full_name': 'Color FERET',\n",
       "  'homepage': 'https://catalog.data.gov/dataset/color-feret-database',\n",
       "  'description': 'The color FERET database is a dataset for face recognition. It contains 11,338 color images of size 512×768 pixels captured in a semi-controlled environment with 13 different poses from 994 subjects.\\r\\n\\r\\nSource: [A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition](https://arxiv.org/abs/1606.02894)\\r\\nImage Source: [https://www.researchgate.net/figure/Sample-results-taken-from-Color-FERET-data-set-testing-using-LBP-algorithm_fig4_308836179](https://www.researchgate.net/figure/Sample-results-taken-from-Color-FERET-data-set-testing-using-LBP-algorithm_fig4_308836179)',\n",
       "  'paper': {'title': 'The FERET Evaluation Methodology for Face-Recognition Algorithms',\n",
       "   'url': 'https://doi.org/10.1109/CVPR.1997.609311'},\n",
       "  'introduced_date': '1997-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Color FERET (Online Open Set)', 'Color FERET'],\n",
       "  'num_papers': 25,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/icdar-2017',\n",
       "  'name': 'ICDAR 2017',\n",
       "  'full_name': 'ICDAR 2017',\n",
       "  'homepage': 'https://zenodo.org/record/835489',\n",
       "  'description': 'ICDAR2017 is a dataset for scene text detection.\\n\\nSource: [Scale-Invariant Multi-Oriented Text Detection in Wild Scene Images](https://arxiv.org/abs/2002.06423)\\nImage Source: [https://rrc.cvc.uab.es/?ch=7](https://rrc.cvc.uab.es/?ch=7)',\n",
       "  'paper': {'title': 'ICDAR2017 Robust Reading Challenge on COCO-Text',\n",
       "   'url': 'https://doi.org/10.1109/ICDAR.2017.234'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Scene Text Detection',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': [' ICDAR 2017 MLT', 'ICDAR 2017 MLT', 'ICDAR 2017'],\n",
       "  'num_papers': 16,\n",
       "  'data_loaders': [{'url': 'https://github.com/open-mmlab/mmocr/blob/main/docs/datasets.md',\n",
       "    'repo': 'https://github.com/open-mmlab/mmocr',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/bucc',\n",
       "  'name': 'BUCC',\n",
       "  'full_name': 'Building and Using Comparable Corpora',\n",
       "  'homepage': 'https://comparable.limsi.fr/bucc2017/',\n",
       "  'description': 'The **BUCC** mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure.\\r\\n\\r\\nSource: [Language-agnostic BERT Sentence Embedding](https://arxiv.org/abs/2007.01852)\\r\\nImage Source: [https://comparable.limsi.fr/bucc2017/](https://comparable.limsi.fr/bucc2017/)',\n",
       "  'paper': {'title': 'Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in Comparable Corpora',\n",
       "   'url': 'https://paperswithcode.com/paper/overview-of-the-second-bucc-shared-task'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Cross-Lingual Bitext Mining',\n",
       "    'url': 'https://paperswithcode.com/task/cross-lingual-bitext-mining'}],\n",
       "  'languages': ['English', 'French', 'German', 'Chinese'],\n",
       "  'variants': ['BUCC German-to-English',\n",
       "   'BUCC French-to-English',\n",
       "   'BUCC Russian-to-English',\n",
       "   'BUCC Chinese-to-English',\n",
       "   'BUCC'],\n",
       "  'num_papers': 24,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/make3d',\n",
       "  'name': 'Make3D',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://make3d.cs.cornell.edu/data.html#make3d',\n",
       "  'description': 'The **Make3D** dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution.\\r\\n\\r\\nSource: [Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation](https://arxiv.org/abs/1908.05794)\\r\\nImage Source: [http://make3d.cs.cornell.edu/data.html#make3d](http://make3d.cs.cornell.edu/data.html#make3d)',\n",
       "  'paper': {'title': 'Make3D: Learning 3D Scene Structure from a Single Still Image',\n",
       "   'url': 'https://doi.org/10.1109/TPAMI.2008.132'},\n",
       "  'introduced_date': '2009-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Monocular Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-depth-estimation'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Make3D'],\n",
       "  'num_papers': 101,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/virtual-kitti',\n",
       "  'name': 'Virtual KITTI',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/',\n",
       "  'description': '**Virtual KITTI** is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.\\r\\n\\r\\nVirtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels (cf. below for download links).\\r\\n\\r\\nSource: [https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/](https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/)\\r\\nImage Source: [https://arxiv.org/pdf/1605.06457.pdf](https://arxiv.org/pdf/1605.06457.pdf)',\n",
       "  'paper': {'title': 'Virtual Worlds as Proxy for Multi-Object Tracking Analysis',\n",
       "   'url': 'https://paperswithcode.com/paper/virtual-worlds-as-proxy-for-multi-object'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/object-tracking'},\n",
       "   {'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Monocular 3D Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/monocular-3d-object-detection'},\n",
       "   {'task': 'Multi-Object Tracking',\n",
       "    'url': 'https://paperswithcode.com/task/multi-object-tracking'},\n",
       "   {'task': 'Optical Flow Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/optical-flow-estimation'},\n",
       "   {'task': 'Visual Odometry',\n",
       "    'url': 'https://paperswithcode.com/task/visual-odometry'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'},\n",
       "   {'task': 'Simultaneous Localization and Mapping',\n",
       "    'url': 'https://paperswithcode.com/task/simultaneous-localization-and-mapping'},\n",
       "   {'task': 'Stereo Matching',\n",
       "    'url': 'https://paperswithcode.com/task/stereo-matching-1'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Virtual KITTI to BDD100K', 'Virtual KITTI 2', 'Virtual KITTI'],\n",
       "  'num_papers': 88,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/nclt',\n",
       "  'name': 'NCLT',\n",
       "  'full_name': 'North Campus Long-Term Vision and LiDAR',\n",
       "  'homepage': 'http://robots.engin.umich.edu/nclt/',\n",
       "  'description': 'The **NCLT** dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects.\\r\\n\\r\\nSource: [http://robots.engin.umich.edu/nclt/nclt.pdf](http://robots.engin.umich.edu/nclt/nclt.pdf)\\r\\nImage Source: [http://robots.engin.umich.edu/nclt/](http://robots.engin.umich.edu/nclt/)',\n",
       "  'paper': {'title': 'University of Michigan North Campus long-term vision and lidar dataset',\n",
       "   'url': 'https://doi.org/10.1177/0278364915614638'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Interactive'],\n",
       "  'tasks': [{'task': 'Pose Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/pose-estimation'},\n",
       "   {'task': 'Visual Place Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/visual-place-recognition'},\n",
       "   {'task': 'Visual Localization',\n",
       "    'url': 'https://paperswithcode.com/task/visual-localization'}],\n",
       "  'languages': [],\n",
       "  'variants': ['NCLT'],\n",
       "  'num_papers': 47,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kitti-depth',\n",
       "  'name': 'KITTI-Depth',\n",
       "  'full_name': None,\n",
       "  'homepage': 'http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction',\n",
       "  'description': 'The **KITTI-Depth** dataset includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras. The depth images are highly sparse with only 5% of the pixels available and the rest is missing. The dataset has 86k training images, 7k validation images, and 1k test set images on the benchmark server with no access to the ground truth.\\n\\nSource: [Confidence Propagation through CNNs for Guided Sparse Depth Regression](https://arxiv.org/abs/1811.01791)\\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction](http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction)',\n",
       "  'paper': {'title': 'Sparsity Invariant CNNs',\n",
       "   'url': 'https://paperswithcode.com/paper/sparsity-invariant-cnns'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Point cloud'],\n",
       "  'tasks': [{'task': 'Depth Estimation',\n",
       "    'url': 'https://paperswithcode.com/task/depth-estimation'},\n",
       "   {'task': 'Depth Completion',\n",
       "    'url': 'https://paperswithcode.com/task/depth-completion'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KITTI-Depth'],\n",
       "  'num_papers': 8,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/sof',\n",
       "  'name': 'SoF',\n",
       "  'full_name': 'Specs on Faces',\n",
       "  'homepage': 'https://sites.google.com/view/sof-dataset',\n",
       "  'description': 'The **Specs on Faces** (**SoF**) dataset, a collection of 42,592 (2,662×16) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more.\\n\\nSource: [https://sites.google.com/view/sof-dataset](https://sites.google.com/view/sof-dataset)\\nImage Source: [https://sites.google.com/view/sof-dataset](https://sites.google.com/view/sof-dataset)',\n",
       "  'paper': {'title': 'AFIF4: Deep Gender Classification based on AdaBoost-based Fusion of Isolated Facial Features and Foggy Faces',\n",
       "   'url': 'https://paperswithcode.com/paper/afif4-deep-gender-classification-based-on'},\n",
       "  'introduced_date': '2017-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Dimensionality Reduction',\n",
       "    'url': 'https://paperswithcode.com/task/dimensionality-reduction'}],\n",
       "  'languages': [],\n",
       "  'variants': ['SoF'],\n",
       "  'num_papers': 3,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kitti-road',\n",
       "  'name': 'KITTI Road',\n",
       "  'full_name': '',\n",
       "  'homepage': 'http://www.cvlibs.net/datasets/kitti/eval_road.php',\n",
       "  'description': 'KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes:\\r\\n* uu - urban unmarked (98/100)\\r\\n* um - urban marked (95/96)\\r\\n* umm - urban multiple marked lanes (96/94)\\r\\n* urban - combination of the three above\\r\\nGround truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category \"um\"). Ground truth is provided for training images only.\\r\\n\\r\\nSource: [http://www.cvlibs.net/datasets/kitti/eval_road.php](http://www.cvlibs.net/datasets/kitti/eval_road.php)\\r\\nImage Source: [http://www.cvlibs.net/datasets/kitti/eval_road.php](http://www.cvlibs.net/datasets/kitti/eval_road.php)',\n",
       "  'paper': {'title': 'A new performance measure and evaluation benchmark for road detection algorithms',\n",
       "   'url': 'https://doi.org/10.1109/ITSC.2013.6728473'},\n",
       "  'introduced_date': '2013-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Point cloud'],\n",
       "  'tasks': [{'task': 'Semantic Segmentation',\n",
       "    'url': 'https://paperswithcode.com/task/semantic-segmentation'},\n",
       "   {'task': 'Scene Understanding',\n",
       "    'url': 'https://paperswithcode.com/task/scene-understanding'},\n",
       "   {'task': 'Autonomous Driving',\n",
       "    'url': 'https://paperswithcode.com/task/autonomous-driving'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KITTI Road'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/kaist-urban',\n",
       "  'name': 'KAIST Urban',\n",
       "  'full_name': '',\n",
       "  'homepage': 'https://sites.google.com/view/complex-urban-dataset',\n",
       "  'description': 'This data set provides Light Detection and Ranging (LiDAR) data and stereo image with various position sensors targeting a highly complex urban environment. The presented data set captures features in urban environments (e.g. metropolis areas, complex buildings and residential areas). The data of 2D and 3D LiDAR are provided, which are typical types of LiDAR sensors. Raw sensor data for vehicle navigation is presented in a file format. For convenience, development tools are provided in the Robot Operating System (ROS) environment.\\r\\n\\r\\nSource: [https://sites.google.com/view/complex-urban-dataset](https://sites.google.com/view/complex-urban-dataset)\\r\\nImage Source: [https://irap.kaist.ac.kr/dataset/](https://irap.kaist.ac.kr/dataset/)',\n",
       "  'paper': {'title': 'Complex urban dataset with multi-level sensors from highly diverse urban environments',\n",
       "   'url': 'https://doi.org/10.1177/0278364919843996'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Visual Localization',\n",
       "    'url': 'https://paperswithcode.com/task/visual-localization'},\n",
       "   {'task': 'Line Segment Detection',\n",
       "    'url': 'https://paperswithcode.com/task/line-segment-detection'},\n",
       "   {'task': 'Loop Closure Detection',\n",
       "    'url': 'https://paperswithcode.com/task/loop-closure-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['KAIST Urban'],\n",
       "  'num_papers': 13,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/manga109',\n",
       "  'name': 'Manga109',\n",
       "  'full_name': 'Manga109',\n",
       "  'homepage': 'http://www.manga109.org/en/',\n",
       "  'description': '**Manga109** has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library “Manga Library Z” (formerly the “Zeppan Manga Toshokan” library of out-of-print manga).\\r\\n\\r\\nSource: [Manga109](http://www.manga109.org/en/)\\r\\nImage Source: [https://arxiv.org/pdf/1510.04389v1.pdf](https://arxiv.org/pdf/1510.04389v1.pdf)',\n",
       "  'paper': {'title': 'Sketch-based Manga Retrieval using Manga109 Dataset',\n",
       "   'url': 'https://paperswithcode.com/paper/sketch-based-manga-retrieval-using-manga109'},\n",
       "  'introduced_date': '2015-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Object Detection',\n",
       "    'url': 'https://paperswithcode.com/task/object-detection'},\n",
       "   {'task': 'Image Super-Resolution',\n",
       "    'url': 'https://paperswithcode.com/task/image-super-resolution'}],\n",
       "  'languages': ['Japanese'],\n",
       "  'variants': ['Manga109-s 15test',\n",
       "   'Manga109',\n",
       "   'Manga109 - 8x upscaling',\n",
       "   'Manga109 - 4x upscaling',\n",
       "   'Manga109 - 3x upscaling',\n",
       "   'Manga109 - 2x upscaling',\n",
       "   'Manga109 - 16x upscaling'],\n",
       "  'num_papers': 122,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/gqa',\n",
       "  'name': 'GQA',\n",
       "  'full_name': 'GQA',\n",
       "  'homepage': 'https://cs.stanford.edu/people/dorarad/gqa/',\n",
       "  'description': 'The **GQA** dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image – convolutional grid features of size 7×7×2048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet×2048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector.\\r\\n\\r\\nSource: [Language-Conditioned Graph Networks for Relational Reasoning](https://arxiv.org/abs/1905.04405)\\r\\nImage Source: [https://arxiv.org/pdf/1902.09506.pdf](https://arxiv.org/pdf/1902.09506.pdf)',\n",
       "  'paper': {'title': 'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering',\n",
       "   'url': 'https://paperswithcode.com/paper/gqa-a-new-dataset-for-compositional-question'},\n",
       "  'introduced_date': '2019-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Visual Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/visual-question-answering'},\n",
       "   {'task': 'Graph Question Answering',\n",
       "    'url': 'https://paperswithcode.com/task/graph-question-answering'}],\n",
       "  'languages': [],\n",
       "  'variants': ['GQA test-std', 'GQA test-dev', 'GQA Test2019', 'GQA'],\n",
       "  'num_papers': 130,\n",
       "  'data_loaders': [{'url': 'https://docs.allennlp.org/models/main/models/vision/dataset_readers/gqa/',\n",
       "    'repo': 'https://github.com/allenai/allennlp-models',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/muse',\n",
       "  'name': 'MUSE',\n",
       "  'full_name': 'MUSE',\n",
       "  'homepage': 'https://github.com/facebookresearch/MUSE',\n",
       "  'description': 'The **MUSE** dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs.\\r\\n\\r\\nSource: [Filtered Inner Product Projection for Multilingual Embedding Alignment](https://arxiv.org/abs/2006.03652)\\r\\nImage Source: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE)',\n",
       "  'paper': None,\n",
       "  'introduced_date': '2018-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Machine Translation',\n",
       "    'url': 'https://paperswithcode.com/task/machine-translation'},\n",
       "   {'task': 'Word Alignment',\n",
       "    'url': 'https://paperswithcode.com/task/word-alignment'},\n",
       "   {'task': 'Word Embeddings',\n",
       "    'url': 'https://paperswithcode.com/task/word-embeddings'}],\n",
       "  'languages': ['English', 'Spanish'],\n",
       "  'variants': ['MUSE Italian-French',\n",
       "   'MUSE English-Spanish',\n",
       "   'MUSE English-Portuguese',\n",
       "   'MUSE English-French',\n",
       "   'MUSE',\n",
       "   'MUSE en-pt',\n",
       "   'MUSE en-de'],\n",
       "  'num_papers': 2,\n",
       "  'data_loaders': [{'url': 'https://github.com/facebookresearch/MUSE',\n",
       "    'repo': 'https://github.com/facebookresearch/MUSE',\n",
       "    'frameworks': ['pytorch']}]},\n",
       " {'url': 'https://paperswithcode.com/dataset/replay-mobile-1',\n",
       "  'name': 'Replay-Mobile',\n",
       "  'full_name': 'Replay-Mobile',\n",
       "  'homepage': 'https://www.idiap.ch/dataset/replay-mobile',\n",
       "  'description': 'The **Replay-Mobile** Database for face spoofing consists of 1190 video clips of photo and video attack attempts to 40 clients, under different lighting conditions. These videos were recorded with current devices from the market -- an iPad Mini2 (running iOS) and a LG-G4 smartphone (running Android). This Database was produced at the Idiap Research Institute (Switzerland) within the framework of collaboration with Galician Research and Development Center in Advanced Telecommunications - Gradiant (Spain).\\r\\n\\r\\nSource: [Replay-Mobile](https://www.idiap.ch/dataset/replay-mobile)\\r\\nImage Source: [https://core.ac.uk/download/pdf/148024307.pdf](https://core.ac.uk/download/pdf/148024307.pdf)',\n",
       "  'paper': {'title': 'The Replay-Mobile Face Presentation-Attack Database',\n",
       "   'url': 'https://doi.org/10.1109/BIOSIG.2016.7736936'},\n",
       "  'introduced_date': '2016-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Videos'],\n",
       "  'tasks': [{'task': 'Face Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/face-recognition'},\n",
       "   {'task': 'Face Anti-Spoofing',\n",
       "    'url': 'https://paperswithcode.com/task/face-anti-spoofing'},\n",
       "   {'task': 'Face Presentation Attack Detection',\n",
       "    'url': 'https://paperswithcode.com/task/face-presentation-attack-detection'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Replay Mobile', 'Replay-Mobile'],\n",
       "  'num_papers': 15,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/netflix-prize',\n",
       "  'name': 'Netflix Prize',\n",
       "  'full_name': 'Netflix Prize',\n",
       "  'homepage': 'https://www.netflixprize.com/',\n",
       "  'description': '**Netflix Prize** consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5.\\r\\n\\r\\nSource: [Indian Regional Movie Dataset for Recommender Systems](https://arxiv.org/abs/1801.02203)\\r\\nImage Source: [https://www.netflixprize.com/](https://www.netflixprize.com/)',\n",
       "  'paper': {'title': 'The netflix prize',\n",
       "   'url': 'http://brettb.net/project/papers/2007%20The%20Netflix%20Prize.pdf'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Tabular'],\n",
       "  'tasks': [{'task': 'Recommendation Systems',\n",
       "    'url': 'https://paperswithcode.com/task/recommendation-systems'},\n",
       "   {'task': 'Matrix Completion',\n",
       "    'url': 'https://paperswithcode.com/task/matrix-completion'},\n",
       "   {'task': 'Fairness', 'url': 'https://paperswithcode.com/task/fairness'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Netflix Prize'],\n",
       "  'num_papers': 80,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/recipe1m-1',\n",
       "  'name': 'Recipe1M+',\n",
       "  'full_name': 'Recipe1M+',\n",
       "  'homepage': 'http://im2recipe.csail.mit.edu/',\n",
       "  'description': '**Recipe1M+** is a dataset which contains one million structured cooking recipes with 13M associated images.\\r\\n\\r\\nSource: [Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images](http://im2recipe.csail.mit.edu/)\\r\\nImage Source: [http://im2recipe.csail.mit.edu/](http://im2recipe.csail.mit.edu/)',\n",
       "  'paper': {'title': 'Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images',\n",
       "   'url': 'https://paperswithcode.com/paper/recipe1m-a-dataset-for-learning-cross-modal'},\n",
       "  'introduced_date': '2018-10-14',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images', 'Texts'],\n",
       "  'tasks': [{'task': 'Image Generation',\n",
       "    'url': 'https://paperswithcode.com/task/image-generation'},\n",
       "   {'task': 'Recipe Generation',\n",
       "    'url': 'https://paperswithcode.com/task/recipe-generation'},\n",
       "   {'task': 'Cross-Modal Retrieval',\n",
       "    'url': 'https://paperswithcode.com/task/cross-modal-retrieval'},\n",
       "   {'task': 'Food Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/food-recognition'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Recipe1M', 'Recipe1M+'],\n",
       "  'num_papers': 28,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/darpa-1',\n",
       "  'name': 'DARPA',\n",
       "  'full_name': 'DARPA',\n",
       "  'homepage': 'https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset',\n",
       "  'description': 'Darpa is a dataset consisting of communications between source IPs and destination IPs. This dataset contains different attacks between IPs.\\n\\nSource: [dynnode2vec: Scalable Dynamic Network Embedding](https://arxiv.org/abs/1812.02356)\\nImage Source: [https://archive.ll.mit.edu/ideval/files/1999_DARPA_EvaulationSumPlans.pdf](https://archive.ll.mit.edu/ideval/files/1999_DARPA_EvaulationSumPlans.pdf)',\n",
       "  'paper': {'title': 'Results of the DARPA 1998 Offline Intrusion Detection Evaluation',\n",
       "   'url': 'http://www.raid-symposium.org/raid99/PAPERS/Lippmann_DARPA.pdf'},\n",
       "  'introduced_date': '1999-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': [],\n",
       "  'tasks': [{'task': 'Anomaly Detection',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection'},\n",
       "   {'task': 'Anomaly Detection in Edge Streams',\n",
       "    'url': 'https://paperswithcode.com/task/anomaly-detection-in-edge-streams'}],\n",
       "  'languages': [],\n",
       "  'variants': ['Darpa', 'DARPA'],\n",
       "  'num_papers': 11,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/holist',\n",
       "  'name': 'HOList',\n",
       "  'full_name': 'HOList',\n",
       "  'homepage': 'https://sites.google.com/view/holist/home',\n",
       "  'description': 'The official **HOList** benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof.\\r\\n\\r\\nSource: [HoList](https://sites.google.com/view/holist/home)\\r\\nImage Source: [https://sites.google.com/view/holist/home](https://sites.google.com/view/holist/home)',\n",
       "  'paper': {'title': 'HOList: An Environment for Machine Learning of Higher-Order Theorem Proving',\n",
       "   'url': 'https://paperswithcode.com/paper/holist-an-environment-for-machine-learning-of'},\n",
       "  'introduced_date': None,\n",
       "  'warning': None,\n",
       "  'modalities': ['Texts'],\n",
       "  'tasks': [{'task': 'Automated Theorem Proving',\n",
       "    'url': 'https://paperswithcode.com/task/automated-theorem-proving'}],\n",
       "  'languages': [],\n",
       "  'variants': ['HOList benchmark', 'HOList'],\n",
       "  'num_papers': 7,\n",
       "  'data_loaders': []},\n",
       " {'url': 'https://paperswithcode.com/dataset/icdar-2003',\n",
       "  'name': 'ICDAR 2003',\n",
       "  'full_name': 'ICDAR 2003',\n",
       "  'homepage': 'http://www.imglab.org/db/index.html',\n",
       "  'description': 'The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images.\\r\\n\\r\\nSource: [Robust Scene Text Recognition Using Sparse Coding based Features](https://arxiv.org/abs/1512.08669)\\r\\nImage Source: [https://www.researchgate.net/figure/The-results-of-text-localization-and-extraction-on-ICDAR-2003-dataset_fig3_290070044](https://www.researchgate.net/figure/The-results-of-text-localization-and-extraction-on-ICDAR-2003-dataset_fig3_290070044)',\n",
       "  'paper': {'title': 'ICDAR 2003 Robust Reading Competitions',\n",
       "   'url': 'https://doi.org/10.1109/ICDAR.2003.1227749'},\n",
       "  'introduced_date': '2003-01-01',\n",
       "  'warning': None,\n",
       "  'modalities': ['Images'],\n",
       "  'tasks': [{'task': 'Optical Character Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/optical-character-recognition'},\n",
       "   {'task': 'Scene Text Recognition',\n",
       "    'url': 'https://paperswithcode.com/task/scene-text-recognition'}],\n",
       "  'languages': ['English'],\n",
       "  'variants': ['ICDAR 2003'],\n",
       "  'num_papers': 46,\n",
       "  'data_loaders': [{'url': 'https://mindee.github.io/doctr/latest/datasets.html#doctr.datasets.IC03',\n",
       "    'repo': 'https://github.com/mindee/doctr',\n",
       "    'frameworks': ['tf', 'pytorch']}]},\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('datasets.json') as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yRqVVtmKJJ4Z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>owner</th>\n",
       "      <th>date</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>keywords</th>\n",
       "      <th>#downloads</th>\n",
       "      <th>#views</th>\n",
       "      <th>#votes</th>\n",
       "      <th>dataset_slug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNIST</td>\n",
       "      <td>The **MNIST** database (**Modified National In...</td>\n",
       "      <td>https://paperswithcode.com/dataset/mnist</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CelebA</td>\n",
       "      <td>CelebFaces Attributes dataset contains 202,599...</td>\n",
       "      <td>https://paperswithcode.com/dataset/celeba</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>CelebFaces Attributes Dataset</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JFT-300M</td>\n",
       "      <td>**JFT-300M** is an internal Google dataset use...</td>\n",
       "      <td>https://paperswithcode.com/dataset/jft-300m</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>JFT-300M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLUE</td>\n",
       "      <td>General Language Understanding Evaluation (**G...</td>\n",
       "      <td>https://paperswithcode.com/dataset/glue</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>General Language Understanding Evaluation benc...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultiNLI</td>\n",
       "      <td>The **Multi-Genre Natural Language Inference**...</td>\n",
       "      <td>https://paperswithcode.com/dataset/multinli</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Multi-Genre Natural Language Inference</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>Dataset for the Article \"Does the Venue of Sci...</td>\n",
       "      <td>Is there any correlation between the impact of...</td>\n",
       "      <td>https://paperswithcode.com/dataset/dataset-for...</td>\n",
       "      <td>2021-05-31</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>WSJ Dow Jones Stock Data</td>\n",
       "      <td>Please see code repository. [https://github.co...</td>\n",
       "      <td>https://paperswithcode.com/dataset/wsj-dow-jon...</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>ZInd</td>\n",
       "      <td>The Zillow Indoor Dataset (ZInD) provides exte...</td>\n",
       "      <td>https://paperswithcode.com/dataset/zind</td>\n",
       "      <td>None</td>\n",
       "      <td>Zillow Indoor Dataset</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>IMDB-Clean</td>\n",
       "      <td>We have cleaned the noisy IMDB-WIKI dataset us...</td>\n",
       "      <td>https://paperswithcode.com/dataset/imdb-clean</td>\n",
       "      <td>2021-06-21</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>TriggerCit 2021 Thailand / Nepal floods</td>\n",
       "      <td>Twitter dataset related to flood events onsets...</td>\n",
       "      <td>https://paperswithcode.com/dataset/triggercit-...</td>\n",
       "      <td>2022-02-24</td>\n",
       "      <td>Twitter dataset of flood-related images for Se...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5627 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                                 MNIST   \n",
       "1                                                CelebA   \n",
       "2                                              JFT-300M   \n",
       "3                                                  GLUE   \n",
       "4                                              MultiNLI   \n",
       "...                                                 ...   \n",
       "5622  Dataset for the Article \"Does the Venue of Sci...   \n",
       "5623                           WSJ Dow Jones Stock Data   \n",
       "5624                                               ZInd   \n",
       "5625                                         IMDB-Clean   \n",
       "5626            TriggerCit 2021 Thailand / Nepal floods   \n",
       "\n",
       "                                            description  \\\n",
       "0     The **MNIST** database (**Modified National In...   \n",
       "1     CelebFaces Attributes dataset contains 202,599...   \n",
       "2     **JFT-300M** is an internal Google dataset use...   \n",
       "3     General Language Understanding Evaluation (**G...   \n",
       "4     The **Multi-Genre Natural Language Inference**...   \n",
       "...                                                 ...   \n",
       "5622  Is there any correlation between the impact of...   \n",
       "5623  Please see code repository. [https://github.co...   \n",
       "5624  The Zillow Indoor Dataset (ZInD) provides exte...   \n",
       "5625  We have cleaned the noisy IMDB-WIKI dataset us...   \n",
       "5626  Twitter dataset related to flood events onsets...   \n",
       "\n",
       "                                                  owner        date  \\\n",
       "0              https://paperswithcode.com/dataset/mnist        None   \n",
       "1             https://paperswithcode.com/dataset/celeba  2015-01-01   \n",
       "2           https://paperswithcode.com/dataset/jft-300m  2017-07-10   \n",
       "3               https://paperswithcode.com/dataset/glue  2019-01-01   \n",
       "4           https://paperswithcode.com/dataset/multinli  2018-01-01   \n",
       "...                                                 ...         ...   \n",
       "5622  https://paperswithcode.com/dataset/dataset-for...  2021-05-31   \n",
       "5623  https://paperswithcode.com/dataset/wsj-dow-jon...  2022-02-15   \n",
       "5624            https://paperswithcode.com/dataset/zind        None   \n",
       "5625      https://paperswithcode.com/dataset/imdb-clean  2021-06-21   \n",
       "5626  https://paperswithcode.com/dataset/triggercit-...  2022-02-24   \n",
       "\n",
       "                                               subtitle keywords #downloads  \\\n",
       "0                                                           None       None   \n",
       "1                         CelebFaces Attributes Dataset     None       None   \n",
       "2                                              JFT-300M     None       None   \n",
       "3     General Language Understanding Evaluation benc...     None       None   \n",
       "4                Multi-Genre Natural Language Inference     None       None   \n",
       "...                                                 ...      ...        ...   \n",
       "5622                                                        None       None   \n",
       "5623                                                        None       None   \n",
       "5624                              Zillow Indoor Dataset     None       None   \n",
       "5625                                                        None       None   \n",
       "5626  Twitter dataset of flood-related images for Se...     None       None   \n",
       "\n",
       "     #views #votes dataset_slug  \n",
       "0      None   None               \n",
       "1      None   None               \n",
       "2      None   None               \n",
       "3      None   None               \n",
       "4      None   None               \n",
       "...     ...    ...          ...  \n",
       "5622   None   None               \n",
       "5623   None   None               \n",
       "5624   None   None               \n",
       "5625   None   None               \n",
       "5626   None   None               \n",
       "\n",
       "[5627 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['name', 'description', 'url', 'introduced_date', 'full_name']]\n",
    "df.rename(columns={\"name\": \"title\", 'full_name':'subtitle', 'introduced_date':'date', 'url':'owner'}, inplace=True)\n",
    "df['keywords'] = None\n",
    "df['#downloads'] = None\n",
    "df['#views'] = None\n",
    "df['#votes'] = None\n",
    "df['dataset_slug'] = ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UCgXDn5yLb4-"
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'date'] = pd.to_datetime(df['date']).dt.strftime(\"%d/%m/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('paperwithcode_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PaperswithCode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
