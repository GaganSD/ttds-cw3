	Unnamed: 0	title	subtitle	keyword	description	totalDownloads	totalViews	totalVotes	ownerUser	dataset_slug
0	0	The Bachelor Contestants	This dataset contains information about past seasons.	[]	"About this dataset
&gt; <p>Creating a bracket for The Bachelor is becoming trendy, so I've taken the data-driven approach and started aggregating data on past seasons to try to gain insights on potential outcomes this season. This dataset includes information available on Wikipedia. The following fields are provided for each contestant:</p>
<ul>
<li>Age</li>
<li>Eliminated (the week of the season the contestant was eliminated)</li>
<li>Hometown</li>
<li>Name</li>
<li>Occupation</li>
<li>Outcome (winner or loser)</li>
<li>Season</li>
</ul>
<p>Data was available for seasons 1, 2, 5, and seasons 9 through 21.</p>
This dataset was created by Adam Erispaha and contains around 500 samples along with Unnamed: 0, Age, technical information and other features such as:
- Hometown
- Eliminated
- and more.
How to use this dataset
&gt; - Analyze Outcome in relation to Season
- Study the influence of Occupation on Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Erispaha 
Start A New Notebook!"	0	5	1	yamqwe	the-bachelor-contestantse
1	1	Mineral photos	+39,000 images of mineral stored in 15 categories	['arts and entertainment']		5	31	0	floriangeillon	mineral-photos
2	2	Deepwater Ports	A Dataset of the Deepwater Port Licensing Program	['business', 'transportation', 'water transport']	"About this dataset
&gt; <p><strong>The Deepwater Port Licensing Program</strong> is the application process designed to promote the construction of <strong>Liquefied Natural Gas (LNG)</strong> and oil deepwater ports. This license system was established by the Deepwater Port Act of 1974, as amended by the <em>Maritime Transportation Security Act of 2002.</em></p>
<p><em>MARAD DOT</em> assesses the financial capability of potential licenses, prepares the project Record of Decision, and issues or denies the deepwater port license. If the license was denied, surrendered, or withdrawn they are not displayed in this layer. The licensed deepwater ports are listed below. <strong>Louisiana Offshore Oil Port (Louisiana) Neptune LNG (Massachusetts) Northeast Gateway (Massachusetts) Port Dolphin (Florida – Gulf of Mexico) Gulf Gateway Energy Bridge (Louisiana)</strong></p>
<p><a href=""https://hifld-dhs-gii.opendata.arcgis.com/datasets/768caec24d6c4b8aa1b2759faacd2bcc_1"" target=""_blank"" rel=""nofollow"">SOURCE</a></p>
This dataset was created by Homeland Infrastructure Foundation and contains around 0 samples along with Lease Block, Docket Number, technical information and other features such as:
- Fuel Type
- Deepwater Port
- and more.
How to use this dataset
&gt; - Analyze Port Type in relation to Maximum Unloading Time
- Study the influence of License on Maximum Send Out Capacity
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Homeland Infrastructure Foundation 
Start A New Notebook!"	1	7	1	yamqwe	deepwater-portse
3	3	ubiquant_train_target		[]		0	1	0	goweiting	ubiquant-train-target
4	4	easy_firefly_weights		[]		0	19	0	tejaschaudhari2811	easy-firefly-weights
5	5	CalWORKs Welfare Program Dataset	Calwork datasets from the California Health and Human Services Agency	['employment', 'government', 'health', 'social issues and advocacy']	"About this dataset
&gt; <p>This dataset includes data on the number of <a href=""http://www.cdss.ca.gov/calworks/"" target=""_blank"" rel=""nofollow"">CalWORKs</a> cases on hand during the month, by recipient and case type. The dataset is a selected subset of the entire report, available on the <a href=""http://www.dss.cahwnet.gov/cdssweb/default.htm"" target=""_blank"" rel=""nofollow"">California Department of Social Services</a>, <a href=""http://www.cdss.ca.gov/research"" target=""_blank"" rel=""nofollow"">Research and Data Reports (RADR) website</a>.</p>
<p>CalWORKs is a welfare program that gives cash aid and services to eligible needy California families. The program serves all 58 counties in the state and is operated locally by county welfare departments. If a family has little or no cash and needs housing, food, utilities, clothing or medical care, they may be eligible to receive immediate short-term help. Families that apply and qualify for ongoing assistance receive benefits each month to help pay for housing, food and other necessary expenses. Monthly CalWORKs data is collected from the counties through submission of the <a href=""http://www.cdss.ca.gov/research/PG281.htm"" target=""_blank"" rel=""nofollow"">CA 237CW CalWORKs Cash Grant Movement Report</a>, which is used to report statistical information on CalWORKs caseload movement for Two Parent Families, Zero Parent Families, All Other Families, Temporary Assistance for Needy Families (TANF) Timed-Out Cases and Safety Net/Fleeing Felon/Long-Term Sanction Cases (SN/FF/LTS).</p>
<h3>About CHHS Open Data</h3>
<p>The role of the <a href=""http://www.chhs.ca.gov/Pages/Home.aspx"" target=""_blank"" rel=""nofollow"">California Health and Human Services</a> (CHHS) Agency is to provide policy leadership and direction to the departments and programs it oversees, to reduce duplication and fragmentation and improve coordination among the departments, to ensure programmatic integrity, and to advance the Governor's priorities on health and human services issues. The California Health and Human Services Agency (CHHS) has launched its <a href=""https://chhs.data.ca.gov/"" target=""_blank"" rel=""nofollow"">Open Data Portal</a> initiative in order to increase public access to one of the State’s most valuable assets – non-confidential health and human services data. Its goals are to spark innovation, promote research and economic opportunities, engage public participation in government, increase transparency, and inform decision-making. ""Open Data"" describes data that are freely available, machine-readable, and formatted according to national technical standards to facilitate visibility and reuse of published data.</p>
<p>Citation: <em>Research Services Branch, California Department of Social Services. 2015.</em></p>
<p><a href=""https://chhs.data.ca.gov/Facilities-and-Services/Monthly-counts-of-California-Work-Opportunity-and-/d27w-f7d6"" target=""_blank"" rel=""nofollow"">DATASET</a></p>
This dataset was created by California Health and Human Services and contains around 40000 samples along with Month, Year, technical information and other features such as:
- Number Of Cases/ Persons
- Case Type
- and more.
How to use this dataset
&gt; - Analyze County in relation to Case/ Person Status
- Study the influence of Month on Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	2	13	1	yamqwe	ca-calworks-caseloade
6	6	Conferencias Mañaneras	Versiones escritas de las conferencias de la presidencia de México.	['government', 'politics', 'mexico']	Las versiones escritas de las conferencias de la presidencia de México, descargadas diariamente del sitio web oficial del gobierno y procesadas para hacerlas un poco más fácil de consumir programáticamente.	70	4996	8	ioexception	mananeras
7	7	Starfishmodel_02		[]		0	0	0	devonstanfield	starfishmodel-02
8	8	R/AskReddit Top and Hot Titles 01/30/2022 - ...	Dataset of Post titles from R/AskReddit from Hot and Top Hourly	['business']		18	199	3	camerinfigueroa	raskreddit
9	9	news_ria		[]		2	4	0	marynepo	news-ria
10	10	tmp_happywhalemodel		[]		0	0	0	dschettler8845	tmp-happywhalemodel
11	11	Binance ETHUSDT 5m OHLCV and more	Binance ETHUSDT OHLCV and more! (5m)	['finance', 'tabular data']	This dataset not only contains ETHUSDT OHLCV data regularly fetched from Binance but other important information such as OI and Long-Short Ratio. Please check the notebook section to see what kind of information is there.	3	43	0	code1110	binance-ethusdt-5m-ohlcv-and-more
12	12	Binance BTCUSDT 5m OHLCV and more	Binance BTCUSDT OHLCV and more! (5m)	['finance', 'tabular data']	This dataset not only contains BTCUSDT OHLCV data regularly fetched from Binance but other important information such as OI and Long-Short Ratio. Please check the notebook section to see what kind of information is there.	4	114	1	code1110	binance-btcusdt-5m-ohlcv-and-more
13	13	Stats-Bomb Football Data	Stats-Bomb Football Analytics Data	['sports', 'data analytics']	"StatsBomb Open Data
StatsBomb are committed to sharing new data and research publicly to enhance understanding of the game of Football. We want to actively encourage new research and analysis at all levels. Therefore we have made certain leagues of StatsBomb Data freely available for public use for research projects and genuine interest in football analytics.
StatsBomb are hoping that by making data freely available, we will extend the wider football analytics community and attract new talent to the industry. We would like to collect some basic personal information about users of our data. By giving us your email address, it means we will let you know when we make more data, tutorials and research available. We will store the information in accordance with our Privacy Policy and the GDPR.
Whilst we are keen to share data and facilitate research, we also urge you to be responsible with the data. Please register your details on https://www.statsbomb.com/resource-centre and read our User Agreement carefully.
Terms & Conditions
By using this repository, you are agreeing to the user agreement.
If you publish, share or distribute any research, analysis or insights based on this data, please state the data source as StatsBomb and use our logo, available in our Media Pack.
Getting Started
The data is provided as JSON files exported from the StatsBomb Data API, in the following structure:
Competition and seasons stored in competitions.json.
Matches for each competition and season, stored in matches. Each folder within is named for a competition ID, each file is named for a season ID within that competition.
Events and lineups for each match, stored in events and lineups respectively. Each file is named for a match ID.
StatsBomb 360 data for selected matches, stored in three-sixty. Each file is named for a match ID.
Some documentation about the meaning of different events and the format of the JSON can be found in the doc directory."	49	832	22	saurabhshahane	statsbomb-football-data
14	14	Nomeroff Russian license plates	Russian license plates recognition dataset	['law', 'automobiles and vehicles', 'computer vision']		13	164	7	evgrafovmaxim	nomeroff-russian-license-plates
15	15	KG20C Scholarly Knowledge Graph	A Scholarly Knowledge Graph Benchmark Dataset	['research', 'earth and nature', 'education', 'computer science', 'recommender systems', 'social networks']	"Context
This knowledge graph is constructed to aid research in scholarly data analysis. It can serve as a standard benchmark dataset for several tasks, including knowledge graph embedding, link prediction, recommendation systems, and question answering about high quality papers from 20 top computer science conferences.
This has been introduced and used in the PhD thesis Multi-Relational Embedding for Knowledge Graph Representation and Analysis and TPDL'19 paper Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space. 
Content
Construction protocol
Scholarly data
From the Microsoft Academic Graph dataset, we extracted high quality computer science papers published in top conferences between 1990 and 2010. The top conference list are based on the CORE ranking A conferences. The data was cleaned by removing conferences with less than 300 publications and papers with less than 20 citations. The final list includes 20 top conferences: AAAI, AAMAS, ACL, CHI, COLT, DCC, EC, FOCS, ICCV, ICDE, ICDM, ICML, ICSE, IJCAI, NIPS, SIGGRAPH, SIGIR, SIGMOD, UAI, and WWW*.
Knowledge graph
The scholarly dataset was converted to a knowledge graph by defining the entities, the relations, and constructing the triples. The knowledge graph can be seen as a labeled multi-digraph between scholarly entities, where the edge labels express there relationships between the nodes. We use 5 intrinsic entity types including Paper, Author, Affiliation, Venue, and Domain. We also use 5 intrinsic relation types between the entities including author_in_affiliation, author_write_paper, paper_in_domain, paper_cite_paper, and paper_in_venue.
Benchmark data splitting
The knowledge graph was split uniformly at random into the training, validation, and test sets. We made sure that all entities and relations in the validation and test sets also appear in the training set so that their embeddings can be learned. We also made sure that there is no data leakage and no redundant triples in these splits, thus, constitute a challenging benchmark for link prediction similar to WN18RR and FB15K-237.
Data content
File format
All files are in tab-separated-values format, compatible with other popular benchmark datasets including WN18RR and FB15K-237. For example, train.txt includes ""28674CFA    author_in_affiliation   075CFC38"", which denotes the author with id 28674CFA works in the affiliation with id 075CFC38. The repo includes these files:
- all_entity_info.txt contains id  name  type of all entities
- all_relation_info.txt contains id of all relations
- train.txt contains training triples of the form entity_1_id  relation_id  entity_2_id
- valid.txt contains validation triples
- test.txt contains test triples
Statistics
Data statistics of the KG20C knowledge graph:
Author | Paper | Conference | Domain | Affiliation
:---: | :---: | :---: | :---: | :---:
8,680 | 5,047 | 20 | 1,923 | 692
Entities | Relations | Training triples | Validation triples | Test triples
:---: | :---: | :---: | :---: | :---:
16,362 | 5 | 48,213 | 3,670 | 3,724
Acknowledgements
For the dataset and semantic query method, please cite:
- Hung Nghiep Tran and Atsuhiro Takasu. Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space. In Proceedings of International Conference on Theory and Practice of Digital Libraries (TPDL), 2019.
For the MEI knowledge graph embedding model, please cite:
- Hung Nghiep Tran and Atsuhiro Takasu. Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion. In Proceedings of the European Conference on Artificial Intelligence (ECAI), 2020.
For the baseline results and extended semantic query method, please cite:
- Hung Nghiep Tran. Multi-Relational Embedding for Knowledge Graph Representation and Analysis. PhD Dissertation, The Graduate University for Advanced Studies, SOKENDAI, Japan, 2020.
For the Microsoft Academic Graph dataset, please cite:
- Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. An Overview of Microsoft Academic Service (MAS) and Applications. In Proceedings of the International Conference on World Wide Web (WWW), 2015.
Inspiration
We include the baseline results for two tasks on the KG20C dataset, link prediction and semantic queries. Link prediction is a relational query task given a relation and the head or tail entity to predict the corresponding tail or head entities. Semantic queries include human-friendly query on the scholarly data. MRR is the mean reciprocal rank, Hit@k is the percentage of correct predictions at top k. 
For more information, please refer to the citations.
Link prediction results
We report results for 4 methods. Random, which is just random guess to show the task difficulty. Word2vec, which is the popular embedding method. SimplE/CP and MEI are two recent knowledge graph embedding methods.
All models are in small size settings, equivalent to total embedding size 100 (50x2 for Word2vec and SimplE/CP, 10x10 for MEI).
Models | MRR | Hit@1 | Hit@3 | Hit@10
:--- | :---: | :---: | :---: | :---:
Random | 0.001 | &lt; 5e-4 | &lt; 5e-4 | &lt; 5e-4
Word2vec (small) | 0.068 | 0.011 | 0.070 | 0.177
SimplE/CP (small) | 0.215 | 0.148 | 0.234 | 0.348
MEI (small) | 0.230 | 0.157 | 0.258 | 0.368
Semantic queries results
The following results demonstrate semantic queries on knowledge graph embedding space, using the above MEI (small) model.
Queries | MRR | Hit@1 | Hit@3 | Hit@10
:--- | :---: | :---: | :---: | :---:
Who may work at this organization? | 0.299 | 0.221 | 0.342 | 0.440
Where may this author work at? | 0.626 | 0.562 | 0.669 | 0.731
Who may write this paper? | 0.247 | 0.164 | 0.283 | 0.405
What papers may this author write? | 0.273 | 0.182 | 0.324 | 0.430
Which papers may cite this paper? | 0.116 | 0.033 | 0.120 | 0.290
Which papers may this paper cite? | 0.193 | 0.097 | 0.225 | 0.404
Which papers may belong to this domain? | 0.052 | 0.025 | 0.049 | 0.100
Which may be the domains of this paper? | 0.189 | 0.114 | 0.206 | 0.333
Which papers may publish in this conference? | 0.148 | 0.084 | 0.168 | 0.257
Which conferences may this paper publish in? | 0.693 | 0.542 | 0.810 | 0.976"	15	843	5	tranhungnghiep	kg20c-scholarly-knowledge-graph
16	16	150k Python Source Code Dataset	Machine Learning for Programming	['computer science', 'programming', 'data cleaning', 'nlp', 'recommender systems', 'text data']	"Dataset
Dataset added from SRILAB Machine Learning for Programming
This project combines programming languages and machine learning for building statistical programming engines -- systems built on top of machine learning models of large codebases. These are new kinds of engines that can provide statistically likely solutions to problems that are difficult or impossible to solve with traditional techniques.
150k Python Dataset
This dataset is released as a part of the Machine Learning for Programming project that aims to create new kinds of programming tools and techniques based on machine learning and statistical models learned over massive codebases. For more information about the project, tools and other resources please visit the main project page.
OVERVIEW
We provide a dataset consisting of parsed Parsed ASTs that were used to train and evaluate the DeepSyn tool. The Python programs are collected from GitHub repositories by removing duplicate files, removing project forks (copy of another existing repository), keeping only programs that parse and have at most 30'000 nodes in the AST and we aim to remove obfuscated files. Furthermore, we only used repositories with permissive and non-viral licenses such as MIT, BSD and Apache. For parsing, we used the Python AST parser included in Python 2.7. We also include the parser as part of our dataset. The dataset is split into two parts -- 100'000 files used for training and 50'000 files used for evaluation.
Published research using this dataset may cite the following paper:
&gt;Raychev, V., Bielik, P., and Vechev, M. Probabilistic Model for Code with Decision Trees. 
&gt;In Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (2016), OOPSLA ’16, ACM"	43	800	8	veeralakrishna	150k-python-dataset
17	17	Opensourcefordatascience	Opensourcefordatascience	[]		0	6	0	natalyguerra	opensourcefordatascience
18	18	Smart Activity Distribution	Algorithm for getting less busy members and smart distribution of activities	['computer science']		0	6	1	jefersonstenio	dataset-test
19	19	fbp_longformer_v2		[]		0	1	0	shobhitupadhyaya	fbp-longformer-v2
20	20	Starfishmodel_01		[]		0	0	0	devonstanfield	starfishmodel-01
21	21	Stock Market Data (NASDAQ, NYSE, S&P500)	Date, Volume, High, Low, Close (updated weekly)	['business', 'finance']	"Context
Daily stock market prices.
Content
Date, Volume, High, Low, and Closing Price (for all NASDAQ, S&P500, and NYSE listed companies).  Updated weekly.
Acknowledgements
Banner Photo: https://unsplash.com/photos/amLfrL8LGls"	3518	43940	183	paultimothymooney	stock-market-data
22	22	Bitcoin and Altcoins prices	Bitcoin and Altcoins prices, changes in the instant service.	['currencies and foreign exchange']		0	1	0	ashutoshsharma02	bitcoin-and-altcoins-prices
23	23	retinanet350epoch410		[]		1	4	0	haibin1	retinanet350epoch410
24	24	Jobs Crawling		[]		4	76	0	azizainunnajib	jobs-crawling
25	25	US S&P500 SPX Components Stocks Historical Price	Updated US S&P500 SPX 505 Components Stocks Historical Price, since IPOs	['business', 'finance', 'text data', 'investing']	"Introduction
These data files are generated by my python script.
Source
The data is collected from Yahoo Finance."	75	1036	10	benjaminpo	sp500-spx-components-stock-price
26	26	detonly_cascade_imgscale_pretrainedLiveTrainAsTest		[]		0	9	0	itaynivtau	detonly-cascade-imgscale-pretrainedlivetrainastest
27	27	Im2Text	Describing Images Using 1 Million Captioned Photographs	['universities and colleges', 'cities and urban areas', 'nlp', 'deep learning', 'image data', 'text data']	"""We develop and demonstrate automatic image description methods using a large
captioned photo collection. One contribution is our technique for the automatic
collection of this new dataset – performing a huge number of Flickr queries and
then filtering the noisy results down to 1 million images with associated visually
relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric
methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to
produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.""
Vicente Ordonez, Girish Kulkarni, Tamara L Berg
Stony Brook University
Stony Brook, NY 11794
source: http://tamaraberg.com/papers/generation_nips2011.pdf"	1	109	1	alroger	im2text
28	28	Wikipedia-based Image Text WIT 0.01 percent_sample	WIT Dataset is a large multimodal multilingual dataset.	[]		1	14	0	alroger	wikipediabased-image-text-wit-001-percent-sample
29	29	LabData	COMPX310 Lab 11 Dataset	['earth and nature']		0	696	0	anthonywilson1	labdata
30	30	Cyclistic Case Study - Google Certificate	Divvy bicycle sharing service	['education', 'cycling', 'exploratory data analysis', 'data visualization', 'data analytics']	Lyft Bikes and Scooters, LLC (“Bikeshare”) operates the City of Chicago’s (“City”) Divvy bicycle-sharing service. Bikeshare and the City are committed to supporting bicycling as an alternative transportation option. As part of that commitment, the City permits Bikeshare to make certain Divvy system data owned by the City (“Data”) available to the public, subject to aggrement	2	36	0	kirilosyossif	cyclistic-case-study-google-certificate
31	31	transformers_lib		[]		1	4	0	dwchen	transformers-lib
32	32	Occupation and Earnings by Ed	The National Center for Education Statistics (NCES)	['business', 'education']	"About this dataset
&gt; <p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting and analyzing data related to education in the U.S. and other nations. NCES is located within the U.S. Department of Education and the Institute of Education Sciences. NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics on the condition of American education; conduct and publish reports; and review and report on education activities internationally.</p>
<ul>
<li>Table 502.10. Occupation of employed persons 25 years old and over, by highest level of educational attainment and sex: 2014 and 2015</li>
<li>Table 502.20. Median annual earnings, number, and percentage of full-time year-round workers 25 years old and over, by highest level of educational attainment and sex: 1990 through 2014</li>
<li>Table 502.30. Median annual earnings of full-time year-round workers 25 to 34 years old and full-time year-round workers as a percentage of the labor force, by sex, race/ethnicity, and educational attainment: Selected years, 1995 through 2014</li>
<li>Table 502.40. Annual earnings of persons 25 years old and over, by highest level of educational attainment and sex: 2014</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://nces.ed.gov/programs/digest/current_tables.asp"" target=""_blank"" rel=""nofollow"">https://nces.ed.gov/programs/digest/current_tables.asp</a></p>
This dataset was created by National Center for Education Statistics and contains around 100 samples along with Unnamed: 11, Unnamed: 17, technical information and other features such as:
- Unnamed: 14
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 13 in relation to Unnamed: 21
- Study the influence of Unnamed: 18 on Unnamed: 9
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Center for Education Statistics 
Start A New Notebook!"	13	108	2	yamqwe	occupation-and-earnings-by-ede
33	33	iLog-Sugar-Rich-Foods	Images of sugar rich foods to and their XML files to analyze eating behavior. 	['mental health', 'diseases', 'image data', 'food', 'diabetes']		0	2	0	laavanya	ilogsugarrichfoods
34	34	Rocket League Championship Series 2021-2022	RLCS 2021-22 games' replays uploaded on ballchasing.com	['games', 'video games', 'global', 'sports', 'text data']	"Introduction: Rocket League & RLCS
&gt; Rocket League is a vehicular soccer video game developed and published by Psyonix. The game was first released for Microsoft Windows and PlayStation 4 in July 2015, with ports for Xbox One and Nintendo Switch being released later on.
&gt; Described as ""soccer, but with rocket-powered cars"", Rocket League has up to eight players assigned to each of the two teams, using rocket-powered vehicles to hit a ball into their opponent's goal and score points over the course of a match. The game includes single-player and multiplayer modes that can be played both locally and online, including cross-platform play between all versions. Later updates for the game enabled the ability to modify core rules and added new game modes, including ones based on ice hockey and basketball.
&gt; [...] 
&gt; Rocket League was praised for its gameplay improvements over Battle-Cars, as well as its graphics and overall presentation, although some criticism was directed towards the game's physics engine. The game earned a number of industry awards, and saw over 10 million sales and 40 million players by the beginning of 2018. Rocket League has also been adopted as an esport, with professional players participating through ESL and Major League Gaming along with Psyonix's own Rocket League Championship Series (RLCS). [...]
Rocket League Wikipédia page
&gt; The Rocket League Championship Series (RLCS) is an annual (previously semiannual) Rocket League Esports tournament series produced by Psyonix, the game's developer. It consists of qualification splits in North America, South America, Europe, Oceania, Middle East/North Africa, Asia, and Sub-Saharan Africa, and culminates in a playoff bracket with teams from those regions. The qualification rounds are played as an online round-robin tournament and the finals are played live in different cities. [...]
RLCS Wikipédia page
Content
This is a collection of over 5 600 RLCS games within all region (EU - Europe, NA - North America, SAM - South America, OCE - Oceania, MENA - Middle East & North Africa, APAC - Asia Pacific North/South and SSA - Sub-Saharan African) and the Fall Major (for now), stored into the ballchasing.com group RLCS 21-22, as a JSON file and covering the following events and phases:
Fall Split (Main Events*)
Regionals 1
Regionals 2
Regionals 3
Fall Major
Winter Split (Main Events*)
Regionals 1
Regionals 2

Closed Qualifier, Tiebreaker series and APAC qualifier for the Fall Major are missing for now. These data will be provided as soon as possible subject to their availability. Open Qualifier games could be added subject to their availability too.
Some games are still missing and some replays are  ""truncated"". The associated data will be added / patched as soon as possible.
For better understatement

Regarding Rocket League gameplay
Rocket League scoring.
Pads & Boost
Powerslide: when a player use boost while drifting.
Unreal Units: Unreal Engine (Rocket League game engine) used UU as inside metric system, for distance 1 UU is equal to 1 cm.
Supersonic speed: reachable car speed at 2.2k UU/s which able a player to demolish an opponent, see more about Supersonic speed and Max Speed with this video.
Overtime: Rocket League games can't generally end with a tie and RLCS match are not exception. An overtime is an unlimited second period of the game where the first team to score win the game.
Regarding RLCS
RLCS rules: regions, splits and more.
Team names in .csv files may vary from the original raw.json file due to numerous aliases for certain teams (FC BARCELONA is also written as BARCA ESPORTS or FCB, for example).
Player could change their username p_name during the season, so the platform ID p_platform_id is a better identifier for each player during the whole season, even if some players could play on different platforms through the season (in that case, most of time, they're using their Epic account in place of the Steam one).
Best of 5/7: depending on the stage of an event, an RLCS match can be played to the best of 5 or 7 games, with special treatment for the Fall Split finals played in 2 winning sets of BO7.
ballchasing.com groups: games could be stored in different groups made by community members. So, the RLCS 21-22 group is not the only place where replays files and data could be extracted.
Acknowledgements
Many thanks to Can't Fly for this amazing work on ballchasing.com through the years, building this website as a Rocket League games hub and with a well done API to get the data available here.
Also, many thanks to Psyonix for the work done with Rocket League and the RLCS, community members and RLCS' admins contributing by uploading the replay files on ballchasing.com.
Inspiration
Data visualisation.
Data analysis.
Score prediction.
Repository
The present datasets are attached to the following project / GitHub repository: ballchasing_ml."	7	271	2	dylanmonfret	rlcs-202122
35	35	age-detection		[]		0	2	0	bhushanaditya	agedetection
36	36	clean_review_hemani		[]		0	0	0	hemanishah	clean-review-hemani
37	37	NBA Drafts Since 1989	NBA Drafts with Additional Data 	['basketball', 'sports']	"About this dataset
&gt; Source: http://www.basketball-reference.com/draft/NBA_2016.html
NBA Drafts since there have been 2 rounds. This dataset was created by Gabe Salzer and contains around 2000 samples along with Draft Year, Box Plus/ Minus, technical information and other features such as:
- Round
- Minuts Played
- and more.
How to use this dataset
&gt; - Analyze Points in relation to Win Share
- Study the influence of Fg% on Pick
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gabe Salzer 
Start A New Notebook!"	12	57	3	yamqwe	nba-drafts-2016-1989e
38	38	Cyclist Capstone		[]		0	0	0	bethstafford	cyclist-capstone
39	39	mwigadata		[]		0	4	0	mikemollel	mwigadata
40	40	Happywhale Test TFRecords 1		[]		0	1	0	mmelahi	happywhale-test-tfrecords-1
41	41	Rent/Buy Houses Information in Quito	Dataset of houses and apartments in Quito	['global', 'housing', 'tabular data']	"Context
Dataset for houses and apartments in Quito. This dataset was web scrapped from a popular real state website. I used to invest in real state in Quito.
Content
The content is very simple. It contains what can be used for a simple analysis of rent and sales in Quito. 
Inspiration
The dataset helped me to learn the most profitable neighborhoods to buy houses or apartments in Quito."	5	166	1	aleespinosa	housingquito
42	42	supply chain in multi mode for ETA		['tabular data', 'e-commerce services', 'travel', 'datetime']		0	2	0	zsham433	supplychain
43	43	Kaggle Weekly GPU Quotas	Complete History of Weekly Kaggle GPU Hour Limits in Notebooks	['gpu']	"Context
In early August 2020, Kaggle announced a ""floating quota for GPU hours"" in Notebooks. Previously, we had 30 GPU hours per week. Those were the dark times. Over the following 2 months, the quotas varied between 36 and a whopping 43 hours. I have been keeping track of the weekly changes, because why not? I will continue to update this amazing dataset. You're welcome.
Content
This dataset documents all weekly GPU quotas since August 8th 2020. This will sound more impressive once more than a few months have passed. Hopefully.
Quotas are being updated and reset every week on midnight CET from Friday to Saturday.
Column Description
The dataset contains an impressive two columns: date, which is the start date, and gpu_hours, which is not the start date.
Acknowledgements
Banner and vignette photo by Christian Wiediger on Unsplash."	43	4181	49	headsortails	kaggle-weekly-gpu-quotas
44	44	Happywhale Test TFRecords 0		[]		0	2	0	mmelahi	happywhale-test-tfrecords-0
45	45	Baby Names for US, States, & Territories	National, State, and Territory level baby names in a series of text files	['social science', 'demographics']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>In 1998, the Social Security Administration published <a href=""https://www.ssa.gov/oact/NOTES/note139/original_note139.html"" target=""_blank"" rel=""nofollow"">Actuarial Note #139</a>, Name Distributions in the Social Security Area, August 1997, on the distribution of given names of Social Security number holders. The note, written by actuary Michael W. Shackleford, gave birth to the present website.</p>
<h2><strong>Data Source</strong></h2>
<p>All names are from Social Security card applications for births that occurred in the United States after 1879. Note that many people born before 1937 never applied for a Social Security card, so their names are not included in our data. For others who did apply, our records may not show the place of birth, and again their names are not included in our data.<br>
All data are from a 100% sample of our records on Social Security card applications as of March 2021.</p>
<p><strong>Find the original <code>.txt</code> files <a href=""https://www.ssa.gov/oact/babynames/limits.html"" target=""_blank"" rel=""nofollow"">here</a>.</strong></p>
<h2><strong>This Dataset</strong></h2>
<p>The Social Security Administration releases National, State, and Territory level baby name datasets in a series of text files. In this dataset, those text files have been combined, column headers have been added, and the new tables were saved as <code>.csv</code> files.</p>
<h2><strong>Usage Notes from the SSA</strong></h2>
<p>People using our data on popular names are urged to explicitly acknowledge the following qualifications.</p>
<ol>
<li>Names are restricted to cases where the year of birth, sex, and state of birth are on record, and where the given name is at least 2 characters long.</li>
<li>National name data is restricted to births in the 50 States and District of Columbia. We also provide popular names for births in U. S. territories (American Samoa, Guam, Northern Mariana Islands, Puerto Rico and the U.S. Virgin Islands). The data on births in U. S. territories are not included in our national data.</li>
<li>Name data are tabulated from the ""First Name"" field of the Social Security Card Application. Hyphens and spaces are removed, thus Julie-Anne, Julie Anne, and Julieanne will be counted as a single entry.</li>
<li>Name data are not edited. For example, the sex associated with a name may be incorrect. Entries such as ""Unknown"" and ""Baby"" are not removed from the lists.</li>
<li>Different spellings of similar names are not combined. For example, the names Caitlin, Caitlyn, Kaitlin, Kaitlyn, Kaitlynn, Katelyn, and Katelynn are considered separate names and each has its own rank.</li>
<li>When two different names are tied with the same frequency for a given year of birth, we break the tie by assigning rank in alphabetical order.</li>
<li>Some names are applied to both males and females (for example, Micah). Our rankings are done by sex, so that a name such as Micah will have a different rank for males as compared to females. When you seek the popularity of a specific name (see ""Popularity of a Name""), you can specify the sex. If you do not specify the sex, we provide rankings for the more popular name-sex combination.</li>
<li>To safeguard privacy, we exclude from our tabulated lists of names those that would indicate, or would allow the ability to determine, names with fewer than 5 occurrences in any geographic area. If a name has less than 5 occurrences for a year of birth in any state, the sum of the state counts for that year will be less than the national count.</li>
</ol>
This dataset was created by Amber Thomas and contains around 30000 samples along with Year, Count, technical information and other features such as:
- Territory Code
- Name
- and more.
How to use this dataset
&gt; - Analyze Sex in relation to Year
- Study the influence of Count on Territory Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	28	156	2	yamqwe	baby-names-for-us-states-territoriese
46	46	h and m ecommenderr	ham ham fashion recommender dataset	['exercise', 'image data']	"Context
The story of my dataset is about
Vancouver Edmonton Sask Winnipeg
Thunderbird SUD Ottawa Montreal
Toronto based transportation plan.
In rewording process and here,
data is about H&M fashion giving you
too many options so we machine
learners can dig into bras, socks,
pants and so on to find something
that is relevant to you that you
otherwise not known forever.
Content
What's inside is are just rows and columns. 
You don't need them but you need the insight.
Acknowledgements
Thank you H&M. I bought some socks.
Inspiration
Do your walk. Mandate Exercise.
Spark fashion walk season is back.
Truckers wear orange."	0	12	0	jkstarc	h-and-m-ecommenderr
47	47	Security-wise Price Volume Data - NSE	This dataset contains security wise price volume data.	['india', 'investing', 'python']	"Context
NSE uploads security wise price volume data daily after the market closes. It is not possible all the get data for all the stocks for all the years. Hence the dataset.
Content
This dataset contains security wise daily price volume data since the beginning."	24	63	2	pratikghoshaiml	securitywise-price-volume-data-nse
48	48	Ajax E-commerce - Sales and Inventory	Anonymous subset of real order and stock move data of an e-commerce store	[]		104	1519	5	switch21	ajax-ecommerce-sales-and-inventory
49	49	yolov5	Ultralytics YoloV5 for offline use	['computer vision', 'pytorch']	"This is the yoloV5 model cloned from ultralytics/yolov5 for offline use.
Setup
Add this dataset to the notebook and run the following commands.
Add model weights. You can use your own or take it from here.
!mkdir /root/.config/Ultralytics/
!cp ../input/yolo-arial/Arial.ttf /root/.config/Ultralytics/Arial.ttf
Load model
You can now call torch.hub.load() to load the yolov5 model offline. Make sure to set the parameters as
repo or dir = './path/to/local/yolov5'
model = 'custom'
source = 'local'
force_reload = True
path = './path/to/best.pt'
Example
Using yolov5x6 weights
import torch
yolov5x6_model = torch.hub.load('../input/yolov5', 'custom', source='local', force_reload=True, path='../input/ultralyticsyolov5aweights/yolov5x6.pt')"	15	578	2	nilavanakilan	yolov5
50	50	MHW dataset	Word Level Traditional Mongolian Offline Handwritten Dataset	['china ', 'computer science', 'classification', 'image data']	"Content
This is a word level tradition Mongolian offline handwriting dataset(MHW). The training set of MHW consists of 5000 words, each word is written 20 times, so it contains 100000 pieces of Mongolian words written by 200 different writers. The testing set I of MHW consists of 1000 words, each word is written 5 times by the same writers with training set. The testing set II of MHW consists of 939 words; each word is written 15 times written by 50 different writers with training set. The words in training and testing set have a few intersections. It is designed for training and testing recognition systems for handwritten Mongolian words. The MHW is available for the purpose of research.
One Sample Example:
Acknowledgements
Thanks to all the people who worked hard during the creation of the dataset.This work was funded by National Natural Science Foun-
dation of China (Grant No. 61763034) .
Inspiration
For more details, please go to https://sourceforge.net/p/mongolian-hw/wiki/Home/"	7	992	6	fandaoerji	mhw-dataset
51	51	Amazon Appliances Recommendation System Dataset	The data of user rating for products of appliances category at Amazon	[]		18	898	2	steven220899	amazon-appliances-recommendation-system-dataset
52	52	Covid19-new_cases	Daily new cases for all countries	['universities and colleges']	"Context
Data from John Hopkins University"	17	148	0	mikayelgrigoryan	covid19new-cases
53	53	SCI01 - UFW - ufw.log - SEMANAL	Log do UFW, atualizado diariamente	[]		0	102	0	rafaelpbmota	sci01-ufw-semanal
54	54	lstm_model_black_white_reversal		[]		0	1	0	narajamalova	lstm-model-black-white-reversal
55	55	kc_house_data		[]		0	1	0	fareshosny55	kc-house-data
56	56	[YOLO] Sheep, colored and occluded	All images captured from drones	['retail and shopping']		2	11	0	bjosttveit	yolo-sheep-colored-and-occluded
57	57	____tmp dataset for 1024x1660 happywhale		['online communities']		0	3	0	dschettler8845	-tmp-dataset-for-1024x1660-happywhale
58	58	Tsla data		[]		0	1	0	dffffssas	tsla-data
59	59	Happywhale 2022	Personal Public Dataset on HappyWhale 2022 competition	[]	"Source:
The original dataset can be found here: https://www.kaggle.com/c/happy-whale-and-dolphin/data
Content:
train.csv - the prepped dataset
embeddings - the image embeddings
train_images & test_images - cropped using this notebook by Awsaf and then resized to 128x128"	0	13	0	andradaolteanu	happywhale-2022
60	60	Tweets Brexit		[]		1	2	0	alvarorev	tweets-brexit
61	61	World Bank Data By Nation And Region	Population, population, gender and other features	['global', 'banking', 'social science', 'economics']	"About this dataset
&gt; <p>Here are some selected data items from the World Bank.<br>
More data can be found here, which can be easily conjoined with this data using the country names or codes:</p>
<p><a href=""http://data.worldbank.org/indicator?tab=all"">http://data.worldbank.org/indicator?tab=all</a></p>
<p>I have also totalled some of the data by world region.  We tend to organize data by ""continent,"" an outdated way of seeing things.</p>
<p>The great geographer H.J. de Blij and his co-authors have re-organized the world into regions which share cultural and economic features.  Their textbook covers these regions in detail:</p>
<p><a href=""https://www.amazon.com/Geography-Realms-Regions-Concepts-16th/dp/1118673956/ref=sr_1_1?ie=UTF8&amp;qid=1481120232&amp;sr=8-1&amp;keywords=de+blij+regions"" target=""_blank"" rel=""nofollow"">https://www.amazon.com/Geography-Realms-Regions-Concepts-16th/dp/1118673956/ref=sr_1_1?ie=UTF8&amp;qid=1481120232&amp;sr=8-1&amp;keywords=de+blij+regions</a></p>
<p>The old idea of continents was never very consistent.  If for example a continent is a land mass surrounded by water, then Europe is not a continent, but a peninsula of Asia.  On the other hand, to treat all of Asia from Indonesia, the world's most populous primarily Islamic Nation, to Israel, as one continent made no sense.  de Blij's regions are much more helpful in understanding the world.</p>
This dataset was created by Gary Hoover and contains around 300 samples along with Popltn Largest City % Of Urban Pop, 2014 Life Expectancy At Birth, Total (years), technical information and other features such as:
- Region Code
- Population Cgr 1960 2015
- and more.
How to use this dataset
&gt; - Analyze Exports Of Goods And Services (% Of Gdp) in relation to Population, Total
- Study the influence of Literacy Rate, Adult Female (% Of Females Ages 15 And Above) on Gdp, Ppp (current International $)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	52	280	4	yamqwe	world-bank-datae
62	62	Dutch News Articles	A dataset containing news articles published by the NOS.	['exploratory data analysis', 'nlp', 'text data', 'news', 'transformers']	"Dutch News Articles
This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.
Features:
datetime: date and time of publication of the article.
title: the title of the news article.
content: the content of the news article.
category: the category under which the NOS filed the article.
url: link to the original article.
About the data
The title and content of features somewhat clean. Meaning extra whites spaces and newlines are removed. Furthermore, these features are normalized (NFKD). The NOS also publishes liveblogs. The posts in this live blog are not part of this dataset. 
Example
I used this dataset in a recent blog post."	296	4057	30	maxscheijen	dutch-news-articles
63	63	Spotify and Genius Track Dataset	SpotGenTrack Popularity Dataset	['music']	"SpotGenTrack Popularity Dataset
The data was collected from Spotify and Genius throughout their respective API's.
Acknowledgements
Martín-Gutiérrez, David; Hernández-Peñaloza, Gustavo; Belmonte-Hernández, Alberto; Álvarez, Federico (2019), “SpotGenTrack Popularity Dataset”, Mendeley Data, V1, doi: 10.17632/4m2x4zngny.1
Inspiration
A database for Music Popularity Prediction, Genre Classification, Automatic music tagging, music similarity and many other interdisciplinary applications within the Music Information Retrieval  (MIR) field."	910	10056	52	saurabhshahane	spotgen-music-dataset
64	64	divvy-tripdata	12 months (2020-10 to 2021-09) of bike-sharing data	['united states', 'transportation', 'cycling', 'data analytics', 'text data']	"12 months (2020-10 to 2021-09) of bike-sharing data from Motivate International Inc. uploaded for the purpose of showcasing my capstone project for the Google Data Analyst course on Coursera.
Motivate International Inc. (“Motivate”) operates the City of Chicago’s (“City”) Divvy bicycle sharing service. Motivate and the City are committed to supporting bicycling as an alternative transportation option. As part of that commitment, the City permits Motivate to make certain Divvy system data owned by the City (“Data”) available to the public, subject to the terms and conditions of this License Agreement (“Agreement”)."	11	682	8	raymondmutyaba	divvytripdata
65	65	E-Commerce Product Reviews - Dataset for ML	Turkish product reviews collected from Turkish E-commerce - Sentiment Analysis	['business', 'classification', 'svm', 'text data', 'binary classification', 'ratings and reviews', 'e-commerce services']	"-&gt; If you use Turkish_Product_Reviews_by_Gozukara_and_Ozel_2016 dataset please cite: https://dergipark.org.tr/en/pub/cukurovaummfd/issue/28708/310341
@research article { cukurovaummfd310341, journal = {Çukurova Üniversitesi Mühendislik-Mimarlık Fakültesi Dergisi}, issn = {1019-1011}, eissn = {2564-7520}, address = {Çukurova Üniversitesi Mühendislik-Mimarlık Fakültesi Dergisi Yayın Kurulu Başkanlığı 01330 ADANA}, publisher = {Cukurova University}, year = {2016}, volume = {31}, pages = {464 - 482}, doi = {10.21605/cukurovaummfd.310341}, title = {Türkçe ve İngilizce Yorumların Duygu Analizinde Doküman Vektörü Hesaplama Yöntemleri için Bir Deneysel İnceleme}, key = {cite}, author = {Gözükara, Furkan and Özel, Selma Ayşe} }
https://doi.org/10.21605/cukurovaummfd.310341
-&gt; Turkish_Product_Reviews_by_Gozukara_and_Ozel_2016 dataset is composed as below:
-&gt;-&gt; Top 50 E-commerce sites in Turkey are crawled and their comments are extracted. Then randomly 2000 comments selected and manually labelled by a field expert.
-&gt;-&gt; After manual labeling the selected comments is done, 600 negative and 600 positive comments are left. 
-&gt;-&gt; This dataset contains these comments.
-&gt; English_Movie_Reviews_by_Pang_and_Lee_2004
-&gt;-&gt; Pang, B., Lee, L., 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts, In Proceedings of the 42nd annual meeting on Association for Computational Linguistics (p. 271).
-&gt;-&gt; Source: https://www.cs.cornell.edu/people/pabo/movie-review-data/ | polarity dataset v2.0 - review_polarity.tar.gz 
-&gt; English_Movie_Reviews_Sentences_by_Pang_and_Lee_2005
-&gt;-&gt; Pang, B., Lee, L., 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales, In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (pp. 115-124), Association for Computational Linguistics
-&gt;-&gt; Source: https://www.cs.cornell.edu/people/pabo/movie-review-data/ | sentence polarity dataset v1.0 - rt-polaritydata.tar.gz
-&gt; English_Product_Reviews_by_Blitzer_et_al_2007
-&gt;-&gt; Article of the dataset: Blitzer, J., Dredze, M., Pereira, F., 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification, In ACL (Vol. 7, pp. 440-447).
-&gt;-&gt; Source: http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ | [processed_acl.tar.gz] (19 M)
-&gt; Turkish_Movie_Reviews_by_Demirtas_and_Pechenizkiy_2013
-&gt;-&gt; Demirtas, E., Pechenizkiy, M., 2013. Cross-lingual polarity detection with machine translation, In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (p. 9). ACM. 
-&gt;-&gt; http://www.win.tue.nl/~mpechen/projects/smm/#Datasets Turkish_Movie_Sentiment.zip
-&gt; The dataset files are provided as used in the article. 
-&gt; Weka files are generated with Raw Frequency of terms rather than used Weighting Schemes
-&gt; The folder Cross_Validation contains 10-fold cross-validation each fold files.
-&gt; Inside Cross_Validation folder, each turn of the cross-validation is named as test_X where X is the turn number
-&gt; Inside test_X folder
* Test_Set_Negative_RAW: Contains raw negative class Test data of that cross-validation turn
* Test_Set_Negative_Processed: Contains pre-processed negative class Test data of that cross-validation turn
* Test_Set_Positive_RAW: Contains raw positive class Test data of that cross-validation turn
* Test_Set_Positive_Processed: Contains pre-processed positive class Test data of that cross-validation turn
* Train_Set_Negative_RAW: Contains raw negative class Train data of that cross-validation turn
* Train_Set_Negative_Processed: Contains pre-processed negative class Train data of that cross-validation turn
* Train_Set_Positive_RAW: Contains raw positive class Train data of that cross-validation turn
* Train_Set_Positive_Processed: Contains pre-processed positive class Train data of that cross-validation turn
* Train_Set_For_Weka: Contains processed Train set formatted for Weka
* Test_Set_For_Weka: Contains processed Test set formatted for Weka
-&gt; The folder Entire_Dataset contains files for Entire Dataset
* Negative_Processed: Contains all negative comments processed data
* Positive_Processed: Contains all positive comments processed data
* Negative_RAW: Contains all negative comments RAW data
* Positive_RAW: Contains all positive comments RAW data
* Entire_Dataset_WEKA: Contains all documents processed data in WEKA format"	51	1283	7	furkangozukara	turkish-product-reviews
66	66	Dota 2 Heroes Dataset	Statistics of all heroes from DOTA2	['games', 'video games']		83	1545	3	psychewolf	dota-2-heroes-dataset
67	67	isears_tf_clahe	Git of https://github.com/isears/tf_clahe	[]		0	17	0	hey24sheep	isears-tf-clahe
68	68	Human Stress Detection	Human stress level detection using physiological data	['mental health', 'classification', 'pca', 'dnn', 'tabular data']	"“Humidity – Temperature – Step count – Stress levels” represents the titles for Stress-Lysis.csv file. 
Based on the human’s physical activity, the stress levels of the human being are detected and analyzed here. A dataset of 2001 samples is provided for human body humidity, body temperature and the number of steps taken by the user. Three different classifications of stress are performed, low stress, normal stress, and high stress. More information on how this data is analyzed can be found at “L. Rachakonda, S. P. Mohanty, E. Kougianos, and P. Sundaravadivel, “Stress-Lysis: A DNN-Integrated Edge Device for Stress Level Detection in the IoMT,” IEEE Trans. Conum. Electron., vol. 65, no. 4, pp. 474–483, 2019.” 
If you are using this dataset in your research, please cite the following: 
1.  L. Rachakonda, S. P. Mohanty, E. Kougianos, and P. Sundaravadivel, “Stress-Lysis: A DNN-Integrated Edge Device for Stress Level Detection in the IoMT,” IEEE Trans. Conum. Electron., vol. 65, no. 4, pp. 474–483, 2019.
2.  L. Rachakonda, P. Sundaravadivel, S. P. Mohanty, E. Kougianos, and M. Ganapathiraju, “A Smart Sensor in the IoMT for Stress Level Detection”, in Proceedings of the 4th IEEE International Symposium on Smart Electronic Systems (iSES), 2018, pp. 141--145."	0	9	0	laavanya	stress-level-detection
69	69	"Tweets em pt com a expressão ""nazismo"" de 2022"		['text mining', 'text data']		0	6	0	leticiat	tweets-em-pt-com-a-expresso-nazismo-de-2022
70	70	American Sign Language	Hand Sign classification for ASL alphabet	['computer vision', 'classification', 'neural networks', 'health conditions', 'python']		0	5	0	angelgortiz	american-sign-language
71	71	cascade_cbv2_r2_pretrainedLiveTestAsTrain		[]		1	12	0	itaynivtau	cascade-cbv2-r2-pretrainedlivetestastrain
72	72	COVID-19 US County JHU Data & Demographics	Johns Hopkins reported cases & deaths together with survey demographic features	['business', 'social science', 'demographics', 'geospatial analysis', 'covid19']	"Context
The United States have recently become the country with the most reported cases of 2019 Novel Coronavirus (COVID-19). This dataset contains daily updated number of reported cases & deaths in the US on the state and county level, as provided by the Johns Hopkins University. In addition, I provide matching demographic information for US counties.
Content
The dataset consists of two main csv files: covid_us_county.csv and us_county.csv. See the column descriptions below for more detailed information. In addition, I've added US county shape files for geospatial plots: us_county.shp/dbf/prj/shx.
covid_us_county.csv: COVID-19 cases and deaths which will be updated daily. The data is provided by the Johns Hopkins University through their excellent github repo. I combined the separate ""confirmed cases"" and ""deaths"" files into a single table, removed a few (I think to be) redundant geo identifier columns, and reshaped the data into long format with a single date column. The earliest recorded cases are from 2020-01-22.
us_counties.csv: Demographic information on the US county level based on the (most recent) 2014-18 release of the Amercian Community Survey. Derived via the great tidycensus package.
Column Description
COVID-19 dataset covid_us_county.csv:
fips: County code in numeric format (i.e. no leading zeros). A small number of cases have NA values here, but can still be used for state-wise aggregation. Currently, this only affect the states of Massachusetts and Missouri.
county: Name of the US county. This is NA for the (aggregated counts of the) territories of American Samoa, Guam, Northern Mariana Islands, Puerto Rico, and Virgin Islands. 
state: Name of US state or territory.
state_code: Two letter abbreviation of US state (e.g. ""CA"" for ""California""). This feature has NA values for the territories listed above.
lat and long: coordinates of the county or territory.
date: Reporting date.
cases & deaths: Cumulative numbers for cases & deaths.
Demographic dataset us_counties.csv:
fips, county, state, state_code: same as above. The county names are slightly different, but mostly the difference is that this dataset has the word ""County"" added. I recommend to join on fips.
male & female: Population numbers for male and female.
population: Total population for the county. Provided as convenience feature; is always the sum of male + female.
female_percentage: Another convenience feature: female / population in percent.
median_age: Overall median age for the county.
Acknowledgements
Data provided for educational and academic research purposes by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE).
Licence
The github repo states that:
This GitHub repo and its contents herein, including all data, mapping, and analysis, copyright 2020 Johns Hopkins University, all rights reserved, is provided to the public strictly for educational and academic research purposes. The Website relies upon publicly available data from multiple sources, that do not always agree. The Johns Hopkins University hereby disclaims any and all representations and warranties with respect to the Website, including accuracy, fitness for use, and merchantability. Reliance on the Website for medical guidance or use of the Website in commerce is strictly prohibited.
Version history
In version 1, a small number of cases had values of `county == ""Unassigned"". Those have been superseded.
Version 5: added US county shape files"	4377	106553	91	headsortails	covid19-us-county-jhu-data-demographics
73	73	Emotions	Emotions Dataset For NLP Classification	['nlp', 'classification']		0	21	0	anonymous1972	emotions
74	74	COVID-19 Tracking Germany	Daily Updated Cases & Deaths - Augmented with geospatial & demographics info	['social science', 'demographics', 'geospatial analysis', 'covid19']	"Read the associated blogpost for a detailed description of how this dataset was prepared; plus extra code for producing animated maps.
Context
The 2019 Novel Coronavirus (COVID-19) continues to spread in countries around the world. This dataset provides daily updated number of reported cases & deaths in Germany on the federal state (Bundesland) and county (Landkreis/Stadtkreis) level. In April 2021 I added a dataset on vaccination progress. In addition, I provide geospatial shape files and general state-level population demographics to aid the analysis.
Content
The dataset consists of thre main csv files: covid_de.csv, demgraphics_de.csv, and covid_de_vaccines.csv. The geospatial shapes are included in the de_state.* files. See the column descriptions below for more detailed information.
covid_de.csv: COVID-19 cases and deaths which will be updated daily. The original data are being collected by Germany's Robert Koch Institute and can be download through the National Platform for Geographic Data (the latter site also hosts an interactive dashboard). I reshaped and translated the data (using R tidyverse tools) to make it better accessible. This blogpost explains how I prepared the data, and describes how to produces animated maps.
demographics_de.csv: General Demographic Data about Germany on the federal state level. Those have been downloaded from Germany's Federal Office for Statistics (Statistisches Bundesamt) through their Open Data platform GENESIS. The data reflect the (most recent available) estimates on 2018-12-31. You can find the corresponding table here.
covid_de_vaccines.csv: In April 2021 I added this file that contains the Covid-19 vaccination progress for Germany as a whole. It details daily doses, broken down cumulatively by manufacturer, as well as the cumulative number of people having received their first and full vaccination. The earliest data are from 2020-12-27.
de_state.*: Geospatial shape files for Germany's 16 federal states. Downloaded via Germany's Federal Agency for Cartography and Geodesy . Specifically, the shape file was obtained from this link.
Column Description
COVID-19 dataset covid_de.csv:
state: Name of the German federal state. Germany has 16 federal states. I removed converted special characters from the original data.
county: The name of the German Landkreis (LK) or Stadtkreis (SK), which correspond roughly to US counties.
age_group: The COVID-19 data is being reported for 6 age groups: 0-4, 5-14, 15-34, 35-59, 60-79, and above 80 years old. As a shortcut the last category I'm using ""80-99"", but there might well be persons above 99 years old in this dataset. This column has a few NA entries.
gender: Reported as male (M) or female (F). This column has a few NA entries.
date: The calendar date of when a case or death were reported. There might be delays that will be corrected by retroactively assigning cases to earlier dates.
cases: COVID-19 cases that have been confirmed through laboratory work. This and the following 2 columns are counts per day, not cumulative counts.
deaths: COVID-19 related deaths.
recovered: Recovered cases.
Demographic dataset demographics_de.csv:
state, gender, age_group: same as above. The demographic data is available in higher age resolution, but I have binned it here to match the corresponding age groups in the covid_de.csv file.
population: Population counts for the respective categories. These numbers reflect the (most recent available) estimates on 2018-12-31.
Vaccination progress dataset covid_de_vaccines.csv:
date: calendar date of vaccination
doses, doses_first, doses_second: Daily count of administered doses: total, 1st shot, 2nd shot.
pfizer_cumul, moderna_cumul, astrazeneca_cumul: Daily cumulative number of administered vaccinations by manufacturer.
persons_first_cumul, persons_full_cumul: Daily cumulative number of people having received their 1st shot and full vaccination, respectively.
Acknowledgements
All the data have been extracted from open data sources which are being gratefully acknowledged:
The Robert Koch Institute for collecting, verifying, and publishing the COVID-19 cases.
The National Platform for Geographic Data provided by ESRI Germany for the COVID-19 dashboard and data access.
The Statistisches Bundesamt) and their open data platform GENESIS for providing access to detailed demographic information about Germany.
The Federal Agency for Cartography and Geodesy for providing geospatial shape files.
The website Impfdashboard.de for providing up-to-date vaccination numbers.
Licence
The demographic data & geospatial shape files are being licensed via the  ""Data licence Germany – attribution – Version 2.0"" available here. 
Version Notes
Version 8: The source data was incomplete on 2020-04-04, therefore the file covid_de.csv remained unchanged. I added county-level shape files.
Version 11: A recovered column was added to the main covid_de.csv file. This reflects the addition of daily counts of recovered cases to the source data. In addition, the 2020-04-07 source data was incomplete and I didn't upload a new update that day.
Version 13: The previous day 2020-04-10 had incomplete source data and I didn't upload a new update that day."	6553	70777	172	headsortails	covid19-tracking-germany
75	75	🚙 Car Sale Dataset	The data was crawled from car sale website	[]		0	7	1	firuzjuraev	-car-sale-dataset
76	76	titanic		[]		0	0	0	doitomoki	titanic
77	77	MLS Player Salaries	6000 samples of MLSPA players from 2010 to 2018	['sports']	"About this dataset
&gt; As provided by MLSPA here: https://mlsplayers.org/resources/salary-guide, with hat tip to Steve Fenn for compiling This dataset was created by Ben Jones and contains around 6000 samples along with Ÿþs, Ÿþs, technical information and other features such as:
- Ÿþs
- Ÿþs
- and more.
How to use this dataset
&gt; - Analyze Ÿþs in relation to Ÿþs
- Study the influence of Ÿþs on Ÿþs
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Ben Jones 
Start A New Notebook!"	13	79	2	yamqwe	mls-player-salaries-2010-2018e
78	78	Mean Sea Level Pulau Langkawi Malaysia 1986 -2018	Capstone Project to analyse Mean Sea Level of Langkawi	['asia', 'earth and nature', 'earth science', 'data analytics', 'travel', 'json']		3	30	2	harrithshaz	mean-sea-level-pulau-langkawi-malaysia-1986-2018
79	79	Kaggle Discussion User Rankings	Ranks of Users in the Discussions Tier of Kaggle ML and Data Science Community	['computer science', 'tabular data']	"Created just for these:
  - What Are You Talking About?
  - Charting User Progress - Discussions
But feel free to re-use it if you wish!"	9	249	7	jtrotman	kaggle-discussion-user-rankings
80	80	Skin Lesion Dermis Dataset		['health conditions']		0	8	0	farhatullah8398	skin-lesion-dermis-dataset
81	81	Marvel Characters	Basic information about 100+ Marvel characters from the Marvel Universe	['movies and tv shows']	"About this dataset
&gt; <h2>About:</h2>
<p>This dataset contains basic information about 100+ characters from the Marvel Universe. While many have been featured in Marvel's films, some of these characters may have appeared solely in the comics.</p>
<p>This dataset was generated by running the SPARQL query <a href=""https://data.world/amberthomas/marvel-characters/workspace/query?queryid=358e2c41-77ad-4ccd-9a33-745ac764c44a"">Source Data</a>. This query accesses and organizes data directly from <a href=""https://www.wikidata.org/wiki/Wikidata:Main_Page"" target=""_blank"" rel=""nofollow"">WikiData</a>.</p>
<p>Like Wikipedia, WikiData contains crowd-sourced information. According to the site:</p>
<blockquote>
<p>Wikidata is a free and open knowledge base that can be read and edited by both humans and machines.</p>
</blockquote>
<h2>See an issue or want to contribute?</h2>
<p>Because these data are pulled directly from WikiData, you can update it directly!</p>
<ul>
<li>Learn to edit Wikidata: follow the <a href=""https://www.wikidata.org/wiki/Special:MyLanguage/Wikidata:Tours"" target=""_blank"" rel=""nofollow"">tutorials</a>.</li>
<li>Work with other volunteers on a subject that interests you: <a href=""https://www.wikidata.org/wiki/Special:MyLanguage/Wikidata:WikiProjects"" target=""_blank"" rel=""nofollow"">join a WikiProject</a>.</li>
</ul>
<h2>Updating Data</h2>
<p>This dataset automatically updates directly from WikiData daily.</p>
<p><em>Need to update the dataset sooner?</em><br>
You can view the <a href=""https://data.world/amberthomas/marvel-characters/workspace/query?queryid=358e2c41-77ad-4ccd-9a33-745ac764c44a"">Source Data</a> SPARQL query and run it by clicking ""Run Query"". Once the query has run, you can click download to save the results to your local computer or to a separate dataset or project on data.world!</p>
This dataset was created by Amber Thomas and contains around 100 samples along with Occupation, Types, technical information and other features such as:
- Superpowers
- Religions
- and more.
How to use this dataset
&gt; - Analyze Gender in relation to Birthplace
- Study the influence of Charname on Universes
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	46	267	3	yamqwe	marvel-characterse
82	82	brain tumor mri  classification 		[]		0	0	0	mohammedhamdy98	brain-tumor-mri-classification
83	83	Soccer match event dataset	All matches of 2017/2018 season for the seven important european competitions	['football']	"Context
THIS IS NOT MY DATASET. 
All the credit is to Luca Pappalardo and Emmanuele Massucco, who shared their open data through this figshare website.
Pappalardo, Luca; Massucco, Emanuele (2019): Soccer match event dataset. figshare. Collection. https://doi.org/10.6084/m9.figshare.c.4415000.v5
I am including this dataset in Kaggle because I am playing with it and I don't want to add the CSV files every time I use it.
Content
From the paper:
&gt; The data refer to season 2017/2018 of five national soccer competitions in Europe: Spanish first division, Italian first division, English first division, German first division, French first division. These competitions are the most important in Europe according to the UEFA country coefficient, which is used to rank the football associations of Europe and thus determine the number of clubs from an association that will participate in the UEFA Champions League and the UEFA Europa League (https://www.uefa.com/memberassociations/uefarankings/country/#/yr/2019). In addition, we provide the data of the World cup 2018 and the European cup 2016, which are competitions for national teams. In total, we provide seven data sets corresponding to information about all competitions, matches, teams, players, events, referees and coaches
From the source.:
&gt; Soccer analytics is attracting an increasing interest of academia and industry, thanks to the availability of sensing technologies that provide high-fidelity data streams extracted from every match. Unfortunately, these detailed data are owned by specialized companies and hence are rarely publicly available for scientific research. To fill this gap, we provide to the public the largest open collection of soccer-logs ever released, collected by Wyscout (https://wyscout.com/) containing all the spatio-temporal events (passes, shots, fouls, etc.) that occur during all matches of an entire season of seven competitions (La Liga, Serie A, Bundesliga, Premier League, Ligue 1, FIFA World Cup 2018, UEFA Euro Cup 2016). A match event contains information about its position, time, outcome, player and characteristics. This dataset has been used recently during the Soccer Data Challenge (https://sobigdata-soccerchallenge.it/) and, to the best of our knowledge, it is the largest public collection of soccer-logs. 
Acknowledgements
Again, all the credit is to Luca Pappalardo and Emmanuele Massucco, who shared their open data through this figshare website.
Inspiration
Personally, I am a football lover and I want to merge my data science knowledge with football analytics to find new interesting patterns."	20	42	1	aleespinosa	soccer-match-event-dataset
84	84	Shark Tank Companies	Datasets with 495 companies who appeared on Shark Tank	['arts and entertainment', 'business', 'economics']	"About this dataset
&gt; <h1>Article</h1>
<p>Thanks to <a href=""https://theconceptcenter.com/simple-research-study-shark-tank-companies/"" target=""_blank"" rel=""nofollow"">The Concept Center</a> for publishing this article, an analysis, and pulling the data.</p>
<h1>Background</h1>
<p><a href=""http://abc.go.com/shows/shark-tank"" target=""_blank"" rel=""nofollow"">Shark Tank</a> is an ABC show where entrepreneurs around the United States can pitch their ideas to billionaires. The show started in Japan in 2009 and has gained traction over the years to what it is today. <a href=""https://en.wikipedia.org/wiki/Shark_Tank"" target=""_blank"" rel=""nofollow"">Wikipedia</a> For this study, I thought that it would be interesting to take the 6 seasons of Shark Tank which consists of 122 episodes and 495 companies, see which ones performed the best and asked the most. In addition, I wanted to see if there are any trends when it comes to those companies getting a deal with at least one shark. Which of the Shark Tank companies will be the best? Let’s explore from the data what we can find.</p>
<h1>Data</h1>
<p>The data was collected from Shark Analytics, who was able to aggregate the information into one relative area.</p>
<ul>
<li>Date Pulled: 11/28/2017 10:30pm</li>
<li>Total Seasons: 6</li>
<li>Total Episodes: 122</li>
<li>Total Companies on the Show: 495</li>
</ul>
This dataset was created by Chase Willden and contains around 500 samples along with Shark3, Shark2, technical information and other features such as:
- Location
- Episode
- and more.
How to use this dataset
&gt; - Analyze Category in relation to Title
- Study the influence of Asked For on Shark1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	767	4956	32	yamqwe	shark-tank-companiese
85	85	starfish_model		[]		3	10	0	dwchen	starfish-model
86	86	Ancient Chinese books	Ancient Chinese books that covers various topics	['literature']		8	421	1	jiaminggogogo	ancient-chinese-books
87	87	Vehicle Volumes	This is a data set of Vehicle Volumes by certain intersections in Chattanooga.	['government', 'transportation', 'automobiles and vehicles', 'public safety']	Vehicle volume counts for intersections with GridSmart cameras. Data represents volume in a specific direction for a given 15 minute interval.	3	126	0	andrewsevignyopmod	vehicle-volumes
88	88	crop yield	data can be use to predict crop yield bases of rain fall	['india', 'architecture', 'intermediate', 'text data']	"Context
this dataset contains India's crop production based on land.
Content
State: {'Jammu and Kashmir':0, 'Jharkhand':1, 'Bihar':2, 'Uttarakhand':3, 'Chhattisgarh':4, 'Punjab':5, 'Arunachal Pradesh':6, 'Odisha':7, 'Kerala':8, 'Himachal Pradesh':9, 'Tamil Nadu':10, 'Andaman and Nicobar Islands':11}
Year: 1997-2015
Crop: {'Black pepper': 0,
 'Mesta': 1,
 'Sannhamp': 2,
 'Cauliflower': 3,
 'Tomato': 4,
 'Citrus Fruit': 5,
 'Lentil': 6,
 'Dry ginger': 7,
 'Drum Stick': 8,
 'Bottle Gourd': 9,
 'Beet Root': 10,
 'Cardamom': 11,
 'Onion': 12,
 'Yam': 13,
 'Samai': 14,
 'other oilseeds': 15,
 'Bhindi': 16,
 'Groundnut': 17,
 'Turnip': 18,
 'Cowpea(Lobia)': 19,
 'Sesamum': 20,
 'Pear': 21,
 'Ginger': 22,
 'Urad': 23,
 'Cashewnut Raw': 24,
 'Papaya': 25,
 'Brinjal': 26,
 'Rubber': 27,
 'Khesari': 28,
 'Soyabean': 29,
 'Maize': 30,
 'Cabbage': 31,
 'Castor seed': 32,
 'Tea': 33,
 'Other  Rabi pulses': 34,
 'Dry chillies': 35,
 'Other Cereals & Millets': 36,
 'Linseed': 37,
 'Jack Fruit': 38,
 'Rice': 39,
 'Beans & Mutter(Vegetable)': 40,
 'Moong(Green Gram)': 41,
 'Coconut ': 42,
 'Plums': 43,
 'Tapioca': 44,
 'Cotton(lint)': 45,
 'Korra': 46,
 'Potato': 47,
 'Pulses total': 48,
 'Coriander': 49,
 'Wheat': 50,
 'Ber': 51,
 'Sugarcane': 52,
 'Pome Granet': 53,
 'Coffee': 54,
 'Pome Fruit': 55,
 'Niger seed': 56,
 'Bajra': 57,
 'Varagu': 58,
 'Moth': 59,
 'Redish': 60,
 'Jute': 61,
 'Water Melon': 62,
 'other misc. pulses': 63,
 'Arecanut': 64,
 'Safflower': 65,
 'Jowar': 66,
 'Sweet potato': 67,
 'Carrot': 68,
 'Grapes': 69,
 'Guar seed': 70,
 'Other Kharif pulses': 71,
 'Lab-Lab': 72,
 'Cond-spcs other': 73,
 'Oilseeds total': 74,
 'Pineapple': 75,
 'Apple': 76,
 'Total foodgrain': 77,
 'Arhar/Tur': 78,
 'Horse-gram': 79,
 'Bitter Gourd': 80,
 'Peach': 81,
 'Cucumber': 82,
 'Ragi': 83,
 'Barley': 84,
 'Paddy': 85,
 'Tobacco': 86,
 'Ribed Guard': 87,
 'Orange': 88,
 'Turmeric': 89,
 'Garlic': 90,
 'Small millets': 91,
 'Snak Guard': 92,
 'Litchi': 93,
 'Mango': 94,
 'Other Vegetables': 95,
 'Peas & beans (Pulses)': 96,
 'Masoor': 97,
 'Blackgram': 98,
 'Banana': 99,
 'Other Fresh Fruits': 100,
 'Gram': 101,
 'Cashewnut': 102,
 'Sunflower': 103,
 'Other Citrus Fruit': 104,
 'Rapeseed &Mustard': 105,
 'Pump Kin': 106,
 'Ash Gourd': 107}
Area: Hectare
Rain: mm
Source
data is collected from multiple sources: The Indian government, data world, kaggle, etc."	23	159	0	shubhamsingh2001	crop-yield
89	89	Mahanagar Yatayat Dataset	Tracking Data of Mahanagar Yatayat (Ringroad Transportation)	['transportation']		1	51	0	nischallal	mahanagar-yatayat-dataset
90	90	Covid 19 India Tracking 	covid19india.org Tracking Data	['time series analysis', 'covid19']	"Update: Regarding this dataset:
covid19india.org has stopped operations of data collection since October 31st , 2021.
For more info please read this blog post where they had cited their reasons.
Meanwhile if I find another source i will add that in another dataset .
This Dataset is Sourced from https://www.covid19india.org/ 
Read their About Page For More details on how they collected this information."	225	1962	14	punyaslokaprusty	covid-19-india-tracking
91	91	 Restaurant Revenue Prediction		[]		0	4	0	gokulstark	restaurant-revenue-prediction
92	92	datasets 1.18.3		[]		0	1	0	suika1	datasets-1183
93	93	Alcohol and Life Expectancy	Alcohol and Life Expectancy by Country	['alcohol', 'health', 'politics']	"About this dataset
&gt; <p>Life Expectancy dated from the World Health Organization, Alcohol consumption by country from <a href=""http://fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">fivethirtyeight.com</a>'s github account</p>
<p>See the original data at <a href=""https://github.com/fivethirtyeight/data/tree/master/alcohol-consumption"" target=""_blank"" rel=""nofollow"">github</a>.</p>
<p>Data Visualization on Tableau: Alcohol and Life Expectancy.twbx</p>
This dataset was created by Jon Loyens and contains around 6000 samples along with Country Code, Country Display, technical information and other features such as:
- Region Display
- Display Value
- and more.
How to use this dataset
&gt; - Analyze Publish State Display in relation to Sex Code
- Study the influence of Region Code on World Bank Income Group Display
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jon Loyens 
Start A New Notebook!"	27	211	3	yamqwe	alcohol-and-life-expectancye
94	94	datadiabetes		[]		0	0	0	arinifadhilah	datadiabetes
95	95	HappyWhale 384x660 Models		[]		0	2	0	dschettler8845	happywhale-384x660-models
96	96	diabetesdata		[]		0	0	0	arinifadhilah	diabetesdata
97	97	Online Sales Dataset		[]		0	3	0	furqanamjad	online-sales-dataset
98	98	World Vaccine Progress	Daily Updates of world countries vaccination progress.	['websites', 'public health', 'demographics', 'time series analysis', 'regression', 'public safety', 'covid19']	"Context
To be honest it's pretty hard for you to find data on vaccine progress and especially time-based data on a country like Pakistan. So, I created this small but interactive notebook that will keep updating the database until everyone is vaccinated. In this project I have used Pandas for easy WebSracping to get the data from pharmaceutical-technology.com then I have created Sqlite3 database to store the data into three tables. It took me a few tries to get everything working smooth so I started using SQL queries to get the data and then used plotly to plot interactive visualization. I was not sure when they will update the website so, I have created few functions to avoid duplication of data and to inform me on telegram about updates. I have also uploaded the processed data to Kaggle from Deepnote which will be updated daily. At last, I have used the Deepnote Schedule notebook feature to run this notebook every day and successfully publishing the article You can find my work on Deepnote.
Content
World_Vaccination_Progress.csv -&gt; Countries Vaccination progress
pakistan_time_series.csv -&gt; Time series data of Pakistan vaccine progress
world_time_series.csv -&gt; Time series data of World vaccine progress
Columns:
- Country :: Names of countries in the world
- Doses Administered: Total Doses Administered
- Doses per 1000 : Number of Doses per thousand
- Fully Vaccinated Population (%) : Percentage of a fully vaccinated person in a country.
- Vaccine being used in a country : Types of vaccines used in a country.
For Time-Series
Date_Time : Timestamp of entry
Acknowledgements
I am thankful for Pharmaceutical Technology for updating the stats on daily basis and publicly provide real-time stats of world's vaccination drive. I also want to thank Deepnote for the introduction of the Schedule notebook feature that has made this automation possible. 
<img alt=""Github"" src=""https://img.shields.io/badge/pharmaceutical_technology.com-1e90ff?logo=SingleStore&logoColor=white&style=for-the-badge"">
Inspiration
The lack of data available in my country drove me to create an automated system that collects data from web. You can read more about it in my article. The second inspiration came from participating in Deepnote competition which was on the data Vaccination drive of your country or World."	849	5152	31	kingabzpro	world-vaccine-progress
99	99	Omicron daily cases by country (COVID-19 variant)	Daily Updated Omicron (COVID-19 Variant) cases	['healthcare', 'public health', 'health', 'time series analysis', 'covid19']	"Tracking the progression of the new omicron COVID-19 variant
The data
location- this is the country for which the variants information is provided;
date - date for the data entry;
variant - this is the variant corresponding to this data entry;
num_sequences - the number of sequences processed (for the country, variant and date);
perc_sequences - percentage of sequences from the total number of sequences (for the country, variant and date);
num_sequences_total - total number of sequences (for the country, variant and date);
Acknowledgements"	17653	100906	752	yamqwe	omicron-covid19-variant-daily-cases
100	100	cancerdataya		[]		0	1	0	nickevitafaizah	cancerdataya
101	101	Wordle Tweets	A daily sample of Wordle results tweets	['online communities']	"A daily sample of Wordle results tweets since Wordle 210. Pulled and updating using the notebook benhamner/pull-wordle-tweets
This is used to guess each day's Wordle only using the distribution of results from these tweets in the notebook Wordle 1/6 🟩🟩🟩🟩🟩"	144	4505	18	benhamner	wordle-tweets
102	102	feed_back_mode1		[]		0	3	0	rookie0ne	feed-back-mode1
103	103	convnext_tf		[]		0	1	0	masatakaaoki	convnext-tf
104	104	List of PyPI Packages	List of packages available on the PyPI package repository, updated daily	['computer science', 'programming']	This is a json packages of a list of of packages available on PyPI, from https://pypi.org/simple/.  It's updated daily.	961	29837	122	rtatman	list-of-pypi-packages
105	105	Lumpy Skin Disease Dataset	Skin Disease Detection using Machine Learning	['healthcare', 'earth and nature', 'health', 'classification', 'binary classification']	"Context
Assessing machine learning techniques in forecasting Lumpy Skin Disease occurrence based on meteorological and geospatial features - dataset
Acknowledgements
Afshari Safavi, Ehsanallah (2021), “Lumpy Skin disease dataset”, Mendeley Data, V1, doi: 10.17632/7pyhbzb2n9.1"	215	2273	16	saurabhshahane	lumpy-skin-disease-dataset
106	106	Cyclistic Bike Share Data	Divvy Historical Bike Sharing Data (Apr 20 - Oct 21)	['cycling', 'tabular data']	This data is made available to the public under the license. You can view other details on their website.	0	52	0	shikharsingh14059	cyclistic-bike-share-data
107	107	Lok Sabha Questions	Questions in the Lok Sabha - Analysis of trends in the Indian Parliament	['india', 'government', 'politics', 'nlp']	"loksabha-questions
<p>
  <img src=""https://upload.wikimedia.org/wikipedia/commons/2/2c/Indian_Parliament.svg"" alt=""A graphic of the Parliament of India, Courtesy: Suthir, Wikipedia"">
</p>

This description is best consumed on the official Github Readme for this project - Here.
Questions asked in the Lok Sabha - collection and analysis of trends. Creating the dataset from scratch. Check out this link for a quick explanation of how the Question Hour works in the Lok Sabha.
The eventual goal of this repo is a complete analysis of the questions asked in the Lok Sabha (House of the People - lower house of the Indian Parliament). As of writing this readme, the December 2021 session is in progress, and the dataset I've put together has 4250 questions from 1st December 2021 to 23rd December 2021.
Each question has the following information:
| Field | Description |
| ---: | --- |
| Question ID | The official question number |
| Question Type | Type of question, starred/unstarred |
| Question Date | Session Date in which question was presented |
| Question From | Asking Member(s) of Parliament |
| Question To | Ministry / Department sought in the question |
| Question Topic | Topic of the question |
| Question Contents | Question body |
| Member Party | Political affiliation of the Asking Member |
| Member State | Asking Member's state |
| Member Constituency | Asking Member's constituency |
| Member Constituency Type | Asking Member's constituency type |
The goal of this project is to be able to answer the following questions (work in progress):
* What are the questions that were asked by an MP?
* What are the questions being asked from a particular state?
* How many questions were asked around a particular topic?
* And many more combinations of these...
Origins
This dataset was compiled by parsing PDFs of session questions. Typical Oral and Written questions look like: (broken links to images)
These PDFs can be found on the Official Lok Sabha Website.
As a starting point for this Dataset, I'm using the Seventeenth (17th) Session of the Lok Sabha. Will continue to add more sessions regularly.
Parsing these PDFs is truly a challenge - a staggering amount of data to scrape, and an even more daunting number of typing inconsistencies to consider.
Mapping the Asking Member to an actual Member of Parliament was also a difficult task. For e.g., consider the following names:
Shri Sunil Kumar Singh
Shri S.K. Singh
Shri Sunil Singh
Singh, Shri Sunil Kumar
Singh, Sunil Kumar
Singh, Sunil K.
These are all valid ways to write a name. Which one do we stick to? I choose to go with the full name approach with Last Name, First Name. Fuzzy-matching of Indian names was a difficult task, but it worked out well, thanks mostly to thefuzz.
Btw, interestingly, we currently have FIVE Shri Sunil Kumar's as MPs:
| Member Name | Party | Constituency |
| --- | --- | --- |
| Mondal, Shri Sunil Kumar | All India Trinamool Congress   | Bardhaman Purba (SC)(West Bengal) |
| Singh, Shri Sunil Kumar   | Bharatiya Janata Party    | Chatra (Jharkhand) |
| Soni, Shri Sunil Kumar    | Bharatiya Janata Party    | Raipur (Chhattisgarh) |
| Sunil Kumar Pintu, Shri   | Janata Dal (United)   | Sitamarhi (Bihar) |
| Kumar, Shri Sunil | Janata Dal (United)   | Valmiki Nagar (Bihar) |
Why are there TWO .csv files in this dataset?
Sometimes, questions are clubbed, and therefore may list multiple asking members. For example, consider Q. 362 and Q. 4142 in the above screenshots.
Therefore, Multiple Members may ask a Single Question, and a Single Member may ask Multiple Questions.
This makes it harder to analyze questions - specifically operations like grouping / aggregating in a dataset.
This Many-to-Many relationship is typically addressed in Databases by means of a bridging table.
However, considering the scale of the problem, I decided to ship an augmented version of this database as well which is called questions_flattened.csv in addition to the questions.csv database. 
A quick look into the number of questions will better demonstrate the above explanation.
Each session has 250 questions on record (20 starred + 230 unstarred).
For 17 sessions, the total comes to 4250 questions. This is why questions.csv has 4250 entries as of now.
Some questions were asked by multiple members, so I replicated these questions for each asking Member of Parliament.
This results in 7050 questions after flattening out the questions - Hence, questions_flattened.csv
Do you need both the files? In most cases, no. It's also easy to derive one from the other. Chances are that most analyses will use the flattened dataset. 
NOTE:
I do not own any of the associated data and is publicly available at http://loksabhaph.nic.in/Questions/questionlist.aspx as on December 15th, 2021. 
Any information presented is merely an interpretation and should not be used as a substitute for real data.
Since this project involves a lot of text analytics, natural language processing, semantic parsing of really messy data, it is prone to errors."	7	244	2	sammitjain	lok-sabha-questions
108	108	Human Resource Data Set (The Company)	Dataset for People Analytics or general HR Systems Use	['employment', 'business', 'demographics']	"Context
Similar to others who have created HR data sets, we felt that the lack of data out there for HR was limiting. It is very hard for someone to test new systems or learn People Analytics in the HR space. The only dataset most HR practitioners have is their real employee data and there are a lot of reasons why you would not want to use that when experimenting. We hope that by providing this dataset with an evergrowing variation of data points, others can learn and grow their HR data analytics and systems knowledge.
Some example test cases where someone might use this dataset:
HR Technology Testing and Mock-Ups
Engagement survey tools
HCM tools
BI Tools
Learning To Code For People Analytics
Python/R/SQL
HR Tech and People Analytics Educational Courses/Tools
Content
The core data CompanyData.txt has the basic demographic data about a worker. We treat this as the core data that you can join future data sets to.
Please read the Readme.md for additional information about this along with the Changelog for additional updates as they are made.
Acknowledgements
Initial names, addresses, and ages were generated using FakenameGenerator.com. All additional details including Job, compensation, and additional data sets were created by the Koluit team using random generation in Excel.
Inspiration
Our hope is this data is used in the HR or Research space to experiment and learn using HR data. Some examples that we hope this data will be used are listed above.
Contact Us
Have any suggestions for additions to the data? See any issues with our data? Want to use it for your project? Please reach out to us!
https://koluit.com/
ryan@koluit.com"	31	181	0	koluit	human-resource-data-set-the-company
109	109	Daily updated vegetable prices in Punjab, Pakistan	Punjab Government official vegetable wholesale and retail prices	['food']		19	891	0	awaisanwar544	daily-updated-vegetable-prices-in-punjab-pakistan
110	110	World Tallest Buildings	100 tallest completed buildings in the world	['architecture']	"100 tallest completed buildings in the world
Columns
RANK
NAME
CITY
COMPLETION  
HEIGHT  
FLOORS  
MATERIAL
FUNCTION
Source
https://www.skyscrapercenter.com/buildings"	0	3	1	stpeteishii	world-tallest-buildings
111	111	allcfg		[]		0	14	0	limjunhao	allcfg
112	112	Reddit - Crypto Posts (r/cryptocurrency, r/eth...)	r/cryptocurrency, r/ethtrader, r/bitcoin, r/dogecoin, r/cryptomoonshots, r/cr...	['finance', 'exploratory data analysis', 'time series analysis', 'currencies and foreign exchange', 'online communities']	"Description
Reddit submissions from cryptocurrency related posts:
| Subreddit | From | To  | Count |
| --------- | ---- | --- | ----- |
r/cryptocurrency | 2022-01-01 00:00:13 | 2022-02-14 13:31:33 | 59480
r/cryptomoonshots | 2022-01-01 00:04:53 | 2022-02-14 13:34:47 | 21028
r/cryptocurrencytrading | 2022-01-01 00:00:05 | 2022-02-14 13:32:56 | 18906
r/dogecoin | 2022-01-01 00:06:16 | 2022-02-14 13:35:50 | 15265
r/ethtrader | 2022-01-01 00:00:09 | 2022-02-14 13:34:50 | 14703
r/bitcoin | 2022-01-01 00:03:05 | 2022-02-14 13:25:54 | 11409
r/crypto_com | 2022-01-01 00:03:10 | 2022-02-14 13:33:23 | 10732
r/loopringorg | 2022-01-01 00:00:39 | 2022-02-14 13:32:43 | 7833
r/cryptomarkets | 2022-01-01 00:02:51 | 2022-02-14 13:31:59 | 7297
r/ethereum | 2022-01-01 00:10:30 | 2022-02-14 13:24:16 | 4528
r/cardano | 2022-01-01 00:03:03 | 2022-02-14 13:30:06 | 4145
r/binance | 2022-01-01 00:14:36 | 2022-02-14 13:20:26 | 4122
r/coinbase | 2022-01-01 00:02:33 | 2022-02-14 12:30:44 | 4002
r/kucoin | 2022-01-01 00:23:50 | 2022-02-14 13:19:32 | 3990
r/crypto_general | 2022-01-01 00:02:14 | 2022-02-14 13:25:10 | 3927
r/btc | 2022-01-01 00:08:55 | 2022-02-14 13:14:24 | 3920
r/ethermining | 2022-01-01 01:01:21 | 2022-02-14 13:24:04 | 3365
r/satoshistreetbets | 2022-01-01 00:11:28 | 2022-02-14 12:48:38 | 3057
r/solana | 2022-01-01 00:56:24 | 2022-02-14 13:38:34 | 2939
r/nicehash | 2022-01-01 00:22:01 | 2022-02-14 13:37:54 | 2412
r/bitcoinbeginners | 2022-01-01 01:45:19 | 2022-02-14 12:49:42 | 2036
r/cryptocurrencies | 2022-01-01 00:59:37 | 2022-02-14 13:02:13 | 1978
r/cryptocurrencymemes | 2022-01-01 00:28:25 | 2022-02-14 13:08:44 | 1953
r/altcoin | 2022-01-01 01:09:13 | 2022-02-14 13:16:15 | 1861
r/gpumining | 2022-01-01 00:57:09 | 2022-02-14 12:47:13 | 1600
r/bitcoinmining | 2022-01-01 06:04:55 | 2022-02-14 13:23:25 | 1416
r/ethdev | 2022-01-01 01:58:07 | 2022-02-14 12:54:30 | 1403
r/monero | 2022-01-01 01:39:54 | 2022-02-14 12:20:26 | 1357
r/decentraland | 2022-01-01 00:05:48 | 2022-02-14 11:00:55 | 1130
r/tezos | 2022-01-01 01:59:32 | 2022-02-14 12:39:15 | 889
r/bitcoincash | 2022-01-01 02:15:28 | 2022-02-14 13:15:27 | 886
r/cryptotechnology | 2022-01-01 01:26:35 | 2022-02-14 12:26:43 | 818
r/xrp | 2022-01-01 05:00:15 | 2022-02-14 12:27:17 | 730
r/nanocurrency | 2022-01-01 02:20:31 | 2022-02-14 09:51:37 | 674
r/neo | 2022-01-01 00:25:46 | 2022-02-14 09:49:49 | 658
r/litecoin | 2022-01-01 03:43:43 | 2022-02-14 13:11:29 | 644
r/polkadot | 2022-01-01 12:40:20 | 2022-02-14 12:31:00 | 629
r/batproject | 2022-01-01 06:24:11 | 2022-02-14 02:36:53 | 599
r/uniswap | 2022-01-01 00:10:30 | 2022-02-14 11:07:00 | 599
r/ripple | 2022-01-01 04:05:57 | 2022-02-14 10:11:26 | 564
r/bitcoinmarkets | 2022-01-01 05:00:15 | 2022-02-14 12:14:57 | 522
r/stellar | 2022-01-01 00:29:38 | 2022-02-14 13:04:59 | 518
r/vechain | 2022-01-01 01:17:30 | 2022-02-14 13:06:16 | 472
r/tronix | 2022-01-01 04:39:55 | 2022-02-14 13:10:02 | 442
r/iota | 2022-01-01 03:45:20 | 2022-02-14 10:21:41 | 404
r/ethfinance | 2022-01-01 06:00:17 | 2022-02-14 06:00:12 | 400
r/eos | 2022-01-01 11:54:53 | 2022-02-14 10:49:33 | 357
r/xmrtrader | 2022-01-01 08:01:11 | 2022-02-14 08:01:12 | 242
r/litecoinmarkets | 2022-01-01 05:01:11 | 2022-02-14 05:01:12 | 203
r/crypto_currency_news | 2022-01-01 00:09:00 | 2022-01-04 10:53:51 | 107
Data
See collection methodology, all times in UTC:
| Column | Description | Type |
| ------ | ----------- | ---- |
submission | The id of the submission | string
subreddit | The subreddit name | string
author | The redditors username | string
created | Time the submission was created | number
retrieved | Time the submission was retrieved | number
edited | Time the submission was modified | number
pinned | Whether or not the submission is pinned | number
archived | Whether or not the submission is archived | number
locked | Whether or not the submission is locked | number
removed | Whether or not the submission is mod removed | number
deleted | Whether or not the submission is user deleted | number
is_self | Whether or not the submission is a text | number
is_video | Whether or not the submission is a video | number
is_original_content | Whether or not the submission has been set as original content | number
title | The title of the submission | string
link_flair_text | The submission link flairs text content | string
upvote_ratio | The percentage of upvotes from all votes on the submission | number
score | The number of upvotes for the submission | number
gilded | The number of gilded awards on the submission | number
total_awards_received | The number of awards on the submission | number
num_comments | The number of comments on the submission | number
num_crossposts | The number of crossposts on the submission | number
selftext | The submission selftext on text posts | string
thumbnail | The submission thumbnail on image posts | string
shortlink | The submission short url | string
Usage
See getting started, data available as csv:
- submission.csv: Load files using pandas or any other framework.
Legal
Provided ""as is"" without guarantee of completeness.
Photo by RODNAE Productions from Pexels.
Last update: 2022-02-14 13:40:44 UTC."	37	503	0	leukipp	reddit-crypto-data
113	113	Pure Waveforms		['electronics']		2	22	0	paulwightmore	pure-waveforms
114	114	Shark Tank India	Shark Tank India data set	['business', 'exploratory data analysis', 'investing']	"Shark Tank India Data set.
Shark Tank India - Season 1 information.
One season of SHARK TANK INDIA was broadcasted in SonyLiv OTT.
In 35 episodes, there were 117 pitches.
Here is the data dictionary for Shark Tank (India) season's dataset.
 - Season Number - Season number
- Episode Number - Episode number within current season
- Episode Title - Episode title in SonyLiv
- Pitch Number - Pitch number
- Startup Name - Startup company name
- Industry - Industry name or type
- Business Description - Business Description
- Number of Presenters - Number of presenters
- Male Presenter - Number of male presenter
- Female Presenter - Number of female presenter
- Yearly Revenue - Yearly revenue, in lakhs INR
- Monthly Sales - Total monthly sales
- Gross Margin - Gross Margin of company, in percentages
- Original Ask Amount - Original Ask Amount, in lakhs INR
- Original Ask Equity - Original Ask Equity, in percentages
- Valuation Requested - Valuation Requested, in lakhs INR
- Received Offer - Received offer or not, 1-received, 0-not received
- Accepted Offer - Accepted offer or not, 1-accepted, 0-rejected
- Total Deal Amount - Total Deal Amount, in lakhs INR
- Total Deal Equity - Total Deal Equity, in percentages
- Total Deal Debt - Total Deal Debt, in lakhs INR
- Valuation Offered - Valuation Offered, in lakhs INR
- Ashneer Investment Amount - Ashneer Investment Amount, in lakhs INR
- Ashneer Investment Equity - Ashneer Investment Equity, in percentages
- Ashneer Debt Amount - Ashneer Debt Amount, in lakhs INR
- Namita Investment Amount - Namita Investment Amount, in lakhs INR
- Namita Investment Equity - Namita Investment Equity, in percentages
- Namita Debt Amount - Namita Debt Amount, in lakhs INR
- Anupam Investment Amount - Anupam Investment Amount, in lakhs INR
- Anupam Investment Equity - Anupam Investment Equity, in percentages
- Anupam Debt Amount - Anupam Debt Amount, in lakhs INR
- Vineeta Investment Amount - Vineeta Investment Amount, in lakhs INR
- Vineeta Investment Equity - Vineeta Investment Equity, in percentages
- Vineeta Debt Amount - Vineeta Debt Amount, in lakhs INR
- Aman Investment Amount - Aman Investment Amount, in lakhs INR
- Aman Investment Equity - Aman Investment Equity, in percentages
- Aman Debt Amount - Aman Debt Amount, in lakhs INR
- Peyush Investment Amount - Peyush Investment Amount, in lakhs INR
- Peyush Investment Equity - Peyush Investment Equity, in percentages
- Peyush Debt Amount - Peyush Debt Amount, in lakhs INR
- Ghazal Investment Amount - Ghazal Investment Amount, in lakhs INR
- Ghazal Investment Equity - Ghazal Investment Equity, in percentages
- Ghazal Debt Amount - Ghazal Debt Amount, in lakhs INR
- Number of sharks in deal - Number of sharks involved in deal"	9	60	5	thirumani	shark-tank-india
115	115	COVID Vaccination in World (updated daily)	COVID Vaccination Dataset which gets updated daily.	['diseases', 'public health', 'people', 'medicine', 'public safety', 'covid19']	"Context
The data is collected from OWID (Our World in Data) GitHub repository, which is updated on daily bases.
Content
This dataset contains only one file vaccinations.csv, which contains the records of vaccination doses received by people from all the countries.
* location: name of the country (or region within a country).
* iso_code: ISO 3166-1 alpha-3 – three-letter country codes.
* date: date of the observation.
* total_vaccinations: total number of doses administered. This is counted as a single dose, and may not equal the total number of people vaccinated, depending on the specific dose regime (e.g. people receive multiple doses). If a person receives one dose of the vaccine, this metric goes up by 1. If they receive a second dose, it goes up by 1 again.
* total_vaccinations_per_hundred: total_vaccinations per 100 people in the total population of the country.
* daily_vaccinations_raw: daily change in the total number of doses administered. It is only calculated for consecutive days. This is a raw measure provided for data checks and transparency, but we strongly recommend that any analysis on daily vaccination rates be conducted using daily_vaccinations instead.
* daily_vaccinations: new doses administered per day (7-day smoothed). For countries that don't report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found here.
* daily_vaccinations_per_million: daily_vaccinations per 1,000,000 people in the total population of the country.
* people_vaccinated: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.
* people_vaccinated_per_hundred: people_vaccinated per 100 people in the total population of the country.
* people_fully_vaccinated: total number of people who received all doses prescribed by the vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.
* people_fully_vaccinated_per_hundred: people_fully_vaccinated per 100 people in the total population of the country.
Note: for people_vaccinated and people_fully_vaccinated we are dependent on the necessary data being made available, so we may not be able to make these metrics available for some countries.
Acknowledgements
This data collected by Our World in Data which gets updated daily on their Github.
Inspiration
Possible uses for this dataset could include:
- Sentiment analysis in a variety of forms
- Statistical analysis over time."	596	6207	18	rsrishav	covid-vaccination-dataset
116	116	Colleges and Universities	Datasets for the Integrated Post Secondary Education System (IPEDS)	['universities and colleges', 'education']	"About this dataset
&gt; <p>The colleges and university dataset is composed of all <strong>Post Secondary Education</strong> facilities as defined by the <strong>Integrated Post Secondary Education System (IPEDS)</strong>, <strong>National Center for Education Statistics, US Department of Education.</strong> Included are <em>Doctoral/Research Universities, Masters Colleges and Universities, Baccalaureate Colleges, Associates Colleges, Theological seminaries, Medical Schools and other health care professions, Schools of engineering and technology, business and management, art, music, design, Law schools, Teachers colleges, Tribal colleges,</em> and other specialized institutions.</p>
<p>Overall, this data layer covers all 50 states, as well as <em>Puerto Rico</em> and other assorted U.S. territories.This feature class contains all MEDS/MEDS+ as approved by NGA. For each field the 'Not available' and 'NULL' designations are used to indicate that the data for the particular record and field is currently unavailable and will be populated when and if that data becomes available.</p>
<p><a href=""https://hifld-dhs-gii.opendata.arcgis.com/datasets/4061dcd767c340d4a42fb7a0c6c5d5b4_0"" target=""_blank"" rel=""nofollow"">SOURCE</a></p>
This dataset was created by Homeland Infrastructure Foundation and contains around 8000 samples along with Naics Code, Inst Size, technical information and other features such as:
- Type
- Sector
- and more.
How to use this dataset
&gt; - Analyze Alias in relation to Val Method
- Study the influence of Cofips on X
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Homeland Infrastructure Foundation 
Start A New Notebook!"	46	350	7	yamqwe	colleges-and-universitiese
117	117	naic_eff_not_bn		[]		0	7	0	ucaszxh	naic-eff-not-bn
118	118	AusDomesticAirlines	Total numbers of cancellation and delay for Australian domestic airlines.	['australia', 'aviation', 'exploratory data analysis', 'linear regression', 'tabular data']	"Content
The dataset includes the total number of cancellations and delays by month, year, distinct airlines, airports, and flight routes in Australia from 2004 to 2021. 
Acknowledgements
The data is collected and cleaned by lecturers and tutors in the course COMM1190: Data, Insights & Decisions in the University of New South Wales (UNSW)."	0	2	0	kwonhoang	ausdomesticairlines
119	119	Test for word segment		[]		0	0	0	bstnst99	test-for-word-segment
120	120	scala language		['computer science']		0	0	0	housseinihadia	scala-language
121	121	csv_files		[]		0	0	0	hakanberkbalcilar	csv-files
122	122	pbvs-6k-multimodal		[]		0	11	0	adityakane	pbvs6kmultimodal
123	123	pbvs-6k		[]		0	11	0	adityakane	pbvs6k
124	124	Tensorflow Official Text Datasets	Contains the three official tensorflow datasets (TFDS) for text classification	['earth and nature', 'artificial intelligence', 'nlp', 'tensorflow']	"TFDS provides a collection of ready-to-use datasets for use with TensorFlow, Jax, and other Machine Learning frameworks.
source
https://www.tensorflow.org/datasets/overview"	139	5888	41	imoore	tensorflow-official-text-datasets
125	125	🤰 Pregnancy, Birth & Abortion Rates (1973 - 2016)	Birth and abortion rates for 1000 women	['united states', 'gender', 'health', 'social science']	"About this dataset
&gt; <p>Source: <a href=""https://osf.io/kthnf/wiki/home/"" target=""_blank"" rel=""nofollow"">OSF</a> | Downloaded on 29 October 2020</p>
<p>This data source is a subset of the original data source. The data has been split by State, Metric and Age Range. It has been limited to pregnancy rate, birth rate and abortion rate per 1,000 women. The original data contains many more measures.</p>
<p>The data was prepared with Tableau Prep.</p>
<p>Summary via OSF -</p>
<p>A data set of comprehensive historical statistics on the incidence of pregnancy, birth and abortion for people of all reproductive ages in the United States. National statistics cover the period from 1973 to 2016, the most recent year for which comparable data are available; state-level statistics are for selected years from 1988 to 2016. For a report describing key highlights from these data, as well as a methodology appendix describing our methods of estimation and data sources used, see <a href=""https://guttmacher.org/report/pregnancies-births-abortions-in-united-states-1973-2016"" target=""_blank"" rel=""nofollow"">https://guttmacher.org/report/pregnancies-births-abortions-in-united-states-1973-2016</a>.</p>
<p>​</p>
This dataset was created by Andy Kriebel and contains around 20000 samples along with Events Per 1,000 Women, State, technical information and other features such as:
- Year
- Age Range
- and more.
How to use this dataset
&gt; - Analyze Metric in relation to Events Per 1,000 Women
- Study the influence of State on Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	249	1754	19	yamqwe	pregnancy-birth-abortion-rates-in-the-united-stae
126	126	dataudara		[]		0	0	0	masdimsss	dataudara
127	127	make n fold csv util	make n fold csv easy tool	[]		0	4	1	rhythmcam	make-n-fold-csv-util
128	128	IPL Auction 2022	List of Players under the auction hammer	['cricket', 'india', 'asia', 'sports']	"Context
The auction is conducted on 12 and 13 February 2022 in Bengaluru. A total of 590 players were selected for the auction. 204 players were sold and INR 5,51,70,00,000 was splurged amongst the ten franchises during the two-day TATA Indian Premier League (IPL) 2022 Auction in Bengaluru. Young Indian cricketers dominated the proceedings at the auction, with Ishan Kishan attracting the topmost bid of INR 15.25 Crore which saw him return to MI. Meanwhile, Deepak Chahar, who returned to CSK for INR 14 Crore, became the most expensive Indian pace bowler to be ever bought at the IPL Auction. KKR broke the bank for Shreyas Iyer and bought the stylish right-handed batter for INR 12.25 Crore. Avesh Khan became the most expensive uncapped player in the history of the IPL, with INR 10 Crore against his name, courtesy the new IPL entrant LSG. All and all, the ten franchises had two fruitful days at the mega auction and that sets up for a fascinating TATA IPL 2022."	114	560	8	vinitshah0110	ipl-auction-2022
129	129	Novel COVID19 Dataset	Novel Corona Virus daily information.	['universities and colleges', 'health', 'internet', 'exploratory data analysis', 'data visualization', 'covid19']	"Context
Johns Hopkins University has made an excellent dashboard using the affected case data. You can access that dashboard here.
This data is available as CSV files in the Johns Hopkins Github repository (here).
I am uploading this data set here so we can use this data in kaggle kernel/notebook.
Acknowledgements
Data Set : JHU CSSE COVID-19 Data, https://github.com/CSSEGISandData/COVID-19.
Photo Credit: Photo by Adam Nieścioruk on Unsplash.
Licence
This data set is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) by the Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020.
Please refer to the Github repository for more details of the Terms of Use details (here)."	925	6205	28	themlphdstudent	novel-covid19-dataset
130	130	US Health Expenditures History	Datasets containing over 500 samples along with Expenditure Amount ( Millions)	['healthcare', 'business', 'health', 'economics', 'medicine', 'news']	"About this dataset
&gt; <p>Every detail about where the money comes from and where it goes, over the years.</p>
<p>source: <a href=""https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/NationalHealthExpendData/NationalHealthAccountsHistorical.html"" target=""_blank"" rel=""nofollow"">https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/NationalHealthExpendData/NationalHealthAccountsHistorical.html</a></p>
This dataset was created by Gary Hoover and contains around 500 samples along with Expenditure Amount ( Millions), 1962, technical information and other features such as:
- 2009
- 2010
- and more.
How to use this dataset
&gt; - Analyze 2002 in relation to 1986
- Study the influence of 1990 on 2014
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	16	144	1	yamqwe	us-health-expenditures-historye
131	131	Luna16_subset0		[]		0	2	0	fedorovvladimir	luna16-subset0
132	132	2021 World Population (updated daily)	2021 World Population dataset which gets updated daily.	['global', 'people', 'social science', 'exploratory data analysis', 'data analytics', 'tabular data']	"Context
2021 World Population dataset which gets updated daily.
Content
2021_population.csv: File contains data for only live 2021 population count which gets updated daily.
Also contains more information about the country's growth rate, area, etc.
timeseries_population_count.csv: File contains data for live population count which gets updated daily but it contains last updated data also. Data in this file is managed day-wise.
Inspiration
This type of data can be used for population-related use cases.
Like, my own dataset COVID Vaccination in World (updated daily), which requires population data.
I believe there are more use cases that I didn't explore yet but might other Kaggler needs this.
Time-series related use-case can be implemented on this data but I know it will take time to compile that amount of data. So stay tuned."	2042	11564	48	rsrishav	world-population
133	133	AirQuality		[]		0	3	0	masdimsss	airquality
134	134	COVID-19 Daily Case in Spain Dataset	UPDATES DAILY. Also containing data for different types of laboratory tests	['europe', 'public health', 'earth and nature', 'health', 'time series analysis', 'covid19']	"Context
Cumulative incidences and transmissibility indicators
The results presented in this COVID-19 Panel are obtained from the declaration of COVID-19 cases to the National Epidemiological Surveillance Network (RENAVE) through the SiViES (Surveillance System of Spain) computer platform via the Web. ) managed by the National Epidemiology Center (CNE). This information comes from the epidemiological case survey that each Autonomous Community carries out when a COVID-19 case is identified.
The COVID-19 Panel presents geographic information on cumulative incidence rates at 14 days and 7 days, for the general population and for those 65+ years of age, and indicators of the evolution of the pandemic's transmissibility. For the calculation of all the parameters, the date of onset of symptoms is used or, failing that, the date of diagnosis minus 6 days (from the start of the pandemic until May 10, 2020) or minus 3 days (from of May 11); for asymptomatic cases, the date of diagnosis is used. In those cases in which there is no date of onset of symptoms or diagnosis, the key date is used (date for statistics [It was lost to the autonomous communities to define the Key date as the date of onset of symptoms and in its absence the date of declaration to the AC, until May 10, 2020. From May 11 onwards, the Key date is the earliest of the dates of consultation or diagnosis. Occasionally it can be replaced by the date of sampling] ). Until May 10, 2020, cases diagnosed by a positive diagnostic test for active infection are included, as well as all those cases hospitalized, admitted to the ICU and deaths; As of May 11, cases confirmed by PCR, or by emergency tests, are included. The population used to calculate the incidence rates comes from the official population figures resulting from the revision of the municipal census as of January 1 of the National Institute of Statistics of 2020.
A regular update of the COVID-19 situation in Spain is carried out, after an extraction from the SiViES database from 3:00 p.m. to 4:00 p.m.
All of the data in this dataset has been sourced from https://cnecovid.isciii.es/covid19/
Should you choose to use said dataset, please cite the National Epidemiological Surveillance Network (RENAVE) and the SiViES (Surveillance System of Spain)
Data
casos_diag_ccaadecl.csv: Number of cases by diagnostic technique and Autonomous Communities (declaration) 
- ccaa_iso: Autonomous Communities ISO code of declaration
- fecha:The date of the diagnosis. In cases prior to May 11, the date of diagnosis is used, in his absence the date of declaration to the community and, in his absence, the key date (date used for statistics by the Autonomous Communities). In the cases after May 10, in the absence of a diagnosis date, the key date
- num_casos:Number of reported cases confirmed with a diagnostic test positive for active infection (PDIA) as established in the Strategy for early detection, surveillance and control of COVID-19 and also cases notified before May 11 that required hospitalization, admission in the ICU or died with a clinical diagnosis of COVID-19, according to the case definitions in force at any given time.
- num_casos_prueba_pcr: Number of cases with PCR laboratory test or molecular techniques
- num_casos_prueba_test_ac: Number of cases with laboratory rapid antibody test
- num_casos_prueba_ag: Number of cases with laboratory antigen detection test
- num_casos_prueba_elisa: Number of cases with high resolution serology laboratory test (ELISA/ECLIA/CLIA)
- num_casos_prueba_desconocida: Number of cases without information on the laboratory test
casos_hosp_uci_def_sexo_edad_provres.csv: Number of cases, hospitalizations, ICU admissions and deaths by sex, age and province of residence
- provincia_iso: ISO code of the province of residence. NC (not stated)
- sexo: Sex of the cases: H (man), M (woman), NC (not stated)
- grupo_edad: Age group to which the case belongs: 0-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, ≥80 years. NC: not stated.
- fecha: Date of registry. Cases: In cases prior to May 11, the date of diagnosis is used, in its absence the date of declaration to the community and, in its absence, the key date (date used for statistics by the CCAA). In cases after May 10, in the absence of diagnosis date the key3 date is used. Hospitalizations, ICU admissions, deaths: hospitalized cases are represented by date of hospitalization (if not, the date of diagnosis, and in failing that, the key date, the ICU cases by date of admission to the ICU (failing that, the date of diagnosis, and failing that, the key date) and deaths by date of death (if not, the date of diagnosis, and if not, the key date.).
- num_casos: Number of confirmed reported cases with a positive diagnostic test for active infection (PDIA) as established in the Early Detection Strategy, surveillance and control of COVID-19 and also the cases notified before May 11 that required hospitalization, admission to the ICU or died with a clinical diagnosis of COVID- 19, according to the case definitions in force at any given time
- num_hosp: Number of hospitalized cases
- num_uci: Number of cases admitted to the ICU
- num_def: Number of deaths
casos_tecnica_ccaa.csv: Number of cases by diagnostic technique and Autonomous Communities (of residence)
- ccaa_iso: Autonomous Communities ISO code of declaration
- fecha: From the start of the pandemic to May 10, the start date of symptoms or, failing that, the date of diagnosis minus 6 days. Starting of May 11, the date of onset of symptoms, or failing that, the date of diagnosis minus 3 days, or the date of diagnosis for cases asymptomatic
- num_casos: Number of cases by diagnostic technique and Autonomous Communities of residence
- num_casos_prueba_pcr: Number of cases with PCR laboratory test or molecular techniques
- num_casos_prueba_test_ac: Number of cases with rapid antibody test laboratory test
- num_casos_prueba_ag: Number of cases with laboratory antigen detection test
- num_casos_prueba_elisa: Number of cases with high resolution serology laboratory test (ELISA/ECLIA/CLIA)
- num_casos_prueba_desconocida: Number of cases without information on the laboratory test
casos_tecnica_provincia.csv: Number of cases by diagnostic technique and province (of residence)
provincia_iso: ISO code of the province of residence. NC (not stated)
fecha: From the start of the pandemic to May 10, the start date of symptoms or, failing that, the date of diagnosis minus 6 days. Starting of May 11, the date of onset of symptoms, or failing that, the date of diagnosis minus 3 days, or the date of diagnosis for cases asymptomatic
num_casos: Number of cases by diagnostic technique and province of residence
num_casos_prueba_pcr: Number of cases with PCR laboratory test or molecular techniques
num_casos_prueba_test_ac: Number of cases with rapid antibody test laboratory test
num_casos_prueba_ag: Number of cases with laboratory antigen detection test
num_casos_prueba_elisa: Number of cases with high resolution serology laboratory test (ELISA/ECLIA/CLIA)
num_casos_prueba_desconocida: Number of cases without information on the laboratory test"	28	288	8	asarvazyan	covid19-daily-case-hospitalization-spain-dataset
135	135	augmented text		['computer science']		0	1	0	jay2333	augmented-text
136	136	BDpracticafinal	Real state data of homes in the United States 	[]		1	61	1	ralarellanolopez	bdpracticafinal
137	137	brain-stroke-data	Isles data from http://www.isles-challenge.org/	[]		21	195	0	manoj312002	brainstrokedata
138	138	California Birth Rates 2008-2016	Number of In-Hospital Births by Mother's Age Group	['united states', 'health', 'beginner', 'data analytics', 'tabular data']	"""This dataset contains counts of in-hospital births by mother’s age groups (i.e., teen mothers, typical aged mothers and older mothers) based on the mother’s county of residence and year. This dataset does not include all births in California; only those births that occurred in a hospital.
Modified on October 11, 2018""
Dataset was taken from this site: https://data.chhs.ca.gov/dataset/inhospitalbirthsbymothersagegroup"	0	6	0	yonkotoshiro	california-birth-rates
139	139	LSTM_model		[]		0	9	1	narminj	lstm-model
140	140	Brain MRI detection and segmentation		[]		0	3	1	arcticai	brain-mri-detection-and-segmentation
141	141	World Coordinates	Longitude, Latitude and ISO names of country.	['geology', 'geospatial analysis']	"State-Location-Coordinates
Contains
240 + country's latitude and longitude.
2.7k+ state's latitude and longitude.
&gt; All data was prepared using geopy."	1	7	0	qramkrishna	world-coordinates
142	142	Sala Mango Images Dataset	Divided into bruised and non-bruised category	['agriculture', 'image data', 'food', 'health conditions']	"Context
Contains various images of fresh and bruised Sala mango (Mangifera Indica)
Content
The dataset contains 710 images of Sala Mango cultivar, also known as Perlis Sunshine variety. 
The images is divided into two categories: bruised (317 images) and non-bruised (393 images)
The image resolution is 1536 x 2048 (portrait orientation)
The background have been removed during preprocessing.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Image classification of high quality vs bruised mango
Image classification of mango variety
Object detection
Image segmentation of bruised or damaged area
Generative Adversarial Network / Style Transfer
License and Copyright
This dataset is licensed under Attribution-ShareAlike 4.0 International (CC BY-SA 4.0). 
You can use the dataset in your work, research, study, etc. Provided that you cite/attribute the dataset properly
Copyright 2021 (c) Mohammad Hafiz bin Ismail"	0	17	0	mypapit	sala-mango-image
143	143	TBDY 2018 - Deprem Kuvveti (Konut Binaları)		[]		0	4	0	tarkankaraay	tbdy-2018-deprem-kuvveti-konut-binalari
144	144	keras_self_attention 		[]		0	1	0	mbonyani	keras-self-attention
145	145	IPL_MEGA_AUCTION_2022	contains information of players and which team bought them at what price	['video games']	"Context
This is the dataset of IPL mega auctions held in 2022. this data has been web scrapped from the below-mentioned source.
https://timesofindia.indiatimes.com/sports/cricket/ipl/top-stories/ipl-2022-mega-auction-players-list-who-got-whom/articleshow/89519531.cms
this data can be used to uncover player stats and to know the likelihood of a player being picked or going unsold.
Inspiration
I was searching for the data of the result of the mega auction but couldn't find any, so decided to web scrape one and upload it here for the community :)"	1	10	1	kratos2597	ipl-mega-auction-2022
146	146	Dataset Tanaman Herbal 		['health']		3	64	1	anefiamutiaraatha	dataset-tanaman-herbal
147	147	wids2022_folds		[]		0	17	0	isha20	wids2022-folds
148	148	POI Review Corpus for Sentiment Analysis	POI review corpus dataset in the Korean language	['text data', 'ratings and reviews', 'korea']	"POI Review Corpus for Sentiment Analysis v1.0
This is a POI review corpus dataset in the Korean language. Reviews were scraped from Kakao Map. Each review is labeled with polarity - negative or positive.
Data description
Language: Korean
Source: Kakao Map
Columns: id, content, label
id: The review id
content: The actual review
label: The polarity of the review (0: negative, 1: positive)
Columns are delimited with tabs.
Data distribution
100K reviews in total
50K negative reviews (originally reviews of ratings 1~2)
50K positive reviews (originally reviews of ratings 4~5)
Neutral reviews (originally reviews of ratings 3) are excluded.
Quick peek
id      content label
139363  한번 가 봤는데 떡 식감이 너무 안좋았음. 떡볶이 쏘스도 별로였음. 단 참치 주먹밥은 먹을만 했음. 그 외에는 그냥 그냥 함   0
155416  싸고 맛있고 최고 분짜도 갠춘 다만 팟타이는 별로 1
148176  가격 감안해도 참 맛없음 0
143040  직원들 불친절함 바쁜건가?       0
71857   9/4 밤에 알바하시는 남자 알바분 엄청 불친절하고 손님이 와도 나와보지도 않아요… 게다가 다음 재료 뭐 넣을지 물어보지도 않고 말 안 하냐는 듯이 쳐다보기만 해서 무서웠어요   0
106816  다 정성이 들어간 느낌 ! 메뉴 모두 성공이에요    1
44078   미니리와 삼겹살 조합도 좋았고 반찬도 정갈하고 껍데기 서비스에 도시락도 좋았구요. 토욜 오후에 갔는데 무엇보다 직 원분 일하는 모습에 감동 제가 어디 사장이면 스카웃 하고 싶다는 ㅎㅎ      1
123452  깔끔하니 맛있는 순대국밥! 반찬도 맛있어요!      1
53617   분위기 좋고, 음식맛 최고였습니다. 직원분들 모두 친절하시고 좋았어요.    1
License
<p>
  <a href=""http://creativecommons.org/publicdomain/zero/1.0/"">
    <img src=""http://i.creativecommons.org/p/zero/1.0/88x31.png"" style=""border-style: none"" alt=""CC0"">
  </a>
</p>"	4	67	1	seongbumseo	poi-review-corpus-for-sentiment-analysis
149	149	sign_language digits	Sign Language Digits Datasets (RGB)	['earth and nature']		1	9	0	amalroshanp	sign-language
150	150	COVID-19 Community Mobility Dataset	Mobility and Infection for world in time series	['covid19']	"The Dataset has been extracted from google https://www.google.com/covid19/mobility/ for 120+ countries and have been tried to look into deep inside the mobility data. This data may help to understand community mobility inpacts on infection rate both countrywise as well as worldwide. File:
community_dataset_with_infection_count.csv
Description of fields :
1.COUNTRY_REGION_CODE-&gt;ISO2 country code
2.COUNTRY_REGION-&gt;Country Name
3.DATE_VAL-&gt;Date
4.DAY_CT-&gt; Day number starting from 15-Feb-2020
5.RETAIL_AND_RECREATION_PCT-&gt;Mobility trends for places like restaurants,
cafes, shopping centers, theme parks,
museums, libraries, and movie theaters.
6.INC_RETAIL_AND_RECREATION_PCT-Increase Mobility trends for places like restaurants,
cafes, shopping centers, theme parks,
museums, libraries, and movie theaters.
7.GROCERY_AND_PHARMACY_PCT-&gt;Mobility trends for places like grocery
markets, food warehouses, farmers
markets, specialty food shops, drug stores,
and pharmacies.
8.INC_GROCERY_AND_PHARMACY_PCT-&gt; Increase Mobility trends for places like grocery
markets, food warehouses, farmers
markets, specialty food shops, drug stores,
and pharmacies.
9.PARKS_PCT-&gt;Mobility trends for places like national parks,
public beaches, marinas, dog parks, plazas,
and public gardens.
10.INC_PARKS_PCT-&gt; Increase Mobility trends for places like national parks,
public beaches, marinas, dog parks, plazas,
and public gardens.
11.TRANSIT_STATIONS_PCT-&gt; Mobility trends for places like public transport
hubs such as subway, bus, and train stations.
12.INC_TRANSIT_STATIONS_PCT-&gt; Increase  Mobility trends for places like public transport
hubs such as subway, bus, and train stations.
13.WORKPLACES_PCT-&gt; Mobility trends for places of work
14.INC_WORKPLACES_PCT -&gt; Increase Mobility trends for places of work
15.RESIDENTIAL_PCT -&gt; Mobility trends for places of residence 
16.INC_RESIDENTIAL_PCT -&gt; Increase Mobility trends for places of residence
17.COVID_CONFIRMED -&gt; Confirm Covid cases for that country.
18.COVID_RECOVERED -&gt; Confirm Recover cases for that country.
19.COVID_DEATHS -&gt; Confirm Death  cases for that country.
20.COVID_CNTY_NEW_CASES -&gt; New Covid cases for that country.
21.COVID_COUNTRY_INC_RATE -&gt; Increase New Covid cases for that country.
22.COVID_WORLD_CONFIRMED --&gt; Confirm Covid cases for that World.
23.COVID_WORLD_RECOVERED -&gt; Confirm Recover cases for that World.
24.COVID_WORLD_DEATHS -&gt; Confirm Death cases for that World.
25.COVID_WORLD_INC_RATE -&gt;Confirm Increase cases for that world."	561	7735	17	arghadeep	covid19-community-mobility-dataset
151	151	COVID-19 Google mobility data	Kaggle mirror of Google COVID-19 mobility data	['computer science', 'transportation', 'covid19']	"This data serves as a Kaggle mirror of the COVID-19 mobility report published by Google. This will be updated regularly as Google pushes out additional reports.
For reference:
https://www.google.com/covid19/mobility/"	969	12466	30	devinaconley	covid19-mobility-data
152	152	Few-shot version of ARC	few-shot-version-of-arc	['games']		8	32	0	gmshroff	few-shot-arc
153	153	IPL Auction 2022 Detailed Dataset	Full Detailed data from official site	['cricket', 'sports', 'exploratory data analysis', 'data analytics', 'retail and shopping']	"Context
TATA IPL Auction - 2022
204 players were sold and INR 5,51,70,00,000 was splurged amongst the ten franchises during the two-day TATA Indian Premier League (IPL) 2022 Auction in Bengaluru. Young Indian cricketers dominated the proceedings at the auction, with Ishan Kishan attracting the topmost bid of INR 15.25 Crore which saw him return to MI. Meanwhile, Deepak Chahar, who returned to CSK for INR 14 Crore, became the most expensive Indian pace bowler to be ever bought at the IPL Auction. KKR broke the bank for Shreyas Iyer and bought the stylish right-handed batter for INR 12.25 Crore. Liam Livingstone, Wanindu Hasaranga, Nicholas Pooran, Shardul Thakur and Lockie Ferguson also made merry at the Auction while Avesh Khan became the most expensive uncapped player in the history of the IPL, with INR 10 Crore against his name, courtesy the new IPL entrant LSG. All and all, the ten franchises had two fruitful days at the mega auction and that sets up for a fascinating TATA IPL 2022. We can not wait!
Content
Players Sold
Players Unsold
Top Buys
TeamAuctionFundsDetails
Acknowledgements
This data is collected from official sources of https://www.iplt20.com/auction
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	1	9	1	purvanahar	ipl-auction-2022-detailed-dataset
154	154	Preventive to Predictive Maintenance	Extensive data set for machine learning on remaining useful life prognosis (PHM)	['universities and colleges', 'business']	"Context:
This data set originates from a practice-relevant degradation process, which is representative for Prognostics and Health Management (PHM) applications. The observed degradation process is the clogging of filters when separating of solid particles from gas. A test bench is used for this purpose, which performs automated life testing of filter media by loading them. For testing, dust complying with ISO standard 12103-1 and with a known particle size distribution is employed. The employed filter media is made of randomly oriented non-woven fibre material. 
Further data sets are generated for various practice-relevant data situations which do not correspond to the ideal conditions of full data coverage. These data sets are uploaded to Kaggle by the user ""Prognostics @ HSE"" in a continuous process. In order to avoid the carryover between two data sets, a different configuration of the filter tests is used for each uploaded practice-relevant data situation, for example by selecting a different filter media.
Detailed specification:
For more information about the general operation and the components used, see the provided description file Preventive to Predicitve Maintenance dataset.pdf
Given data situation:
The data set Preventive to Predicitve Maintenance is about the transition of a preventive maintenance strategy to a predictive maintenance strategy of a replaceable part, in this case a filter. To aid the realisation of predictive maintenance, life cycles have already been recorded from the application studied. However, the preventive maintenance in place so far causes them to be replaced after a fixed period of time, regardless of the condition of the degrading part. As a result, the end of life is not known for most records and thus they are right-censored. The so given training data are recorded runs of the filter up to a periodic replacement interval. 
When specifying the interval length for preventive maintenance, a trade-off has to be made between wasted life and the frequency of unplanned downtimes that occur, when having a particularly short life. The interval here is chosen so that, on average, failure is observed at the shortest 10% of the filter lives in the training data. The other lives are censored. The filter failure occurs when the differential pressure across the filter exceeds 600 Pa. The maintenance interval length depends on the amount of dust fed in per time, which is constant within a test run. For example, at twice the dust feed, the maintenance interval is half as long. The same relationship therefore applies to the respective censoring time, which scales inversely proportional with the particle feed. The variations between lifetimes are therefore primarily based on the type of dust, the flow rate and manufacturing tolerances. The filter medium CC 600 G was used exclusively for these measurement samples, which are included in this data set.
Task:
The objective of the data set is to precisely predict the remaining useful life (RUL) of the filter for the given test data, so a transition to predictive maintenance is made possible. For this purpose, the dataset contains training and test data, consisting both of 50 life tests respectively. The test data contains randomly right-censored run-to-failure measurements and the respective RUL as a ground truth to the prediction task. The main challenge is how to make the most use of the right-censored life data within the training data.
Due to the detailed description of the setup and the various physical filter models described in literature, it is possible to support the actual data-driven models by integrating physical knowledge respectively models in the sense of theory-guided data science or informed machine learning (various names are common).
Acknowledgement: 
Thanks go to Marc Hönig (Scientific Employee), Marcel Braig (Scientific Employee) and Christopher Rein (Research Assistant) for contributing to the recording of these life tests. 
Data set Creator:
Hochschule Esslingen - University of Applied Sciences
Research Department Reliability Engineering and Prognostics and Health Management
Robert-Bosch-Straße 1
73037 Göppingen
Germany
Dataset Citation:
Hagmeyer, S., Mauthe, F., & Zeiler, P. (2021). Creation of Publicly Available Data Sets for Prognostics and Diagnostics Addressing Data Scenarios Relevant to Industrial Applications. International Journal of Prognostics and Health Management, Volume 12, Issue 2, DOI: 10.36001/ijphm.2021.v12i2.3087"	316	3283	12	prognosticshse	preventive-to-predicitve-maintenance
155	155	deoldify single channeled converted images 	single channeled images colorized from happywhale competition	['arts and entertainment', 'religion and belief systems']	"Context
These are the colorized single channeled images in the whale competition https://www.kaggle.com/c/happy-whale-and-dolphin/
please note the dataset contains only converted black and white images you can find images I've converted with bw_images array npy file for other images refer to original data
Content
The dataset is generated with help of DeOldify models
github link https://github.com/jantic/DeOldify
link for data creation notebook https://www.kaggle.com/somesh88/coloring-single-channel-images-deoldify?scriptVersionId=87787281
Acknowledgements
@init27 thanks for idea 
Your data will be in front of the world's largest data science community. What questions do you want to see answered?
If this dataset helped you please don't forget to upvote it gives me encouragement :)"	3	62	2	somesh88	deoldify-single-channeled-converted-images
156	156	Large Scale Image Dataset of Wood Surface Defects		['business']		1	6	0	nomihsa965	large-scale-image-dataset-of-wood-surface-defects
157	157	clean_business		[]		0	0	0	hemanishah	clean-business
158	158	cbnetv2_wz		[]		0	0	0	lonelygeese	cbnetv2-wz
159	159	Cryptocurrency Dataset(1984-2020)	Cryptocurrency Publications(1984-2020)	['currencies and foreign exchange']	"Context
The Cryptocurrency research & publication dataset, which was indexed by Scopus from 1984 to 2020.
The dataset contains data authors, authors ID Scopus, title, year, source title, volume, issue, article number in Scopus, DOI, link, affiliation, abstract, index keywords, references, Correspondence Address, editors, publisher, conference name, conference date, conference code, ISSN, language, document type, access type, and EID
Acknowledgements
septianto, andre; purnomo, agung (2021), “Cryptocurrency dataset (1984-2020)”, Mendeley Data, V1, doi: 10.17632/f4zt4myrb7.1"	128	1933	11	saurabhshahane	cryptocurrency-dataset19842020
160	160	Martian/Lunar Crater Detection Dataset	Image dataset for crater detection on Mars/Moon surface	['astronomy', 'computer vision', 'image data']	"Why this dataset
Efficient detection of craters can be of vital significance in various space exploration missions. Previous reseraches have already made significant progress on this task, however the versatility and robustness of existing methods are still limited. While modern object detection methods using deep learning is gaining popularity and is probably a solution to aforementioned problems, public-accessible data for training is hard to find. This is the primary reason we propose this dataset.
What's in the dataset
The dataset mainly contains:
* Image Data: Images of Mars and Moon surface which MAY contain craters. The data source is mixed. For Mars images, images are mainly from ASU and USGS; Currently all Moon images are from NASA Lunar Reconnaissance Orbiter mission.
* Labels: Each image has its associated labelling file in YOLOv5 text format. The anotation work was performed by ourselves, and mainly serves the purpose of object detection.
* Trained YOLOv5 model file: For each new version, we will upload our pretrained YOLOv5 model file using the latest version of data. The network strcture currently in use is YOLOv5m6.
Methods to use
This dataset is somewhat challenging compared to trivial object detection task:
* Craters can greatly vary in size
* The dataset combines Mars and Moon surface images, where craters can be different in shape/color etc.
* Currently only around 100 images are available for training (if train-test-valid split is performed). However, please notice that more images will be added in the future.
In our own training with YOLOv5 framework using YOLOv5m6 pretrained model, we achieve a mAP_0.5 score of 0.6919. A sample notebook explaining the procedure will be available soon. Below are two sample detection results using our trained model.
This dataset is also available on the RoboFlow platform.
Credits
This dataset is a mixture of various data sources, we would like to thank each individual who participated. A detailed list of data source will be available soon."	0	23	0	lincolnzh	martianlunar-crater-detection-dataset
161	161	um-dataset-pub		[]		25	1706	0	dnpdatasetstudio	dataset-um-pub
162	162	trip_21_02		[]		0	1	0	charlesowolabi	trip-21-02
163	163	Philippine Presidential Elections Dataset	A dataset containing the historical data for all candidates from 1935 - 2022	['asia', 'politics', 'beginner', 'exploratory data analysis', 'data analytics']	"Context
This dataset is designed to help my fellow Filipinos to further understand the quality of the presidential candidates throughout the history of Philippine Elections. While historical textbooks have given the background of who the resulting presidents were, we also need to know who the contenders of these candidates were so that we can make better decisions as to which candidate should we elect for the upcoming elections.
Content
This data was acquired through manual imputation and research in Wikipedia articles and the Genealogy website (for the heritage of each candidate). This dataset contains the attributes of each candidate in terms of demographics, educational/work background, and their presence in the elections. Not all candidates, however, have complete information due to incomplete records.
Acknowledgements
This research would not be made possible without the contributors of the Wiki pages, mainly those who have researched and documented records of the previous elections.
Inspiration
Apart from understanding who and what the candidates are, I would really want to know if it is possible to predict the winnability of each candidate through personal background alone and whether it is possible to determine which among the candidates of the present elections (2022) would win the elections."	7	28	0	abeperez	ph-presidential-elections
164	164	Wheat Head Dataset		['nutrition']		0	35	5	hungkhoi	wheat-head-dataset
165	165	happywhale-tfrecords-v1		[]		0	3	0	fss331	happywhale-tfrecords-v1
166	166	ICDAR-2011-Signature-Verification		[]	Source -www.iapr-tc11.org-ICDAR_2011_Signature_Verification_Competition_(SigComp2011)	0	2	0	paulrohan2020	icdar2011signatureverification
167	167	Blood Cell Detection Datatset	Dataset contains images of blood cells which can be used for classification	['health', 'advanced', 'computer vision', 'classification', 'image data']		326	4827	26	adhoppin	blood-cell-detection-datatset
168	168	Dancho Danchev's Cybercrime Forum Data Set Torrent	"Dancho Danchev's Official ""Cybercrime Forum Data Set"" 108GB Torrent"	['online communities']		5	273	1	danchodanchev	dancho-danchevs-cybercrime-forum-data-set-torrent
169	169	Database Film Indonesia		['business']		3	5	0	haryodwi	database-film-indonesia
170	170	cities and rainfall		['cities and urban areas', 'earth science', 'tabular data']	"Context
Just wanted to compare rainfall between cities cause the city i live in got flooded. So I created this dataset manually the values in this dataset are from internet
Content
there are 5 columns in the dataset namely city, average annual rainfall in mm, average annual rainy days, country, snow"	1	27	0	kishorvijayjagadale	cities-and-rainfall
171	171	F1 Visa Experiences	Daily updated f1 visa experiences	['united states', 'india', 'education', 'nlp']	"Context
F1-Visa is a type of non-immigrant visa provided by the US Government to allow students to temporarily live in the US for a defined period of time while studying at a school. It is the 3rd most applied visa in the whole world. For many students, it is the culmination of years of hard work and dedication. 
After appearing for TOEFL/IELTS, GRE and applying for schools of interest, and getting an admit, attending the Visa Interview is the next step, which arguably is also the toughest. It is a subjective process, where a Visa officer asks a serious of question to the prospective student and arrives at a conclusion deeming his fitness. 
An example for the denial could be,
- Signs of family ties already present in the US (makes a student highly unlikely to return back to home country)
- Lack of funds for education
- No ties between experience and the major chosen
- Questionable universities and academic performances
The data comes from a telegram channel and all the visa experiences mainly are from India. I am working on adding more sources. So watch out!
Code:
You can find the scrapper and the Github action for this dataset here,
https://github.com/adiamaan92/f1-visa-experience-scrapper
&gt; The scrapper is scheduled to run every day at 8 AM using a Github Action workflow.
Acknowledgements
A huge thanks to the mods of the Telegram channel, t.me/f1interviewreviews for moderating the community and pushing out new experiences frequently.
Inspiration
What are some of the commonalities for getting your visa rejected?
India has embassies across multiple cities like Delhi, Mumbai, Kolkatta, Hyderabad, Chennai etc.
Are the denials more likely to happen in a city? etc.."	75	401	5	adiamaan	f1-visa-experiences
172	172	large movie dataset tar	Large movie dataset in tar for easy download	['arts and entertainment', 'beginner', 'nlp', 'text data', 'transformers']	Taken from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz	9	930	7	fanbyprinciple	large-movie-dataset-tar
173	173	AutoXGB	AutoXGB: XGBoost + Optuna (train + tune + serve)	['computer science', 'classification', 'xgboost', 'regression', 'automl']	"AutoXGB
This is the official Kaggle dataset for AutoXGB
The GitHub version can be found here
You can also install AutoXGB locally using: pip install autoxgb"	19	1089	47	abhishek	autoxgb
174	174	The IBDColEpi dataset	251 HE/CD3 colon biopsies. 31k epithelium annotations. Active/inactivate IBD.	['health', 'health conditions']	"IMPORTANT: DataverseNO is currently struggeling with interrupted download of large files. An alternative download site has therefore been established here: https://drive.google.com/drive/folders/1eUVs1DA1UYayUYjr8_aY3O5xDgV1uLvH?usp=sharing
The IBDColEpi dataset follows the arXiv preprint of the paper ""Code-free development and deployment of deep segmentation models for digital pathology"" by Henrik Sahlin Pettersen, Ilya Belevich, Elin Synnøve Røyset, Erik Smistad, Eija Jokitalo, Ingerid Reinertsen, Ingunn Bakke, and André Pedersen. The dataset is stored at https://dataverse.no/dataset.xhtml?persistentId=doi:10.18710/TLA01U . Use of the dataset in publications should cite the paper ( https://arxiv.org/abs/2111.08430 ).
General description:
The dataset consists of 140 HE and 111 CD3-stained whole slide images (WSIs) from formalin fixed paraffin embedded (FFPE) biopsies of colonic mucosa of active and inactivate inflammatory bowel disease where all the epithelium is annotated (the IBDColEpi dataset). 
Each database ID number used in this study was changed to new anonymized IDs only containing the information “active” or “inactive” disease and whether the WSI has haematoxylin-eosin (HE) staining or CD3 immunostaining. The biopsies included in the biobank are sampled such that one biopsy from an unaffected/inactive area and one from an area affected/active area were included from each patient and given a separate ID number. Hence, two biopsies with different ID numbers can be from the same patient. ""Active"" is defined as the presence of intraepithelial granulocytes in one or more location in the biopsies. Still, the changes may be focal, hence majority of the epithelium may still lack intraepithelial granulocytes or other signs of active disease (crypt abscesses, granulation tissue, etc.).
Annotation procedures:
All annotations were created using the NoCodeSeg pipeline from the paper (https://arxiv.org/abs/2111.08430) using QuPath/DeepMIB/FastPathology and all scripts for the pipeline are available at https://github.com/andreped/NoCodeSeg.
While annotating epithelium, we strived to draw lines as close to the basement membrane as possible where visible. All annotations were finally refined by the following QuPath scripts to ensure consistency throughout the datasets: - Minimum fragment size min 75 um^2 - Minimum hole size min 100 um^2 - remove white background with intensity greater than 220 average all channels with sigma 2.5 - expand and shrink all annotations 1.0 mm.
We have created a video tutorial on how the dataset was annotated and how to annotate and visualize other similar histopathological whole slide images without coding using QuPath/DeepMIB/FastPathology:
File descriptions:
WSI_part. zip: The colon biopsy whole slide images (WSIs) in .ndpi format (can be read by QuPath). For each ID, the staining (HE/CD3) X label, and active/inactive label Y is stored in the filename, with the format: ""ID-X_Y.ndpi"". The full dataset is split into five zip-files to make downloading more manageable.
TIFF-annotations.zip: the corresponding annotations to the WSIs. The filenames of the annotations are in the same structure as the corresponding WSIs, with the format: ""ID-X_Y.tiff"".
patch-dataset-*.zip: the corresponding patch images and labels, split into train/validation/test sets, relevant for the evaluation of the design in the publication both for the HE and CD3 dataset.
qupath-project-annotations.zip: the qupath project file, also containing the annotations of all WSIs, but can be directly read in QuPath (after renaming of WSI paths).
Laboratory methods:
FFPE sections of 4 µm were cut, mounted on slides and either stained with hematoxylin (Mayer’s) and Eosin (Y) (HE) or subjected to standard pre-treatment with quenching of endogenous peroxidase and boiling in Tris EDTA pH9 for antigen retrieval before immunohistochemistry. Primary antibody for the T lymphocyte marker was mouse anti-human CD3 (M7254, clone F7.2.38, Dako Agilent, CA, USA), diluted 1:50 in antibody diluent Tris buffer with 0.025% Tween-20 and 1% BSA and incubated overnight at 4 °C. Immunoreactions were visualized with the secondary antibody rabbit/mouse EnVisionHRP/DAB+ kit (K5007, Dako Agilent) and counterstaining with haematoxylin. Omission of the primary antibody was used as negative control and sections from human peripheral lymph node as positive control.
Ethics approval:
The biopsies were extracted from the NTNU/St. Olavs hospital, Trondheim University Hospital (Norway) biobank of patients with confirmed inflammatory bowel disease or healthy controls with gastrointestinal symptoms but no macroscopic- or microscopic disease. Inclusion and colonoscopies were performed at the Department of Gastroenterology and Hepatology at St. Olavs hospital, Trondheim University Hospital from 2007 to 2018. All patients gave written informed consent and ethical approvals were obtained from the Central Norway Regional Committee for Medical and Health Research Ethics (reference number 2013/212/REKMidt). Consent to publish the anonymized whole slide image (WSI) dataset was given by REKMidt in 2021."	17	287	3	henrikpe	251-he-cd3-wsis-annotated-epithelium-ibdcolepi
175	175	Weather/climate data for JHU's COVID19 sites	Using JHU's data to try and get weather/climate data	['weather and climate']	"Context
This is the weather and climate data that I was able to pull for all of JHU's confirmed COVID19 injection sites.
Content
The weather and climate data are very generously Powered by Dark Sky: https://darksky.net/poweredby/.
I started by forking JHU's daily confirmed infections Github repository. As such, some of the file and directory structure is the same as theirs. I then used their time_series_19-covid-Confirmed.csv file to create the general template of how to capture data. Their data start with 1/22/20. In the interest of capturing lead time, I added columns for the rest of January.
I then called Dark Sky's API to capture the data for each given date and location. Please see https://darksky.net/dev/docs for description of the various fields that are captured. All of the following details pertain to data from the 'daily' element in the JSON file that is returned. Where applicable, the units that are returned are in SI units 
tMax.csv
'temperatureHigh' field, provided as degrees Celsius
tMin.csv
'temperatureLow' field, provided as degrees Celsius
humidity.csv
'humidity' field, provided as relative humidity (percent out of 100)
uv.csv
'uvIndex' field, provided as UV index. Honestly, I'm not sure what the units here are.
cloud.csv
'cloudCover' field, provided as The percentage of sky occluded by clouds (out of 100)
precip.csv
'precipProbability' field, provided as the probability of precipitation occurring (out of 100)
dew.csv
'dewPoint' field, provided as degrees Fahrenheit (according to the documentation) I made the request for SI units
Frankly, I don't know enough about this to know if the numbers look like Fahrenheit or Celsius
pressure.csv
'pressure"" field, provided as sea-level air pressure in millibars
wind.csv
'windSpeed' field, provided as wind speed in miles per hour (according to the documentation). Again as above,
I made the request for SI units. I don't know enough to know which one is more likely.
ozone.csv
'ozone' field, provided as columnar density of total atmospheric ozone at the given time in Dobson units
sunrise.csv
'sunriseTime' field, provided as Unix time (https://en.wikipedia.org/wiki/Unix_time)
sunset.csv
'sunsetTime' field, provided as Unix time
The above CSV files are in the ./csv directory. 
Caveat: None of my code, data, or methodology have been audited or verified or anything. Please keep that in mind before trying to draw any conclusions.
Values that are set to -1000 are dummy values for when there was an error in the JSON response. I'll eventually get to mitigating this as much as possible.
Acknowledgements
Thanks to the folks at JHU for maintaining their repository at https://github.com/CSSEGISandData/COVID-19
Thanks to Dark Sky for providing an easy-to-use and affordable API. If anyone wants to their their own hand at it, Dark Sky provides 1,000 free API calls per day.  https://darksky.net/poweredby/
Inspiration
Maybe these could help in correlating with JHU's own data, and/or other data sets to try to help and figure out how the virus's spread is affected by weather."	517	6465	10	eeemonts	weatherclimate-data-covid19
176	176	Hourly Dataset of Vehicle For Traffic Analysis	Traffic Flow Prediction and Vehicle Analysis	['automobiles and vehicles']		0	5	0	tabarkarajab	hourly-dataset-of-vehicle-for-traffic-analysis
177	177	Beehives	Temperature and Relative Humidity Data of Beehives	['biology', 'agriculture', 'datetime']	"Context
This dataset is a part of my master's thesis ""Machine Learning Based Smart Beehive Monitoring System Without Internet Network"".
Data collected from hive number 17, 36 and 85 in an apiary located in Çanakkale, Turkey.
The data used with unsupervised anomaly detection algorithms to determine the critical temperature and relative humidity values for calibrating the alarm system.
The results showed that fatal anomalies can be detected weeks prior to colony loss.
Content
+1000 rows and 10 columns.
Columns' description are listed below.
DateTime : Datetime in ""dd.mm.yyyy hh:mm"" format
T17 : Temperature of hive 17
RH17 : Relative humidity of hive 17
AT17 : Apparent temperature of hive 17
Tamb : Ambient temperature
RHamb : Ambient relative humidity
ATamb : Ambient apparent temperature
T17-Tamb : T17 - Tamb
AT17-ATamb : AT17 - ATamb
Acknowledgements
Data from Esra Ece Var.
Image from Esra Ece Var.
If you're reading this, please upvote."	18	392	6	vivovinco	beehives
178	178	testtt		[]		0	0	0	albertagungandika	testtt
179	179	My data	i made it for a test	[]		0	2	0	sarayabesi	my-data
180	180	2022 IPL Auctions list	TATA IPL Auction 2022	['cricket', 'india', 'retail and shopping']	"**### Context
IPL 2022 Auction list
Content
All the data about the players sold, unsold and which team bought them during the 2022 IPL auction 
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	3	22	0	venomo12334	2022-ipl-auctions-list
181	181	NBMS_dataset		[]		0	15	0	arvinddevarkonda	nbms-dataset
182	182	Indonesia Traffic Sign		['law']		0	1	0	cakrulgaming	indonesia-traffic-sign
183	183	DATA_1s		[]		0	0	0	mithiljoshi	data-1s
184	184	Autism-Prediction 		[]		0	3	0	anjalisharma123	autismprediction
185	185	Masked Fer2013		['biotechnology', 'artificial intelligence', 'computer science', 'image data', 'covid19']		0	4	1	shubhanjaypandey	masked-fer2013
186	186	CCIHP dataset	Characterized Crowd Instance-level Human Parsing	['artificial intelligence', 'computer science', 'programming', 'computer vision', 'deep learning', 'image data']	"Characterized Crowd Instance-level Human Parsing (CCIHP) dataset
CCIHP dataset is devoted to fine-grained description of people in the wild with localized & characterized semantic attributes. It contains 20 attribute classes and 20 characteristic classes split into 3 categories (size, pattern and color).
The annotations were made with Pixano, an opensource, smart annotation tool for computer vision applications: https://pixano.cea.fr/
CCIHP dataset provides pixelwise image annotations for:
human segmentation, 
semantic attribute segmentation and 
semantic attribute characterization.
Dataset description
Images:   
The image data are the same as CIHP dataset (see Section Related work) proposed at the LIP (Look Into Person) challenge. They are available at google drive and baidu drive. (Baidu link does not need access right).
Annotations:
Please download and unzip the CCIHP_icip.zip file. The CCIHP annotations can be found in the Training and Validation sub-folders of CCIHP_icip2021/dataset/ folder. They correspond to, respectively, 28,280 training images and 5,000 validation images. Annotations consist of:
Human_ids: person instance labels
Instance_ids: attribute instance labels
Category_ids: attribute category labels
Size_ids: size category labels
Pattern_ids: pattern category labels
Color_ids: color category labels
Label meaning for semantic attribute/body parts:
Hat: Hat, helmet, cap, hood, veil, headscarf, part covering the skull and hair of a hood/balaclava, crown...
Hair
Glove
Sunglasses/Glasses: Sunglasses, eyewear, protective glasses...
UpperClothes: T-shirt, shirt, tank top, sweater under a coat, top of a dress...
Face Mask: Protective mask, surgical mask, carnival mask, facial part of a balaclava, visor of a helmet...
Coat: Coat, jacket worn without anything on it, vest with nothing on it, a sweater with nothing on it...
Socks
Pants: Pants, shorts, tights, leggings, swimsuit bottoms... (clothing with 2 legs)
Torso-skin
Scarf: Scarf, bow tie, tie...
Skirt: Skirt, kilt, bottom of a dress...
Face
Left-arm (naked part)
Right-arm (naked part)
Left-leg (naked part)
Right-leg (naked part)
Left-shoe
Right-shoe
Bag: Backpack, shoulder bag, fanny pack... (bag carried on oneself)
Others: Jewelry, tags, bibs, belts, ribbons, pins, head decorations, headphones...
Label meaning for size characterization:
Short: Small, short, narrow
Long: Long, large, big
Undetermined: If the attribute is partially hidden
Sparse/bald: For hair attribute only
Label meaning for pattern characterization:
Solid
Geometrical: Stripes, Checks, Dots...
Fancy: Flowers, Military...
Letters: Letters, numbers, symbols...
Label meaning for color characterization:
Dark: No dominant color, includes black, navy blue
Medium: No dominant color, including gray
Light: No dominant color, including white
Brown
Red
Pink
Yellow
Orange
Green
Blue
Purple
Multicolor: When there is a pattern with several colors
Related work
Our work is based on CIHP image dataset from: 
Ke Gong, Xiaodan Liang, Yicheng Li, Yimin Chen, Ming Yang and Liang Lin, ""Instance-level Human Parsing via Part Grouping Network"", ECCV 2018.
Evaluation
To evaluate the predictions given by a Human Parsing with Characteristics model, you can run the python scripts in CCIHP_icip2021/evaluation/ folder.
Requirements
python==3.6+
opencv-python
pillow
Evaluation steps
Run generate_characteristic_instance_part_ccihp.py 
Run eval_test_characteristic_inst_part_ap_ccihp.py for mean Average Precision based on characterized region (AP^(cr)_(vol)). It evaluates the prediction of characteristic (class & score) relative to each instanced and characterized attribute mask, independently of the attribute class prediction. 
Run metric_ccihp_miou_evaluation.py for a mIoU performance evaluation of semantic predictions (attribute or characteristics).
License
Data annotations are under Creative Commons Attribution Non Commercial 4.0 license (see LICENSE file).
Evaluation codes are under MIT license.
Citation
A. Loesch and R. Audigier, ""Describe Me If You Can! Characterized Instance-Level Human Parsing,"" 2021 IEEE International Conference on Image Processing (ICIP), 2021, pp. 2528-2532, doi: 10.1109/ICIP42928.2021.9506509.
BibTeX
@INPROCEEDINGS{ccihp_dataset_2021,
  author={Loesch, Angelique and Audigier, Romaric},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Describe Me If You Can! Characterized Instance-Level Human Parsing}, 
  year={2021},
  volume={},
  number={},
  pages={2528-2532},
  doi={10.1109/ICIP42928.2021.9506509}},
Contact
If you have any question about this dataset, you can contact us by email at:
ccihp-dataset@cea.fr"	19	979	6	angeliqueloesch	ccihp-characterized-crowd-instancelevel-hp
187	187	gradsflow	AutoML & Model Training Library for PyTorch	[]		1	667	0	aniketmaurya	gradsflow
188	188	NY-TIMES COVID-19 USA dataset (Regular updates)	NYTIMES Coronavirus (Covid-19) Stats for the United States	['global', 'health', 'law', 'military', 'politics', 'covid19']	"Historical Coronavirus (Covid-19) Data for the United States
NEW: We are publishing the data behind our excess deaths tracker in order to provide researchers and the public with a better record of the true toll of the pandemic. This data is compiled from official national and municipal data for 24 countries. See the data and documentation in the excess-deaths/ directory.
[ U.S. Data (Raw CSV) | U.S. State-Level Data (Raw CSV) | U.S. County-Level Data (Raw CSV) ]
The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak.
Since late January, The Times has tracked cases of coronavirus in real time as they were identified after testing. Because of the widespread shortage of testing, however, the data is necessarily limited in the picture it presents of the outbreak.
We have used this data to power our maps and reporting tracking the outbreak, and it is now being made available to the public in response to requests from researchers, scientists and government officials who would like access to the data to better understand the outbreak.
The data begins with the first reported coronavirus case in Washington State on Jan. 21, 2020. We will publish regular updates to the data in this repository. 
Live and Historical Data
We are providing two sets of data with cumulative counts of coronavirus cases and deaths: one with our most current numbers for each geography and another with historical data showing the tally for each day for each geography.
The historical data files are at the top level of the directory and contain data up to, but not including the current day. The live data files are in the live/ directory.
A key difference between the historical and live files is that the numbers in the historical files are the final counts at the end of each day, while the live files have figures that may be a partial count released during the day but cannot necessarily be considered the final, end-of-day tally..
The historical and live data are released in three files, one for each of these geographic levels: U.S., states and counties.
Each row of data reports the cumulative number of coronavirus cases and deaths based on our best reporting up to the moment we publish an update. Our counts include both laboratory confirmed and probable cases using criteria that were developed by states and the federal government. Not all geographies are reporting probable cases and yet others are providing confirmed and probable as a single total. Please read here for a full discussion of this issue.
We do our best to revise earlier entries in the data when we receive new information. If a county is not listed for a date, then there were zero reported confirmed cases and deaths.
State and county files contain FIPS codes, a standard geographic identifier, to make it easier for an analyst to combine this data with other data sets like a map file or population data.
Download all the data or clone this repository by clicking the green ""Clone or download"" button above.
Historical Data
U.S. National-Level Data
The daily number of cases and deaths nationwide, including states, U.S. territories and the District of Columbia, can be found in the us.csv file.  (Raw CSV file here.)
date,cases,deaths
2020-01-21,1,0
...
State-Level Data
State-level data can be found in the states.csv file. (Raw CSV file here.)
date,state,fips,cases,deaths
2020-01-21,Washington,53,1,0
...
County-Level Data
County-level data can be found in the counties.csv file. (Raw CSV file here.)
date,county,state,fips,cases,deaths
2020-01-21,Washington,Snohomish,53061,1,0
...
In some cases, the geographies where cases are reported do not map to standard county boundaries. See the list of geographic exceptions for more detail on these.
Live Data
The files in the live/ directory are also available at three geographic levels and contain all the fields the historical data files have, but with only data for the current day. We try to update these files multiple times per day. 
Because these are updated throughout the day, they can have inconsistent counts, are more likely to contain errors, and should be considered less reliable than the historical data. Different areas of the country update at different times and our data collection process can move at a different pace as well.
In addition to the columns that are in the historical files, these files also include new columns that include detail on the number of confirmed and probable cases, separately.
In the live files, the case and death fields have the following definitions:
cases: The total number of cases of Covid-19, including both confirmed and probable.
deaths: The total number of deaths from Covid-19, including both confirmed and probable.
confirmed_cases: The number of laboratory confirmed Covid-19 cases only, or blank if not available.
confirmed_deaths: The number of laboratory confirmed Covid-19 deaths only, or blank if not available.
probable_cases: The number of probable Covid-19 cases only, or blank if not available.
probable_deaths: The number of probable Covid-19 deaths only, or blank if not available.
We understand this breakout would also be valuable historically, and are working toward providing that. Please bear with us as we roll out this new and more complicated data.
The live data can be found in files at the U.S. level in the us.csv file, at the state level in the states.csv file, and at the county level in the counties.csv file.
Methodology and Definitions
The data is the product of dozens of journalists working across several time zones to monitor news conferences, analyze data releases and seek clarification from public officials on how they categorize cases. 
It is also a response to a fragmented American public health system in which overwhelmed public servants at the state, county and territorial level have sometimes struggled to report information accurately, consistently and speedily. On several occasions, officials have corrected information hours or days after first reporting it. At times, cases have disappeared from a local government database, or officials have moved a patient first identified in one state or county to another, often with no explanation. In those instances, which have become more common as the number of cases has grown, our team has made every effort to update the data to reflect the most current, accurate information while ensuring that every known case is counted.
When the information is available, we count patients where they are being treated, not necessarily where they live.
In most instances, the process of recording cases has been straightforward. But because of the patchwork of reporting methods for this data across more than 50 state and territorial governments and hundreds of local health departments, our journalists sometimes had to make difficult interpretations about how to count and record cases.
For those reasons, our data will in some cases not exactly match with the information reported by states and counties. Those differences include these cases: When the federal government arranged flights to the United States for Americans exposed to the coronavirus in China and Japan, our team recorded those cases in the states where the patients subsequently were treated, even though local health departments generally did not. When a resident of Florida died in Los Angeles, we recorded her death as having occurred in California rather than Florida, though officials in Florida counted her case in their own records. And when officials in some states reported new cases without immediately identifying where the patients were being treated, we attempted to add information about their locations later, once it became available.
Confirmed Cases
Confirmed cases and deaths are counts of individuals whose coronavirus infections were confirmed by a laboratory test and reported by a federal, state, territorial or local government agency.
The number of cases includes all cases, including those who have since recovered or died.
""Probable"" Cases and Deaths
Probable cases and deaths count individuals who did not have a confirmed test but were evaluated using criteria developed by states and the federal government.
On April 5, the Council of State and Territorial Epidemiologists Centers advised states to include both confirmed cases, based on laboratory testing, and probable cases, based on specific criteria for symptoms and exposure. The Centers for Disease Control adopted these definitions and national CDC data began including confirmed and probable cases on April 14.
Some governments continue to report only confirmed cases, while others are reporting both confirmed and probable numbers. And there is also another set of governments that are reporting the two types of numbers combined without providing a way to separate the confirmed from the probable.
Please see the Geographic Exceptions section below for more details on specific areas, with the understanding that this changes frequently.
Dates
For each date, we show the cumulative number of confirmed cases and deaths as reported that day in that county or state. All cases and deaths are counted on the date they are first announced.
Each date includes all cases and deaths announced that day through midnight Eastern Time. As the West Coast and Hawaii tend to release all of their new data early enough in the day.
Declining Counts
In some cases, the number of cases or deaths for a state or county will decline. This can occur when a state or county corrects an error in the number of cases or deaths they've reported in the past, or when a state moves cases from one county to another. When we are able, we will historically revise counts for all impacted dates. In other cases, this will be reflected in a single-day drop in the number of cases or deaths.
Counties
In some instances, we report data from multiple counties or other non-county geographies as a single county. For instance, we report a single value for New York City, comprising the cases for New York, Kings, Queens, Bronx and Richmond Counties. In these instances the FIPS code field will be empty. (We may assign FIPS codes to these geographies in the future.) See the list of geographic exceptions. 
Cities like St. Louis and Baltimore that are administered separately from an adjacent county of the same name are counted separately.
“Unknown” Counties
Many state health departments choose to report cases separately when the patient’s county of residence is unknown or pending determination. In these instances, we record the county name as “Unknown.” As more information about these cases becomes available, the cumulative number of cases in “Unknown” counties may fluctuate.
Sometimes, cases are first reported in one county and then moved to another county. As a result, the cumulative number of cases may change for a given county.
Geographic Exceptions
New York
All cases for the five boroughs of New York City (New York, Kings, Queens, Bronx and Richmond counties) are assigned to a single area called New York City. There is a large jump in the number of deaths on April 6th due to switching from data from New York City to data from New York state for deaths.
For all New York state counties, starting on April 8th we are reporting deaths by place of fatality instead of residence of individual. There were no new deaths reported by the state on April 17th or April 18th.
On June 30, the New York City health department announced an additional 692 deaths in New York City residents, most of which had taken place outside the city more than three weeks ago. We are counting these deaths when they are in excess of the number of deaths in New York City residents counted by the state.
Georgia
Starting April 12th, our case count excludes cases labeled by the state as ""Non-Georgia Resident"" leading to a one day drop in cases. These cases were previously included as cases with ""Unknown"" county.
Alabama
Alabama's numbers for April 17th contained an error in reporting of lab test results that the state is working to correct. The number of deaths drops on April 23rd for an unknown reason.
Kansas City, Mo.
Four counties (Cass, Clay, Jackson and Platte) overlap the municipality of Kansas City, Mo. The cases and deaths that we show for these four counties are only for the portions exclusive of Kansas City. Cases and deaths for Kansas City are reported as their own line.
Joplin, Mo.
Starting June 25, cases and deaths for Joplin are reported separately from Jasper and Newton counties. The cases and deaths reported for those counties are only for the portions exclusive of Joplin. Joplin cases and deaths previously appeared in the counts for those counties or as Unknown.
Alameda County, Calif.
Counts for Alameda County include cases and deaths from Berkeley and the Grand Princess cruise ship.
Douglas County, Neb.
Counts for Douglas County include cases brought to the state from the Diamond Princess cruise ship.
Chicago
All cases and deaths for Chicago are reported as part of Cook County.
Guam
Counts for Guam include cases reported from the USS Theodore Roosevelt.
Puerto Rico
On April 21, the territory's health department revised their number of cases downward, saying they had been double counting some coronavirus patients in official reports, leading to a higher number of cases reported than actually confirmed. 
North Dakota
On May 25, North Dakota announced that due to a laboratory equipment malfunction they were removing 82 positive results from their total case count, pending a retest of the samples.
Connecticut
On May 27, Connecticut announced announced that they were removing 356 positive cases, which were determined to be duplicates, from their total case count.
The number of deaths reported by the state in four counties on June 1 was anomalously high and several deaths are removed in the data for June 2.
Louisiana
On May 29, Louisiana announced that due to a technical error they would not have an update on the number of total cases that day.
On June 13, Louisiana reported an additional 560 backlogged cases from multiple labs and facilities from between April 25 and June 9.
On June 16, Louisiana reported an additional 148 backlogged cases, the majority of which date back to mid-April.
On June 19, Louisiana removed 1,666 duplicate and out of state cases from their total.
Massachusetts
On April 24, Massachusetts reported the results of a large number of backlogged tests performed by Quest Diagnostics dating back to April 13, leading to a large one day jump in the number of total cases.
On June 30, the number of confirmed and probable deaths declined due to Massachusetts removing duplicate reports.
Mississippi
From June 18 to 21, the Mississippi State Department of Health reported technical difficulties that prevented them from updating their case and death counts.
Texas
On June 16, Texas reported an additional 1,476 backlogged cases from prison inmates in Anderson and Brazoria counties.
Washington
On June 17, Washington began removing from their totals deaths where Covid-19 was not a factor, for instance homicides, overdoses, suicides and car accidents. Four deaths from King County and three from Yakima county that were due to homicide, suicide or overdose were removed.
Probable Cases and Deaths
Colorado
Numbers reflect the combined number of lab-confirmed and probable cases and deaths as reported by the state. On April 25th, the state revised downward the number of deaths after removing ""about 29 duplicates"" from the number of ""probable deaths"" included in the total.
Hawaii
Numbers reflect the combined number of lab-confirmed and probable cases and deaths as reported by the state.
Illinois
On June 8, Illinois started reporting probable cases and deaths in their data. We are including these cases and deaths in our total numbers for the state.
Louisiana
The total cases number and total deaths number include only lab-confirmed cases and deaths. The state is reporting the deaths of probable Covid-19 cases separately from their total number of deaths statewide and in each parish, and we are including those deaths in our total number of deaths for the state.
Massachusetts
On June 1, Massachusetts started reporting probable cases and deaths in their data. The total number of cases and deaths on that day include probable cases and deaths going back to March 1, leading to a large one day jump in the totals.
Michigan
On June 1, we began recording probable cases and deaths reported by Michigan's county and regional health districts and adding them to the individual county and statewide totals. On June 5, the state also started to report probable cases and deaths statewide, leading to a jump in total cases and deaths.
New Jersey
On June 25, New Jersey began reporting probable deaths, adding 1,854 probable deaths that may date back to earlier in the outbreak to their total.
Ohio
The state reports lab-confirmed and probable cases and deaths separately at the state level but combine lab-confirmed and probable cases and deaths at the county level. Our statewide and county numbers combine both case types.
Pennsylvania
The total cases number includes lab-confirmed and probable cases starting around April 16th.
Virginia
The state reports lab-confirmed and probable cases and deaths separately at the state level but combine lab-confirmed and probable cases and deaths at the county level. Our statewide and county numbers combine both case types.
Wisconsin
Wisconsin started reporting probable cases and deaths on June 10, causing a large spike in the number of cases on that day. The total number of cases that day includes 2,407 newly reported probable cases.
Puerto Rico
Puerto Rico reports confirmed and probable cases and deaths separately. Our statewide and municipality numbers combine both case types.
Starting April 12, the count of deaths for Puerto Rico include some probable Covid-19 related deaths. From April 19 to April 22, these were then removed. Starting April 23, the numbers again include probable deaths. We will revise these numbers as possible.
License and Attribution
In general, we are making this data publicly available for broad, noncommercial public use including by medical and public health researchers, policymakers, analysts and local news media.
If you use this data, you must attribute it to “The New York Times” in any publication. If you would like a more expanded description of the data, you could say “Data from The New York Times, based on reports from state and local health agencies.”
If you use it in an online presentation, we would appreciate it if you would link to our U.S. tracking page at https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html.
If you use this data, please let us know at covid-data@nytimes.com.
See our LICENSE for the full terms of use for this data.
This license is co-extensive with the Creative Commons Attribution-NonCommercial 4.0 International license, and licensees should refer to that license (CC BY-NC) if they have questions about the scope of the license.
Contact Us
If you have questions about the data or licensing conditions, please contact us at:
covid-data@nytimes.com
Contributors
Mitch Smith, Karen Yourish, Sarah Almukhtar, Keith Collins, Danielle Ivory and Amy Harmon have been leading our U.S. data collection efforts.
Data has also been compiled by Jordan Allen, Jeff Arnold, Aliza Aufrichtig, Mike Baker, Robin Berjon, Matthew Bloch, Nicholas Bogel-Burroughs, Maddie Burakoff, Christopher Calabrese, Andrew Chavez, Robert Chiarito, Carmen Cincotti, Alastair Coote, Matt Craig, John Eligon, Tiff Fehr, Andrew Fischer, Matt Furber, Rich Harris, Lauryn Higgins, Jake Holland, Will Houp, Jon Huang, Danya Issawi, Jacob LaGesse, Hugh Mandeville, Patricia Mazzei, Allison McCann, Jesse McKinley, Miles McKinley, Sarah Mervosh, Andrea Michelson, Blacki Migliozzi, Steven Moity, Richard A. Oppel Jr., Jugal K. Patel, Nina Pavlich, Azi Paybarah, Sean Plambeck, Carrie Price, Scott Reinhard, Thomas Rivas, James G. Robinson, Michael Robles, Alison Saldanha, Alex Schwartz, Libby Seline, Shelly Seroussi, Rachel Shorey, Anjali Singhvi, Charlie Smart, Ben Smithgall, Steven Speicher, Michael Strickland, Albert Sun, Thu Trinh, Tracey Tully, Maura Turcotte, Bella Virgilio, Miles Watkins, Phil Wells, Jeremy White, Josh Williams, Jin Wu and Yanxing Yang."	805	7474	60	imoore	us-covid19-dataset-live-hourlydaily-updates
189	189	netflix_titlesss.csv		[]		0	1	0	sridharan1819	netflix-titlessscsv
190	190	COVID-19 Deaths Dataset	COVID-19 deaths statistics around the world	['health', 'covid19']	"This is a daily updated dataset of COVID-19 deaths around the world. The dataset contains data of 45 countries. This data was collected from
The New York Times
The Economist
us-counties.csv contains data of the daily number of new cases and deaths, the seven-day rolling average and the seven-day rolling average per 100,000 residents of US at county level. The average reported is the seven day trailing average i.e. average of the day reported and six days prior.
all_weekly_excess_deaths.csv collates detailed weekly breakdowns from official sources around the world.
Image credits: Unsplash - schluditsch
Let's pray for the ones who lost their lives fighting the battle and for the ones who risk their lives against this virus 🙏"	1862	11259	46	dhruvildave	covid19-deaths-dataset
191	191	NIFTY ALL STOCK dataset, daily update	NIFTY 50, NIFTY BANK, NIFTY 500, RELIANCE, ZOMATO all equity dataset	['time series analysis', 'xgboost', 'lstm', 'tabular data', 'investing']	"Content
This dataset is a playground for  technical analysis. It is said that 30% of traffic on stocks is already generated by machines, can trading be fully automated? If not, there is still a lot to learn from historical data. .
Context
https://www.kaggle.com/akshaypawar7/nifty-50-interactive-dashboard
Dataset consists of following files:
500 files of equity stock historical candle data
1_bhavcopy.csv =&amp;gt;
 Daily prices of all NSE equity stocks . Most of data spans from 2020 to the end today, for companies new on stock market date range is shorter.
1_meta.csv =&amp;gt;
    General description of each company with division on index and industry,
  as  shown in  below chart.
Inspiration
Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return."	22	128	8	akshaypawar7	nifty-dataset
192	192	vtb-data-fusion-2022		[]	"Content
transactions.csv: Список транзакций
clickstream.csv: Кликстрим
train_matching.csv: Целевая переменная — табличный файл с точным соответствием между id клиентов в данных транзакций и кликстримов. Разметка представлена не для всех клиентов, присутствующих в данных транзакций и кликстрима
mcc_codes.csv: Расшифровка MCC кодов транзакций
click_categories.csv: Расшифровка кодов категорий кликстрима
currency_rk.csv: Расшифровка кодов валют
train.csv: Табличный файл с целевой переменной: меткой наличия у клиентов ВТБ высшего образования — 0 и 1. Разметка представлена не для всех клиентов
sample_submission.csv: Пример тестового решения с случайным угадыванием целевой переменной"	0	4	0	konstantinalbul	vtbdatafusion2022
193	193	pix2pix_torch_0_15		[]		0	5	0	pezhmansamadi	pix2pix-torch-0-15
194	194	Happywhale Train TFRecords 0		[]		0	1	0	mmelahi	happywhale-train-tfrecords-0
195	195	Happywhale Train TFRecords 1		[]		0	0	0	mmelahi	happywhale-train-tfrecords-1
196	196	Happywhale Train TFRecords 2		[]		0	0	0	mmelahi	happywhale-train-tfrecords-2
197	197	Happywhale Train TFRecords 3		[]		0	0	0	mmelahi	happywhale-train-tfrecords-3
198	198	mydta_		[]		0	0	0	dilawar4122	mydta
199	199	netflix_titless.csv		[]		0	0	0	sridharan1819	netflix-titlesscsv
200	200	S&P 500 Stocks (daily updated)	Stock and company data on all members of the popular financial index.	['business', 'finance', 'banking', 'time series analysis', 'investing']	"About this dataset
&gt; The Standard and Poor's 500 or S&P 500 is the most famous financial benchmark in the world. 
&gt; This stock market index tracks the performance of 500 large companies listed on stock exchanges in the United States. As of December 31, 2020, more than $5.4 trillion was invested in assets tied to the performance of this index.
&gt; Because the index includes multiple classes of stock of some constituent companies—for example, Alphabet's Class A (GOOGL) and Class C (GOOG)—there are actually 505 stocks in the gauge.
How to use this dataset
&gt; - Create a time series regression model to predict S&P value and/or stock prices.
- Explore the most the returns, components and volatility of the S&P 500 index.
- Identify high and low performance stocks among the list.
Highlighted Notebooks
&gt; - Your kernel can be featured here!
- More datasets
Acknowledgements
&gt; ### License
CC0: Public Domain
&gt; ### Splash banner
Stonks by unknown memer."	1169	7399	86	andrewmvd	sp-500-stocks
201	201	E-Commerce Products Dataset For Record Linkage	Turkish products information collected from Turkish E-commerce - Clustering	['business', 'data cleaning', 'clustering', 'text data', 'e-commerce services']	"-&gt; If you use Turkish_Ecommerce_Products_by_Gozukara_and_Ozel_2016 dataset, please cite: https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxab179/6425234
@article{10.1093/comjnl/bxab179,
    author = {Gözükara, Furkan and Özel, Selma Ayşe},
    title = ""{An Incremental Hierarchical Clustering Based System For Record Linkage In E-Commerce Domain}"",
    journal = {The Computer Journal},
    year = {2021},
    month = {11},
    abstract = ""{In this study, a novel record linkage system for E-commerce products is presented. Our system aims to cluster the same products that are crawled from different E-commerce websites into the same cluster. The proposed system achieves a very high success rate by combining both semi-supervised and unsupervised approaches. Unlike the previously proposed systems in the literature, neither a training set nor structured corpora are necessary. The core of the system is based on Hierarchical Agglomerative Clustering (HAC); however, the HAC algorithm is modified to be dynamic such that it can efficiently cluster a stream of incoming new data. Since the proposed system does not depend on any prior data, it can cluster new products. The system uses bag-of-words representation of the product titles, employs a single distance metric, exploits multiple domain-based attributes and does not depend on the characteristics of the natural language used in the product records. To our knowledge, there is no commonly used tool or technique to measure the quality of a clustering task. Therefore in this study, we use ELKI (Environment for Developing KDD-Applications Supported by Index-Structures), an open-source data mining software, for performance measurement of the clustering methods; and show how to use ELKI for this purpose. To evaluate our system, we collect our own dataset and make it publicly available to researchers who study E-commerce product clustering. Our proposed system achieves 96.25\% F-Measure according to our experimental analysis. The other state-of-the-art clustering systems obtain the best 89.12\% F-Measure.}"",
    issn = {0010-4620},
    doi = {10.1093/comjnl/bxab179},
    url = {https://doi.org/10.1093/comjnl/bxab179},
    note = {bxab179},
    eprint = {https://academic.oup.com/comjnl/advance-article-pdf/doi/10.1093/comjnl/bxab179/41133297/bxab179.pdf},
}
-&gt; elki-bundle-0.7.2-SNAPSHOT.jar Is the ELKI bundle that we have compiled from the github source code of ELKI. The date of the source code is 6 June 2016. The compile command is as below:
-&gt;-&gt; mvn -DskipTests -Dmaven.javadoc.skip=true -P svg,bundle package
-&gt;-&gt; Github repository of ELKI: https://github.com/elki-project/elki
-&gt;-&gt; This bundle file is used for all of the experiments that are presented in the article
-&gt; Turkish_Ecommerce_Products_by_Gozukara_and_Ozel_2016 dataset is composed as below:
-&gt;-&gt; Top 50 E-commerce websites that operate in Turkey are crawled, and their attributes are extracted.
-&gt;-&gt; The crawling is made between 2015-01-13 15:12:46 ---- 2015-01-17 19:07:53 dates.
-&gt;-&gt; Then 250 product offers from Vatanbilgisayar are randomly selected.
-&gt;-&gt; Then the entire dataset is manually scanned to find which other products that are sold in different E-commerce websites are same as the selected ones.
-&gt;-&gt; Then each product is classified respectively.
-&gt;-&gt; This dataset contains these products along with their price (if available), title, categories (if available), free text description (if available), wrapped features (if available), crawled URL (the URL might have expired) attributes
-&gt; The dataset files are provided as used in the study. 
-&gt; ARFF files are generated with Raw Frequency of terms rather than used Weighting Schemes for All_Products and Only_Price_Having_Products. The reason is, we have tested these datasets with only our system and since our system does incremental clustering, even if provide TF-IDF weightings, they wouldn't be same as used in the article. More information provided in the article.
-&gt;-&gt; For Macro_Average_Datasets we provide both Raw frequency and TF-IDF scheme weightings as used in the experiments
-&gt; There are 3 main folders
-&gt; All_Products: This folder contains 1800 products.
-&gt;-&gt; This is the entire collection that is manually labeled. 
-&gt;-&gt; They are from 250 different classes.
-&gt; Only_Price_Having_Products: This folder contains all of the products that have the price feature set. 
-&gt;-&gt; The collection has 1721 products from 250 classes. 
-&gt;-&gt; This is the dataset that we have experimented.
-&gt; Macro_Average_Datasets: This folder contains 100 datasets that we have used to conduct more reliable experiments. 
-&gt;-&gt; Each dataset is composed of selecting 1000 different products from the price having products dataset and then randomly ordering them.
-&gt; When doing dynamic/incremental Hierarchical Clustering, the order of the input (records) does matter. 
-&gt;-&gt; Thus, we provide our datasets with the same order used in our experiments.
-&gt; The file names and their description inside each folder is presented below:
-&gt;-&gt; All files contains first the ID of the product and separated from the value by ';' character
* Preprocessed_Unigram_Word_Dataset: Contains preprocessed product titles. Each title is tokenized as unigram words
* Preprocessed_Unigram_Word_Plus_N2_Gram_Dataset: Contains preprocessed product titles. Each title is tokenized as unigram words + 2-gram character based transformation
* Product_Classes: Contains product_id and the product class value
* Product_Prices: Contains product_id and the product price value
* Original_Crawled_URLs: Contains product_id and crawled URL of the product
* Raw_Entire_Dataset: Contains raw titles of the products that are in that dataset
* Raw_Product_Categories: Contains raw categories of the products that are in that dataset. It may not be available. Each category is separated by '&gt;' character
* Raw_Product_Explanation: Contains raw extra information of the products that are in that dataset. It may not be available.
* Raw_Product_Wrapped_Features: Contains raw features of the products that are in that dataset. It may not be available.
** Each feature is '$$' and each feature label and the feature value is separated by '##'
-&gt; In our experiments, we have only utilized the title and the price attributes of the products
-&gt;-&gt; However, we still provide the other attributes as well for the researchers who may utilize them
-&gt; The list of the E-commerce websites and several statistics of the crawling are presented below
-&gt;-&gt; The final crawling is made at the date 2015-01-17 19:17:01
Root Site Id;Root Site Url;Total Crawled Urls Count;Product Pages Count;Total Crawl Success Times;Crawling Failed Times
1;http://www.vatanbilgisayar.com;9,650;7,891;9,670;158
2;http://www.teknosa.com/;11,582;10,935;11,580;193
3;http://www.istanbulbilisim.com.tr/;54,331;41,210;54,593;2,138
4;http://www.gold.com.tr/;27,752;15,484;34,092;1,267
5;http://www.hizlial.com/;222,073;191,604;222,367;5,204
6;http://www.bimeks.com.tr/;12,673;4,329;18,407;1,223
7;http://www.mediamarkt.com.tr/;12,543;10,704;12,562;480
8;http://www.kliksa.com/;112,019;89,059;112,569;2,299
9;http://www.webdenal.com/;73,939;55,574;82,686;1,211
10;http://www.ereyon.com.tr/;48,704;42,419;49,105;2,116
11;http://www.pcdepo.com;4,584;2,148;4,882;125
12;http://www.incehesap.com/;11,197;3,524;11,214;229
13;http://www.exa.com.tr/;3,366;2,086;3,360;152
14;http://www.alisverisyap.com/;400;236;400;2
15;http://www.eksenbilgisayar.com/;4,249;3,810;4,253;102
16;http://www.bizdeuygun.com/;36,725;33,414;73,456;69
17;http://www.bedavacim.com.tr/;13,679;10,523;13,681;629
18;http://www.netsiparis.com/;46,635;35,179;49,355;923
19;http://www.alalimbakalim.com/;4,116;1,836;4,118;82
20;http://www.buroteknik.com/;40,455;23,243;81,000;68
21;http://www.teknobiyotik.com/;3,915;2,897;3,910;113
22;http://www.netreyonum.com/;767;399;770;3
23;http://www.sanalreyonum.com/;27,477;14,588;27,455;1,704
24;http://www.deveyuku.com/;20,106;14,766;18,908;14,920
25;http://www.aynigunkargo.com/;17,368;14,776;16,812;6,977
26;http://www.novabilgisayar.com/;1,361;1,038;1,363;26
27;http://www.bizdehesapli.com/;13,683;9,983;13,683;282
28;http://www.herseyenucuz.com;482;106;721;14
29;http://www.webdensiparis.com;24,365;14,419;24,285;1,796
30;http://www.birnumaram.com;10,823;8,944;10,825;233
31;http://www.kapiteslim.com;5,475;4,221;5,475;136
32;http://www.dijitall.com;12,754;8,191;9,640;37,356
33;http://www.bilisimport.com;14,932;12,538;14,941;436
34;http://www.deposhop.com;22,377;17,911;23,461;22,004
35;http://www.telepanya.com/;3,160;2,068;3,172;750
36;http://www.tvt.com.tr;877;293;877;15
37;http://www.extraucuzluk.com;11,236;8,361;11,365;352
38;http://www.ucuzlukcu.com;10,109;9,118;10,108;302
39;https://www.yuppigo.com;19,576;10,443;19,584;389
40;https://www.tuslagelsin.com;3,875;1,174;7,697;248
41;http://www.teknolojiankara.com;3,189;2,838;3,189;128
42;http://www.hemensepet.com;15,110;7,538;15,110;444
43;http://www.websibir.com;300;202;600;0
44;http://www.aksarayiletisim.com;119;89;239;0
45;http://www.enhesapliyiz.com;1,217;469;619;7,189
46;http://www.nasabilgisayar.com;1,891;1,395;1,891;205
47;http://www.trdukkan.com;7,094;3,609;7,092;163
48;http://www.worldteknoloji.com;4,561;4,197;4,561;173
49;http://www.jettekno.com;1,863;1,703;1,865;116
50;http://www.escmarket.com;560;404;560;92"	33	1260	11	furkangozukara	ecommerce-products-dataset-for-record-linkage
202	202	GSDMM: Short text clustering	Gibbs sampling algorithm for a Dirichlet Mixture Model	['nlp']	From https://github.com/rwalk/gsdmm	43	2184	5	ptfrwrd	gsdmm-short-text-clustering
203	203	music-library		['music']	Personal music library	0	6	0	codyikaika	music-lib
204	204	COVID-19 data from John Hopkins University	Updated daily at 6am UTC in both raw and convenient form	['universities and colleges', 'covid19']	"This is a daily updating version of COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU). The data updates every day at 6am UTC, which updates just after the raw JHU data typically updates. 
I'm making it available in both a raw form (files with the prefix RAW) and convenient form (files prefixed with CONVENIENT). 
The data covers:
- confirmed cases and deaths on a country level
- confirmed cases and deaths by US county
- some metadata that's available in the raw JHU data
The RAW version is exactly as it's distributed in the original dataset.
The CONVENIENT version is aiming to be easier to analyze. The data is organized by column rather than by row. The metadata is stripped out into a separate file. And it converted to daily change rather than cumulative totals. 
If you find any issues in the data, then you can share them in this discussion thread. I will attempt to address the most upvoted issues.
If you have any requests for changing or enriching this data, please add them on this discussion thread. Again, I will attempt to address the most upvoted requests. 
I have a notebook that updates just after each data dump updates, giving a brief overview of the latest data. It's also a useful reference if you want to see how to read the CONVENIENT data into a pandas DataFrame."	15745	83689	349	antgoldbloom	covid19-data-from-john-hopkins-university
205	205	Indonesia's Trending YouTube Video Statistics	Daily Updated Trending Video Statistics in Indonesia	['religion and belief systems', 'data analytics', 'tabular data', 'video data', 'social networks', 'pandas']	"Context
YouTube provide trending videos to help viewer's see what's happening on YouTube and in the world. To do this, some measures like view count, how quickly the video is generating views (hotness), views origin (including outside YouTube), the age of the videos, etc., are to be accounted.
Trending on YouTube isn't personalized and hence display the same list of videos to users in a country. The list of trending videos is updated roughly every 15 minutes in which each update, videos may move up, down, or stay in the same position in the list.
Content
This dataset is inspired by this dataset.
This dataset only contains Indonesia's trending youtube videos which are (and will be) updated daily or twice in a day. Hence, it includes not only the trending date but including the trending time. Each update may consist of 200 videos in the list.
category.json
This file consists of the category identifier number for Indonesia.
trending.csv
The dataset includes some features like video title, description, video publish time, tags, views, dislikes, etc. In general, those features are extracted from more broad properties below:
snippet, which contains basic detail of videos
contentDetails, which contain more detailed video information
statistics, which contains statistics of the videos
&gt; Repo to generate the data is available here
Acknowledgements
The dataset is extracted using the YouTube API.
Heavily inspired by the work of @rsrishav and @datasnaek with similar dataset mentioned earlier.
Inspiration
Possible use cases or tasks using this dataset include, but not limited to:
Exploratory Data Analysis
Classification task, such as sentiment analysis, category, etc.
Any machine learning downstream tasks"	543	6165	38	syahrulhamdani	indonesias-trending-youtube-video-statistics
206	206	investment		['investing']		0	2	0	pruthvinadepalli	investment
207	207	Train Validation Splits New Preprocess		[]		1	5	0	fereshtej	train-validation-splits-new-preprocess
208	208	Latest COVID-19 India Statewise Vaccine Data	Covid-19 statewise vaccine data as on February 13, 2022	['india', 'public health', 'beginner', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
Covid-19 Vaccine data from all the states and union territories of India as on February 13, 2022.
Attribute Information
State/UTs - Names of states and union territories of India
Total Vaccination Doses- Total number of vaccine doses given
Dose 1 - Number of first dose of vaccine given
Dose 2 - Number of second dose of vaccine given
Population - Population of the state/UT
Source
Vaccine Data : https://www.mygov.in/covid-19
Population Data : https://www.indiacensus.net/
Other Updated Covid19 Datasets
https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting  👍
Thank You"	1185	5019	82	anandhuh	covid19-india-statewise-vaccine-data
209	209	Train Validation Splits New Preprocess		[]		1	5	0	fereshtej	train-validation-splits-new-preprocess
210	210	Latest COVID-19 India Statewise Vaccine Data	Covid-19 statewise vaccine data as on February 13, 2022	['india', 'public health', 'beginner', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
Covid-19 Vaccine data from all the states and union territories of India as on February 13, 2022.
Attribute Information
State/UTs - Names of states and union territories of India
Total Vaccination Doses- Total number of vaccine doses given
Dose 1 - Number of first dose of vaccine given
Dose 2 - Number of second dose of vaccine given
Population - Population of the state/UT
Source
Vaccine Data : https://www.mygov.in/covid-19
Population Data : https://www.indiacensus.net/
Other Updated Covid19 Datasets
https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting  👍
Thank You"	1185	5019	82	anandhuh	covid19-india-statewise-vaccine-data
211	211	Github Rpository Data	Top 900 Repos from Top 30 Topics	['exploratory data analysis']		0	12	1	himanshunayal	github-rpository-data
212	212	malaysia-map-data	Malaysia OpenStreetMap Map Data Queried in Overpass Turbo via Overpass API	['geospatial analysis', 'json']		0	118	2	jerrychong25	map-data
213	213	Bike-share dataset	Bike share data by DIVVY	['retail and shopping']	"Context
During the Google Data Analytics Specialization course on Coursera, I had to do case study to apply all the skills that I have learned during the course. I decided to do it using this data on bike sharing. I was provided with a case study and this was the scenario:
You are a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director
of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore,
your team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights,
your team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives
must approve your recommendations, so they must be backed up with compelling data insights and professional data
visualizations. 
Note: Cyclistic is a fictional company. For the purpose of this case study, the datasets are appropriate and will enable you to answer the business questions.
Content
The dataset contains data from the company that operates bike sharing services in Chicago city. The data below is for the year of 2021. Following are the columns in the dataset and what they represent:
- ride_id : the unique id to refer to each trip
- rideable_type : the type of the bike used for the trip
- started_at : date-time for when the trip started
- ended_at : date-time for when the trip ended
- start_station_name : name of the station from where the trip started
- start_station_id : unique id of the station from where the trip started
- end_station_name : name of the station where the trip ended
- end_station_id : unique id of the station where the trip ended
- start_lat : latitude of the start station
- start_lng : longitude of the start station
- end_lat : latitude of the end station
- end_lng : longitude of the end station
- member_casual : member denotes the users who have subscribed to annual membership, and casual denotes the users who haven't 
Acknowledgements
The data was made available by Motivate International Inc. under this license. Credits to Google Data Analytics team too, for sharing this information.
Inspiration
Some of the questions that you can answer using this dataset:
- How do annual members and casual riders use Cyclistic bikes differently?
- Why would casual riders buy Cyclistic annual memberships?
-  How can Cyclistic use digital media to influence casual riders to become members?"	4	112	1	nabajithazowary95	bikeshare-data
214	214	How Does a Bike-Share Navigate Speedy Success?	Data more than 7 Million rows for the years of 2018 and 2019	['data cleaning', 'data visualization', 'data analytics', 'tabular data', 'sql']	"Cyclistic is a bike-share program that features more than 5,800 bicycles and 600 docking stations. The director of marketing of the company has set a clear goal, to convert casual riders into annual members, which will make the company earn more profits. In order to do that the analyst team needs to better understand how annual and casual riders differ, why casual riders would buy a membership, and how digital media could affect the marketing tactics. So the marketing director wants to guide the future program with the following 3 question:
How do annual members and casual riders use Cyclistic bikes differently?
Why would casual riders buy Cyclistic annual memberships?
How can Cyclistic use digital media to influence casual riders to become members?
In this case study i will attempt to answer the first question.
Tools Used:
1. SQLite, PowerBi and Spreadsheets and the analysis in R Markdown
2. R Language
The case study is done twice, first with SQLite and PowerBi, and the second in R Language. Both analysis get us graphs of similar conclusions.
Note: only 2 years are chosen for this case study (2018-2019) since the whole data is very big. Any year will lead to the same conclusions and recommendations
Data Source: https://divvy-tripdata.s3.amazonaws.com/index.html
Data License Agreement: https://www.divvybikes.com/data-license-agreement"	29	1222	5	sergegeukjian	how-does-a-bikeshare-navigate-speedy-success
215	215	Dataset-Jamumas-Readme	Repositories Project Source on Github	['software']		0	6	0	mayakarya	dataset-readme
216	216	Gujarati Spoken Digits Dataset	Wav files for digits 0 through 9 in Gujarati language from regions of Gujarat	['india', 'beginner', 'intermediate', 'classification', 'audio data']	"Context
This dataset is for the enthusiasts of audio and speech tasks like me. It helps beginners to try and understand the patterns of spoken digits in Gujarati language. This dataset is collected from different regions of state of Gujarat in India.
Content
We introduce a new database of isolated Gujarati digits recordings with the goal of supporting research on speech recognition systems. This database contains genuine recordings of digits spoken by various users of 5 different regions of Gujarat. Specifically, the database contains recordings in a variety of environmental conditions with different forms of background noise. It's a simple audio/speech dataset consisting of recordings of spoken digits in wav files at 44kHz (Nyquist frequency). The recordings are trimmed so that they have near minimal silence at the beginnings and ends. 
Github Link : https://github.com/Nikunj1729/free-spoken-gujarati-digit-dataset
Details
- 20 speakers
- 1940 recordings
- Gujarati pronunciations
- 5 Regions - Central Zone, North Zone, South Zone, Saurashtra, Kutch Region
Naming convention
Files are named in the following format:
{Region_Number}{Speaker_Number}{Trial_Number}{Digit_Number}.wav
Example: R1S1T1D1.wav. Here R1 suggests region 1, S1 suggests speaker 1 from that region, T1 suggests trial 1, and D1 suggests Digit 1.
Acknowledgements
This dataset was collected by @@nikunjdalsaniya and @@sapanmankad.
For more details,
Kindly refer the following article : https://link.springer.com/chapter/10.1007/978-981-15-4828-4_18
If you use this dataset for any of your research work, kindly cite this paper as:
Development of a Novel Database in Gujarati Language for Spoken Digits Classification (Dalsaniya N., Mankad S.H., Garg S., Shrivastava D. (2020)). In: Thampi S. et al. (eds) Advances in Signal Processing and Intelligent Recognition Systems. SIRS 2019. Communications in Computer and Information Science, vol 1209. Springer, Singapore
Inspiration
We would like the interested from community to use this dataset and help in improving the audio and speech research field in terms of native regional languages as well. Some tasks which may be done on this dataset are:
Spoken Digits classification (obviously)
Speaker Recognition
Detection of speaker's region (i.e. location) from the speech
Gender Recognition
Other exciting stuff !!"	0	23	2	sapanmankad	gujarati-spoken-digits-dataset
217	217	SHEEP LIVESTOCK	livestock auction of sheeps	[]		7	973	0	yunussalman	sheep-livestock
218	218	Zimbabwe Stock Exchange Daily Price Sheets	ZSE Daily Price Sheets	['business', 'finance', 'banking', 'economics']	"Context
I have realised there is not enough datasets for african stock exchanges when i was doing my thesis, well i decided to create one for public consumption and data analysis.
Content
The dataset contains daily pricesheets from the Zimbabwe Stock Exchange Website. The data has row and columns of price highs and lows etc..
Acknowledgements
All information is from the ZSE"	76	1450	4	bevennyamande	zimbabwe-stock-exchange-daily-price-sheets
219	219	diabetes		['diabetes']		0	3	0	gerinsenapratama	diabetes
220	220	sleep-apnea		[]		0	16	0	rahuls123	sleepapnea
221	221	weights		['exercise']		0	14	0	donnyle	weights
222	222	GP_FINAL_DATA_5_REP		[]		0	5	0	manarmoh	gp-final-data-5-rep
223	223	Yolor_weights		[]		0	6	0	zedalaas	yolor-weights
224	224	test.txt		[]		1	21	0	hanlin12138	testtxt
225	225	LOAN_DATASET	LOAN_DATA WITH 40 COLUMNS IT CONTAINS LOTS OF EDA 	[]		0	3	0	karthikeyankd	loan-dataset
226	226	script		[]		0	8	0	limjunhao	script
227	227	Gold bitcoin coin on background of exchange chart	Pexels photo from https://pexels.com/photo/7788007/	['currencies and foreign exchange']		5	609	0	wakefulcloud	pexels-photo-7788007
228	228	IBM Data Science Capstone 	IBM Data Science Capstone 	['business']		0	35	0	crysllobo	ibm-data-science-capstone
229	229	Fish and Overfishing	How the Global Fish production is done and managed throught the years	['earth and nature', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	"Global production of fish and seafood has quadrupled over the past 50 years. Not only has the world population more than doubled over this period, the average person now eats almost twice as much seafood as half a century ago.
This has increased pressure on fish stocks across the world. Globally, the share of fish stocks which are overexploited – meaning we catch them faster than they can reproduce to sustain population levels – has more than doubled since the 1980s and this means that current levels of wild fish catch are unsustainable.
One innovation has helped to alleviate some of the pressure on wild fish catch: aquaculture, the practice of fish and seafood farming. The distinction between farmed fish and wild catch is similar to the difference between raising livestock rather than hunting wild animals. Except that for land-based animals, farming is many thousand years old while it was very uncommon for seafood until just over 50 years ago.
In the visualizations and tables we see:
- Captured and farmed (production) levels per year and per country or region
- Consumption levels throughout the world for the past 50 years
- Levels of sustainable vs overexploited fish
- Global fishery types and their production levels
- Types of fish produced per country"	11	147	0	sergegeukjian	fish-and-overfishing
230	230	Pergerakan Saham di Bursa Efek Indonesia	Pergerakan saham-saham di Bursa Efek Indonesia	['economics', 'beginner', 'time series analysis']	"Konten
Dataset ini merupakan data pergerakan harga saham tiap harinya di Bursa Efek Indonesia. Seluruh informasi emiten diambil dengan pergerakannya sejak 29 Juli 2019. Data akan terus diupdate tiap harinya secara otomatis beberapa jam setelah bursa pada hari tersebut tutup.
Kepemilikan Data
Seluruh data yang berada di sini merupakan milik PT Bursa Efek Indonesia. Sesuai dengan ketentuan PT Bursa Efek Indonesia, data ini tidak boleh digunakan untuk kepentingan komersial. Kreator dataset tidak bertanggung jawab apabila pengguna dataset tidak mematuhi peraturan ini dan tidak dapat dituntut secara hukum.
Frekuensi Update
Berhubung Kaggle API tidak support untuk upload folder maka saya tidak dapat mengupload data secara otomatis ke Kaggle setiap harinya. Saya harus secara manual mengklik ""Update"" di dataset saya agar Kaggle memfetch otomatis data dari Github atau menggunakan auto fetch yang paling cepat hanya seminggu sekali.
Oleh karena itu, untuk Kaggle, saya memutuskan untuk menggunakan fitur auto fetch tersebut. Untuk tetap dapat menggunakan dataset paling terbaru, silakan kunjungi projek ini di Github saya."	818	6968	35	tiwill	saham-idx
231	231	Querido Diário Census	Brazilian government gazette collaborative census	['brazil', 'government', 'politics', 'tabular data']		25	990	15	bcbernardo	censusqd2020
232	232	Real-time Covid 19 Data 	Click on Update button to get fresh version of Data 	['research', 'public health', 'health', 'data visualization', 'covid19']	"Coronavirus disease 2019 (COVID-19) time series listing confirmed cases, reported deaths and reported recoveries. Data is disaggregated by country (and sometimes subregion). Coronavirus disease (COVID-19) is caused by the Severe acute respiratory syndrome Coronavirus 2 (SARS-CoV-2) and has had a worldwide effect. On March 11 2020, the World Health Organization (WHO) declared it a pandemic, pointing to the over 118,000 cases of the Coronavirus illness in over 110 countries and territories around the world at the time.
This dataset includes time series data tracking the number of people affected by COVID-19 worldwide, including:
confirmed tested cases of Coronavirus infection
the number of people who have reportedly died while sick with Coronavirus
the number of people who have reportedly recovered from it"	2479	16800	95	gauravduttakiit	covid-19
233	233	Salesforce Stock Date - Latest and Updated	Salesforce Stock Performance from IPO till date	['business', 'science and technology', 'computer science', 'tabular data', 'investing']	"Salesforce - A Company Info from Wikipedia
&gt; Salesforce is an American cloud-based software company headquartered in San Francisco, California. It provides customer relationship management (CRM) service and also provides enterprise applications focused on customer service, marketing automation, analytics, and application development.
&gt; In 2021, Fortune magazine ranked Salesforce second on its list, ""100 Best Companies to Work For,"" based on an employee survey of satisfaction. They had previously placed sixth in 2020.
&gt; The company was founded on February 3, 1999 by former Oracle executive Marc Benioff, together with Parker Harris, Dave Moellenhoff, and Frank Dominguez as a software as a service (SaaS) company, and was launched publicly between September and November 1999.
&gt; In June 2004, the company had its initial public offering on the New York Stock Exchange under the stock symbol CRM and raised USD110 million. Early investors include Larry Ellison, Magdalena Yesil, Halsey Minor, Stewart Henderson, Mark Iscaro, and Igor Sill, a founding member of Geneva Venture Partners.
Let us analyze the performance of this solid star!
Let us analyze and visualize"	148	1340	16	kalilurrahman	salesforce-stock-date-latest-and-updated
234	234	titanet-checkpoint		[]		1	5	0	anbn14	titanetcheckpoint
235	235	cbnetv2git	cbnetv2gitcbnetv2gitcbnetv2git	[]		0	23	0	itaynivtau	cbnetv2git
236	236	Campus Placement		['universities and colleges']		2	8	0	sabyasachimoitra	campus-placement
237	237	visdrone2000		[]		0	9	0	limjunhao	visdrone2000
238	238	MrBeast Youtube Stats & Thumbnails (Updated Daily)	Daily pull of the most recent MrBeast video stats	[]		12	181	7	robikscube	mrbeast-youtube-stats-daily
239	239	UK Power Networks Grid and Primary substations	List of Active Grid and Primary Sites and key characteristics.	['energy']		3	103	0	yiushingpang	uk-power-networks-grid-and-primary-substations
240	240	Practical Statistics for Data Scientists	Autors: Andrew Bruce, Peter C. Bruce, Peter Gedeck	[]		65	1061	0	bhavinmoriya	loan-csv
241	241	LogHub - Apache Log Data	A dataset of logs from Apache server instances	['business']		18	437	1	omduggineni	loghub-apache-log-data
242	242	spx_holdings_and_spx_closeprice	Data for 417 S&P 500 companies from 2000 to 2013	['north america', 'business', 'exploratory data analysis', 'data visualization', 'data analytics', 'investing']		7	513	1	ralarellanolopez	spx-holdings-and-spx-closeprice
243	243	Self Driving Car	Behavioural Cloning Complete Guide	['video games', 'earth and nature', 'robotics', 'artificial intelligence', 'computer science', 'programming', 'dnn', 'cnn', 'keras']	"SELF-DRIVING CAR USING UDACITY’S CAR SIMULATOR ENVIRONMENT AND TRAINED BY DEEP NEURAL NETWORKS COMPLETE GUIDE
Table of Contents
Introduction
Problem Definition
Solution Approach
Technologies Used
Convolutional Neural Networks (CNN)
Time-Distributed Layers
Udacity Simulator and Dataset
The Training Process
Augmentation and image pre-processing
Experimental configurations
Network architectures
Results
Value loss or Accuracy
Why We Use ELU Over RELU
The Connection Part
Files
Overview
References
Introduction
Self-drivi cars has become a trending subject with a significant improvement in the technologies in the last decade. The project purpose is to train a neural network to drive an autonomous car agent on the tracks of Udacity’s Car Simulator environment. Udacity has released the simulator as an open source software and  enthusiasts have hosted a competition (challenge) to teach a car how to drive using only camera images and deep learning. Driving a car in an autonomous manner requires learning to control steering angle, throttle and brakes. Behavioral cloning technique is used to mimic human driving behavior in the training mode on the track. That means a dataset is generated in the simulator by user driven car in training mode, and the deep neural network model then drives the car in autonomous mode. Ultimately, the car was able to run on Track 1 generalizing well. The project aims at reaching the same accuracy on real time data in the future.
Problem Definition
Udacity released an open source simulator for self-driving cars to depict a real-time environment. The challenge is to mimic the driving behavior of a human on the simulator with the help of a model trained by deep neural networks. The concept is called Behavioral Cloning, to mimic how a human drives. The simulator contains two tracks and two modes, namely, training mode and autonomous mode. The dataset is generated from the simulator by the user, driving the car in training mode. This dataset is also known as the “good” driving data. This is followed by testing on the track, seeing how the deep learning model performs after being trained by that user data.
Solution Approach
The problem is solved in the following steps: 
The simulator can be used to collect data by driving the car in the training mode using a joystick or keyboard, providing the so called “good-driving” behavior input data in form of a driving_log (.csv file) and a set of images. The simulator acts as a server and pipes these images and data log to the Python client. 
The client (Python program) is the machine learning model built using Deep Neural Networks. These models are developed on Keras (a high-level API over Tensorflow). Keras provides sequential models to build a linear stack of network layers. Such models are used in the project to train over the datasets as the second step. Detailed description of CNN models experimented and used can be referred to in the chapter on network architectures. 
Once the model is trained, it provides steering angles and throttle to drive in an autonomous mode to the server (simulator). 
These modules, or inputs, are piped back to the server and are used to drive the car autonomously in the simulator and keep it from falling off the track.
Technologies Used
Technologies that are used in the implementation of this project and the motivation behind using these are described in this section.
TensorFlow: This an open-source library for dataflow programming. It is widely used for machine learning applications. It is also used as both a math library and for large computation. For this project Keras, a high-level API that uses TensorFlow as the backend is used. Keras facilitate in building the models easily as it more user friendly. 
Different libraries are available in Python that helps in machine learning projects. Several of those libraries have improved the performance of this project. Few of them are mentioned in this section. First, “Numpy” that provides with high-level math function collection to support multi-dimensional metrices and arrays. This is used for faster computations over the weights (gradients) in neural networks. Second, “scikit-learn” is a machine learning library for Python which features different algorithms and Machine Learning function packages. Another one is OpenCV (Open Source Computer Vision Library) which is designed for computational efficiency with focus on real-time applications. In this project, OpenCV is used for image preprocessing and augmentation techniques. 
The project makes use of Conda Environment which is an open source distribution for Python which simplifies package management and deployment. It is best for large scale data processing. The machine on which this project was built, is a personal computer. 
Convolutional Neural Networks (CNN)
CNN is a type of feed-forward neural network computing system that can be used to learn from input data. Learning is accomplished by determining a set of weights or filter values that allow the network to model the behavior according to the training data. The desired output and the output generated by CNN initialized with random weights will be different. This difference (generated error) is backpropagated through the layers of CNN to adjust the weights of the neurons, which in turn reduces the error and allows us produce output closer to the desired one. 
CNN is good at capturing hierarchical and spatial data from images. It utilizes filters that look at regions of an input image with a defined window size and map it to some output. It then slides the window by some defined stride to other regions, covering the whole image. Each convolution filter layer thus captures the properties of this input image hierarchically in a series of subsequent layers, capturing the details like lines in image, then shapes, then whole objects in later layers. CNN can be a good fit to feed the images of a dataset and classify them into their respective classes. 
Time-Distributed Layers
Another type of layers sometimes used in deep learning networks is a Time- distributed layer. Time-Distributed layers are provided in Keras as wrapper layers. Every temporal slice of an input is applied with this wrapper layer. The requirement for input is that to be at least three-dimensional, first index can be considered as temporal dimension. These Time-Distributed can be applied to a dense layer to each of the timesteps, independently or even used with Convolutional Layers. The way they can be written is also simple in Keras as shown in Figure 1 and Figure 2.
Fig. 1: TimeDistributed Dense layer
Fig. 2: TimeDistributed Convolution layer
Udacity Simulator and Dataset
We will first download the simulator to start our behavioural training process. Udacity has built a simulator for self-driving cars and made it open source for the enthusiasts, so they can work on something close to a real-time environment. It is built on Unity, the video game development platform. The simulator consists of a configurable resolution and controls setting and is very user friendly. The graphics and input configurations can be changed according to user preference and machine configuration as shown in Figure 3. The user pushes the “Play!” button to enter the simulator user interface. You can enter the Controls tab to explore the keyboard controls, quite similar to a racing game which can be seen in Figure 4. 
Fig. 3: Configuration screen                                                                    
Fig. 4: Controls Configuration
The first actual screen of the simulator can be seen in Figure 5 and its components are discussed below. The simulator involves two tracks. One of them can be considered as simple and another one as complex that can be evident in the screenshots attached in Figure 6 and Figure 7. The word “simple” here just means that it has fewer curvy tracks and is easier to drive on, refer Figure 6. The “complex” track has steep elevations, sharp turns, shadowed environment, and is tough to drive on, even by a user doing it manually. Please refer Figure 6. There are two modes for driving the car in the simulator: (1) Training mode and (2) Autonomous mode. The training mode gives you the option of recording your run and capturing the training dataset. The small red sign at the top right of the screen in the Figure 6 and 7 depicts the car is being driven in training mode. The autonomous mode can be used to test the models to see if it can drive on the track without human intervention. Also, if you try to press the controls to get the car back on track, it will immediately notify you that it shifted to manual controls. The mode screenshot can be as seen in Figure 8. Once we have mastered how the car driven controls in simulator using keyboard keys, then we get started with record button to collect data. We will save the data from it in a specified folder as you can see below.
<img alt=""7"" src=""https://user-images.githubusercontent.com/91852182/147298975-e05dc738-2fb7-4dca-a28d-9756285d94cc.png"">
The simulator’s feature to create your own dataset of images makes it easy to work on the problem. Some reasons why this feature is useful are as follows: 
The simulator has built the driving features in such a way that it simulates that there are three cameras on the car. The three cameras are in the center, right and left on the front of the car, which captures continuously when we record in the training mode. 
The stream of images is captured, and we can set the location on the disk for saving the data after pushing the record button. The image set are labelled in a sophisticated manner with a prefix of center, left, or right indicating from which camera the image has been captured. 
Along with the image dataset, it also generates a datalog.csv file. This file contains the image paths with corresponding steering angle, throttle, brakes, and speed of the car at that instance. 
A few images from the dataset are shown below .
<img alt=""8"" src=""https://user-images.githubusercontent.com/91852182/147299066-17bb3db0-5f1c-44ff-ab7e-41786c243d4f.png"">
Pic wich i have in folder IMG only for visualization your data will be more than 5000 pic.
A sample of driving_log.csv file is shown in Figure 9.
Column 1, 2, 3:  contains paths to the dataset images of center, right and left respectively Column 4: contains the steering angle 
Column 4:  value as 0 depicts straight, positive value is right turn and negative value is left turn. 
Column 5:  contains the throttle or acceleration at that instance 
Column 6:  contains the brakes or deceleration at that instance 
Column 7:  contains the speed of the vehicle 
Fig 9 . driving_log.csv
The Training Process
For the process of getting the self driving car working, we have to upload the images that we recorded using the simulator. First of all, we will open GitHub Desktop.If we do not have an account, we will create a new one. With that, we will create a new repository.
We will be using Google Colab for doing the training process or Kaggle.
We will open a new python3 notebook and get started. Next, we will git clone the repo.
!git clone https://github.com/Asikpalysik/Self-Driving-Car.git
We will now import all the libraries needed for training process. It will use Tensorflow backend and keras at frontend.
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from imgaug import augmenters as iaa
import cv2
import pandas as pd
import ntpath
import random
import warnings
warnings.filterwarnings(""ignore"")
We wil use datadir as the name given to the folder itself and take the parameters itself. Using head, we will show the first five values for the CSV on the desired format
dir = ""/Users/asik/Desktop/SelfDrivingCar""
columns = [""center"", ""left"", ""right"", ""steering"", ""throttle"", ""reverse"", ""speed""]
data = pd.read_csv(os.path.join(dir, ""driving_log.csv""), names=columns)
pd.set_option(""display.max_colwidth"", -1)
data.head()
As this is picking up the entire path from the local machine, we need to use ntpath function to get the network path assigned. We will declare a name path_leaf and assign accordingly.
def pathleaf(path):
    head, tail = ntpath.split(path)
    return tail
data[""center""] = data[""center""].apply(pathleaf)
data[""left""] = data[""left""].apply(pathleaf)
data[""right""] = data[""right""].apply(pathleaf)
data.head()
We will bin the number of values where the number will be equal to 25 (odd number aimed to get center distribution). We will see the histogram using the np.histogram option on data frame ‘steering’, we will divide it to the number of bins. We keep samples at 400 and then we draw a line. We see the data is centered along the middle that is 0.
num_bins = 25
samples_per_bin = 400
hist, bins = np.histogram(data[""steering""], num_bins)
print(bins)
Plot on it
center = (bins[:-1] + bins[1:]) * 0.5
plt.bar(center, hist, width=0.05)
plt.plot(
    (np.min(data[""steering""]), np.max(data[""steering""])),
    (samples_per_bin, samples_per_bin),
)
<img alt=""14"" src=""https://user-images.githubusercontent.com/91852182/147301498-cb9aac07-837e-4f66-8825-36608843dab4.png"">
print(""Total Data:"", len(data))
-&gt;&gt;Total Data: 1795
We wil specify a variable remove_list.We will specify samples we want to remove using looping construct through every single bin we will iterate through all the steering data. We will shuffle the data and romve some from it as it is now uniformly structured after shuffling.The output will be the distribution of steering angle that are much more uniform. There are significant amount of left steering angle and right steering angle eliminating the bias to drive straight all the time.
remove_list = []
for j in range(num_bins):
    list_ = []
    for i in range(len(data[""steering""])):
        if data[""steering""][i] &amp;gt;= bins[j] and data[""steering""][i] &amp;lt;= bins[j + 1]:
            list_.append(i)
    list_ = shuffle(list_)
    list_ = list_[samples_per_bin:]
    remove_list.extend(list_)
print(""Removed:"", len(remove_list))
-&gt;&gt; Removed: 945
data.drop(data.index[remove_list], inplace=True)
print(""Remaining:"", len(data))
-&amp;gt;&amp;gt;Remaining: 850
Plot on it
hist, _ = np.histogram(data[""steering""], (num_bins))
plt.bar(center, hist, width=0.05)
plt.plot(
    (np.min(data[""steering""]), np.max(data[""steering""])),
    (samples_per_bin, samples_per_bin),
)
<img alt=""15"" src=""https://user-images.githubusercontent.com/91852182/147301598-185b5e70-17f8-41db-81d3-50cfc5db3902.png"">
print(data.iloc1)
We will now load the image into array to manipulate them accordingly. We will define a function named locd_img_steering. We will have image path as empty list and steering as empty list and then loop through. We use iloc selector as data frame based on the specific index, we will use cut data for now.
def load_img_steering(datadir, df):
    image_path = []
    steering = []
    for i in range(len(data)):
        indexed_data = data.iloc[i]
        center, left, right = indexed_data[0], indexed_data1, indexed_data2
        image_path.append(os.path.join(datadir, center.strip()))
        steering.append(float(indexed_data3))
        image_path.append(os.path.join(datadir, left.strip()))
        steering.append(float(indexed_data3) + 0.15)
        image_path.append(os.path.join(datadir, right.strip()))
        steering.append(float(indexed_data3) - 0.15)
    image_paths = np.asarray(image_path)
    steerings = np.asarray(steering)
    return image_paths, steerings
We will be splitting the image path as well as storing arrays accordingly.
image_paths, steerings = load_img_steering(dir + ""/IMG"", data)
X_train, X_valid, y_train, y_valid = train_test_split(
    image_paths, steerings, test_size=0.2, random_state=6
)
print(""Training Samples: {}\nValid Samples: {}"".format(len(X_train), len(X_valid)))
-&gt;&gt;Training Samples: 2040 
-&gt;&gt;Valid Samples: 510
We will have the histograms now.
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].hist(y_train, bins=num_bins, width=0.05, color=""blue"")
axes[0].set_title(""Training set"")
axes1.hist(y_valid, bins=num_bins, width=0.05, color=""red"")
axes1.set_title(""Validation set"")
<img alt=""17"" src=""https://user-images.githubusercontent.com/91852182/147301799-0e3ca6a5-6f4e-41a9-b2c5-7bf5b1dad400.png"">
Augmentation and image pre-processing
The biggest challenge was generalizing the behavior of the car on Track_2 which it was never trained for. In a real-life situation, we can never train a self-driving car model for every track possible, as the data will be too huge to process. Also, it is not possible to gather the dataset for all the weather conditions and roads. Thus, there is a need to come up with an idea of generalizing the behavior on different tracks. This problem is solved using image preprocessing and augmentation techniques.
Zoom 
The images in the dataset have relevant features in the lower part where the road is visible. The external environment above a certain image portion will never be used to determine the output and thus can be cropped. Approximately, 30% of the top portion of the image is cut and passed in the training set. The snippet of code and transformation of an image after cropping and resizing it to original image can be seen in  below.
<img alt=""18"" src=""https://user-images.githubusercontent.com/91852182/147301992-fdc9d029-3a33-4f2f-8dc2-222067a07a0a.png"">
def zoom(image):
    zoom = iaa.Affine(scale=(1, 1.3))
    image = zoom.augment_image(image)
    return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
zoomed_image = zoom(original_image)
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title(""Original Image"")
axs1.imshow(zoomed_image)
axs1.set_title(""Zoomed Image"")
Flip (horizontal) 
The image is flipped horizontally (i.e. a mirror image of the original image is passed to the dataset). The motive behind this is that the model gets trained for similar kinds of turns on opposite sides too. This is important because Track 1 includes only left turns. The snippet of code and transformation of an image after flipping it can be seen in below. 
<img alt=""19"" src=""https://user-images.githubusercontent.com/91852182/147302076-3bd4311e-79b2-4750-84a7-c1061ffcdae3.png"">
def random_flip(image, steering_angle):
    image = cv2.flip(image, 1)
    steering_angle = -steering_angle
    return image, steering_angle
random_index = random.randint(0, 1000)
image = image_paths[random_index]
steering_angle = steerings[random_index]
original_image = mpimg.imread(image)
flipped_image, flipped_steering_angle = random_flip(original_image, steering_angle)
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title(""Original Image - "" + ""Steering Angle:"" + str(steering_angle))
axs1.imshow(flipped_image)
axs1.set_title(""Flipped Image - "" + ""Steering Angle:"" + str(flipped_steering_angle))
Shift (horizontal/vertical)
The image is shifted by a small amount, it is vertical shift and horizontal shift as below.
<img alt=""20"" src=""https://user-images.githubusercontent.com/91852182/147302200-34bae3a8-2236-42d7-819d-ea694a382a02.png"">
def pan(image):
    pan = iaa.Affine(translate_percent={""x"": (-0.1, 0.1), ""y"": (-0.1, 0.1)})
    image = pan.augment_image(image)
    return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
panned_image = pan(original_image)
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title(""Original Image"")
axs1.imshow(panned_image)
axs1.set_title(""Panned Image"")
Brightness 
To generalize to the weather conditions with bright sunny day or cloudy, lowlight conditions, the brightness augmentation can prove to be very useful. The code snippet and increase of brightness can be seen below. Similarly, I have randomly also lowered down the level of brightness for other conditions. 
<img alt=""21"" src=""https://user-images.githubusercontent.com/91852182/147302277-defccf7a-43f4-459c-a0db-536f01b77110.png"">
def random_brightness(image):
    brightness = iaa.Multiply((0.2, 1.2))
    image = brightness.augment_image(image)
    return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
brightness_altered_image = random_brightness(original_image)
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title(""Original Image"")
axs1.imshow(brightness_altered_image)
axs1.set_title(""Brightness altered image "")
To have a look what we have at this moment
```
def random_augment(image, steering_angle):
    image = mpimg.imread(image)
    if np.random.rand() &lt; 0.5:
        image = pan(image)
    if np.random.rand() &lt; 0.5:
        image = zoom(image)
    if np.random.rand() &lt; 0.5:
        image = random_brightness(image)
    if np.random.rand() &lt; 0.5:
        image, steering_angle = random_flip(image, steering_angle)
    return image, steering_angle
ncol = 2
nrow = 10
fig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))
fig.tight_layout()
for i in range(10):
    randnum = random.randint(0, len(image_paths) - 1)
    random_image = image_paths[randnum]
    random_steering = steerings[randnum]
    original_image = mpimg.imread(random_image)
    augmented_image, steering = random_augment(random_image, random_steering)
    axs[i][0].imshow(original_image)
    axs[i][0].set_title(""Original Image"")
axsi.imshow(augmented_image)
axsi.set_title(""Augmented Image"")
```
I continued by doing some image processing. I cropped the image to remove the unnecessary features, changes the images to YUV format, used gaussian blur, decreased the size for easier processing and normalized the values.
def img_preprocess(img):
    ## Crop image to remove unnecessary features
    img = img[60:135, :, :]
    ## Change to YUV image
    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    ## Gaussian blur
    img = cv2.GaussianBlur(img, (3, 3), 0)
    ## Decrease size for easier processing
    img = cv2.resize(img, (200, 66))
    ## Normalize values
    img = img / 255
    return img
To compare and visualize I plotted the original and the pre-processed image.
image = image_paths[100]
original_image = mpimg.imread(image)
preprocessed_image = img_preprocess(original_image)
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title(""Original Image"")
axs1.imshow(preprocessed_image)
axs1.set_title(""Preprocessed Image"")
<img alt=""25"" src=""https://user-images.githubusercontent.com/91852182/147302472-ac64e7d7-1ec5-403a-82ff-046258584145.png"">
```
def batch_generator(image_paths, steering_ang, batch_size, istraining):
    while True:
        batch_img = []
        batch_steering = []
    for i in range(batch_size):
        random_index = random.randint(0, len(image_paths) - 1)
    if istraining:
        im, steering = random_augment(
            image_paths[random_index], steering_ang[random_index]
        )

    else:
        im = mpimg.imread(image_paths[random_index])
        steering = steering_ang[random_index]

    im = img_preprocess(im)
    batch_img.append(im)
    batch_steering.append(steering)

yield (np.asarray(batch_img), np.asarray(batch_steering))

So far so good. Next, I converted all the images into numpy array.
x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))
x_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(x_train_gen[0])
axs[0].set_title(""Training Image"")
axs1.imshow(x_valid_gen[0])
axs1.set_title(""Validation Image"")
```
<img alt=""26"" src=""https://user-images.githubusercontent.com/91852182/147302514-54b40af4-baea-491e-ba7b-73e6d752872e.png"">
Experimental configurations
Configurations used to set up the models for training the Python Client to provide the Neural Network outputs that drive the car on the simulator. The tweaking of parameters and rigorous experiments were tried to reach the best combination. Though each of the models had their unique behaviors and differed in their performance with each tweak, the following combination of configuration can be considered as the optimal: 
The sequential models built on Keras with deep neural network layers are used to train the data. 
Models are only trained using the dataset from Track 1. 
80% of the dataset is used for training, 20% is used for testing. 
Epochs = 10, i.e. number of iterations or passes through the complete dataset. Experimented with larger number of epochs also, but the model tried to “overfit”. In other words, the model learns the details in the training data too well, while impacting the performance on new dataset. 
Batch-size = 100, i.e. number of image samples propagated through the network, like a subset of data as complete dataset is too big to be passed all at once.
Learning rate = 0.0001, i.e. how the coefficients of the weights or gradients change in the network. 
There are different combinations of Convolution layer, Time-Distributed layer, MaxPooling layer, Flatten, Dropout, Dense and so on, that can be used to implement the Neural Network models. 
Network architectures
The design of the network is based on the NVIDIA model, which has been used by NVIDIA for the end-to-end self driving test. As such, it is well suited for the project.It is a deep convolution network which works well with supervised image classification / regression problems. As the NVIDIA model is well documented, I was able to focus how to adjust the training images to produce the best result with some adjustments to the model to avoid overfitting and adding non-linearity to improve the prediction.
I've added the following adjustments to the model.
I used Lambda layer to normalized input images to avoid saturation and make gradients work better.
I've added an additional dropout layer to avoid overfitting after the convolution layers.
I've also included ELU for activation function for every layer except for the output layer to introduce non-linearity.
In the end, the model looks like as follows:
Image normalization
Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU
Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU
Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU
Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU
Drop out (0.5)
Fully connected: neurons: 100, activation: ELU
Fully connected: neurons: 50, activation: ELU
Fully connected: neurons: 10, activation: ELU
Fully connected: neurons: 1 (output)
As per the NVIDIA model, the convolution layers are meant to handle feature engineering and the fully connected layer for predicting the steering angle. However, as stated in the NVIDIA document, it is not clear where to draw such a clear distinction. Overall, the model is very functional to clone the given steering behavior. The below is a model structure output from the Keras which gives more details on the shapes and the number of parameters.
We will design our Model architecture. We have to classify the traffic signs too that's why we need to shift from Lenet 5 model to NVDIA model. With behavioural cloning, our dataset is much more complex then any dataset we have used. We are dealing with images that have (200,66) dimensions. Our current datset has 5386 images to train with but MNSIT has around 60,000 images to train with. Our behavioural cloning code has simply has to return appropriate steering angle which is a regression type example. For these things, we need a more advanced model which is provided by nvdia and known as nvdia model.
For defining the model architecture, we need to define the model object. Normalization state can be skipped as we have already normalized it. We will add the convolution layer. As compared to the model, we will organize accordingly. The Nvdia model uses 24 filters in the layer along with a kernel of size 5,5. We will introduce sub sampling. The function reflects to stride length of the kernel as it processes through an image, we have large images. Horizontal movement with 2 pixels at a time, similarly vertical movement to 2 pixels at a time. As this is the first layer, we have to define input shape of the model too i.e., (66,200,3) and the last function is an activation function that is “elu”.
Revisting the model, we see that our second layer has 36 filters with kernel size (5,5) same subsampling option with stride length of (2,2) and conclude this layer with activation ‘elu’.
According to Nvdia model, it shows we have 3 more layers in the convolutional neural network. With 48 filters, with 64 filters (3,3) kernel 64 filters (3,3) kernel Dimensions have been reduced significantly so for that we will remove subsampling from 4th and 5th layer.
Next we add a flatten layer. We will take the output array from previous convolution neural network to convert it into a one dimensional array so that it can be fed to fully connected layer to follow.
Our last convolution layer outputs an array shape of (1,18) by 64.
We end the architecture of Nvdia model with a dense layer containing a single output node which will output the predicted steering angle for our self driving car. Now we will use model.compile() to compile our architecture as this is a regression type example the metrics that we will be using will be mean squared error and optimize as Adam. We will be using relatively a low learning rate that it can help on accuracy. We will use dropout layer to avoid overfitting the data. Dropout Layer sets the input of random fraction of nodes to “0” during each update. During this, we will generate the training data as it is forced to use a variety of combination of nodes to learn from the same data. We will have to separate the convolution layer from fully connected layer with a factor of 0.5 is added so it converts 50 percent of the input to 0. We Will define the model by calling the nvdia model itself. Now we will have the model training process.To define training parameters, we will use model.fit(), we will import our training data X_Train, training data -&gt;y_train, we have less data on the datasets we will require more epochs to be effective. We will use validation data and then use Batch size.
def NvidiaModel():
  model = Sequential()
  model.add(Convolution2D(24,(5,5),strides=(2,2),input_shape=(66,200,3),activation=""elu""))
  model.add(Convolution2D(36,(5,5),strides=(2,2),activation=""elu""))
  model.add(Convolution2D(48,(5,5),strides=(2,2),activation=""elu"")) 
  model.add(Convolution2D(64,(3,3),activation=""elu"")) 
  model.add(Convolution2D(64,(3,3),activation=""elu""))
  model.add(Dropout(0.5))
  model.add(Flatten())
  model.add(Dense(100,activation=""elu""))
  model.add(Dropout(0.5))
  model.add(Dense(50,activation=""elu""))
  model.add(Dropout(0.5))
  model.add(Dense(10,activation=""elu""))
  model.add(Dropout(0.5))
  model.add(Dense(1))
  model.compile(optimizer=Adam(lr=1e-3),loss=""mse"")
  return model
model = NvidiaModel()
print(model.summary())
Results
The following results were observed for described architectures. I had to come up with two different performance metrics. 
- Value loss or Accuracy (computed during training phase) 
- Generalization on Track 1 (drive performance)
Value loss or Accuracy
The first evaluation parameter considered here is “Loss” over each epoch of the training run. To calculate value loss over each epoch, Keras provides “val_loss”, which is the average loss after that epoch. The loss observed during the initial epochs at the beginning of training phase is high, but it falls gradually, and that is evident by the screenshots below which shows the run of Architecture in the training phase.
history = model.fit_generator(
    batch_generator(X_train, y_train, 100, 1),
    steps_per_epoch=300,
    epochs=10,
    validation_data=batch_generator(X_valid, y_valid, 100, 0),
    validation_steps=200,
    verbose=1,
    shuffle=1,
)
Why We Use ELU Over RELU
We can have dead relu this is when a node in neural network essentially dies and only feeds a value of zero to nodes which follows it. We will change from relu to elu. Elu function has always a chance to recover and fix it errors means it is in a process of learning and contributing to the model. We will plot the model and then save it accordingly in h5 format for a keras file.
plt.plot(history.history[""loss""])
plt.plot(history.history[""val_loss""])
plt.legend([""training"", ""validation""])
plt.title(""Loss"")
plt.xlabel(""Epoch"")
<img alt=""30"" src=""https://user-images.githubusercontent.com/91852182/147302963-abb40cb3-dd32-4991-8db0-f56e81fee23f.png"">
Save model
model.save('model.h5')
The Connection Part
This step is required to run the model in the simulated car. For implementing web service using python, we need to install flask. We will use Anaconda environment. Flask is a python micro framework that is used to build the web app. We will use Visual Studio Code.
We will open the folder where we kept the saved .h5 file, then again open a file but before that, we will install some dependencies. We will also create an anaoconda environment too for doing our work. Click Create Python 3.8.12. with my experience I need to mention that i had some problems with environments, the was a lot of conflicts so I will advise you just to follow my steps exactly the same.
After we created new env on Python 3.8.12 moving to VScode and opening it under SDC environment. So basically, we now will work with special environment on special Python version. Terminal need to be as well under same.
I will create a list conda list -e &amp;gt; requirements.txt requirements.txt for you to know what exactly I used there.  It important to use keras 2.4.3 and be careful with python-engineio=3.13.0 and lastly most important python-socketio=4.6.1(follow requirements.txt). As well you will see drive.py file without this code will not work you can simple copy paste it I will not go deep about this file. 
Now when all your requirements are installed, we are ready to run magic code in terminal. In your folder you will see something like I show below.
Just type python drive.py model.h5 in your Terminal.  wait few few second open Udacity Simulator and choose option Autonomous mode. That it, you will see something as I show below.
Files
The project contains the following files:
- SelfDrivingCar.py (script used to create the model and train the model)
- drive.py (script to drive the car - feel free to modify this file)
- driving_log.csv (csv file from simulator - data)
- IMG (folder) (folder with images from simulator - data)
- model.h5 (a trained Keras model)
- requirements.txt (important requirements for project)
- SelfDrivingCar.ipynb (full explanation in notebook)
- SelfDrivingCar.pdf (full explanation in PDF)
Overview
In this project, we use deep neural networks and convolutional neural networks to clone driving behavior. The model is trained, validated and tested using Keras. The model outputs a steering angle to an autonomous vehicle. The autonomous vehicle is provided as a simulator. Image data and steering angles are used to train a neural network and drive the simulation car autonomously around the track.
This project started with training the models and tweaking parameters to get the best performance on the track 
The use of CNN for getting the spatial features and RNN for the temporal features in the image dataset makes this combination a great fit for building fast and lesser computation required neural networks. Substituting recurrent layers for pooling layers might reduce the loss of information and would be worth exploring in the future projects. 
It is interesting to find the use of combinations of real world dataset and simulator data to train these models. Then I can get the true nature of how a model can be trained in the simulator and generalized to the real world or vice versa. There are many experimental implementations carried out in the field of self-driving cars and this project contributes towards a significant part of it. 
References
GitHub - https://github.com/woges/UDACITY-self-driving-car/tree/master/term1_project3_behavioral_cloning
GitHub - https://github.com/SakshayMahna/P4-BehavioralCloning
Deep Learning for Self-Driving Cars - https://towardsdatascience.com/deep-learning-for-self-driving-cars-7f198ef4cfa2
GitHub - https://github.com/llSourcell/How_to_simulate_a_self_driving_car
GitHub - https://github.com/naokishibuya/car-behavioral-cloning
End to End Learning for Self-Driving Cars - https://arxiv.org/pdf/1604.07316v1.pdf
Aditya Babhulkar - https://scholarworks.calstate.edu/downloads/fx719m76s
GitHub - https://github.com/udacity/self-driving-car-sim"	46	835	3	aslanahmedov	self-driving-carbehavioural-cloning
244	244	NEPSE Trading Data	Nepal Stock Exchange Trading Data (400+ companies)	['investing']	"Context
Obtained NEPSE (Nepal Stock Exchange) trading data from the website to find the patterns of stock exchange.
Content
NEPSE trading data of more than 400 companies from 2000-01-01 to 2021-12-31.
Reference
nepse-data"	262	105	0	sarojrana	nepse-data
245	245	USA EPA MSW Municipal Solid Waste Data 1960-2018	An overview of 5+ decades of MSW management in the USA	['energy']		1	42	1	zsyver	usa-epa-msw-municipal-solid-waste-data-19602018
246	246	Currency Exchange Rates	Foreign exchange rates for every currency	['business', 'finance', 'economics', 'tabular data', 'currencies and foreign exchange']	"Content
&gt; Foreign exchange rates for every currency (updated daily)
Inspiration
&gt; Time series analysis of foreign exchange rates for different currencies"	177	1650	29	ruchi798	currency-exchange-rates
247	247	Scrabble data from woogles.io	Analyze games played by BasicBot	['games', 'nlp']	"Context
This is a dataset of Scrabble games played by the BasicBot on the website woogles.io. It was created using two notebooks:
Fetch the data: https://www.kaggle.com/mrisdal/fetch-scrabble-data-from-woogles-io
Process the data: https://www.kaggle.com/mrisdal/process-scrabble-data-from-woogles-io
Content
There are four CSV files:
games.csv contains metadata about individual games, time control, how the game ended, the winner, timestamps, etc.
scores.csv contains final score and rating data for games and players
turns.csv contains data about individual plays in each turn of every scrabble game in the dataset
games_raw.csv is the file before some processing done in this notebook: https://www.kaggle.com/mrisdal/process-scrabble-data-from-woogles-io
Acknowledgements
Thank you to woogles.io for providing this platform for playing Scrabble!"	13	426	9	mrisdal	scrabble-data-from-wooglesio
248	248	COVID-19 Infection Patterns (IP) Dataset 	Infection Patterns from COVID-19 Recovered Patients	['diseases', 'health', 'computer science', 'exploratory data analysis', 'tabular data', 'heart conditions']	"The IP dataset includes 131 subjects using a survey method. The dataset includes COVID-19 positive cases who spent a quarantine period in an isolation facility. Negative cases, positive cases that were critical and subjected to hospital’s admission, none critical cases who’s subjected for home isolation and those with missing information were excluded in the survey.
The survey questionnaire includes a biographic data such as age, gender, nationality, smoking, weight, height, blood type, and sport. It also includes a medical history such as multivitamins, medicine usage before infection, medicine usage after infection, symptoms after infection, chronic disease, previous infection of MERS Co-V, previous infection of COVID-19, and period of isolation.
The collected data from the questionnaire is tabulated. The data is manually processed and converted to a binary format.
The dataset is only for research and educational purposes. If you use this dataset, please refer to the following reference:
T. Alafif et al., ""DISCOVID: Discovering Patterns of COVID-19 Infection from Recovered Patients"", International Journal of Information Technology, Springer, 2022."	1	12	0	tarikalafif	covid19-infection-patterns-dataset
249	249	Malaysia COVID-19	Official COVID-19 data published by Malaysia Ministry of Health ported to Kaggle	[]		57	1280	0	jesendo	malaysia-covid19
250	250	DeepCorr: Tor Flow Correlation Data	Nasr, DeepCorr: Strong Flow Correlation Attacks on Tor Using Deep Learning, 2018	[]	"The dataset from the paper M.Nasr et al., DeepCorr: Strong Flow Correlation Attacks on Tor Using Deep Learning, 2018. 
All copyrights belongs to the original paper author."	3	143	1	ipidkaggle	deepcorr-flow-correlation-of-tor
251	251	Removals Company Perth- CBD Movers	CBD Movers Perth Provides Australia's No.1 Removals Services.	[]	A few moving tips and assistance of Removals Company Perth can ease out the moving process to a great extent. If you are looking for such a company then you can contact CBD Movers Perth via 1300585828	0	12	0	removalistsperth	removals-company-perth-cbd-movers
252	252	COVID-19 Vaccination and Case Trends	COVID-19 Vaccination and Case Trends by Age Group, United States	['public health', 'public safety']	"Context
Trends in vaccinations and cases by age group, at the US national level. Data is stratified by at least one dose and fully vaccinated. Data also represents all vaccine partners including jurisdictional partner clinics, retail pharmacies, long-term care facilities, dialysis centers, Federal Emergency Management Agency and Health Resources and Services Administration partner sites, and federal entity facilities.
Acknowledgements
License - Public Domain U.S. Government"	3	406	1	saurabhshahane	covid19-vaccination-and-case-trends
253	253	Raw woogles.io games	Raw data from BasicBot's Scrabble games	['games', 'nlp']	"Context
This is the raw Scrabble data from woogles.io generated by this Python notebook that fetches the data on a monthly schedule: https://www.kaggle.com/mrisdal/fetch-scrabble-data-from-woogles-io. It then gets processed in this R notebook (which you can also find and fork from the ""Code"" tab of this dataset): https://www.kaggle.com/mrisdal/process-scrabble-data-from-woogles-io.
Content
There are two files:
turns.csv was already processed in the Python notebook that fetched the data and it contains data about individual plays in each turn of every scrabble game in the dataset
woogles.json a JSON file of game and player metadata
Acknowledgements
Thank you to woogles.io for providing this platform for playing Scrabble!"	2	281	5	mrisdal	raw-wooglesio-games
254	254	IMDb TV show data sets (Top 250 TV shows on IMDb)	This is a collection of data sets focusing on the top 250 shows on IMDb	['arts and entertainment', 'data cleaning', 'tabular data', 'pandas', 'seaborn']	"Context
IMDB hosts a lot of information about TV shows and movies. It is a web platform that is information rich in terms of details about movies, TV shows or mini-series. The information it has about the movies/TVshows are about the airdate, ratings, description, director, et al. In case of TV shows, it also has information about each episode. Due to its neat setup, IMDb is ripe for data collection. Hence, it is easy to create a data set on the TV shows on IMDb. A lot of people often hype up their favorite TV shows on a weekly basis, and it would be great to actually examine how these shows perform in comparison to one another. Hence, the objective of this project is to collect data on the episodes of all the TV shows in the top 250 TV shows on IMDb. Then, to proceed to analyze them to extract insights on how they perform.
Content
The three data sets are designed to focus mostly on the Top 250 shows on IMDb. After collecting information about the top 250 shows on IMDb and how they are ranked according to their ratings, information of their episodes was also collected to build a huge database in order to conduct in-depth analysis of the shows. Then, another data containing information about the top 1000 episodes on IMDb was collected to portray how the top 250 shows contribute towards producing prolific content. The two sub-folders include information about each shows' csv file data on their episodes information.
Resources
IMDb
Link to Top 250 TV shows on IMDb
Link to Top 1000 TV series episodes on IMDb
Repository
Here is a link to the GitHub Repository that contains the code that was used to collect data.
Dashboard
Here is a link to the dashboard that was built on Tableau using this data.
Inspiration
After watching too many TV shows and keeping track of their IMDb ratings, I was inspired to do research into how the best TV shows perform."	0	20	0	muby98	imdb-tv-show-data-sets-top-250-tv-shows-on-imdb
255	255	Divvy Bike Share Trip Data	Covers dates from December 2020 to November 2021	['united states', 'transportation', 'cycling', 'data analytics', 'retail and shopping', 'travel']	Motivate International Inc. (“Motivate”) operates the City of Chicago’s (“City”) Divvy bicycle sharing service. Motivate and the City are committed to supporting bicycling as an alternative transportation option. As part of that commitment, the City permits Motivate to make certain Divvy system data owned by the City (“Data”) available to the public, subject to the terms and conditions of this License Agreement (“Agreement”).	2	270	0	bkermen	divvy-trip-data-12-months
256	256	Wikipedia mentions of product names	Mentions of products of tech unicorns and the largest internet companies.	['nlp', 'text mining', 'text data']	"Context
More info here: https://dataqa.ai/building-a-detector-of-product-launches/"	4	893	6	dataqaai	wikipedia-mentions-of-product-names
257	257	Housing JPMC	Housing Data from JPMC Hackathon	['social issues and advocacy']		12	911	0	abhilashreddys	housing-jpmc
258	258	paww-lib	A Custom PyTorch Wrapper for the PetFinder.my - Pawpularity Contest competition	['intermediate', 'classification', 'deep learning', 'cnn', 'pytorch']	Model files and code for the 'PetFinder.my - Pawpularity Contest' Kaggle competition organized by PetFinder	5	1260	8	sauravmaheshkar	pawwlib
259	259	"My analysis of the ""bike share"" data: Google S."	This is a part of the capstone project for the Google Data Analytics certificate	['business']	"Context
 One analysis Done in spreadsheets with 202004 and 202005 data 
Content
To adjust for outlier Ride lengths like the max and min below:
Max RL
=MAX(N:N)978:40:02
minimum RL
=MIN(N:N)-0:02:56
TRIMMean to shave off the top and bottom of a dataset.
TRIMMEAN
=TRIMMEAN(N:N,5%)0:20:20
=TRIMMEAN(N:N,2%)0:21:27
Otherwise the Ride length for 202004 is 
Average RL
0:35:51
The most common day of the week is Sunday. There are 61,148 members and 23,628 casual riders.
mode of DOW
1
CountIf member of MC
61148
CountIf casual of MC
23628
Pivot table 1 2020-04
member_casual
AVERAGE of ride_length
Same calculations for 2020-05
Average RL
0:33:23
Max RL
481:36:53
minimum RL
-0:01:48
mode of DOW
7
CountIf member of MC
113365
CountIf casual of MC
86909
TRIMMEAN
0:25:22
0:26:59
There are 4 pivot tables included in seperate sheets for other comparisons. 
Acknowledgements
I gathered this data using the sources provided by the Google Data Analytics course. All work seen is done by myself. 
Inspiration
I want to further use the data in SQL, and Tableau."	4	422	1	lamarmcmillan	my-analysis-of-the-bike-share-data-google-s
260	260	online payday loans canada	$100 to $5000 online payday loans	[]		0	18	0	redpayday	online-payday-loans-canada
261	261	BrainAGE-Cam(2D)		['arts and entertainment']		0	8	0	lijingamber	brainagecam2d
262	262	HappyWhale Individual Model/Embed - 384i - 512e		[]		0	2	0	dschettler8845	happywhale-individual-modelembed-384i-512e
263	263	Reliance Stock and News Data	Daily Reliance Stock Prices with News Sentiment Analysis Scores	['business', 'time series analysis']	"Context
This dataset contains daily stock market data of Reliance Industries  ,  its daily news articles and news sentiments , created and managed by this notebook by scheduling notebook daily. This dataset is created with the aim to create stock market strategy or decision based on news sentiments. 
Content
This dataset contains 3 files - reliance_news.json , reliance_news_sentiment.csv and reliance_stock_history.csv.
reliance_news.json  contains -  author , title , description , url , source , image , category , language , country , published_at
reliance_stock_history.csv contains - Date ,Open , High , Low , Close , Volume , Dividends , Stock splits 
reliance_news_sentiment.csv contains - published _ at , title , description ,  url  , sentiment , sentiment _ score 
Acknowledgements
Daily stock market data is fetched using yfinance api and news data is fetched using mediastack api ."	29	835	4	yashvi	reliancestockandnewsdata
264	264	CSCI 270 Corpus		[]		1	55	0	gabrielferrer	csci-270-corpus
265	265	Carla Traffic Lights Images	Dataset of more then 2500 traffic lights from Carla 	['auto racing', 'artificial intelligence', 'automobiles and vehicles', 'computer vision', 'image data']	"Context
Over 2500 images of traffic lights decided by color: red, green, yellow and back, indicating that you look at the TL from the back or from the side 
Content
There are three folders: 
1. train - contains 4 folders by color with approximately 500 - 530 images each
2. val - contains 4 folders by color with 100 (only yellow has 90) images each"	0	6	0	sachsene	carla-traffic-lights-images
266	266	FGF21 as a potential biomarker for mice behaviour	behaviour change after tackling Obesity	[]		0	12	0	alishaparveen	obesity
267	267	SPX prices	Data for 417 S&P 500 companies from 2000 to 2013	['business']		8	566	1	ralarellanolopez	spx-prices
268	268	RSNA-BrainTumorClassification (64, 256, 256)	Competition data resized to (64, 256, 256)	['medicine', 'image data']	"Creation
This dataset was created using this notebook.
This is mostly taken from the PNG Dataset. 
My problem with the PNG dataset was that resizing took 2-3 seconds per image, so which resulted in long train time so I needed to modify that a bit and thought to share it.
Data Info
This saves each patient data into a (4, 1, 64, 256, 256).
One image for each scan type [ flair, tw1ce, t1w, t2w] respectively.
The files were created using torch.save, according to docs this uses pickle to write data.
Hope this is not too late to help somebody.
Also sorry for the weird filename."	9	1173	6	abdelrhmanhosny	rsnabraintumorclassification-64-256-256
269	269	USA COVID-19 Vaccinations	State-by-state data on COVID-19 vaccinations in the United States	['public health', 'medicine', 'public safety', 'covid19']	"Context
There is a vaccine for COVID-19 that is available in the USA
Content
State-by-state data on COVID-19 vaccinations in the United States
Columns:
[date,location,total_distributed,total_vaccinations,distributed_per_hundred,total_vaccinations_per_hundred,people_vaccinated,people_vaccinated_per_hundred,people_fully_vaccinated,people_fully_vaccinated_per_hundred,daily_vaccinations_raw,daily_vaccinations,daily_vaccinations_per_million,share_doses_used]
Acknowledgements
Data from https://ourworldindata.org/us-states-vaccinations and https://covid.ourworldindata.org/data/vaccinations/us_state_vaccinations.csv
 shared under CC 4.0 license"	2175	14295	52	paultimothymooney	usa-covid19-vaccinations
270	270	cots_dataset		[]		0	2	0	jobayerhossain	cots-dataset
271	271	DATA_FOR_TESTING		[]		0	3	0	manarmoh	data-for-testing
272	272	Indic NLP Library	 Indic NLP Library is  for common text processing and NLP for Indian languages	['linguistics', 'psychology', 'text data']	"Special thanks to Anoop Kunchukuttan for making this wonderful package.
Source: https://github.com/rwightman/pytorch-image-models
Installation: see this notebook for quick tutorial
import sys
sys.path.append('../input/indic-nlp-library')
import  indicnlp 
License: MIT License"	5	898	14	sayantankirtaniya	indic-nlp-library
273	273	Sz_DFL	forzzhandhisotheraccount	[]		0	159	0	stevezsz	sz-dfl
274	274	1M Big Captcha Dataset	6-digit Captcha consist of Alphanumeric Characters	['artificial intelligence', 'classification', 'deep learning', 'neural networks', 'image data']		7	41	1	muzzamalhameed	1m-big-captcha-dataset
275	275	AvocadoPrice	AvocadoPrice-DATASET	[]		0	2	0	hamedetezadi	avocadoprice
276	276	Geografia, racialidade e genero		[]		0	6	0	datarepositoty1	geo-racialidade-genero
277	277	Geografia e racialidade		[]		0	6	0	datarepositoty1	geografia-e-racialidade
278	278	385 Bird Species - Classification	56046Train, 1925 Test, 1925 Validation images 224X224X3  jpg format	['biology', 'computer vision', 'classification', 'cnn', 'image data', 'tensorflow']	"Data set of 375 bird species.54652 training images, 1875 test images(5 images per species) and 1875 validation images(5 images per species.
All images are 224  X 224 X 3 color images in jpg format. Data set includes a train set, test set and validation set. Each set contains 375 sub directories, one for each bird species. The data structure is convenient if you use the Keras ImageDataGenerator.flow_from_directory to create the train, test and valid data generators. The data set also include a file Bird Species.csv. This cvs file contains three columns. The filepaths column contains the  file path to an image file. The labels column contains the class name associated with the image file. The Bird Species.csv file if read in using  df= pandas.birds_csv(Bird Species.csv) will create a pandas dataframe which then can be split into  train_df, test_df and valid_df  dataframes to create your own partitioning of the data into train, test and valid data sets.
NOTE: The test and validation images in the data set were hand selected to be the ""best"" images so your model will probably get the highest accuracy score using those data sets versus creating your own test and validation sets. However the latter case is more accurate in terms of model performance on unseen images.
Images were gather from internet searches by species name. Once the image files for a species was downloaded they were checked for duplicate images using a python duplicate image detector program I developed. All duplicates detected were deleted in order to prevent their being images common between the training, test and validation sets.
After that the images were cropped so that the bird occupies at least 50% of the pixel in the image. Then the images were resized to 224 X  224 X3 in jpg format. The cropping ensures that when processed by a CNN their is adequate information in the images to create a highly accurate classifier. Even a moderately robust model should achieve training, validation and test accuracies in the high 90% range. Because of the large size of the dataset I recommend if you try to train a model use and image size of 150 X 150 X3 in order to reduce training time. All files were also numbered sequential starting from one for each species. So test images are named 1.jpg to 5.jpg. Similarly for validation images. Training images are also numbered sequentially with ""zeros"" padding. For example 001.jpg, 002.jpg ....010.jpg, 011.jpg .....099.jpg, 100jpg, 102.jpg etc. The zero's padding preserves the file order when used with python file functions and Keras flow from directory.
The training set is not balanced, having a varying number of files per species. However each species has at least 120 training image files. This imbalanced did not effect my kernel classifier as it achieved over 98% accuracy on the test set.
One significant imbalance in the data set is the ratio of male species  images to female species images. About 85% of the images are of the male and 15% of the female. Males typical are far more diversely colored while the females of a species are typically bland.  Consequently male and female images may look entirely different .Almost all test and validation images are taken from the male of the species. Consequently the classifier may not perform as well on female specie images."	20741	140693	682	gpiosenka	100-bird-species
279	279	Dataset Hewan Ternak		[]		6	51	1	zulfafebriana	dataset-hewan-ternak
280	280	Whale-tfrecord-train-224		[]		0	4	0	ludovick	whaletfrecordtrain224
281	281	Affordability of Diets	Cost and affordability of nutritious diets across and within countries	['nutrition', 'public health', 'income', 'health', 'food']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>These data were used in the article <a href=""https://ourworldindata.org/diet-affordability"" target=""_blank"" rel=""nofollow"">Three billion people cannot afford a healthy diet</a> published on <a href=""https://ourworldindata.org/"" target=""_blank"" rel=""nofollow"">Our World in Data</a> in July 2021.</p>
<p>This data is sourced from the work of <a href=""https://sites.tufts.edu/foodpricesfornutrition"" target=""_blank"" rel=""nofollow"">Hereforth et al. (2020)</a>, which is a background paper for the UN FAO State of Food Security and Nutrition in the World report. It is based on data on prices for locally available food items from the <a href=""https://icp.worldbank.org/"" target=""_blank"" rel=""nofollow"">World Bank's International Comparison Program (ICP)</a> attached to other data on food composition and dietary requirements.</p>
<p>The nutritional requirements used in this study are in line with the WHO's recommendations for the median woman of reproductive age. The authors note two key two reasons for this:</p>
<ol>
<li>Requirements fall roughly at the median of the entire population distribution, in the sense that least-cost diets to meet energy and nutrient requirements for people in this reference group approximate the median level of least costs for all sex-age groups over the entire life cycle. This reference group is therefore a good representation of the population as a whole.</li>
<li>Women of reproductive age are typically a nutritionally vulnerable population group, as seen in their increased risk of dietary inadequacies (due to social practices and norms that often disadvantage them in terms of access to food), which have important consequences for themselves and their children. Previous studies have also based their analyses on this reference group.</li>
</ol>
<p><strong>Citation</strong><br>
Herforth, A., Y. Bai, A. Venkat, K. Mahrt, A. Ebel &amp; W.A. Masters (2020). Cost and affordability of healthy diets across and within countries. Background paper for the State of Food Security and Nutrition in the World 2020. FAO Agricultural Development Economics Technical Study No. 9. Rome: FAO (108 pages).</p>
<h2><strong>Types of Diets</strong></h2>
<ol>
<li><em>“Energy sufficient”</em> diets provide adequate calories for energy balance at a given level of physical activity and body size, using only the least-cost starchy staple in each country. For example, such a diet could consist of only the lowest cost type of rice in that country, or only maize porridge.</li>
<li><em>“Nutrient adequate”</em> diets provide not only adequate calories but also adequate levels of all essential nutrients – namely, carbohydrates, protein, fat, vitamins and minerals, within the upper and lower bounds needed to prevent deficiencies and avoid toxicity.</li>
<li><em>“Healthy”</em> diets meet a set of dietary recommendations intended to provide nutrient adequacy and long-term health. There are many definitions of a “healthy” diet pattern at national, regional and global levels. In this case, we select the national food-based dietary guidelines (FBDGs) of several countries from diverse regions, in order to represent a range of dietary recommendations which have been articulated by UN Member States. Dietary patterns have been studied extensively in the nutrition epidemiology literature, relating specific foods and proportionality of different food groups to disease incidence and prevention. Nutrients alone do not explain the relationship of food to health, as there are many non-nutrient components of food, including but not limited to fibre, phytochemicals, the food matrix, and interactions between these. FBDGs focus on foods rather than nutrients, and typically concentrate on proportionality of food group intake. Furthermore, proportionality in food group intake ensures a culturally acceptable diet meeting at least a minimum standard for palatability and cultural norms, so the healthy diet is closer to actual food preferences, in terms of dietary pattern, than the energy sufficient or nutrient adequate diets.</li>
</ol>
<h2><strong>Findings</strong></h2>
<p>Some findings from this data are available in <a href=""https://ourworldindata.org/diet-affordability"" target=""_blank"" rel=""nofollow"">article form</a>, written by Hannah Ritchie and more can be found in Our World in Data's <a href=""https://ourworldindata.org/explorers/food-prices"" target=""_blank"" rel=""nofollow"">Food Prices Data Explorer</a>.</p>
<p><img src=""https://media.data.world/84AgmvRHTMa0BRrl4Rz8_Diet-costs-marimekko.png"" alt=""A bar chart showing how the cost of a healthy diet compares to daily median income for countries around the world. For many countries, the cost of a healthy diet exceeds that median income for that country"" style=""""></p>
<h2><strong>License</strong></h2>
<p>These data are released under the most restrictive license of the source material: <a href=""https://creativecommons.org/licenses/by-nc-sa/2.0/"" target=""_blank"" rel=""nofollow"">CC-BY-NC-SA</a></p>
This dataset was created by Amber Thomas and contains around 200 samples along with Cost Of Healthy Diet (2017 Usd Per Day), Healthy Diet Cost (number Cannot Afford), technical information and other features such as:
- Healthy Diet Cost (% Of $1.20 Poverty Line)
- Calorie Sufficient Diet Cost (% Of $1.20 Poverty Line)
- and more.
How to use this dataset
&gt; - Analyze Calorie Sufficient Diet Cost (% Of Average Food Expenditure) in relation to Nutrient Adequate Diet Cost (% Of $1.20 Poverty Line)
- Study the influence of Cost Of Calorie Sufficient Diet (2017 Usd Per Day) on Calorie Sufficient Diet Cost (% Cannot Afford)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	149	913	7	yamqwe	affordability-of-dietse
282	282	School Quality Reports - Early Childhood Schools	School Quality Report Results for Early Childhood.	['universities and colleges', 'education', 'primary and secondary schools']	New York City Department of Education 2020 - 2021 School Quality Report Results for Early Childhood. The Quality Review is a process that evaluates how well schools are organized to support student learning and teacher practice. It was developed to assist New York City Department of Education (NYCDOE) schools in raising student achievement by looking behind a school’s performance statistics to ensure that the school is engaged in effective methods of accelerating student learning.	0	5	1	isharab	school-quality-reports-early-childhood
283	283	Central Ohio Points of Interest	Dataset of Crime Camera Locations owned by the city of Columbus Ohio.	['crime', 'finance', 'public safety']	"About this dataset
&gt; <p>This data presents the locations and metadata of Crime Camera locations owned by the city of Columbus Ohio.</p>
<p>Source: <a href=""https://public-morpc.opendata.arcgis.com/datasets"" target=""_blank"" rel=""nofollow"">https://public-morpc.opendata.arcgis.com/datasets</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2019-05-01</p>
This dataset was created by Kelly Garrett and contains around 7000 samples along with Feature, Feature, technical information and other features such as:
- Feature
- Feature
- and more.
How to use this dataset
&gt; - Analyze Feature in relation to Feature
- Study the influence of Feature on Feature
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	15	129	4	yamqwe	central-ohio-points-of-intereste
284	284	Beijing 2022 Winter Olympics	Full data about medals, athletes, coaches, and technical officials	['hockey', 'china ', 'sports', 'data visualization', 'tabular data']	"Medals and Results Dataset will be updated every couple of days during the Olympics!
You can support the dataset via the upvote button!
This is an Olympic Games dataset that describes medals and athletes for Beijing 2020. The data was created from Beijing Olympics.
Medas, Events, Disciplines, Results, Athletes, Coaches, and Technical Officials  (with some personal data: date and place of birth, height, etc.) of the XXIV Olympic Winter Games you can find here.
Discplines
Alpine Skiing, Biathlon, Bobsleigh, Cross-Country Skiing, Curling, Figure Skating, Freestyle Skiing, Ice Hockey,  Luge, Nordic Combined, Short Track Speed Skating, Skeleton, Ski Jumping, Snowboard, Speed Skating
Dataset Description
|Table |Description|
| --- | --- |
|athletes.csv| personal information about all athletes |
|coaches.csv|personal information about all coaches|
|curling_results.csv|curling team results (men & women)|
|entries_discipline.csv|athletes entries (grouped by discipline)|
|events.csv|all events that had a place (qualifications are included)|
|hockey_players_stats.csv|hockey players stats (men & women)|
|hockey_results.csv|hockey team results (men & women)|
|medals.csv|general information on all athletes who won a medal|
|medals_total.csv |all medals (grouped by country)|
|technical_officials.csv|personal information about all technical officials|
Related Datasets
Olympic Games, 1986-2020
Tokyo 2020 Olympics
Tokyo 2020 Paralympics
Tokyo 2020 Horses
Olympic Games Hosts
Dataset History
2022-13-22 - dataset is updated (Game Days 6, 7, 8,  9 and 10)
2022-08-22 - dataset is updated (Game Days 4 & 5)
2022-06-22 - dataset is updated (Game Day 3)
2022-05-22 - dataset is updated (Game Day 2)
2022-02-22 - dataset is created.
Q&A
If you have some questions please start a discussion.athletes.csv"	81	660	6	piterfm	beijing-2022-olympics
285	285	freight-transportation-in-multi-Mode		[]		1	3	0	zsham433	freight-multi-mode
286	286	Women in Movies	The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women	['arts and entertainment', 'movies and tv shows', 'art']	"About this dataset
&gt; <h1><a href=""http://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/"" target=""_blank"" rel=""nofollow"">The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women</a></h1>
<p>April 1, 2014 article from <a href=""http://fivethirtyeight.com/"" target=""_blank"" rel=""nofollow"">FiveThirtyEight</a> dispelling the myth that films with strong female roles do not make as much money as films with male leads.</p>
<p>Includes data on films passing the <a href=""http://dykestowatchoutfor.com/"" target=""_blank"" rel=""nofollow"">Bechdel Test</a></p>
<p><img src=""https://media.npr.org/programs/atc/features/2008/sep/bechdel/rule_540-4b147460dfbfb89a5e4f5aad992bf3be3358e7cc-s1400.jpg"" alt=""The Rule"" style=""""></p>
<p>Data Source: <a href=""http://BechdelTest.com"" target=""_blank"" rel=""nofollow"">BechdelTest.com</a></p>
<p>Data Source: <a href=""http://The-Numbers.com"" target=""_blank"" rel=""nofollow"">The-Numbers.com</a></p>
This dataset was created by Carolee Mitchell and contains around 2000 samples along with Intgross, Budget, technical information and other features such as:
- Binary
- Intgross 2013$
- and more.
How to use this dataset
&gt; - Analyze Period Code in relation to Decade Code
- Study the influence of Title on Test
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Carolee Mitchell 
Start A New Notebook!"	51	426	4	yamqwe	women-in-moviese
287	287	BLDG-SIM Building Simulation Email List Text	1999-2019 Corpus of Emails from a large building simulation community email list	['energy', 'text mining', 'text data', 'electricity']	"This dataset and the explanation below are from the 2019 IBPSA Paper: Twenty years of building simulation trends: Text mining and topic modeling of the Bldg-sim email list archive
Context
The recent growth of the building performance re- search community has been in parallel with the onebuilding.org Bldg-sim email list. This list was formed in 1999 and steadily grew into a major venue for building simulation community announcements, dis- cussions, and questions and answers (Q&A). This pa- per presents an analysis of the Bldg-sim email archive to determine how traffic has grown over the years and what general trends have come and gone in this par- ticular user community.
Content
We use a text mining ap- proach to find the most prominent topics over the years and create trend metrics from the frequency of the most common words from those topics. The results illustrate the relative rise and fall of various soft- ware tools, organizations, and simulation topics from the portion of the simulation community who used the email list. Visualizations showing the trends are presented and discussed. The paper also discusses the generalizability of results as it should be noted that the users of this email list are primarily English- speaking simulation practitioners from North Amer- ica. All data and code used in this analysis are avail- able in a reproducible GitHub repository.
Acknowledgements
This paper and analysis is only possible with the help of Jason Glazer, the founder of the list serv
Inspiration
Slice and dice the email list content to find trends and interesting insights about the building simulation community
Banner image source"	23	1197	11	claytonmiller	bldgsim-building-simulation-email-list-text
288	288	DurLAR	128-channel LiDAR Dataset for Autonomous Driving	['artificial intelligence', 'automobiles and vehicles', 'image data', 'cv2']		3	12	0	l1997i	durlar
289	289	Birds are flying	A Photographic Collection of Birds for Multi-class Image Classification	['arts and entertainment', 'biology', 'classification', 'image data', 'multiclass classification', 'multilabel classification']	"birds multiclass image classification
A Photographic Collection of Birds for Multi-class Classification
Dataset contains JPG images of varying dimensions.
birds_annotation.csv contains file_name that corresponds to the image file names and label that corresponds to multi-classes.
The four classes described here are:
1. flying
2. standing on surface
3. standing at elevation
4. swimming
Annotations are made manually.
birds_labels.jpg is a pie-chart describing the label-balance."	25	422	1	rajkumarl	birds-are-flying
290	290	Italian Vaccination Progress	Vaccination for COVID-19 in Italy by region, age, sex	['public health', 'health', 'public safety', 'covid19']	"Context
The vaccination campaign in Italy started on December 27th 2020. The data was pulled from the Italian Open Data Github repository, the column names were translated from Italian into English.
Content
The data contains the following information:
Administration date: date of the vaccine administration
Vaccine supplier
Region: abbreviation of the Italian region
Age range: age group
Number of males: number of vaccinated males
Number of females: number of vaccinated females
First dose: number of administered first vaccine doses
Second dose: number of administered second vaccine doses
Previous infection: number of administrated doses to subjects previously infected with COVID-19
Additional dose: number of administrated additional doses for immunocompormised persons
Booster dose: number of administrated booster doses
NUTS1 code: European code for major socio-economic regions
NUTS2 code: European code for basic regions for the application of regional policies
ISTAT code: region code by Italian National Institute of Statistics
Region name: full name of Italian regions
Acknowledgments
This dataset is possible thanks to datateam-opendata which is a part of the Extraordinary Commissioner for the Covid-19 emergency.
Inspiration
Track vaccination campaign in Italy and answer questions:
What region is the most efficient in the vaccination? Normalize it to the region population
Is there any difference in vaccine suppliers among the regions?
How vaccination priorities evolved over time?"	282	2863	7	arthurio	italian-vaccination
291	291	Discord Survey (Russian and unclean)	Результаты опроса 403 участников дискорда (без изменений)	['russia', 'internet', 'survey analysis', 'tabular data', 'online communities']	"Результаты опроса 403 участников дискорда. Выборка была случайной, сервера были случайными, много людей отказывались проходить, но кто-то соглашался. Опрашивал только русскоговорящий людей. При создании я уведомлял пользователей, что после этого собираюсь провести анализ полученных данных и выложить результаты в открытый доступ. Никаких личных данных пользователей также не собиралось.
В целом, можно заметить, что мне нравится дискорд, а также некоторая психологическая направленность вопросов. Опыта проведения чего-то подобного у меня нет, но всё-таки пытался сделать все как можно более правильно.
В данной версии результаты не приведены и не очищены (к примеру, не удален столбец с мнением об опросе). Если кому интересно - милости прошу."	0	2	0	yonkotoshiro	discord-survey-russian-and-not-unclean
292	292	Crime Victim Statistics	National Crime Victimization Survey (NCVS) Data Collection as a whole	['united states', 'crime', 'social science', 'public safety']	"About this dataset
&gt; <h1>National Crime Victimization Survey (NCVS)</h1>
<p><a href=""http://www.bjs.gov/developer/ncvs/index.cfm"" target=""_blank"" rel=""nofollow"">http://www.bjs.gov/developer/ncvs/index.cfm</a></p>
<blockquote>
<p>The NCVS RESTful API is a web service that provides criminal victimization data obtained annually from a nationally representative sample of about 90,000 households and 160,000 persons interviewed each year. NCVS data describe the frequency, characteristics and consequences of criminal victimization in the United States. The NCVS provides the largest national forum for victims to describe the impact of crime and characteristics of violent offenders. It is one of two primary data collections about crime in the United States and is the only source of data about crimes not reported to the police.</p>
</blockquote>
<h2>About the Datasets</h2>
<p>The variables contained in the datasets presented here make up a small fraction of the full population of variables in the NCVS data collection as a whole. The full dataset in csv format, including both personal and household levels, exceeds 250 mb in size. It can be retrieved in full from <a href=""http://www.bjs.gov/developer/ncvs/index.cfm"" target=""_blank"" rel=""nofollow"">http://www.bjs.gov/developer/ncvs/index.cfm</a></p>
<h4><em>NCVS_PERSONAL_VICTIMIZATION_1993-2014.csv</em></h4>
<p>This dataset contains variables related to crime victims and the characteristics of the victimization events. The data are pertinent to the individual-level (as opposed to household level perspective presented later). The variables make up a small fraction of the full population of variables in the NCVS data collection as a whole. The full data, including both personal and household levels, exceeds 250 mb in size.</p>
<p><a href=""https://data.world/nrippner/bjs-crime-victim-statistics/file/data_dict.csv"">data dictionary</a></p>
<h4><em>NCVS_HOUSEHOLD_VICTIMIZATION_1993-2014.csv</em></h4>
<p>This file contains variables pertaining to crime victimization at the level of households.</p>
<p><a href=""http://www.bjs.gov/developer/ncvs/householdFields.cfm"" target=""_blank"" rel=""nofollow"">data dictionary</a></p>
<h2>Initial Exploration</h2>
<p><em>The analysis presented below represents a small fraction of what may be possible to explore within these victimization datasets, and an infinitessimally small fraction of the possibilities contained in the full NCVS data.</em></p>
<p><em>Values are adjusted to account for differential proportions in population by race</em></p>
<h4>1. Men are victims of both violent and non-violent crimes at a higher rate than women.</h4>
<p><img src=""https://i.imgur.com/CJGUWh6.png"" alt=""Imgur"" style=""""></p>
<h4>2. Victims of serious violent crimes (eg, rape, sexual assault, personal robbery, or aggravated assault) are more likely to be Black than White or Hispanic. However, victims of non-violent crimes tend to be White.</h4>
<p><img src=""https://i.imgur.com/bLehIgl.png"" alt=""Imgur"" style=""""></p>
<h4>3. Crimes that don't involve a weapon tend to involve victims who are White. When a firearm is involved, victimization affects Blacks roughly twice as often as Whites.</h4>
<p><img src=""https://i.imgur.com/MKW8SN6.png"" alt=""Imgur"" style=""""></p>
This dataset was created by Noah Rippner and contains around 50000 samples along with Hincome, Treatment, technical information and other features such as:
- Marital2
- Age
- and more.
How to use this dataset
&gt; - Analyze Ethnic1r in relation to Year
- Study the influence of Region on Race1r
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Noah Rippner 
Start A New Notebook!"	28	233	3	yamqwe	bjs-crime-victim-statisticse
293	293	YFinance Stock Price Data for Numerai Signals	Daily Updates of Stock OHLCV (Close, High, Low, Open, Volume)	['business', 'finance', 'tabular data', 'investing']	This Stock price OHLCV data is updated daily to be used for the weekly submission to the Numerai Signals. Note that there are some very strange values especially for (adjusted) close and volume data, which are known to be an issue with the Yahoo! Finance API. When you use this data, make sure that you deal with these unrealisitc values.	2194	6285	43	code1110	yfinance-stock-price-data-for-numerai-signals
294	294	Specics		[]		0	9	0	ahmedstohy	specics
295	295	gbr_fasterrcnn_resnet50_model	weights for resnet50	['exercise']		1	6	0	mahdio	gbr-fasterrcnn-resnet50-model
296	296	Kenya - Boy Child VS Girls	Child Enrollment comparison at Primary school level by County	['africa', 'business', 'data cleaning', 'data visualization', 'data analytics', 'primary and secondary schools']	"Kenya - Boy Child VS Girls Child Enrollment comparison at Primary school level by County
DESCRIPTION
SUMMARY
Kenya - Boy Child VS Girls Child Enrollment comparison at Primary school level by County
SUMMARY
Original Title: Kenya - Boy Child VS Girls Child Enrollment comparison at Primary school level by County
Boy Child VS Girls Child Enrollment comparison at Primary school level by County
Methodology - Registry
Source: https://data.humdata.org/dataset/kenya-boy-child-vs-girls-child-enrollment-comparison-at-primary-school-level-by-county
Last updated at https://data.humdata.org/organization/kenya-open-data-initiative : 2021-09-23
License -
Creative Commons Attribution International
This data is for education and training in data science and analysis
About tasks:
Data processing and analysis with an explanation of the results."	1	11	4	qusaybtoush	kenya-boy-child-vs-girls
297	297	GOV.UK COVID-19 Dashboard Data	UK COVID-19 Cases, Deaths, Hospital and Vaccine Metrics in a Sqlite Database	['business', 'covid19', 'sql']	"Background
This Sqlite database contains data publicly available from GOV.UK and can be found here: https://coronavirus.data.gov.uk/.  The data is available via a REST API and come data is available in CSV format.  However, it can be difficult to pull all this data together, so this Sqlite database contains a number of tables which includes all the data imported via the API.
How was this data generated?
For more information on how to generate this database, and extract and load the data using the REST API, you can use the additional Jupyter Notebooks which can be found in the following Git Repo: https://github.com/happyadam73/c19-notebooks
Currently this data runs up to 13 February 2022.
&gt; NOTE:  As of 31st January 2022, publish date based cases include all episodes but historic data has not been updated.  It is recommended for historical analysis to use specimen date cases.  For more details, see: https://coronavirus.data.gov.uk/details/whats-new/record/beb802ac-1ed2-47ac-b314-69a5c3f712b5
Data Dictionary
The following provides a list of all 9 tables and the columns that can be found in each table.
| table_name                                     | column_name                                       | column_type   | column_nullability   |
|:-----------------------------------------------|:--------------------------------------------------|:--------------|:---------------------|
| c19dashboard_uk__ltla_daily_metrics            | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__ltla_daily_metrics            | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__ltla_daily_metrics            | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__ltla_daily_metrics            | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__ltla_daily_metrics            | new_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_cases_by_publish_date_rate                    | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | new_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_cases_by_specimen_date_rate                   | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | new_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_deaths_28_days_by_publish_date_rate           | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | new_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__ltla_daily_metrics            | cum_deaths_28_days_by_death_date_rate             | NUMERIC       | Nullable             |
| c19dashboard_uk__national_cases_by_age_gender  | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | gender                                            | TEXT          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | age                                               | TEXT          | Not Nullable         |
| c19dashboard_uk__national_cases_by_age_gender  | rate                                              | NUMERIC       | Nullable             |
| c19dashboard_uk__national_cases_by_age_gender  | cum_cases                                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_cases_by_age_gender  | new_cases                                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_daily_metrics        | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_daily_metrics        | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__national_daily_metrics        | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__national_daily_metrics        | new_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_cases_by_publish_date_rate                    | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_cases_by_specimen_date_rate                   | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | male_cases                                        | TEXT          | Nullable             |
| c19dashboard_uk__national_daily_metrics        | female_cases                                      | TEXT          | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_pillar_one_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_pillar_one_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_pillar_two_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_pillar_two_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_pillar_three_tests_by_publish_date            | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_pillar_three_tests_by_publish_date            | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_admissions_by_age                             | TEXT          | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_tests_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_tests_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | covid_occupied_mv_beds                            | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | hospital_cases                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_deaths_28_days_by_publish_date_rate           | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_deaths_28_days_by_death_date_rate             | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_people_vaccinated_first_dose_by_publish_date  | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_people_vaccinated_second_dose_by_publish_date | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | new_people_vaccinated_third_injection_by_publish_date | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_people_vaccinated_first_dose_by_publish_date  | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_people_vaccinated_second_dose_by_publish_date | NUMERIC       | Nullable             |
| c19dashboard_uk__national_daily_metrics        | cum_people_vaccinated_third_injection_by_publish_date | NUMERIC       | Nullable             |
| c19dashboard_uk__nhsregion_daily_metrics       | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__nhsregion_daily_metrics       | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__nhsregion_daily_metrics       | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__nhsregion_daily_metrics       | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__nhsregion_daily_metrics       | new_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__nhsregion_daily_metrics       | cum_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__nhsregion_daily_metrics       | cum_admissions_by_age                             | NUMERIC       | Nullable             |
| c19dashboard_uk__nhsregion_daily_metrics       | covid_occupied_mv_beds                            | NUMERIC       | Nullable             |
| c19dashboard_uk__nhsregion_daily_metrics       | hospital_cases                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__region_daily_metrics          | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__region_daily_metrics          | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__region_daily_metrics          | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__region_daily_metrics          | new_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_cases_by_publish_date_rate                    | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | new_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_cases_by_specimen_date_rate                   | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | male_cases                                        | TEXT          | Nullable             |
| c19dashboard_uk__region_daily_metrics          | female_cases                                      | TEXT          | Nullable             |
| c19dashboard_uk__region_daily_metrics          | new_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_deaths_28_days_by_publish_date_rate           | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | new_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__region_daily_metrics          | cum_deaths_28_days_by_death_date_rate             | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__summary_daily_metrics         | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__summary_daily_metrics         | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__summary_daily_metrics         | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__summary_daily_metrics         | new_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_cases_by_publish_date_rate                    | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_cases_by_specimen_date_rate                   | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_pillar_one_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_pillar_one_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_pillar_two_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_pillar_two_tests_by_publish_date              | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_pillar_three_tests_by_publish_date            | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_pillar_three_tests_by_publish_date            | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_pillar_four_tests_by_publish_date             | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_pillar_four_tests_by_publish_date             | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_admissions                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_tests_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_tests_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | covid_occupied_mv_beds                            | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | hospital_cases                                    | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | planned_capacity_by_publish_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_deaths_28_days_by_publish_date_rate           | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | new_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__summary_daily_metrics         | cum_deaths_28_days_by_death_date_rate             | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | area_type                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__utla_daily_metrics            | area_name                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__utla_daily_metrics            | area_code                                         | TEXT          | Not Nullable         |
| c19dashboard_uk__utla_daily_metrics            | date                                              | DATE          | Not Nullable         |
| c19dashboard_uk__utla_daily_metrics            | new_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_cases_by_publish_date                         | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_cases_by_publish_date_rate                    | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | new_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_cases_by_specimen_date                        | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_cases_by_specimen_date_rate                   | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | new_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_deaths_28_days_by_publish_date                | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_deaths_28_days_by_publish_date_rate           | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | new_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_deaths_28_days_by_death_date                  | NUMERIC       | Nullable             |
| c19dashboard_uk__utla_daily_metrics            | cum_deaths_28_days_by_death_date_rate             | NUMERIC       | Nullable             |
| reference_geography__age_gender_populations    | category                                          | TEXT          | Not Nullable         |
| reference_geography__age_gender_populations    | area_code                                         | TEXT          | Not Nullable         |
| reference_geography__age_gender_populations    | gender                                            | TEXT          | Not Nullable         |
| reference_geography__age_gender_populations    | age                                               | TEXT          | Not Nullable         |
| reference_geography__age_gender_populations    | population                                        | NUMERIC       | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | ltla_code                                         | TEXT          | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | ltla_name                                         | TEXT          | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | utla_code                                         | TEXT          | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | utla_name                                         | TEXT          | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | region_code                                       | TEXT          | Not Nullable         |
| reference_geography__ltla_utla_region_mappings | region_name                                       | TEXT          | Not Nullable         |"	18	440	0	happyadam73	uk-covid19-dashboard-data-sqlite-compressed
298	298	Alcohol Available for Consumption		['alcohol', 'business', 'economics', 'data visualization', 'data analytics']	"Alcohol Available for Consumption
DESCRIPTION
SUMMARY
These releases provide estimates of the quantity of alcoholic beverages and tobacco available for consumption in New Zealand. As the tobacco available for consumption series was discontinued in September 2010, for reasons of confidentiality, the release title was changed accordingly. Data for years back to December 2004 are available from the source URL (see below).
This data is for education and training in data science and analysis
About tasks:
Data processing and analysis with an explanation of the results."	1	12	3	qusaybtoush	alcohol-available-for-consumption
299	299	Mars2020-Image-Catalogue	List of all Images from NASA/JPL Website	['arts and entertainment', 'geology', 'astronomy', 'engineering', 'image data']	"Context
This is a list of all images available on NASA/JPLs website for Mission Mars 2020
Content
Metadata for all images as shown here
Acknowledgements
The maintainer of this dataset does not own this data.
All Data is hereby accredited to: NASA/Caltech-JPL/ASU(MastCamZ)"	100	17757	27	sakethramanujam	mars2020imagecatalogue
300	300	Global wine points		['alcohol', 'business', 'economics', 'logistic regression', 'linear regression']	"Global wine points
This data is for education and training in data science and analysis
About the DataSet ,
Wine points and prices across the world by vintage, vineyard and variety
About tasks:
Data processing and analysis with an explanation of the results."	2	3	3	qusaybtoush	global-wine-points
301	301	DeepSig.io RADIOML 2018.01A (NEW)	24 digital and analog modulation types of RF signals	['research', 'games', 'mobile and wireless', 'classification', 'deep learning', 'rnn']	"Context
A dataset that includes both synthetic simulated channel effects and over-the-air recordings of 24 digital and analog modulation types has been heavily validated.
This dataset was used for Over-the-air deep learning-based radio signal classification published 2017 in IEEE Journal of Selected Topics in Signal Processing, which provides additional details and a description of the dataset.
Data was stored in hdf5 format as complex floating-point values, with 2 million examples, each 1024 samples long.
Content
I converted the data to .npy format since I found it took much less time to load.
Acknowledgements
Thanks to Tim O’Shea, Tamoghna Roy, T. Charles Clancy for posting Over the Air Deep Learning Based Signal Classification
Inspiration
The success of the community depends on the success of each of us, the more information the better"	12	153	5	aleksandrdubrovin	deepsigio-radioml-201801a-new
302	302	Wine, Beer, and Liquor Reviews		['alcohol', 'business', 'exploratory data analysis', 'seaborn']	"DESCRIPTION
A list of over 1,000 reviews on beer, liquor, and wine sold online.
SUMMARY
About This Data
This is a list of over 2,000 reviews for beer, liquor, and wine sold online provided by Datafiniti's Product Database. The dataset includes address, city, state, business name, business categories, menu data, phone numbers, and more.
Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.
What You Can Do With This Data
You can use this data to discover insights into how consumers review alcoholic beverages. E.g.:
Which brands have the best reviews?
Does white wine or red wine get better reviews?
What words are most commonly associated with each beverage type?
Data Schema
A full schema for the data is available in our support documentation.
This data is for education and training in data science and analysis"	2	3	3	qusaybtoush	wine-beer-and-liquor-reviews
303	303	Binance Future Daily	Binance future 24h aggregated data.	['finance', 'tabular data', 'news']	Binance future 24 hour data fetched from its public API (https://binance-docs.github.io/apidocs/futures/en/#24hr-ticker-price-change-statistics).	0	93	3	code1110	binance-future-daily
304	304	Food   1	Mobile Food Facility Permit	['business', 'data cleaning', 'data visualization', 'food', 'pandas']	"Food
Mobile Food Facility Permit
This data is for education and training in data science and analysis
About the DataSet ,
Mobile Food Facility Permits including name of vendor, location, type of food sold and status of permit. Mobile Food Facility Permit Schedule is here https://data.sfgov.org/d/jjew-r69b
About tasks:
Data processing and analysis with an explanation of the results."	0	3	3	qusaybtoush	food-1
305	305	Football Data from Transfermarkt	Football (Soccer) data scraped from Transfermarkt website	['games', 'football', 'sports', 'computer science']	"TL;DR
&gt; Clean, structured and automatically updated football data from Transfermarkt, including
&gt; * 40,000+ games from many seasons on all major competitions
&gt; * 300+ clubs from those competitions
&gt; * 20,000+ players from those clubs
&gt; * 900,000+ player appearance records from all games
Why did we build it?
Though football (soccer) datasets for match aggregates are widely available, those datasets are usually not regularly updated or maintained. Moreover, structured, publicly available datasets at player appearance level for those matches, such as that contained in player detailed performance page in Transfermarkt, are difficult to find. This dataset aims to present up-to-date football data down to the level of individual player appearances in an accessible, standard format.
What does it contain?
The dataset is composed of multiple CSV files with information on competitions, games, clubs, players and appearances that is automatically updated once a week. Each file contains the attributes of the entity and the IDs that can be used to join them together.
For example, the appearances file contains one row per player appearance, i.e. one row per player per game played. For each appearance you will find attributes such as goals, assists or yellow_cards and IDs referencing other entities within the dataset, such as player_id and game_id.
&gt; Idea: Chekout the mate data.world project Football Data from Transfermarkt for examples of how to use the resources in this dataset to create different types of analysis.
How did we build it?
The source code that maintains this dataset, as well as the data pipeline, is available in Github. On a high level, the project uses transfermarkt-scraper to pull the data from Transfermark website and a set of Python scripts to curate it and publish it here.
&gt; Idea: Watch the transfermarkt-datasets Github repository for updates on improvements and bugfixes on this dataset
What is the status?
This dataset is a live project subject to regular updates and new enhancements. The best way to find out about the different initiatives within this project and their status is to check the Issues section on transfermarkt-datasets.
&gt; Bugfixes and enhancements contributing to this dataset are most welcome. If you want to contribute, the best way to start is by opening a new issue or picking an existing one from the Issues section in Github."	910	7247	37	davidcariboo	player-scores
306	306	Novel Coronavirus COVID-19 (2019-nCoV) 	Data Repository by Johns Hopkins CSSE	['literature', 'healthcare', 'covid19']	"Context
This is a dataset from https://github.com/CSSEGISandData/COVID-19. I added it so that I may use it in my notebook and perform some analysis related to this epidemic.
I dont not own this github repo. I am merely cloning it for educational purposes.
Acknowledgements
https://github.com/CSSEGISandData/COVID-19"	64	2260	2	nabeelhassan	covid19-data-repository-by-johns-hopkins-csse
307	307	Chromium Blink APIs	List of Blink APIs shipped with Chromium	[]	"Chromium API List
This repository contains a single file that will contain the list of Blink APIs
that are shipped with Chromium. The list should be updated roughly every day.
The goal here is to track changes to the APIs over time. There will be some
schema changes that happen along the way, and that will affect whether a simple
'diff' is sufficient to tell what changed between two snapshots.
To build the API list, start with a Chromium checkout and build the
generate_high_entropy_list target. E.g.:
sh
autoninja -C out/Debug generate_high_entropy_list
The list is written to a file named high_entropy_list.csv in out/Debug
or whatever your output directory is."	12	2236	0	asankaherath	chromium-blink-apis
308	308	Prices of Second Hand Cars		[]		1	4	1	nevinselby	prices-of-second-hand-cars
309	309	Comma-200k	Images from Comma 2k19. Images. Just Images. In a tarball.	['arts and entertainment', 'united states', 'intermediate', 'image data', 'transfer learning']	"Context
Dataset extracted from the Comma 2k19 dataset. Images. Just Images.
Training : 220k
Validation: 15k
Why Are they TarBalls? Use Pytorch Webdataset. It allows for a more distributed and faster training with large datasets.
Are you in no mood to torture yourselves? Use version_1 for the simple JPEG files.
Tasks
[ ] Large scale unsupervised pre-traninig
[ ] World-modelling (every 120th frame roughly should be a new sequence)
Who is that guy in the picture?
George Hotz. Don't ask. credits @ Google (Search it yourself)"	0	24	0	neelg007	comma-200k
310	310	Obitos por Doencas Pulmonares	Dataset de obitos por doencas pulmonares na Inglaterra	[]		0	5	0	luizgrneto	bitos-por-doenas-pulmonares
311	311	Re:Zero Rem Anime Faces For GAN Training	725 Unique Rem Face Images	['popular culture', 'arts and entertainment', 'gan', 'image data', 'anime and manga']	"Context
The release of StyleGAN2 ADA made it possible to train a high quality GAN on your own dataset with very limited data. After seeing This Hayasaka Does Not Exist, I wanted to try it out myself from scratch, collecting images to training the data. 
A pretrained anime face model on Danbooru Portraits can be used to get much better looking images with less training time than training on other pre-trained models. 
Content
Images were scraped from Pixiv, cropped with a custom trained YOLOv5 model (found that to work much better than using lbpcascade_animeface). Images smaller than 512x512 were upscaled with waifu2x and cropped to size. Finally, the dataset was hand pruned to select relevant and valid images.
I also have provided a pretrained model which you can use just to generate images without waiting for over 20 hours. 
rem_fakes contains containing 1000 fakes Rem images, generated by StyleGAN2.
Acknowledgements
Inspired by This Hayasaka Does Not Exist.
Making Anime Faces With StyleGAN
Inspiration
Try training the data on your own GAN. 
After generating GAN images, try training a CNN to classify real and fake images. 
How does training with StyleGAN2 ADA differ from transfer learning onto StyleGAN. 
Train the data using another pre-trained model. 
Collect your own dataset and train it with StyleGAN2 ADA."	46	1239	24	andy8744	rezero-rem-anime-faces-for-gan-training
312	312	typing_extensions_3.10.0.2		[]		0	3	1	atamazian	typing-extensions-31002
313	313	CrackDataset_Sample		[]		0	11	0	zekaili	crackdataset-sample
314	314	Snakes species 	Image of 160 different species of snakes 	['classification', 'gan']	"Directory
Csv - Contains csv file with details about training and test set , information regarding the binomial name , country where they are found and etc.
train - Contains folder differentiated by the class_id(check csv) , around 31k images.
test - Contains folder differentiated by the class_id(check csv) ,around 4k images."	117	1241	16	goelyash	165-different-snakes-species
315	315	ump_simple_cnn_20x15_model		[]		0	10	0	juetaochen	ump-simple-cnn-20x15-model
316	316	Google Stock Price-2011-to-2022		[]		1	3	0	avizyt	google-stock-price2011to2022
317	317	MSDS 430 Python	Python Work done for MSDS	[]		0	9	0	trevorspina	msds-430-module-1
318	318	GNCNN_InformationSecurity		[]		0	2	0	yuzhenyu	gncnn-informationsecurity
319	319	ECB speeches etc. 1997 to 2022-01-02	Datamart on CSV by ECB with data up to 2019-10-25, extended up to 2022-01-02	['europe', 'business', 'banking', 'economics']	"Context
I am preparing a book on change to add to my publications (https://robertolofaro.com/published), and I was looking into speeches delivered by ECB, and the search on the website wasn't what I needed.
Content
In late October/early November 2019, ECB posted on Linkedin a link to a CSV dataset extending from 1997 up to 2019-10-25 with all the speeches delivered, as per their website
The dataset was ""flat""- and I needed to both search quickly for associations of people to concepts, and to see directly the relevant speech in a human-readable format (as some speeches had pictures, tables, attachments, etc)
So, I recycled a concept that I had developed for other purposes and used in an experimental ""search by tag cloud on structured content"" on https://robertolofaro.com/BFM2013tag
The result is https://robertolofaro.com/ECBSpeech, that contains information from the CSV file (see website for the link to the source), with the additional information as shown within the ""About this file"".
The concept behind this sharing of the dataset on Kaggle, and releasing on my public website the application I use to navigate date (I have a local Xampp where I use this and other applications to support the research side of my past business and current publication activities) is shared on http://robertolofaro.com/datademocracy
This tag cloud contains the most common words 1997-2020 across the dataset
Acknowledgementsupdate, with items released on the ECB website up to 2021-12-09
current content: 3341 records (2565 speeches, 458 interviews, 259 press conferences, 34 blog posts, 25 ECB Podcasts), released between 1997-02-07 and 2022-01-09
update, with items released on the ECB website up to 2021-12-09
current content: 3341 records (2565 speeches, 458 interviews, 259 press conferences, 34 blog posts, 25 ECB Podcasts), released between 1997-02-07 and 2022-01-09
Thanks to the ECB for saving my time (I was going to copy-and-paste or ""scrape"" with R from the speeches posted on their website) by releasing the dataset https://www.ecb.europa.eu/press/key/html/downloads.en.html
Inspiration
In my cultural and organizational change activities and within data collection, collation, and processing to support management decision-making (including my own) since the 1980s, I always saw that the more data we collect, the less time to retrieve it when needed there is.
I usually worked across multiple environments, industries, cultures, and ""collecting"" was never good enough if I could not then ""retrieve by association"".
In storytelling is fine just to roughly remember ""cameos from the past"", but in data storytelling (or when trying to implement a new organization, process, or even just software or data analysis) being able to pinpoint a source that might have been there before is equally important.
So, I am simply exploring different ways to cross-reference information from different domains, as I am quite confident that within all the open data (including the ECB speeches) there are the results of what niche experts saw on various items.
Therefore, why should time and resources be wasted on redoing what was done from others, when you can start from their endpoint, before adapting first, and adopting then (if relevant)?
Updates
2020-01-25: added GITHUB repository for versioning and release of additional material
as the upload of the new export_datamart.csv wasn't possible, it is now available at: https://github.com/robertolofaro/ecbspeech
changes in the dataset: 
1. fixed language codes
2. added speeches published on the ECB website in January 2020 (up to 2020-01-25 09:00 CET)
3. added all the items listed under the ""interview"" section of the ECB website
current content: 340 interviews, 2374 speeches
2020-01-29: the same file on GITHUB released on 2020-01-25, containing both speeches and interviews, and within an additional column to differentiate between the two, is now available on Kaggle
current content: 340 interviews, 2374 speeches
2020-02-26: monthly update, with items released on the ECB website up to 2020-02-22
current content:  2731 items, 345 interviews, 2386 speeches
2020-03-25: monthly update, with items released on the ECB website up to 2020-03-20
since March 2020, the dataset includes also press conferences available on he ECB website
current content:  2988 records (2392 speeches, 351 interviews, 245 press conferences)
2020-06-07: update, with items released on the ECB website up to 2020-06-07
since June 2020, the dataset includes also press conferences, blog posts, and podcasts available on the ECB website
current content:  3030 records (2399 speeches, 369 interviews, 247 press conferences, 8 blog posts, 7 ECB Podcast).
2020-09-02: update, with items released on the ECB website up to 2020-08-30
current content:  3073 records (2418 speeches, 385 interviews, 248 press conferences, 14 blog posts, 8 ECB Podcast), released between 1997-02-07 and 2020-08-30
2020-12-06: update, with items released on the ECB website up to 2020-12-05
current content:  3149 records (2458 speeches, 410 interviews, 250 press conferences, 19 blog posts, 12 ECB Podcasts), released between 1997-02-07 and 2020-12-05
2021-03-01: update, with items released on the ECB website up to 2021-02-28
current content:  3181 records (2476 speeches, 419 interviews, 252 press conferences, 20 blog posts, 14 ECB Podcasts), released between 1997-02-07 and 2021-02-28
2021-06-13: update, with items released on the ECB website up to 2021-06-13
current content:  3232 records (2501 speeches, 434 interviews, 255 press conferences, 26 blog posts, 16 ECB Podcasts), released between 1997-02-07 and 2021-02-28
2021-09-05: update, with items released on the ECB website up to 2021-09-05
current content:  3268 records (2516 speeches, 446 interviews, 257 press conferences, 30 blog posts, 19 ECB Podcasts), released between 1997-02-07 and 2021-09-05
2021-12-05: update, with items released on the ECB website up to 2021-12-05
current content: 3329 records (2560 speeches, 455 interviews, 258 press conferences, 33 blog posts, 23 ECB Podcasts), released between 1997-02-07 and 2021-12-05
From 2022-01-01, the Kaggle frequencies dataset update will be on a weekly basis, with a monthly ""alignment"" refresh on the speeches part from the ECB speeches data, as since QTR IV 2021 ECB updated on a monthly basis its own dataset (latest ECB dataset update: 2021-12-01)
2022-01-08 attached the excel file containing the cross-check between:
- ECB file as of 2022-01-01
- export data file as 2022-01-02
From 2022-01-09: updated on a weekly basis, see outline on the search by tag cloud website"	89	3824	3	robertolofaro	ecb-speeches-1997-to-20191122-frequencies-dm
320	320	HappyWhale_TF_512X512	HappyWhale_TF_512X512	[]		0	4	0	soumya5891	happywhale-tf-512x512
321	321	munich_osm		[]		0	1	0	tuncbileko	munich-osm
322	322	Palantir - Stock Data - Latest and Updated	Palantir Stock Data - Downloaded through Yahoo! finance API - From IPO	['business', 'finance', 'computer science', 'data cleaning', 'data analytics', 'deep learning', 'investing']	"Palantir Technologies is a public American software company that specializes in big data analytics. Headquartered in Denver, Colorado, it was founded by Peter Thiel, Nathan Gettings, Joe Lonsdale, Stephen Cohen, and Alex Karp in 2003. The company's name is derived from The Lord of the Rings where the magical palantíri were ""seeing-stones,"" described as indestructible balls of crystal used for communication and to see events in other parts of the world.
The company is known for three projects in particular: Palantir Gotham, Palantir Metropolis, and Palantir Foundry. Palantir Gotham is used by counter-terrorism analysts at offices in the United States Intelligence Community (USIC) and United States Department of Defense. In the past, Gotham was used by fraud investigators at the Recovery Accountability and Transparency Board, a former US federal agency which operated from 2009 to 2015. Gotham was also used by cyber analysts at Information Warfare Monitor, a Canadian public-private venture which operated from 2003 to 2012. Palantir Metropolis is used by hedge funds, banks, and financial services firms. Palantir Foundry is used by corporate clients such as Morgan Stanley, Merck KGaA, Airbus, and Fiat Chrysler Automobiles NV.
Palantir's original clients were federal agencies of the USIC. It has since expanded its customer base to serve state and local governments, as well as private companies in the financial and healthcare industries.
Let us analyze and visualize"	79	1189	15	kalilurrahman	palantir-stock-data-latest-and-updated
323	323	Super market sales analysis		[]		1	6	0	valton	super-market-sales-analysis
324	324	CO2 Mauna Loa Weekly	Atmospheric Carbon Dioxide (ppm) monitored weekly at Mauna Loa	['atmospheric science', 'environment']	"Context
I'm fitting and plotting these data to see if Greta Thunberg and other's calls for climate action actually lead to a change from the business-as-usual CO2 growth as we move into/through 2020.
Content
See the NOAA ESRL header information in the data file.
Acknowledgements
These data are NOAA ESRL DATA: ""These data are made freely available to the public and the scientific community in the belief that their wide dissemination will lead to greater understanding and new scientific insights. The availability of these data does not constitute publication of the data.  NOAA relies on the ethics and integrity of the user to ensure that ESRL receives fair credit for their work. ..."""	145	5135	6	dan3dewey	co2-mauna-loa-weekly
325	325	H&M Parquet Table and HDF5 Image Data	Parquet Table and HDF5 Image Data for the H&M Recommendation Competition	['clothing and accessories', 'image data', 'text data']	"Memory Efficient File Format for Fast Data Loading for Images and Tables
The goal of this notebook is to convert all the data, especially the images which are 105.100 Files in 86 Folders, in only one HDF5 file with 86 groups and 105.100 datasets. Since the size of the HDF5 file increases rapidly with the number of images, we convert the images in binary format at first. The csv files will be converted in compressed Parquett files.
Links:
- Link to the dataset.
- Link to parquet homepage.
- Link to hdf5 homepage.
- Link to How to use hdf5 files with keras.
Although the HDF5 file format is very efficient format to read and write image files, the hdf5 file size increases rapidly with the number of images. This happens because the numpy array takes more storage space than the original image files. To overcome this problem, we will store the images as binary files. The strategy is, to create one HDF5 image dataset and group all the subdirectories in that one file. Here is the file structure:
images.h5 (Created HDF5 file)
010 (Group 1: The first three digits of the article_id)
0108775015 (Image 1: article_id)
0108775044 (Image 2: article_id)
...
010 (Group 2)
0110065001 (Image 1)
..."	1	10	0	ismailbaris	hum-parquet-hdf5
326	326	Urdu to Roman Stop words 	Stop words written in urdu translated to roman urdu	[]		0	5	0	kane6543	urdu-to-roman-stop-words
327	327	GP_DATA_WITHOUT_ORDERED_CLASSES_TRANSPOSED		[]		0	7	0	manarmoh	gp-data-without-ordered-classes-transposed
328	328	400+ crypto currency pairs at 1-minute resolution	Historical crypto currency data from the Bitfinex exchange including Bitcoin	['business', 'finance', 'economics', 'investing', 'currencies and foreign exchange']	"About this dataset
With the rise of crypto currency markets the interest in creating automated trading strategies, or trading bots, has grown. Developing algorithmic trading strategies however requires intensive backtesting to ensure profitable performance. It follows that access to high resolution historical trading data is the foundation of every successful  algorithmic trading strategy. This dataset therefore provides open, high, low, close (OHLC) data at 1 minute resolution of various crypto currency pairs for the development of automated trading systems.
Content
This dataset contains the historical trading data (OHLC) of more than 400 trading pairs at 1 minute resolution reaching back until the year 2013. It was collected from the Bitfinex exchange as described in this article.
The data in the CSV files is the raw output of the Bitfinex API. This means, there are no timestamps for time periods in which the exchange was down. Also if there were time periods without any activity or trades there will be no timestamp as well. 
Inspiration
This dataset is intended to facilitate the development of automatic trading strategies. Machine learning algorithms, as they are available through various open source libraries these days, typically require large amounts of training data to unveil their full power. Also the process of backtesting new strategies before deploying them rests on high quality data. Most crypto trading datasets that are currently available either have low temporal resolution, are not free of charge or focus only on a limited number of currency pairs. This dataset on the other hand provides high temporal resolution data of more than 400 currency pairs for the development of new trading algorithms."	12265	96793	454	tencars	392-crypto-currency-pairs-at-minute-resolution
329	329	Employee		[]		0	4	1	asadullahabduljabbar	employee
330	330	LinkedIn Poll Data	Simple Data for visualization	['business', 'beginner', 'data visualization', 'survey analysis', 'tabular data', 'social networks', 'numpy']	"Context
This is collated through a LinkedIN Poll
Content
Basic data columns and details
Acknowledgements
We wouldn't be here without the help of poll takers of the quizzes held in LinkedIN
Inspiration
All the Kagglers and LinkedIn Users"	40	1275	16	kalilurrahman	linkedin-poll-data
331	331	Online content sharing platform	Engagement score prediction	['exploratory data analysis', 'data visualization', 'regression', 'social networks']	"Context
ABC is an online content sharing platform that enables users to create, upload and share the content in the form of videos. It includes videos from different genres like entertainment, education, sports, technology and so on. The maximum duration of video is 10 minutes.
Users can like, comment and share the videos on the platform. 
Based on the user’s interaction with the videos, engagement score is assigned to the video with respect to each user. Engagement score defines how engaging the content of the video is. 
Understanding the engagement score of the video improves the user’s interaction with the platform. It defines the type of content that is appealing to the user and engages the larger audience.
Objective
The main objective of the problem is to develop the machine learning approach to predict the engagement score of the video on the user level."	8	51	6	vardhansiramdasu	online-content-sharing-platform
332	332	Latest Covid-19 India Statewise Data	Covid-19 India Statewise Data as on February 13, 2022	['india', 'social science', 'exploratory data analysis', 'data visualization', 'covid19']	"About
This dataset contains latest Covid-19 India state-wise data as on February 13, 2022. This dataset can be used to analyze covid in India. 
This dataset is great for Exploratory Data Analysis 
Attribute Information
State/UTs - Names of Indian States and Union Territories.
Total Cases - Total number of confirmed cases
Active - Total number of active cases
Discharged - Total number of discharged cases
Deaths - Total number of deaths
Active Ratio (%) - Ratio of number of active cases to total cases
Discharge Ratio (%) - Ratio of number of discharged cases to total cases
Death Ratio (%) - Ratio of number of deaths to total cases
Population  - Population of State/UT
Source
Covid Data : https://www.mygov.in/covid-19
Population Data : https://www.indiacensus.net/
Other Updated Covid Datasets
https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	19572	89862	687	anandhuh	latest-covid19-india-statewise-data
333	333	knowcovid19	KnowCOVID-19 Dataset	[]		0	5	0	rolandoruche	knowcovid19
334	334	Kenya_sign_language		[]		0	1	0	trongminhle	kenya-sign-language
335	335	TensorFlow Kaggle Competition Model  Data		['computer science']		0	7	0	pratyeaggarwal	tensorflow-kaggle-competition-model-data
336	336	my_pets	my pets in home cats and dogs	[]		0	3	0	amitviner	my-pets
337	337	rubert-tiny-toxicity		['health conditions']		0	5	0	elenamarelmi	ruberttinytoxicity
338	338	Global Covid Cases and Deaths	based on data provided by Shad Reynolds	['text data', 'covid19']		9	38	2	janaislu	global-covid-cases-and-deaths
339	339	E-commerce Noisy Reviews Dataset	The dataset comprises of 8,000 Noisy reviews gathered from e-commerce website.  	['linguistics', 'computer science', 'programming', 'beginner', 'advanced', 'nlp', 'text data', 'e-commerce services']	"Context
To perform the task of text normalization, the noisy reviews are hardly discoverable. This dataset provides reviews which contains noisy and their corrected form which can help the community to perform various tasks related to text analysis. 
Content
A sample JSON object is provided below. 
{ 'tid': '1', 'index': '1', 'input': [ 'the', 'cam', 'is', 'gud'], 'output': [ 'the', 'camera', 'is', 'good'] } 
Inspiration
The question is to understand the language more efficiently in recent times where people uses short forms or other word combinations to express their self."	1	27	0	sakshijain23	noisy-amazon-reviews-dataset
340	340	Lead-Scoring-	assign a lead score to each of the leads .	['business', 'education', 'beginner', 'logistic regression', 'tabular data', 'scipy']	"An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. 
The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. 
Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. 
X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.
Goals of the Case Study
There are quite a few goals for this case study.
1) Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.
2) There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well."	19	322	0	ashusri4	leadscoring
341	341	dataOC3		[]		0	0	0	franckhebert	dataoc3
342	342	2winedata		[]		0	3	0	amalianurkahasanah	2winedata
343	343	WineData		[]		0	2	0	amalianurkahasanah	winedata
344	344	King Abdulaziz University Mammogram Dataset	Breast cancer mammogram dataset	['cancer']	"The current study aims to build the first digitalized mammogram dataset for breast cancer in Saudi Arabia, depend on the BI-RADS categories, to solve the availability problem of local public datasets by collecting, categorizing, and annotating mammogram images, supporting the medical field by providing physicians with different diagnosed cases especially in Saudi Arabia
The dataset was collected from Sheikh Mohammed Hussein Al-Amoudi Center of Excellence in Breast Cancer at King Abdulaziz University in Jeddah, Saudi Arabia, from April 2019 to March 2020 and the annotated was between April and June 2020. The dataset contains 1416 cases; all cases include images with two types of views (CC and MLO) for both breasts (right and left), making a total of 5662 mammogram images. The dataset was classified into 1 to 5 categories in accordance with BI-RADS .
The first dataset available here (https://www.kaggle.com/asmaasaad/mammogram-dataset-kaumds)
the authors : Asmaa S. Alsolami, Dr.Wafaa Shalash , Dr.Wafaa Alsaggaf ,Dr. Sawsan Ashoor, Haneen Refaat and Dr.Mohammed Elmogy
Our paper : https://www.mdpi.com/2306-5729/6/11/111 
Acknowledgements
We would like to thank Miss Haneen Refaat and Dr. Sawsan for support us. We also would like to thank Sheikh Mohammed Hussien Al-Amoudi Center of Excellence in Breast Cancer at King Abdul-Aziz University and the Faculty of Computer and Information Technology."	246	1871	7	asmaasaad	king-abdulaziz-university-mammogram-dataset
345	345	Chess (King-Rook vs King)	A Endgame Database for White King and Rook against Black King (KRK)	['games', 'board games', 'computer science', 'classification']	"About this dataset
&gt; <p>Chess Endgame Database for White King and Rook against Black King (KRK).# Source:<br>
Creators:<br>
Database generated by Michael Bain and Arthur van Hoff at the Turing Institute, Glasgow, UK.<br>
Donor:<br>
Michael Bain (mike '@' <a href=""http://cse.unsw.edu.au"" target=""_blank"" rel=""nofollow"">cse.unsw.edu.au</a>), AI Lab, Computer ScienceUniversity of New South Wales, Sydney 2052, Australia.(tel) +61 2 385 3939(fax) +61 2 663 4576</p>
<h1>Data Set Information:</h1>
<p>An Inductive Logic Programming (ILP) or relational learning framework is assumed (Muggleton, 1992). The learning system is provided with examples of chess positions described only by the coordinates of the pieces on the board. Background knowledge in the form of row and column differences is also supplied. The relations necessary to form a correct and concise classifier for the target concept must be discovered by the learning system (the examples already provide a complete extensional definition). The task is closely related to Quinlan's (1983) application of ID3 to classify White King and Rook against Black King and Knight (KRKN) positions as lost 2-ply or lost 3-ply. The framework is similar in that the example positions supply only low-grade data. An important difference is that additional background predicates of the kind supplied in the KRKN study via hand-crafted attributes are not provided for this KRK domain.<br>
Chess endgames are complex domains which are enumerable. Endgame databases are tables of stored game-theoretic values for the enumerated elements (legal positions) of the domain. The game-theoretic values stored denote whether or not positions are won for either side, or include also the depth of win (number of moves) assuming minimax-optimal play. From the point of view of experiments on computer induction such databases provide not only a source of examples but also an oracle (Roycroft, 1986) for testing induced rules. However a chess endgame database differs from, say, a relational database containing details of parts and suppliers in the following important respect. The combinatorics of computing the required game-theoretic values for individual position entries independently would be prohibitive. Therefore all the database entries are generated in a single iterative process using the ``standard backup'' algorithm (Thompson, 1986).<br>
A KRK database was described by Clarke (1977). The current database was described and used for machine learning experiments in Bain (1992; 1994). It should be noted that our database is not guaranteed correct, but the class distribution is the same as Clarke's database. In (Bain 1992; 1994) the task was classification of positions in the database as won for white in a fixed number of moves, assuming optimal play by both sides. The problem was structured into separate sub-problems by depth-of-win ordered draw, zero, one, ..., sixteen. When learning depth d all examples at depths &gt; d are used as negatives. Quinlan (1994) applied Foil to learn a complete and correct solution for this task.<br>
The typical complexity of induced classifiers in this domain suggest that the task is demanding when background knowledge is restricted.</p>
<h1>Attribute Information:</h1>
<ol>
<li>White King file (column)  2. White King rank (row)  3. White Rook file  4. White Rook rank  5. Black King file  6. Black King rank  7. optimal depth-of-win for White in 0 to 16 moves, otherwise drawn {draw, zero, one, two, ..., sixteen}.</li>
</ol>
<h1>Relevant Papers:</h1>
<p>M. Bain. ""Learning optimal chess strategies"", ILP 92: ICOT TM-1182, S. Muggleton, Institute for New Generation Computer Technology, Tokyo, Japan.<br>
M. Bain. Learning Logical Exceptions in Chess. PhD dissertation. University of Strathclyde. 1994.<br>
M. R. B. Clarke. A Quantitative Study of King and Pawn Against King. Advances in Computer Chess, 1, 108-110. M. R. B. Clarke, ed. Edinburgh University Press. Edinburgh. 1977<br>
S. Muggleton. Inductive Logic Programming, 3<em>27. S. Muggleton, ed. Academic Press, London, 1992.<br>
J. R. Quinlan. Learning Efficient Classification Procedures and their Application to Chess End Games.Machine Learning: An Artificial Intelligence Approach. 464</em>482. R. Michalski and J. Carbonnel and T. Mitchell, eds. Tioga, 1983. Palo Alto, CA.<br>
A. J. Roycroft. Database ""Oracles'': Necessary and desirable features. International Computer Chess Association Journal. 8, 2, 1986. 100*104.<br>
K. Thompson. Retrograde Analysis of Certain Endgames.International Computer Chess Association Journal. 8, 3, 1986. 131-139.</p>
<h1>Papers That Cite This Data Set1:</h1>
<p>Manuel Oliveira. Library Release Form Name of Author: Stanley Robson de Medeiros Oliveira Title of Thesis: Data Transformation For Privacy-Preserving Data Mining Degree: Doctor of Philosophy Year this Degree Granted. University of Alberta Library. 2005.</p>
<ul>
<li>Marcus Hutter and Marco Zaffalon. Distribution of Mutual Information from Complete and Incomplete Data. CoRR, csLG/0403025. 2004.</li>
<li>Ira Cohen and Fabio Gagliardi Cozman and Nicu Sebe and Marcelo Cesar Cirelo and Thomas S. Huang. Semisupervised Learning of Classifiers: Theory, Algorithms, and Their Application to Human-Computer Interaction. IEEE Trans. Pattern Anal. Mach. Intell, 26. 2004.</li>
<li>Douglas Burdick and Manuel Calimlim and Jason Flannick and Johannes Gehrke and Tomi Yiu. MAFIA: A Performance Study of Mining Maximal Frequent Itemsets. FIMI. 2003.</li>
<li>James Bailey and Thomas Manoukian and Kotagiri Ramamohanarao. Fast Algorithms for Mining Emerging Patterns. PKDD. 2002.</li>
<li>Russell Greiner and Wei Zhou. Structural Extension to Logistic Regression: Discriminative Parameter Learning of Belief Net Classifiers. AAAI/IAAI. 2002.</li>
<li>Tanzeem Choudhury and James M. Rehg and Vladimir Pavlovic and Alex Pentland. Boosting and Structure Learning in Dynamic Bayesian Networks for Audio-Visual Speaker Detection. ICPR (3). 2002.</li>
<li>Marco Zaffalon and Marcus Hutter. Robust Feature Selection by Mutual Information Distributions. CoRR, csAI/0206006. 2002.</li>
<li>Michael G. Madden. Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm. CoRR, csLG/0211003. 2002.</li>
<li>Jie Cheng and Russell Greiner. Learning Bayesian Belief Network Classifiers: Algorithms and System. Canadian Conference on AI. 2001.</li>
<li>Boonserm Kijsirikul and Sukree Sinthupinyo and Kongsak Chongkasemwongse. Approximate Match of Rules Using Backpropagation Neural Networks. Machine Learning, 44. 2001.</li>
<li>Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao and Limsoon Wong. DeEPs: A New Instance-based Discovery and Classification System. Proceedings of the Fourth European Conference on Principles and Practice of Knowledge Discovery in Databases. 2001.</li>
<li>Jinyan Li and Guozhu Dong and Kotagiri Ramamohanarao. Instance-Based Classification by Emerging Patterns. PKDD. 2000.</li>
<li>Mark A. Hall. Department of Computer Science Hamilton, NewZealand Correlation-based Feature Selection for Machine Learning. Doctor of Philosophy at The University of Waikato. 1999.</li>
<li>Adam J. Grove and Dale Schuurmans. Boosting in the Limit: Maximizing the Margin of Learned Ensembles. AAAI/IAAI. 1998.</li>
<li>Yk Huhtala and Juha Kärkkäinen and Pasi Porkka and Hannu Toivonen. Efficient Discovery of Functional and Approximate Dependencies Using Partitions. ICDE. 1998.</li>
<li>Ron Kohavi. Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid. KDD. 1996.</li>
<li>Brian R. Gaines. Structured and Unstructured Induction with EDAGs. KDD. 1995.</li>
<li>Ron Kohavi and Dan Sommerfield. Feature Subset Selection Using the Wrapper Method: Overfitting and Dynamic Search Space Topology. KDD. 1995.</li>
<li>Omid Madani and David M. Pennock and Gary William Flake. Co-Validation: Using Model Disagreement to Validate Classification Algorithms. Yahoo! Research Labs.</li>
<li>M. A. Galway and Michael G. Madden. DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm. Department of Information Technology National University of Ireland, Galway.</li>
<li>BayesianClassifi552 Pat Langley and Wayne Iba. In Proceedings of the Tenth National ConferenceonArtifi256 Intelligence( 42840. Lambda Kevin Thompson.</li>
<li>Jerome H. Friedman and Ron Kohavi and Youngkeol Yun. To appear in AAAI-96 Lazy Decision Trees. Statistics Department and Stanford Linear Accelerator Center Stanford University.</li>
<li>Grigorios Tsoumakas and Ioannis P. Vlahavas. Fuzzy Meta-Learning: Preliminary Results. Greek Secretariat for Research and Technology.</li>
<li>Nikunj C. Oza and Stuart J. Russell. Online Bagging and Boosting. Computer Science Division University of California.</li>
<li>Hankil Yoon and Khaled A. Alsabti and Sanjay Ranka. Tree-based Incremental Classification for Large Datasets. CISE Department, University of Florida.</li>
</ul>
<h1>Citation Request:</h1>
<p>Please refer to the Machine Learning <a href=""http://archive.ics.uci.edu/ml/citation_policy.html"" target=""_blank"" rel=""nofollow"">Repository's citation policy.</a><br>
[1] Papers were automatically harvested and associated with this data set, in collaborationwith <a href=""http://rexa.info/"" target=""_blank"" rel=""nofollow"">Rexa.info</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Chess+%28King-Rook+vs.+King%29"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Chess+(King-Rook+vs.+King)</a></p>
This dataset was created by UCI and contains around 30000 samples along with Draw, 3, technical information and other features such as:
- C
- B
- and more.
How to use this dataset
&gt; - Analyze A in relation to 1
- Study the influence of 2 on Draw
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	13	175	4	yamqwe	chess-king-rook-vs-kinge
346	346	Clinical BERT		['health']		0	2	0	tatsukisato	clinical-bert
347	347	trialanderror924_video_data	Video data extracted from its Youtube channel and Wiki page	[]	"Content
The video data from the beginning of the YouTube channel https://www.youtube.com/channel/UCcfdsRaC34Y4IKTiihYxBrQ till 01-Nov-2021, extracted through YouTube Data API v3 and Wiki page https://zh.wikipedia.org/wiki/%E8%A9%A6%E7%95%B6%E7%9C%9F.
(special credit to https://wikitable2csv.ggor.de, which is a very convenient tool of transforming Wiki tables into CSVs)
Notes
[2022-01-08]
The video ""試乜都得《預防HPV特約：修爺深夜會客室 feat. Dr. HUI YIN》"" (id = e1OEAuWWVIc) has its comments turned off, happened between 06-Jan-2022 and 08-Jan-2022 morning HKT.  The commentCount of this video will be marked as ""-1"" from now on.
[2021-12-20]
Please note that Youtube has masked the field ""dislikeCount"" from the response of statistics API call.  The value of ""dislikeCount"" will be filled with ""0"" since 2021-12-20."	4	390	0	edmundtyliu	trialanderror924-videos-data
348	348	my_ubidata		[]		0	3	0	zhouyiheng	my-ubidata
349	349	submit-files		['arts and entertainment']		0	0	0	kihan1234	submitfiles
350	350	Augmented_health_Heart_Rate		['biology', 'health', 'tabular data', 'text data', 'health conditions']		1	38	5	shaukathussain	augmented-health-heart-rate
351	351	Breweries & Brew Pubs in the USA	This is a list of over 7,000 breweries and brewpubs in the USA 	['alcohol', 'business']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of over 7,000 breweries and brewpubs in the USA provided by <a href=""https://datafiniti.co/products/business-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Business Database</a>. The dataset includes the category, name, address, city, state, and more for each listing.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with This Data</h1>
<p>You can use this geographical and categorical information for business locations to <a href=""https://datafiniti.co/craft-beer-popular-america/"" target=""_blank"" rel=""nofollow"">determine which cities and states have the most breweries</a>. E.g.:</p>
<ul>
<li>What is the number of breweries in each state?</li>
<li>What are the cities with the most breweries per person?</li>
<li>What are the states with the most breweries per person?</li>
<li>What are the top cities for breweries?</li>
<li>What are the top states for breweries?</li>
<li>What industry categories are typically grouped with breweries?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/business-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/business-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 20000 samples along with City, Menus, technical information and other features such as:
- Latitude
- Websites
- and more.
How to use this dataset
&gt; - Analyze Postal Code in relation to Address
- Study the influence of Longitude on Hours
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	39	222	5	yamqwe	breweries-brew-pubs-in-the-usae
352	352	Zillow Houses: Buy vs Rent	Dataset containing data from 2017 Q1 to 2017Q1	['housing', 'business', 'real estate']	"About this dataset
&gt; <h2>Additional Data Products</h2>
<p>Date: 2017 Q1</p>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Additional Data Products</h2>
<ul>
<li>Zillow Home Value Forecast (ZHVF): The ZHVF is the one-year forecast of the ZHVI. Our forecast methodology is <a href=""http://www.zillow.com/research/2013/01/24/zillow-home-value-forecast-methodology-2/"" target=""_blank"" rel=""nofollow"">methodology post</a>.</li>
<li>Zillow creates our negative equity data using our own data in conjunction with data received through our partnership with TransUnion, a leading credit bureau. We match estimated home values against actual outstanding home-related debt amounts provided by TransUnion. To read more about how we calculate our negative equity metrics, please see our <a href=""http://www.zillow.com/research/methodology-negative-equity-3180/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Cash Buyers: The share of homes in a given area purchased without financing/in cash. To read about how we calculate our cash buyer data, please see our <a href=""http://www.zillow.com/research/top-markets-for-cash-purchases-9696/"" target=""_blank"" rel=""nofollow"">research brief</a>.</li>
<li>Mortgage Affordability, Rental Affordability, Price-to-Income Ratio, Historical ZHVI, Historical ZHVI and Houshold Income are calculated as a part of Zillow’s quarterly Affordability Indices. To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment). Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. For quarters where median income is not available from the U.S. Census Bureau, we calculate future quarters of median household income by estimating it using the Bureau of Labor Statistics’ Employment Cost Index. The affordability forecast is calculated similarly to the current affordability index but uses the one year Zillow Home Value Forecast instead of the current Zillow Home Value Index and a specified interest rate in lieu of PMMS. It also assumes a 20 percent down payment. We calculate rent affordability similarly to mortgage affordability; however we use the Zillow Rent Index, which tracks the monthly median rent in particular geographical regions, to capture rental prices. Rents are chained back in time by using U.S. Census Bureau American Community Survey data from 2006 to the start of the Zillow Rent Index, and Decennial Census for all other years.</li>
<li>The mortgage rate series is the average mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate mortgage in 15-minute increments during business hours, 6:00 AM to 5:00 PM Pacific. It does not include quotes for jumbo loans, FHA loans, VA loans, loans with mortgage insurance or quotes to consumers with credit scores below 720. Federal holidays are excluded. The jumbo mortgage rate series is the average jumbo mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate, jumbo mortgage in one-hour increments during business hours, 6:00 AM to 5:00 PM Pacific Time. It does not include quotes to consumers with credit scores below 720. Traditional federal holidays and hours with insufficient sample sizes are excluded.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 30000 samples along with Breakeven, County Name, technical information and other features such as:
- Metro
- Cbsa Title
- and more.
How to use this dataset
&gt; - Analyze Be Prop Count in relation to Region Name
- Study the influence of State Name on Size Rank
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	45	408	6	yamqwe	buy-vs-rente
353	353	NSE 200 scripts OHLC daily (2000-2022)	22 years of daily prices of 200 stocks.(NIFTY50, NIFTYNEXT50 ,NIFTYMIDCAP100)	['business', 'finance', 'investing', 'news']	"Context
I struggled to find adjusted historical prices of NSE top shares. I spend lot of time to find and download this information. This data is clean, with adjusted prices and complete. 
Content
National Stock Exchange of India Limited (NSE) is the leading stock exchange of India, located in Mumbai, Maharashtra. It is world’s largest derivatives exchange in 2021 by number of contracts traded based on the statistics maintained by Futures Industry Association (FIA), a derivatives trade body.
This data contains 20 years of OHLCV daily prices of stocks from following indices:
1) NIFTY 50 : The NIFTY 50 is a benchmark Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange.
2) NIFTY NEXT 50 : The Nifty Next 50 is an index that represents the performance of the next 50 stocks that come after the top 50 stock in the order of the float market capitalization, and they can even be potential candidates for inclusion in Nifty 50 in the future.
3) NIFTY MIDCAP 100: The objective of the NIFTY Midcap 100 Index is to capture the movement and be a benchmark of the midcap segment of the market. The NIFTY Midcap 100 Index represents about 9.9% of the market capitalization of the stocks listed on NSE as on March 29, 2019.
Acknowledgements
I used many online python packages. and finally investpy is the only package which can gives such rich information. ""investpy"" is a Python package to retrieve data from Investing.com
Inspiration
I would like to ask:
1) Can you predict long entry points with high Alpha and low Beta? e.g. trade which can give me 10% profit with 2.5% - 3% stoploss in next few trading sessions(between 5-22)? 
2) Can you predict outcome of tomorrow's file which can beat benchmark of present price."	7	120	0	bhavesh09	nse-200-scripts-ohlc-daily-20002022
354	354	Images from Hobbit Movies	1.8k+ picutres from three movies	['popular culture', 'arts and entertainment', 'literature', 'movies and tv shows', 'image data']	"Context
The Hobbit is a book written by Tolkien, which became the basis of the well-known and popular film trilogy of the same name directed by Peter Jackson. The moviess quickly became popular and profitable like the ""Lord of the Rings"" series, but were not recognized by viewers as positively as the series from several years ago. The database below presents a series of photos obtained on the largest portal that rates IMDb films from each of the three parts of the film trilogy.
Content
The data is grouped into folders where one folder corresponds to one movies. We have a total of 3, as we took the all 3 parts of trilogy. In each of the folders we can find pictures from pcitures from a specific movie. The number of pictures in each series differ from each other during the data on imdb. Data was obtained using webscraping. Python was used for this process with the ""BeautifulSoup"", ""requests"", ""re"", ""urllib"", and ""os"" packages. For each movie, using the preview of the HTML page, we were able to find direct links to the photos. Then, in a loop, each photo was downloaded in turn and saved to the appropriate folder with the appropriate name based on the link and the title of the part.
Inspiration
The inspiration to create a notebook based on this data may be a comparative analysis of the movies based on the pictures or the creation of a model which, based on the pictures, can predict which part they come from.
Photo by <a href=""https://unsplash.com/@timreb9?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Tim Rebkavets</a> on <a href=""https://unsplash.com/s/photos/fantasy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	1	51	6	michau96	images-from-hobbit-movies
355	355	Moroccan Universities Dataset		['universities and colleges']		0	2	1	lseddir	moroccan-universities-dataset
356	356	Boston housing		['social issues and advocacy']		4	18	3	rocklen	boston-housing
357	357	trading		['investing']		0	3	0	danilalapokin	trading
358	358	tfrecords-v1-whale2-cropped-dataset-128x128		[]		0	6	0	slow5620	tfrecords-v1-whale2-cropped-dataset-128x128
359	359	U.S. Influenza Surveillance Report	Dataset for analyzing the Influence of Influenza on age 65+	['united states', 'public health', 'health', 'public safety', 'covid19']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://www.cdc.gov/flu/weekly/weeklyarchives2017-2018/images/ILI21_small.gif"" alt="""" style=""""></p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE: <a href=""https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html"" target=""_blank"" rel=""nofollow"">CDC</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
<h1><strong>Notes</strong></h1>
<ol>
<li>Data as of 5 June 2018</li>
<li>Data at the national level - Regional and State level data is available <a href=""https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html"" target=""_blank"" rel=""nofollow"">here</a></li>
</ol>
This dataset was created by Andy Kriebel and contains around 1000 samples along with Week, Season, technical information and other features such as:
- Age 25 49
- % Weighted Ili
- and more.
How to use this dataset
&gt; - Analyze Age 5 24 in relation to Year
- Study the influence of Age 65 on %unweighted Ili
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	22	110	3	yamqwe	2018-w25-u-s-influenza-surveillance-reporte
360	360	Russian literature	A collection of Russian literature	['literature', 'russia', 'nlp', 'text data']	"Content
This repository contains a collection of Russian literature in txt format (all in UTF-8 encoding).  In addition, for each author there is a csv file containing information about the year of writing of each work.
This dataset was created for a project to determine the authorship of a piece of text, but I'm sure that you can use this dataset for anything 😉.
The main feature that allows this dataset to be used for any purpose is that the data is not processed at all. The text has not been pre-processed in any way, the designations of authors, chapters and references to the translation of foreign inserts have not been removed.
Acknowledgements
Thanks Ilibrary, LitLib, Wikisource and all-all-all."	797	4675	36	d0rj3228	russian-literature
361	361	efficientnet	Contains a Keras (and TensorFlow Keras) reimplementation of EfficientNet	[]		1	494	0	lucca9211	efficientnet
362	362	National Parks Sorted by Total Attendance & Year	20K Samples along with state, attendance, year, technical info	['travel']	"About this dataset
&gt; <p>from:</p>
 This dataset was created by Gary Hoover and contains around 20000 samples along with State, Unit Type, technical information and other features such as:
- Unit Code
- Visitors
- and more.
How to use this dataset
&gt; - Analyze Year Raw in relation to Unit Name
- Study the influence of State on Unit Type
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	28	127	1	yamqwe	national-parks-sorted-by-total-attendance-and-bye
363	363	img-quality	Repository for NIMA Implementation	['programming']		2	582	0	maindolaamit	imgquality
364	364	Cyclistic Dataset	Bicycle ride data for the year 2021	['cycling', 'beginner', 'data cleaning', 'data visualization', 'data analytics']		0	8	0	aryanbarnwal52	google-capstone
365	365	my_dataset		[]		1	2	0	sabbo26	my-dataset
366	366	California WIC Vendors	Dataset provided by the US Health and Human Services Agency	['nutrition', 'public health', 'health', 'food']	"About this dataset
&gt; <p>The <a href=""http://www.fns.usda.gov/wic/women-infants-and-children-wic"" target=""_blank"" rel=""nofollow"">Women, Infants and Children (WIC) Program</a> is a federally-funded health and nutrition program that provides assistance to pregnant women, new mothers, infants and children under age five. WIC helps California families by providing food instruments and vouchers that can be used to purchase healthy supplemental foods from over 4000 WIC-authorized vendor stores throughout the State. WIC also provides nutritional education, breastfeeding support and help finding healthcare and other community services. Participants must meet income guidelines and other criteria. Currently, 84 WIC agencies provide services monthly to over 1.45 million participants at over 650 sites in local communities throughout the State.</p>
<p>This dataset contains the vendor name and location of all currently active vendors authorized to offer supplemental foods to participants for redemption with WIC food instruments and vouchers.</p>
<h3>About CHHS Open Data</h3>
<p>The role of the <a href=""http://www.chhs.ca.gov/Pages/Home.aspx"" target=""_blank"" rel=""nofollow"">California Health and Human Services</a> (CHHS) Agency is to provide policy leadership and direction to the departments and programs it oversees, to reduce duplication and fragmentation and improve coordination among the departments, to ensure programmatic integrity, and to advance the Governor's priorities on health and human services issues. The California Health and Human Services Agency (CHHS) has launched its <a href=""https://chhs.data.ca.gov/"" target=""_blank"" rel=""nofollow"">Open Data Portal</a> initiative in order to increase public access to one of the State’s most valuable assets – non-confidential health and human services data. Its goals are to spark innovation, promote research and economic opportunities, engage public participation in government, increase transparency, and inform decision-making. ""Open Data"" describes data that are freely available, machine-readable, and formatted according to national technical standards to facilitate visibility and reuse of published data.</p>
<p>Citation: <em>California Women, Infants and Children Program. 2014. WIC Authorized Vendors-2014-CA-CDPH.</em></p>
<p><a href=""https://chhs.data.ca.gov/Facilities-and-Services/Women-Infants-and-Children-WIC-Authorized-Vendors/x5nq-b49e/data"" target=""_blank"" rel=""nofollow"">DATASET</a></p>
This dataset was created by California Health and Human Services and contains around 4000 samples along with Vendor, Address, technical information and other features such as:
- County
- Location
- and more.
How to use this dataset
&gt; - Analyze Zip Code in relation to City
- Study the influence of Second Address on Vendor
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	1	33	3	yamqwe	california-wic-vendorse
367	367	Clouds GAN Dataset	1628 Unique Cloud Images	['art', 'artificial intelligence', 'gan', 'image data']	"Content
Images were scraped from Flickr and resized to 512x512 (center cropped). An improvement on the dataset could be to find specific types of cloud formations so the GAN generalizes even better. 
Acknowledgements
Images were scraped using pyimgdata"	0	41	4	andy8744	clouds-gan-dataset
368	368	happywhale-tfrecords-v1-512x512		[]		0	1	0	sega1031	happywhale-tfrecords-v1-512x512
369	369	datatask_6000		[]		0	5	0	punitg07	datatask-6000
370	370	datatask		[]		0	3	0	punitg07	datatask
371	371	Facebook Stock Data - Live and Latest	Facebook Stock info downloaded using Yahoo! finance API. Updated regularly	['finance', 'computer science', 'deep learning', 'investing', 'social networks']	"Facebook is an American online social media and social networking service owned by Facebook, Inc.
Founded in 2004 by Mark Zuckerberg with fellow Harvard College students and roommates Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes, its name comes from the face book directories often given to American university students. Membership was initially limited to Harvard students, gradually expanding to other North American universities and, since 2006, anyone over 13 years old. As of 2020, Facebook claimed 2.8 billion monthly active users, and ranked seventh in global internet usage. It was the most downloaded mobile app of the 2010s."	416	3800	20	kalilurrahman	facebook-stock-data-live-and-latest
372	372	MasterCard Stock Data - Latest and Updated	MasterCard Stock Data - Downloaded using a Python Script and Yahoo! Finance API	['business', 'finance', 'computer science', 'linear regression', 'investing']	"Mastercard Inc. (stylized as MasterCard from 1979 to 2016 and MasterCard since 2016) is an American multinational financial services corporation headquartered in the Mastercard International Global Headquarters in Purchase, New York. The Global Operations Headquarters is located in O'Fallon, Missouri, a municipality of St. Charles County, Missouri. Throughout the world, its principal business is to process payments between the banks of merchants and the card-issuing banks or credit unions of the purchasers who use the ""Mastercard"" brand debit, credit, and prepaid cards to make purchases. Mastercard Worldwide has been a publicly traded company since 2006. Prior to its initial public offering, Mastercard Worldwide was a cooperative owned by the more than 25,000 financial institutions that issue its branded cards.
Mastercard, originally known as Interbank from 1966 to 1969 and Master Charge from 1969 to 1979, was created by an alliance of several regional bank card associations in response to the BankAmericard issued by Bank of America, which later became the Visa credit card issued by Visa Inc.
Mastercard is one of the best performing stocks of the decade of 2011-2020"	439	4254	27	kalilurrahman	mastercard-stock-data-latest-and-updated
373	373	keras-applications	Keras deep learning library	['earth and nature']		1	501	0	lucca9211	kerasapplications
374	374	Covid-19 Pandemic Dataset	Pandemic dataset w/ created and merged attributes(active cases, population etc.)	['global', 'health', 'data cleaning', 'tabular data', 'covid19', 'pandas']	"Context
This dataset is a product of a GitHub repository. Sources and methodology can be seen there step by step. Inconsistent values(negative increase etc.) are also explained there.
Please note that the data of Turkey are unstable for 3 months. The Ministry changed its definition of cases at some point and only the symptomatic cases were announced as confirmed cases in this duration. If you need this data you can check my estimations for this gap and here is my methodology to come up with them. You can also check the exact timeline for Turkey's data in these. These estimations were not placed in this dataset. 
Acknowledgements
Most of the data in this dataset is taken from ""COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University"", here is the link for the repo.
Covid-19 data of France are taken from ""Ministère des Solidarités et de la Santé"" (Ministry of Health in France).
Population data are taken from United Nations. Here is the link for it.
Inspiration
I've needed to create a Covid-19 dataset for my design project at my university. Here is the dashboard as one of its products(out of date). For many reasons, I've faced a lot of problems during the processes. After I've finished it; I've decided to put the data collection part to Python, to create something that I can easily update and show the solutions of the problems that challenged me the most. The resulting dataset can be useful here."	272	3361	25	ocaktan	covid19pandemic-dataset
375	375	US Hospital Locations	Hospitals in the US states and territories of Puerto Rico	['healthcare', 'health', 'hospitals and treatment centers', 'covid19']	"About this dataset
&gt; <p>This dataset is sourced from the HIFLD open data portal, from the US Dept of Homeland Security.</p>
<p>""This feature class/shapefile contains locations of Hospitals for 50 US states, Washington D.C., US territories of Puerto Rico, Guam, American Samoa, Northern Mariana Islands, Palau, and Virgin Islands. The dataset only includes hospital facilities based on data acquired from various state departments or federal sources which has been referenced in the SOURCE field. Hospital facilities which do not occur in these sources will be not present in the database. The source data was available in a variety of formats (pdfs, tables, webpages, etc.) which was cleaned and geocoded and then converted into a spatial database. The database does not contain nursing homes or health centers. Hospitals have been categorized into children, chronic disease, critical access, general acute care, long term care, military, psychiatric, rehabilitation, special, and women based on the range of the available values from the various sources after removing similarities. In this update the TRAUMA field was populated for 172 additional hospitals and helipad presence were verified for all hospitals."" (This description is copied from the source.)</p>
<p><strong><em>Source:</em></strong> <a href=""https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals"" target=""_blank"" rel=""nofollow"">https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals</a></p>
<p><strong><em>License:</em></strong> public domain</p>
<p><strong><em>Updated</em></strong>: synced from source weekly. As of 4/27/20 source last updated 10/7/219.</p>
This dataset was created by Liz Friedman and contains around 8000 samples along with X, Naics Code, technical information and other features such as:
- County
- Longitude
- and more.
How to use this dataset
&gt; - Analyze Helipad in relation to Type
- Study the influence of Val Method on Country
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Liz Friedman 
Start A New Notebook!"	12	102	4	yamqwe	us-hospital-locationse
376	376	Forex data since 2011-1-1	Daily updated 53 currencies - per USD	['business', 'finance', 'currencies and foreign exchange']	"Daily Forex Data Since 2011-01-01
The currency data for each day will be added automatically the next day at 12 PM UTC via GitHub actions. 
The currencies in the data (per USD)
Argentine Peso
Australian Dollar
Bahraini Dinar
Botswana Pula
Brazilian Real
British Pound
Bruneian Dollar
Bulgarian Lev
Canadian Dollar
Chilean Peso
Chinese Yuan Renminbi
Colombian Peso
Croatian Kuna
Czech Koruna
Danish Krone
Emirati Dirham
Euro
Hong Kong Dollar
Hungarian Forint
Icelandic Krona
Indian Rupee
Indonesian Rupiah
Iranian Rial
Israeli Shekel
Japanese Yen
Kazakhstani Tenge
Kuwaiti Dinar
Libyan Dinar
Malaysian Ringgit
Mauritian Rupee
Mexican Peso
Nepalese Rupee
New Zealand Dollar
Norwegian Krone
Omani Rial
Pakistani Rupee
Philippine Peso
Polish Zloty
Qatari Riyal
Romanian New Leu
Russian Ruble
Saudi Arabian Riyal
Singapore Dollar
South African Rand
South Korean Won
Sri Lankan Rupee
Swedish Krona
Swiss Franc
Taiwan New Dollar
Thai Baht
Trinidadian Dollar
Turkish Lira
Venezuelan Bolivar"	316	4079	23	altinsoyemrecan	daily-updated-forex-data-since-201111
377	377	clean_business_csv_1		[]		0	4	0	hemanishah	clean-business-csv-1
378	378	cots_valid_tfrecord		[]		0	17	0	jobayerhossain	cots-valid-tfrecord
379	379	cots_train_tfreocrd		[]		0	22	0	jobayerhossain	cots-train-tfreocrd
380	380	miBBCnews		[]		0	0	0	fernandobordi	mibbcnews
381	381	numeraiPerf		[]		168	913	0	robwishart	numeraiperf
382	382	2010 Prescription Drug Profiles	Prescription Drug Profiles PUF drawn from Medicare Drug Types 	['healthcare', 'health', 'drugs and medications']	"About this dataset
&gt; <p>This release contains the Prescription Drug Profiles Public Use Files (PUFs) drawn from Medicare prescription drug claims for the year of the date on which the prescription was filled. The PUFs includes beneficiary demographics (e.g., gender, age), plan characteristics (e.g., plan type, drug benefit type, gap coverage), prescription drug characteristics (i.e., active ingredient, drug class), prescriber characteristics, and payment information (e.g., average patient pay amount).</p>
<p>The CMS Prescription Drug Profiles PUFs are aggregated files in which each record is a profile or cell defined by the characteristics of Medicare beneficiaries, drugs, plans, and prescribers.</p>
<p>The Prescription Drug Profiles PUFs represents 100% of prescription drug claims (subject to suppression) for Medicare beneficiaries covered by Part D for the reference year.  These files provide various measures of utilization as averages for different groups of Medicare beneficiaries, or profiles. Prescription drug events (or Part D Drug Event Files) are merged with the following files to construct the Prescription Drug Profiles PUF for the reference year:</p>
<p>Then, the information is aggregated into profiles defined by 14 variables: (1) Gender; (2) Age category; (3) Drug name; (4) Drug RxNorm Concept Unique Identifier RxCUI; (5) Drug class; (6) Drug major class;  (7) Drug type, (8) Plan type, (9) Coverage type; (10) Benefit phase; (11) Drug benefit type; (12) Prescriber taxonomy; (13) Plan gap coverage indicator; and (14) Tier identifier.  Finally, number of events and average utilization measures for each combination of profile variables are calculated.</p>
<p>Source:<br>
<a href=""https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/BSAPUFS/Prescription_Drug_Profiles.html"" target=""_blank"" rel=""nofollow"">https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/BSAPUFS/Prescription_Drug_Profiles.html</a><br>
Centers for Medicare &amp; Medicaid Services</p>
This dataset was created by Liz Petrocelli and contains around 2700000 samples along with Pde Drug Type Cd, Bene Age Cat Cd, technical information and other features such as:
- Prescriber Type
- Gap Coverage
- and more.
How to use this dataset
&gt; - Analyze Mean Rxhcc Score in relation to Ave Ptnt Pay Amt
- Study the influence of Benefit Phase on Tier Id
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Liz Petrocelli 
Start A New Notebook!"	11	106	4	yamqwe	2010-prescription-drug-profilese
383	383	happywhale-tfrecords-cropped-768x768-v2		[]		0	3	0	mofumofuchan	happywhale-tfrecords-cropped-768x768-v2
384	384	Forex tick data huge database since april 2020	Forex major currencies bid/ask tick data since april 2020	['banking', 'economics', 'investing', 'currencies and foreign exchange']	"Content
Below, you’ll find the major currency pairs with tick-by-tick historical rates. This data is top-of-book, tick-by-tick market data, with fractional pip spreads in millisecond detail. All timestamps are based on GMT."	205	2524	6	joseserrat	forex-april-2020-to-june-2021-tick-data
385	385	Global Housing Valuation Watch	Global Housing Watch tracks developments across the globe on a quarterly basis.	['housing', 'business', 'finance', 'real estate', 'economics']	"About this dataset
&gt; <p>The Global Housing Watch tracks developments in housing markets across the world on a quarterly basis. It provides current data on house prices as well as metrics used to assess valuation in housing markets, such as house price-to-rent and house price-to-income ratios.</p>
<p><strong>Latest Update Date as of Download:</strong> 05/03/2017</p>
<hr>
<p>The Data is provided to Users “as is” and without warranty of any kind, either express or implied, including, without limitation, warranties of merchantability, fitness for a particular purpose and noninfringement.</p>
<p>The IMF Data is available free of charge from the IMF.</p>
<p>Users are prohibited from infringing upon the integrity of the IMF data and in particular shall refrain from any act of alteration of the IMF data that intentionally affects its nature or accuracy. If the IMF data is materially transformed by the user, this must be stated explicitly along with the required source citation.</p>
<p>All other terms set forth in the IMF's general terms and conditions (<a href=""http://www.imf.org/external/terms.htm"" target=""_blank"" rel=""nofollow"">http://www.imf.org/external/terms.htm</a>) shall continue to apply to use of IMF Data.</p>
<p><strong><em>Source:</em></strong> <a href=""http://www.imf.org/external/research/housing/index.htm"" target=""_blank"" rel=""nofollow"">http://www.imf.org/external/research/housing/index.htm</a></p>
This dataset was created by International Monetary Fund and contains around 0 samples along with Latest Quarter, Latest Quarter, technical information and other features such as:
- Latest Quarter
- Latest Quarter
- and more.
How to use this dataset
&gt; - Analyze Latest Quarter in relation to Latest Quarter
- Study the influence of Latest Quarter on Latest Quarter
- More datasets
Acknowledgements
If you use this dataset in your research, please credit International Monetary Fund 
Start A New Notebook!"	18	155	5	yamqwe	global-housing-watche
386	386	yolos_video_fold		[]		53	233	0	shigengtian	yolos-video-fold
387	387	VideoUserEngagementScoreData		[]		0	0	0	shilpaar	videouserengagementscoredata
388	388	Zillow Homes: Buyer-Seller Index	The dataset contains information about homes sold on Zillow.	['housing', 'business', 'real estate']	"About this dataset
&gt; <h2>Other Metrics</h2>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Other Metrics</h2>
<ul>
<li>Median List Price ($): Median of the list price (or asking price) for homes listed on Zillow.</li>
<li>Median Sale Price ($): Median of the selling price for all homes sold in a given region.</li>
<li>Median List Price Per Sq Ft ($): Median of list prices divided by the square footage of a home.</li>
<li>Median Sale Price Per Sq Ft ($): Median of sale prices divided by the square footage of a home.</li>
<li>Sold for Loss (%): The percentage of homes in an area that sold for a price lower than the previous sale price.</li>
<li>Sold for Gain (%): The percentage of homes in an area that sold for a price higher than the previous sale price.</li>
<li>Increasing Values (%): The percentage of homes in an given region with values that have increased in the past year.</li>
<li>Decreasing Values (%): The percentage of homes in an given region with values that have decreased in the past year.</li>
<li>Listings With Price Cut (%): The percentage of current listings on Zillow with a price cut during the month.</li>
<li>Median price cut (%): Median of the percentage price reduction for homes with a price reduction during the month.</li>
<li>Sold in Past Year (Turnover) (%): The percentage of all homes in a given area that sold in the past 12 months.</li>
<li>Homes Foreclosed: The number of homes (per 10,000 homes) that were foreclosed upon in a given month. A foreclosure occurs when a homeowner loses their home to their lending institution or it is sold to a third party at an auction.</li>
<li>Foreclosure Re-Sales (%): The percentage of home sales in a given month in which the home was foreclosed upon within the previous year (e.g. sales of bank-owned homes after the bank took possession of a home following a foreclosure).</li>
<li>Sale-to-List Price Ratio: Median of the ratio between the sale price and the list price for all homes (e.g. if a home with a list price of $200k sells for $250k, its ratio would be 5:4 or 1.25).</li>
<li>Inventory (Raw): Median of weekly snapshot of for-sale homes within a region for a given month.</li>
<li>Age of Inventory: Each Wednesday, age of inventory is calculated as the median number of days all active listings as of that Wednesday have been current. These medians are then aggregated into the number reported by taking the median across weekly values.</li>
<li>Price to Rent Ratio: This ratio is first calculated at the individual home level, where the estimated home value is divided by 12 times its estimated monthly rent price. The the median of all home-level price-to-rent ratios for a given region is then calculated.</li>
<li>Buyer/Seller Index: This index combines the sale-to-list price ratio, the percent of homes that subject to a price cut and the time properties spend on the market (measured as Days on Zillow). Higher numbers indicate a better buyers’ market, lower numbers indicate a better sellers’ market, relative to other markets within a metro.</li>
<li>Market Health Index: This index indicates the current health of a given region’s housing market relative to other markets nationwide. It is calculated on a scale from 0 to 10, with 0 representing the least healthy markets and 10 the healthiest markets.</li>
<li>Days on Zillow: The median days on market of homes sold within a given month, including foreclosure re-sales. The latest data is for one month prior to the current ZHVI (e.g., if the most current month for ZHVI data is January, the most current month for Days on Zillow data will be December).</li>
<li>Home Sales: The number of homes sold during a given month. Our sales series methodology is available <a href=""http://www.zillow.com/research/home-sales-methodology-7733/"" target=""_blank"" rel=""nofollow"">here</a>. Note: ZIP code sales data is not latency adjusted, but is filtered.</li>
<li>Home Sales, SA: The number of homes sold during the given month, seasonally adjusted using the X-12-Arima method. Our sales series methodology is available <a href=""http://www.zillow.com/research/home-sales-methodology-7733/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Unfiltered Transactions: The number of homes sold during the given month. This series is latency adjusted, but no transaction-type filtering is done. For example, non-arm’s-length sales, foreclosures and title transfers would be all be included.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 2000 samples along with State, Cbsa Title, technical information and other features such as:
- Size Rank Metro
- Size Rank City
- and more.
How to use this dataset
&gt; - Analyze Days On Market in relation to Buyer Seller Index
- Study the influence of Region Type on Region Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	13	124	3	yamqwe	buyer-seller-indexe
389	389	Orange County COVID-19 Data	Daily data for OC COVID-19 cases as reported by OC Health Care Agency	['public health', 'health', 'covid19']	This data is scraped by the OC Civic Coders from the OC Health website. It is sourced from a GitHub repo containing these files.	15	2075	0	shubhamkulkarni01	orange-county-covid19-data
390	390	New York Times - COVID-19 Dataset	Github Dataset of COVID-19 Dataset from NewYork Times  - Public data	['exploratory data analysis', 'data visualization', 'tabular data', 'news', 'covid19', 'plotly']	"Context
New York Times COVID Dataset from 
https://www.github.com/nytimes/covid-19-data/
Content
Provides data at the state and county level
Acknowledgements
From NYT Github site"	187	1659	11	kalilurrahman	new-york-times-covid19-dataset
391	391	Anime_data		[]		0	1	0	soumyabratabairagi	anime-data
392	392	PuseudoLabelingJigsaw		[]		4	19	0	columbia2131	puseudolabelingjigsaw
393	393	Wordle (ES) Dataset | Spanish 5-characters words	Collection of 12123 (5-characters) words for playing Wordle (ES).	['text data']		0	14	1	vctorloureirosancho	wordle-es-spanish-5-chars-words
394	394	Jobhathon_dataset		[]		0	2	0	aarishshahab	jobhathon-dataset
395	395	The Lick	The Lick (jazz meme) Dataset for Machine Learning	['music']		18	87	1	andychamberlain	the-lick
396	396	ratingTest		[]		0	1	0	hamedetezadi	ratingtest
397	397	SimpleMovieDataset	SimpleMovieDataset-v1	[]		0	3	0	hamedetezadi	simplemoviedataset
398	398	CodeSearchNet	A dataset of code created by GitHub	[]		8	859	3	omduggineni	codesearchnet
399	399	Synthetic Chess Board Images	Photorealistic synthetic images representing pictures of chess game states	['board games', 'computer vision', 'classification', 'image data']	"Context
Data collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.
With high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. 
As an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.
Content
The dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.
The data has been generated using the Python scripts and .blend file present in this repository. The chess board and pieces models that have been used for those renders are not provided with the code.
Data characteristics :
Images : 1280x1280 JPEG images representing pictures of chess game boards.
Annotations : JSON files containing two variables : 
""config"", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.
""corners"", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.
config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : 
""cellsCoordinates"", a dictionary associating a cell name to its coordinates on the board. We have for example {""A1"" : [0,0], ""A2"" : [1,0], ...}
""piecesTypes"", a list of strings containing the types of pieces present in the renders.
No distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.
A proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.
Acknowledgements
I would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. 
Inspiration
Two interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : 
Erroll Wood, Tadas Baltrušaitis, Charlie Hewitt (2021) Fake It Till You Make It: Face analysis in the wild using synthetic data alone https://arxiv.org/abs/2109.15102
Salehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision https://arxiv.org/abs/2112.09290"	1	51	0	thefamousrat	synthetic-chess-board-images
400	400	longformer 1		[]		0	28	0	shreyasadhari123	longformer-1
401	401	Titanic_Train		[]		0	1	0	jagdishhaldar	titanic
402	402	FitBit Fitness Tracker Data ( google case study ) 		['exercise']		2	2	0	saintaugustin	dataa-
403	403	enchanter	Machine Learning Pipeline, Training and Logging for Me. License See My GitHub	['video games']		1	3474	3	hirotaka0122	enchanter
404	404	Practical ML implementations in Python	Implementation of usage of ML libraries	['research', 'computer science', 'programming', 'nlp', 'python']	The dataset contains implementations of different use-cases in the Machine Learning life cycle - from data extraction through deployment. There are paper implementations from scratch, and examples of file handling, model conversion, web scraping, deployment using APIs etc.	52	936	11	rajat95gupta	practical-ml-implementations-in-python
405	405	Keras Temporal Convolutional Network		['computer science']		0	32	0	diedioskuren	keras-temporal-convolutional-network
406	406	Top 2000 R&D Companies	List of companies with relative share of R&D Expenditure, Patents & Trademarks 	['business']	"Context
Data relate to companies in the top 2000 corporate R&D sample, ranked by R&D investment in 2018. The IP bundle refers to the number of patents and trademarks filed in 2016-18, owned by the top R&D companies, using fractional counts.
Acknowledgements
Source of this data is - JRC-OECD, COR&DIP© database v.3., 2021.
Inspiration
Possible questions which could be answered are:
What is the contribution of R&D towards different industries?
How are different industry stacked in terms of Patent share?
Does R&D expenditure has any correlation with Patent or Trademark share?
How are different countries performing in terms of Patent and Trademark share?
Which country is a leader in Patent share across different industries?"	3	23	3	nishantjr	top-2000-rd-companies
407	407	Narendra Modi - Text Speeches	Speeches of the Prime Minister of India	['india', 'government', 'politics', 'nlp']	"PM Modi text speech scrapper
Context
Narendra Damodaradas Modi is an Indian politician serving as the 14th and current prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to 2014 and is a Member of Parliament from Varanasi.
Modi had a long political career, before quickly rising within his party from the Chief Minister of Gujarat (2001 - 2014) to the Primi Minister candidate in the 2014 election. Known for his excellent oratorical skills and ability to connect to the common man, this dataset gives access to all his text speeches starting from 2018.
The dataset contains the speech translated primarily from Hindi to English. The dataset includes speeches starting from 2018.
Code:
Code is hosted here, https://github.com/adiamaan92/modi-speech-scrapper
&gt; The scrapper is scheduled to run every day at 8 AM using a Github Action workflow. 
Data description:
The dataset contains the following columns,
| Column       | Description                                                               |
| ------------ | ------------------------------------------------------------------------- |
| id           | Unique ID                                                                 |
| url          | URL from where the data is scraped                                        |
| title        | Title of the speech                                                       |
| article_text | Speech in text form                                                       |
| images       | URL of Images taken during the speech, mostly of Modi                     |
| publish_info | Publish info including by, date and time                                  |
| tags         | Tags from the scrapped website, can be used for categorizing the speeches |"	80	339	4	adiamaan	modi-speeches
408	408	transformers_hf	03/04/2020: HuggingFace Transformers	['movies and tv shows']		18	976	0	nazariio	transformers-270
409	409	International Matches & IPL Cricket Data	Dataset containing information about all the International and IPL Matches	['cricket', 'sports', 'beginner', 'intermediate', 'data visualization']	"&gt; Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 22-yard (20-metre) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The game proceeds when a player on the fielding team, called the bowler, ""bowls"" (propels) the ball from one end of the pitch towards the wicket at the other end. The batting side's players score runs by striking the bowled ball with a bat and running between the wickets, while the fielding side tries to prevent this by keeping the ball within the field and getting it to either wicket, and also tries to dismiss each batter (so they are ""out""). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side either catching a hit ball before it touches the ground, or hitting a wicket with the ball before a batter can cross the crease line in front of the wicket to complete a run. When ten batters have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches.
&gt; Forms of cricket range from Twenty20, with each team batting for a single innings of 20 overs and the game generally lasting three hours, to Test matches played over five days. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core layered with tightly wound string."	94	925	18	kalilurrahman	international-cricket-data-latest
410	410	Health Workforce Shortage Areas	Areas and sites with shortage of primary care, dental or mental health providers	['united states', 'healthcare', 'employment', 'health', 'tabular data', 'public safety']	"Context
Health workforce shortage areas are geographic areas, populations, and facilities that have a shortage of outpatient primary care, dental, and mental health providers and services.  These areas are designated by the Health Resources and Services Administration (HRSA), a federal agency in the United States Department of Health and Human Services. 
There are several types of shortage designations including:
- Health Professional Shortage Areas (HPSAs)
- Medically Underserved Areas and Populations (MUAPs)
- Exceptional Medically Underserved Population (Exceptional MUPs)
- Governor's-Designated Secretary-Certified Shortage Areas for Rural Health Clinics
HRSA's Bureau of Health Workforce operates a cooperative agreement and evaluates applications submitted by the Primary Care Office (PCO) of each U.S. state and territory as part of the process to designate some types of shortage areas.  These applications are reviewed by HRSA to determine if they meet specific designation criteria which differs by the type of shortage area.  Other shortage area types are automatically designated by federal statute or at the request of a state governor.  Once HPSAs are designated, score is calculated which represents a relative measure of need for health care services for that discipline.  Both HPSAs and MUAPs can be designated to indicate a shortage of primary care services while only HPSAs can be designated to indicate a shortage of dental or mental health services.  Shortage area designations and scores are used by various federal programs for distributing resources.  Some shortage area designations may also be used by state programs. 
See the shortage designation website for more information.
Content
The health workforce shortage area data in the included files represent the HPSA and MUAP (including Exceptional MUP) designation information at a single point in time. The dataset is refreshed weekly from the source data files on data.hrsa.gov.
HPSAs
All three file contain the same columns but represent only a single healthcare discipline.  Each record represents either a ""component"" (county, county subdivision or census tract) of a Geographic/Population HPSA service area or represents the physical location of facility HPSA.
Files:
- BCD_HPSA_FCT_DET_PC.csv: Primary Care HPSAs
- BCD_HPSA_FCT_DET_DH.csv: Dental Health HPSAs
- BCD_HPSA_FCT_DET_MH.csv: Mental Health HPSAs
Fields of interest:
- [HPSA ID]: Unique identifier for each HPSA designation
- [Designation Type]: Type of HPSA Designation. Types for areas designated for a geographic area include ""Geographic HPSA"", ""High Needs Geographic HPSA"" and ""HPSA Population""
- [HPSA Discipline Class]
MUAPs
Each record in this file represents a ""component"" (county, county subdivision or census tract) of a Medically Underserved Area or Medically Underserved Population Group service area
Files:
- MUA/_DET.csv: Medically Underserved Areas/Populations
Fields of interest:
Acknowledgements
Inspiration"	9	173	1	geobrando	health-professional-shortage-areas
411	411	IMDB_Movie_CaseStudy	IMDb is the most popular movie website and it combines movie plot description.	['movies and tv shows', 'beginner', 'data visualization', 'tabular data']	"IMDb (an acronym for Internet Movie Database) is an online database of information related to films, television programs, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews. An additional fan feature, message boards, was abandoned in February 2017. Originally a fan-operated website, the database is now owned and operated by IMDb.com, Inc., a subsidiary of Amazon.
As of December 2020, IMDb has approximately 7.5 million titles (including episodes) and 10.4 million personalities in its database,2 as well as 83 million registered users."	53	528	0	ashusri4	imdb-movie-casestudy
412	412	Mobile phone rating	Rating of mobile phones: Camera, Selfie, Audio, Display and Battery 	['mobile and wireless', 'clustering', 'tabular data']	"Context
Mobile phone rating from various aspects (camera, selfie, audio, display, and battery)
Acknowledgements
Data are scraped from https://www.dxomark.com/rankings/#smartphones-mobile.
Inspiration
The performance of mobile phone from various aspects over time."	116	632	7	prasertk	mobile-phone-rating
413	413	mail-dataset	Spam Mail Prediction | Machine Leaning	['bayesian statistics', 'internet', 'software', 'classification', 'binary classification']		0	36	10	mohinurabdurahimova	maildataset
414	414	Arxiv-all-papers-metadata		['education']		0	4	0	niranjankn	arxivallpapersmetadata
415	415	roberta-base-config	roberta-base configuration files	[]		0	619	1	lucca9211	robertabaseconfig
416	416	COVID-19	Data shows total number of confirmed, recovered and death cases by location	['literature', 'business', 'health']	"Context
The novel coronavirus that has infected more than 79,551 people worldwide (as of time of writing this context) is spreading rapidly, and independently, in countries outside of China, including Italy, South Korea, and Iran.
The viral illness is being diagnosed among hundreds of people in South Korea, Italy and Iran who have no connection to China.
Content
In the notebook I use the time series data. Time series data columns are described in the column description.
Acknowledgements
Thanks to the Johns Hopkins University for providing this data-set for educational purposes.
https://github.com/CSSEGISandData/COVID-19
Inspiration
To visualize COVID-19 spread world wide."	1935	23489	42	atilamadai	covid19
417	417	umap_clustering	Data preprocessed version of csv files based on discourse & topic essay kfolds.	[]	"Context
This is the data preprocessed version of CSV files based on discourse and topic essay kfolds."	16	7	0	devanshchowdhury	umap-clustering
418	418	United Arab Emirates Weather	Historical temperature of United Arab Emirates federation of seven emirates 	['weather and climate']	"Context
This dataset is meant to complement the Dubai Pulse is your single source for data on Dubai, Dubai Pulse
Content
Data from 2000 to 2022
station     The Meteostat ID of the weather station (only if query refers to multiple stations)     String
time    The date    Datetime64
tavg    The average air temperature in °C   Float64
tmin    The minimum air temperature in °C   Float64
tmax    The maximum air temperature in °C   Float64
prcp    The daily precipitation total in mm     Float64
snow    The snow depth in mm    Float64
wdir    The average wind direction in degrees (°)   Float64
wspd    The average wind speed in km/h  Float64
wpgt    The peak wind gust in km/h  Float64
pres    The average sea-level air pressure in hPa   Float64
tsun    The daily sunshine total in minutes (m)     Float64
Acknowledgements
Dataset from https://github.com/meteostat/meteostat-python"	2	40	3	rahulvks	united-arab-emirates-weather
419	419	notdataset		[]		0	12	0	limjunhao	notdataset
420	420	models_feedback_prize		[]		0	3	0	masayoshi64	models-feedback-prize
421	421	Market Prediction Model		['investing']		0	21	0	electrospirit	market-prediction-model
422	422	OpenML_mnist_786.csv	Unmodified database form OpenML	[]		0	3	1	krishnendusinhaml	openml-mnist-786csv
423	423	Position salary with level		['jobs and career']		0	3	2	imnoob	position-salary-with-level
424	424	solar_power		[]		0	1	0	anandkumhar	solar-power
425	425	atis-pp		[]		0	1	0	panhaitao	atispp
426	426	exp62v5s		[]		0	2	0	dengyuchen	exp62v5s
427	427	Dataset for teenagers chat in Telegram groups	Dataset for teenagers' chat in Telegram groups(Persian)	['online communities']		66	2018	5	rezaali	dataset-for-teenagers-chat-telegram-group-persian
428	428	Речник	Индекс свих речи свих језика. Књиге.	[]		0	53	0	sinisa632	recnik
429	429	yolov5m6_tuned_scratch		[]		0	3	0	kennyxie	yolov5m6-tuned-scratch
430	430	Own dataset		[]		0	1	0	muhammadyaqub	own-dataset
431	431	xlmRoberta-english-squad		[]		0	9	8	bhavikardeshna	xlmrobertaenglishsquad
432	432	happywhale-tfrecords-v1-whale2-cropped-dataset		[]		0	9	0	slow5620	happywhale-tfrecords-v1-whale2-cropped-dataset
433	433	India Power Infrastructure Data (2004-2021)	Statewise Power Infrastructure Data by RBI	['india', 'energy', 'data visualization', 'time series analysis', 'data analytics', 'electricity']	"Context
This Dataset is extracted from the RBI(Reserve Bank Of India) Annual Publication ""HANDBOOK OF STATISTICS ON INDIAN STATES"" and is open to be used for research and educational purpose. The particular dataset contains a CSV(Comma Separated Values) file on Indian Statewise Electricity Infrastructure from 2004 to 2021. To be used by researchers, analysts, professionals and students to carry out various projects and research especially for Time Series Analysis and Forecasting.
Content
State/Union Territory: States and Union Territories of India
Year: Year from FY2004-2005 to FY2020-2021
Power Requirement Net Crore Units: Power Requirement by each state in Net Crore Units
Availability Of Power Net Crore Units: Power Availability for each state in Net Crore Units 
Availability Of Power Per Capita kiloWatt-Hour: Availability of PPC in kiloWatt-Hour
Installed Power Capacity MegaWatt: Installed Power Capacity in MegaWatt
Note: 
i) Per Capita Availability of Power is worked out based on Census Population 2011 and for the population for Andhra Pradesh and Telangana drawn from both Governments’ portals for the years 2014-15 and 2018-19.
ii) Figures of Telangana are w.e.f. June 2014 due to bifurcation of Andhra Pradesh into Andhra Pradesh and Telangana w.e.f. June 2, 2014
Acknowledgements
Database on Indian Economy: https://dbie.rbi.org.in/DBIE/dbie.rbi?site=home
Central Electricity Authority, Ministry of Power, Government of India: https://cea.nic.in/?lang=en"	12	178	4	hangryjay	india-power-infrastructure-data
434	434	F1 Race and Qualifying Archive 1982-2021	All race and qualifying data from 1982 to 2021	['arts and entertainment', 'auto racing', 'sports', 'automobiles and vehicles', 'tabular data']	"Context
Formula One is the highest class of international racing for open-wheel single-seater racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). Ever since its inaugural season in 1950, Formula1 has been regarded as the pinnacle of motorsport.
Content
This dataset contains detailed information about qualifying and race results for all the tracks over the course of multiple seasons. There is a separate directory for each season. There are 2 sub-directories for each season, namely: Qualifying Results and Race Results. The Race Results directory contains an overall_race_results.csv file which summarizes the race results throughout the entire season. It also contains multiple .csv files for the results of each race in the season. The Qualifying Results directory contains multiple .csv files for the qualifying results before the start of each race.
Note
For the 1982 season and before the qualifying results contain only 1 entry in the file which is that of the polesitter. The lap times of the other drivers were not accounted for, and on the official website there is only 1 entry under the qualifying results.
Inspiration
F1 is one of my favorite sports and I almost never miss a race 😄
The motivation behind creating this dataset was to learn more about web scraping and try to perform a statistical analysis of the data. Some of the things you could do with the entire dataset are as follows:
- Identify the driver with the most poles
- Compare qualifying times of different drivers (championship contenders, team-mates, etc)
- Determine how often a particular driver out-qualifies his team-mate
- Compare qualifying lap times of a race from previous seasons
- Identify the driver with the most number of wins at a particular track
- Analyze how the championship battle unfolded based on the number of points scored by the drivers (specially interesting for the 2021 f1 season 👀)
- Identify drivers with the highest number of wins, podiums, DNFs, etc
- Compare the average lap times of different tracks to identify the slowest and fastest tracks on the calendar
- Compare the number of laps for each race in the season (Belgium 2021 being the clear winner 😂)
- Find out who won the Driver's Championship based on the total number of points
- Find out who won the Constructor's Championship based on the total number of points for each team
Some Common F1 Terms You Might Come Across
DNF: Did Not Finish. Commonly used nomenclature for drivers that crashed/failed to complete the entire race
DNQ: Did Not Qualify. Eliminated missing values from the qualifying datasets by introducing this abbreviation for drivers who failed to qualify.
NC: Not Confirmed. For drivers that DNF the term NC is used in the Position column
DQ: Disqualified. Generally drivers are disqualified from races due to technical infringements or a breach of sporting regulations (Example: Sebastian Vettel was disqualified from the 2021 Hungarian Grand Prix due to fuel irregularites and stripped of all the points he earned from finishing the race in P2)
Future Work
As I collect more data for the previous seasons, I will create new versions for the dataset. The goal with this dataset is to create an archive of qualifying and race data from 1950-2021. The dataset will also be updated when the 2022 season commences."	38	419	10	rprkh15	f1-race-and-qualifying-data
435	435	Light Novel for Text Generation	4 Japanese Light Novel's translated into English. Created for Text Generation	['literature']		0	33	4	utsavk02	4-light-novel-for-text-generation
436	436	nyc_data		[]		4	15	0	sayan12	nyc-data
437	437	raw_data		[]		0	2	0	nagraj0308	raw-data
438	438	GBRCOTMODEL		[]		0	3	0	l87wang	gbrcotmodel
439	439	Google Cyclistic Case Study	First Case Study for Google Data Analytics Course	['data cleaning', 'data analytics', 'dplyr', 'ggplot2', 'tidyverse']		0	17	0	marctan9	google-cyclistic-case-study
440	440	multi-sentence-cls		[]		0	16	0	longhuqin	multisentencecls
441	441	Investments into India	A list of companies investing into India	['india', 'investing']		20	191	0	virajkulkarni952	investments-into-india
442	442	StAnD (Large problems)	Large problems from the Static Analysis Dataset	['earth and nature', 'engineering']	"The Static Analysis Dataset (StAnD) is a large collection of solved linear static analysis problems on frame structures.
This dataset contains the subset of StAnD composed of large problems.
[Abstract] [Paper] [Code]
Context
Many algorithms for solving sparse linear systems are published at a great pace, but currently there is no standard dataset to compare their running time on real problems. To best of our knowledge there is no existing large dataset of sparse linear systems. 
A few sparse matrices (often less than 10) are published for many engineering problems in the Matrix Market or in the SuiteSparse matrix collection, but their limited number is not sufficient to measure the running time in the average case or to measure reliably how the resolution algorithm scales with the size and the number of non-zeros in the sparse matrix. Additionally, no constant term is provided in conjunction with the matrices. The constant term, is maybe not fundamental when direct methods are used, but it becomes important if we want to measure the effectiveness of iterative methods, whose behavior depends on the relationship between the initialization of the solution and the real solution.
Content
StAnD is composed of 303.000 static analysis problems of frame structures divided into 6 parts: 100.000 small training problems, 100.000 medium training problems, 100.000 large training problems, 1.000 small test problems, 1.000 medium test problems and 1.000 large training problems. The size of a problem is determined by the number of degrees of freedom (DOFs) of the structure model or equivalently by the number of rows or columns of the stiffness matrix associated to the structure. Small problems have 2115 DOFs on average (and at most 5166 DOFs), medium problems have about 7000 DOFs (and at most 14718 DOFs), while large problems can have up to 31770 DOFs with about 15500 DOFs on average.
The dataset is programmatically created using OpenSeesPy, the Python interface to the OpenSees finite element solver. For each structural model with its loading configuration, a static analysis is performed in order to compute nodal displacements. Therefore, every problem in the dataset is a tuple (K, f, u), where K is the sparse (symmetric positive-definite) stiffness matrix associated to the structure, f is a load vector and u is the ground-truth displacement vector such that K · u = f.
In the training set, for the same structure (i.e. the same stiffness matrix K), we apply different load configurations. In the test set, we use a single load configuration for every structure to maximize the variability.
Every solved system is saved in a separate .npz file. The name of the file follows the scheme {seed}{K hash}{u f hash}.npz. Problems in files with the same seed and the same K hash come from the same structure. The sparse stiffness matrix is saved in COO format.
Let us call n is the number of rows (and columns) of K and nnz is the number of non-null elements of K, then the keys of the .npz file are:
- A_indices: (2, nnz)-shaped array of row and column coordinates of non-null elements of K;
- A_values: nnz-shaped array of coefficients of non-null elements of K;
- x: n-shaped array containing to the linear system solution u;
- b: n-shaped array containing to the linear system constant term f.
Inspiration
We introduce StAnD to ease the comparison and the evaluation of new sparse linear system solvers and to spur research in resolution methods specifically tailored to structural engineering and static analysis problems."	1	36	0	zurutech	stand-large-problems
443	443	HuggingFace Transformer Bert	HuggingFace Transformer Bert Pytorch version	[]		0	764	0	lucca9211	huggingface-transformer-bert
444	444	roberta-large-config	roberta-large configuration files	[]		1	653	0	lucca9211	robertalargeconfig
445	445	LaLiga Data	Available data from LaLiga (spanish football league)	['football', 'sports', 'data visualization', 'data analytics', 'python']	"Context
LaLiga is the trademark behind the Spanish Football Competitions and their data can be viewed on laliga.com but there is no way to download a csv file with the whole information.
To that end, I've built a Python scraper to retrieve and public these data. Data is updated weekly.
Content
So far, here you have the available contents:
Player Data: You'll find all available player data from LaLiga with a huge amount of columns for these competitions: female first division, male first division and male second division. Each file is identified by SXX-YY at the beginning meaning the season XX-YY.
Acknowledgements
Thanks Python!"	33	300	3	sdelquin	laliga-data
446	446	happywhale-tfrecords-v1-384x384		[]		0	4	0	slow5620	happywhale-tfrecords-v1-384x384
447	447	nothing		['music']		2	15	0	tanviranjum	nothing
448	448	CPS 2015		[]		0	0	0	amschelder	cps-2015
449	449	stock-ml		[]		0	6	0	willawang	stock-ml
450	450	Grups equips IABD	Clustering psicologico. Afinidad, HADA, eneagrama	[]		0	5	0	joerobcia	grups-equips-iabd
451	451	UFC Rankings	Historical UFC rankings	['sports', 'martial arts']	"Content
This dataset contains the historical rankings of UFC fighters. From their inception in 2013 up until today. The rankings are published weekly, more or less, by the UFC and compiled via a vote by media members. God bless.
In each weight class, the fighters are listed by their rank, except for the champion who is marked as ranked at 0.
The dataset isn't 100% complete, there are weeks missing here and there but it's probably over 95% complete. Rankings are published only after UFC events so every 2-3 week period without new rankings does not necessarily mean missing data.
Acknowledgements
The data was pulled from The Internet Archive's records of ufc.com."	818	16683	22	martj42	ufc-rankings
452	452	helpdesk-dataset		['business']		0	3	1	ahmadnuryanto	helpdesk-dataset
453	453	GP_DATA_WITHOUT_ORDERED_CLASSES		[]		0	4	0	manarmoh	gp-data-without-ordered-classes
454	454	GP_DATA_WITHOUT_ORDERED_CLASSES		[]		0	4	0	manarmoh	gp-data-without-ordered-classes
455	455	COVID-19 Open Research Dataset (CORD-19)	29,000 covid scholarly articles, including over 13,000 full texts	['education', 'health', 'computer science', 'nlp', 'text mining', 'covid19']	"About this dataset
&gt; <p>The <a href=""https://pages.semanticscholar.org/coronavirus-research"" target=""_blank"" rel=""nofollow"">COVID-19 Open Research Dataset</a> is “a free resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19 and the coronavirus family of viruses for use by the global research community.”</p>
<p><code>in-the-news</code>: On March 16, 2020, the White House issued a <a href=""https://www.whitehouse.gov/briefings-statements/call-action-tech-community-new-machine-readable-covid-19-dataset/"" target=""_blank"" rel=""nofollow"">“call to action to the tech community”</a> regarding the dataset, asking experts “to develop new text and data mining techniques that can help the science community answer high-priority scientific questions related to COVID-19.”</p>
<p>Included in this dataset:</p>
<ul>
<li><a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=comm_use_subset.tar.gz"">Commercial use subset</a> (includes PMC content) -- 9000 papers, 186Mb</li>
<li><a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=noncomm_use_subset.tar.gz"">Non-commercial use subset</a> (includes PMC content) -- 1973 papers, 36Mb</li>
<li><a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=pmc_custom_license.tar.gz"">PMC custom license subset</a> -- 1426 papers, 19Mb</li>
<li><a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=biorxiv_medrxiv.tar.gz"">bioRxiv/medRxiv subset</a> (pre-prints that are not peer reviewed) -- 803 papers, 13Mb</li>
</ul>
<p>Each paper is represented as a single JSON object. The schema is available <a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=json_schema.txt"">here</a>.</p>
<p>We also provide a comprehensive metadata file of 29,000 coronavirus and COVID-19 research articles with links to <a href=""https://www.ncbi.nlm.nih.gov/pmc/?term=%22COVID-19%22+OR+Coronavirus+OR+%22Corona+virus%22+OR+%222019-nCoV%22+OR+%22SARS-CoV%22+OR+%22MERS-CoV%22+OR+%E2%80%9CSevere+Acute+Respiratory+Syndrome%E2%80%9D+OR+%E2%80%9CMiddle+East+Respiratory+Syndrome%E2%80%9D"" target=""_blank"" rel=""nofollow"">PubMed</a>, <a href=""https://aka.ms/AA7q3eb"" target=""_blank"" rel=""nofollow"">Microsoft Academic</a> and the <a href=""https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov"" target=""_blank"" rel=""nofollow"">WHO COVID-19 database of publications</a> (includes articles without open access full text):</p>
<ul>
<li><a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=all_sources_metadata_2020-03-13.csv"">Metadata file</a> (<a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=all_sources_metadata_2020-03-13.readme"">readme</a>) -- 47Mb</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://pages.semanticscholar.org/coronavirus-research"" target=""_blank"" rel=""nofollow"">https://pages.semanticscholar.org/coronavirus-research</a><br>
<strong><em>Updated:</em></strong> Weekly<br>
<strong><em>License:</em></strong> <a href=""https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=COVID.DATA.LIC.AGMT.pdf"">https://data.world/kgarrett/covid-19-open-research-dataset/workspace/file?filename=COVID.DATA.LIC.AGMT.pdf</a></p>
<blockquote>
<p>See more COVID-19 data at <a href=""https://data.world/resources/coronavirus/"">data.world's Coronavirus (COVID-19) Data Resource Hub</a></p>
</blockquote>
This dataset was created by Kelly Garrett and contains around 30000 samples along with Abstract, Title, technical information and other features such as:
- Publish Time
- Who # Covidence
- and more.
How to use this dataset
&gt; - Analyze Microsoft Academic Paper Id in relation to Authors
- Study the influence of License on Sha
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	38	638	7	yamqwe	covid-19-open-research-dataset-cord-19e
456	456	Chicago Enterprise Zone 	Chicago Enterprise Zone Program Data	['business']	"About this dataset
rel=""nofollow""&gt;https://data.cityofchicago.org/d/64xf-pyvh -- The Illinois Enterprise Zone Program is designed to stimulate economic growth and neighborhood revitalization in economically depressed areas of the state. For more information about this program, go to <a href=""http://www.commerce.state.il.us/dceo/Bureaus/Business_Development/Tax+Assistance/Enterprise-Zone.htm"" target=""_blank"">http://www.commerce.state.il.us/dceo/Bureaus/Business_Development/Tax+Assistance/Enterprise-Zone.htm</a>. This dataset is in a format for spatial datasets that is inherently tabular but allows for a map as a derived view. Please click the indicated link below for such a map. To export the data in either tabular or geographic format, please use the Export button on this dataset.<p></p>
<p>Source: <a href=""https://data.cityofchicago.org/d/y9xh-uwte"" target=""_blank"">https://data.cityofchicago.org/d/y9xh-uwte</a><br>
Last updated at <a href=""https://data.cityofchicago.org/"" target=""_blank"">https://data.cityofchicago.org/</a> : 2021-10-01</p>
This dataset was created by City of Chicago and contains around 0 samples along with Display Co, Display Co, technical information and other features such as:
- Display Co
- Display Co
- and more.
How to use this dataset
&gt; - Analyze Display Co in relation to Display Co
- Study the influence of Display Co on Display Co
- More datasets
Acknowledgements
If you use this dataset in your research, please credit City of Chicago 
Start A New Notebook!"	7	159	6	yamqwe	boundaries-enterprise-zones-deprecated-october-2e
457	457	Bert-uncased	BERT base model (uncased). Pretrained model on English language	[]		2	837	0	lucca9211	bertuncased
458	458	COVID-19 Vaccination Progress in Malaysia	Weekly Update on the latest vaccination progress in Malaysia	['public health', 'public safety']		127	1016	2	maisarahmohamedpauzi	csv-covid-malaysia
459	459	Future Near Earth Asteroids	A visual dataset of known and tracked Near Earth Asteroids	['earth and nature', 'astronomy', 'science and technology']	"About this dataset
&gt; <h1>Asteroids</h1>
<p>A visualization of known and tracked Near Earth Asteroids, or NEAs that will make a close approach to Earth within the next 12 months, or have made a close approach within the last 12 months.</p>
<p>See the live visualization at <a href=""http://untilanasteroid.com"" target=""_blank"" rel=""nofollow"">untilanasteroid.com</a>.</p>
<p><img src=""https://data.world/api/markmarkoh/dataset/future-asteroids/file/raw/Screenshot%202016-08-04%2010.30.33.png"" alt=""asteroid screenshot"" style=""""></p>
This dataset was created by Mark Di Marco and contains around 600 samples along with Vrelative(km, Ca Distance Nominal(ld, technical information and other features such as:
- Ca Distance Minimum(ld
- H(mag)
- and more.
How to use this dataset
&gt; - Analyze Vinfinity(km in relation to Nsigma
- Study the influence of Ref on Class
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Mark Di Marco 
Start A New Notebook!"	28	322	6	yamqwe	future-asteroidse
460	460	NJ Meet of Champions 2021-2000	A collection of scraped milesplit results by a custom webscraper	['sports']		0	3	0	mileswright	nj-meet-of-champions-20212000
461	461	Bio_Clinical_BERT		[]	"This BERT model is from ClinicalBERT - Bio + Clinical BERT Model for clinical text.
Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	1	0	wanko123	bio-clinical-bert
462	462	 Population	 Births and deaths: Year ended December 2020	['pollution', 'business', 'social science', 'matplotlib', 'pandas', 'seaborn']	"Population
Births and deaths: Year ended December 2020
This data is for education and training in data science and analysis
About the columns ,
Period
Birth Death
Region
Count
About tasks:
Data processing and analysis with an explanation of the results."	1	7	3	qusaybtoush	population
463	463	# Labor market 2	Employment indicators weekly paid jobs	['business', 'jobs and career', 'pandas', 'plotly', 'plyr', 'seaborn']	"Labour market 2
Employment indicators weekly paid jobs
This data is for education and training in data science and analysis
About the columns ,
Week end
Indicator
High industry
Value
About tasks:
Data processing and analysis with an explanation of the results."	1	2	3	qusaybtoush	-labor-market-2
464	464	Labour market	 Employment indicators	['business', 'linguistics', 'linear regression', 'jobs and career', 'matplotlib']	"Labor market
Employment indicators
This data is for education and training in data science and analysis
About the columns ,
Series reference
Period
Data value
Suppressed
STATUS
UNITS 
Magnitude 
Subject 
Group
Series_title_1
Series_title_2
About tasks:
Data processing and analysis with an explanation of the results."	0	3	4	qusaybtoush	labour-market
465	465	US counties COVID 19 dataset	NYT's github CSV on COVID19 per US counties	['united states', 'covid19']	"From the New York Times GITHUB source:
CSV US counties
""The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak.
Since late January, The Times has tracked cases of coronavirus in real time as they were identified after testing. Because of the widespread shortage of testing, however, the data is necessarily limited in the picture it presents of the outbreak.
We have used this data to power our maps and reporting tracking the outbreak, and it is now being made available to the public in response to requests from researchers, scientists and government officials who would like access to the data to better understand the outbreak.
The data begins with the first reported coronavirus case in Washington State on Jan. 21, 2020. We will publish regular updates to the data in this repository.
United States Data
Data on cumulative coronavirus cases and deaths can be found in two files for states and counties.
Each row of data reports cumulative counts based on our best reporting up to the moment we publish an update. We do our best to revise earlier entries in the data when we receive new information.""
The specific data here, is the data PER US COUNTY.
The CSV link for counties is: https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv"	17788	135719	411	fireballbyedimyrnmom	us-counties-covid-19-dataset
466	466	TestCheck2		[]		7	2455	0	jarvis2324	testcheck2
467	467	International Debt Statistics	International Debt, Exploratory Data Analysis	['business', 'finance', 'lending', 'banking', 'economics']	"About this dataset
&gt; <p>Focuses on financial flows, trends in external debt, and other major financial indicators for developing and advanced economies (data from Quarterly External Debt Statistics and Quarterly Public Sector Debt databases).</p>
<p>Includes over 200 time series indicators from <em>1970 to 2014</em>, for most reporting countries, and pipeline data for scheduled debt service payments on existing commitments to <em>2022</em>.</p>
<p>Attribution: <a href=""http://data.worldbank.org/data-catalog/international-debt-statistics"">International Debt Statistics, The World Bank</a>.</p>
<p><a href=""http://web.worldbank.org/WBSITE/EXTERNAL/0,,contentMDK:22547097~pagePK:50016803~piPK:50016805~theSitePK:13,00.html"" target=""_blank"" rel=""nofollow"">World Bank Data Catalog Terms of Use</a></p>
This dataset was created by World Bank and contains around 30000 samples along with 1983, 1977, technical information and other features such as:
- 2003
- 1991
- and more.
How to use this dataset
&gt; - Analyze 1989 in relation to 1999
- Study the influence of 1971 on 2012
- More datasets
Acknowledgements
If you use this dataset in your research, please credit World Bank 
Start A New Notebook!"	34	319	4	yamqwe	international-debt-statisticse
468	468	Industries 2015-2020	Unique passenger counts over 100 by NZ port and citizenship	['business', 'economics', 'time series analysis', 'logistic regression', 'linear regression', 'seaborn']	"Industries
Unique passenger counts over 100 by NZ port and citizenship
This data is for education and training in data science and analysis
About the columns ,
NZ Port 
Citizenship
Count
Year
About tasks:
Data processing and analysis with an explanation of the results."	2	3	4	qusaybtoush	industries-20152020
469	469	Industries 2	 Rental price indexes: December 2021 	['business', 'economics', 'data visualization', 'data analytics', 'datetime']	"Industries
Rental price indexes: December 2021
This data is for education and training in data science and analysis
About the columns ,
SER REF
TIME REF
DATA VAL
STATUS
UNITS 
Subject
Group 
Series title 1
Series title 2
Series title 3
About tasks:
Data processing and analysis with an explanation of the results."	0	2	3	qusaybtoush	industries-2
470	470	mmdetection_2.21.0		[]	"Source
https://github.com/open-mmlab/mmdetection"	1	44	0	atamazian	mmdetection-2210
471	471	Global Government Finance Statistics Manual (GFSM)	The Global Government Finance Statistics Manual (GFSM)	['business', 'finance', 'government', 'economics']	"About this dataset
&gt; <p>The Government Finance Statistics (GFS) contains data for all reporting countries in the framework of the Government Finance Statistics Manual, 2014 (GFSM 2014). The GFSM 2014 classifications remain largely the same as in the GFSM 2001 presentations. However, some tables now include an expanded list of GFS items, where others have dropped some GFS items. Nonetheless, there are no changes in the aggregates and balancing items of the converted data when compared with the GFSM 2001 data. All new data reported in the GDSM 2001 format, as well as historical data in the previous GFS database, have been converted to the GFSM 2014 format.</p>
<p>GFS data are compiled either on a cash basis of recording or noncash basis of recording (e.g., an accrual basis or a commitment basis). Please refer to the <a href=""http://data.imf.org/?sk=6d69e42a-c628-405a-bf14-f05129f87ce8"" target=""_blank"" rel=""nofollow"">map available in the About GFS section</a> to view the bases of recording for all reporting countries.</p>
<p>GFS contains detailed data on revenue, expense, transactions in assets and liabilities, and stocks positions in assets and liabilities of general government and its subsectors. GFS also includes preset presentations of GFS data by country, and by indicator, and also provides the option to download data with customized queries.</p>
<p>GFS is divided into seven databases, as indicated below.</p>
<ol>
<li>Government Finance Statistics (GFS), Main Aggregates and Balances</li>
<li>Government Finance Statistics (GFS), Revenue</li>
<li>Government Finance Statistics (GFS), Expense</li>
<li>Government Finance Statistics (GFS), Expenditure by Function of Government (COFOG)</li>
<li>Government Finance Statistics (GFS), Integrated Balance Sheet (Stock Positions and Flows in Assets and Liabilities)</li>
<li>Government Finance Statistics (GFS), Financial Assets and Liabilities by Counterpart Sector</li>
<li>Government Finance Statistics (GFS), Statement of Sources and Uses of Cash</li>
</ol>
<p><strong>Latest Update Date as of Download:</strong> 08/19/2017</p>
<hr>
<p>The Data is provided to Users “as is” and without warranty of any kind, either express or implied, including, without limitation, warranties of merchantability, fitness for a particular purpose and noninfringement.</p>
<p>The IMF Data is available free of charge from the IMF.</p>
<p>Users are prohibited from infringing upon the integrity of the IMF data and in particular shall refrain from any act of alteration of the IMF data that intentionally affects its nature or accuracy. If the IMF data is materially transformed by the user, this must be stated explicitly along with the required source citation.</p>
<p>All other terms set forth in the IMF's general terms and conditions (<a href=""http://www.imf.org/external/terms.htm"" target=""_blank"" rel=""nofollow"">http://www.imf.org/external/terms.htm</a>) shall continue to apply to use of IMF Data.</p>
<p><strong><em>Source:</em></strong> <a href=""http://data.imf.org/?sk=a0867067-d23c-4ebc-ad23-d3b015045405"" target=""_blank"" rel=""nofollow"">http://data.imf.org/?sk=a0867067-d23c-4ebc-ad23-d3b015045405</a></p>
This dataset was created by International Monetary Fund and contains around 500000 samples along with 1996, 1994, technical information and other features such as:
- 2014
- 1993
- and more.
How to use this dataset
&gt; - Analyze 1986 in relation to 2012
- Study the influence of 2008 on 2002
- More datasets
Acknowledgements
If you use this dataset in your research, please credit International Monetary Fund 
Start A New Notebook!"	10	129	1	yamqwe	government-finance-statistics-gfse
472	472	Current Antarctic large iceberg positions	Current Antarctic large iceberg positions derived from ASCAT and OSCAT-2	['earth and nature', 'atmospheric science', 'weather and climate']	"Context
This dataset fetches the data from the GitHub repo https://github.com/Joel-hanson/Iceberg-locations. This is a project which automatically scrapes data from https://www.scp.byu.edu/current_icebergs.html to get the current location of all the large iceberg in the Antarctic, The position is derived from ASCAT and OSCAT-2. The JSON iceberg_location.json contains all the information collected from the page. This JSON is typically updated once or twice a week(as per the updates on the website), typically on Mondays and possibly Fridays. Positions reported here are extracted from near real-time ASCAT and OSCAT-2 data in tandem. Positions reported in the full iceberg database are generated from science data and have been more accurately tracked. The full database is updated only a few times per year which can be accessed from https://www.scp.byu.edu/data/iceberg/database1.html.
Content
JSON Schema
The file iceberg_location.json is structured in the format
{
    ""$schema"": ""http://json-schema.org/draft-06/schema#"",
    ""type"": ""object"",
    ""additionalProperties"": {
        ""type"": ""array"",
        ""items"": {
            ""$ref"": ""#/definitions/ScriptElement""
        }
    },
    ""definitions"": {
        ""ScriptElement"": {
            ""type"": ""object"",
            ""additionalProperties"": false,
            ""properties"": {
                ""iceberg"": {
                    ""type"": ""string""
                },
                ""recent_observation"": {
                    ""type"": ""string""
                },
                ""longitude"": {
                    ""type"": ""integer""
                },
                ""dms_longitude"": {
                    ""type"": ""string""
                },
                ""dms_lattitude"": {
                    ""type"": ""string""
                },
                ""lattitude"": {
                    ""type"": ""integer""
                }
            },
            ""required"": [
                ""dms_lattitude"",
                ""dms_longitude"",
                ""iceberg"",
                ""lattitude"",
                ""longitude"",
                ""recent_observation""
            ],
            ""title"": ""ScriptElement""
        }
    }
}
Example
{
    ""02/12/21"": [
        {
            ""iceberg"": ""a23a"",
            ""recent_observation"": ""02/09/21"",
            ""longitude"": -400.0,
            ""dms_longitude"": ""40 0'W"",
            ""dms_lattitude"": ""75 45'S"",
            ""lattitude"": -7545.0
        },
        {
            ""iceberg"": ""a63"",
            ""recent_observation"": ""02/09/21"",
            ""longitude"": -5447.0,
            ""dms_longitude"": ""54 47'W"",
            ""dms_lattitude"": ""71 41'S"",
            ""lattitude"": -7141.0
        },
        {
            ""iceberg"": ""a64"",
            ""recent_observation"": ""02/09/21"",
            ""longitude"": -6038.0,
            ""dms_longitude"": ""60 38'W"",
            ""dms_lattitude"": ""69 23'S"",
            ""lattitude"": -6923.0
        },
        ................
Acknowledgements
All of the data collected here are from the NASA SCP website
Inspiration
All people should know that this is happening due to climate change. If we don't act now then we will not have anything left."	16	978	6	joelhanson	iceberg-locations
473	473	Italian Coronavirus Cases by Age group and Sex	Data extracted from reports published by Italian National Institute of Health	['health', 'covid19']	"Italy Coronavirus Cases by Age group and Sex (ICCAS)
This repository contains datasets about the number of Italian Sars-CoV-2 confirmed cases and deaths disaggregated by age group and sex. The data is (automatically) extracted from pdf reports (like this) published by Istituto Superiore di Sanità (ISS) two times a week. A link to the most recent report can be found in this page under section ""Documento esteso"".
PDF reports are usually published on Tuesday and Friday and contains data updated to the 4 p.m. of the day day before their release. 
I wrote a script that is runned periodically in order to automatically update this repository when a new report is published. The code is hosted in a separate repository.
For feedback and issues refers to the GitHub repository.
Data folder structure
The data folder is structured as follows:
data
├── by-date                  
│   └── iccas_{date}.csv   Dataset with cases/deaths updated to 4 p.m. of {date}
└── iccas_full.csv         Dataset with data from all reports (by date)
The full dataset is obtained by concatenating all datasets in by-date and has an additional date column. If you use pandas, I suggest you to read this dataset using a multi-index on the first two columns:
python
import pandas as pd
df = pd.read_csv('iccas_full.csv', index_col=(0, 1))  # ('date', 'age_group') 
NOTE: {date} is the date the data refers to, NOT the release date of the report it was extracted from: as written above, a report is usually released with a day of delay. For example, iccas_2020-03-19.csv contains data relative to 2020-03-19 which was extracted from the report published in 2020-03-20.
Dataset details
Each dataset in the by-date folder contains the same data you can find in ""Table 1"" of the corresponding ISS report. This table contains the number of confirmed cases, deaths and other derived information disaggregated by age group (0-9, 10-19, ..., 80-89, &gt;=90) and sex.
WARNING: the sum of male and female cases is not equal to the total number of cases, since the sex of some cases is unknown. The same applies to deaths.
Below, {sex} can be male or female.
| Column                    | Description                                                                                  |
|---------------------------|----------------------------------------------------------------------------------------------|
| date                    | (Only in iccas_full.csv) Date the format YYYY-MM-DD; numbers are updated to 4 p.m of this date |
| age_group               | Values: ""0-9"", ""10-19"", ..., ""80-89"", ""&amp;gt;=90""                                               |
| cases                   | Number of confirmed cases (both sexes + unknown-sex; active + closed)                        |
| deaths                  | Number of deaths (both sexes + unknown-sex)                                                  |
| {sex}_cases             | Number of cases of sex {sex}                                                                 |
| {sex}_deaths            | Number of cases of sex {sex} ended up in death                                               |
| cases_percentage        | 100 * cases / cases_of_all_ages                                                            |
| deaths_percentage       | 100 * deaths / deaths_of_all_ages                                                          |
| fatality_rate           | 100 * deaths / cases                                                                       |
| {sex}_cases_percentage  | 100 * {sex}_cases / (male_cases + female_cases) (cases of unknown sex excluded)            |
| {sex}_deaths_percentage | 100 * {sex}_deaths / (male_deaths + female_deaths) (cases of unknown sex excluded)         | 
| {sex}_fatality_rate     | 100 * {sex}_deaths / {sex}_cases                                                           |
All columns that can be computed from absolute counts of cases and deaths (bottom half of the table above) were all re-computed to increase precision."	101	2713	3	giangip	iccas
474	474	covid-19 data of india	Weekly/Dialy Updated covid-19 dataset of india	['india', 'covid19', 'json']	"Want Dialy Updated?
Head to the github repo of this dataset
Want something to be added?
Then make tell me about what you need to be added in the discussion section.
Source
covid19india.org
License
CC BY-SA 4.0
About Covid-19
The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, China. The World Health Organization declared the outbreak a Public Health Emergency of International Concern in January 2020 and a pandemic in March 2020. As of 8 March 2021, more than 117 million cases have been confirmed, with more than 2.59 million deaths attributed to COVID-19, making it one of the deadliest pandemics in history. wikipedia"	349	2938	8	chetansharma0402	covid19-data-of-india
475	475	tfgbrcoco		[]		8	13	1	rishirajacharya	tfgbrcoco
476	476	Instagram_image		[]		0	0	0	mohineshsharma	instagram-image
477	477	Stroke Dataset		['health']		0	4	0	priyabhattmtech2021	stroke-dataset
478	478	Brazilian Soccer Database	Matches of the main competitions in which Brazilian soccer teams participate	['football', 'brazil', 'south america', 'sports', 'text data']	"Brazilian Soccer Data
This repository consists of collecting the history and current data of all the most important competitions that Brazilian teams compete, the principal competitions are:
Brasileirão(Brazilian soccer league)
Libertatodes(Principal South america Competition)
Sudamericana(South American secondary competition)
Copa do Brasil(Brazilian Cup)
Next Steps:
- structure the collection of the games of the sudamericana and copa do brasil
- Gather data from the main state championships(SP, RJ, MG, RS)
- Gather more data from these championships, such as match statistics
Any questions or suggestions are welcome, feel free to collaborate on the github repository"	80	1042	9	ricardomattos05	brazilian-soccer-database
479	479	COVID-19 India	Updated COVID-19 dataset	['india', 'healthcare', 'diseases', 'education', 'health']	"About
Community collected, cleaned and organized COVID-19 datasets about India sourced from different government websites which are freely available to all. Here we have digitized them, so it can be used by all the researchers and students.
Infected Data
Column Discription
Main file in this dataset is COVID-19_India_Data.csv and the detailed descriptions are below. 
Date_reported : Date of the observation in YYYY-MM-DD
cum_cases : Cumulative number of confirmed cases till that date
cum_death : Cumulative number of deaths till that date
cum_recovered : Cumulative number of recovered patients till that date
new_recovered : Daily new recovery
new_cases : New confirmed cases. Calculated by:  current cum_cases - previous cum_case
new_death : New confirmed deaths. Calculated by: current cum_death - previous cum_death
cum_active_cases : Cumulative number of infected person till that date. Calculated by: cum_cases - cum_death - cum_recovered
Vaccination data
Column Discription
Main file in this dataset is Vaccination.csv and the detailed descriptions are below. 
date: date of the observation.
total_vaccinations: total number of doses administered. For vaccines that require multiple doses, each individual dose is counted. If a person receives one dose of the vaccine, this metric goes up by 1. If they receive a second dose, it goes up by 1 again. If they receive a third/booster dose, it goes up by again.
people_vaccinated: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.
people_fully_vaccinated: total number of people who received all doses prescribed by the vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.
daily_vaccinations_raw: daily change in the total number of doses administered. It is only calculated for consecutive days. This is a raw measure provided for data checks and transparency, but we strongly recommend that any analysis on daily vaccination rates be conducted using daily_vaccinations instead.
daily_vaccinations: new doses administered per day (7-day smoothed). For countries that don't report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found here.
total_vaccinations_per_hundred: total_vaccinations per 100 people in the total population of the country.
people_vaccinated_per_hundred: people_vaccinated per 100 people in the total population of the country.
Acknowledgements
Johns Hopkins University for making the data available for educational and academic research purposes.
World Health Organization (WHO): https://www.who.int/
Government of India: https://www.mygov.in/covid-19,  Ministry of Health and Family Welfare: https://www.mohfw.gov.in/
Our World in Data: https://ourworldindata.org/covid-vaccinations?country=~IND
https://data.covid19india.org/"	246	1511	9	mdahmadjami	covid19-india
480	480	Berkshire Hathaway - Stock - Latest and Updated	Latest Stock Info of Berkshire Hathaway - Pulled through a Yahoo! Finance API	['business', 'finance', 'beginner', 'tabular data', 'investing']	Berkshire Hathaway Inc. is an American multinational conglomerate holding company headquartered in Omaha, Nebraska, United States. The company wholly owns GEICO, Duracell, Dairy Queen, BNSF, Lubrizol, Fruit of the Loom, Helzberg Diamonds, Long & Foster, FlightSafety International, Shaw Industries, Pampered Chef, Forest River, and NetJets, and also owns 38.6% of Pilot Flying J; and significant minority holdings in public companies Kraft Heinz Company, American Express, The Coca-Cola Company, Bank of America, and Apple. Beginning in 2016, the company acquired large holdings in the major US airline carriers, namely United Airlines, Delta Air Lines, Southwest Airlines, and American Airlines, but sold all of its airline holdings early in 2020. Berkshire Hathaway has averaged an annual growth in book value of 19.0% to its shareholders since 1965, while employing large amounts of capital, and minimal debt. The company is known for its control and leadership by Warren Buffett, who serves as chairman and chief executive, and Charlie Munger, the company's vice chairman.	144	1140	19	kalilurrahman	berkshire-hathaway-stock-latest-and-updated
481	481	International football results from 1872 to 2021	An up-to-date dataset of over 40,000 international football results	['football', 'global', 'sports', 'history', 'international relations']	"Context
Well, what happened was that I was looking for a semi-definite easy-to-read list of international football matches and couldn't find anything decent. So I took it upon myself to collect it for my own use. I might as well share it.
Content
This dataset includes 43,170 results of international football matches starting from the very first official match in 1972 up to 2019.  The matches range from FIFA World Cup to FIFI Wild Cup to regular friendly matches. The matches are strictly men's full internationals and the data does not include Olympic Games or matches where at least one of the teams was the nation's B-team, U-23 or a league select team.
results.csv includes the following columns:
date - date of the match
home_team - the name of the home team
away_team - the name of the away team
home_score - full-time home team score including extra time, not including penalty-shootouts
away_score - full-time away team score including extra time, not including penalty-shootouts
tournament - the name of the tournament
city - the name of the city/town/administrative unit where the match was played
country - the name of the country where the match was played
neutral - TRUE/FALSE column indicating whether the match was played at a neutral venue
shootouts.csv includes the following columns:
date - date of the match
home_team - the name of the home team
away_team - the name of the away team
winner - winner of the penalty-shootout
Note on team and country names: 
For home and away teams the current name of the team has been used. For example, when in 1882 a team who called themselves Ireland played against England, in this dataset, it is called Northern Ireland because the current team of Northern Ireland is the successor of the 1882 Ireland team. This is done so it is easier to track the history and statistics of teams. 
For country names, the name of the country at the time of the match is used. So when Ghana played in Accra, Gold Coast in the 1950s, even though the names of the home team and the country don't match, it was a home match for Ghana. This is indicated by the neutral column, which says FALSE for those matches, meaning it was not at a neutral venue.
Acknowledgements
The data is gathered from several sources including but not limited to Wikipedia, rsssf.com and individual football associations' websites.
Inspiration
Some directions to take when exploring the data:
Who is the best team of all time
Which teams dominated different eras of football
What trends have there been in international football throughout the ages - home advantage, total goals scored, distribution of teams' strength etc
Can we say anything about geopolitics from football fixtures - how has the number of countries changed, which teams like to play each other
Which countries host the most matches where they themselves are not participating in
How much, if at all, does hosting a major tournament help a country's chances in the tournament
Which teams are the most active in playing friendlies and friendly tournaments - does it help or hurt them
The world's your oyster, my friend.
Contribute
If you notice a mistake or the results are being updated fast enough for your liking, you can fix that by submitting a pull request on Github: https://github.com/martj42/international_results"	48042	296324	1407	martj42	international-football-results-from-1872-to-2017
482	482	Cars Price Dataset		[]		2	5	0	priyabhattmtech2021	cars-price-dataset
483	483	CO2 and Greenhouse Gas Emissions	CO2 emissions, Greenhouse gas emissions and Energy	['energy', 'renewable energy', 'electricity', 'oil and gas']	"Context
This complete CO2 and Greenhouse Gas Emissions dataset is a collection of key metrics maintained by Our World in Data. It is updated regularly and includes data on CO2 emissions (annual, per capita, cumulative and consumption-based), other greenhouse gases, energy mix, and other relevant metrics.
Content
CO2 emissions: this data is sourced from the Global Carbon Project. The Global Carbon Project typically releases a new update of CO2 emissions annually.
Greenhouse gas emissions (including methane, and nitrous oxide): this data is sourced from the CAIT Climate Data Explorer, and downloaded from the Climate Watch Portal.
Energy (primary energy, energy mix and energy intensity): this data is sourced from a combination of two sources. The BP Statistical Review of World Energy is published annually, but it does not provide data on primary energy consumption for all countries. For countries absent from this dataset, we calculate primary energy by multiplying the World Bank, World Development Indicators metric Energy use per capita by total population figures. The World Bank sources this metric from the IEA.
Other variables: this data is collected from a variety of sources (United Nations, World Bank, Gapminder, Maddison Project Database, etc.). More information is available in our codebook.
Acknowledgements
Our World in Data
Edouard Mathieu
Bobbie Macdonald
Hannah Ritchie"	42	277	6	danielrpdias	co2-and-greenhouse-gas-emissions
484	484	Scream Dataset	This is for thesis purpose , various scream of human 	[]		1	74	0	sanzidaakterarusha	scream-dataset
485	485	bert-base-multilingual-uncased	bert-base-multilingual-uncased files	[]		9	1154	0	lucca9211	bertbasemultilingualuncased
486	486	COVID-19 Guatemala	Dataset on coronavirus spread in Guatemala.	['public health', 'biology', 'health', 'covid19']	"Last updated April 6rd, 2020
Context
Guatemala is a small, beautiful country in Central America. Although far away from the hotspot in Wuhan, China, the first coronavirus patient was confirmed on March 13th, 2020. Government response was immediate and strong measures were taken from the beginning, but health infrastructure is not as developed as in Spain, France, Italy or US putting citizens at greater risk. Being aware of the havoc and struggle coronavirus has created around the world, we want to: 
Get descriptive statistics and beatiful visualizations of the coronavirus spread in Guatemala
Know how Guatemala's corona virus spread relates and compares to other countries that are a) seemingly winning the fight (China, South Korea, Singapore, etc), b) in the rush to 'flatten the curve' (Italy, Spain, France, UK, etc) and c) starting to see the impact (the rest of the world).
Based on this data,  infer information such as estimates of the real number of infections, projections on the infection rate estimates on the age-sex groups that might be affected the most to what level.
Content
At the moment we have collected confirmed patient information including: age, sex, nationality, infection cause, infection date and others. Find an full english description of the data in the file README_en.md, and a spanish description in the file README_es.md (una descripción completa en español en el archivo README_es.md)
We hope to add more data as it becomes available from official sources.
Acknowledgements
We want to thank to all members and volunteers that are taking hours from their busy schedules to put this dataset together. 
Banner photo is Semuc Champey, an astonishing natural spot in the northern region of Guatemala. Photo by Christopher Crouzet on Unsplash.
Github Repositories
https://github.com/ncovgt2020/covid19_guatemala"	207	5091	9	ncovgt2020	covid19-guatemala
487	487	COVID-19 dataset from Región de Murcia, Spain	COVID-19 dataset based on data from @regiondemuria twitter account 	['covid19']	"Context
COVID-19 dataset based on data from Gobierno de la Región de Murcia, Spain. Data extracted from the daily tweets of the official twitter account of Gobierno de la Región de Murcia.
The data comes from this github repository, and you can explore it and build models in this Kaggle page.
Content
The dataset has one row per day, from the official twitter account of Gobierno de la Región de Murcia, Spain. They publish the daily account via tweet, in an attached image.
They are reporting the information since March 10th, with the following columns:
|Column| Description                                                       | Format             |
|------|-------------------------------------------------------------------|--------------------|
|Fecha | Date of the row, corresponding to the last information of the day | DD/MM/YYYY HH24:MM |
|Personas afectadas | Affected people | Number |
|Aislamiento domiciliario | Home insulation | Number |
|Ingresos totales | Hospitalized patients | Number |
|Ingresos en cuidados intensivos | Intensive care patients | Number |
|Personas curadas | Recovered persons | Number |
|Fallecidos | Deaths | Number |
|Pruebas realizadas | Tests performed | Number |
| Fuente | Data source (tweet from @regiondemurcia) | text |
Acknowledgements
I compile the data checking the tweets by hand, hence, I try to be careful. 
Inspiration
The Gobierno de la Región de Murcia provides the information about COVID-19 by an attached image in a tweet without historical context. Therefore, it's not easy to know how the virus is growing."	46	2641	0	edumardo	covid19-dataset-from-regin-de-murcia-spain
488	488	China fund data in Github	China fund data in Github，refresh eveday。基金数据，包括基金排名和基金净值，每日自动更新。	['investing']	"Context
China fund data in Github，refresh weekly。基金数据，包括基金排名和基金净值，每周自动更新。
www.kaggle.com/dataset/64da2d5a90f702be06cf58e1c02dfccd0b18972cbc7a4217b402110f78e71cb9
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	92	5050	6	darwinwin	china-future-data-in-github
489	489	Exposure DEX State		['health']		3	1702	0	deepeddy	exposure-dex-state
490	490	Dine With Me	Selected Photographs of Food for Image Classification	['culture and humanities', 'image data', 'food', 'restaurants']	"Food Image Classification
Dataset for Multi-target image classification
This dataset contains a selected photographs of ready-to-consume food from different parts of world.
food_annotated.csv file contains the following features to enable supervised image classification:
1. file_name : str [unique values] - jpg image file names
2. type : str [vegetarian,non_vegetarian] - type of food
3. something_sweet : str [present, not_present] - present if there is sweet, dessert, fruit, sweetened drink, sweet corn, etc., and not_present otherwise
4. green_leaves : str [present,not_present] - present if there are green leaves and spinach, and not_present otherwise.
Annotations are performed manually.
Acknowledgements
Direct source of this dataset is a GitHub repo: https://github.com/NandhiniPython/food 
Most of the images are collected from unsplash with thanks."	3	180	1	rajkumarl	dine-with-me
491	491	MineETC		[]		0	2	0	kellymaynard	mineetc
492	492	Embankment dataset		['science and technology']		17	15	0	tom99763	embankment-dataset
493	493	Smiling Friends	"All dialogues from ""Smiling Friends"" animated series"	['arts and entertainment', 'intermediate', 'nlp', 'text mining', 'text data']	"Context
I'm a big fan of the show and i wanted to make some NLP models based on it, because i can't wait to see the next season
Content
Dataset contains all characters lines, even Glep's, i tried my best to write Glep's speech down.
Acknowledgements
Huge thanks to HBO MAX and opensubtitles.org"	5	209	6	jahysama	smilingfriends
494	494	Lyme (Clean & Dirty) Trained Models + Tests		[]		1	5	0	yoctoman	lyme-trained-models-tests
495	495	Hospital admission rates for COVID-19	Hospital and ICU admission rates and occupancy for COVID-19	['health', 'hospitals and treatment centers', 'covid19']	"Context
These data files contain information about hospitalisation and Intensive Care Unit (ICU) admission rates and current occupancy for COVID-19 by date and country. The data are updated weekly. 
Source
https://data.europa.eu/euodp/en/data/dataset/hospital-and-icu-admission-rates-and-occupancy-for-covid-19"	32	1043	4	hgultekin	hospital-admission-rates-for-covid19
496	496	ml_smote	Upsampling API for multilabel classififcation	['computer science']		0	1150	0	senoratiramisu	ml-smote
497	497	Opportunity Insights real time Economic Tracker US	Timely and granular datasets on consumer spending, job openings, and more.	['business', 'economics', 'covid19']	"Context
This dataset is created as part of Opportunity Insights' Economic Tracker, which you can access directly (and with a very nice user interface) at https://www.tracktherecovery.org. The objective of this data is to allow everyone to track the state of the US economy in ""real time"" - in other words, the data is made available almost as soon as it comes in, and thus there is very little lag between the dataset and the current date. Another interesting feature is that the dataset is very granular, meaning that it can inform you about the economy even at the county or city level, and in some cases with breakdowns by economic sector or by income bracket.
Content
In short, the data originates from big companies that offer payments services, or job web portals, etc - these companies share their usage data with Opportunity Insight (in a way that protects user privacy, as you can read in detail in OI's website) and they, in turn, match that data with what would correspond to official statistics. Thus, the dataset can be reasonably interpreted as being a sort of tracker for the economy, even though OI does not tap from the same multitude of sources like the official statistical agencies. Further information about the content, including the specific files, can be found at the README page below.
Acknowledgements
This dataset is fully attributed to Opportunity Insights, and it is their complete merit. If you use it, please make sure to cite it adequately, by pointing to their website (as above) and to the following paper:
""How Did COVID-19 and Stabilization Policies Aﬀect Spending and Employment? A New Real-Time Economic Tracker Based on Private Sector Data"", by Raj Chetty, John Friedman, Nathaniel Hendren, Michael Stepner, and the Opportunity Insights Team. June 2020. Available at: https://opportunityinsights.org/wp-content/uploads/2020/05/tracker_paper.pdf"	766	35442	54	douglaskgaraujo	opportunity-insights-real-time-economic-tracker-us
498	498	Herokuapp Makeup Products	Makeup products from the Heruko App Makeup API	['business', 'make-up and cosmetics']	"Context
This dataset contains makeup products from the Heruko App Makeup API.
It as different brands and product types (like lipsticks and so on) and prices and tags (like vegas etc)."	289	5831	7	oftomorrow	herokuapp-makeup-products
499	499	pytorch-transformers	https://github.com/huggingface/pytorch-transformers	['arts and entertainment']		13	5311	6	vikas15	pytorchtransformers
500	500	Red Rot Sugarcane Disease Leaf Dataset	Binary classification for the red rot	['universities and colleges', 'agriculture', 'beginner', 'cnn', 'image data']	"Context
Collected it for a university project.
Content
In total there are over 900 images. I'd recommend augmenting them to get better results. One thing that might trouble you is that some images have their extensions in capital letters and some have in small letters."	0	13	1	alihussainkhan24	red-rot-sugarcane-disease-leaf-dataset
501	501	NFL-Stats		[]		1	153	0	edsonjaramillo	nflstats
502	502	HappyWhale_TF_128X128	Happy Whale Comp TF Dataset	[]		2	7	0	soumya5891	happywhale-tf-128x128
503	503	ransomware-attacks	ransomware attacks in USA 	[]		0	12	1	samara2022	ransomware-attacks
504	504	yolov5m6Tuned		[]		0	3	0	kennyxie	yolov5m6tuned
505	505	NFL scores and betting data	Scores and descriptive game info for National Football League games	['sports', 'gambling']	"Context
National Football League historic game and betting info
Content
National Football League (NFL) game results since 1966 with betting odds information since 1979. Dataset was created from a variety of sources including games and scores from a variety of public websites such as ESPN, NFL.com, and Pro Football Reference. Weather information is from NOAA data with NFLweather.com a good cross reference. Betting data was used from http://www.repole.com/sun4cast/data.html for 1978-2013 seasons. Pro-football-reference.com data was then cross referenced for betting lines and odds as well as weather data. From 2013 on betting data reflects lines available at sportsline.com.
Acknowledgements
Helpful sites with interest in football and sports betting include:
https://github.com/fivethirtyeight/nfl-elo-game
http://www.repole.com/sun4cast/data.html
https://www.pro-football-reference.com/
http://www.espn.com/nfl/
http://www.nflweather.com/
http://www.noaa.gov/weather
https://www.sportsline.com/
https://github.com/jp-wright/nfl_betting_market_analysis
http://www.aussportsbetting.com/data/historical-nfl-results-and-odds-data/
Inspiration
Can you build a predictive model to better predict NFL game outcomes and identify successful betting strategies?"	13391	95984	308	tobycrabtree	nfl-scores-and-betting-data
506	506	THE VOLCANOES OF EARTH	A detailed list of volcanoes since ancient times	['earth and nature', 'geology', 'natural disasters', 'exploratory data analysis', 'tabular data']	"Context
This dataset contains a detailed list of every volcanos to ever exist on Earth.
Content
A volcano is a rupture in the crust of a planetary-mass object, such as Earth, that allows hot lava, volcanic ash, and gases to escape from a magma chamber below the surface. On Earth, volcanoes are most often found where tectonic plates are diverging or converging, and most are found underwater.
Columns:
Volcano_Name
Volcano_Image
Volcano_Type
Country
Region
Subregion
epoch_period
Last_Eruption
Summit_and_Elevation
Latitude
Longitude
population_within_5km
population_within_10km
population_within_30km
population_within_100km
Acknowledgements
Site & Database of https://volcano.si.edu/"	46	295	16	deepcontractor	the-volcanoes-of-earth
507	507	Urdu Data Set	Roman Urdu DataSet with toxicity in language	['health']		6	591	0	lucca9211	urdu-data-set
508	508	iterative-stratification	Cross validators with stratification for multilabel data.	['earth and nature']		1	445	2	lucca9211	iterativestratification
509	509	Vietnam SARS-CoV-2 | COVID-19 Data	Compiled from Reliable Sources	['finance', 'artificial intelligence', 'computer science', 'deep learning']	"Context
Vietnam SARS-CoV-2 | COVID-19 Compiled Data from Several Reliable Sources such as VnExpress.net, Ministry of Health.
Content
Brief summary of cases 17 - 53.
14/3/2020: Confirmed: 53, Recovered: 16, Death: 0
Column Headers:
- Date: Date of reporting case
- Case: Case ID
- Gender
- Age
- Origin: Last known location before reported
- (Potential) Infection Source: Additional travel information
- Current Location: Last known treatment location
- Confirmed: Tested Positive
- Recovered: Recovered | Tested Negative | No Longer Quarantined
- Death
- Source Information: References
Acknowledgements
NA
Inspiration
I'm interested in SARS-CoV-2 | COVID-19 spread."	476	9455	37	chungtranduc89	vietnam-sarscov2-covid19-data
510	510	COVID19 Cases Switzerland		[]		419	9447	15	daenuprobst	covid19-cases-switzerland
511	511	Dragon Ball Z: Dokkan Battle Card Data	Card data, things such as passives, stats, and links	['games', 'video games', 'tabular data']	"Content
There are two csvs in this dataset, one for normal Dokkan cards and one for Pettan Battle cards. 
dokkan_cards.csv - columns are generally straightforward as to what they're about; the order of stats follows Base, at max level, at 55% HP, and rainbowed; the order of stats in regard to EZAs is for each of the 5 level-increments
pettan_cards.csv - columns are pretty straightforward; you can likely disregard Description if you use this csv
Acknowledgements
The DBZ Dokkan Battle wiki and their team, as that's where this data is scraped from
Inspiration
Do cards of different types have a vastly different distribution of stats?
How has the length of passives changed as time has went on?"	29	424	5	josephvm	dragon-ball-z-dokkan-battle-card-data
512	512	Sensor Based Aquaponics Fish Pond Datasets	IoT Fish Pond Monitoring Datasets	['earth and nature']		38	433	0	ogbuokiriblessing	sensor-based-aquaponics-fish-pond-datasets
513	513	Image.png		['online communities']		0	1	2	dhamur	imagepng
514	514	train_data		[]		1	5	0	jay2333	train-data
515	515	250 IMDB Movies Website		['movies and tv shows']		0	6	0	riteshpanhalkar	250-imdb-movies-website
516	516	git-yolov5		[]		0	4	0	rajanikadebnath	gityolov5
517	517	IDAO22		[]		0	0	0	arnausaumell	idao22
518	518	gbr_starfish_base		[]		0	4	0	lucaordronneau	gbr-starfish-base
519	519	d2chars3		[]		0	8	0	daehoyang	d2chars3
520	520	std_performance		[]		0	4	0	saeedamersaeed	std-performance
521	521	Global Smartphone Shipments (in Mn)	Global Smartphone Shipments Quarterly from Q1 2019 - Q4 2021	['electronics', 'tabular data']	"Content
Global Smartphone Quarterly Market Data (Q1 2019 – Q4 2021)
Acknowledgements
Data source: https://www.counterpointresearch.com
Inspiration
Forecast sales of each smartphone brand in the next year.
Smartphone sales data analysis"	1	18	0	stevieadrianus	global-smartphone-shipments-in-mn
522	522	dla resources		['education']		0	2	0	nazmuddhohaansary	dla-resources
523	523	dataset for D-Data Drift 2022		[]		0	5	1	akash201432	dataset-for-ddata-drift-2022
524	524	Words for Wordle	Frequent Words For Solving Wordle	['games', 'asia', 'beginner', 'data analytics', 'tabular data']	This dataset is a curated list of frequently used English Dictionary Words. This dataset is mainly designed to help a user solve the Wordle Game in a much faster way.	1	79	4	uniquekale	wordle-words
525	525	COVID-19 containment and mitigation measures	Policies to reduce the transmission of COVID-19	['business', 'covid19']	"Context
Dataset of COVID-19 containment and mitigation measures
http://epidemicforecasting.org/containment
The dataset attempts to cover all measures of national significance intended to reduce the transmission of COVID-19, in all the worlds nations.
This is work in progress.  Currently there are &gt; 1000 entries.  More details at http://epidemicforecasting.org/containment
This dataset pairs well with this dataset that has more than 10,000 entires.
Content
Each measure in the database has entries on:
- Country (and state for the US)
 - Textual description of the measure
 - Start date of measure
 - End date (if available)
 - URL to source of more information
 - Systematic keyword labels (e.g. ""travel ban"" or ""hygiene enforcement"")
Acknowledgements
Banner Photo by CDC on Unsplash 
Data from http://epidemicforecasting.org/containment"	4370	59356	162	paultimothymooney	covid19-containment-and-mitigation-measures
526	526	rsna trainlabels 		[]		0	0	0	shivanir23	rsna-trainlabels
527	527	OECD languages avg. characters in NOs 1-1000	Mean number of characters in the written numbers 1-1000 in OECD languages	[]		0	10	0	livizhere	oecd-languages-avg-characters-in-nos-11000
528	528	EDA ON AIRLINE DATA SET (UNIVARIATE)	UNIVARIATE DATA ANALYSIS	[]		1	2	0	soyabshekh	eda-on-airline-data-set-univariate
529	529	Mall_Customers		[]		0	2	0	menesakpinar	mall-customers
530	530	Pakistan COVID-19 Dataset	This is the data repository for the 2019 Novel Coronavirus cases in Pakistan.	['asia', 'diseases', 'people', 'health', 'covid19']	"Pakistan COVID-19 Dataset
This is the data repository for the 2019 Novel Coronavirus cases in Pakistan.
Daily reports (daily_reports)
This folder contains daily case reports. All timestamps are in UTC (GMT+0). Provincial Data is only available from 11th April 2020, previous reports have data of Pakistan as whole.
File naming convention
YYYY-MM-DD.csv in UTC.
Field description
<b>Province_State</b>: Province, state or dependency name.
<b>Country_Region</b>: Country, region or sovereignty name.
<b>Last Update</b>: YYYY-MM-DD HH:mm:ss  (24 hour format, in UTC).
<b>Lat</b> and <b>Long_</b>: Latitude and Longitude locations on the map. All points shown on the map are based on geographic centroids, and are not representative of a specific address, building or any location at a spatial scale finer than a province/state.
<b>Confirmed</b>: Counts include confirmed and probable (where reported).
<b>Deaths</b>: Counts include confirmed and probable (where reported).
<b>Recovered</b>: Recovered cases are estimates based on local media reports, and state and local reporting when available, and therefore may be substantially lower than the true number.
<b>Active:</b> Active cases = total cases - total recovered - total deaths.
<b>Incident_Rate</b>: Incidence Rate = cases per 100,000 persons.
<b>Case_Fatality_Ratio (%)</b>: Case-Fatality Ratio (%) = Number recorded deaths / Number cases.
All cases, deaths, and recoveries reported are based on the date of initial report.
Combined report (combined_report.csv)
This file contains all the daily cases reports combined into one.
Data sources
Government of Pakistan
JHU CSSE COVID-19 Dataset"	27	751	2	dekhpakistan	pakistan-covid19-dataset
531	531	keras_retinanet	Keras implementation of RetinaNet object detection.	['software', 'computer vision']	"Source repository
https://github.com/fizyr/keras-retinanet
License
Apache License 2.0"	21	3141	0	mak4alex	keras-retinanet
532	532	Tower Apartment in Japan	List of tower apartments in Japan	[]	"List of tower apartments in Japan
Columns
Name: (Japanese)
Height(m)
Number floors
Year of completion
Latitude
Longitude
Source
https://skyskysky.net/apartment-list.html
https://ja.wikipedia.org/wiki/%E8%B6%85%E9%AB%98%E5%B1%A4%E3%83%9E%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%B3"	1	14	2	stpeteishii	tower-apartment-in-japan
533	533	rsna pnemonia data & weight 		[]		1	8	0	shivanir23	rsna-pnemonia-data-weight
534	534	online_retail		[]		0	0	0	menesakpinar	online-retail
535	535	NBME Preprocessing AlBERT Public		['standardized testing']		0	5	1	markwijkhuizen	nbme-preprocessing-albert-public
536	536	Speech Enhancement		[]		1	246	0	zenbot99	speech-enhancement
537	537	Astor Wines	Wine in sales on https://www.astorwines.com/	['alcohol']		37	272	3	philmod	astor-wines
538	538	TF LongFormer v3 splits2		['video games']		0	3	0	dddmdd	tf-longformer-v3-splits2
539	539	Solar power by country	Solar power by country from 2016-2020	['renewable energy', 'tabular data']	"Context
New installation and total solar power capacity (MW) in each country from 2016-2020.
Acknowledgements
Data source: https://en.wikipedia.org/wiki/Solar_power_by_country
Inspiration
Forecast installed solar power capacity."	134	756	15	prasertk	solar-power-by-country
540	540	Air Quality Monitoring from EcoCity	Air Quality Monitoring in Vinnytsia city and region	['environment', 'pollution', 'business', 'data visualization', 'time series analysis']	"Context
The dataset contains data from public monitoring of air quality in Ukraine (for Vinnytsia city and region - population: 1 575 808, square: 26513 sq. km).
Content
Many stations monitor air quality day and night according to the following indicators: 
pm1 - PM1.0 (dust particles with a size of 1 μm), mkg/cub.m
pm25 - PM2.5 (dust particles with a size of 2.5 μm), mkg/cub.m
pm10 - PM10 (dust particles with a size of 10 μm), mkg/cub.m
CO2 - carbon dioxide, mkg/cub.m
temperature - temperature, in degrees Celsius
humidity - Humidity, %
pressure - Pressure, millimeters of mercury
In some stations, there may be additional indicators.
The stations belong to the monitoring network ""Eco-City"".
Data are measured at intervals of 15 seconds to several minutes (145 seconds or more). Sometimes longer periods of time are possible, due to power outages and/or communications.
Acknowledgments
I thank service ""Eco-City"" for the opportunity to download data.
Inspiration
It is interesting to solve the following problems, which are typical for other cities and regions around the world:
1. Analyze the basic patterns of data change in the city or regions as a whole and for each station separately (seasonality, anomalies, etc.).
2. Forecast data for the following dates, which is easy to check for new data from service ""Eco-City"". New data in the dataset will be updated periodically.
3. Forecast the data for the current day. For comparison, they can be downloaded from service ""Eco-City"" via API.
4. Compare public monitoring data with state monitoring data available through API.
I will prepare baseline notebooks to solve these problems and invite others to improve them."	98	1713	14	vbmokin	air-quality-monitoring-from-ecocity
541	541	TCS Stock Data - Live and Latest	TCS Stock information from India's NSE - From IPO till date	['business', 'finance', 'economics', 'computer science', 'deep learning']	"Tata Consultancy Services (TCS) is an Indian multinational information technology (IT) services and consulting company headquartered in Mumbai, Maharashtra, India with its largest campus located in Chennai, Tamil Nadu, India. As of February 2021, TCS is the largest IT services company in the world by market capitalisation ($200 billion). It is a subsidiary of the Tata Group and operates in 149 locations across 46 countries.
TCS is the second largest Indian company by market capitalisation and is among the most valuable IT services brands worldwide.In 2015, TCS was ranked 64th overall in the Forbes World's Most Innovative Companies ranking, making it both the highest-ranked IT services company and the top Indian company. As of 2018, it is ranked eleventh on the Fortune India 500 list.In April 2018, TCS became the first Indian IT company to reach $100 billion in market capitalisation and second Indian company ever (after Reliance Industries achieved it in 2007) after its market capitalisation stood at ₹6.793 trillion (equivalent to ₹7.3 trillion or US$100 billion in 2019) on the Bombay Stock Exchange.
In 2016–2017, parent company Tata Sons owned 72.05% of TCS and more than 70% of Tata Sons' dividends were generated by TCS. In March 2018, Tata Sons decided to sell stocks of TCS worth $1.25 billion in a bulk deal.As of 15 September 2021, TCS has recorded a market capitalisation of US$200 billion, making it the first Indian IT firm to do so."	285	2306	25	kalilurrahman	tcs-stock-data-live-and-latest
542	542	COVID19 Global Cases	covid19 data from github	[]		3	314	1	sophiaray	covid19-global-cases
543	543	EfficientNet Keras Source Code	https://github.com/qubvel/efficientnet	['computer science', 'software', 'computer vision', 'cnn']	"About this
This is a mirror of the EfficientNet repo for offline usage. Please refer to the readme for more information.
References
Image retrieved from the efficientnet blog post"	663	6883	51	xhlulu	efficientnet-keras-source-code
544	544	Covid in African Countries - Latest Data	Covid-19 Data as on February 12, 2022	['africa', 'health', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of African countries as on February 12, 2022
Attribute Information
Country - Name of African countries
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population- Total Cases per 1 million of the population
Deaths/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests done per 1 million of the population
Population - Population of the country
Source
Link : https://www.worldometers.info/coronavirus/#countries
Other Updated Covid19 Datasets
Link : https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting 👍
Thank You"	521	3098	53	anandhuh	covid-in-african-countries-latest-data
545	545	whocovid19globaldata	WHO Coronavirus (COVID-19)	[]		3	43	0	emirhanozkan	whocovid19globaldata
546	546	"Netflix ""Top 10"" TV Shows and Films"	Dataset of Top 10 Netflix films and TV shows globally and by countries	['arts and entertainment', 'movies and tv shows', 'education', 'data visualization', 'tabular data']	"Every Tuesday, Netflix publishes four global Top 10 lists for films and TV: Film (English), TV (English), Film (Non-English), and TV (Non-English). These lists rank titles based on weekly hours viewed: the total number of hours that members around the world watched each title from Monday to Sunday of the previous week.
Each season of a series and each film is considered on their own, so you might see both Stranger Things seasons 2 and 3 in the Top 10. Because titles sometimes move in and out of the Top 10, there is also the total number of weeks that a season of a series or film has spent on the list.
Netflix also publishes Top 10 lists for nearly 100 countries and territories (the same locations where there are Top 10 rows on Netflix). Country lists are also ranked based on hours viewed but don’t show country-level viewing directly.
Finally, Netflix provides a list of the Top 10 most popular Netflix films and TV (branded Netflix in any country) in each of the four categories based on the hours that each title was viewed during its first 28 days."	1259	6762	60	dhruvildave	netflix-top-10-tv-shows-and-films
547	547	Python Data Visualization Essentials Guide	For Exercises in Data Visualization	['computer science', 'programming', 'data visualization', 'classification', 'regression', 'python']	"context
The information collated for Python Data Visualization Essentials Guide - Book
sources
Generally available in the public internet
inspiration
All the great data scientists, statisticians, programmers and enthusiasts
availability
Available in a github page https://github.com/kalilurrahman/dataset"	162	2735	18	kalilurrahman	python-data-visualization-essentials-guide
548	548	COVID-19 India Dataset @ IndoML 2021	Detailed COVID-19 Data from Daily Health Bulletins Published by Indian States	['india', 'health', 'exploratory data analysis', 'data visualization', 'time series analysis', 'covid19']	"COVID-19 Datathon @ IndoML 2021
The COVID-19 India Dataset is one of the most comprehensive datasets on the pandemic in India. It aggregates data from health bulletins published online daily by governments of major Indian states. This Datathon event challenges you to flex your brains on this dataset and come up with your own models for analysis, prediction, and insights on the evolution of the pandemic in India.
Unlike the other entries into the IndoML Datathon, this challenge is completely open-ended. This means that there is no specific task to solve. Instead, we want you to push your limits. However, if you do want some sample tasks you can get started with, we have some examples¸below. 
The Dataset &nbsp; Challenge Tasks
Participating in the Datathon
Join our Slack community and follow instructions pinned to the #indoml channel. 
Clone this repository and start exploring the COVID-19 India Dataset. 
Let your imagination run wild. 🤓 
🔢 Submissions
Your final submission will require two elements: 
A report documenting your efforts to be judged by our judges, and
A GitHub (or equivalent) link to your code that generated the findings in the report. This could be either a link to a PR merged into this codebase or a standalone repository containing your code. 
Submit
🏆 Prizes
Winners at the end of the Datathon will be determined by our selection of judges. The Top 3 participants, as determined by our judges, will receive cash prizes with a combined worth of approximately 75,000 INR. In addition, all major contributors will be invited to join the whitepaper on the COVID-19 India Dataset to include their findings, analysis, and models. Make sure to read the terms and conditions to ensure eligibility for cash prizes.
To encourage participation from India, we require that your team must have one collaborator from India. Please make sure to read the terms and conditions before participating.
Terms and Conditions &nbsp; Slack &nbsp; Kaggle
Timeline
Dec 1 2021 Start of Datathon
Dec 16 - 18 IndoML Symposium
Dec 06 - 14 NeurIPS 2021 (Progress Report)
Jan 31 2022 End of Datathon
Feb 22 - Mar 01 2022 AAAI 2022 (Progress Report)
Mar 01 2022 Winners Announced  
Judges
Thank you to all our judges and mentors who have joined us to guide our participants along the way. 🙌 
Tavpritesh Sethi 
Biplav Srivastava
Pawan Goyal"	129	2031	12	datathon2021	covid19-india-dataset-indoml-2021
549	549	Spatio-Temporal GAN		[]		4	112	0	dhruvsheth12345	spatiotemporal-gan
550	550	2013 US Flight Data	Flight Delay and Cancellation Prediction	['united states', 'aviation']	"Context
The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report, published about 30 days after the month's end, as well as in on-time data posted on this website.
Content
A flight is considered delayed if it is late by more than 15 minutes.
Factors considered:
Year: The year of the flight (all records are from 2013)
Month: The month of the flight
DayofMonth: The day of the month on which the flight departed
DayOfWeek: The day of the week on which the flight departed - from 1 (Monday) to 7 (Sunday)
Carrier: The two-letter abbreviation for the airline.
OriginAirportID: A unique numeric identifier for the departure aiport
OriginAirportName: The full name of the departure airport
OriginCity: The departure airport city
OriginState: The departure airport state
DestAirportID: A unique numeric identifier for the destination aiport
DestAirportName: The full name of the destination airport
DestCity: The destination airport city
DestState: The destination airport state
CRSDepTime: The scheduled departure time
DepDelay: The number of minutes departure was delayed (flight that left ahead of schedule have a negative value)
DelDelay15: A binary indicator that departure was delayed by more than 15 minutes (and therefore considered ""late"")
CRSArrTime: The scheduled arrival time
ArrDelay: The number of minutes arrival was delayed (flight that arrived ahead of schedule have a negative value)
ArrDelay15: A binary indicator that arrival was delayed by more than 15 minutes (and therefore considered ""late"")
Cancelled: A binary indicator that the flight was cancelled
Acknowledgements
The flight delay and cancellation data was collected and published by the DOT's Bureau of Transportation Statistics."	6	56	2	aksathomas	2013-us-flight-data
551	551	Autoruparsing0122		[]		0	0	0	hytryidmitry	autoruparsing0122
552	552	articles_extended	H&M Personalized Fashion Recommendations - Extended Articles File	['clothing and accessories', 'tabular data', 'retail and shopping', 'social networks']	"Extended the articles.csv by the image path for each article.
Competition link - https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations"	0	13	0	oberfink	articles-extended
553	553	Detect COTS Models		['earth and nature']		4	44	0	hngbiquc	detect-cots-models
554	554	Board Game Geek Rankings 	Board Game Geek top 2000 games rankings	['games', 'board games']	Board Game Geek top 2000 games rankings and other info	204	3212	14	mseinstein	bgg_top2000
555	555	jigsaw-toxic-2021-99th-place		['exercise']	"Solution to 99th place of competition https://www.kaggle.com/c/jigsaw-toxic-severity-rating
Here the code https://github.com/vilka-lab/JigsawRate
Model weights for described solution are here."	0	3	0	ivanilyushchenko	jigsawtoxic202199thplace
556	556	car_subsricption		[]		0	6	0	sahranjit77	car-subsricption
557	557	CrossFit Dataset	A collection of CrossFit Games Data from 2013-2015	['exercise', 'sports']	"About this dataset
&gt; <h2>Sumary</h2>
<p>There isn't a ton of open data about CrossFit. It's a relatively new phenomenon after all. That said, recently, CFHQ released some CrossFit games data and smart guys like Sam Swift have starting doing some awesome work. I contacted Sam and with his blessing I'm posting it here. I've only taken some of the data from Sam's work, the original complete dataset and commentary can be found <a href=""http://swift.pw/crossfit-games-data-2012-2015/"" target=""_blank"" rel=""nofollow"">here</a>.</p>
<p>I'm curious to see what kind of patterns, trends, and learnings can be pulled out of this data. From first glance it appears the data here is around the competition and athlete statistics from 2013-2015.</p>
<p>Somethings I can see that need to be done to start making this actionable:</p>
<ul class=""task-list"">
<li class=""task-list-item""><input class=""task-list-item-checkbox"" disabled="""" type=""checkbox""> Add labels to data</li>
<li class=""task-list-item""><input class=""task-list-item-checkbox"" disabled="""" type=""checkbox""> Figure out if any data cleaning is needed</li>
<li class=""task-list-item""><input class=""task-list-item-checkbox"" disabled="""" type=""checkbox""> Generate initial exploratory visualizations</li>
<li class=""task-list-item""><input class=""task-list-item-checkbox"" disabled="""" type=""checkbox""> Propose hypothesis about what this data could reveal.</li>
</ul>
<hr>
<p><em>Here is the original commentary from Sam's post.</em></p>
<h2>Mission part 1: Bring CrossFit data mainstream.  Success!</h2>
<p>Part 1 in my mission to bring more data to the CrossFit Open is now a success.  I always wondered why CrossFit HQ didn’t do more to bring the story of the Open to the fans and participants through its data.  Now they have.  The 4 x “M” team of Mike Macpherson and Megan Mitchell brought us the first ever analytical piece from HQ breaking down performances on 15.3.  It bears a striking resemblance to the analytical approach I’ve taken here in analyses of 2014, 15.1, and 15.2, and I’m happy to have made an impression.  This is CrossFit, you can’t get out to a lead and expect that nobody will catch up.  It’s better if they do.</p>
<h2>Mission part 2: Take CrossFit data to the next level.</h2>
<p>I’m definitely not the only one playing with this data.  I’ve linked to some of the other interesting projects this year:</p>
<p>Chris Simpkins’ <a href=""http://csimpkinslatd.github.io/cf-open-2015/"" target=""_blank"" rel=""nofollow"">dynamic histograms with D3</a><br>
Beyond the White Board’s <a href=""http://blog.beyondthewhiteboard.com/2015/03/06/crossfit-open-15-2-is-14-2-workout-analysis-breakdown/"" target=""_blank"" rel=""nofollow"">breakdowns of their data</a><br>
Harold Doran’s <a href=""https://hdoran.shinyapps.io/openAnalysis/"" target=""_blank"" rel=""nofollow"">super interactive shiny app</a>.</p>
<p>I think we’ve got a lot more skills out there but the overhead of scraping all the data is somewhat prohibitive, especially in the case of athlete profile data. I’d be thrilled if more people could contribute interesting analyses, so I thought I’d share everything I’ve collected and see where it takes us.  CrossFit has an appreciation for the quantitative approach in its genes.  Lets see if we can make it an example for other amateur sports of what’s possible.</p>
<p>Please let me know if you’re working on something interesting, and I’d be interested to collaborate if I have time.  If you use the data I’ve collected for something public, please just reference this post, thanks!</p>
<h2>What are we working with</h2>
<p>All data posted in .csv, zipped .csvs (.zip), and R binary (.RData) for your convenience.</p>
<ul>
<li><a href=""https://drive.google.com/drive/u/0/#folders/0B-JqrzitvsWSZ2NyM1NKd2EwRnc/0B-JqrzitvsWSfmVjY1BmQzVPblZ5WjEzVTktakt2b195ZEpLS1UxZmtDMU5iWERDZ0RFUVU"" target=""_blank"" rel=""nofollow"">Leaderboard scores</a>, ranks, athlete_ids, and scaled designations.  2012, 2013, 2014, and 2015 to date.  2015 will be added as available.</li>
<li>fields refer to the URL parameters used by the games.crossfit site<br>
division: 1= male, 2=female</li>
<li>stage: 0 = roster (all athletes who signed up), 1 = WoD 1 of that year, etc</li>
<li>score: expressed in reps or seconds</li>
<li>scaled: 0 = Rx, scaled = 1.  all NA before 2015</li>
<li>I refer to 15.1A as 15.1.1 so that the database field can be numeric<br>
the scraping process is not 100% successful, so there may be a small number of missing records.  Not a problem if you’re summarizing trends.  Might be a problem if you’re doing reporting for individuals.</li>
<li><a href=""https://drive.google.com/drive/u/0/#folders/0B-JqrzitvsWSZ2NyM1NKd2EwRnc/0B-JqrzitvsWSfmVjY1BmQzVPblZ5WjEzVTktakt2b195ZEpLS1UxZmtDMU5iWERDZ0RFUVU"" target=""_blank"" rel=""nofollow"">Athlete profiles</a>: everything on the athlete profile pages (age, weight, affiliate, team, lift PRs, workout PRs, background, training and diet descriptives).<br>
athlete profiles can change at any time.  73% of records have a retrieved_datetime field to make this less ambiguous.  The rest were scraped before I thought to do that, but were scraped in March 2015.</li>
<li>some of the profile fields are very sparse and are optionally self-reported. amateur statisticians beware.</li>
<li>some (~20%) user profile pages do not exist.  these are mostly athletes who only participated in the earlier years, but there’s no real rhyme or reason to it as far as I can tell.</li>
<li><a href=""https://github.com/swiftsam/CrossfitRankings"" target=""_blank"" rel=""nofollow"">Code (written in R)</a> to scrape, compile, and analyze as a starting point</li>
</ul>
This dataset was created by Brandon Gadoci and contains around 400000 samples along with Region, Height, technical information and other features such as:
- Candj
- Run5k
- and more.
How to use this dataset
&gt; - Analyze Eat in relation to Backsq
- Study the influence of Background on Experience
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Brandon Gadoci 
Start A New Notebook!"	26	236	1	yamqwe	crossfit-datae
558	558	Human Detection Dataset	CCTV footage of humans 	['arts and entertainment', 'earth and nature', 'image data']	"Context
It is important to detect humans on footage of CCTV, so, let us use this dataset to train a neural network to do it
Content
see 'footage' folder
Dataset contains CCTV footage images(as indoor as outdoor), a half of them w humans and a half of them is w/o humans. Images is marked as follow:
0_n.png or 1_n.png
the first digit is a class of image, 0 means a scene without humans, and 1 means a scene with humans.
n is just a number of an image in the whole dataset
Sources of dataset:
1) cctv footage from youtube;
2) open indoor images dataset;
3) footage from my cctv."	1206	12665	77	constantinwerner	human-detection-dataset
559	559	exp61v5s		[]		0	2	0	dengyuchen	exp61v5s
560	560	deleteeefientdata		[]		0	5	0	mollelmichael	deleteeefientdata
561	561	Silicon Valley Diversity Data	This database contains EEO-1 reports filed by Silicon Valley tech companies.	['business']	"About this dataset
&gt; <h1>Silicon Valley diversity data</h1>
<p>This database contains EEO-1 reports filed by Silicon Valley tech companies. It was compiled by Reveal from The Center for Investigative Reporting.</p>
<p>Please <a href=""https://www.revealnews.org/article/how-we-analyzed-silicon-valley-tech-companies-diversity-data"" target=""_blank"" rel=""nofollow"">read our complete methodology</a> for details on this data.</p>
<h3>Copyright and license</h3>
<p>The EEO-1 database is licensed under the Open Database License (ODbL) by Reveal from The Center for Investigative Reporting.</p>
<p>You are free to copy, distribute, transmit and adapt the spreadsheet, so long as you:</p>
<ul>
<li>credit Reveal as specified below;</li>
<li>inform Reveal that you are using the data in your work by emailing Sinduja Rangarajan at <a href=""mailto:srangarajan@revealnews.org"" target=""_blank"" rel=""nofollow"">srangarajan@revealnews.org</a>; and</li>
<li>offer any new work under the same license.</li>
</ul>
<p>The <a href=""https://opendatacommons.org/licenses/odbl/1.0/"" target=""_blank"" rel=""nofollow"">full legal code</a> explains your rights and responsibilities.</p>
<h3>How to credit Reveal</h3>
<p>We require that you use the credit “Reveal from The Center for Investigative Reporting”. If it is distributed online, the credit must link to <a href=""https://www.revealnews.org/svdiversity"" target=""_blank"" rel=""nofollow"">https://www.revealnews.org/svdiversity</a>.</p>
<p>You must also make it clear to anyone who requests access to the data that it is available under the Open Database License. You can <a href=""https://opendatacommons.org/licenses/odbl/1.0/"" target=""_blank"" rel=""nofollow"">link directly to the license</a>.</p>
<h3>Fields in the data</h3>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Column name</th>
<th>Format</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>company</td>
<td>String</td>
<td>Company name</td>
</tr>
<tr>
<td>year</td>
<td>Integer</td>
<td>For now, 2016 only</td>
</tr>
<tr>
<td>race</td>
<td>String</td>
<td>Possible values: ""American_Indian_Alaskan_Native"", ""Asian"", ""Black_or_African_American"", ""Latino"", ""Native_Hawaiian_or_Pacific_Islander"", ""Two_or_more_races"", ""White"", ""Overall_totals""</td>
</tr>
<tr>
<td>gender</td>
<td>String</td>
<td>Possible values: ""male"", ""female"". Non-binary gender is not counted in EEO-1 reports.</td>
</tr>
<tr>
<td>job_category</td>
<td>String</td>
<td>Possible values: ""Administrative support"", ""Craft workers"", ""Executive/Senior officials &amp; Mgrs"", ""First/Mid officials &amp; Mgrs"", ""laborers and helpers"", ""operatives"", ""Professionals"", ""Sales workers"", ""Service workers"", ""Technicians"", ""Previous_totals"", ""Totals""</td>
</tr>
<tr>
<td>count</td>
<td>String</td>
<td>Mostly integer values, but contains ""na"" for a no-data variable.</td>
</tr>
</tbody>
</table></div>
<p>Source: <a href=""https://github.com/cirlabs/Silicon-Valley-Diversity-Data"" target=""_blank"" rel=""nofollow"">https://github.com/cirlabs/Silicon-Valley-Diversity-Data</a></p>
This dataset was created by Technology and contains around 4000 samples along with Race, Gender, technical information and other features such as:
- Company
- Year
- and more.
How to use this dataset
&gt; - Analyze Job Category in relation to Count
- Study the influence of Race on Gender
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Technology 
Start A New Notebook!"	26	255	2	yamqwe	silicon-valley-diversity-datae
562	562	iPhone Sales	Apple Apple unit sales and other features for Yoy Growth	['business', 'finance', 'economics']	"About this dataset
&gt; <h1>Apple iPhone unit sales &amp; revenue</h1>
<p><code>In-the-News</code> <a href=""https://news.google.com/news/story?ncl=dc-JBTVrNmljyuMou3xSKieAZj6IM&amp;q=apple+iphone+7&amp;lr=English&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj9nLXe4qXPAhUX-mMKHYaMD3cQqgIIKDAA"" target=""_blank"" rel=""nofollow"">Full coverage from Google News</a></p>
<p>Source: all data and charts from <a href=""https://barefigur.es/companies/apple/iphone/"" target=""_blank"" rel=""nofollow"" title=""Title"">Bare Figures</a></p>
<p>""<em>Please note</em>: Quarters are calendar quarters, not fiscal quarters. Apple’s fiscal quarters: Q1 Oct–Dec, Q2 Jan–Mar, Q3 Apr–Jun, Q4 Jul–Sep.""</p>
<p><img src=""http://i.imgur.com/rf5IOQe.png"" alt=""iPhone unit sales"" style=""""></p>
<p><img src=""http://i.imgur.com/Wyf8E1W.png"" alt=""iPhone revenue"" style=""""></p>
This dataset was created by Rafael Pereira and contains around 0 samples along with Yoy Growth, Yoy Growth, technical information and other features such as:
- Yoy Growth
- Yoy Growth
- and more.
How to use this dataset
&gt; - Analyze Yoy Growth in relation to Yoy Growth
- Study the influence of Yoy Growth on Yoy Growth
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Rafael Pereira 
Start A New Notebook!"	103	712	4	yamqwe	iphone-salese
563	563	Covid19 in World Countries-Latest Data	Covid-19 Data as on February 12, 2022	['global', 'health', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of world countries as on February 12, 2022
Attribute Information
Country - Name of world countries
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population- Total Cases per 1 million of the population
Death/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests done per 1 million of the population
Population - Population of the country
Source
Link : https://www.worldometers.info/coronavirus/#countries
Other Updated Covid 19 Datasets
Link : https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting  ❤️ 
Thank You"	1545	8953	75	anandhuh	covid19-in-world-countrieslatest-data
564	564	corpusoliver		[]		0	2	0	fernandobordi	corpusoliver
565	565	GBR Starfish TFRecords Mini 2X 2 0		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-2x-2-0
566	566	GBR Starfish TFRecords Mini 2X 2 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-2x-2-1
567	567	GBR Starfish TFRecords Mini 2X 0 2		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-2x-0-2
568	568	Active AutoPark 		[]		0	2	0	bachoge	active-autopark
569	569	AutoPark Active 20220131.CSV		[]		0	0	0	bachoge	autopark-active-20220131csv
570	570	Bank Loan Default		[]		3	16	0	namit2303	bank-loan-default
571	571	ubi ihopeIamright		[]		0	18	0	yus002	ubi-ihopeiamright
572	572	Microsoft Stock Details - Updated Regularly	Yahoo! Download of historic stock info	['business', 'finance', 'data analytics', 'investing']	"Microsoft Corporation is an American multinational technology corporation that produces computer software, consumer electronics, personal computers, and related services. Its best-known software products are the Microsoft Windows line of operating systems, the Microsoft Office suite, and the Internet Explorer and Edge web browsers. Its flagship hardware products are the Xbox video game consoles and the Microsoft Surface lineup of touchscreen personal computers. Microsoft ranked No. 21 in the 2020 Fortune 500 rankings of the largest United States corporations by total revenue; it was the world's largest software maker by revenue as of 2016. It is considered one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Amazon, and Facebook.
Context
Stock data is a very good tool for analysis in terms of EDA, Visualization a and predictions. Microsoft stock data is a brilliant one to consider
Content
Historic stock data downloaded from Yahoo! Finance
Acknowledgements
Yahoo! Finance Python API developers
Inspiration
All the Kagglers and budding data scientists!"	124	1589	14	kalilurrahman	microsoft-stock-details-updated-regularly
573	573	speeches - modi	Speeches given by PM Narendra Modi from Aug '14 to Aug '20	['government', 'politics']	"Context
Narendra Damodardas Modi is an Indian politician serving as the 14th Prime Minister of India since 2014. He was the Chief Minister of Gujarat from 2001 to 2014 and the Member of Parliament for Varanasi. 
Content
Transcript of the speeches given by Narendra Modi made publicly available at https://www.pmindia.gov.in/
The same speeches are available in both CSV and JSON format.
Scraped from https://www.pmindia.gov.in/en/tag/pmspeech/ using Selenium and Python's Beautiful Soup package."	133	2015	9	abhisheksjmr	speeches-modi
574	574	Smart Mobility Hubs Terminals	Data about all the terminals deployed under Smart Mobility Hubs project.	['business']	"About this dataset
&gt; <p>This dataset presents information about all the terminals deployed under the Smart Mobility Hubs project. A Terminal is a single sensor associated with the account.</p>
<p>Source: <a href=""https://www.ikesmartcity.com/"" target=""_blank"" rel=""nofollow"">https://www.ikesmartcity.com/</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2020-03-25</p>
This dataset was created by Kelly Garrett and contains around 3000 samples along with Id, Serial, technical information and other features such as:
- Last Sync At
- Last Sync Ip
- and more.
How to use this dataset
&gt; - Analyze Venue Id in relation to Name
- Study the influence of Id on Serial
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	5	53	1	yamqwe	smart-mobility-hubs-terminalse
575	575	rsna pnemonia detection challenge & weight files		[]		1	12	0	shivanir23	rsna-pnemonia-detection-challenge-weight-files
576	576	happywhale_classification_val		[]		0	4	0	wenzhengdu	happywhale-classification-val
577	577	Indian Equity Data 	Historical Data of Indian Equities from Jan 2020	['business', 'investing']	"Indian Stock Market Data
Daily Data of almost 2000 stocks listed on the National Stock Exchange of India from 1 January 2020.
I will update this database on a weekly basis.
Any recommendations are welcome.
Thank you!"	1	10	0	harshsharma1805	indian-equity-data
578	578	Every single Walmart store location	A comprehensive list of Walmart Stores in the US.	['business', 'retail and shopping']	"About this dataset
&gt; <h3>Walmart Store Location Data</h3>
<p>Walmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores, headquartered in Bentonville, Arkansas</p>
<p>This is a complete list of all Walmart store locations, along with their geographic coordinates, Street addresses, City, State, ZIP code etc in the US.</p>
<h3>Get data for free</h3>
<p>Contact Datahut (<a href=""https://datahut.co/"" target=""_blank"" rel=""nofollow"">https://datahut.co/</a>) for more information and a fresh data set. We give this data for free for startups, journalists bloggers, researchers, analysts, etc.</p>
This dataset was created by Tony Paul and contains around 5000 samples along with Youtube, State, technical information and other features such as:
- Website
- Open Hours
- and more.
How to use this dataset
&gt; - Analyze Fax 2 in relation to City
- Study the influence of Fax 1 on Facebook
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tony Paul 
Start A New Notebook!"	14	162	2	yamqwe	walmart-store-location-datasete
579	579	all_dataset		[]		1	7	0	jyothiellur	all-dataset
580	580	Generated Abstract Art	Using GANs to generate abstract artworks	['art', 'intermediate', 'advanced', 'deep learning', 'gan', 'image data']	"Context
I used Generative x Diffusion models to generate 512x512 AI images with abtract style art.
Content
Hundreds of generated images to come, stay tuned !"	5	128	9	bryanb	generated-abstract-dataset-diffusion
581	581	mask data		[]		0	5	0	reshesh	mask-data
582	582	zybishe		[]		0	2	0	troyyxu	zybishe
583	583	YelpDataset	Yelp-REVIEWS-Dataset	['ratings and reviews']		0	4	0	hamedetezadi	yelpdataset
584	584	Samsung Electronics Stock Historical Price 	005930.KS Stock Price from Jun 2019 - Feb 2022	['business', 'finance', 'banking', 'economics', 'investing', 'news']	"Samsung Electronics Stock Historical Price (005930.KS)
from June 2019 until February 2022
▶ Context 📝
The dataset is taken from Yahoo Finance website. It is about historical stock price for Samsung Electronics Co., Ltd.
▶ Acknowledgements 🙏
I'd like to clarify that I'm only making data about historical stock price of Samsung Electronics Co., Ltd. available to Kaggle community. 
▶ Inspiration 💭
Forecasting stock price after June 2019
Implementing machine learning models.
Perform data analysis/data visualization of historical stock price.
📷 Image by Babak."	91	633	12	caesarmario	samsung-electronics-stock-historical-price
585	585	corona virus update - dd26		[]	"corona virus data obtained from:
2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE
https://github.com/CSSEGISandData/COVID-19
country population data obtained from:
https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)
Australia state population data obtained from:
https://www.abs.gov.au"	405	12186	1	darryldias	corona-virus-update-dd26
586	586	Women's_Shoes_Prices	Women's_Shoe_Prices in Datafiniti Dataset	['exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	"UPVOTE PLS !!!! 🐋
About This Data 🐕
This is a list of 10,000 women's shoes and their product information provided by Datafiniti's Product Database.
The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries.
Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.
What You Can Do with This Data 🦝
You can use this data to determine brand markups, pricing strategies, and trends for luxury shoes. E.g.:
What is the average price of each distinct brand listed?
Which brands have the highest prices?
Which ones have the widest distribution of prices?
Is there a typical price distribution (e.g., normal) across brands or within specific brands?
Further processing data would also let you:
Correlate specific product features with changes in price.
You can cross-reference this data with a sample of our Men's Shoe Prices to see if there are any differences between women's brands and men's brands.
Data Schema 🐆
A full schema for the data is available in here"	18	192	8	nguyenngocphung	womens-shoe-prices
587	587	World Bank's Education Indicators By Country	Over 3,000 internationally comparable educational indicators	['business', 'education', 'banking']	"About this dataset
&gt; <p>The World Bank EdStats All Indicator Query holds around 3,000 internationally comparable indicators that describe education access, progression, completion, literacy, teachers, population, and expenditures. The indicators cover the education cycle from pre-primary to vocational and tertiary education.</p>
 This dataset was created by World Bank and contains around 1100000 samples along with Indicator Name, 2015, technical information and other features such as:
- 1990
- 2065
- and more.
How to use this dataset
&gt; - Analyze 2060 in relation to 2075
- Study the influence of 2035 on 1978
- More datasets
Acknowledgements
If you use this dataset in your research, please credit World Bank 
Start A New Notebook!"	12	152	1	yamqwe	education-statisticse
588	588	Netflix Stock Data - Live and Latest	Regular Updates from Yahoo! Finance Downloaded data	['finance', 'computer science', 'programming', 'data visualization', 'time series analysis', 'linear regression', 'investing']	"Source: Wikipedia
Netflix, Inc. is an American over-the-top media service and original programming production company. It offers subscription-based video on demand from a library of films and television series, 40% of which is Netflix original programming produced in-house. Netflix has also played a prominent role in independent film distribution. As of July 2021, Netflix had 209 million subscribers, including 72 million in the United States and Canada. 
Netflix was founded in 1997 by Reed Hastings and Marc Randolph in Scotts Valley, California. Netflix's initial business model included DVD sales and rental by mail.
The company is ranked 164th on the Fortune 50011 and 284th on the Forbes Global 2000. It is the largest entertainment/media company by market capitalization. In 2021, Netflix was ranked as the 8th most trusted brand globally by Morning Consult. During the 2010s decade, Netflix was the top-performing stock in the S&P 500 stock market index, with a total return of 3,693%.
Context
Netflix is a booming stock.  Netflix stock Analysis will be a great eye-treat.
Content
Downloaded using a python script, the source is gathered through Yahoo! Finance API"	605	6141	31	kalilurrahman	netflix-stock-data-live-and-latest
589	589	Schema2020		[]		2	697	1	solyoh21	schema2020
590	590	VADER Sentiment Analysis	VADER Dataset for sentiment analysis from GitHub	['earth and nature']		4	414	0	yunussalman	vader-sentiment-analysis
591	591	Autocorrelation		[]		0	2	0	sandeeptaksande	autocorrelation
592	592	Portfolio Analytics		['finance']		0	5	0	sandeeptaksande	portfolio-analytics
593	593	Portfolio optimization		[]		1	8	0	sandeeptaksande	portfolio-optimization
594	594	An Online Shop Business Transaction	An online shop sales transaction for one year	['business', 'tabular data', 'retail and shopping', 'e-commerce services']	"Context
E-commerce has become a new channel to support businesses development. Through e-commerce, businesses can get access and establish a wider market presence by providing cheaper and more efficient distribution channels for their products or services. E-commerce has also changed the way people shop and consume products and services. Many people are turning to their computers or smart devices to order goods, which can easily be delivered to their homes. 
Content
This is a sales transaction dataset of UK-based B2C e-commerce for one year. The organization sells gifts and homeware for adults and children primarily through a website since 2007. The customers are from all over the world and usually make a purchase directly for themselves. There are also small businesses that buy in bulk and sell to other customers through the retail outlet channel.
The dataset contains 500K rows and 9 columns. The following is the description of each column. 
1.  TransactionNo (nominal): a six-digit unique number that defines each transaction. The letter “C” in the code indicates a cancellation.
2.  Date (numeric): the date when each transaction was generated.
3.  ProductNo (nominal): a five or six-digit unique character used to identify a specific product.
4.  Product (nominal): product/item name.
5.  Price (numeric): the price of each product per unit in pound sterling (£).
6.  Quantity (numeric): the quantity of each product per transaction. Negative values related to cancelled transactions.
7.  Revenue: the amount of income generated by a transaction in pound sterling (£). Revenue is calculated from the price times quantity.
8.  CustomerNo (nominal): a five-digit unique number that defines each customer.
9.  Country (nominal): name of the country where the customer resides.
Inspiration
Information is a main asset of businesses nowadays. The success of a business in a competitive environment depends on its ability to acquire, store, and utilize information. Data is one of the main sources of information. Therefore, data analysis is an important activity for acquiring new and useful information. Analyze this dataset and try to answer the following questions:
1.  How was the sales trend over the months?
2.  What are the most frequently purchased products?
3.  How many products does the customer purchase in each transaction?
4.  What are the most profitable segment customers?
5.  Based on your findings, what strategy could you recommend to the business to gain more profit?
Photo by <a href=""https://unsplash.com/@cardmapr?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">CardMapr</a> on <a href=""https://unsplash.com/s/photos/online-shopping?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	144	735	11	gabrielramos87	an-online-shop-business
595	595	COVID19_datasets	COVID-19 datasets obtained from github.com/nytimes/covid-19-data/ and cdc sites	['united states', 'diseases', 'public health', 'beginner', 'intermediate', 'tabular data', 'public safety']	"Collected COVID-19 datasets from various sources as part of DAAN-888 course, Penn State, Spring 2022.
Collaborators: Mohamed Abdelgayed, Heather Beckwith, Mayank Sharma, Suradech Kongkiatpaiboon, and Alex Stroud
1 - COVID-19 Data in the United States 
Source:  The data is collected from multiple public health official sources by NY Times journalists and compiled in one single file. 
Description:  Daily count of new COVID-19 cases and deaths for each state. Data is updated daily and runs from 1/21/2020 to 2/4/2022. 
URL:  https://github.com/nytimes/covid-19-data/blob/master/us-states.csv 
Data size:  38,814 row and 5 columns.
2 - Mask-Wearing Survey Data 
Source:  The New York Times is releasing estimates of mask usage by county in the United States. 
Description:  This data comes from a large number of interviews conducted online by the global data and survey firm Dynata, at the request of The New York Times. The firm asked a question about mask usage to obtain 250,000 survey responses between July 2 and July 14, enough data to provide estimates more detailed than the state level. 
URL:  https://github.com/nytimes/covid-19-data/blob/master/mask-use/mask-use-by-county.csv 
Data size:  3,142 rows and 6 columns 
3a - Vaccine Data – Global 
Source: This data comes from the US Centers for Disease Control and Prevention (CDC), Our World in Data (OWiD) and the World Health Organization (WHO). 
Description:  Time series data of vaccine doses administered and the number of fully and partially vaccinated people by country.  This data was last updated on February 3, 2022 
URL: https://github.com/govex/COVID-19/blob/master/data_tables/vaccine_data/global_data/time_series_covid19_vaccine_global.csv
Data Size: 162,521 rows and 8 columns 
3b -Vaccine Data – United States 
Source: The data is comprised of individual State's public dashboards and data from the US Centers for Disease Control and Prevention (CDC). 
Description: Time series data of the total vaccine doses shipped and administered by manufacturer, the dose number (first or second) by state. This data was last updated on February 3, 2022. 
URL: https://github.com/govex/COVID-19/blob/master/data_tables/vaccine_data/us_data/time_series/vaccine_data_us_timeline.csv
Data Size: 141,503 rows and 13 columns 
4 - Testing Data 
Source: The data is comprised of individual State's public dashboards and data from the U.S. Department of Health & Human Services. 
Description:  Time series data of total tests administered by county and state. This data was last updated on January 25, 2022. 
URL: https://github.com/govex/COVID-19/blob/master/data_tables/testing_data/county_time_series_covid19_US.csv
Data size:  322,154 rows and 8 columns 
5 – US State and Territorial Public Mask Mandates 
Source:  Data from state and territory executive orders, administrative orders, resolutions, and proclamations is gathered from government websites and cataloged and coded by one coder using Microsoft Excel, with quality checking provided by one or more other coders. 
Description:  US State and Territorial Public Mask Mandates from April 10, 2020 through August 15, 2021 by County by Day 
URL:  https://data.cdc.gov/Policy-Surveillance/U-S-State-and-Territorial-Public-Mask-Mandates-Fro/62d6-pm5i 
Data Size:  1,593,869 rows and 10 columns 
6 – Case Counts & Transmission Level 
Source:  This open-source dataset contains seven data items that describe community transmission levels across all counties. This dataset provides the same numbers used to show transmission maps on the COVID Data Tracker and contains reported daily transmission levels at the county level. The dataset is updated every day to include the most current day's data. The calculating procedures below are used to adjust the transmission level to low, moderate, considerable, or high. 
Description: US State and County case counts and transmission level from 16-Aug-2021 to 03-Feb-2022 
URL:  https://data.cdc.gov/Public-Health-Surveillance/United-States-COVID-19-County-Level-of-Community-T/8396-v7yb 
Data Size: 550,702 rows and 7 columns 
7 - World Cases & Vaccination Counts 
Source: This is an open-source dataset collected and maintained by Our World in Data. OWID provides research and data to help against the world’s largest problems.
Description: This dataset includes vaccinations, tests & positivity, hospital & ICU, confirmed cases, confirmed deaths, reproduction rate, policy responses and other variables of interest. 
URL: https://github.com/owid/covid-19-data/tree/master/public/data 
Data Size: 67 columns and 157,000 rows 
8 - COVID-19 Data in the European Union 
Source: This is an open-source dataset collected and maintained by ECDC. It is an EU agency aimed at strengthening Europe's defenses against infectious diseases.
Description: This dataset contains information on COVID-19 vaccinations in the EU/EEA region. This dataset includes weekly vaccination counts (split into first, second and booster dose), vaccine name, country, population, and region. 
URL: https://www.ecdc.europa.eu/en/covid-19/data 
Data Size: 14 columns and 221,000 rows 
9 – State and County FIPS 
Source: A single GitHub user created this dataset to act as a tool for linking county identifiers to the county name.
Description:  The Federal Information Processing System (FIPS) code for each county in the United States with the county name, state name, region, and division.
URL: https://github.com/kjhealy/fips-codes/blob/master/county_fips_master.csv 
Data Size: 3,147 rows and 13 columns"	9	58	0	suradechk	covid19-datasets
596	596	ICD10 WHO data [mortality]	[https://www.who.int/data/data-collection-tools/who-mortality-database]	[]		3	41	0	caroto	morticd10
597	597	UNICEF_State_of_World_Children_2021	State of World Children in 2021- Data of UNICEF	['social science', 'exploratory data analysis', 'data cleaning', 'statistical analysis']	"DESCRIPTION OF EACH TABLE 🐱 🐭 🐹
TABLE 1. DEMOGRAPHICS 🦀
The demographics table contains selected indicators on some of the most important demographic information of each population, including the total population and broken down by age, as well as annual population growth rates. The annual number of births is a function of both population size and current fertility. The total fertility rate allows for comparison of fertility levels internationally. A total fertility level of 2.1 is called ‘replacement level’ and represents a level at which, in the long term, the population would remain the same size. Life expectancy at birth is a measure of the health status and the development of a population and continues to increase in almost all countries in the world. The dependency ratio is the ratio of the not-working-age population (i.e., the economically ‘dependent’ population) to the working-age population (15–64 years). This can be divided into a child dependency ratio (ratio of children under 15 to working age population) and an old-age dependency ratio (ratio of population 65 and older to working-age population). The total dependency ratio is usually U-shaped over time reflecting a changing age structure as a result of the demographic transition. This can be understood as the combination of opposing trends in child and old-age dependency ratios. For example, decreasing fertility leads to a decreasing share of children in the population and therefore to a decrease in the child dependency ratio. Increasing life expectancy (as consequence of decreasing mortality) will lead to a larger share of older people and therefore to an increase in the old-age dependency ratio. The proportion of the urban population and the annual urban population growth rate describe the status and dynamics of the urbanization process. The net migration rate refers to the difference between the number of immigrants and the number of emigrants; a country/ area with more immigrants than emigrants shows a positive value, while a country with less immigrants than emigrants shows a negative value. 
================================================
TABLE 2. CHILD MORTALITY 🦎
Each year, in The State of the World’s Children report, UNICEF presents a series of mortality estimates for children. These figures represent the best estimates available at the time of printing and are based on the work of the United Nations Inter-Agency Group for Child Mortality Estimation (UN IGME), which includes UNICEF, the World Health Organization (WHO), the World Bank group and the United Nations Population Division. UN IGME mortality estimates are updated annually through a detailed review of all newly available data, which can result in adjustments to previously reported estimates. As a result, consecutive editions of The State of the World’s Children should not be used for analysing mortality trends over time. Comparable global and regional under five mortality estimates for the period 1990–2019 are presented below. 
===============================================
TABLE 3. MATERNAL AND NEWBORN HEALTH 🐯
The maternal and newborn health table includes a combination of demographic and intervention coverage indicators. The demographic indicators consist of life expectancy for females, adolescent birth rate, and maternal mortality estimates including the number of maternal deaths, maternal mortality ratio, and lifetime risk of maternal death.
=============================================
TABLE 4. CHILD HEALTH  🐦
The child health table includes a set of indicators that capture information on the coverage of effective interventions delivered to children under the age of five years and at the household level. These include a range of immunization indicators (described below) and indicators on interventions for the prevention or treatment of pneumonia, diarrhoea and malaria (the three leading killers of young children). The main data sources for the indicators on prevention and treatment of childhood illnesses are nationally representative household surveys such as the DHS and MICS. Regional and global estimates are calculated by using a weighted average method. Variables used for weighting are indicator-specific and applied to each country. They accord with the appropriate target population for each indicator (the denominator) and are derived from the latest edition of the World Population Prospects. Only the most recent data points from 2015– 2020 for each country were used to calculate regional and global estimates. For indicators that capture information about households, total population was used.
===============================================
TABLE 5. ADOLESCENT HEALTH 🐋
This table contains a set of key indicators related to adolescent health, well-being and mortality. Mortality indicators include adolescent mortality rate for ages 10–19, the number for adolescent deaths as well as the annual rate of reduction in the adolescent mortality rate for the period 2000–2019. Reproductive health indicators presented in this table include adolescent birth rate, early childbearing (which refers to women aged 20–24 years who gave birth before age 18) and demand for family planning satisfied with modern methods among adolescents aged 15–19. The following maternal health indicators are presented for adolescents aged 15–19: Antenatal care with at least four visits and skilled birth attendant. The following risk factors for noncommunicable diseases (NCDs) are presented: Alcohol use among adolescents ages 15–19, tobacco use among adolescents ages 13–15 and insufficient physical activity among school going adolescents ages 11–17. Vaccination against human papillomavirus (HPV) is presented for girls who received the last dose of the HPV vaccine per national schedule. WHO/UNICEF produce two main coverage indicators for HPV vaccination. One is the HPV vaccination programme performance coverage that describes vaccination coverage according to a national schedule and the programme’s eligibility criteria for each calendar year (programme’s target population up to 14 years of age). The second describes HPV vaccination coverage by age 15, representing the proportion of the population turning 15 in the reporting year who have been vaccinated against HPV at any time between the ages of 9–14, at any time up to the calendar year in question.
==============================================
TABLES 6,7: HIV/AIDS 🦂
In 2021, the Joint United Nations Programme on HIV/AIDS (UNAIDS) released new global, regional and country-level HIV and AIDS estimates for 2020 that reflect the most up-to-date epidemiological estimates. The estimates also reflect coverage data for antiretroviral therapy (ART), prevention of mother-to-child transmission (PMTCT) and early infant diagnosis for HIV. The estimates are based on the most current available science and WHO programme guidelines. These guidelines have resulted in improvements in assumptions of the probability of HIV transmission from mother-to-child, fertility among women by age and HIV serostatus, net survival rates for children living with HIV and more. Based on this refined methodology, UNAIDS has retrospectively generated new estimates of HIV prevalence, the number of people living with HIV and those needing treatment, AIDS-related deaths, new HIV infections, and other important trends in the HIV epidemic. Key indicators on the HIV response for children are divided into two tables: Table 6. HIV/AIDS: epidemiology and Table 7. HIV/AIDS: interventions.
================================================
TABLES 8, 9: NUTRITION 🐴
Table 8 encompasses estimates of malnutrition at birth among pre-school-aged children, school-aged children and women of reproductive age as well as coverage of birth weighing and key micronutrient programmes. Table 9 encompasses feeding practices for infants and young children. Estimates for low birthweight, stunting and overweight among pre-school children, thinness and overweight among school-aged children, and maternal underweight and anaemia are from country models. For this reason, these may be different from survey-reported estimates. For all other indicators, when raw data were available, the country-level estimates were re-analysed to conform to standard analysis methods and may therefore differ from survey-reported values.
=========================================================
TABLE 11: EDUCATION 🐙
This table contains a set of indicators on the aspects of children’s education: equitable access, school completion and learning outcomes. This table first provides information about equitable access, as measured by the out-of-school children rate (SDG4.1.4). Estimates shown in this table were calculated using the UNESCO Institute for Statistics (UIS) database. The out-of-school children rate identifies the population part in the official age range for a given level of education not attending school, in order to formulate targeted policies that can be put in place to ensure equitable access to education.
===============================================
TABLE 12. CHILD PROTECTION 🐞
Child protection refers to the prevention of and response to violence, exploitation and abuse of children in all contexts. There are many violations that children can be subjected to, but the lack of comparable data limits reporting on the full spectrum. In view of this, the child protection table presents data on a few issues for which comparable and nationally representative data are available. This includes two manifestations of harmful traditional practices, some forms of violence and exploitation as well as the official recording of births.
===============================================
TABLE 14. WASH 💐
This table contains a set of indicators on access to basic drinking water, sanitation and hygiene (WASH) services in households, schools and healthcare facilities. The WASH estimates in this report come from the WHO/ UNICEF Joint Monitoring Programme for Water Supply, Sanitation and Hygiene (JMP). Full details of the JMP indicator definitions, data sources and methods used to produce sub-national, national, regional and global estimates can be found at"	14	94	6	nguyenngocphung	unicef-state-of-world-children-2021
598	598	IFND dataset		['earth and nature']	"We will never ask for money to share the datasets. If someone claims that s/he has all the raw data and wants a payment, please be careful. 
About Data
This IFND dataset covers news pertaining to India only. This dataset is created by scraping Indian fact checking websites. The dataset contains two types of news fake and real News. This dataset was collected from real-world sources.TThe truthful news and fake news were collected from different reliable fact-checking websites. The dataset contains different types of articles on different topics, however, the majority of news focus on political news.
The above CSV files is comma-separated file and have the following columns:
id- Unique identifider  for each news
Statement- Title of the news article
Image- Image Url
Category-Topic of news
Date- Date of news
Label- 1 indicate True news and 0 indicate fake news
The image folder is divided into True and fake images. Images can be accessed using this link- https://drive.google.com/drive/u/3/folders/1K4NacXO8_9ZLxWBpyoKL0fLXr0uRsa2W
Note that there are absolutely no guarantees with this data, and we provide this dataset ""as is"", but you are welcome to report the issues of the preliminary version of this data.
You are allowed to use this dataset for research purposes only.
For more questions about the dataset, please contact: Sonal Garg, sonugarg174@gmail.com
References
If you use this dataset, as an agreement, please cite the following papers:
Sharma, D. K., & Garg, S. (2021). IFND: a benchmark dataset for fake news detection. Complex & Intelligent Systems, 1-21.
Garg, S., & Sharma, D. K. (2020, December). New Politifact: A Dataset for Counterfeit News. In 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART) (pp. 17-22). IEEE.
Garg, S., & Sharma, D. K. (2020, December). Phony News Detection using Machine Learning and Deep-Learning Techniques. In 2020 9th International Conference System Modeling and Advancement in Research Trends (SMART) (pp. 27-32). IEEE.
Sharma, D. K., & Garg, S. (2021, July). Machine Learning Methods to identify Hindi Fake News within social-media. In 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT) (pp. 1-6). IEEE.
Sharma, Sunidhi, and Dilip Kumar Sharma. ""Fake News Detection: A long way to go."" 2019 4th International Conference on Information Systems and Computer Networks (ISCON). IEEE, 2019.
Sharma, D.K., and Garg, S.”, A Framework for automatic fake content identification”, In IEEE Sponsored 1st International Conference” Simulation, Automation& Smart Manufacturing”, SASM 2021, held at GLA University, India."	0	3	0	sonalgarg174	ifnd-dataset
599	599	COVID 19 Tracking In USA	The COVID Tracking Project By The Atlantic	['united states', 'business', 'time series analysis', 'covid19']	This is Collected from The COVID Tracking Project which is a volunteer organization launched from The Atlantic and dedicated to collecting and publishing the data required to understand the COVID-19 outbreak in the United States.	113	1479	10	punyaslokaprusty	covid-19-tracking-in-usa
600	600	Brazilian Soccer Database	Matches of the main competitions in which Brazilian soccer teams participate	['football', 'brazil', 'south america', 'tabular data', 'r']	"Brazilian Soccer Data
This dataset consists of collecting the history and current data of all the most important competitions that Brazilian teams compete, the principal competitions are:
Brasileirão(Brazilian soccer league)
Libertatodes(Principal South america Competition)
Sudamericana(South American secondary competition)
Copa do Brasil(Brazilian Cup)
Next Steps:
- structure the collection of the games of the sudamericana
- Gather data from the main state championships(SP, RJ, MG, RS)
- Gather more data from these championships, such as match statistics
Any questions or suggestions are welcome, feel free to collaborate on the github repository"	199	1961	15	ricardomattos05	jogos-do-campeonato-brasileiro
601	601	ST-GCN Dataset		['video data']		0	9	0	jialia	stgcn-dataset
602	602	pneumonia weights maskrcnn		[]		1	8	0	tanushrikumar	pneumonia-weights-maskrcnn
603	603	Installed wind power capacity (MW)	Wind power by country from 2014-2020	['renewable energy', 'tabular data']	"Context
Installed wind power capacity (MW) in each country from 2014-2020.
Acknowledgements
Data source: https://en.wikipedia.org/wiki/Wind_power_by_country
Inspiration
Forecast installed wind power capacity."	19	134	8	prasertk	installed-wind-power-capacity-mw
604	604	Dataset for Traffic Sign Recognition		[]		1	12	0	jiawei666	dataset-for-traffic-sign-recognition
605	605	bhuvana1		[]		0	4	0	talaribhuvana	bhuvana1
606	606	Book_text		[]		0	3	0	mdkowsaralamshuvo	book-text
607	607	Fruits and Vegetables Image Recognition Dataset	Fruit and Vegetable Images for Object Recognition	['computer vision', 'image data', 'multiclass classification', 'cooking and recipes', 'food']	"Context
This dataset contains images of the following food items:
* fruits- banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.
* vegetables- cucumber, carrot,  capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.
Content
This dataset contains three folders:
* train (100 images each)
* test (10 images each)
* validation (10 images each)
each of the above folders contains subfolders for different fruits and vegetables wherein the images for respective food items are present
Data Collection
The images in this dataset were scraped by me from Bing Image Search for a project of mine.
Inspiration
The idea was to build an application which recognizes the food item(s) from the captured photo and gives its user different recipes that can be made using the food item(s).
Citation
Kritik Seth, ""Fruits and Vegetables Image Recognition Dataset,"" Kaggle 2020 [https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition]"	4347	43968	92	kritikseth	fruit-and-vegetable-image-recognition
608	608	Updated 376 Cryptocurrencies Historical Price	Updated 376 cryptocurrencies historical price	['investing', 'currencies and foreign exchange']		159	1719	13	benjaminpo	crypto-historical-price
609	609	PlantStat	simple statistical and ML (AutoML, CV) package	['computer science', 'programming', 'computer vision', 'statistical analysis', 'scipy', 'pytorch', 'automl']	"PlantStat
PlantStat on GitHub
A package with a set of functions for fast and convenient statistical processing of experimental data. Also includes simple AutoML algorithms for classification and regression. Designed for needs of the LPBPS (Laboratory of Physiology and Biochemistry of Plant Stress; Kharkiv, Ukraine).
The package is under development, and therefore its functionality is still very limited. Bugs and errors are also possible since the package functions were tested on a small amount of real data. The number of functions will be expanded, and possible problems will be fixed during the next updates.
This package was written by a non-professional developer and was originally intended for the needs of a narrow circle of specialists in the field of plant sciences (mainly plant physiology and biochemistry).
Many approaches and methods used in it are still far from ideal. The author publishes the package on GitHub and Kaggle to share with the community and improve with feedbacks.
Installing from the source:
pip install git+https://github.com/Nordant/plantstat.git#egg=plantstat
AutoML algorithms:
| AutoML_Classifier | AutoML_Regressor | Clusterer | KNN |
| --- | --- | --- | --- |
| LogisticRegression | LinearRegression | KMeans | NearestNeighbors |
| LinearSVC | Ridge | DBSCAN | |
| KNN | Lasso | | |
| DecisionTree | ElasticNet | | |
| RandomForest | RandomForest | | |
| GradientBoosting | GradientBoosting | | |
| XGB | XGB | | |
|  | SVR | | |
Data generators:
| Generator |
| --- |
| ClusterData |
| RegressionData |
python
from plantstat.data_generators import ClusterData
data_gen = ClusterData(n_features = 5, n_samples = 1000, cluster_std = 1.2,
                       centers_range = (4, 5), random_state = 0, return_labels = False)
X = pd.DataFrame(data_gen.generate(save = True))
python
from plantstat.data_generators import RegressionData
data_gen = RegressionData(n_features = 5, n_samples = 1000, n_informative = 3, n_targets = 1,
                          bias = 0.0, noise = 0.2, shuffle = True, random_state = 0, return_labels = True)
X, y = data_gen.generate(save = True)
Examples:
Variable_Analyzer - the main class for statistical data processing.
```python
from plantstat import Variable_Analyzer
Define Analyzer
a = Variable_Analyzer(data, labels)
An example of step-by-step analysis with saving in a local directory
EDA
a.boxplot(save = True)
a.outliers()
a.corrs(method = 'pearson', heatmap = True, save = True)
a.QQplot(save = True)
a.pair_plot(save = True)
a.basic_stats(save = True)
Statistical tests
a.var_compare(save = True)
```
AutoML_Classifier - the main class for classification.
```python
from plantstat import AutoML_Classifier
Define AutoML_Classifier
model = AutoML_Classifier(n_iter = 100)
model.fit(X_train, y_train)
Model detailed information
model.cv_results_
model.best_estimator_
model.best_pipeline
Prediction and classification report (with prediction saving)
model.predict(X_test, save = True)
model.predict_proba(X_test, save = True, f_format = 'csv')
model.classification_report(X_test, y_test, labels = class_names, cmap = 'cividis', save = True)
AutoML model without some algorithms
model = AutoML_Classifier(n_iter = 100, XGB = False, GradientBoosting = False)
model.fit(X_train, y_train)
model.classification_report(X_test, y_test, labels = class_names)
```
AutoML_Regressor - the main class for regression.
```python
from plantstat import AutoML_Regressor
Define AutoML_Classifier
model = AutoML_Regressor(n_iter = 100)
model.fit(X_train, y_train)
Model detailed information
model.cv_results_
model.best_estimator_
model.best_pipeline
Predcition and report (with prediction saving)
model.predict(X_test, save = True)
model.prediction_report(X_test, y_test, save = True)
AutoML model without some algorithms
model = AutoML_Regressor(n_iter = 100, XGB = False, GradientBoosting = False)
model.fit(X_train, y_train)
model.prediction_report(X_test, y_test)
```
Clusterer - the main class for clustering.
```python
from plantstat import Clusterer
from plantstat.data_generators import ClusterData
Create syntetic data with 5 features, 1000 samples and 4 clusters
data_gen = ClusterData(n_features = 5, n_samples = 1000, cluster_std = 1.2,
                       centers_range = (4, 5), random_state = 0)
X = pd.DataFrame(data_gen.generate())
Create K-means model for clustering (the model includes PCA with 2 components)
kmeans = Clusterer(is_pca = True, clusterer = 'kmeans')
kmeans.fit(X, save = True)
Score values for various number of clusters
kmeans.scores
Preprocessed data (after scaling and PCA)
kmeans.X
Prediction (basis on fit results)
preds = kmeans.predict(k = 4, save = True)
The real number of labels
print('Unique labels: %i' %len(np.unique(data_gen.labels)))
Create DBSCAN model for clustering (the model includes PCA with 2 components)
dbscan = Clusterer(is_pca = True, clusterer = 'DBSCAN')
dbscan.fit(X, save = True)
Score values for various eps values
dbscan.scores
Preprocessed data (after scaling and PCA)
dbscan.X
Prediction (basis on fit results)
preds = dbscan.predict(eps = 0.36, save = True)
```
KNN - the main class for Nearest Neighbors similarity finding.
```python
import pandas as pd
from sklearn.datasets import load_iris
iris = pd.DataFrame(load_iris().data)
from plantstat import KNN
Create and fit KNN with 5 neighbors
nn = KNN(5)
nn.fit(iris, save = True)
Find neighbors for data subset
nn.find_neighbors(iris.iloc[:10, :], save = True)
all kinds of data in the class
nn.self_distances_
nn.self_indices_
nn.model_
nn.all_data_
nn.distances
nn.indices
OpenStomataPredictor - the main class for stomata open/close classes prediction.python
from plantstat.vision.stomata_vision import OpenStomataPredictor
predictor = OpenStomataPredictor('PATH', batch_size = 16)
predictor.predict(save = True)
predictor.visualize(save = True)
predictor.report_
predictor.test_img_paths_
predictor.test_preds_
predictor.test_classes_
```"	110	2336	12	maksymshkliarevskyi	plantstat-package-statistics-and-automl
610	610	California Housing prices dataset		['north america', 'computer science', 'beginner', 'linear regression', 'tabular data', 'pandas']	"Context
The data set is for creating basic regression models.
Content
It a data set I got in the course of learning python and ML
Acknowledgements
The source for me is a udemy course I am following:
https://www.udemy.com/course/machine-learning-data-science-python/"	1	10	0	narayanaswamych	california-housing-prices-dataset
611	611	drop_nn_ub		[]		0	45	0	derekaustin	drop-nn-ub
612	612	dataset_two		[]		0	0	0	mrhagchwh2	dataset-two
613	613	beedatasets		[]		0	2	0	mtlsuda	beedatasets
614	614	image_forgery_FAU	dastkari_tasvir_data	[]		0	522	0	alerium	image-forgery-fau
615	615	netflix_titles.csv		[]		0	1	0	sridharan1819	netflix-titlescsv
616	616	pwtest		[]		1	4	0	lezlie	pwtest
617	617	util.py		['computer science']		0	0	0	karna123	utilpy
618	618	adawdawd		[]		0	4	0	zhouyiheng	adawdawd
619	619	OECD Data - Crude Oil Production	Crude Oil Production data by OECD.org	['energy', 'beginner', 'text data', 'oil and gas']	"Our World in Data - COVID-19
▶ About OECD Data 🏢
OECD Data website
▶ Context 📝
Crude oil production is defined as the quantities of oil extracted from the ground after the removal of inert matter or impurities. It includes crude oil, natural gas liquids (NGLs) and additives. This indicator is measured in thousand tonne of oil equivalent (toe). Crude oil is a mineral oil consisting of a mixture of hydrocarbons of natural origin, yellow to black in colour, and of variable density and viscosity. NGLs are the liquid or liquefied hydrocarbons produced in the manufacture, purification and stabilisation of natural gas. Additives are non-hydrocarbon substances added to or blended with a product to modify its properties, for example, to improve its combustion characteristics (e.g. MTBE and tetraethyl lead). Refinery production refers to the output of secondary oil products from an oil refinery.
▶ Acknowledgements 🙏
I'd like to clarify that I'm only making data about crude oil production provided by OECD available to Kaggle community. 
This dataset is downloaded from OECD website here.
▶ Inspiration 💭
Forecasting oil production in specific country.
Implementing machine learning models.
Perform data analysis/data visualization of oil production in different countries.
📷 Image by Robin Sommer."	43	462	19	caesarmario	oecd-data-crude-oil-production
620	620	quark_data_set		[]		0	0	0	tharunkunduru	quark-data-set
621	621	testdata	vr ar startups from all over the world in one place	['science and technology']		1	457	0	juliarekamie	testdata
622	622	assignment 2		[]		0	0	0	chance3924	assignment-2
623	623	assignment 2		[]		0	0	0	chance3924	assignment-2
624	624	commonlit-train		[]		0	2	0	yujiariyasu	commonlit-train
625	625	Water Company Brazil	Electrical Dataset of a water company	['energy']		0	7	5	oakthyago	water-company-brazil
626	626	Corona Virus Complete Dataset	Auto Updated Dataset from WHO via https://ourworldindata.org/	['diseases', 'health']	"Context
This data is sourced from the World Health Organization (WHO) Situation Reports . The WHO Situation Reports are published daily . The main section of the Situations Reports are long tables of the latest number of confirmed cases and confirmed deaths by country.
Content
Data files present the total confirmed cases, total deaths and daily new cases and deaths by country."	224	3792	24	medyasun	corona-virus-complete-dataset
627	627	GBR Starfish TFRecords Mini 2X 1 2		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-2x-1-2
628	628	GBR Starfish TFRecords Mini 2X 1 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-2x-1-0
629	629	GBR Starfish TFRecords Mini 2X 2 2		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-2x-2-2
630	630	GBR Starfish TFRecords Mini 2X 1 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-2x-1-1
631	631	GBR Starfish TFRecords Mini 2X 0 1		[]		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-2x-0-1
632	632	GBR Starfish TFRecords Mini 2X 0 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-2x-0-0
633	633	GBR Starfish TFRecords Mini 2 2		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-2-2
634	634	GBR Starfish TFRecords Mini 2 1		[]		0	14	0	mmelahi	gbr-starfish-tfrecords-mini-2-1
635	635	GBR Starfish TFRecords Mini 2 0		[]		0	10	0	mmelahi	gbr-starfish-tfrecords-mini-2-0
636	636	GBR Starfish TFRecords Mini 1 0		[]		0	31	0	mmelahi	gbr-starfish-tfrecords-mini-1-0
637	637	GBR Starfish TFRecords Mini 1 1		[]		0	36	0	mmelahi	gbr-starfish-tfrecords-mini-1-1
638	638	GBR Starfish TFRecords Mini 1 2		[]		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-1-2
639	639	GBR Starfish TFRecords Mini 0 2		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-0-2
640	640	GBR Starfish TFRecords Mini 0 1		[]		0	12	0	mmelahi	gbr-starfish-tfrecords-mini-0-1
641	641	GBR Starfish TFRecords Mini 0 0		[]		0	26	0	mmelahi	gbr-starfish-tfrecords-mini-0-0
642	642	Youtube Weekly Trending Videos and Stats	Weekly updated list of the top trending videos on youtube	[]	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	10	170	2	robikscube	youtube-weekly-trending-videos-and-stats
643	643	Partial Bank Loan Dataset	Singapore private bank latest updated version	['banking']	"Latest updated version of one of Singapore private bank loan partial dataset that customers applied for. Previous dataset was deleted here as I lost access to my account.
Bank XY (made-up name as it is kept as a confidential information) faced an enormous surge of bank loan application in 2020. XY wants to identify applicants that are eligible for the loan amount requested so that those who met the requirements will be granted loan and reject those who failed. Data was recorded based on informations provided by the customer through filling in loan application form during the year 2020.
Dataset details are described below.
Loan_ID -&gt; Unique loan ID
Gender -&gt; Male/Female
Married -&gt; Marriage status (TRUE/FALSE)
Dependent_No -&gt; Number of dependent/s
Education -&gt; Education status (Graduate/Not Graduate)
Self_Employed -&gt; Employment status (TRUE/FALSE) 
Applicant_Income -&gt; Amount of applicant's income
CoApplicant_Income -&gt; Amount of coapplicant's income
Loan_Amount -&gt; Amount of loan requested
Loan_Amount_Term -&gt; Term of loan in months
Credit_History -&gt; Applicant's credit history (0/1
Property_District -&gt; Applicant's district of property owned
Loan_Status -&gt; Loan approval status (TRUE/FALSE)"	3	12	0	vikramkumar001	partial-bank-loan
644	644	covid_new		[]		0	7	1	xduzichentang	covid-new
645	645	COVID-19 in Belgium (Auto-update)	Evolution of the coronavirus pandemic in Belgium	['healthcare', 'public health', 'automobiles and vehicles', 'covid19']	"Context
Coronavirus pandemic has caused social and economic disruption in many countries, including Belgium.
Content
The datasets collect information about the evolution of the disease in Belgium as of March 31, including the total number of cases per age group and municipality, number of tests performed daily, the number of hospitalizations and deaths.
This information can support decision making in the control of the epidemic. 
This dataset is auto-updated on a weekly basis.
Acknowledgements
The data is published by Sciensano, the Belgian institute for health, which is responsible for the epidemiological follow-up of the COVID-19 epidemic in collaboration with its partners and other healthcare actors."	42	1922	3	evgeniako	covid19-in-belgium
646	646	gans-pytorch		[]		5	3287	0	dremovd	ganspytorch
647	647	FBI NICS Firearm Background Checks	Data from National Instant Criminal Background Check System (NICS)	['crime', 'computer science', 'public safety']	"Context
FBI NICS Firearm Background Check Data
This dataset is from the FBI's National Instant Criminal Background Check System.
&gt; Mandated by the Brady Handgun Violence Prevention Act of 1993 and launched by the FBI on November 30, 1998, NICS is used by Federal Firearms Licensees (FFLs) to instantly determine whether a prospective buyer is eligible to buy firearms or explosives. Before ringing up the sale, cashiers call in a check to the FBI or to other designated agencies to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. More than 100 million such checks have been made in the last decade, leading to more than 700,000 denials.
Content
The dataset contains the number of FBI NICS firearm background checks by month, state, and type between November 1998 and now. The FBI provides it in PDF at https://www.fbi.gov/file-repository/nics_firearm_checks_-_month_year_by_state_type.pdf/view. Jeremy Singer-Vine at BuzzFeed News has developed a parser to convert the PDF to CSV and made it available at https://github.com/BuzzFeedNews/nics-firearm-background-checks.
License
The MIT License (MIT)
Copyright (c) 2015, Jeremy Singer-Vine, BuzzFeed News"	68	664	2	masakii	fbi-nics-firearm-background-checks
648	648	GP_DATA_TRANSPOSED		[]		0	9	0	manarmoh	gp-data-transposed
649	649	NBME data		['standardized testing']		0	6	0	yunhonghe	nbme-dataset
650	650	Sample Time Series Data		['business']		0	7	3	phanttan	sample-time-series-data
651	651	COVID-19 WORLDWIDE DATASET	Daily COVID-19 stats for every country in the world, includes USA, Canada, China	['news', 'covid19']	"Context
Each workbook contains daily COVID-19 stats by each country affected. Additional sheets have also been added for more specific breakdown by different locations within Australia, Canada, China, and USA. Worked with BNO News to put this together. Additional credits include: Michael Van Poppel and Carlos Robles. Github updated every 24 hrs can be found here: https://github.com/jamesvalles/CORONAVIUS-COVID-19-DAILYSTATS"	139	3316	10	jamesvalles	covid19-worldwide-dataset
652	652	COVID-19 cases in Brazil at city level 	10.1590/SciELOPreprints.362	[]	"Citation
Full description of the data at https://doi.org/10.1590/SciELOPreprints.362
Full dataset daily updated
The most updated dataset is available at https://github.com/wcota/covid19br and https://covid19br.wcota.me/
Description
Confirmed cases and deaths by day, using official information given by Ministério da Saúde, data at the municipal level by Brasil.IO and the most recent reported cases by @CoronavirusBra1.
The data contains the IBGE identifier, GPS coordinates of the cities and temporal evolution of the number of cases and deaths."	109	3028	3	wlcota	covid19-cases-in-brazil-at-city-level
653	653	NHS PROMs case study	NHS Patient-Reported Outcomes Measures (PROMs) data of hip & knee replacements.	['education', 'health', 'computer science']	"JADS NHS PROMs
A data science case study of the NHS Digital PROMs data of hip and knee replacements.
This case study was developed as at Jheronimus Academy of Data Science (JADS) Professional Education for educational purposes. 
Background: NHS Digital PROMs portal and the guide to the PROMs methodology
Intended audience: Master level and professional education
Learning objectives:
Python: become sufficiently fluent to perform common machine learning related tasks including data understanding, data preparation, modelling and evaluation.
Machine learning: gain hands-on experience with a set of most commonly used machine learning tasks including regression, classification
By the end of this course, you should be able to understand and reproduce the analysis in this paper by Huber et al. (2019) where PROMs following hip and knee replacement surgery are predicted using supervised learning.
Dataset
Original data available in separate zip files in data/external.
Prepared dataset available in .parquet format in data/interim.
Project structure
├── LICENSE
├── README.md             &amp;lt;- The top-level README for this project.
├── data
│       ├── external          &amp;lt;- Data from third party sources.
│       └── interim           &amp;lt;- Intermediate data that has been transformed.
│
├── environment.yml       &amp;lt;- conda environment file for reproducing the analysis environment
│
├── index.ipynb           &amp;lt;- Main notebook with links to separate lectures.
│
├── notebooks             &amp;lt;- Jupyter notebooks per lecture. Naming convention is a number (for ordering),
│                            the creator's initials, and a short - delimited description, e.g.
│                            1.0-initial-data-exploration.
│
├── references            &amp;lt;- Data dictionaries, manuals, and all other explanatory materials.
│
├── requirements.txt      &amp;lt;- The requirements file for reproducing the analysis environment, e.g.
│                            generated with pip freeze &amp;amp;gt; requirements.txt
│
├── treebeard.yaml        &amp;lt;- Configuration for treebeard envinroment.
│
└── treebeard-setup.md    &amp;lt;- Instructions for setup Treebeard envirnoment.
License
JADS NHS PROMs Data Science Case Study by Daniel Kapitan is licensed under under a
Creative Commons Attribution-ShareAlike 4.0 International License."	60	1595	5	dkapitan	nhs-proms-case-study
654	654	Playlist 8		[]		0	0	0	amauriramirez	playlist-8
655	655	in the love of Egypt		[]		0	2	0	omdas1	loveegypt
656	656	tradedata		[]		0	7	0	massimilianohasan	tradedata
657	657	Trinidad and Tobago NLCB Cash Pot Lottery draws	NLCB Cash Pot Lottery draws  from 1999 - 2022	[]		0	4	1	cardoza	trinidad-and-tobago-nlcb-cash-pot-lottery-draws
658	658	Cute80-Dataset	A lightweight dataset for text detection in natural scene images	['earth and nature']		66	2756	7	mohtashimnawaz	cute80dataset
659	659	OXFORD COVID-19 GOVERNMENT RESPONSE TRACKER	Systematic information on which governments have taken which measures, and when	['government', 'covid19']	"Context
OXFORD COVID-19 GOVERNMENT RESPONSE TRACKER
Data from https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker
Content
Governments are taking a wide range of measures in response to the COVID-19 outbreak. The Oxford COVID-19 Government Response Tracker (OxCGRT) aims to record these unfolding responses in a rigorous, consistent way across countries and across time.
OxCGRT collects publicly available information on 11 indicators of government response, such as school closings, travel bans, or other measures. For a full description of the data and how they are collected, see this working paper.
For more information see https://www.bsg.ox.ac.uk/research/publications/variation-government-responses-covid-19 and https://www.bsg.ox.ac.uk/research/publications/variation-government-responses-covid-19.
There are currently &gt; 9,000 entries.
Acknowledgements
Data from https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker
Banner Photo by Trust ""Tru"" Katsande on Unsplash"	542	15529	36	paultimothymooney	oxford-covid19-government-response-tracker
660	660	COVID-19 DATA by AGID	Official COVID-19 dataset published by Italian Cabinet	['health']	CSV updated with data of the last day for Italy.	12	1619	0	alfredodimaria	covid19-data-by-agid
661	661	Ask-NSE Index Data Distribution	NSE Index Data - Nifty50 and NiftyBank	['business', 'finance', 'investing']	"Context
NSE Index Data With Distribution' of Closing price. 
Content
To make decisions based on the distribution. 
Acknowledgements
Thanks To my family for the wonder-full support during my work. Especially my son Eshan Karthik and Wife. 
Inspiration
Any valuable ask or input's are welcome."	24	766	4	arun7pulse	askindex
662	662	pytorch YOLOv4		[]		6	672	0	unfinity	pytorch-yolov4
663	663	Intraday market data	3 second interval price data	['investing']	"Context
There is a lot of buzz about institutional algorithmic trading on the stock market. There is also great interest in machine learning (here obviously). As an absolute beginner in ML with an interest in stock trading, I went looking for a market dataset that I could use. 
While there are datasets available with daily closing prices, I couldn't find any publicly available with more granular data, much less anything close to having a real time data stream.
I have a trading account with TD Ameritrade. Their main desktop trading platform is called ThinkorSwim (TOS) and it has awesome capabilities. One feature is a Real Time Data (RTD) interface where Excel can pull price data from the platform at about a 3 second update rate. Using Visual Studio, I set up an Excel 'tunnel' to pull in data and then save it to a SQL Server DB. This dataset is that capture.
A version of the core data acquisition method is located at https://github.com/brtnsmth/TOS-RTD-core .
While the TOS RTD interface is one way (you cannot send trade orders to the platform), I can imagine a system that could deliver trading advice based on this near real time data.
Content
The datasets are 3 second interval price data, by week, using the futures trading windows - basically 24-7, from 6 PM EST Sunday to 6 PM Friday.
The example group includes futures, stocks and ETFs. No attempt was made to limit specific captures to 'market open' time periods. 
There were also a few TOS refusal exceptions where data points may have been lost.
Acknowledgements
ThinkorSwim is an awesome trading platform. Excellent work.
Inspiration
Some basic data pattern analysis packages leading to trade recommendations for individual traders would be great. A multi-timescale momentum prediction tool?
Sharing
I have made this data freely available. I ask that others share HOW they are using this data to the depth and detail they are comfortable with."	530	9483	19	brtnsmth	intraday-market-data
664	664	Amazon.com,Inc.(AMZN) Stock Price from May 15 1997	Stock Price of Amazon.com from May 15 1997 to Feb 11th 2022	['business', 'finance', 'time series analysis', 'investing', 'datetime']	"Context
This dataset is stock price of the Amazon Inc till this day.
Content
Whole dataset has total seven columns. Data, Opening price, High price of the day, lowest price of the day, closing price when market close, adjusted price after market close and the volume."	82	1371	9	devtaz	amazoncom-inc-amzn
665	665	IN_AMs_CMV_vs_TGBb_8wks.csv		[]		0	1	0	mikalelhajjar	in-ams-cmv-vs-tgbb-8wkscsv
666	666	dadatest	https://github.com/yuanyichuangzhi/fund-data/blob/master/submission.csv	[]		10	2944	1	darwinwin	dadatest
667	667	Life in Sea	Photographs of Ocean and Sea Living Beings for Image Classification	['arts and entertainment', 'earth and nature', 'animals', 'image data', 'fish and aquaria']	"life-in-sea Image Classification dataset
Photographs of Life in sea as jpg images
sealife_annotated.csv file contains the following features:
1. file_name: str [unique values] - image file name
2. land_visible: bool [True,False] - True if land is visible, False otherwise
3. seabed_visible: bool [True,False] - True if seabed is visible, False otherwise
4. tentacles: str ['present', 'not present'] - present if tectacles are there, not present otherwise
5. legs: str ['present', 'not present'] - present if legs are there, not present otherwise
6. shell: str ['present', 'not present'] - present if shell is there, not present otherwise
Annotations are made manually.
This dataset can be used for multi-target image classification. 
Once targets are defined, all other features can be used as metadata to help improve the model.
Acknowledgements
I would like to acknowledge the Repo https://github.com/NandhiniPython/life-in-sea and Unsplash.com for enabling access to these image collection. I would like to extend my gratitude to the photographers who took these splendid photographs and published them licence-free on unsplash.com"	19	551	7	rajkumarl	life-in-sea
668	668	IOT Botnets Attack Detection Dataset	Anomaly Detection on Multiple Devices Data	['social networks']	"Context
The original data comes from the work of Meidan et al. 1. It was preprocessed in this setting for comparative analysis of anomaly detection. The following steps have been taken as preprocessing: (1) five devices have been selected: Danmini doorbell, Ecobee thermostat, Philips baby monitor, Provision security camera, Samsung webcam, (2) for each botnet, the malicious traffic of all five behaviour types have been merged, (3) for each device and botnet combination, malicious requests have been sampled to comprise 5% of the final dataset.
1 Meidan, Y., Bohadana, M., Mathov, Y., Mirsky, Y., Shabtai, A., Breitenbacher, D., & Elovici, Y. (2018). N-baiot—network-based detection of iot botnet attacks using deep autoencoders. IEEE Pervasive Computing, 17(3), 12-22.
Target variable - 'Target'"	81	1652	11	saurabhshahane	anomaly-detection-using-deep-learning
669	669	Pokemon Legends Arceus Pokedex	Base stats for only those Pokemon encountered in Pokemon Legends: Arceus	['arts and entertainment', 'video games', 'anime and manga']	"Hi, I'm Mike, thanks for working with my data!
This is a complete Pokedex containing base stats for only those Pokemon in the new Pokemon Legends: Arceus game. The Pokedex was scraped from RankBoost.com. My scraping function was unable to capture the images or alt text in the Type columns, so I manually copied the Type information for all those Pokemon new to Arceus, and captured the rest by performing a LEFT JOIN using the coalesce function on another Kaggle dataset (credited below). Afterward, there were still a few Pokemon with Type NULL. They were neither new to Arceus, nor captured in the credited dataset. I manually recorded their Types. 
I am creating a project focused on evaluating the strengths of each starter Pokemon, but I hope that others will use this data to draw their own conclusion about teams, types, and matchups. 
Acknowledgements
Data from: https://www.kaggle.com/abcsds/pokemon
AND
https://rankedboost.com/pokemon-legends-arceus/pokedex/"	4	24	1	h2omelondiet	pokemon-legends-arceus-pokedex
670	670	divvy_data_uncleaned		[]		0	0	0	jenniferchechowich	divvy-data-uncleaned
671	671	yolox-cots-models-v2		[]		0	4	0	sangayb	yolox-cots-models-v2
672	672	Chelsea-Test-20220211		[]		0	26	0	cmdereck	chelseatest20220211
673	673	StellaRosbags		[]		0	4	0	youssefad	stellarosbags
674	674	opl550		[]		0	2	0	roco1911	opl550
675	675	Minneapolis Police Use of Force		[]		0	5	0	paulreiners	minneapolis-police-use-of-force
676	676	Flight Delays	Data collected from the Bureau of Transportation Statistics	[]		4	16	0	danialh	flight-delays
677	677	Industries 	Food price index: December 2021 	['business', 'data cleaning', 'linear regression', 'food', 'investing']	"Industries
Food price index: December 2021
This data is for education and training in data science and analysis
About the columns ,
Series_reference
Period
Data value
STATUS
UNITS
Subject 
Group 
Series_title_1
About tasks:
Data processing and analysis with an explanation of the results."	0	8	3	qusaybtoush	industries
678	678	Health 2 	 Serious injury outcome indicators 2000 - 2020	['business', 'health', 'law', 'data analytics', 'logistic regression', 'svm']	"Health
Serious injury outcome indicators 2000 - 2020
This data is for education and training in data science and analysis
About the columns ,
Series_reference
Period
Type
Data_value
Lower_CI
Upper_CI
Units
Indicator
Cause
Validation
Population
Age
Severity
About tasks:
Data processing and analysis with an explanation of the results."	1	12	6	qusaybtoush	health-2
679	679	Health 	Injury statistics – work-related claims: 2018	['business', 'health', 'classification', 'logistic regression', 'linear regression', 'k-means']	"Health
Injury statistics – work-related claims: 2018
This data is for education and training in data science and analysis
About the columns ,
Year
Sex 
Age 
group 
(years) at date of injury
Geographic
 region
 where 
injury
 occurred
Employment status
Occupation
Injury/illness/disease group
Type of injury/illness/disease
Industry    Industry subgroup
Value
Measure
Status
About tasks:
Data processing and analysis with an explanation of the results."	1	17	4	qusaybtoush	health
680	680	COVID-19 in Ukraine: daily data	Daily data for number of confirmed cases	['health', 'covid19']	"Context
COVID-19 in Ukraine: daily data from April 1
Content
Total and daily data about new Coronavirus cases, deaths, active cases, and number of recovered people in Ukraine, per day, GMT+0
I found a way to automatically upload data through API, so I stopped updating this table from 2020-10-17.
From 06-12-2020 I added new tables:
* Number of PCR tests on COVID-19 in Ukraine with daily data
* Number of hospitalizations of patients with COVID-19 in Ukraine with daily data
I will also make new notebooks here that will use data from the same source.
From 2020-11-28 I am adding forecast data files (from previous versions of public notebooks of this dataset) to compare the forecasting performance of my optimal model based on the Prophet model.
Acknowledgements
Thanks to the portal of the National Security and Defense Council of Ukraine.
Thanks to the FB page of the Ministry of Health of Ukraine.
Thanks for the image to <a href=""https://pixabay.com/ru/users/iXimus-2352783/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5062659"">iXimus</a> from <a href=""https://pixabay.com/ru/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5062659"">Pixabay</a>
Inspiration
It is necessary to find patterns and learn to make a short-term and long-term forecast, identify the influencing factors for the studying scenarios of the development of events.
The experience gained will be of interest to other countries of the world as well."	236	3276	33	vbmokin	covid19-in-ukraine-daily-data
681	681	Government finance statistics	 ( general government)  Year ended June 2021	['business', 'government', 'time series analysis', 'matplotlib', 'pandas']	"Government finance statistics
( general government): Year ended June 2021
This data is for education and training in data science and analysis
About the columns ,
Series_reference
Period
Data_value
STATUS  UNITS
MAGNTUDE
Subject
Group
Series_title_1
Series_title_2
Series_title_3
Series_title_4
Series_title_5
About tasks:
Data processing and analysis with an explanation of the results."	0	5	5	qusaybtoush	government-finance-statistics
682	682	wine_data		[]		0	2	0	jingyiran	wine-data
683	683	Pistachio Dataset	Pistachio Dataset 16 Features and 28 Features 	['categorical data', 'agriculture', 'artificial intelligence', 'computer science', 'binary classification']	"Citation Request :
OZKAN I.A., KOKLU M. and SARACOGLU R. (2021). Classification of Pistachio Species Using Improved K-NN Classifier. Progress in Nutrition, Vol. 23, N. 2, pp. DOI:10.23751/pn.v23i2.9686
DATASET: https://www.muratkoklu.com/datasets/
Article Download (PDF): https://www.mattioli1885journals.com/index.php/progressinnutrition/article/view/9686/9178
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/
ABSTRACT: In order to keep the economic value of pistachio nuts which have an important place in the agricultural economy, the efficiency of post-harvest industrial processes is very important. To provide this efficiency, new methods and technologies are needed for the separation and classification of pistachios. Different pistachio species address different markets, which increases the need for the classification of pistachio species. In this study, it is aimed to develop a classification model different from traditional separation methods, based on image processing and artificial intelligence which are capable to provide the required classification. A computer vision system has been developed to distinguish two different species of pistachios with different characteristics that address different market types. 2148 sample image for these two kinds of pistachios were taken with a high-resolution camera. The image processing techniques, segmentation and feature extraction were applied on the obtained images of the pistachio samples. A pistachio dataset that has sixteen attributes was created. An advanced classifier based on k-NN method, which is a simple and successful classifier, and principal component analysis was designed on the obtained dataset. In this study; a multi-level system including feature extraction, dimension reduction and dimension weighting stages has been proposed. Experimental results showed that the proposed approach achieved a classification success of 94.18%. The presented high-performance classification model provides an important need for the separation of pistachio species and increases the economic value of species. In addition, the developed model is important in terms of its application to similar studies. 
Keywords: Classification, Image processing, k nearest neighbor classifier, Pistachio species"	16	246	10	mkoklu42	pistachio-dataset
684	684	wine_data		[]		0	2	0	jingyiran	wine-data
685	685	Pistachio Dataset	Pistachio Dataset 16 Features and 28 Features 	['categorical data', 'agriculture', 'artificial intelligence', 'computer science', 'binary classification']	"Citation Request :
OZKAN I.A., KOKLU M. and SARACOGLU R. (2021). Classification of Pistachio Species Using Improved K-NN Classifier. Progress in Nutrition, Vol. 23, N. 2, pp. DOI:10.23751/pn.v23i2.9686
DATASET: https://www.muratkoklu.com/datasets/
Article Download (PDF): https://www.mattioli1885journals.com/index.php/progressinnutrition/article/view/9686/9178
https://www.kaggle.com/mkoklu42
DATASET: https://www.muratkoklu.com/datasets/
ABSTRACT: In order to keep the economic value of pistachio nuts which have an important place in the agricultural economy, the efficiency of post-harvest industrial processes is very important. To provide this efficiency, new methods and technologies are needed for the separation and classification of pistachios. Different pistachio species address different markets, which increases the need for the classification of pistachio species. In this study, it is aimed to develop a classification model different from traditional separation methods, based on image processing and artificial intelligence which are capable to provide the required classification. A computer vision system has been developed to distinguish two different species of pistachios with different characteristics that address different market types. 2148 sample image for these two kinds of pistachios were taken with a high-resolution camera. The image processing techniques, segmentation and feature extraction were applied on the obtained images of the pistachio samples. A pistachio dataset that has sixteen attributes was created. An advanced classifier based on k-NN method, which is a simple and successful classifier, and principal component analysis was designed on the obtained dataset. In this study; a multi-level system including feature extraction, dimension reduction and dimension weighting stages has been proposed. Experimental results showed that the proposed approach achieved a classification success of 94.18%. The presented high-performance classification model provides an important need for the separation of pistachio species and increases the economic value of species. In addition, the developed model is important in terms of its application to similar studies. 
Keywords: Classification, Image processing, k nearest neighbor classifier, Pistachio species"	16	246	10	mkoklu42	pistachio-dataset
686	686	Government finance	local authority statistics sep -2021	['business', 'government', 'data visualization', 'data analytics', 'deep learning']	"Government finance
local authority statistics sep -2021
This data is for education and training in data science and analysis
About the columns ,
Series reference
Period
Reference period    Data value
STATUS
UNITS
MAGNTUDE
Subject
Group
Series title
About tasks:
Data processing and analysis with an explanation of the results."	0	7	4	qusaybtoush	government-finance
687	687	Body performance Data	multi class classification	['public health', 'health', 'exploratory data analysis', 'classification', 'multiclass classification', 'heart conditions']	"Context
This is data that confirmed the grade of performance with age and some exercise performance data.
Content
data shape : (13393, 12)
age : 20 ~64 
gender : F,M
height_cm : (If you want to convert to feet, divide by 30.48)
weight_kg 
body fat_%
diastolic : diastolic blood pressure (min)
systolic : systolic blood pressure (min)
gripForce
sit and bend forward_cm
sit-ups counts
broad jump_cm
class : A,B,C,D ( A: best) / stratified
Source
link (Korea Sports Promotion Foundation)
Some post-processing and filtering has done from the raw data."	3157	21173	99	kukuroo3	body-performance-data
688	688	TerrariaTrees	Terraria Trees for Image Recognition	['games', 'video games', 'earth and nature', 'image data']	"Context
This is my first dataset on Kaggle. I really like Terraria and decided that for my first Image Recognition task Terraria trees would be really good.
Content
There are total of 319 files splitted by train and validation folders, 291 and 29 images in each. Most of the trees are from Terraria 1.3 version. There are some Night screenshots, it's up to you to use them or not.
Inspiration
Can you recognize a tree?"	0	19	2	kirillvavilov	terrariatrees
689	689	sgptbeasym		[]		0	4	0	muennighoff	sgptbeasym
690	690	Student_marks.csv		[]		0	5	0	kishan911	student-markscsv
691	691	mm_detection_2.4		[]		0	6	0	dwchen	mm-detection-24
692	692	C-Tran		[]		0	427	0	makorromanuel	ctran
693	693	sp_beaches_update		[]	"Context
Por conta da pandemia do novo coronavírus, não há medições de nenhuma das praias nos seguintes intervalos de tempo:
23/03/2020 a 20/07/2020
15/03/2021 a 12/04/2021
Content
Acknowledgements
Referências:
CETESB - Resultados Microbiológicos
Beaches Water Quality in São Paulo, Brasil
Inspiration"	17	962	0	ivcr24	sp-beaches-update
694	694	SCI01 - TRAEFIK - traefik.log - SEMANAL	Log principal do traefik	[]		0	18	0	rafaelpbmota	traefik-traefiklog-sci01
695	695	Bottleneck Transformers Pytorch	Implementation of Bottleneck Transformer in Pytorch	['business', 'computer vision', 'deep learning']	"Implementation of Bottleneck Transformer, SotA visual recognition model with convolution + attention that outperforms EfficientNet and DeiT in terms of performance-computes trade-off, in Pytorch
sys.path.append('../input/bottleneck-transformers-pytorch')"	13	1384	9	debarshichanda	bottleneck-transformers-pytorch
696	696	License plates on vehicles	Image dataset of license plates in the wild	['law']	"Context
This is a dataset with images of (mostly Dutch) license plates of all kinds of vehicles. With this data you could create a machine learning model that detects and locates license plates."	389	3923	17	pcmill	license-plates-on-vehicles
697	697	COVID-19	GitHub Data from JHU	[]		8	443	1	ankitaguha	covid19
698	698	Scientific Corrosion Assessment 		[]		0	1	0	tigerrryin	scientific-corrosion-assessment
699	699	GP_DATA_BEFORE_PRE		[]		0	10	0	manarmoh	gp-data-before-pre
700	700	Linguistic Diversity Research Dataset (1976-2020)	The linguistic diversity research and publication dataset	['social science']	"Context
The linguistic diversity research and publication dataset, which was indexed by Scopus from 1976 to 2020.
The dataset contains data authors, authors ID Scopus, title, year, source title, volume, issue, article number in Scopus, DOI, link, affiliation, abstract, index keywords, references, Correspondence Address, editors, publisher, conference name, conference date, conference code, ISSN, language, document type, access type, and EID.
Acknowledgements
Firdaus, Mega (2021), “Linguistic Diversity Research Dataset (1976-2020)”, Mendeley Data, V1, doi: 10.17632/fx6whr5c23.1"	60	858	16	saurabhshahane	linguistic-diversity-research-dataset-19762020
701	701	Largest US Retailers	Contains Sales Per Store information and more	['business', 'economics', 'retail and shopping']	"About this dataset
&gt; Includes Sales Per  Store This dataset was created by Gary Hoover and contains around 100 samples along with Unnamed: 4, Unnamed: 5, technical information and other features such as:
- Unnamed: 6
- Unnamed: 2
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 7 in relation to Unnamed: 8
- Study the influence of Unnamed: 3 on Unnamed: 4
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	160	951	7	yamqwe	largest-us-retailers-2015e
702	702	"""The_Daily_show""_guest"		['arts and entertainment']	"Context
The raw data behind the story Every Guest Jon Stewart Ever Had On ‘The Daily Show’"	0	2	1	muhammad92	the-daily-show-guest
703	703	Police-Shootings_dataset	police-shooting-data_in_different_cities	[]		0	3	1	muhammad92	policeshootings-dataset
704	704	🏫 Elem and Secondary Child Care	Dataset from the National Center for Education Statistics Statistics	['united states', 'education', 'primary and secondary schools']	"About this dataset
&gt; <p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting and analyzing data related to education in the U.S. and other nations. NCES is located within the U.S. Department of Education and the Institute of Education Sciences. NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics on the condition of American education; conduct and publish reports; and review and report on education activities internationally.</p>
<ul>
<li>Table 202.10. Enrollment of 3-, 4-, and 5-year-old children in preprimary programs, by age of child, level of program, control of program, and attendance status: Selected years, 1970 through 2014</li>
<li>Table 202.20. Percentage of 3-, 4-, and 5-year-old children enrolled in preprimary programs, by level of program, attendance status, and selected child and family characteristics: 2014</li>
<li>Table 202.25.WEB-ONLY TABLE—Percentage of 3- and 4-year-old children enrolled in school, by race/ethnicity and state: 2014</li>
<li>Table 202.30. Number of children under 6 years old and not yet enrolled in kindergarten, percentage in center-based programs, average weekly hours in nonparental care, andpercentage in various types of primary care arrangements, by selected child and family characteristics: 2012</li>
<li>Table 202.35 Primary child care arrangements of 4- and 5-year-old children who are not yet enrolled in kindergarten, by race/ethnicity, poverty status, and mother's highest level of education: Selected years, 1995 through 2012</li>
<li>Table 202.40. Child care arrangements of -3 to 5-year-old children who are not yet in kindergarten, by age and race/ethnicity: Selected years, 1991 through 2012</li>
<li>Table 202.50. Percentage distribution of children at about 2 and 4 years of age, by type of child care arrangement and selected child and family characteristics: 2003-04 and 2005-06</li>
<li>Table 202.60. Percentage distribution of quality rating of child care arrangements of children at about 4 years of age, by type of arrangement and selected child and family characteristics: 2005-06</li>
<li>Table 202.65. Percentage distribution of first-time kindergartners, by primary type of child care arrangement during the year prior to kindergarten entry and selected child, family, and school characteristics: 2010-11</li>
<li>Table 202.70. Number and percentage distribution of -3 to 5-year-olds not enrolled in school and all children enrolled in prekindergarten through second grade, by grade level and selected maternal and householdcharacteristics: 2001, 2005, and 2012</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://nces.ed.gov/programs/digest/current_tables.asp"" target=""_blank"" rel=""nofollow"">https://nces.ed.gov/programs/digest/current_tables.asp</a></p>
This dataset was created by National Center for Education Statistics and contains around 100 samples along with Unnamed: 42, Unnamed: 7, technical information and other features such as:
- Unnamed: 40
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 17 in relation to Unnamed: 25
- Study the influence of Unnamed: 14 on Unnamed: 12
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Center for Education Statistics 
Start A New Notebook!"	21	215	5	yamqwe	elem-and-secondary-child-caree
705	705	flower258	258 types of flowers genus occur in Hong Kong	['asia', 'environment', 'plants', 'cnn', 'image data']	"Context
The dataset is planned to use in the project of predicting Hong Kong plants images.
https://github.com/r48n34/leafers"	0	21	1	r48n34	flower258
706	706	contest		[]		0	18	0	abhishekprajapat	contest
707	707	turtulebot3_imu_trajs_100min		[]		0	4	0	geekgineer	turtulebot3-imu-trajs-100min
708	708	Education_indication_for_Pakistan	Female's data in education sector of pakistan	['education']		0	6	1	muhammad92	education-indication-for-pakistan
709	709	HeightEarnings		[]		0	5	0	angeli07	heightearnings
710	710	correctData-for-2021		[]		0	10	0	zarayi	correctdatafor2021
711	711	Body Wash Product Reviews for 100 SKUs on Amazon	Customer reviews for 100 unique Body Wash products available for sale on Amazon	['text data', 'retail and shopping', 'ratings and reviews', 'e-commerce services']	"Dataset contains customer reviews for 100 unique body wash products on Amazon USA.
The dataset contains the following:
1. A single product list file containing the list of all products covered.
2. A file for each of the 100 products containing product reviews for each product. Each product can be identified in the product list as the asin number of product is mentioned in the reviews file's filename."	17	206	4	unwrangle	shower-get-product-reviews-for-100-skus-on-amazon
712	712	Marketing Data Analysis		[]		3	8	0	upasanapurohit	marketing-data-analysis
713	713	tweets		['online communities']		0	1	0	tomriddle1912	tweets
714	714	Playlist4		[]		0	0	0	amauriramirez	playlist4
715	715	Playlist		['arts and entertainment']		0	0	0	amauriramirez	playlist
716	716	FTI_Hack_Train		[]		0	6	0	ostapkalapun	fti-hack-train
717	717	World Bank's Gender Statistics	The Gender Statistics Database is a comprehensive source.	['gender', 'social science', 'demographics']	"About this dataset
&gt; <p>The Gender Statistics database is a comprehensive source for the latest sex-disaggregated data and gender statistics covering demography, education, health, access to economic opportunities, public life and decision-making, and agency.</p>
<p><strong><em>Source:</em></strong> <a href=""http://data.worldbank.org/data-catalog/gender-statistics"">http://data.worldbank.org/data-catalog/gender-statistics</a></p>
<p><a href=""http://web.worldbank.org/WBSITE/EXTERNAL/0,,contentMDK:22547097~pagePK:50016803~piPK:50016805~theSitePK:13,00.html"" target=""_blank"" rel=""nofollow"">World Bank Data Catalog Terms of Use</a></p>
This dataset was created by World Bank and contains around 200000 samples along with 2011, 1983, technical information and other features such as:
- 2005
- 1987
- and more.
How to use this dataset
&gt; - Analyze Country Code in relation to 2004
- Study the influence of 1962 on 1989
- More datasets
Acknowledgements
If you use this dataset in your research, please credit World Bank 
Start A New Notebook!"	39	394	2	yamqwe	gender-statisticse
718	718	CPS2015MH		[]		0	2	0	massimilianohasan	cps2015mh
719	719	happywhale-tfrecords-v1		[]	128 x 128	11	14	0	slow5620	happywhale-tfrecords-v1
720	720	Drownings and Nicholas Cage		[]		0	4	0	michaeldinardi	drownings-and-nicholas-cage
721	721	NER assignment		[]		0	0	0	zzhou016	ner-assignment
722	722	flower400	400 types of flowers occur in Hong Kong	['asia', 'environment', 'plants', 'cnn', 'image data']	"Context
The dataset is planned to use in the project of predicting Hong Kong plants images.
https://github.com/r48n34/leafers"	0	21	1	r48n34	flower400
723	723	COCA Dataset	The Corpus of Contemporary American English	['research', 'education', 'beginner', 'tabular data']	"About
The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English. COCA is probably the most widely-used corpus of English, and it is related to many other corpora of English that we have created. These corpora were formerly known as the ""BYU Corpora""), and they offer unparalleled insight into variation in English.
The corpus contains more than one billion words of text (25+ million words each year 1990-2019) from eight genres: spoken, fiction, popular magazines, newspapers, academic texts, and (with the update in March 2020): TV and Movies subtitles, blogs, and other web pages.
For more information, go to https://www.english-corpora.org/coca/.
For more information on the categories used, click here."	39	427	6	ironicninja	coca-dataset
724	724	☢️ Nuclear Power Plants	Nuclear power plants currently in operation	['business', 'energy']	"About this dataset
&gt; <p>Nuclear power plants is defined as the number of nuclear units in operation as of 1 June 2013. It is measured as a number.</p>
<h3>Citation</h3>
<pre><code>OECD (2017), Nuclear power plants (indicator). 
doi: 10.1787/3cc1191d-en (Accessed on 30 August 2017)
</code></pre>
<p><strong><em>Source:</em></strong> <a href=""https://data.oecd.org/energy/nuclear-power-plants.htm"" target=""_blank"" rel=""nofollow"">https://data.oecd.org/energy/nuclear-power-plants.htm</a></p>
This dataset was created by OECD and contains around 0 samples along with Subject, Frequency, technical information and other features such as:
- Indicator
- Time
- and more.
How to use this dataset
&gt; - Analyze Measure in relation to Flag Codes
- Study the influence of Value on Subject
- More datasets
Acknowledgements
If you use this dataset in your research, please credit OECD 
Start A New Notebook!"	31	400	4	yamqwe	nuclear-power-plantse
725	725	Job Dataset Indeed India	This dataset includes job data from Indeed India	['jobs and career']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 473277  Domain Name : indeed.co.in  Date Range : 01st Jul 2021 - 30th Sep 2021   File Extension : ldjson
Available Fields : uniq_id, crawl_timestamp, url, job_title, category, company_name, city, state, country, post_date, job_description, job_type, company_description, job_board, geo, job_post_lang, html_job_description, inferred_iso2_lang_code, is_remote, test1_cities, test1_states, test1_countries, site_name, domain, postdate_yyyymmdd, predicted_language, inferred_iso3_lang_code, test1_inferred_city, test1_inferred_state, test1_inferred_country, inferred_city, inferred_state, inferred_country, has_expired, last_expiry_check_date, latest_expiry_check_date, dataset, postdate_in_indexname_format, segment_name, duplicate_status, job_desc_char_count, fitness_score   
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud, DataStock and live job data from JobsPikr.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	11	63	3	promptcloud	job-dataset-indeed-india
726	726	kaggle_task		[]		0	1	0	amalismaiil	kaggle-task
727	727	Murder Stats in Big US Cities	Recorded Murders, technical information and more	['crime']	"About this dataset
&gt; <p>The raw data behind the story <a href=""http://fivethirtyeight.com/features/a-handful-of-cities-are-driving-2016s-rise-in-murders/"" target=""_blank"" rel=""nofollow"">A Handful Of Cities Are Driving 2016’s Rise In Murders</a> from <a href=""FiveThirtyEight.com"" target=""_blank"" rel=""nofollow"">FiveThirtyEight.com</a>.</p>
<ul>
<li>
<p>murder_2015.csv contains full-year 2014 and 2015 murder counts for all U.S. cities with at least 250,000 residents. The source is FBI Uniform Crime Reports.</p>
</li>
<li>
<p>murder_2016_prelim.csv contains preliminary 2016 murder counts for 79 large U.S. cities. 2015 figures are counts through the same data a year ago. Sources are listed in the file.</p>
</li>
</ul>
<p><img src=""http://i.imgur.com/inkgdPX.png"" alt=""alt text"" style=""""></p>
<p><em>Chart from article</em></p>
<p>Source: <a href=""https://github.com/fivethirtyeight/data/tree/master/murder_2016"" target=""_blank"" rel=""nofollow"">FiveThirtyEight Github</a></p>
This dataset was created by Selene Arrazolo and contains around 100 samples along with Source, 2016 Murders, technical information and other features such as:
- 2015 Murders
- As Of
- and more.
How to use this dataset
&gt; - Analyze Change in relation to State
- Study the influence of Source on 2016 Murders
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	54	363	3	yamqwe	murder-stats-in-big-us-citiese
728	728	flighdata		[]		1	7	0	irmesh	flighdata
729	729	COVID-19 dataset	Dataset coronavirus pandemic	['exploratory data analysis', 'data visualization', 'time series analysis', 'deep learning', 'covid19']	"Content:
""Our World in Data"" which in collaboration with The University of Oxford have developed a reliable repository of datasets about dozens of topics focusing on those big problems which affect the world. This is why since the beginning of COVID-19 outbreak several researchers have been collecting data from every country in the world about multiple indicators which can make us take better decisions, what is more amazing is the fact that this dataset offered is updated every day for all countries allowing people to keep track of it. In the following link you can find fascinating charts about the pandemic and obviously the World COVID-19 dataset (up to date) containing over 60 features which you can download for free:
https://ourworldindata.org/covid-vaccinations
Important to consider:
I will be updating this dataset every week according to the published data by the organization, if you found this dataset or the link given useful I would really appreciate your upvote!
Acknowledgements and Citation
Mathieu, E., Ritchie, H., Ortiz-Ospina, E. et al. A global database of COVID-19 vaccinations. Nat Hum Behav (2021)"	1335	6436	43	georgesaavedra	covid19-dataset
730	730	Crime Data São Paulo from SSP	Crime Data of São Paulo from SSP (Secretaria de Segurança Pública)	['cities and urban areas', 'brazil', 'public safety', 'python']	"Hi Fellas!
This dataset is about crime data in São Paulo City Brazil. I've scraping this data in SSP (Secretária de Segurança Pública) website. SSP is a government agency of public security in São Paulo state. A observation, São Paulo city is capital of state the of São Paulo, equal in Rio de Janeiro. 
Any questions, is just send me message in email: marcus.rodrigues4003@gmail.com
SSP website: http://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx"	31	441	2	markfinn1	crime-data-in-so-paulosp-brazil
731	731	Готовность российских компаний к киберугрозам.	Cyber risks readiness. Russia 2018-2020	['russia', 'economics', 'statistical analysis', 'linear regression', 'tabular data']	"Дорогие коллеги! 
Рады представить Вам исследование готовности российских компаний к киберугрозам, подготовленное ООО ""Высшая Школа образования"" для целей исследования готовности крупных российских компаний к киберугрозам.
Данные
Данные представлены в виде 1146 наблюдений за 3 года с 2018 по 2020 гг. для 382 российских компаний из отраслей IT-Telecom, финансов (банки, страховые компании), строительства, производства, энергетики, медицины и других. Данные представлены в панельном виде и подразумевают панельный анализ.
Показатель совокупной способности противодействия киберугрозам (CYBERSEC) изначально собран из данных промежуточного, более широкого опросника, а потом агрегированы по пунктам (Z-pca score), предложенным ниже:
1. Показатель INFR. Организация обладает достаточным  уровнем IT-инфраструктуры для противостояния киберугрозам (Осуществлялись ли инвестиции в проекты, направленные на предотвращение киберугроз, нанято достаточно IT-профессионалов, используется обновленное ПО  и тд) (от 1 до 5). 
2. Показатель PEOPLE Менеджмент организации уделяет достаточное внимание вопросам кибербезопасности (Данная цель включена в приоритетные стратегические цели компании, внедряются и обновляются практики по обеспечению кибербезопасности и тд).Сотрудники организации обладают достаточным уровнем компетенции в вопросах кибербезопасности (регулярно проводятся тренинги, тренинги по вопросам кибер-безопасности доступны для сотрудников и тд) (от 1 до 5)
3. Показатель PARTNERS. Партнеры и поставщики организации будут сотрудничать с организацией в целях предотвращения и/или ликвидации инцидентов, связанных с киберугрозой (например, утечкой данных) (от 1 до 5)
С остальной информацией по исследованию можно ознакомится в файле расшифровке переменных."	2	17	0	stanislavkurovskiy	cybersecurity-russia2018-2020
732	732	2D ANALYZE OF FRAME	2 DIMENSIONAL ANALYSIS OF FRAME AND TRUSS USING ANASTRUCT	['earth and nature', 'statistical analysis']	This is created for the people who do structural analysis of a building. Many of them are not affordable to purchase a software to do analysis and for some one its diificult to write a code for analysis. this will be useful for them.	27	1269	7	axelnovo	2d-analyze-of-frame
733	733	Resale Properties in Rural	Properties resale in Rural USA	[]		11	2473	0	hangrand	resale-properties-in-rural
734	734	retinanetweights		[]		0	120	0	orangecai	retinanetweights
735	735	Air Pollution Effects: Asthma Prevalence by Year 	Estimate percentage of Californians with asthma (asthma prevalence)	['health', 'health conditions']	"About this dataset
&gt; <p>This dataset contains the estimated percentage of Californians with asthma (asthma prevalence). Two types of asthma prevalence are included: 1) lifetime asthma prevalence describes the percentage of people who have ever been diagnosed with asthma by a health care provider, 2) current asthma prevalence describes the percentage of people who have ever been diagnosed with asthma by a health care provider AND report they still have asthma and/or had an asthma episode or attack within the past 12 months. The tables “Lifetime Asthma Prevalence by County” and “Current Asthma Prevalence by County” are derived from the California Health Interview Survey (CHIS) and include data stratified by county and age group (all ages, 0-17, 18+, 0-4, 5-17, 18-64, 65+) reported for 2-year periods. The table “Asthma Prevalence, Adults (18 and older)” is derived from the California Behavioral Risk Factor Surveillance System (BRFSS) and includes statewide data on adults reported by year.</p>
<p>Source: <a href=""https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/EHIB/CPE/Pages/CaliforniaBreathing.aspx"" target=""_blank"" rel=""nofollow"">https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/EHIB/CPE/Pages/CaliforniaBreathing.aspx</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2020-07-31<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 0 samples along with 95% Ci Lower Limit, Measure, technical information and other features such as:
- Unnamed: 7
- Unnamed: 9
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 6 in relation to Unnamed: 8
- Study the influence of Unnamed: 5 on 95% Ci Upper Limit
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	64	586	4	yamqwe	asthma-prevalencee
736	736	happy-whale-tfrecords-512x880		[]		0	5	0	dschettler8845	happywhaletfrecords512x880
737	737	AWS CLI and boto3 github issues	Title, body and labels of both issues repositories	['websites', 'law', 'nlp', 'text mining']	"Context
I processed the data from the AWS CLI and boto3 repositories using the GitHub API, if the data is processed in a meaningful way, it could be a great way to help the mainteiners of those repositories to get better insights about the issues the customers are having by clustering them.
Content
The data is updated at the time and date this dataset was made public. 
Both JSON files have the same structure, ""closed_issues"" and ""open_issues"", it is an array of issues, where each element have the title, the body and the assigned labels.
Considerations
Old closed issues (last ones, I did not add the date per issue) don't have assigned labels, and if they have, it is ""bug"" or ""guidance"", it seems that the open ones all have labels. Even though, NLP may be really helpful for to get insights of the data."	0	105	2	elizarrarasl	aws-cli-and-boto3-github-issues
738	738	IP Whale Dateset		[]		1	9	0	akhileshdkapse	ip-whale-dateset
739	739	Battlerite Match+Metadata	Battlerite (esports game) match data from 05-2018 to 08-2019	['video games', 'tabular data']	"Context
Data fetched to do the simple analysis for battleritebuilds.com, a statistics page I built for the game called Battlerite.
The game is semi-defunct these days, and the developers pulled most of the support at the end of 2019. I figured it'd be a waste to let all the data I collected for analysis go to waste. The dataset contains 405 479 matches, from 227 756 users in 326 734 teams, so it's rather comprehensive.
Content
The dataset contains 4 tables of data, as csvs.
*  matchrounds.csv: contains each player's performance on a given round, of a given match. Matches are either decided after 2 or 3 rounds. Less if someone disconnects
* playermatch.csv: each player participating in a match, and data on their build, and if they won/lost the game.
* playerteams.csv: all the teams a player was a member of
* teams.csv: rank and general stats for each team
* assets.tar.gz: If anyone would like to visualize what properties each skill/hero/map actually has, this is provided here, on a patch-by-patch basis. There are even some images included for visualizing skills and heroes. 
The data was collected on ranked matches from may 2018 to august 2019.
Acknowledgements
Stunlock Studios should have thanks for providing good APIs for fetching this data.
Inspiration
It would be really cool to see some visual exploration of this data. I only did what is visible on battleritebuilds.com, but there should be quite a lot of unused data in the set."	0	64	7	negation	battlerite20182019
740	740	Coronavirus (Covid-19) Data of United States (USA)	The New York Times data on coronavirus cases and deaths in the U.S	['music', 'united states', 'health', 'covid19']	"Coronavirus (COVID-19) Data in the United States
[ U.S. State-Level Data (Raw CSV) | U.S. County-Level Data (Raw CSV) ]
The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak.
Since late January, The Times has tracked cases of coronavirus in real-time as they were identified after testing. Because of the widespread shortage of testing, however, the data is necessarily limited in the picture it presents of the outbreak.
We have used this data to power our maps and reporting tracking the outbreak, and it is now being made available to the public in response to requests from researchers, scientists, and government officials who would like access to the data to better understand the outbreak.
The data begins with the first reported coronavirus case in Washington State on Jan. 21, 2020. We will publish regular updates to the data in this repository. 
United States Data
Data on cumulative coronavirus cases and deaths can be found in two files for states and counties.
Each row of data reports cumulative counts based on our best reporting up to the moment we publish an update. We do our best to revise earlier entries in the data when we receive new information.
Both files contain FIPS codes, a standard geographic identifier, to make it easier for an analyst to combine this data with other data sets like a map file or population data.
Download all the data or clone this repository by clicking the green ""Clone or download"" button above.
State-Level Data
State-level data can be found in the states.csv file. (Raw CSV file here.)
date,state,fips,cases,deaths
2020-01-21,Washington,53,1,0
...
County-Level Data
County-level data can be found in the counties.csv file. (Raw CSV file here.)
date,county,state,fips,cases,deaths
2020-01-21,Snohomish,Washington,53061,1,0
...
In some cases, the geographies where cases are reported do not map to standard county boundaries. See the list of geographic exceptions for more detail on these.
Methodology and Definitions
The data is the product of dozens of journalists working across several time zones to monitor news conferences, analyze data releases and seek clarification from public officials on how they categorize cases. 
It is also a response to a fragmented American public health system in which overwhelmed public servants at the state, county and territorial levels have sometimes struggled to report information accurately, consistently and speedily. On several occasions, officials have corrected information hours or days after first reporting it. At times, cases have disappeared from a local government database, or officials have moved a patient first identified in one state or county to another, often with no explanation. In those instances, which have become more common as the number of cases has grown, our team has made every effort to update the data to reflect the most current, accurate information while ensuring that every known case is counted.
When the information is available, we count patients where they are being treated, not necessarily where they live.
In most instances, the process of recording cases has been straightforward. But because of the patchwork of reporting methods for this data across more than 50 state and territorial governments and hundreds of local health departments, our journalists sometimes had to make difficult interpretations about how to count and record cases.
For those reasons, our data will in some cases not exactly match the information reported by states and counties. Those differences include these cases: When the federal government arranged flights to the United States for Americans exposed to the coronavirus in China and Japan, our team recorded those cases in the states where the patients subsequently were treated, even though local health departments generally did not. When a resident of Florida died in Los Angeles, we recorded her death as having occurred in California rather than Florida, though officials in Florida counted her case in their records. And when officials in some states reported new cases without immediately identifying where the patients were being treated, we attempted to add information about their locations later, once it became available.
Confirmed Cases
Confirmed cases are patients who test positive for the coronavirus. We consider a case confirmed when it is reported by a federal, state, territorial or local government agency.
Dates
For each date, we show the cumulative number of confirmed cases and deaths as reported that day in that county or state. All cases and deaths are counted on the date they are first announced.
Counties
In some instances, we report data from multiple counties or other non-county geographies as a single county. For instance, we report a single value for New York City, comprising the cases for New York, Kings, Queens, Bronx and Richmond Counties. In these instances, the FIPS code field will be empty. (We may assign FIPS codes to these geographies in the future.) See the list of geographic exceptions. 
Cities like St. Louis and Baltimore that are administered separately from an adjacent county of the same name are counted separately.
“Unknown” Counties
Many state health departments choose to report cases separately when the patient’s county of residence is unknown or pending determination. In these instances, we record the county name as “Unknown.” As more information about these cases becomes available, the cumulative number of cases in “Unknown” counties may fluctuate.
Sometimes, cases are first reported in one county and then moved to another county. As a result, the cumulative number of cases may change for a given county.
Geographic Exceptions
New York City
All cases for the five boroughs of New York City (New York, Kings, Queens, Bronx and Richmond counties) are assigned to a single area called New York City.
Kansas City, Mo.
Four counties (Cass, Clay, Jackson, and Platte) overlap the municipality of Kansas City, Mo. The cases and deaths that we show for these four counties are only for the portions exclusive of Kansas City. Cases and deaths for Kansas City are reported as their line.
Alameda, Calif.
Counts for Alameda County include cases and deaths from Berkeley and the Grand Princess cruise ship.
Chicago
All cases and deaths for Chicago are reported as part of Cook County. 
License and Attribution
In general, we are making this data publicly available for broad, noncommercial public use including by medical and public health researchers, policymakers, analysts and local news media.
If you use this data, you must attribute it to “The New York Times” in any publication. If you would like a more expanded description of the data, you could say “Data from The New York Times, based on reports from state and local health agencies.”
If you use it in an online presentation, we would appreciate it if you would link to our U.S. tracking page at https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html.
If you use this data, please let us know at covid-data@nytimes.com and indicate if you would be willing to talk to a reporter about your research.
See our LICENSE for the full terms of use for this data.
This license is co-extensive with the Creative Commons Attribution-NonCommercial 4.0 International license, and licensees should refer to that license (CC BY-NC) if they have questions about the scope of the license.
Contact Us
If you have questions about the data or licensing conditions, please contact us at:
covid-data@nytimes.com
Contributors
Mitch Smith, Karen Yourish, Sarah Almukhtar, Keith Collins, Danielle Ivory, and Amy Harmon have been leading our U.S. data collection efforts.
Data has also been compiled by Jordan Allen, Jeff Arnold, Aliza Aufrichtig, Mike Baker, Robin Berjon, Matthew Bloch, Nicholas Bogel-Burroughs, Maddie Burakoff, Christopher Calabrese, Andrew Chavez, Robert Chiarito, Carmen Cincotti, Alastair Coote, Matt Craig, John Eligon, Tiff Fehr, Andrew Fischer, Matt Furber, Rich Harris, Lauryn Higgins, Jake Holland, Will Houp, Jon Huang, Danya Issawi, Jacob LaGesse, Hugh Mandeville, Patricia Mazzei, Allison McCann, Jesse McKinley, Miles McKinley, Sarah Mervosh, Andrea Michelson, Blacki Migliozzi, Steven Moity, Richard A. Oppel Jr., Jugal K. Patel, Nina Pavlich, Azi Paybarah, Sean Plambeck, Carrie Price, Scott Reinhard, Thomas Rivas, Michael Robles, Alison Saldanha, Alex Schwartz, Libby Seline, Shelly Seroussi, Rachel Shorey, Anjali Singhvi, Charlie Smart, Ben Smithgall, Steven Speicher, Michael Strickland, Albert Sun, Thu Trinh, Tracey Tully, Maura Turcotte, Miles Watkins, Jeremy White, Josh Williams, and Jin Wu.
Context
There's a story behind every dataset and here's your opportunity to share yours.# Coronavirus (Covid-19) Data in the United States
[ U.S. State-Level Data (Raw CSV) | U.S. County-Level Data (Raw CSV) ]
The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak.
Since late January, The Times has tracked cases of coronavirus in real time as they were identified after testing. Because of the widespread shortage of testing, however, the data is necessarily limited in the picture it presents of the outbreak.
We have used this data to power our maps and reporting tracking the outbreak, and it is now being made available to the public in response to requests from researchers, scientists and government officials who would like access to the data to better understand the outbreak.
The data begins with the first reported coronavirus case in Washington State on Jan. 21, 2020. We will publish regular updates to the data in this repository. 
United States Data
Data on cumulative coronavirus cases and deaths can be found in two files for states and counties.
Each row of data reports cumulative counts based on our best reporting up to the moment we publish an update. We do our best to revise earlier entries in the data when we receive new information.
Both files contain FIPS codes, a standard geographic identifier, to make it easier for an analyst to combine this data with other data sets like a map file or population data.
Download all the data or clone this repository by clicking the green ""Clone or download"" button above.
State-Level Data
State-level data can be found in the states.csv file. (Raw CSV file here.)
date,state,fips,cases,deaths
2020-01-21,Washington,53,1,0
...
County-Level Data
County-level data can be found in the counties.csv file. (Raw CSV file here.)
date,county,state,fips,cases,deaths
2020-01-21,Snohomish,Washington,53061,1,0
...
In some cases, the geographies where cases are reported do not map to standard county boundaries. See the list of geographic exceptions for more detail on these.
Methodology and Definitions
The data is the product of dozens of journalists working across several time zones to monitor news conferences, analyze data releases and seek clarification from public officials on how they categorize cases. 
It is also a response to a fragmented American public health system in which overwhelmed public servants at the state, county and territorial level have sometimes struggled to report information accurately, consistently and speedily. On several occasions, officials have corrected information hours or days after first reporting it. At times, cases have disappeared from a local government database, or officials have moved a patient first identified in one state or county to another, often with no explanation. In those instances, which have become more common as the number of cases has grown, our team has made every effort to update the data to reflect the most current, accurate information while ensuring that every known case is counted.
When the information is available, we count patients where they are being treated, not necessarily where they live.
In most instances, the process of recording cases has been straightforward. But because of the patchwork of reporting methods for this data across more than 50 state and territorial governments and hundreds of local health departments, our journalists sometimes had to make difficult interpretations about how to count and record cases.
For those reasons, our data will in some cases not exactly match with the information reported by states and counties. Those differences include these cases: When the federal government arranged flights to the United States for Americans exposed to the coronavirus in China and Japan, our team recorded those cases in the states where the patients subsequently were treated, even though local health departments generally did not. When a resident of Florida died in Los Angeles, we recorded her death as having occurred in California rather than Florida, though officials in Florida counted her case in their own records. And when officials in some states reported new cases without immediately identifying where the patients were being treated, we attempted to add information about their locations later, once it became available.
Confirmed Cases
Confirmed cases are patients who test positive for the coronavirus. We consider a case confirmed when it is reported by a federal, state, territorial or local government agency.
Dates
For each date, we show the cumulative number of confirmed cases and deaths as reported that day in that county or state. All cases and deaths are counted on the date they are first announced.
Counties
In some instances, we report data from multiple counties or other non-county geographies as a single county. For instance, we report a single value for New York City, comprising the cases for New York, Kings, Queens, Bronx and Richmond Counties. In these instances the FIPS code field will be empty. (We may assign FIPS codes to these geographies in the future.) See the list of geographic exceptions. 
Cities like St. Louis and Baltimore that are administered separately from an adjacent county of the same name are counted separately.
“Unknown” Counties
Many state health departments choose to report cases separately when the patient’s county of residence is unknown or pending determination. In these instances, we record the county name as “Unknown.” As more information about these cases becomes available, the cumulative number of cases in “Unknown” counties may fluctuate.
Sometimes, cases are first reported in one county and then moved to another county. As a result, the cumulative number of cases may change for a given county.
Geographic Exceptions
New York City
All cases for the five boroughs of New York City (New York, Kings, Queens, Bronx and Richmond counties) are assigned to a single area called New York City.
Kansas City, Mo.
Four counties (Cass, Clay, Jackson and Platte) overlap the municipality of Kansas City, Mo. The cases and deaths that we show for these four counties are only for the portions exclusive of Kansas City. Cases and deaths for Kansas City are reported as their own line.
Alameda, Calif.
Counts for Alameda County include cases and deaths from Berkeley and the Grand Princess cruise ship.
Chicago
All cases and deaths for Chicago are reported as part of Cook County. 
License and Attribution
In general, we are making this data publicly available for broad, noncommercial public use including by medical and public health researchers, policymakers, analysts and local news media.
If you use this data, you must attribute it to “The New York Times” in any publication. If you would like a more expanded description of the data, you could say “Data from The New York Times, based on reports from state and local health agencies.”
If you use it in an online presentation, we would appreciate it if you would link to our U.S. tracking page at https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html.
If you use this data, please let us know at covid-data@nytimes.com and indicate if you would be willing to talk to a reporter about your research.
See our LICENSE for the full terms of use for this data.
This license is co-extensive with the Creative Commons Attribution-NonCommercial 4.0 International license, and licensees should refer to that license (CC BY-NC) if they have questions about the scope of the license.
Contact Us
If you have questions about the data or licensing conditions, please contact us at:
covid-data@nytimes.com
Contributors
Mitch Smith, Karen Yourish, Sarah Almukhtar, Keith Collins, Danielle Ivory and Amy Harmon have been leading our U.S. data collection efforts.
Data has also been compiled by Jordan Allen, Jeff Arnold, Aliza Aufrichtig, Mike Baker, Robin Berjon, Matthew Bloch, Nicholas Bogel-Burroughs, Maddie Burakoff, Christopher Calabrese, Andrew Chavez, Robert Chiarito, Carmen Cincotti, Alastair Coote, Matt Craig, John Eligon, Tiff Fehr, Andrew Fischer, Matt Furber, Rich Harris, Lauryn Higgins, Jake Holland, Will Houp, Jon Huang, Danya Issawi, Jacob LaGesse, Hugh Mandeville, Patricia Mazzei, Allison McCann, Jesse McKinley, Miles McKinley, Sarah Mervosh, Andrea Michelson, Blacki Migliozzi, Steven Moity, Richard A. Oppel Jr., Jugal K. Patel, Nina Pavlich, Azi Paybarah, Sean Plambeck, Carrie Price, Scott Reinhard, Thomas Rivas, Michael Robles, Alison Saldanha, Alex Schwartz, Libby Seline, Shelly Seroussi, Rachel Shorey, Anjali Singhvi, Charlie Smart, Ben Smithgall, Steven Speicher, Michael Strickland, Albert Sun, Thu Trinh, Tracey Tully, Maura Turcotte, Miles Watkins, Jeremy White, Josh Williams and Jin Wu.
Context
There's a story behind every dataset and here's your opportunity to share yours."	713	7010	18	joelhanson	coronavirus-covid19-data-in-the-united-states
741	741	Slogan Dataset	Dataset for analyzing slogans of various organizations	['marketing', 'data analytics']	"Context
""Just do it"" (nike), ""Das Auto""(Volkswagen), ""High Performance Delivered"" (Accenture), ""Think Different"" (Apple) are some of the all-time famous slogans (mottos/taglines) of the famous corporations. They have left a mark in the minds of the audience. I was curious to understand the thought process behind the slogans, aside from knowing which company has which slogan. These tagline/mottos are a reflection of the company, the organization culture, the psychology and mindset. More than anything else, it is a symbol.
Content
I acquired the data by scraping off a bunch of websites. I started with scraping the data from slogan-list.com (using basic Python libraries - beautiful soup and requests)
Acknowledgements
I owe a lot to the websites through which slogans were captured.
- slogan-list.com
Above all, I owe it to the companies/organizations and their marketing/advertising teams responsible for coming up with such constructive and sticky slogans/taglines.
Inspiration
I hope this dataset serves useful for all the marketing analysts, businessmen/entrepreneurs, strategy folks.
Some open questions we can find answers to:
1. What are some of the commonly used words in sticky slogans?
2. How many words does a usual slogan have?
3. What's the secret behind the popular slogans?
4. Are there any trends emerging from the slogans?
5. Are certain slogans specific to certain industry? Is there any correlation between the two?"	1612	21186	89	chaibapat	slogan-dataset
742	742	Virginia Lamb Auction Data	Livestock auction data from Virginia,  US	[]		9	2372	1	kaypong	virginia-lamb-auction-data
743	743	yolov5		[]		3	30	0	ageage	yolov5
744	744	Global Political tweets 	Tweets across the globe with trending #Politics hashtag	['politics', 'exploratory data analysis', 'nlp', 'news', 'online communities', 'social networks']	"Social media is becoming a key medium through which we communicate with each other: it is at the center of the very structures of our daily interactions. Yet this infiltration is not unique to interpersonal relations. Political leaders, governments, and states operate within this social media environment, wherein they continually address crises and institute damage control through platforms such as Twitter.
With the proliferation of the internet into mass masses, social media is emerging as a potential way of communication. It provides a direct channel to politicians for communicating, connecting, and engaging with the public. The power of social media, especially Twitter and Facebook has been proved by its successful application during recent US presidential elections and Arabian countries' revolts. In India too, as the general election is about to knock at the door during early 2014, political parties and leaders are trying to harness the power of social media.
Content
The tweets have the #Politics hashtag. The collection started on 24/7/2021, and will be updated on a daily basis.
Information regarding the data
The data totally consists of 1 lakh+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers an account currently has. |
| 6 | user_friends | The number of friends an account currently has. |
| 7 | user_favourites | The number of favorites an account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | date  | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #Politics  |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Inspiration
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, and look at trends."	100	912	4	kaushiksuresh147	political-tweets
745	745	ByteTrack		[]		0	8	0	yamanity	bytetrack
746	746	Formula 1 (F1)  trending tweets 🏎 🏁	Tweets posted with the trending #f1 hashtag	['auto racing', 'sports', 'exploratory data analysis', 'data visualization', 'data analytics', 'text data']	"Formula One (also known as Formula 1 or F1) is the highest class of international auto racing for single-seater formula racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). The World Drivers' Championship, which became the FIA Formula One World Championship in 1981, has been one of the premier forms of racing around the world since its inaugural season in 1950. The word formula in the name refers to the set of rules to which all participants' cars must conform. A Formula One season consists of a series of races, known as Grands Prix, which take place worldwide on both purpose-built circuits and closed public roads.
The craze for F1 among the fans is astonishing, which has been creating quite a buzz in major social media platforms like Twitter. The dataset brings you such tweets posted with the #f1 hashtag. 
Information regarding the data
The data totally consists of 50k+ records with 13 columns. The collection started on 25/7/2020 and will be updated regularly. The description of the features is given below.
Inspiration
        ""I am an artist, the track is my canvas and the car is my brush."" – Graham Hill"	44	799	3	kaushiksuresh147	formula-1-trending-tweets
747	747	💰 International Financial Statistics By Country	International Financial Statistics database covers 200 countries	['business', 'finance', 'government', 'economics', 'investing', 'news']	"About this dataset
&gt; <p>The International Financial Statistics database covers about 200 countries and areas, with some aggregates calculated for selected regions, plus some world totals. Topics covered include balance of payments, commodity prices, exchange rates, fund position, government finance, industrial production, interest rates, international investment position, international liquidity, international transactions, labor statistics, money and banking, national accounts, population, prices, and real effective exchange rates.</p>
<p><strong>Geographic Coverage</strong><br>
IFS covers 194 countries and areas. Under the Fund’s legal framework, the member in effective control of a territory must report data respecting that territory. With respect to data on any territory whose status is the subject of a dispute between members, the IMF’s use of data on that territory provided by a member, either for surveillance or any other Fund activities, does not constitute a judgment by the IMF on the status of that territory.</p>
<p><strong>Latest Update Data as of download</strong><br>
02/24/2017</p>
<p><strong>Methodology</strong><br>
The International Financial Statistics is based on various IMF data collections. It includes exchange rates series for all Fund member countries plus Anguilla, Aruba, China, P.R.: Hong Kong, China, P.R.: Macao, Montserrat, and the Netherlands Antilles. It also includes major Fund accounts series, real effective exchange rates, and other world, area, and country series. Data are available for most IMF member countries with some aggregates calculated for select regions, plus some world totals.</p>
<p><strong>Sectoral Coverage</strong><br>
Sectoral Coverage: National Accounts, Indicators of Economic Activity, Labor Markets, Prices, Government and Public Sector Finance, Financial Indicators, Balance of Payments, International Investment Position, International Reserves, Fund Accounts, External Trade, Exchange Rates, and Population.</p>
<p><strong>Temporal Converage</strong><br>
Data available starting in the 1948 for many IMF member countries. Varies by country.</p>
<hr>
<p>The Data is provided to Users “as is” and without warranty of any kind, either express or implied, including, without limitation, warranties of merchantability, fitness for a particular purpose and noninfringement.</p>
<p>The IMF Data is  available free of charge from the IMF.</p>
<p>Users are prohibited from infringing upon the integrity of the IMF data and in particular shall refrain from any act of alteration of the IMF data that intentionally affects its nature or accuracy. If the IMF data is materially transformed by the user, this must be stated explicitly along with the required source citation.</p>
<p>All other terms set forth in the IMF's general terms and conditions (<a href=""http://www.imf.org/external/terms.htm"" target=""_blank"" rel=""nofollow"">http://www.imf.org/external/terms.htm</a>) shall continue to apply to use of IMF Data.</p>
<p><strong><em>Source:</em></strong> <a href=""http://data.imf.org/?sk=388DFA60-1D26-4ADE-B505-A05A558D9A42&amp;sId=1479329132340"" target=""_blank"" rel=""nofollow"">http://data.imf.org/?sk=388DFA60-1D26-4ADE-B505-A05A558D9A42&amp;sId=1479329132340</a></p>
This dataset was created by International Monetary Fund and contains around 200000 samples along with 1962q2, 1975q1, technical information and other features such as:
- 2012m4
- 2016m9
- and more.
How to use this dataset
&gt; - Analyze 2009m10 in relation to 1950m9
- Study the influence of 1999m6 on 1959m2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit International Monetary Fund 
Start A New Notebook!"	35	303	5	yamqwe	international-financial-statistics-ifse
748	748	CT SCAN FOR COVID 		[]		6	44	3	balasubramaniamv	ct-scan-for-covid
749	749	ECG_Image_Cropped	ECG Dataset including COVID-19	['signal processing', 'classification', 'cnn', 'image data', 'covid19']	"Context
This is a dataset i've using of ECG Classification of Patients including COVID-19
Content
The dataset regroups cropped images of differents kinds of ECG : COVID-19, Abnormal Heartbeat (HB), Normal Heartbeat, Myocardial Infarction (MI) and Patient that have History of MI (PMI ). The original dataset is a 12-lead based standard ECG images for differents patients with differents heart conditions.
Acknowledgements
Sources :
Original dataset (Non-cropped images) : https://doi.org/10.1016/j.dib.2021.106762
Cropped Dataset : https://github.com/hardikroutray/ECG/tree/main/CroppedECGImages_data_v2 and https://github.com/hardikroutray/ECG/tree/main/CroppedECGImages/ECGImagesofCOVID-19Patients_cropped"	31	88	0	marcjuniornkengue	ecg-image-cropped
750	750	yaa-seeee-utils		[]		0	2	0	yaaseeee	yaaseeeeutils
751	751	kesci_2021_underwater_optics		[]		0	5	0	dwchen	kesci-2021-underwater-optics
752	752	Elementary Excel Data	This is elementary excel Data for EDA and training various ML models.	['education']		0	3	0	yashvardhanprasad	workof-comp
753	753	wer prediction		[]		0	0	0	kunalsah	wer-prediction
754	754	SpatialBiLSTMtpu		[]		0	1	0	mahdibb	spatialbilstmtpu
755	755	happywhale-tfrecords-v1-512x512-cropped		[]		0	1	0	sega1031	happywhale-tfrecords-v1-512x512-cropped
756	756	LTE technical KPIs		['business']		2	4	1	vladimirfadeev	lte-technical-kpis
757	757	AV jbfeb22 Engage Score		[]		0	3	0	mohamedziauddin	av-jbfeb22-engage-score
758	758	heart_analysis	The dataset includes Health analytics related to Healthy heart status	['exploratory data analysis', 'feature engineering', 'pca', 'health conditions', 'automl']	"The dataset includes Health analytics related to Healthy heart status
The purpose of the dataset is to demonstrate Auto Feature Importance package using  Bregman Divergence:  pip install auto-feat-selection==0.0.5
Implementation available
https://www.kaggle.com/rupakroy/auto-feature-selection-using-bregman-divergence"	17	139	2	rupakroy	heart-analysis
759	759	turtle-recall-tfrecords-no-extra	DeepMind Turtle Dataset train and test	['computer science']		0	10	1	ferlockx	turtle-recall-tfrecords-no-extra
760	760	commonlit-test		[]		0	2	0	yujiariyasu	commonlit-test
761	761	timm-20220211		[]		0	3	0	haqishen	timm-20220211
762	762	models		['clothing and accessories']		0	8	0	bastiennes	models
763	763	happywhale-tfrecords-cropped-v1		[]		0	6	0	mofumofuchan	happywhale-tfrecords-cropped-v1
764	764	RFMID_Dataset		[]		3	119	1	tasnimsamir71	rfmid-dataset
765	765	Trained models for CIFAR-10 dataset		['computer vision', 'deep learning', 'image data', 'pytorch']		0	29	8	firuzjuraev	trained-models-for-cifar10-dataset
766	766	HappyWhale Cartoon		[]		0	3	0	ayuraj	happywhale-cartoon
767	767	SUV_Purchase		[]		1	2	0	parthsalke	suv-purchase
768	768	housing		['social issues and advocacy']		0	3	0	mayu326	housing
769	769	binance_usdt_busd_1h		[]		1	17	0	johanvandervlugt	binance-usdt-busd-1h
770	770	Asthma ED Visit Rates (LGHC Indicator)	Counts and rates (per 10,000 residents) of asthma	['public health', 'health', 'covid19']	"About this dataset
&gt; <p>This is a source dataset for a Let's Get Healthy California indicator at <a href=""https://letsgethealthy.ca.gov/"" target=""_blank"" rel=""nofollow"">https://letsgethealthy.ca.gov/</a>. This dataset contains counts and rates (per 10,000 residents) of asthma (ICD9-CM, 493.0-493.9) emergency department visits among California residents by County and age group (all ages, 0-17, 18+). The data are derived from the Office of Statewide Health Planning and Development emergency department databases. These data include emergency department visits from all licensed hospitals in California. These data are based only on primary discharge diagnosis codes (ICD9-CM). NOTE: Rates are calculated from the total number of Asthma ED Visits (not the unique number of individuals).</p>
<p>Source: <a href=""https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/EHIB/CPE/Pages/CaliforniaBreathing.aspx"" target=""_blank"" rel=""nofollow"">https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/EHIB/CPE/Pages/CaliforniaBreathing.aspx</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2021-05-10<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 9000 samples along with Lghc Target Rate, Lghc Indicator Name, technical information and other features such as:
- Strata
- Rate
- and more.
How to use this dataset
&gt; - Analyze Geography in relation to Lghc Indicator Id
- Study the influence of Age Group on Numerator
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	33	273	6	yamqwe	asthma-ed-visit-rates-lghc-indicatore
771	771	Top 100 Cryptocurrencies Historical Dataset 	Bitcoin, Ethereum, Binance, Ripple, Cardano, Polkadot and other leading coins	['finance', 'exploratory data analysis', 'data visualization', 'investing', 'currencies and foreign exchange']	"Context
Wait, What's a cryptocurrency🤔🤔?
A cryptocurrency or crypto is a digital asset designed to work as a medium of exchange wherein individual coin ownership records are stored in a ledger existing in a form of a computerized database using strong cryptography to secure transaction records, to control the creation of additional coins, and to verify the transfer of coin ownership.
A report from exchange Crypto.com estimated that there were 106 million crypto users around the world in January, following a 16% jump in participants last month alone.
A separate survey from financial advisory group deVere found 70% of its clients aged over 55 had already invested in digital currencies or were planning to do so, in 2021, despite bitcoin and others being strongly associated with younger, millennial investors.
Interesting facts about cryptocurrency:
You Can’t Lose Your Wallet
Beware of Cryptojacking
Bitcoin Inventor Is Unknown
Cryptocurrency Value Is Extremely Volatile
China Is The Biggest Miner Of Cryptocurrency
Cryptocurrency Can’t Be Physically Banned
Around 6 countries Have Banned Cryptocurrency
Cryptocurrency Is Great For Ecommerce
International Transactions Without Exchange
Resource: 9 facts that you need to know about cryptocurrency
Content
The Goal of the dataset is to bring the  historical data of the top 100 cryptocurrencies( Ranked based on the market cap)
What is Market capitalization?
Market capitalization is an indicator that measures and keeps track of the market value of a cryptocurrency. Market cap is used as an indicator of the dominance and popularity of cryptocurrencies. Though this metric is widely used, more information before making trading decisions is recommended.
About dataset
The dataset is a Zip file that consists of the crypto prices of nearly 75 coins from their start date till 30/11/2021 (Also working on the other 25 cryptocurrency data as promised). A leaderboard file as of 30/11/2021 has also been included in the zip file named Current Crypto Leaderboard.csv, which has the market capitalization, rank, coin name, symbol, Price, and a tagging variable that mentions the availability of the corresponding coin in the dataset.
Historical data(day-interval) of all the coins mentioned in the leaderboard is provided in the dataset. The leaderboard is subject to change as the market cap increases/decreases. Any new coin which enters the top 100 will be added to the dataset. The leaderboard and the dataset will be updated on a monthly basis.
Starter Notebook:
What is Cryptocurrency? 🤔
What, Why, Where, and How of Blockchain 🤔🙄
How to identify bull market using candlestick 🐂
Bitcoin Volatility analysis with interactive visualization 📈
Performance analysis of Top 10 Crypto's in Oct 2021 🔟💵
Content
Note: The datasets consist of the following columns and will be updated on a monthly basis 
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | Date | Date of the crypto prices |
| 2 | Close | Closing prices of crypto(dollars) |
| 3 | Open | Opening price of crypto on the respective date(Dollars) |
| 4 | High | Highest price of crypto on the respective date(Dollars) |
| 5 | Low | Lowest price of crypto on the respective date(Dollars) |
| 6 | Vol. | Volume of crypto on the respective date(Dollars). |
Disclaimer
The author doesn't recommend or advise investing in any of these coins mentioned as each investment is subject to risk. The dataset is purely for research and exploration purposes to understand the behavior and volatility of the crypto coins.
Inspiration
Do you believe that cryptocurrencies will revolutionize the industry? Well, I believe it has already begun to.
Are cryptocurrencies the future?
How volatile will these cryptocurrencies be?
Is cryptocurrency a good investment?
Can we predict the prices of cryptocurrency?
Is it better than the other traditional investment assets like Stocks, Funds, Tangibles?
Is investing in crypto a good diversification strategy?
and so on and on. There are too many questions that can arise given that we have limited information. But let's make the most out of it and understand these cryptos.
Task✍️
EDA on trending & potential Coins 
Can you predict the prices of these coins?
Can you find significant patterns and trends in the prices of these coins?
Compare different coins and their prices and behavior
Acknowledgements
The dataset was extracted using web scrapping and various python packages like investpy, yahoo finance, pandas data reader
Queries & Suggestion
Feel free to use the Discussion session if you have any queries accessing the data or if you find any error, which you would like to point out. Otherwise, Have a great day and happy exploring and kindly upvote the dataset 😇 ✌️🎉"	1491	7252	31	kaushiksuresh147	top-10-cryptocurrencies-historical-dataset
772	772	eff_b6_5fold		[]		3	18	0	librauee	eff-b6-5fold
773	773	Passsize		[]		1	2	0	yashsethi24	passsize
774	774	Customer Segmentation Classification	Classify the customers into four segments	['business', 'internet', 'automobiles and vehicles', 'classification', 'multiclass classification', 'online communities']	"Context
An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4, and P5). After intensive market research, they’ve deduced that the behavior of the new market is similar to their existing market. 
In their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for a different segment of customers. This strategy has work e exceptionally well for them. They plan to use the same strategy for the new markets and have identified 2627 new potential customers. 
You are required to help the manager to predict the right group of the new customers.
Content
|Variable|Definition|
|--|--|
|ID|Unique ID|
|Gender|Gender of the customer|
|Ever_Married|Marital status of the customer|
|Age|Age of the customer|
|Graduated|Is the customer a graduate?|
|Profession|Profession of the customer|
|Work_Experience|Work Experience in years|
|Spending_Score|Spending score of the customer|
|Family_Size|Number of family members for the customer (including the customer)|
|Var_1|Anonymised Category for the customer|
|Segmentation|(target) Customer Segment of the customer|
Acknowledgements
This dataset was acquired from the Analytics Vidhya hackathon."	3315	26547	43	kaushiksuresh147	customer-segmentation
775	775	FAANG (FB,Amazon,Apple,Netflix,Google) Stocks 📈	Historical OHLC dataset of FAANG stocks since the IPO	['business', 'finance', 'exploratory data analysis', 'data visualization', 'time series analysis', 'investing']	"FAANG
FAANG is an acronym referring to the stocks of the five most popular and best-performing American technology companies: Meta (formerly known as Facebook), Amazon, Apple, Netflix, and Alphabet (formerly known as Google). 
In addition to being widely known among consumers, the five FAANG stocks are among the largest companies in the world, with a combined market capitalization of nearly $7.1 trillion as of Aug. 19, 2021.
Some have raised concerns that the FAANG stocks may be in the midst of a bubble, whereas others argue that their growth is justified by the stellar financial and operational performance they have shown in recent years.
Each of the FAANG stocks trades on the Nasdaq exchange and is included in the S&P 500 Index. Since the S&P 500 is a broad representation of the market, the movement of the market mirrors the index's movement. As of August 2021, the FAANGs make up about 19% of the S&P 500—a staggering figure considering the S&P 500 is generally viewed as a proxy for the United States economy as a whole.
This large influence over the index means that volatility in the stock price of the FAANG stocks can have a substantial effect on the performance of the S&P 500 in general. In August 2018, for example, FAANG stocks were responsible for nearly 40% of the index’s gain from the lows reached in February 2018.
What Makes FAANG Stocks So Popular?
The five stocks that make up the “FAANG” acronym - Meta (FB), Amazon (AMZN), Apple (AAPL), Netflix (NFLX), and Alphabet (GOOG) are all well-known brands among consumers. But they are also famous for their remarkable growth in recent years, with market capitalizations ranging from $240 billion (in the case of Netflix)3 to $2.4 trillion (in the case of Apple), as of August 2021.
From an investment perspective, these five stocks are generally praised for their stellar historical track records and clear leadership positions within their industries.
Datset Information
The dataset consists of the historical stock prices of the FAANG companies. The dataset has been cleaned and uploaded for easy use and analysis.
Google Class A(GOOGL) consists of 4340 data records of stock prices starting from 20/08/2004 to date(11/11/2021)
Google Class C(GOOG) consists of 1924 records of GOOG stocks starting from 28/03/2004 to date.
Apple (AAPL) consists of 10319 records starting from 12/12/1980 to date
Amazon(AMZN) consists of 6167 records starting from 15/5/1997 to date
Meta(FB) consists of 2389 records from 18/05/2012 records to date
Netflix(NFLX) consists of 4904 records from 24/05/2002 to date
Note:
You might find two different types of google stocks GOOGL and GOOG in the dataset.
GOOGL
GOOGL shares are categorized as Class A shares. Class A shares are known as common shares. They give investors an ownership stake and, typically, voting rights. They are the most common type of shares.
GOOG
GOOG shares are the company's Class C shares. Class C shares give stockholders an ownership stake in the company, just like Class A shares, but unlike common shares, they do not confer voting rights to shareholders. 
Class A: Held by a regular investor with regular voting rights (GOOGL)
Class B: Held by the founders with 10 times the voting power compared to Class A
Class C: No voting rights, normally held by employees and some Class A stockholders (GOOG)
Inspiration
Annual growth rate of FAANG companies 
Forecast the growth rate of FAANG companies in the next ten years 
Exploratory data analysis"	260	2141	10	kaushiksuresh147	faang-fbamazonapplenetflixgoogle-stocks
776	776	Mortage and Rental Affordability	Housing & Demographics	['housing', 'business', 'real estate', 'social science']	"About this dataset
&gt; <h2>Additional Data Products</h2>
<p>Date: 2017 Q2</p>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Additional Data Products</h2>
<ul>
<li>Zillow Home Value Forecast (ZHVF): The ZHVF is the one-year forecast of the ZHVI. Our forecast methodology is <a href=""http://www.zillow.com/research/2013/01/24/zillow-home-value-forecast-methodology-2/"" target=""_blank"" rel=""nofollow"">methodology post</a>.</li>
<li>Zillow creates our negative equity data using our own data in conjunction with data received through our partnership with TransUnion, a leading credit bureau. We match estimated home values against actual outstanding home-related debt amounts provided by TransUnion. To read more about how we calculate our negative equity metrics, please see our <a href=""http://www.zillow.com/research/methodology-negative-equity-3180/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Cash Buyers: The share of homes in a given area purchased without financing/in cash. To read about how we calculate our cash buyer data, please see our <a href=""http://www.zillow.com/research/top-markets-for-cash-purchases-9696/"" target=""_blank"" rel=""nofollow"">research brief</a>.</li>
<li>Mortgage Affordability, Rental Affordability, Price-to-Income Ratio, Historical ZHVI, Historical ZHVI and Houshold Income are calculated as a part of Zillow’s quarterly Affordability Indices. To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment). Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. For quarters where median income is not available from the U.S. Census Bureau, we calculate future quarters of median household income by estimating it using the Bureau of Labor Statistics’ Employment Cost Index. The affordability forecast is calculated similarly to the current affordability index but uses the one year Zillow Home Value Forecast instead of the current Zillow Home Value Index and a specified interest rate in lieu of PMMS. It also assumes a 20 percent down payment. We calculate rent affordability similarly to mortgage affordability; however we use the Zillow Rent Index, which tracks the monthly median rent in particular geographical regions, to capture rental prices. Rents are chained back in time by using U.S. Census Bureau American Community Survey data from 2006 to the start of the Zillow Rent Index, and Decennial Census for all other years.</li>
<li>The mortgage rate series is the average mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate mortgage in 15-minute increments during business hours, 6:00 AM to 5:00 PM Pacific. It does not include quotes for jumbo loans, FHA loans, VA loans, loans with mortgage insurance or quotes to consumers with credit scores below 720. Federal holidays are excluded. The jumbo mortgage rate series is the average jumbo mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate, jumbo mortgage in one-hour increments during business hours, 6:00 AM to 5:00 PM Pacific Time. It does not include quotes to consumers with credit scores below 720. Traditional federal holidays and hours with insufficient sample sizes are excluded.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 1000 samples along with 1987 03, 1996 12, technical information and other features such as:
- 1990 12
- 1992 06
- and more.
How to use this dataset
&gt; - Analyze 1994 09 in relation to 2000 09
- Study the influence of 2008 03 on 1994 06
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	72	501	5	yamqwe	mortage-and-rental-affordability-price-to-incomee
777	777	Matic(Polygon) Cryptocurrency Historical Dataset	Historical Matic cryptocurrency data since Jun 2019	['finance', 'intermediate', 'exploratory data analysis', 'data visualization', 'time series analysis', 'investing', 'currencies and foreign exchange']	"Context
<img src=""https://miro.medium.com/max/561/1*hOf3Wsoor1Bm5Jl_4uMJ9w.png"">
Some background on Matic:
An ideal blockchain platform, Matic provides cheaper and lightning-fast transactions eliminating the complexities involved in the decentralized ecosystem. Matic has been created with the sole objective of a multifunctional and multipurpose advantage in all walks of life.
Matic Network was brought to life by CEO Jaynti Kanani, Sandeep Nailwal, and Anurag Arjun in October 2017 after Vitalik Buterin and Joseph Poon released a whitepaper on the Plasma framework. It was observed by the duo that Ethereum was not fully scalable and hence they coined the benefit of using PoS side chains connected to the root chain. Here each and every individual chain deals with its independent blockchain with its consensus mechanism, block validators and it can create more “child chains” of its own.
Content
The dataset consists of MAT(Matic) prices from June-2019 to the current date (625 days) and the dataset will be updated on a weekly basis. 
Information regarding the data
The data totally consists of 625 records(625 days) with 7 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | Date | Date of the MAT prices |
| 2 | Price | Prices of MAT(dollars) |
| 3 | Open | Opening price of MAT on the respective date(Dollars) |
| 4 | High | Highest price of MAT on the respective date(Dollars) |
| 5 | Low | Lowest price of MAT on the respective date(Dollars) |
| 6 | Vol. | Volume of MAT on the respective date(Dollars). |
| 7 | Change % | Percentage of Change in MAT prices on the respective date | |
Acknowledgements
The dataset was extracted from investing.com
Inspiration
Going by Wallet Investor’s Matic price prediction for 2020-2025, “Matic is an excellent long term instrument for investment. The price of Matic can go up from 0.0156 USD to 0.0236 USD in one year. Matic also shows a long-term earning potential is +51.05% in one year. The future price of Matic will surely be 0.0572 USD. Matic is set to have a bullish cycle and earn a profit for its investors. It is highly recommended as a virtual currency.” 
Do you believe it? Well, let's find it by building our own creative models to predict if the statement is true."	194	2071	16	kaushiksuresh147	maticpolygon-crytocurrency-historical-dataset
778	778	Ethereum Cryptocurrency Historical Dataset 	Historical Ethereum data from the start(2016) to 2021	['finance', 'exploratory data analysis', 'data visualization', 'time series analysis', 'currencies and foreign exchange']	"<img src=""https://www.bernardmarr.com/img/What%20Is%20The%20Difference%20Between%20Bitcoin%20and%20Ethereum.png"">
Context
Ethereum a decentralized, open-source blockchain featuring smart contract functionality was proposed in 2013 by programmer Vitalik Buterin. Development was crowdfunded in 2014, and the network went live on 30 July 2015, with 72 million coins premined. 
Some interesting facts about Ethereum(ETH):
- Ether (ETH) is the native cryptocurrency of the platform. It is the second-largest cryptocurrency by market capitalization, after Bitcoin. Ethereum is the most actively used blockchain.
- Some of the world’s leading corporations joined the EEA(Ethereum Alliance, is a collaboration of many block start-ups) and supported “further development.” Some of the most famous companies are Samsung SDS, Toyota Research Institute, Banco Santander, Microsoft, J.P.Morgan, Merck GaA, Intel, Deloitte, DTCC, ING, Accenture, Consensys, Bank of Canada, and BNY Mellon.
Content
The dataset consists of ETH prices from March-2016 to the current date(1830days) and the dataset will be updated on a weekly basis. 
Information regarding the data
The data totally consists of 1813 records(1813 days) with 7 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | Date | Date of the ETH prices |
| 2 | Price | Prices of ETH(dollars) |
| 3 | Open | Opening price of ETH on the respective date(Dollars) |
| 4 | High | Highest price of ETH on the respective date(Dollars) |
| 5 | Low | Lowest price of ETH on the respective date(Dollars) |
| 6 | Vol. | Volume of ETH on the respective date(Dollars). |
| 7 | Change % | Percentage of Change in ETH prices on the respective date | |
Acknowledgements
The dataset was extracted from investing.com
Inspiration
Experts say that ethereum has a huge potential in the future. Do you believe it? Well, let's find it by building our own creative models to predict if the statement is true."	675	5275	25	kaushiksuresh147	ethereum-cryptocurrency-historical-dataset
779	779	Coca Cola Stock - Live and Updated	Performance of Coca Cola Stock from 1962 to today 	['business', 'finance', 'economics', 'retail and shopping', 'investing']	"The Coca-Cola Company is an North American multinational beverage corporation incorporated under Delaware's General Corporation Law[a] and headquartered in Atlanta, Georgia. The Coca-Cola Company has interests in the manufacturing, retailing, and marketing of non-alcoholic beverage concentrates and syrups, and alcoholic beverages. The company produces Coca-Cola, the sugary drink for which it is best known for, invented in 1886 by pharmacist John Stith Pemberton. At the time, the product was made with coca leaves, which added an amount of cocaine to the drink, and with kola nuts, which added caffeine, so that the coca and the kola together provided a stimulative effect. This stimulative effect is the reason the drink was sold to the public as a healthy ""tonic"", and the coca and the kola are also the source of the name of the product and of the company.In 1889, the formula and brand were sold for $2,300 (roughly $68,000 in 2021) to Asa Griggs Candler, who incorporated The Coca-Cola Company in Atlanta in 1892.
Since 1919, Coca-Cola has been a publicly traded company. Its stock is listed on the New York Stock Exchange under the ticker symbol ""KO"". One share of stock purchased in 1919 for $40, with all dividends reinvested, would have been worth $9.8 million in 2012, a 10.7% annual increase adjusted for inflation. A predecessor bank of SunTrust received $100,000 for underwriting Coca-Cola's 1919 public offering; the bank sold that stock for over $2 billion in 2012. In 1987, Coca-Cola once again became one of the 30 stocks which makes up the Dow Jones Industrial Average, which is commonly referenced as a proxy for stock market performance; it had previously been a Dow stock from 1932 to 1935. Coca-Cola has paid a dividend since 1920 and, as of 2019, had increased it each year for 57 years straight."	685	4157	30	kalilurrahman	coca-cola-stock-live-and-updated
780	780	clean_datap		[]		0	2	0	wallacefqq	clean-datap
781	781	Latest Covid-19 Cases Maharashtra, India	District wise Covid Data as on February 11, 2022	['india', 'beginner', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
District-wise  Covid-19 data of Maharashtra, a state in India as on February 11, 2022. 
The data include number of positive cases, active cases, recovered, deceased cases, recovery rate and fatality rate.
Attribute Information
Cumulative Cases by Districts
Districts - Name of districts in Maharashtra, India
Positive Cases - Number of positive cases
Active Cases - Number of active cases
Recovered - Number of recovered cases
Deceased - Number of deaths
Recovery Rate (%) - Ratio of number of recovered cases to positive cases
Fatality Rate (%) - Ratio of number of deaths to positive cases
Source
Link : https://www.covid19maharashtragov.in/mh-covid/dashboard
Other Updated Covid19 Datasets
Link : https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting 👍 
Thank You"	370	1387	48	anandhuh	latest-covid19-cases-maharashtra-india
782	782	Famous Quotes	Dataset consisting on list of Famous author's quptes and likes given	['literature']		2	8	1	iampunitkmryh	funny-quotes
783	783	Abstracts of 10,000 Covid Research Papers	Title, Abstract and URL of Covid Research Papers	['global', 'education', 'beginner', 'nlp', 'covid19']	"Content
Title, Abstract and URL of 10,000 Covid Research Papers
Attribute Information
title - Title of research paper
abstract - Abstract of the research paper
url - URL of the research paper
Source
This data was collected from PubMed using Python code. 
PubMed is a free resource supporting the search and retrieval of biomedical and life sciences literature
Other Updated Covid Datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	25	391	17	anandhuh	covid-abstracts
784	784	TED Talks	TED Talks  data upto Feb 2022	['art', 'business', 'text data']	"Context
This is a collection of TED Talks data up to Feb 2022
Content
I fetched the data using TED GraphQL API and Beautfulsoap"	5	92	9	derrickmwiti	ted-talks
785	785	Amazon reviews		[]		0	10	0	prathameshkashid	amazon-reviews
786	786	Restaurant Reviews		[]		0	7	0	ahmedaliomar	restaurant-reviews
787	787	Happywhale tfrec 512 max		[]		3	3	0	thomasbrandon	happywhale-tfrec-512-max
788	788	hornet_detector_model	Asian Hornet detector model with a Swin Transformer backbone	[]		0	6	0	johanmoncouti	hornet-detector-model
789	789	Happywhale: Cropped Dataset [YOLOv5] ds		[]		19	113	6	awsaf49	happywhale-cropped-dataset-yolov5-ds
790	790	Classification for Beginners	predict the class based on given X 	['classification', 'deep learning', 'multiclass classification']	"some non-linearly separable spiral data.
source :  here"	2	21	5	vardhansiramdasu	classification-for-beginners
791	791	Multilingual Trail and Test Dataset		[]		5	5	0	amarendradeo	multilingual-custom-dataset
792	792	housecsv		[]		0	2	0	sandychen996	housecsv
793	793	KTH_TIPS		[]		0	2	0	thanakornchaisen	kth-tips
794	794	DATA_3s		[]		0	4	1	mithiljoshi	data-3s
795	795	Ambition Box Companies Dataset		['business']		0	7	2	riteshpanhalkar	ambition-box-companies-dataset
796	796	fastrcnn2000		[]		0	10	0	toongzhhang	fastrcnn2000
797	797	multilingual-en-hi-custom-dataset		[]		4	1	0	amarendradeo	multilingual-en-hi-custom-dataset
798	798	airplan		[]		0	1	0	bh000111	airplan
799	799	Happywhale: BoundingBox [YOLOv5] Dataset		[]		4	17	0	awsaf49	happywhale-boundingbox-yolov5-dataset
800	800	tfbrdat12		[]		1	18	0	v1olet	tfbrdat12
801	801	Ambulance Audio Dataset		[]		0	2	0	tabarkarajab	ambulance-audio-dataset
802	802	Tobacco use legislative data on by state	State-level legislative data on tobacco use prevention and control policies	['public health', 'health']	"About this dataset
&gt; <p>1995-2016. Centers for Disease Control and Prevention (CDC). State Tobacco Activities Tracking and Evaluation (STATE) System. E-Cigarette LegislationâPreemption. The STATE System houses current and historical state-level legislative data on tobacco use prevention and control policies. Data are reported on a quarterly basis. Data include information related to statutory state preemption of more stringent local laws on advertising, smokefree indoor air, youth access and licensure.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/cdc-state-system-e-cigarette-legislation-preemption-0ece7"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/cdc-state-system-e-cigarette-legislation-preemption-0ece7</a></p>
This dataset was created by Health and contains around 80000 samples along with Effective Date, Measure Id, technical information and other features such as:
- Provision Alt Value
- Quarter
- and more.
How to use this dataset
&gt; - Analyze Provision Desc in relation to Location Desc
- Study the influence of Provision Group Id on Location Abbr
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	44	443	5	yamqwe	cdc-state-system-preemptione
803	803	sentence-transformers setup		['nlp']	"Source Code
This is sentence-transformers source code from GitHub repo. (https://github.com/UKPLab/sentence-transformers)
Date: 2022.2.11
Version: commit 65922ad07bcd152aed3b5b981bda8218946cf674
License: Apache-2.0 License"	0	2	0	cccwher	sentencetransformers-setup
804	804	tgbrrun		[]		0	32	0	v1olet	tgbrrun
805	805	JOB-A-THON - February 2022		[]	"Problem Statement
ABC is an online content sharing platform that enables users to create, upload and share the content in the form of videos. It includes videos from different genres like entertainment, education, sports, technology and so on. The maximum duration of video is 10 minutes.
Users can like, comment and share the videos on the platform. 
Based on the user’s interaction with the videos, engagement score is assigned to the video with respect to each user. Engagement score defines how engaging the content of the video is. 
Understanding the engagement score of the video improves the user’s interaction with the platform. It defines the type of content that is appealing to the user and engages the larger audience.
Objective
The main objective of the problem is to develop the machine learning approach to predict the engagement score of the video on the user level.
Data Dictionary
You are provided with 3 files - train.csv, test.csv and sample_submission.csv"	4	112	0	gopidurgaprasad	jobathon-february-2022
806	806	Healthcare Spending as Percentage of GDP		['news']		0	3	0	cansinacarer	healthcare-spending-as-percentage-of-gdp
807	807	OECD: How many use broadband connection by Country	Dataset of business use of broadband as a measure of country infrastructure	['business', 'internet']	"About this dataset
&gt; <p>This indicator provides information on how many businesses use a broadband connection. Data typically come from surveys/questionnaires given to a sample subset of businesses. The results are then extrapolated for the country as a whole. The drawbacks of the survey data are that it is collected infrequently and that the questions asked about broadband usage are not necessarily uniform across OECD countries. This indicator is measured in percentage of all businesses and is available by business size or by business sector.</p>
<h3>Citation</h3>
<pre><code>OECD (2017), Business use of broadband (indicator). 
doi: 10.1787/6e2e8a67-en (Accessed on 30 August 2017)
</code></pre>
<p><strong><em>Source:</em></strong> <a href=""https://data.oecd.org/broadband/business-use-of-broadband.htm"" target=""_blank"" rel=""nofollow"">https://data.oecd.org/broadband/business-use-of-broadband.htm</a></p>
This dataset was created by OECD and contains around 700 samples along with Flag Codes, Time, technical information and other features such as:
- Frequency
- Value
- and more.
How to use this dataset
&gt; - Analyze Subject in relation to Indicator
- Study the influence of Measure on Location
- More datasets
Acknowledgements
If you use this dataset in your research, please credit OECD 
Start A New Notebook!"	21	188	6	yamqwe	business-use-of-broadbande
808	808	Precipitation_Test_Kaggle		[]		0	3	0	venkatkumar001	precipitation-test-kaggle
809	809	Creativity in Vine MicroVideos	How vine was killing off its six-second loop service	['arts and entertainment', 'art', 'internet', 'online communities', 'social networks']	"Note: This dataset is a bit outdated: It is uploaded with the original description
About this dataset
&gt; <p>Like the creative writing that Twitter's 140 character limit made possible, Vine's 6 second limit helped conceive a <a href=""https://www.technologyreview.com/s/532806/yahoo-labs-algorithm-identifies-creativity-in-6-second-vine-videos/"" target=""_blank"">new 'genre' of filmmaking</a>. Using crowdsourcing and algorithms, scientist at Yahoo Labs published research suggesting machines can distinguish between creative and non-creative content (most of the time). Their source for the creative and not so creative works was Vine. This dataset is composed of the videos that Miriam Redi et al. used in the 2014 study. Many of the videos are still up. Sadly, Vine will be shutting down soon. This research can serve as an homage to Vine that helped millenials across the web creative and inspiring researchers to explore the medium of 6 second art.</p>
<p><img src=""https://i.imgur.com/2RMMttt.gif"" alt=""Vine video""><br>
<em>Source: <a href=""https://vine.co/v/b3eZWJWBImT"" target=""_blank"">https://vine.co/v/b3eZWJWBImT</a></em></p>
<blockquote>
<p>Why did people love Vine?<br>
I can’t tell you; I’ll have to show you. Just know this: Vines turned nonsense into short bursts of hilarious art.</p>
</blockquote>
<p><code>In-the-News</code>:</p>
<ul>
<li><em>New York Times</em>: <a href=""http://www.nytimes.com/2016/10/28/technology/vine-is-closing-down-and-the-internet-cant-stand-it.html"" target=""_blank"">Vine Is Closing Down, and the Internet Can’t Stand It<br>
</a></li>
<li><em>Popular Mechanics</em>: <a href=""http://www.popularmechanics.com/technology/apps/news/a23576/vine-is-dead/"" target=""_blank"">Vine Is Dead, Twitter is killing off its six-second loop service.</a></li>
<li><em>Slate</em>: <a href=""http://www.slate.com/blogs/browbeat/2016/10/27/vine_is_shutting_down_and_the_internet_will_be_a_lesser_places_without_it.html"" target=""_blank"">All the Proof You Need That Vine Made the Internet a Better Place</a></li>
<li><em>The Creators Project</em>: <a href=""http://thecreatorsproject.vice.com/en_au/blog/can-an-algorithm-recognize-creativity"" target=""_blank"">Can an Algorithm Recognize Creativity?</a></li>
<li><em>MIT Technology Review</em>: <a href=""https://www.technologyreview.com/s/532806/yahoo-labs-algorithm-identifies-creativity-in-6-second-vine-videos/"" target=""_blank"">Yahoo Labs' Algorithm Identifies Creativity in 6-Second Vine Videos</a></li>
</ul>
<p><img src=""https://i.imgur.com/xRHgLfJ.png"" alt=""alt text""><br>
<em>Source: <a href=""http://www.micheletrevisiol.com/papers/cvpr2014_redi.pdf"" target=""_blank"">6 Seconds of Sound and Vision</a></em></p>
<p>The notion of creativity, as opposed to related concepts such as beauty or interestingness, has not been studied from the perspective of automatic analysis of multimedia content. Meanwhile, short online videos shared on social media platforms, or micro-videos, have arisen as a new medium for creative expression. In this paper we study creative micro-videos in an effort to understand the features that make a video creative, and to address the problem of automatic detection of creative content. Defining creative videos as those that are novel and have aesthetic value, we conduct a crowdsourcing experiment to create a dataset of 4,000 micro-videos labelled as creative and non-creative. We propose a set of computational features that we map to the components of our definition of creativity, and conduct an analysis to determine which of these features correlate most with creative video. Finally, we evaluate a supervised approach to automatically detect creative video, with promising results, showing that it is necessary to model both aesthetic value and novelty to achieve optimal classification accuracy.</p>
<p><strong>Dataset</strong><br>
Data used in the article <a href=""http://www.micheletrevisiol.com/papers/cvpr2014_redi.pdf"" target=""_blank"">6 Seconds of Sound and Vision: Creativity in Micro-Videos</a> published for the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
<p>The dataset is composed by the set of vine videos with the corresponding creative/non-creative annotations. The list of videos is partitioned according to the agreement score between the annotators. The name of the files is formatted as follows:</p>
<p><strong>annotated_videos_&lt;datasetid&gt;.txt</strong></p>
<p>where &lt;datasetid&gt; can be:</p>
<ul>
<li>D_60: videos with at least 60% agreement</li>
<li>D_80: videos with at least 80% agreement</li>
<li>D_100: videos with full agreement</li>
</ul>
<p>For example the file <strong>annotated_videos_D_80.txt</strong> contains the set of the annotated videos that show at least a 80% agreement between annotators.</p>
<p>In those files each line represents an annotation with this format (values are tab separated):</p>
<p>&lt;annotation&gt; &lt;videoid&gt;</p>
<p>where &lt;annotation&gt; assumes the value 0 for non-creative and 1 for creative videos and &lt;videoid&gt; is the unique Vine identifier. To compose the URL of the video available in the Vine web portal follow this scheme:</p>
<p><a href=""https://vine.co/v/"" target=""_blank"">https://vine.co/v/</a>&lt;videoid&gt;*</p>
<p><em>*data.world has added a vine_url column in the datasets</em></p>
<p>We provide also the composition of the training and test sets for the three dataset D_60, D_80, and D_100. The files are named as follows:</p>
<p>&lt;partitionid&gt;_&lt;datasetid&gt;.txt</p>
<p>where &lt;partitionid&gt; can be train or test if it refers to, respectively, the training or test set for a given dataset. The format of each line is the same of the annotated_videos_&lt;datasetid&gt;.txt files.</p>
<p>Refer to the <a href=""http://www.di.unito.it/~schifane/dataset/vine-dataset-cvpr14/"" target=""_blank"">paper</a> for additional details on the annotation experiment.</p>
<p>If you are going to use the dataset for your research, the authors have requested you cite this paper as:</p>
<p><code>@inproceedings{redi:creativity:cvpr14,</code><br>
<code>author = {Redi, Miriam and O'Hare, Neil and Schifanella, Rossano and Trevisiol, Michele and Jaimes, Alejandro},</code><br>
<code>title = {6 Seconds of Sound and Vision: Creativity in Micro-Videos},</code><br>
<code>booktitle = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},</code><br>
<code>year = {2014},</code><br>
<code>location = {Columbus, Ohio, USA},</code><br>
<code>isbn = {978-1-4799-5118-5},</code><br>
<code>pages = {4272--4279},</code><br>
<code>numpages = {8},</code><br>
<code>url = {http://dx.doi.org/10.1109/CVPR.2014.544},</code><br>
<code>doi = {10.1109/CVPR.2014.544},</code><br>
<code>publisher = {IEEE Computer Society}</code><br>
<code>}</code></p>
<p><em>We decided to make available to the scientific community the dataset we collected for the study in order to guarantee the reproducibility of the results and support future works in the field of computational creavity.</em></p>
<p><em>The dataset doesn't contain any personal information about annotators or creators of the videos, but it exposes only the public identifiers of the videos in the Vine system and the corresponding annotations in an aggregated form.</em></p>
<p><strong>Contact</strong><br>
For any information about the dataset, the paper or the research we are carrying on, please, contact the authors:</p>
<p><a href=""redi@yahoo-inc.com"" target=""_blank"">Miriam Redi</a><br>
<a href=""nohare@yahoo-inc.com"" target=""_blank"">Neil O'Hare</a><br>
<a href=""schifane@di.unito.it"" target=""_blank"">Rossano Schifanella</a><br>
<a href=""trevisiol@acm.org"" target=""_blank"">Michele Trevisiol</a><br>
<a href=""ajaimes@yahoo-inc.com"" target=""_blank"">Alejandro Jaimes</a></p>
<p>“We value you, your Vines, and are going to do this the right way,” the company wrote on Medium. “You’ll be able to access and download your Vines.” - Vine</p>
This dataset was created by Social Media Data and contains around 2000 samples along with Creative 1 Noncreative 0, Creative 1 Noncreative 0, technical information and other features such as:
- Creative 1 Noncreative 0
- Creative 1 Noncreative 0
- and more.
How to use this dataset
&gt; - Analyze Creative 1 Noncreative 0 in relation to Creative 1 Noncreative 0
- Study the influence of Creative 1 Noncreative 0 on Creative 1 Noncreative 0
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Social Media Data 
Start A New Notebook!"	1	101	4	yamqwe	creativity-in-vine-microvideose
810	810	UK Road Safety Dataset 1979 - 2020	Road Traffic Accident Dat from UK Gov	['europe', 'automobiles and vehicles']	"Dataset from data.gov.uk
https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data
Data License
https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/
A comprehensive dataset of all of the road traffic accidents recorded by the British police"	37	558	1	keithcooper	uk-road-safety-dataset-1979-2020
811	811	2021 -What movies to watch today?	Movies list from imdb website updated weekly	['movies and tv shows']	"Context
With Covid in place,when we sit to pick movies to watch as a family, we end up browsing for about 40 + minutes pondering through what movies to watch as a family with kids. THen I realised why not access the database of movies, use my knowledge in R to bring out something useful for folks so that they can use this link to pick their favourite movies to watch per the genre.
Content
I have downloaded this data set from ""https://www.imdb.com/interfaces/"" -This link is linked here and will be updated/refreshed weekly.
Acknowledgements
Thanks to imdb website folks for making this data public
Subsets of IMDb data are available for access to customers for personal and non-commercial use. You can hold local copies of this data, and it is subject to our terms and conditions. Please refer to the Non-Commercial Licensing and copyright/license and verify compliance.
Inspiration
With this data, I want to bring out answers to common questions 
1) WHat movies can I watch as a family under science fiction , horror , doggy movies or christmas movies ?
2) AS I analyse the data, I would want to ultimately make a shiny App page showcasing this for folks to use and benefit."	31	832	4	gayathrirprog	2021-what-movies-to-watch-today
812	812	US Vaccination Progress	State-by-State Vaccination Progress	['united states', 'public health', 'tabular data', 'public safety', 'covid19']	"Data collected from Our World in Data's github repository which collects US vaccination data from the CDC. Includes vaccination progress of the US as a whole.
Description
location: name of the state or federal entity.
date: date of the observation.
total_vaccinations: total number of doses administered. This is counted as a single dose, and may not equal the total number of people vaccinated, depending on the specific dose regime (e.g. people receive multiple doses). If a person receives one dose of the vaccine, this metric goes up by 1. If they receive a second dose, it goes up by 1 again.
total_vaccinations_per_hundred: total_vaccinations per 100 people in the total population of the state.
daily_vaccinations_raw: daily change in the total number of doses administered. It is only calculated for consecutive days. This is a raw measure provided for data checks and transparency, but we strongly recommend that any analysis on daily vaccination rates be conducted using daily_vaccinations instead.
daily_vaccinations: new doses administered per day (7-day smoothed). For countries that don't report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found here.
daily_vaccinations_per_million: daily_vaccinations per 1,000,000 people in the total population of the state.
people_vaccinated: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.
people_vaccinated_per_hundred: people_vaccinated per 100 people in the total population of the state.
people_fully_vaccinated: total number of people who received all doses prescribed by the vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.
people_fully_vaccinated_per_hundred: people_fully_vaccinated per 100 people in the total population of the state.
total_distributed: cumulative counts of COVID-19 vaccine doses recorded as shipped in CDC's Vaccine Tracking System.
total_distributed_per_hundred: cumulative counts of COVID-19 vaccine doses recorded as shipped in CDC's Vaccine Tracking System per 100 people in the total population of the state.
share_doses_used: share of vaccination doses administered among those recorded as shipped in CDC's Vaccine Tracking System.
Acknowledgements
I'm not involved in the collecting and creation of this data; all credit goes to Our World in Data.
Questions
Which states lead in vaccinating its residents?
When will the US have a fully vaccinated population based on current rates?"	456	3427	21	bumjunkoo	us-vaccination-progress
813	813	One Year Mortgage Loan Prediction for Zillow Homes	ZHVI: The One Year Loan Prediction of Mortgage	['housing', 'business', 'real estate']	"About this dataset
&gt; <h2>Additional Data Products</h2>
<p>Product: <a href=""http://www.zillow.com/research/zillow-home-value-forecast-methodology-2-3740/"" target=""_blank"" rel=""nofollow"">Zillow Home Value Forecast</a></p>
<p>Date: August 2017</p>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Additional Data Products</h2>
<ul>
<li>Zillow Home Value Forecast (ZHVF): The ZHVF is the one-year forecast of the ZHVI. Our forecast methodology is <a href=""http://www.zillow.com/research/2013/01/24/zillow-home-value-forecast-methodology-2/"" target=""_blank"" rel=""nofollow"">methodology post</a>.</li>
<li>Zillow creates our negative equity data using our own data in conjunction with data received through our partnership with TransUnion, a leading credit bureau. We match estimated home values against actual outstanding home-related debt amounts provided by TransUnion. To read more about how we calculate our negative equity metrics, please see our <a href=""http://www.zillow.com/research/methodology-negative-equity-3180/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Cash Buyers: The share of homes in a given area purchased without financing/in cash. To read about how we calculate our cash buyer data, please see our <a href=""http://www.zillow.com/research/top-markets-for-cash-purchases-9696/"" target=""_blank"" rel=""nofollow"">research brief</a>.</li>
<li>Mortgage Affordability, Rental Affordability, Price-to-Income Ratio, Historical ZHVI, Historical ZHVI and Houshold Income are calculated as a part of Zillow’s quarterly Affordability Indices. To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment). Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. For quarters where median income is not available from the U.S. Census Bureau, we calculate future quarters of median household income by estimating it using the Bureau of Labor Statistics’ Employment Cost Index. The affordability forecast is calculated similarly to the current affordability index but uses the one year Zillow Home Value Forecast instead of the current Zillow Home Value Index and a specified interest rate in lieu of PMMS. It also assumes a 20 percent down payment. We calculate rent affordability similarly to mortgage affordability; however we use the Zillow Rent Index, which tracks the monthly median rent in particular geographical regions, to capture rental prices. Rents are chained back in time by using U.S. Census Bureau American Community Survey data from 2006 to the start of the Zillow Rent Index, and Decennial Census for all other years.</li>
<li>The mortgage rate series is the average mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate mortgage in 15-minute increments during business hours, 6:00 AM to 5:00 PM Pacific. It does not include quotes for jumbo loans, FHA loans, VA loans, loans with mortgage insurance or quotes to consumers with credit scores below 720. Federal holidays are excluded. The jumbo mortgage rate series is the average jumbo mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate, jumbo mortgage in one-hour increments during business hours, 6:00 AM to 5:00 PM Pacific Time. It does not include quotes to consumers with credit scores below 720. Traditional federal holidays and hours with insufficient sample sizes are excluded.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 20000 samples along with City Name, Msa Name, technical information and other features such as:
- State
- Forecast Yo Y Pct Change
- and more.
How to use this dataset
&gt; - Analyze County Name in relation to Region
- Study the influence of Region Name on City Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	10	150	2	yamqwe	zillow-home-value-forecaste
814	814	faster_rcnn		[]		1	8	0	manyuli	faster-rcnn
815	815	ensemble_boxes_1.0.4nkh		[]		0	4	0	sakura1986	ensemble-boxes-104nkh
816	816	hello1113		[]		0	0	0	iftiben10	hello1113
817	817	japan-trade-statistics3	Japan trade statistics Latest data	['business', 'education', 'investing']		10	688	0	zanjibar	japantradestatistics3
818	818	kaggleinput00		[]		0	4	0	caviuna	kaggleinput00
819	819	Daily Covid19 Hospitalisation Data	COVID-19 hospitalizations and intensive care (ICU) 	['public health', 'health', 'hospitals and treatment centers', 'covid19']	"Daily Covid19 Hospitalization Data by Region
Fields
| Column field | Description                                                                  |
|--------------|------------------------------------------------------------------------------|
| entity     | Name of the country (or region within a country)                            |
| iso_code   | ISO 3166-1 alpha-3 – three-letter country code |
| date       | Date of the observation                                                     |
| indicator  | Indicator name. See below our list of indicators and their definition |
| value      | Value of the indicator |
Indicators
| Indicator name | Description |
|----------------|-------------|
| Daily hospital occupancy                      | Number of COVID-19 patients in hospital on a given day |
| Daily hospital occupancy per million          | Daily hospital occupancy per million people |
| Daily ICU occupancy                           | Number of COVID-19 patients in ICU on a given day |
| Daily ICU occupancy per million               | Daily ICU occupancy per million people |
| Weekly new hospital admissions                 | Number of COVID-19 patients newly admitted to hospitals in a given week |
| Weekly new hospital admissions per million     | Weekly new hospital admissions per million people |
| Weekly new ICU admissions                      | Number of COVID-19 patients newly admitted to ICU in a given week |
| Weekly new ICU admissions per million          | Weekly new ICU admissions per million people |
Ackwnowledgements
All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license."	473	3290	36	shivamb	daily-covid19-hospitalisation-data
820	820	pretrain model vanilla loss		[]		0	22	0	rujengelal	pretrain-model-vanilla-loss
821	821	happywhale-tfrecords-holdout-v1		[]		0	2	0	hwigeon	happywhale-tfrecords-holdout-v1
822	822	United HealthCare Stock data	United HealthCare Stock data from IPO Days	['business', 'finance', 'tabular data', 'insurance', 'investing']	"UHG is a very big market cap player. 
UnitedHealth Group Incorporated is an American multinational managed healthcare and insurance company based in Minnetonka, Minnesota. It offers health care products and insurance services. UnitedHealth Group is the world's eighth-largest company by revenue and second-largest healthcare company behind CVS Health by revenue, and the largest insurance company by net premiums. UnitedHealthcare revenues comprise 80% of the Group's overall revenue
The company is ranked 8th on the 2021 Fortune Global 500. UnitedHealth Group has a market capitalization of $400.7 billion as of March 31, 2021."	38	326	9	kalilurrahman	united-healthcare-stock-data
823	823	feed_longlarge_5fold_220211		[]		0	1	0	sskkaz	feed-longlarge-5fold-220211
824	824	Animal Crossing Villager Sprites	"Villager Sprites in Front Facing and ""Profile"" Modes"	['video games']		0	5	1	csalexi	animal-crossing-sprites
825	825	3)training		['education']		1	85	0	yiflee	3training
826	826	2)training		['education']		5	131	0	yiflee	2training
827	827	1)training		['education']		4	89	0	yiflee	1training
828	828	Grapes classification		['food']		0	11	0	rifandrid	grapesfruit
829	829	Region wise deployment of ATMs September 2021	Region wise deployment of ATMs for the quarter ended September 2021 (Revised)	['banking']		1	5	1	riddhinagadia	region-wise-deployment-of-atms-september-2021
830	830	model_font		[]		4	11	0	phumiphatc	model-font
831	831	Gem index_stuck		[]		0	1	0	wallacefqq	gem-index-stuck
832	832	Fun Fact from Twitter	"Tweet about ""Fun Fact"""	['text data', 'social networks']	"Context
""Fun Fact"" is a magic word that attracts readers to carefully listen to the entire text written there. Our eyes will automatically stop when we see the words ""Fun Fact"". Is this means that there is a psychological tendency that the words ""Fun Fact"" can change a person's habits? This is interesting to be analyzed.
Content
This dataset contains data from Twitter for each tweet that has the phrase ""Fun Fact"" in its content. Taken from English and will continue to be updated to find the data at any time. 
- Keyword ""fun fact"" lang:en
- Original tweet, reply, and quoted tweet included
- Retweet excluded
Acknowledgements
Thank you Twitter for allowing us to gain our abilities through your Tweet dataset. I'm using Twitter API v2 to retrieve this data, it's a very handy and useful tool. 
Inspiration
From this data, it is hoped that it can be used to help human life, both in terms of information, education, and entertainment."	60	834	11	linkgish	fun-fact-from-twitter
833	833	CATL_stuck_宁德时代		[]		0	2	0	wallacefqq	catl-stuck
834	834	Indonesian Tweet About Terrorism (Teroris)	"Indonesian Tweet About ""teroris"" (English: Terrorism)"	['politics', 'social issues and advocacy']	"Context
The issue of terrorism has always been a hot topic of discussion among Indonesian netizens. In recent days, this discussion has strengthened because there are issues with several related institutions. Terrorists are something that is hated by everyone, disturbing the security and comfort of society. There is nothing good to be gained from an act of terrorism and everyone agrees on that. But when this is linked to a certain entity, it becomes a question mark, is it true? Or is it just an opinion drive?
Content
This dataset is raw data about tweets pulled from twitter with the keyword terrorist or terrorist in Indonesian.
- Keyword used: teroris lang:id
- Original tweet, reply, quote tweet included 
- Retweet excluded
Acknowledgements
Big thanks to Twitter for API v2
Inspiration
It is hoped that this dataset can be used as an in-depth analysis of terrorism in Indonesia. There are various types of conspiracies that occur in terrorism in Indonesia, this is very interesting to find out more deeply and analyze."	105	1097	12	linkgish	indonesian-tweet-about-teroris-terrorism
835	835	mnist-csv		['computer science']		0	4	0	chengchuan1024	mnist-csv
836	836	amazonreview		[]		5	8	0	zmjjiang	amazonreview
837	837	vgg16 model		['clothing and accessories']		9	42	0	ajaybu	vgg16-model
838	838	wage2015		[]		0	2	0	farisalwohaibi	wage2015
839	839	USCPS2015		[]		0	3	0	farisalwohaibi	uscps2015
840	840	Exploratory Data Analysis  | EDA - use case 	Exploratory Data Analysis  | EDA - use case 	['categorical data', 'intermediate', 'tabular data', 'datetime']		0	21	6	mohinurabdurahimova	exploratory-data-analysis-eda-use-case
841	841	General detection set for RMRR	Mainly critters and other things that pass by my cams.	['business']		0	8	0	avatar42	rmrr-model
842	842	DeFi Dataset from DefiLlama	Explore the world of Decentralized Finance	['business', 'finance', 'tabular data', 'currencies and foreign exchange']	"Context
Decentralized Finance (DeFi) is gaining momentum day by day and a number of Defi protocols are coming up everyday. DefiLlama does a great job of capturing all the data related to Defi and open sourcing the same for us to analyze and use. 
Content
Details about different defi protocols, the chains they belong to, total value locked etc are present in this dataset at daily basis.
Acknowledgements
DefiLlama is the one who made this dataset and I am just publishing it here for ease of analysis.
Photo Credits: Photo by <a href=""https://unsplash.com/@theshubhamdhage?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Shubham Dhage</a> on <a href=""https://unsplash.com/s/photos/decentralized?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>
Inspiration
Some questions are
1. Which DeFi protocols are gaining momentum?
2. Which chains are gaining prominence in the DeFi space?
3. Change in TVL value over time at protocol / chain level?"	54	1390	21	sudalairajkumar	defi-dataset-from-defillama
843	843	TGBR yolov5 models		['clothing and accessories']		41	92	0	gmhost	tgbr-yolov5-models
844	844	Skin Disease Dataset [Testing]		['health']		7	9	0	rajkkapadia	skin-disease-dataset-testing
845	845	Art As Dots	Visualize Art.... Literally	['arts and entertainment', 'art']	"About This Dataset
This dataset is all the works currently on view at the Museum of Modern Art reimagined as a set of X, Y coordinates that can be plotted. See the gif below for an example. 
If you're interested in playing around with this yourself, all you need to do is plot the X and Y columns and group by the URL. 
How to Do this
If you're interested in creating a scatterplot image, check out my GitHub! 
Tableau Visualization
If you want to see what this dataset was used for/ how all these different scatterplots work, check out this tableau dashboard!"	6	136	6	jackogozaly	art-as-dots
846	846	TrainData10x624991		[]		0	0	0	bstnst99	traindata10x624991
847	847	Train705970 and Weight		[]		0	1	0	bstnst99	train705970-and-weight
848	848	feedback_newtrain		[]		0	4	0	rookie0ne	feedback-newtrain
849	849	Class data		['education']		0	9	0	salimmouline	class-data
850	850	Cat Sound Classification Dataset		['audio data']		6	11	0	yagtapandeya	cat-sound-classification-dataset
851	851	Fitabase Data 4.12.16-5.12.16		[]		0	0	0	amandamcgraw	fitabase-data-4121651216
852	852	project0		[]		2	46	0	jumiarliu	project0
853	853	Playlist2		[]		0	2	0	amauriramirez	playlist2
854	854	Human Trafficking Dataset	Dataset from the National Uniform Crime Reporting about Human Trafficking	['crime', 'law', 'public safety', 'social issues and advocacy', 'human rights']	"About this dataset
&gt; <p>Human Trafficking, 2016, marks the fourth report from the national Uniform Crime Reporting (UCR) Program’s Human Trafficking data collection. As state participation has grown, the UCR Program has seen an increase in data submissions. The program will continue efforts to expand, gather, and make available information regarding human trafficking incidents.</p>
<h3>Trafficking Victims Protection Act</h3>
<p>In January 2013, the national UCR Program began collecting offense and arrest data regarding human trafficking as authorized by the William Wilberforce Trafficking Victims Protection Reauthorization Act of 2008. The act requires the FBI to collect human trafficking offense data and to make distinctions between prostitution, assisting or promoting prostitution, and purchasing prostitution.</p>
<p>To comply with the Wilberforce Act, the national UCR Program created two additional offenses in the Summary Reporting System (SRS) and the National Incident-Based Reporting System (NIBRS) through which the UCR Program collects both offense and arrest data. The definitions for these offenses are:</p>
<p>Human Trafficking/Commercial Sex Acts:  inducing a person by force, fraud, or coercion to participate in commercial sex acts, or in which the person induced to perform such act(s) has not attained 18 years of age.</p>
<p>Human Trafficking/Involuntary Servitude:  obtaining of a person(s) through recruitment, harboring, transportation, or provision, and subjecting such persons by force, fraud, or coercion into involuntary servitude, peonage, debt bondage, or slavery (not to include commercial sex acts).</p>
<p>The data in the tables included in this report reflect the offenses and arrests recorded by state and local law enforcement agencies (LEAs) that currently have the ability to report the data to the national UCR Program. As such, they should not be interpreted as a definitive statement of the level or characteristics of human trafficking as a whole. The data declaration pages, which will help the user better understand the data, and the methodology used for the four following tables are located in the Data Declarations and Methodology section near the end of this report. In addition, a Question and Answer section about human trafficking data is provided as a supplement to this report.</p>
<p><em>Note:  Regarding the data reported to the UCR Program, it is important to note that these data represent only one view of a complex issue—the law enforcement perspective. However, due to the nature of human trafficking, many of these crimes are never reported to the local, state, tribal, and federal LEAs that investigate them. In addition to the law enforcement facet in fighting these crimes, there are victim service organizations whose mission it is to serve the needs of the victims of human trafficking. In order to have the complete picture of human trafficking, it would be necessary to gather information from all of these sources.</em></p>
<p>Source: <a href=""https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/additional-publications/human-trafficking"" target=""_blank"" rel=""nofollow"">https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016/additional-publications/human-trafficking</a></p>
This dataset was created by Uniform Crime Reports and contains around 100 samples along with Unnamed: 4, Unnamed: 10, technical information and other features such as:
- Unnamed: 9
- Unnamed: 5
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 6 in relation to Table 3
- Study the influence of Unnamed: 2 on Unnamed: 1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Uniform Crime Reports 
Start A New Notebook!"	131	1036	10	yamqwe	human-trafficking-2016e
855	855	CreditoAdimplentesInadimplentes		[]		0	0	0	caioandre182	creditoadimplentesinadimplentes
856	856	Homes Listed On Zillow 	The dataset contains information about homes listed on Zillow	['housing', 'business', 'real estate']	"About this dataset
&gt; <h2>Other Metrics</h2>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Other Metrics</h2>
<ul>
<li>Median List Price ($): Median of the list price (or asking price) for homes listed on Zillow.</li>
<li>Median Sale Price ($): Median of the selling price for all homes sold in a given region.</li>
<li>Median List Price Per Sq Ft ($): Median of list prices divided by the square footage of a home.</li>
<li>Median Sale Price Per Sq Ft ($): Median of sale prices divided by the square footage of a home.</li>
<li>Sold for Loss (%): The percentage of homes in an area that sold for a price lower than the previous sale price.</li>
<li>Sold for Gain (%): The percentage of homes in an area that sold for a price higher than the previous sale price.</li>
<li>Increasing Values (%): The percentage of homes in an given region with values that have increased in the past year.</li>
<li>Decreasing Values (%): The percentage of homes in an given region with values that have decreased in the past year.</li>
<li>Listings With Price Cut (%): The percentage of current listings on Zillow with a price cut during the month.</li>
<li>Median price cut (%): Median of the percentage price reduction for homes with a price reduction during the month.</li>
<li>Sold in Past Year (Turnover) (%): The percentage of all homes in a given area that sold in the past 12 months.</li>
<li>Homes Foreclosed: The number of homes (per 10,000 homes) that were foreclosed upon in a given month. A foreclosure occurs when a homeowner loses their home to their lending institution or it is sold to a third party at an auction.</li>
<li>Foreclosure Re-Sales (%): The percentage of home sales in a given month in which the home was foreclosed upon within the previous year (e.g. sales of bank-owned homes after the bank took possession of a home following a foreclosure).</li>
<li>Sale-to-List Price Ratio: Median of the ratio between the sale price and the list price for all homes (e.g. if a home with a list price of $200k sells for $250k, its ratio would be 5:4 or 1.25).</li>
<li>Inventory (Raw): Median of weekly snapshot of for-sale homes within a region for a given month.</li>
<li>Age of Inventory: Each Wednesday, age of inventory is calculated as the median number of days all active listings as of that Wednesday have been current. These medians are then aggregated into the number reported by taking the median across weekly values.</li>
<li>Price to Rent Ratio: This ratio is first calculated at the individual home level, where the estimated home value is divided by 12 times its estimated monthly rent price. The the median of all home-level price-to-rent ratios for a given region is then calculated.</li>
<li>Buyer/Seller Index: This index combines the sale-to-list price ratio, the percent of homes that subject to a price cut and the time properties spend on the market (measured as Days on Zillow). Higher numbers indicate a better buyers’ market, lower numbers indicate a better sellers’ market, relative to other markets within a metro.</li>
<li>Market Health Index: This index indicates the current health of a given region’s housing market relative to other markets nationwide. It is calculated on a scale from 0 to 10, with 0 representing the least healthy markets and 10 the healthiest markets.</li>
<li>Days on Zillow: The median days on market of homes sold within a given month, including foreclosure re-sales. The latest data is for one month prior to the current ZHVI (e.g., if the most current month for ZHVI data is January, the most current month for Days on Zillow data will be December).</li>
<li>Home Sales: The number of homes sold during a given month. Our sales series methodology is available <a href=""http://www.zillow.com/research/home-sales-methodology-7733/"" target=""_blank"" rel=""nofollow"">here</a>. Note: ZIP code sales data is not latency adjusted, but is filtered.</li>
<li>Home Sales, SA: The number of homes sold during the given month, seasonally adjusted using the X-12-Arima method. Our sales series methodology is available <a href=""http://www.zillow.com/research/home-sales-methodology-7733/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Unfiltered Transactions: The number of homes sold during the given month. This series is latency adjusted, but no transaction-type filtering is done. For example, non-arm’s-length sales, foreclosures and title transfers would be all be included.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 10000 samples along with 2014 06, 2017 04, technical information and other features such as:
- 2012 08
- 2013 03
- and more.
How to use this dataset
&gt; - Analyze 2010 05 in relation to 2012 11
- Study the influence of 2010 06 on 2013 05
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	39	332	3	yamqwe	median-list-pricee
857	857	FNlist		[]		0	6	0	truonghuymai	fnlist
858	858	The Cereal Industry Sudden Demand Spike In 2021	Sudden demand spike of cereal while a deadly disease stalks its factory workers	['united states', 'business', 'economics', 'food']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/vAqrQGOzSyCUh6nC7870_Screenshot%202021-03-21%20at%201.30.54%20pm.png"" alt="""" style=""""></p>
<p>​</p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE ARTICLE: <a href=""https://www.bloomberg.com/opinion/articles/2021-02-24/beyond-grape-nuts-cereal-makers-had-a-very-weird-year"" target=""_blank"" rel=""nofollow"">Bloomberg</a></p>
<p>DATA SOURCE: <a href=""https://apps.bea.gov/iTable/iTable.cfm?ReqID=19&amp;step=2#reqid=19&amp;step=2&amp;isuri=1&amp;1921=underlying"" target=""_blank"" rel=""nofollow"">Bureau of Economic Analysis</a></p>
<p>NOTES:</p>
<ol>
<li>Personal Consumption Expenditures on Food and beverages purchased for off-premises consumption</li>
<li>Millions of dollars; quarters and months are seasonally adjusted at annual rates])</li>
</ol>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what could be improved with this chart?</li>
<li>How can you make it better?</li>
</ul>
This dataset was created by Andy Kriebel and contains around 10000 samples along with Month, Category, technical information and other features such as:
- Millions Of Dollars
- Sub Category
- and more.
How to use this dataset
&gt; - Analyze Month in relation to Category
- Study the influence of Millions Of Dollars on Sub Category
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	80	611	6	yamqwe	2021-w12-the-cereal-industry-had-a-very-weird-yee
859	859	Bike routes in Chicago	All Bike routes in Chicago	['transportation', 'cycling', 'retail and shopping']	"About this dataset
&gt; <p>Bike routes in Chicago. To view or use these files outside of a web brower, compression software and special GIS software, such as ESRI ArcGIS, is required.</p>
<p>Source: <a href=""https://data.cityofchicago.org/d/3w5d-sru8"" target=""_blank"" rel=""nofollow"">https://data.cityofchicago.org/d/3w5d-sru8</a><br>
Last updated at <a href=""https://data.cityofchicago.org/"" target=""_blank"" rel=""nofollow"">https://data.cityofchicago.org/</a> : 2022-01-28</p>
This dataset was created by City of Chicago<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span> and contains around 700 samples along with Bikeroute, F Street, technical information and other features such as:
- Street
- T Street
- and more.
How to use this dataset
&gt; - Analyze Bikeroute in relation to F Street
- Study the influence of Street on T Street
- More datasets
Acknowledgements
If you use this dataset in your research, please credit City of Chicago<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span> 
Start A New Notebook!"	19	204	7	yamqwe	bike-routese
860	860	Playlist3		[]		0	0	0	amauriramirez	playlist3
861	861	Age of Characters and Actors in Teen TV Shows	Characters and Species from the show	['arts and entertainment', 'movies and tv shows']	"About this dataset
&gt; <h2>About</h2>
<p>This is a manually collected dataset collected to gain a better understanding of the age differences between teen characters in TV shows and the actors who portray them.</p>
<h3>Why does this exist?</h3>
<p>Honestly, I watch a lot of teen TV shows. When I started watching <a href=""https://www.netflix.com/title/80241248"" target=""_blank"" rel=""nofollow"">The Politician</a> on Netflix, I realized something interesting. The show stars Ben Platt, an approximately 26 year old actor playing a high school senior. This isn't particularly remarkable since 20-somethings portray teens all the time, but Ben had done something interesting. The characters he portrayed were younger than him, yes, but they were <em>getting younger</em>. In 2012 he played a college freshman in the movie <em>Pitch Perfect</em>, then in 2019 he played a high school senior in <em>The Politician</em>, and in 2021 he played a high school junior in the movie <em>Dear Evan Hansen</em>.</p>
<p>I originally set out to see if any other actors that ""play young"" were Benjamin-Button-ing their way through roles, but ended up focusing solely on TV shows so that I wasn't buried in data collection forever. <strong>If you'd like to help me expand this dataset, let me know in the <a href=""https://data.world/amberthomas/age-of-characters-and-actors-in-teen-tv-shows/discuss/-/-"">Discussion</a> tab of this dataset!</strong></p>
<h2>Methods</h2>
<p>As mentioned above, this dataset was manually collected. The data includes TV shows that meet the following criteria:</p>
<ul>
<li>The show's first season was released between 2000 and 2021 in the US</li>
<li>The show primarily takes place either in a high school or centrally focuses teen characters</li>
</ul>
<h3>Sources</h3>
<p>For each show, I recorded data for the <strong>main</strong> characters and <strong>supporting</strong> characters using the following sources:</p>
<p><strong>Actor birthdays</strong> were collected on each respective actor's <a href=""https://imdb.com"" target=""_blank"" rel=""nofollow"">imdb</a> page. Any missing data indicates that the actor had no birthday listed.</p>
<p><strong>Character year &amp; Character age</strong> were collected primarily through the Fandom website for each TV show. The links to each Fandom community wiki can be found in the <code>titles_tv.csv</code> file. In a few cases, these data were collected manually by watching the show.</p>
<p><strong>Character gender</strong> was collected in the same way as character year &amp; character age. In cases where the character explicitly specified their gender <em>in</em> the show, the explicit statement is what was recorded. In some cases, the character gender and actor gender may not be the same, so this data refers <em>only to character gender</em>.</p>
<h3>Calculations</h3>
<p>All calculations used to derive <code>actor_character_age_difference</code> file can be found in the <a href=""https://data.world/amberthomas/cleaning-teen-tv-data/workspace/query?queryid=4cdd8fb6-9a0b-4ee0-981b-18422f797015"">Calculate Age Difference</a> query.</p>
<p>Determining <strong>approximate character age</strong> when only school year is known used the following assumptions:</p>
<ul>
<li><code>college freshman</code> = 18 years old</li>
<li><code>hs senior</code> = 17</li>
<li><code>hs junior</code> = 16</li>
<li><code>hs sophomore</code> = 15</li>
<li><code>hs freshman</code> = 14</li>
<li><code>5th Year</code> = 17</li>
<li><code>7th grade</code> = 12</li>
<li><code>6th grade</code> = 11</li>
</ul>
<h2>Caveats</h2>
<p>This is a manually collected dataset so it comes with a lot of caveats.</p>
<ul>
<li>This is not a <em>complete</em> list of all teen TV shows released between 2000 and 2021, but does attempt to be a representative sample of shows released during that time</li>
<li>This dataset is based on data from several user-collected sources</li>
<li><code>love_interest</code> in the <code>characters_tv</code> file may be incomplete and may contain love interests that occur outside of Season 1. I did my best to limit this to only other major characters and relationships in Season 1, but if I hadn't personally seen the show I was more reliant on Fandom reporting.</li>
<li><code>actor_age</code> is calculated based on the <code>actor_birthday</code> and the tv show's Season 1 Episode 1 <code>release_date</code> <strong>not</strong> the date in which the show was filmed. Honestly, I couldn't track down filming schedules for all of these, so <code>actor_age</code> will be skewed slightly older than the actors were when filming.</li>
</ul>
<h2>Looking for Help!</h2>
<p>I've put this dataset together in my spare time as a fun side project. If you're interested in helping me expand it, let me know in the <a href=""https://data.world/amberthomas/age-of-characters-and-actors-in-teen-tv-shows/discuss/-/-"">Discussion</a> tab of this dataset.</p>
<p>A few areas I can see that could be expanded:</p>
<ul>
<li>Increasing the number of TV shows listed</li>
<li>Expanding to include teen movies</li>
<li>Finding a more automated way to accomplish this task -- perhaps by combining the <a href=""https://data.world/linked-data/linkedmdb"">linkedmdb</a> and data from Fandom's API? Would love to hear any thoughts you may have!</li>
</ul>
This dataset was created by Amber Thomas and contains around 200 samples along with Name, Character Year, technical information and other features such as:
- Character Age
- Character Gender
- and more.
How to use this dataset
&gt; - Analyze Love Interest in relation to Title
- Study the influence of Birthday on Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	46	373	4	yamqwe	age-of-characters-and-actors-in-teen-tv-showse
862	862	Playlist 2		[]		0	2	0	amauriramirez	playlist-2
863	863	Bike-Shairing-Assignment	Forecast Use of CIty Bike Share System	['cities and urban areas', 'automobiles and vehicles', 'cycling', 'beginner', 'tabular data', 'retail and shopping', 'sklearn']	"Bike-Sharing-Assignment
Problem Statement: A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a ""dock"" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.
A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.
In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.
They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:
Which variables are significant in predicting the demand for shared bikes. How well those variables describe the bike demands Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.
Business Goal: You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market."	69	1111	6	ashusri4	bikeshairingassignment
864	864	Starbucks Stock Data - Live and Latest	Stock Performance of Starbucks Share - Downloaded from Yahoo! Finance API	['finance', 'time series analysis', 'deep learning', 'retail and shopping', 'investing']	"Content from Wikipedia
Starbucks Corporation is an American multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington. As the world's largest coffeehouse chain, Starbucks is seen to be the major representation of the United States' second wave of coffee culture. As of September 2020, the company had 32,660 stores in 83 countries, including 16,637 company operated stores and 16,023 licensed stores. Of these 32,660 stores, 18,354 were in the United States, Canada, and Latin America. Starbucks locations serve hot and cold drinks, whole-bean coffee, micro-ground instant coffee, espresso, caffe latte, full and loose-leaf teas, juices, Frappuccino beverages, pastries, and snacks. Some offerings are seasonal or specific to the locality of the store. Depending on the country, most locations offer free Wi-Fi.
Headquartered in the Starbucks Center, the company was founded in 1971 by Jerry Baldwin, Zev Siegl, and Gordon Bowker at Seattle's Pike Place Market. During the early 1980s, they sold the company to Howard Schultz who – after a business trip to Milan, Italy – decided to make the coffee bean store a coffeeshop serving espresso-based drinks. While chief executive officer from 1986 to 2000, Schultz's first tenure led to an aggressive expansion of the franchise, first in Seattle, then across the West Coast of the United States."	639	6023	37	kalilurrahman	starbucks-stock-data-live-and-latest
865	865	Species Profile Files	A collection of images and sound files for Hawaiian birds	['arts and entertainment']		0	5	1	amandanavine	species-profile-files
866	866	wheels-cache-blurr		[]		0	8	0	bobber	wheels-cache-blurr
867	867	COVID-19 Coronavirus data - weekly	COVID-19 Coronavirus data - weekly (from 17 December 2020)	['health', 'covid19']	"Content
The dataset contains a weekly situation update on COVID-19, the epidemiological curve and the global geographical distribution (EU/EEA and the UK, worldwide).
Since the beginning of the coronavirus pandemic, ECDC’s Epidemic Intelligence team has collected the number of COVID-19 cases and deaths, based on reports from health authorities worldwide. This comprehensive and systematic process was carried out on a daily basis until 14/12/2020. See the discontinued daily dataset: COVID-19 Coronavirus data - daily. ECDC’s decision to discontinue daily data collection is based on the fact that the daily number of cases reported or published by countries is frequently subject to retrospective corrections, delays in reporting and/or clustered reporting of data for several days. Therefore, the daily number of cases may not reflect the true number of cases at EU/EEA level at a given day of reporting. Consequently, day to day variations in the number of cases does not constitute a valid basis for policy decisions.
ECDC continues to monitor the situation. Every week between Monday and Wednesday, a team of epidemiologists screen up to 500 relevant sources to collect the latest figures for publication on Thursday. The data screening is followed by ECDC’s standard epidemic intelligence process for which every single data entry is validated and documented in an ECDC database. An extract of this database, complete with up-to-date figures and data visualisations, is then shared on the ECDC website, ensuring a maximum level of transparency.
ECDC receives regular updates from EU/EEA countries through the Early Warning and Response System (EWRS), The European Surveillance System (TESSy), the World Health Organization (WHO) and email exchanges with other international stakeholders. This information is complemented by screening up to 500 sources every day to collect COVID-19 figures from 196 countries. This includes websites of ministries of health (43% of the total number of sources), websites of public health institutes (9%), websites from other national authorities (ministries of social services and welfare, governments, prime minister cabinets, cabinets of ministries, websites on health statistics and official response teams) (6%), WHO websites and WHO situation reports (2%), and official dashboards and interactive maps from national and international institutions (10%). In addition, ECDC screens social media accounts maintained by national authorities on for example Twitter, Facebook, YouTube or Telegram accounts run by ministries of health (28%) and other official sources (e.g. official media outlets) (2%). Several media and social media sources are screened to gather additional information which can be validated with the official sources previously mentioned. Only cases and deaths reported by the national and regional competent authorities from the countries and territories listed are aggregated in our database.
Disclaimer: National updates are published at different times and in different time zones. This, and the time ECDC needs to process these data, might lead to discrepancies between the national numbers and the numbers published by ECDC. Users are advised to use all data with caution and awareness of their limitations. Data are subject to retrospective corrections; corrected datasets are released as soon as processing of updated national data has been completed. 
Source
https://data.europa.eu/euodp/en/data/dataset/covid-19-coronavirus-data-weekly-from-17-december-2020"	42	1238	2	hgultekin	covid19-coronavirus-data-weekly
868	868	Huggingface BERT	BERT models directly retrieved and updated from: https://huggingface.co/	['clothing and accessories', 'computer science', 'nlp', 'neural networks', 'transformers']	"This dataset contains many popular BERT weights retrieved directly on Hugging Face's model repository, and hosted on Kaggle. It will be automatically updated every month to ensure that the latest version is available to the user.  By making it a dataset, it is significantly faster to load the weights since you can directly attach a Kaggle dataset to the notebook rather than downloading the data every time. See the speed comparison notebook.
The banner was adapted from figures by Jimmy Lin (tweet; slide) released under CC BY 4.0. BERT has an Apache 2.0 license according to the model repository.
Quick Start
To use this dataset, simply attach it the your notebook and specify the path to the dataset. For example:
python
from transformers import AutoTokenizer, AutoModelForMaskedLM
MODEL_DIR = ""/kaggle/input/huggingface-bert/""
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR + ""bert-large-uncased"")
model = AutoModelForMaskedLM.from_pretrained(MODEL_DIR + ""bert-large-uncased"")
Acknowledgements
All the copyrights and IP relating to BERT belong to the original authors (Devlin et. al 2019) and Google. All copyrights relating to the transformers library belong to Hugging Face. The banner image was created thanks to Jimmy Lin so any modification of this figure should mention the original author and respect the conditions of the license; all copyrights related to the images belong to him.
Some of the models are community created or trained. Please reach out directly to the authors if you have questions regarding licenses and usage."	458	6003	84	xhlulu	huggingface-bert
869	869	imdb reviews of Movies-Updated weekly	Data set of movie title reviews	['movies and tv shows']		10	680	0	gayathrirprog	imdb-reviews-of-moviesdownloaded-on-jan-19-2021
870	870	Environment	Environmental protection expenditure account  2009	['environment', 'business', 'education', 'economics', 'time series analysis', 'data analytics']	"Environment
Environmental protection expenditure account  2009
This data is for education and training in data science and analysis
About the columns ,
year
sector
class
variable1
variable2
units
magnitude
source
data value
flag
About tasks:
Data processing and analysis with an explanation of the results."	0	7	3	qusaybtoush	environment
871	871	Econom	Annual balance sheets and accumulation accounts 2008	['business', 'finance', 'economics', 'data visualization', 'time series analysis', 'data analytics']	"Economy
Annual balance sheets and accumulation accounts 2008
This data is for education and training in data science and analysis
About the columns ,
Year
Institutional sector name 
Institutional sector code 
Descriptor
SNA08TRANS
Asset liability code
Status
Values
About tasks:
Data processing and analysis with an explanation of the results."	1	6	4	qusaybtoush	econom
872	872	Handwritten_Digit_Corpus(10.0k files)	Image Samples for ML Models 	['arts and entertainment']		0	6	0	jamesanavatan	handwritten-digit-10000
873	873	Wholesale trade survey 	Trade data - Sep -2021	['exploratory data analysis', 'time series analysis', 'data analytics', 'investing', 'pandas']	"Wholesale trade survey
Trade data - Sep -2021
This data is for education and training in data science and analysis
About the columns ,
Series reference
Period
Data value
Suppressed
STATUS
UNITS
Magnitude
Subject
Group
Series_title_1
Series_title_2
Series_title_3
Series_title_4
Series_title_5
About tasks:
Data processing and analysis with an explanation of the results."	1	4	5	qusaybtoush	wholesale-trade-survey
874	874	Business Employment	 Business Employment -  Data September 2021 	['business', 'time series analysis', 'data analytics', 'logistic regression', 'linear regression']	"Business Employment
Data september 2021
This data set about Business Employment , we have 19044 rows and 11 columns 
Series_reference
Period
Data_value
Suppressed
STATUS
UNITS 
Magnitude
Subject 
Group
Series_title_1
Series_title_2
Series_title_3
Series_title_4
Series_title_5"	1	11	3	qusaybtoush	business-employment
875	875	Imbalanced Tuberculosis and Pnuemonia dataset	Imbalanced image dataset of Tuberculosis, Pneumonia and Normal images 	['data cleaning', 'deep learning', 'image data', 'multiclass classification', 'tensorflow']	"This was a dataset created using other open source datasets and they were merged into one. The dataset is highly imbalanced and it might contain duplicate images. The motivation for creating this dataset was to build a model that could correctly classify Tuberculosis, Pneumonia and Normal/Healthy Chest X-ray images. There were many open source datasets available on Kaggle for Pneumonia and Tuberculosis separately so they were just combined and merged into a single dataset. 
Building a deep learning model for this dataset should be fun. But the actual blissful part of this dataset is the data cleaning process. 
Datasets that were used to create this imbalanced dataset
1. https://www.kaggle.com/tawsifurrahman/tuberculosis-tb-chest-xray-dataset
2. https://www.kaggle.com/usmanshams/tbx-11
3. https://www.kaggle.com/tolgadincer/labeled-chest-xray-images
4. https://www.kaggle.com/raddar/tuberculosis-chest-xrays-montgomery
5. https://www.kaggle.com/raddar/tuberculosis-chest-xrays-shenzhen
I would like to thank @tawsifurrahman, @tolgadincer, @usmanshams and @raddar for creating these amazing datasets."	44	189	1	roshanmaur	imbalanced-tuberculosis-and-pnuemonia-dataset
876	876	EfficientNet-PyTorch		[]		0	976	0	xujingzhao	efficientnetpytorch
877	877	Awesome Architectures	Splendid Photographs of Architectures for Image Classification	['culture and humanities', 'arts and entertainment', 'art', 'architecture', 'image data']	"Architectures for Multi-target Image Classification
This dataset contains splendid photographs of architecures as jpg images.
architecture_annotated.csv is the annotation file where images are classified into following three targets:
1. file_name: str[unique values] - represents jpg image file name
2. trees: str['present', 'not present'] - present represents presence of trees and not present represents otherwise.
3. lights_on: str['yes', 'no'] - yes represents presence of turned-on-lights and no represents otherwise.
4. people: str['present', 'not present'] - present represents presence of people and not present represents otherwise.
Annotations are performed manually.
JPG images have  varying dimensions. 
Dataset is automatically updated from a github repo.
Acknowledgements
Direct data source: https://github.com/NandhiniPython/Architectures
This Github repo collects licence-free photographs of great architectures from unsplash.com."	13	425	8	rajkumarl	awesome-architectures
878	878	Handwritten text classification using cnn		[]		0	8	0	prathameshkashid	handwritten-text-classification-using-cnn
879	879	bikepricessssssssssss		[]		0	2	0	viktoria1956	bikepricies
880	880	Coffee	A Custom PyTorch Wrapper for the chaii competition	['intermediate', 'nlp', 'neural networks', 'pytorch', 'transformers']	"My Model files and code for 'chaii - Hindi and Tamil Question Answering' Kaggle competition organized by Google
Link to Weights and Biases 🔥 Interactive Dashboard.
All models were taken from Huggingface Question Answering Models trained using @rhtsingh's processed dataset [ External Data - MLQA, XQUAD Preprocessing ] using huggingface/transformers inbuilt weights and biases logger.
The Model Weights can be found here
|Name                                                |Training Loss| Evaluation Loss         |
|-----------------------------------------------------|----------|-------------------|
|electra-base-squad2                                  |1.9823    |2.27 |
|distilbert-base-cased-distilled-squad                |1.1694    |1.31 |
|bert-base-cased-squad2                               |1.0992    |1.26  |
|distilbert-base-uncased-distilled-squad              |1.0642    |1.19 |
|bert-large-uncased-whole-word-masking-squad2         |0.9206    |1.02 |
|bert-large-uncased-whole-word-masking-finetuned-squad|0.9068    |1.01  |
|xlm-roberta-base-squad2                              |0.7908    |0.90 |
|distilbert-multi-finetuned-for-xqua-on-tydiqa        |0.7827    |0.89  |
|bert-multi-uncased-finetuned-xquadv1                 |0.7072    |0.93 |
|bert-multi-cased-finetuned-xquadv1                   |0.6517    |0.74 |
|bert-base-multilingual-cased-finetuned-squad         |0.6257    |0.73 |
|xlm-multi-roberta-large-squad2                       |0.6209    |0.74  |
|bert-multi-cased-finedtuned-xquad-tydiqa-goldp       |0.6156    |0.70 |
|roberta-large-squad2                                 |0.2488    |0.36 |
|roberta-base-squad2                                  |0.236     |0.35|"	58	1831	8	sauravmaheshkar	coffee
881	881	wholesale customer data		['retail and shopping']		0	3	0	prinskumar	wholesale-customer-data
882	882	SCI01 - TRAEFIK - access.log - SEMANAL	Access.log do traefik	[]		7	205	0	rafaelpbmota	sci01-traefik-semanal
883	883	IPL-2021	IPL2021 over by over scorecard	['cricket', 'exploratory data analysis', 'data cleaning', 'data visualization', 'tabular data']	"Plot
Cricket is like a festival in India. People sometimes forget their political fight, corporate dateline when it comes to cheering up their favorite cricket team. Every five to ten minutes people search the cricket scores. 
in the case of a T20 cricket tournament like IPL, the duration is much less. 
As we know this year is different due to the pandemic but there is no drop of enthusiasm in cricket fan's minds. After overcoming all the hardel IPL is happing. 
Content
This Data set contains the results of each match that happened in IPL 2021. Along with the scoreboard information after the end of each over. If any supper over took place that information is also included in a separated CSV file.
Acknowledgements
Data is gold in the data science community so the gathering of quality data is hard. I like to thank CRICBUZZ for uploading ball by ball commentary of each IPL match.
Note : This data may help data lovers, Ml engineers to predict the scoreboard after an over."	395	3754	30	sankha1998	indianpremierleague2021
884	884	Climate Change Dataset	Data from World Development Indicators and a repository for over 8000 samples	['atmospheric science', 'environment', 'natural disasters', 'people and society', 'beginner', 'social issues and advocacy']	"About this dataset
&gt; <p>Data from World Development Indicators and <a href=""http://sdwebx.worldbank.org/climateportal/"" target=""_blank"" rel=""nofollow"">Climate Change Knowledge Portal</a> on climate systems, exposure to climate impacts, resilience, greenhouse gas emissions, and energy use.</p>
<p>In addition to the data available here and through the Climate Data API, the <a href=""http://sdwebx.worldbank.org/climateportal/"" target=""_blank"" rel=""nofollow"">Climate Change Knowledge Portal</a> has a web interface to a collection of water indicators that may be used to assess the impact of climate change across over 8,000 water basins worldwide. You may use the web interface to download the data for any of these basins.</p>
<p>Here is how to navigate to the water data:</p>
<ul>
<li>Go to the Climate Change Knowledge Portal home page (<a href=""http://climateknowledgeportal.worldbank.org/"" target=""_blank"" rel=""nofollow"">http://climateknowledgeportal.worldbank.org/</a>)</li>
<li>Click any region on the map Click a country In the navigation menu</li>
<li>Click ""Impacts"" and then ""Water"" Click the map to select a specific water basin</li>
<li>Click ""Click here to get access to data and indicators"" Please be sure to observe the disclaimers on the website regarding uncertainties and use of the water data.</li>
</ul>
<p>Attribution: Climate Change Data, World Bank Group.</p>
<p>World Bank Data Catalog Terms of Use</p>
<p><strong><em>Source:</em></strong> <a href=""http://data.worldbank.org/data-catalog/climate-change"">http://data.worldbank.org/data-catalog/climate-change</a></p>
This dataset was created by World Bank and contains around 10000 samples along with Series Name, 2004, technical information and other features such as:
- 2003
- 2001
- and more.
How to use this dataset
&gt; - Analyze 1993 in relation to 2010
- Study the influence of Series Code on Country Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit World Bank 
Start A New Notebook!"	290	2614	14	yamqwe	climate-change-datae
885	885	job-a-thon-february-2022	Predict the engagement score of the video on the user level	['tabular data', 'regression']	"Problem Statement
ABC is an online content sharing platform that enables users to create, upload and share the content in the form of videos. It includes videos from different genres like entertainment, education, sports, technology and so on. The maximum duration of video is 10 minutes.
Users can like, comment and share the videos on the platform. 
Based on the user’s interaction with the videos, engagement score is assigned to the video with respect to each user. Engagement score defines how engaging the content of the video is. 
Understanding the engagement score of the video improves the user’s interaction with the platform. It defines the type of content that is appealing to the user and engages the larger audience.
Objective
The main objective of the problem is to develop the machine learning approach to predict the engagement score of the video on the user level.
Data Dictionary
You are provided with 3 files - train.csv, test.csv and sample_submission.csv
Acknowledgements
https://datahack.analyticsvidhya.com/contest/job-a-thon-february"	58	758	11	prokaggler	jobathonfebruary2022
886	886	Document Objects Detection		[]		2	75	0	chaitanyathombare	document-objects-detection
887	887	Bikeas		[]		0	2	0	viktoria1956	bikeprices
888	888	Klasifikasi Hewan		[]		4	5	0	haryodwi	klasifikasi-hewan
889	889	Lojas da PETZ	Endereço e informações das lojas da PETZ (PETZ3)	['brazil', 'business', 'beginner', 'tabular data', 'pandas']	"Fala Rapaziada!
Segue o dataset com as informações das lojas da PETZ. Essas informações foram adquiridas através de um scraping. Qualquer dúvida é só me chamar!
site da petz: https://www.petz.com.br/nossas-lojas
email: marcus.rodrigues4003@gmail.com
whatsapp: (11) 94937-0306
Atenciosamente Marcus Vinicius!"	5	69	4	markfinn1	lojas-da-petz
890	890	🏫 College Scorecard Since	Data from 1996 for all institutions of higher education	['universities and colleges', 'education']	"About this dataset
&gt; <p>College Scorecards make it easier for students to search for a college that is a good fit for them. They can use the College Scorecard to find out more about a college's affordability and value so they can make more informed decisions about which college to attend.</p>
<p>Download the data that appear on the College Scorecard, as well as supporting data on student completion, debt and repayment, earnings, and more. The files include data from 1996 through 2015 for all undergraduate degree-granting institutions of higher education. This data was last updated on March 2nd, 2016.</p>
<p>Attribution: <a href=""https://collegescorecard.ed.gov/data/"" target=""_blank"" rel=""nofollow"">U.S. Department of Education, College Scorecard</a></p>
<p><a href=""https://collegescorecard.ed.gov/data/documentation/"" target=""_blank"" rel=""nofollow"">College Scorecard Documentation</a></p>
This dataset was created by Education and contains around 8000 samples along with Comp 4yr Trans Yr6 Rt, Noncom Rpy 3yr Rt Supp, technical information and other features such as:
- Nopell Wdraw 4yr Trans Yr3 Rt
- Rpy 3yr Rt Supp
- and more.
How to use this dataset
&gt; - Analyze Md Inc Comp Orig Yr2 Rt in relation to Pell Enrl 2yr Trans Yr6 Rt
- Study the influence of Dep Unkn 2yr Trans Yr2 Rt on Not1stgen Comp 2yr Trans Yr3 Rt
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Education 
Start A New Notebook!"	74	535	6	yamqwe	college-scorecarde
891	891	Omicron Rising	Tweets about the new Covid-19 variant, Omicron	['public health', 'online communities', 'social networks', 'covid19']	"Context
These tweets are collected using Twitter API and a Python script. A query for this high-frequency hashtag (#Omicro) is run on a daily basis for a certain time period, to collect a larger number of tweets samples.
<img src=""https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F769452%2F35db2dd68238bfd958efdabebc9fef8f%2Fcovid-19-4961257_1280-e1586986896105.jpg?generation=1595760042647275&alt=media"">
The collection script is a variation of such script: https://github.com/gabrielpreda/covid-19-tweets
Content
The tweets have #Omicron hashtag. Collection started on 30/11/2021, with an initial 3.1k batch and will continue on a daily basis.
Inspiration
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	979	7358	38	gpreda	omicron-rising
892	892	Georgia COVID-19 Daily Status Report	from https://dph.georgia.gov/covid-19-daily-status-report	['news']	"Source
This is just a download of the COVID-19 Status Report data from the Georgia Department of Health."	5	176	1	michaelsolberg	georgia-covid19-daily-status-report
893	893	Reddit Just no Mother in Law 	r/JUSTNOMIL subreddit - posts about nasty, cruel, toxic, abusive MILs & moms	['people and society', 'law', 'nlp', 'text mining', 'text data', 'social networks']	"Context
Support for those with nasty, cruel, toxic, abusive MILs & moms (r/JUSTNOMIL), is a subreddit for posting about your MIL or Mother who is just the worst. A place for those that need support, come for advice, or just to vent and get it all out. Some of the posts and comments are actually about imaginary people and imaginary events, posted just for fun. 
The data is not filtered.
There are some ""domain specific"" keywords/acronyms in the dataset, which probably doesn't make too much sense for those not frequenting this subreddit. Here are few of them:
MIL - Mother in Law;
JNMIL - Just no Mother in Law;
DO - Dear one (i.e. husband/wife);
SAHD - Stay at Home Dad (sometime is spelled sad);
SAH - Stay at Home;
SD - Single Dad;
DH - Dear Husband;
BEC - Someone that you just cannot stand - from Bitch eating crackers - apparently a meme with a person that you instantly dislike.
Collection
Reddit posts from subreddit r/JUSTNOMIL.
Script used for collection can be found here: Reddit extract content
Inspiration
Use the texts in this dataset to:   
train to do text data analysis; 
perform sentiment analysis; 
try to detect which posts refers to real people and issues and which ones are invented (because there are a lot in this subreddit);
perform topic modelling on the text corpus."	7	291	6	gpreda	reddit-just-no-mother-in-law
894	894	Reddit Conspiracy Theory	r/ConspiracyTheory subreddit posts and comments	['politics', 'economics', 'text data', 'currencies and foreign exchange', 'news', 'social networks']	"Context
Cryptocurrency (r/ConspiracyTheory), is a subreddit where people discuss about various Conspiracy Theories.
The data is not filtered.
Collection
Reddit posts from subreddit ConspiracyTheory, downloaded from https://www.reddit.com/r/ConspiracyTheory using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
* title - relevant for posts
 score - relevant for posts - based on impact, number of comments
 id - unique id for posts/comments
 url - relevant for posts - url of post thread
 commns_num - relevant for post - number of comments to this post
 created - date of creation
 body - relevant for posts/comments - text of the post or comment
* timestamp - timestamp
Inspiration
You can use the data to:
* Perform sentiment analysis;
* Identify discussion topics;"	231	4511	18	gpreda	reddit-conspiracy-theory
895	895	Birds Aren't Real	Posts from s/BirdsArentReal subreddit (a Generation Z conspiracy theory)	['animals', 'nlp', 'text data', 'social networks']	"Context
Birds Aren't Real (r/BirdsArentReal), is the official subreddit for the ""most woke among us"". It is described as ""a safe haven for believers to gather, support one another in these times of adversity, and share images and stories that propel the cause forward. The birds work for the bourgeoisie"".
A bit of context here: a significant number of members of Generation Z actively propagate (as a joke or seriously) the myth that birds doesn't exist anymore, because were gradually replaced by Government with drones. 
The movement took a certain momentum recently, here is a selection of articles documenting this strange phenomena:
* Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory
* ‘Birds Aren’t Real’: How A Parody Conspiracy Movement Fought ‘misinformation With Lunacy’
Collection
Reddit posts and commits from subreddit r/BirdsArentReal.
Script used for collection can be found here: Reddit extract content.
The data is not filtered.
Inspiration
Use the texts in this dataset to:   
train to do text data analysis; 
perform sentiment analysis; 
perform topic modelling on the text corpus.
Note: you can combine this data (from Twitter) with data collected from Twitter, here: Birds Aren't Real on Twitter, Either."	41	1352	17	gpreda	birds-arent-real
896	896	Reddit Vaccine Myths	r/VaccineMyths subreddit posts and comments	['education', 'data analytics', 'pandas']	"Context
VaccineMyths (r/VaccineMyths), is a subreddit where people discuss about various Vaccine Myths.
The data might contain a small percent of harsh language, the posts were not filtered.
Colection
Reddit posts from subreddit VaccineMyths , downloaded from https://www.reddit.com/r/VaccineMyths/ using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
title - relevant for posts
score - relevant for posts - based on impact, number of comments
id - unique id for posts/comments
url - relevant for posts - url of post thread
commns_num - relevant for post - number of comments to this post
created - date of creation 
body - relevant for posts/comments - text of the post or comment
timestamp - timestamp 
Inspiration
You can use the data to:
* Perform sentiment analysis;
* Identify discussion topics;"	19070	227146	1477	gpreda	reddit-vaccine-myths
897	897	1st Playlist		['music']		0	2	0	katharinewu	1st-playlist
898	898	SnakeCLEF2021	aicrowd - snakeclef2021-snake-species-identification-challenge 	['biology']	Source of Dataset - icrowd.com/challenges/snakeclef2021-snake-species-identification-challenge	0	7	1	paulrohan2020	snakeclef2021
899	899	Coswara Dataset	Data repository of Project Coswara	['business']		36	761	3	himanshu007121	coswara-dataset
900	900	Covid-19 in Angola	Covid-19 evolution in Angola. Updated weekly	[]		20	709	3	osvaldojunior	covid19-in-angola
901	901	CSVFer2013		[]		5	11	0	mohammedaaltaha	csvfer2013
902	902	Japan Trade Statistics Processed	A CSV version of the Japan Trade Statistics dataset by Tadashi Nagao	['finance']		28	49	3	srinivas66	japan-trade-statistics-processed
903	903	Recipes from Reddit	"1500+ recipes from the ""recipes"" subreddit"	['internet', 'text data', 'cooking and recipes', 'food', 'social networks']	"Context
Reddit is one of the most popular sites on the internet - it's a huge forum and resource for everything. About 175 million users share their knowledge, interests and opinions there. Today we will take a look at recipes shared by users of the recipes on the ""recipes"" subreddit.
Content
The database contains a number of information about published recipes in such a way that 1 row = 1 recipe. We have a lot of information about each recipe (full recipe, title of the post with the recipe, publication date, Twitter users posting the recipe or the number of characters including the recipe - see the descriptions of individual columns).
The data was obtained using webscraping. The R language with the ""RedditExtractoR"" and ""tidyverse"" packages was used for this. Only recipes from the ""recipes"" subreddit are selected. The longest comments were extracted from each topic and only those with more than 350 characters were selected. There are not too many regulations at the same time, because reddit limits the number of topics on one page to 500-1000, but over time the base will increase as it will be combined with new regulations and downloaded to a separate base where they will be combined.
Attention! Not every row can be a ""straight"" recipe (as an instruction), it can also be a recipe discussion or commentary to a published recipe.
Inspiration
We have text data, so we can apply various methods related to Text Mining/NLP. Examples of inspiration and notebooks using this database can be checking which recipes are more likely to share Reddit users (breakfast, dinner or maybe desserts), what ingredients are the most popular, what cuisine (Italian, Chinese or maybe some other?). I'm waiting for creative notebooks!"	128	1656	22	michau96	recipes-from-reddit
904	904	Abdo audio		[]		0	1	0	abdoeltawil	abdo-audio
905	905	Meteorite  Landing		['earth science']		0	3	0	neelmistry7077	meteorite-landing
906	906	1999 Czech Financial Dataset		[]		0	5	0	mariammariamr	1999-czech-financial-dataset
907	907	WiDSDatathon2022 ClimateChange		[]		0	4	0	ssapanasubedi	widsdatathon2022-climatechange
908	908	class playlist		['education']		0	0	0	kayleetam	class-playlist
909	909	UCI_SMS_SpamCollection		[]		0	3	0	mdwasimakhtar03	uci-sms-spamcollection
910	910	1111111		[]		0	22	0	freshair1996	1111111
911	911	Spelling mistake data 1Mn		['nlp', 'text data']	"Generated spelling mistakes data from new headlines (that should be correctly spelled) using nlpaug library to generate different types of spelling mistakes. 
Use this data to build and explore spell correction models.
https://www.kaggle.com/samarthagarwal23/spelling-mistakes-data-generator"	0	20	0	samarthagarwal23	spelling-mistake-data-1mn
912	912	Latest Worldwide Covid-19 Vaccine Data	Vaccine Data of all Countries till February 10, 2022	['beginner', 'advanced', 'tabular data', 'covid19']	"Content
Latest Covid-19 Vaccine Status of all the Countries in the World as on February 10, 2022
Attribute Information
Countries - Name of countries
Doses administered per 100 people _ Number of vaccine doses administered per 100 people
Total doses administered - Total number of doses administered
% of population vaccinated - Percentage of population vaccinated
% of population fully vaccinated- Percentage of population fully vaccinated
Source
Link : https://www.nytimes.com/interactive/2021/world/covid-vaccinations-tracker.html 
Other Updated Covid Datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	1583	7788	86	anandhuh	latest-worldwide-vaccine-data
913	913	PaddleDetection	PaddleDetection is an end-to-end object detection development kit	['artificial intelligence', 'computer vision']	Object Detection toolkit based on PaddlePaddle. It supports object detection, instance segmentation, multiple object tracking and real-time multi-person keypoint detection.	11	274	4	chenjiexu	paddledetection
914	914	crack_noncrack		[]		0	5	0	jirapaweer	crack-noncrack
915	915	Comcast Telecom Consumer Complaints		[]		0	0	0	rahilsareen	comcast-telecom-consumer-complaints
916	916	USA Statewise Latest Covid-19 Data	Statewise Covid-19 data USA as on February 10, 2022	['united states', 'exploratory data analysis', 'data visualization', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of all the states in USA as on February 10, 2022
Attribute Information
States - States of USA
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of  Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population - Total Cases per 1 million of the population
Death/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests per 1 million of the population
Population - Population of the states
Source
Link : https://www.worldometers.info/coronavirus/country/us/
Other Updated Covid Datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	498	2200	59	anandhuh	usa-statewise-latest-covid19-data
917	917	tempmodel		[]		1	3	0	caiorange9	tempmodel
918	918	Datasets of the article 'COVID-19 vaccines...'		[]		0	8	0	agatharodrigues	covid19-vaccine-maternal-population
919	919	Covid-19 in Europe - Latest Data	Covid-19 Data as on February 10, 2022	['europe', 'health', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of European ountries as on February 10, 2022
Attribute Information
Country/Other - Name of European countries and islands
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population- Total Cases per 1 million of the population
Death/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests done per 1 million of the population
Population - Population of the Countries
Source
Link : https://www.worldometers.info/coronavirus/#countries
Other Updated Covid Datasets
https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	479	2903	44	anandhuh	latest-covid19-data-of-european-countries
920	920	fasterrcnn_1		[]		0	3	0	handudu	fasterrcnn-1
921	921	🚌 London Bus Safety 	20000 samples along with Route, Victim Category, and more	['transportation']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/opVvgdS7uA0NapMACtQq_Screenshot%202018-12-15%20at%209.24.20%20pm.png"" alt=""Screenshot 2018-12-15 at 9.24.20 pm.png"" style=""""></p>
<h1><strong>About this Dataset</strong></h1>
<p>DATA SOURCE: <a href=""https://tfl.gov.uk/corporate/publications-and-reports/bus-safety-data"" target=""_blank"" rel=""nofollow"">Transport for London</a></p>
<p>DASHBOARD: <a href=""https://tfl.gov.uk/cdn/static/cms/documents/q2-18-london-bus-safety-dashboard.pdf"" target=""_blank"" rel=""nofollow"">London Buses Safety Dashboard – Q2</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
This dataset was created by Andy Kriebel and contains around 20000 samples along with Route, Victim Category, technical information and other features such as:
- Group Name
- Borough
- and more.
How to use this dataset
&gt; - Analyze Injury Result Description in relation to Victims Sex
- Study the influence of Year on Operator
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	134	665	3	yamqwe	2018-w51-london-bus-safety-performancee
922	922	new_cbnet		[]		0	4	0	ccvipchenbin	new-cbnet
923	923	abdullah_audio		[]		1	4	0	sselim99	abdullah-audio
924	924	Alcohol & Drugs In Elem and Secondary Schools 	Prediction of alcohol in high school students across the U.S	['alcohol', 'united states', 'education', 'health']	"About this dataset
&gt; <p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting and analyzing data related to education in the U.S. and other nations. NCES is located within the U.S. Department of Education and the Institute of Education Sciences. NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics on the condition of American education; conduct and publish reports; and review and report on education activities internationally.</p>
<ul>
<li>Table 232.10. Percentage of students in grades 12-Sep who reported using alcohol at least 1 day during the previous 30 days, by location and selected student characteristics: Selected years, 1993 through 2015</li>
<li>Table 232.20. Percentage distribution of students in grades 9-12, by number of days they reported using alcohol anywhere or on school property during the previous 30 days and selected student characteristics: Selected years, 2009 through 2015</li>
<li>Table 232.30. Percentage of public school students in grades 12-Sep who reported using alcohol at least 1 day during the previous 30 days, by location and state: Selected years, 2005 through 2015</li>
<li>Table 232.40. Percentage of students in grades 12-Sep who reported using marijuana at least one time during the previous 30 days, by location and selected student characteristics: Selected years, 1993 through 2015</li>
<li>Table 232.50. Percentage distribution of students in grades 9-12, by number of times they reported using marijuana anywhere or on school property during the previous 30 days and selected student characteristics: Selected years, 2009 through 2015</li>
<li>Table 232.60. Percentage of public school students in grades 12-Sep who reported using marijuana at least one time during the previous 30 days, by location and state: Selected years, 2005 through 2015</li>
<li>Table 232.70. Percentage of students in grades 12-Sep who reported that illegal drugs were made available to them on school property during the previous 12 months, by selected student characteristics: Selected years, 1993 through 2015</li>
<li>Table 232.80. Percentage of public school students in grades 12-Sep who reported that illegal drugs were made available to them on school property during the previous 12 months, by state: Selected years, 2003 through 2015</li>
<li>Table 232.90. Percentage of high school seniors reporting use of alcohol and illicit drugs, by frequency of use and substance used: Selected years, 1975 through 2015</li>
<li>Table 232.95. Percentage of 12- to 17-year-olds reporting use of illicit drugs, alcohol, and cigarettes during the past 30 days and the past year, by substance used, sex, and race/ethnicity: Selected years, 1985 through 2014</li>
</ul>
<p>\1\The term ""anywhere"" is not used in the Youth Risk Behavior Survey (YRBS) questionnaire; students were simply asked how many days during the previous 30 days they had at least one drink of alcohol.</p>
<p>\2\Race categories exclude persons of Hispanic ethnicity.</p>
<p>\3\Before 1999, Asian students and Pacific Islander students were not categorized separately, and students could not be classified as Two or more races. Because the response categories changed in 1999, caution should be used in comparing data on race from 1993, 1995, and 1997 with data from later years.</p>
<p>\4\Refers to the Standard Metropolitan Statistical Area (MSA) status of the respondent's household as defined by the U.S. Census Bureau. Categories include ""central city of an MSA (Urban),"" ""in MSA but not in central city (Suburban),"" and ""not MSA (Rural).""</p>
<p>\5\In the question about drinking alcohol at school, ""on school property"" was not defined for survey respondents. Data on alcohol use at school were not collected in 2013 and 2015.</p>
<p>SOURCE: Centers for Disease Control and Prevention, Division of Adolescent and School Health, Youth Risk Behavior Surveillance System (YRBSS), 1993 through 2015. (This table was prepared July 2016.)</p>
<p><strong><em>Source:</em></strong> <a href=""https://nces.ed.gov/programs/digest/current_tables.asp"" target=""_blank"">https://nces.ed.gov/programs/digest/current_tables.asp</a></p>
This dataset was created by National Center for Education Statistics and contains around 100 samples along with Unnamed: 13, Unnamed: 23, technical information and other features such as:
- Unnamed: 15
- Unnamed: 17
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 12 in relation to Unnamed: 9
- Study the influence of Unnamed: 34 on Unnamed: 3
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Center for Education Statistics 
Start A New Notebook!"	26	325	1	yamqwe	elem-and-secondary-stimulantse
925	925	English Words		[]		1	5	0	mostafafathy4869	english-words
926	926	Boombike		[]		0	4	1	dhruvilksheth	boombike
927	927	Ads CTR Optimisation		[]		0	4	0	ahmedaliomar	ads-ctr-optimisation
928	928	SARS-CoV2 Genome data	Genomics data of SARS-CoV2 variants for mutation modeling	['biology']		0	16	0	remananr	sarscov2-genome-data
929	929	Bitcoin Tweets	Tweets with trending #Bitcoin and #btc hashtag	['beginner', 'data visualization', 'text data', 'currencies and foreign exchange', 'social networks']	"Context
Bitcoin(₿) is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.
Bitcoin is a decentralized digital currency, without a central bank or single administrator, that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries. Transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. Bitcoins are created as a reward for a process known as mining. They can be exchanged for other currencies, products, and services. 
On 30 November 2020, bitcoin hit a new all-time high of $19,860 topping the previous high from December 2017. On 19 January 2021 Elon Musk placed #Bitcoin in his Twitter profile tweeting “In retrospect, it was inevitable”, which caused the price to briefly rise about $5000 in an hour to $37,299.
Content
The tweets have #Bitcoin and #btc hashtag. Collection star started on 6/2/2021, with an initial 1 lakh tweets, and will continue on a daily basis.
Information regarding the data
The data totally consists of 1 lakh+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers an account currently has. |
| 6 | user_friends | The number of friends a account currently has. |
| 7 | user_favourites | The number of favorites a account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | date | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #Bitcoin & #btc  |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Inspiration
The tweets were extracted using tweepy, Refer to this notebook for the complete extraction process https://www.kaggle.com/kaushiksuresh147/twitter-data-extraction-for-ipl2020
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	3044	23241	76	kaushiksuresh147	bitcoin-tweets
930	930	ensemble_boxes		[]		0	13	0	ccvipchenbin	ensemble-boxes
931	931	chess416		[]		9	10	0	baohoa	chess416
932	932	SACM Pretrained Models		[]		2	44	0	edalwang	sacm-pretrained-models
933	933	TestMTG		[]		2	832	1	nathanforyou	testmtg
934	934	YoloR_Git	implementation of paper	['computer vision']	implementation of paper - You Only Learn One Representation: Unified Network for Multiple Tasks (https://arxiv.org/abs/2105.04206)	10	233	5	chenjiexu	yolor-git
935	935	audio dataset		['music']		0	2	0	sselim99	audio-dataset
936	936	Grapes		['food']		0	6	0	nananano	grapes
937	937	STUDENT		['universities and colleges']		1	11	0	anushruthikae	student
938	938	vehicle and non vehicle 1		['automobiles and vehicles']		0	3	0	dimuthu10	vehicle-and-non-vehicle-1
939	939	models_best		[]		9	26	0	lushisb	models-best
940	940	fbp_longformer		[]		0	1	0	shobhitupadhyaya	fbp-longformer
941	941	art512		[]		0	0	0	jiccrlla	art512
942	942	ssdinnovation		[]		0	9	0	wangqiang2	ssdinnovation
943	943	QIQC ta cahya dataset2		[]		0	0	0	pinklava	qiqc-ta-cahya-dataset2
944	944	QIQC TA cahya dataset1		[]		0	0	0	pinklava	qiqc-ta-cahya-dataset1
945	945	trained_data		[]		0	1	0	rakshamayank	trained-data
946	946	HappyWhale ArcFace eff6		[]		6	34	0	aikhmelnytskyy	happywhale-arcface-eff6
947	947	SMTX Arrest Data	& some other stuff about data science politics & police. 	['data visualization', 'statistical analysis', 'social issues and advocacy', 'racial equity']	"see README.md for more information on this project. 
UPDATE 7/10/20
daniel used his formula magic to offset and match the data into one file: arrest list - total. this file contians 2017, 2018, 2019 arrests with demographic data for SMPD. 
UPDATE 7/9/20
The existing files are presented here in their original raw format as received from SMPD. I am presently working on formatting & scrubbing them."	114	3007	24	motherofdata	smpd-data
948	948	Data Humans	Combining the power of emotions and logic for better decision making	[]		37	2698	0	chardo	data-humans
949	949	covid_data	Covid Data Belgium downloaded directly from Sciensano	['europe', 'healthcare', 'diseases', 'education', 'health', 'tabular data']	"Context
I'm mainly just getting started learning Jupyter notebook and datascience in general.
It seemed like a good dataset to learn the basics while perhaps also benefiting others
Content
I aquired the data from here : https://epistat.wiv-isp.be/covid/
Also useful is the codebook mentioned on their site, which contain an explanation of all the fields
https://epistat.sciensano.be/COVID19BE_codebook.pdf
Acknowledgements
Disclaimer: I'm not affliated with sciensano.
The following excerpt is taken from there website in regards to this data:
""Sciensano, the Belgian institute for health, is responsible for the epidemiological follow-up of the COVID-19 epidemic in collaboration with its partners and other healthcare actors. The data collected can provide insight into the dynamics of the epidemic, help to anticipate different scenarios and to elaborate possible measures to curb the spread of the virus.
From 31 March, Sciensano will make a set of data available to interested parties on a daily basis. This information is a support for decision making in the control of the epidemic. To achieve its mission, Sciensano has reinforced or set up different surveillance systems thanks to the participation of health professionals.
Sciensano would like to extend its deepest gratitude to all healthcare professionals for their daily investment in the fight against coronavirus and their commitment to patients.""
Inspiration
I was inspired to look into this data by the website http://coronavirus.app"	5	46	1	kennydemetter	covid-data
950	950	Agile Project management		[]		0	11	0	nehas2123	agile-project-management
951	951	GP_DATA		[]		1	9	0	manarmoh	gp-data
952	952	5 deberta v3 base nbme		['standardized testing']		2	12	4	nbroad	5-deb-v3
953	953	mirflickr25k		['arts and entertainment']	"Source of the Dataset
MIRFLICKR-25000 (released in 2008 aka mirflickr08 or mirflickr25k)
1. Image collection (incl. tags, EXIF), approx 2.9 Gb
Collection in 1 large file (md5: A23D0A8564EE84CDA5622A6C2F947785)
mirflickr25k.zip
2. Annotations
mirflickr25k_annotations_v080.zip"	0	9	0	paulrohan2020	mirflickr25k
954	954	fold-6		[]		0	2	0	vincentyong97	fold6
955	955	Enrolled students in German universities	Number of students enrolled degree programs in German universities (2010-2020)	['universities and colleges']		2	12	0	phoellermann	enrolled-students-in-german-universities
956	956	Demo_1		[]		1	2	0	sophyia	demo-1
957	957	wuzzuf data analyst jobs 2022		[]		1	2	0	mahmoudeldesuky	wuzzuf-data-analyst-jobs-2022
958	958	covid-mexico	Mexico Coronavirus (COVID-19) reports from Dirección General de Epidemiología	['covid19']	"Context
Currently the Dirección General de Epidemiología publishes a daily report of people who might be positive case of COVID-19, unfortunately the format is in pdf
I wanted to make this information easy to consume for data analysis and visualization.
Content
This data set is the daily report about people who might be a positive case of COVID-19, the same data from Coronavirus (COVID-19) -Comunicado Técnico Diario but in .csv format
https://www.gob.mx/salud/documentos/nuevo-coronavirus-2019-ncov-comunicado-tecnico-diario
Acknowledgements
Many thanks to Dirección General de Epidemiología for publishing the daily reports"	226	4126	10	adangalvan	covidmexico
959	959	Data_microsoft	Data from Microsoft for it's Data Science 30 day Challenge	[]		5	305	0	amanpoddar	data-microsoft
960	960	bms-smalldataset		[]		1	15	0	fidanmusazade	bmssmalldataset
961	961	TrainCMix		[]		1	5	0	bstnst99	traincmix
962	962	latest_data		[]		0	4	0	adityasanju	latest-data
963	963	Covid-19 ECDC World	Covid-19 from European Centre for Disease Prevention and Control	['health', 'computer science', 'news', 'covid19']	"Context
This is datasets weekly updated about COVID-19
Content
Each row or entry contains the number of new cases reported per day and per country in world.
Acknowledgements
Thanks for European Centre for Disease Prevention and Control for sharing this data
Inspiration
I hope that  it will be helpful for the Kaggle community"	49	1061	5	holzersoahita	covid19-ecdc-europa
964	964	Global Mobility Data	Global mobility data as published by Google	['health', 'covid19']	"Google Mobility Data- COVID19
Source: https://www.google.com/covid19/mobility/
Direct download of well formatted PDF with graphs and context information for each country available at the source mentioned above.
As per the whitepaper by Jen Fitzpatrick and Karen DeSalvo of Google:
The reports use aggregated, anonymized data to chart movement trends over time by geography, across different high-level categories of places such as retail and recreation, groceries and pharmacies, parks, transit stations, workplaces, and residential. We’ll show trends over several weeks, with the most recent information representing 48-to-72 hours prior. While we display a percentage point increase or decrease in visits, we do not share the absolute number of visits. To protect people’s privacy, no personally identifiable information, like an individual’s location, contacts or movement, is made available at any point. 
All right reserved with original data publisher, ie, Google."	116	3218	4	sudhendu	global-mobility-data
965	965	Freqtrade Dataset		['earth and nature']		1	37	0	christianhansen12	freqtrade-dataset
966	966	Market Basket Optimization 		[]		3	9	0	ahmedaliomar	market-basket-optimization
967	967	Toursim Management System-Project		[]		1	6	0	manmathvasistha	toursim-management-systemproject
968	968	ssdinnovation2022		[]		3	12	0	wangqiang2	ssdinnovation2022
969	969	UNET Lung Segmentation Weights for Chest X Rays	Pre-Trained UNET Segmentation Model for Lung Masking from Chest X-rays	['computer science', 'classification', 'neural networks', 'transfer learning', 'health conditions', 'keras', 'tensorflow']	"Context
Often CXRs contain a lot of noise around them, for cardiovascular disease identification, the Lung is an essential part of the CXR and mostly the only object of interest. To eliminate learning from noise, it is often advisable to preprocess datasets first using UNET lung Segmentation and then apply Object Detection/Classification Algorithms. hence this model is being uploaded.
Starter Code
I strongly recommend this notebook for training.
Model Architecture :
```python
def unet(input_size=(256,256,1)):
    inputs = Input(input_size)
conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)
conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)
up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)
conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)
up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)
conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)
up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)
conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)
up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)
conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)
conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)
return Model(inputs=[inputs], outputs=[conv10])
```
Acknowledgements
This model would not be possible without Nikhil Pandey.
Here is the Source Notebook.
Also the dataset over which it is trained : Chest Xray Masks and Labels
Inspiration
Go forth and apply your own amazing DEEP NEURAL NETWORKS!"	91	2364	23	farhanhaikhan	unet-lung-segmentation-weights-for-chest-x-rays
970	970	MTG JSON	Magic the Gathering Card Data (cloned from mtgjson.com)	[]		16	942	0	mirhagk	mtg-json
971	971	Numerai	Train and Test Sets for Numerai Competition -- Updated Weekly	[]		4	920	1	eladwar	numerai
972	972	audio2numpy	MIT License  Copyright (c) 2019 Jiajun Yang	[]		6	2204	0	eladwar	audio2numpy
973	973	Brazilian Football Championship	Results of the first division of the Brazilian championship since 2013	['football', 'brazil']	Results of matches played in the first division of the Brazilian championship since 2013	83	1922	0	gabrielmeireles	brazilian-football-championship
974	974	gamifier		[]		0	1	0	anunnikrishnan	gamifier
975	975	people	1111111111111111111111111111	['online communities']		1	22	0	panfei748	people
976	976	#IndiaWantsCrypto tweets	Tweets with trending #IndiaWantsCrypto and #IndiaWantsBitcoin hashtag	['finance', 'exploratory data analysis', 'nlp', 'data visualization', 'currencies and foreign exchange', 'social networks']	"Context
India will go ahead with a complete ban on investment in cryptocurrencies while providing existing investors a transition period to exit their holdings. Cryptocurrency isn’t fiat currency backed by the Reserve Bank of India and its usage in all forms will be banned through the new law that will be introduced in Parliament, a senior Finance Ministry official said on condition of anonymity. 
The official cited earlier said India’s new law will be modeled on China’s regulatory regime, which has effectively banned on trading and usage of cryptocurrency, while the government is working on issuing its own virtual currency. China imposed a ban on initial coin offerings in 2017 and asked crypto exchanges to shut down. 
Since then, the Chinese central bank has also blocked all access to all domestic and foreign cryptocurrency exchanges and ICO websites. While banning cryptocurrencies, the Indian government will allow the use of technology underlying the cryptocurrency for research or any other such purposes, the official said, adding the proposed law will be soon sent to the Union Cabinet for approval.
Read more at https://www.bloombergquint.com/business/intent-on-ban-india-to-give-transition-time-to-crypto-investors-bq-exclusive
Content
The tweets have #IndiawantsCrypto & #IndiaWantsBitcoin hashtag. Collection star started on 6/2/2021, with an initial 2k+ tweets, and will continue on a daily basis.
Information regarding the data
The data totally consists of 2k+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers an account currently has. |
| 6 | user_friends | The number of friends a account currently has. |
| 7 | user_favourites | The number of favorites a account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | date | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #IndiawantsCrypto & #IndiawantsBitcoin  |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Inspiration
The tweets were extracted using tweepy, Refer to this notebook for the complete extraction process https://www.kaggle.com/kaushiksuresh147/twitter-data-extraction
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	53	1150	9	kaushiksuresh147	india-wants-crypto-tweets
977	977	IPL 2020 to 2022 Tweets	Explore the tweets made by fans with #ipl2020, 21, & 22 hashtag!	['cricket', 'sports', 'internet', 'nlp', 'text mining', 'text data']	"Context
The Indian Premier League (IPL) is a professional men's Twenty20 cricket league, contested by ten teams based out of ten Indian cities. The league was founded by the Board of Control for Cricket in India (BCCI) in 2007. It is usually held between March and May of every year and has an exclusive window in the ICC Future Tours Programme.
The IPL is the most-attended cricket league in the world and in 2014 was ranked sixth by average attendance among all sports leagues. In 2010, the IPL became the first sporting event in the world to be broadcast live on YouTube. The brand value of the IPL in 2019 was ₹47,500 crore (US$6.3 billion), according to Duff & Phelps. According to BCCI, the 2015 IPL season contributed ₹1,150 crores (US$150 million) to the GDP of the Indian economy. The 2020 IPL season set a massive viewership record with 31.57 million average impressions and with an overall consumption increase of 23 percent from the 2019 season.
There have been fourteen seasons of the IPL tournament. The current IPL title holders are the Chennai Super Kings, winning the 2021 season.
In August 2021, the BCCI announced that two new franchises would join the league starting from the 2022 season. It was also announced that the franchises would be based in two of the six cities shortlisted by the BCCI; Ahmedabad, Lucknow, Cuttack, Guwahati, Ranchi, and Dharamshala. In closed bidding held on 25 October, RPSG Group and CVC Capital won bids for the two teams. RPSG paid ₹7,000 crores (US$930 million) for Lucknow, whereas CVC won Ahmedabad for ₹5,200 crores (US$690 million).
Below the all 8 IPL 2020 Team squad,
1. Chennai Super Kings
2. Royal Challengers Bangalore
3. Delhi Capital
4. Mumbai Indians
5. Kolkata Knight Riders
6. Rajasthan Royals
7. Kings XI Punjab
8. Sunrisers Hyderabad
9. Team Ahmedabad (New entry in 2022)
10. Team Lucknow (New entry in 2022)
This data consists of the tweets with the trending #ipl2020, #ipl2021, and #ipl2022 hashtags made by the fans of cricket
The data is extracted using TwitterAPI and a python script! Please refer to this notebook for the data extraction process https://www.kaggle.com/kaushiksuresh147/twitter-data-extraction-for-ipl2020. The data will be updated on a daily basis.
The data consists of 3 years of tweets made by fans during the IPL seasons 2020, 2021, and upcoming 2022 
Feel free to use the dataset and explore the craze of IPL and understand the sentiments of the fans towards IPL tournaments
Content
Information regarding the data
The data totally consists of 10k+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers an account currently has. |
| 6 | user_friends | The number of friends an account currently has. |
| 7 | user_favourites | The number of favorites an account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | dates | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #ipl2020  |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Task
Please refer to the task sections of the dataset to find interesting tasks assigned! 
Inspiration
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	680	10185	29	kaushiksuresh147	ipl2020-tweets
978	978	HWDI_tfrecords_train_128		[]		1	9	0	sashak89999	hwdi-tfrecords-train-128
979	979	HIU-DMTL tfrecords (NoMasks)		[]		0	5	0	rustyelectron	hiudmtl-tfrecords-nomasks
980	980	faster_ma-ml_gpu8_batch4_others		[]		0	12	0	yuanwenjie	faster-maml-gpu8-batch4-others
981	981	300 years of inflation rate in US	Annual inflation rate from 1700-2022	['business', 'finance', 'economics', 'time series analysis', 'investing']	"Context
Take a look at $1 dollar value in 1701 when adjusted for inflation.
Acknowledgements
Data source: https://www.in2013dollars.com/"	139	813	14	prasertk	300-years-of-inflation-rate-in-us
982	982	weather		[]		0	5	0	wallacefqq	weather
983	983	Basic Python Tutorial		[]		1	5	0	athirapisharody	basic-python-tutorial
984	984	HIU-DMTL tfrecords (WithMasks)		[]		0	7	0	rustyelectron	hiudmtl-tfrecords-withmasks
985	985	energy_out		[]		0	0	0	wallacefqq	energy-out
986	986	CDI Dataset	U.S. Chronic Disease Indicators (CDI) 	['healthcare', 'diseases', 'health', 'medicine', 'health conditions']	"Context
This notebook was my final project for the Ironhack Data Analytics Bootcamp. This project also has a Tableau presentation.
Content
The chronic disease indicators (CDI) are a set of surveillance indicators developed by consensus among CDC, the Council of State and Territorial Epidemiologists (CSTE), and the National Association of Chronic Disease Directors (NACDD). CDI enables public health professionals and policymakers to retrieve uniformly defined state-level data for chronic diseases and risk factors that have a substantial impact on public health. These indicators are essential for surveillance, prioritization, and evaluation of public health interventions. Several of the current chronic disease indicators are available and reported on other websites, either by the data source/custodians or by categorical chronic disease programs. However, CDI is the only integrated source for comprehensive access to a wide range of indicators for the surveillance of chronic diseases, conditions, and risk factors at the state level.
The original CDI consisted of 73 indicators adopted in 1998 and amended in 2002. In 2012-13, CDC, CSTE, and NACDD collaborated on a series of reviews that were informed by subject-matter expert opinion to make recommendations for updating CDI. The goal of this review was to ensure that CDI is responsive to the expanded scope and priorities of chronic disease prevention programs in state health departments.
As a result, CDI increased to 124 indicators in the following 18 topic groups: alcohol; arthritis; asthma; cancer; cardiovascular disease; chronic kidney disease; chronic obstructive pulmonary disease; diabetes; immunization; nutrition, physical activity, and weight status; oral health; tobacco; overarching conditions; and new topic areas that include disability, mental health, older adults, reproductive health, and school health. For the first time, CDI includes 22 indicators of systems and environmental change. A total of 201 individual measures are included for the 124 indicators, many of which overlap multiple chronic disease topic areas or are specific to a certain sex or age group.
CDI is an example of collaboration among CDC and state health departments in building a consensus set of state-based health surveillance indicators. This update will help ensure that CDI remains the most relevant and current collection of chronic disease surveillance data for state epidemiologists, chronic disease program officials, and reproductive health and maternal and child health officials. The standardized indicator definitions will also encourage consistency in chronic disease surveillance at the national, state, and local public health levels.
The data has been downloaded from https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi
Data Columns Reference https://www.cdc.gov/mmwr/pdf/rr/rr6401.pdf
Acknowledgements
I wouldn't be here without the help of others. 
This notebook was made with the information from the work of Daniel Wu, and Pedro Moreno. 
Inspiration
What is the most common disease in the USA? 
What are the factors affecting the top disease?
Does a disease respect borders? In other words, is a disease limited by state borders?
How does a disease change over time?"	4	63	2	ed777kaggle	cdi-dataset
987	987	Covid Vaccine Tweets	Tweets with trending #CovidVaccine hashtag	['health', 'internet', 'online communities', 'social networks', 'covid19']	"Context
COVID-19 is an infectious disease caused by a newly discovered strain of coronavirus, a type of virus known to cause respiratory infections in humans. This new strain was unknown before December 2019, when an outbreak of pneumonia of unidentified cause emerged in Wuhan, China.
Ever since the Covid-19 pandemic there has been quite a buzz in social media platforms and news sites regarding the need for COVID-19 Vaccine. As the number of people getting affected by Covid-19 has been increasing drastically. This data set brings you the twitter  tweets made with the hashtag #CovidVaccine
Content
The tweets have #CovidVaccine hashtag. The collection started on 1/8/2020, and will be updated on a daily basis.
Information regarding the data
The data totally consists of 1 lakh+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers a account currently has. |
| 6 | user_friends | The number of friends a account currently has. |
| 7 | user_favourites | The number of favorites a account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | date | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #CovidVaccine  |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Inspiration
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	2796	22734	68	kaushiksuresh147	covidvaccine-tweets
988	988	test_hackathon1		[]		1	10	0	nutchanonww	test-hackathon1
989	989	MSG Files to PST for MS Outlook with Large Files	Direct conversion from MSG to PST with all email files	['email and messaging']	"If you are an MS Outlook user and searching for a solution to how to export MSG files into PST file format. If you are facing a problem with the migration of MSG to PST format and you have no idea how to convert Outlook messages into PST then do not worry, this blog will help you to export .msg files in .pst format with two types of methods.
MSG file is a file format that has been developed in MS Outlook, which saves all the message data containing several attributes like sender, date, recipient, message body, etc. To access and read the MSG file in MS Outlook must import the MSG emails into PST file format. In any case, a portion of the clients goes through issues when there are many MSG emails, which is to relocate to the Outlook application. In this blog, we discuss all the easy and possible solutions to export MSG as PST. Now, let’s get started with different methods to import MSG in PST files.
Reason to import MSG files into PST
It is better to convert large number of MSG files into PST file format because it takes a minimum storage of local drive and there is less chance of data corruption.
For quick data accessibility in MS Outlook app, you need to convert MSG files into PST files.
To prevent the hectic of managing MSG files you can export both large size files and other MSG files to the PST format.
Export MSG into PST with Manual Method
Drag & Drop Method
Open MS Outlook and make a new folder.
Now, rename your newly generate folder, then drag MSG files that are stored in the local drive to the newly generated folder to import MSG files into Outlook.
When all the MSG files are exported, users can simply open and view MSG files.
Now, you can see all MSG files with information.
Copy & Paste Method
First, go to the location on your computer where you saved all Outlook MSG files and copy them all.
After that, open the MS Outlook application and paste all copied MSG files into the folder of MS Outlook.
Disadvantages of Manual Process
This process only works when you have a few MSG files because this manual method cannot convert multiple MSG files into PST. This process is not good for batch conversion and there is no guarantee that your entire data is saved in PST format after the conversion process. Without the MS Outlook application, this process does not work without expert skills. Non-technical users cannot convert their MSG files onto PST with this method.
Automated Solution for Converting MSG files into PST
After seeing all the drawbacks and limitations of the manual method, to overcome it, there is third-party software existing to convert MSG to PST file format. So let’s know the perfect and automated solutions to export .msg to PST file.
CubexSoft MSG Export tool is perfect for direct conversion of MSG files into PST file format. This tool effortlessly exports multiple Outlook email messages into PST without error and data loss. With this software, you can convert your data in batch mode within a single process. There is no need to install the MS Outlook application because this tool is standalone functionality. Technical and non-technical users also can export their data with ease because this tool is very easy to use.
This tool easily exports MSG files with all attachments and email properties. The MSG Converter is very helpful because it allows users to import Outlook MSG files to other file formats like PDF, HTML, MBOX, EML, EMLX, RTF, TXT, DOC, DOCX, MHT, etc. And this utility is not time-consuming and also gives you 100% accurate results for file conversion. This software works with all MS Outlook editions like Outlook 2019, 2016, 2013, etc. And also support Windows OS, Windows 11, 10, 8.1, etc.
MSG to PST Software Steps
Download the MSG to PST Software and open it on the system.
Now, you can select the MSG Files or MSG Folder(s).
Next, you can see MSG files/folders in an organized way. And select or deselect the files with a help of checkboxes.
Next, you can choose the file format PST for your MSG files from the list. If you want to store a separate PST file, then click on the Create Separate PST Option.
Next, click on the browse button and give the desired location for your file and you can select advanced filter Options. Then click on the Convert button.
After clicking on the convert button your migration is started and ends with the message ‘Process Completed Successfully’.
In the end, click the OK button to finish the process.
Conclusion
Migrating MSG files is always a hectic task but users can easily convert MSG files to PST with the help of this software. Users can effortlessly export email files with all attachments. To know more about the conversion process you can download the free demo version of the software.
Read more:
CubexSoft MBOX Converter for MAC
MDaemon WorldClient to Office 365"	0	8	2	emerlycarl	msg-files-to-pst-for-ms-outlook-with-large-files
990	990	Coronavirus (COVID-19) Vaccinations	Complete Vaccination Data of People in the World (Updated Weekly)	['healthcare', 'public health', 'people and society', 'health', 'science and technology', 'computer science', 'public safety']	"Coronavirus Vaccination Data
43.5% of the world population has received at least one dose of a COVID-19 vaccine.
5.98 billion doses have been administered globally, and 28.8 million are now administered each day.
Only 2% of people in low-income countries have received at least one dose.
Vaccinations
|Variable|  Description|
| --- | --- |
|total_vaccinations |Total number of COVID-19 vaccination doses administered|
|people_vaccinated  |Total number of people who received at least one vaccine dose|
|people_fully_vaccinated    |Total number of people who received all doses prescribed by the vaccination protocol|
|total_boosters |Total number of COVID-19 vaccination booster doses administered (doses administered beyond the number prescribed by the vaccination protocol)|
|new_vaccinations   |New COVID-19 vaccination doses administered (only calculated for consecutive days)|
|new_vaccinations_smoothed  |New COVID-19 vaccination doses administered (7-day smoothed). For countries that don't report vaccination data on a daily basis, we assume that vaccination changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window|
|total_vaccinations_per_hundred |Total number of COVID-19 vaccination doses administered per 100 people in the total population|
|people_vaccinated_per_hundred  |Total number of people who received at least one vaccine dose per 100 people in the total population|
|people_fully_vaccinated_per_hundred    |Total number of people who received all doses prescribed by the vaccination protocol per 100 people in the total population|
|total_boosters_per_hundred |Total number of COVID-19 vaccination booster doses administered per 100 people in the total population|
|new_vaccinations_smoothed_per_million  |New COVID-19 vaccination doses administered (7-day smoothed) per 1,000,000 people in the total population|
Acknowledgements
The mission is to make data and research on the world's largest problems understandable and accessible."	3553	9060	27	pavan9065	coronavirus-covid19-vaccinations
991	991	testdata	test amaçlı veriler içerir	['business']		0	1119	0	telatkaya	testdata
992	992	Ghana COVID-19 Dataset	Records of COVID-19 cases confirmed in Ghana	['africa', 'health', 'covid19']	"Context
This repository contains dataset of the Novel Corona Virus Disease (COVID-19) cases recorded in Ghana. The data available in the dataset comes from updates given by the Ghana Health Service, the Ministry of Information and the Ghana Statistical Service. 
Content
Ghana_Covid19_DailyActive.csv
Number of columns = 8
confirmed - total number of confirmed positve cases in a given day
recovered - total number of people who recovered from the virus in a given day
death     - total number of people who died from the virus in a given day
date      - day on which confirmed, recovered and death were reported
cumulative_confirmed  - cumulative count of confirmed positive cases 
cumulative_recovered  - cumulative count of people who have recovered from the virus
cumulative_death      - cumulative count of people who have died from the virus
active_cases          - total number of existing positive cases on a given day
                        (active_cases = cumulative_confirmed - cumulative_recovered - cumulative_death)
Acknowledgements
Ghana Health Service
Ghana Statistical Service
Ministry of Information, Ghana"	303	5196	19	sammyhawkrad	ghana-covid19-dataset
993	993	Tesla Deaths	Record of Tesla accidents that involved a death	['automobiles and vehicles']	Tesla Deaths is a record of Tesla accidents that involved a driver, occupant, cyclist, motorcyclist, or pedestrian death. We record information about Tesla fatalities that have been reported and as much related crash data as possible such as location of crash, names of deceased. This dataset also tallies claimed and confirmed Tesla autopilot crashes, that is instances when Autopilot was activated during a Tesla crash that resulted in death.	139	3727	0	tesladeaths	tesla-deaths-tesla-crashes-that-involved-a-death
994	994	Apple Stock Data - Live and Latest - From IPO Date	Apple Stock Data - Live and Latest - From IPO Date	['business', 'economics', 'mobile and wireless', 'retail and shopping', 'investing']	"Apple Stock Data 
Info from Wikipedia
- Apple Inc. is an American multinational technology company that specializes in consumer electronics, computer software and online services. Apple is the largest information technology company by revenue (totaling $365.8 billion in 2021) and, since January 2021, the world's most valuable company. As of 2021, Apple is the fourth-largest PC vendor by unit sales and fourth-largest smartphone manufacturer. It is one of the Big Five American information technology companies, alongside Amazon, Alphabet (Google), Meta (Facebook), and Microsoft.
Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977, and sales of its computers, among them the Apple II, grew quickly. It went public in 1980, to instant financial success. Over the next few years, Apple shipped new computers featuring innovative graphical user interfaces, such as the original Macintosh, announced in a critically acclaimed advertisement, ""1984"", directed by Ridley Scott. The high cost of its products and limited application library caused problems, as did power struggles between executives. In 1985, Wozniak departed Apple amicably, while Jobs resigned to found NeXT, taking some Apple employees with him.
As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of Microsoft Windows on Intel PC clones. The board recruited CEO Gil Amelio, who prepared the struggling company for eventual success with extensive reforms, product focus and layoffs in his 500-day tenure. In 1997, Amelio bought NeXT to resolve Apple's unsuccessful operating-system strategy and entice Jobs back to the company; he replaced Amelio. Apple became profitable again through a number of tactics. First, a revitalizing campaign called ""Think different"", and by launching the iMac and iPod. In 2001, it opened a retail chain, the Apple Stores, and has acquired numerous companies to broaden its software portfolio. In 2007, the company launched the iPhone to critical acclaim and financial success. Jobs resigned in 2011 for health reasons, and died two months later. He was succeeded as CEO by Tim Cook.
The company receives significant criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive behavior and materials sourcing. In August 2018, Apple became the first publicly traded U.S. company to be valued at over $1 trillion. Two years later, it became the first company valued at over $2 trillion, and the first valued at over $3 trillion in January 2022. The company enjoys a high level of brand loyalty, and is ranked as the world's most valuable brand; as of January 2021, there are 1.65 billion Apple products in active use."	201	1311	9	kalilurrahman	apple-stock-data-live-and-latest-from-ipo-date
995	995	NLP from scratch	Implementation of NLP algorithms from scratch	['earth and nature', 'computer science', 'programming', 'nlp', 'deep learning', 'python']	This dataset contains implementations of various Natural Language Processing algorithms from scratch. I will keep adding more implementations with time as I learn.	3	50	4	rajat95gupta	nlp-from-scratch
996	996	Mask Face Dataset (No, Mask, Improper)	Mask face images dataset 	['earth and nature']	"In this study, photographs of mask (name start 1), no mask (name start 2), and improper mask (name start 3) were collected by researchers via internet search. The discovered photos were combined with 4072 photos that were uploaded to the Kaggle website by Larxel (https://www.kaggle.com/andrewmvd/face-mask-detection). A face detection application was created to obtain face images from all the photos in the database. There may be more than one photo of the same individual in the database. This program, coded in C# language, detected automatically faces from photos. Through visual inspection, we eliminated a few low-quality face-mask images. Finally, we collected 529 improper mask, 992 mask, and 554 no mask face images. The used dataset contains 2075 facial images taken from different profiles. By using this dataset, a model for face mask-wearing sensitive doors has been proposed. Moreover, this dataset is a hybrid dataset. We created this facial image dataset using open-source face mask datasets. The most important attribute that distinguishes this dataset from other datasets is the creation of the improper mask class.
In order to use the dataset here, the following article must be cited.
AYDEMİR, E., YALÇINKAYA, M. A., Barua, P. D., BAYĞIN, M., Faust, O., DOĞAN, Ş., … Udyavara, R. A. (2022). Hybrid Deep Feature Generation for Appropriate Face Mask Use Detection. International Journal of Environmental Research and Public Health, 19(4), Doi: https://dx.doi.org/10.3390/ijerph19041939"	2	6	0	emrahaydemr	mask-face-dataset-no-individual-improper
997	997	Project OpenCV		['computer science', 'programming']		1	15	0	kartikeyasharma27	project-opencv
998	998	Nifty50 Index Data (26 June 2009 - 31 Dec 2021)	Nifty50, since the computation was changed to a free-float methodology.	['india', 'intermediate', 'time series analysis', 'tabular data', 'investing']	"Context
Nifty50 is onr of the two main Indian stock market benchmark indices. It is studied and analyzed by broad audience from hobbist to professionals.
Content
Nifty50 price history since 26 June 2009, when the computation was changed to a free-float methodology.
Acknowledgements
Thanka National Stock Exchange of India Ltd. for making data avilable freely on the NSE website."	0	19	0	scgupta	dataset-nifty50
999	999	demoWav		[]		0	0	0	g318522988	demowav
1000	1000	fbp_bigbird_rb		[]		0	5	0	shobhitupadhyaya	fbp-bigbird-rb
1001	1001	ATP Tour Ranking - Decade-wise and year-wise	A Github Dataset for ATP Tennis Tour	['tennis', 'sports', 'exploratory data analysis', 'data analytics', 'tabular data']	"The ATP Tour (known as the ATP World Tour from January 2009 until December 2018) is a worldwide top-tier tennis tour for men organized by the Association of Tennis Professionals. The second-tier tour is the ATP Challenger Tour and the third-tier is ITF Men's World Tennis Tour. The equivalent women's organisation is the WTA Tour.
The ATP Tour comprises ATP Masters 1000, ATP 500, and ATP 250.1 The ATP also oversees the ATP Challenger Tour,2 a level below the ATP Tour, and the ATP Champions Tour for seniors. Grand Slam tournaments, a small portion of the Olympic tennis tournament, the Davis Cup, and the entry-level ITF World Tennis Tour do not fall under the purview of the ATP, but are overseen by the ITF instead and the International Olympic Committee (IOC) for the Olympics. In these events, however, ATP ranking points are awarded, with the exception of the Olympics. The four-week ITF Satellite tournaments were discontinued in 2007. Players and doubles teams with the most ranking points (collected during the calendar year) play in the season-ending ATP Finals, which, from 2000–2008, was run jointly with the International Tennis Federation (ITF). The details of the professional tennis tour are:
Event   Number  Total prize money (USD) Winner's ranking points Governing body
Grand Slam  4   See individual articles 2,000   ITF
ATP Finals  1   4,450,000   1,100–1,500 ATP (2009–present)
ATP Masters 1000    9   2,450,000 to 3,645,000  1000    ATP
ATP 500 13  755,000 to 2,100,000    500 ATP
ATP 250 39  416,000 to 1,024,000    250 ATP
Olympics    1   See individual articles 0   IOC
ATP Challenger Tour 178 40,000 to 220,000   80 to 125   ATP
ITF Men's Circuit   534 10,000 and 25,000   18 to 35    ITF
The dataset is from Jeff Sackmann(https://github.com/JeffSackmann/tennis_atp)"	9	118	6	kalilurrahman	atp-tennis-player-ranking-dataset
1002	1002	COVID-19 Cases in ICU (Malaysia)	Weekly updated dataset scrapped from kpkesihatan.com	['asia', 'health', 'data visualization', 'tabular data']		100	1147	6	maisarahmohamedpauzi	covid19-cases-deaths-in-icu-malaysia
1003	1003	COVID-19 International Clinical Trials	COVID-19 Clinical Trials from WHO ICTRP, ClinicalTrials.gov, etc.	['public health', 'covid19']	"Intro
This is the most comprehensive dataset of all the Clinical Trials that are registered in the USA, China, EU, Australia, Brazil, South Korea, India, Cuba, Germany, Iran, UK, Japan, Africa, Peru, Sri Lanka, Thailand, Netherland.
The data automatically fetched, converted, and cleaned from ClinicalTrials.gov and WHO International Clinical Trials Registry Platform (ICTRP).
Context
Clinical trial registration is the practice of documenting clinical trials before they are performed in a clinical trials registry so as to combat publication bias and selective reporting.
There are more than 17 Clinical trial registries in the world. WHO ICTRP acts as a data collector and provides access to the data from all of these registries.
The Data Providers of the WHO ICTRP Dataset currently are:
ClinicalTrials.gov
Chinese Clinical Trial Register (ChiCTR)
EU Clinical Trials Register (EU-CTR)
Australian New Zealand Clinical Trials Registry (ANZCTR)
Brazilian Clinical Trials Registry (ReBec)
Clinical Research Information Service (CRiS), Republic of Korea
Clinical Trials Registry - India (CTRI)
Cuban Public Registry of Clinical Trials (RPCEC)
German Clinical Trials Register (DRKS)
Iranian Registry of Clinical Trials (IRCT)
The United Kingdoms registry (ISRCTN)
Japan Primary Registries Network (JPRN)
Pan African Clinical Trial Registry (PACTR)
Peruvian Clinical Trials Registry (REPEC)
Sri Lanka Clinical Trials Registry (SLCTR)
Thai Clinical Trials Register (TCTR)
The Netherlands National Trial Register (NTR)
The WHO's ICTRP search portal is currently down due to heavy traffic generated by the COVID-19 outbreak. They release the COVID-19 data in Excel format and I converted this data to CSV using scripts that you can find in this Github repo.
Content
Here I gathered, cleaned, and converted all available data for COVID-19 trials from WHO ICTRP and ClinicalTrials.gov. 
The WHO ICTRP dataset is best used to have access to all international trials.
The ClinicalTrials.gov dataset is best used for its comprehensive information that are in the JSON format.
This Dataset will be updated on a daily basis. Last Update: April 14, 2020.
Sample part of the JSON Data:
Acknowledgements
I appreciate @savannareid for sharing these data sources and pointing me in this direction."	2089	20339	26	panahi	covid-19-international-clinical-trials
1004	1004	APEX-Pytorch		[]	"Context
This  dataset  is   set  for    APEX-pytorch ,  and   it   will  be  updated   everyweek  automatically , so  it   is  always  up-to-date."	30	3649	16	xujingzhao	apexpytorch
1005	1005	Japanese Stocks Statistics from JPX	derived from JPX (Japan Exchange Group)	['investing']	"Statistics about Japanese stocks from JPX (Japan Exchange Group) 's website
All the contents in this dataset are JPX (Japan Exchange Group). Note that I do not own the data. 
For the details in each file, please visit the official website.
The contents are subject to be updated weekly (on Wednesday), if any."	49	675	8	code1110	japanese-stocks-statistics-from-jpx
1006	1006	ImageMatching		[]		0	3	0	muhammadmoaz	imagematching
1007	1007	faster_ma-ml_gpu8_batch4		[]		0	7	0	yuanwenjie	faster-maml-gpu8-batch4
1008	1008	Blogg5		[]		0	2	0	aimeecameron	blogg5
1009	1009	Blogg4		[]		0	2	0	aimeecameron	blogg4
1010	1010	Blogg3		[]		0	2	0	aimeecameron	blogg3
1011	1011	Blogg2		[]		0	2	0	aimeecameron	blogg2
1012	1012	Blogg1		[]		1	3	0	aimeecameron	blogg1
1013	1013	Cyclistic Bike Share Data 2021		[]		0	3	0	kabilanv11	cyclistic-bike-share-data-2021
1014	1014	Write a paper in IEEE format - How to Guide		[]		0	2	0	benalexx	write-a-paper-in-ieee-format-how-to-guide
1015	1015	Writing an impressive autobiography - 2022		[]		0	1	0	benalexx	writing-an-impressive-autobiography-2022
1016	1016	Tips to focus on while writing an autobiography		[]		0	1	0	benalexx	tips-to-focus-on-while-writing-an-autobiography
1017	1017	Write a perfect introduction for research paper		[]		0	3	0	benalexx	write-a-perfect-introduction-for-research-paper
1018	1018	House-Price-Prediction		[]		2	27	0	sreekanthmaila	house-price-prediction
1019	1019	CelebA_30k		[]		0	6	0	thanhuitha	celeba-30k
1020	1020	morph parse		['computer science', 'programming']		0	2	0	karahanahin	morph-parse
1021	1021	Global AI Challenge 2022		['earth and nature']		0	6	0	aleron751	global-ai-challenge-2022
1022	1022	OxCGRT	Oxford COVID-19 Government Response Tracker	['government', 'covid19']	"OXFORD COVID-19 GOVERNMENT RESPONSE TRACKER
Governments are taking a wide range of measures in response to the COVID-19 outbreak. The Oxford COVID-19 Government Response Tracker (OxCGRT) aims to record these unfolding responses in a rigorous, consistent way across countries and across time.
Systematic information on which governments have taken which measures, and when, can help decision-makers and citizens understand the robustness of governmental responses in a consistent way, aiding efforts to fight the pandemic. The OxCGRT collects information on several different common policy responses governments have taken, scores the stringency of such measures, and aggregates these scores into a common Stringency Index.
Data is collected from public sources by a team of dozens of Oxford University students and staff from every part of the world.
DATA
OxCGRT collects publicly available information on 11 indicators of government response, such as school closings, travel bans, or other measures. For a full description of the data and how they are collected, see this working paper.
Download
You can download the data and notes. Recommended citation for database: Hale, Thomas and Samuel Webster (2020). Oxford COVID-19 Government Response Tracker. Data use policy: Creative Commons Attribution CC BY standard.
Please note
This is an ongoing collation project of live data. If you see any inaccuracies in the underlying data please contact us on on the feedback from below. The underlying index is evolving as the situation and data evolves from country to country, and will change over time as the data gets more accurate.
GOVERNMENT RESPONSE STRINGENCY INDEX
Our baseline measure of variation in governments’ responses is the COVID-19 Government Response Stringency Index (Stringency Index). This composite measure is a simple additive score of seven indicators measured on an ordinal scale, rescaled to vary from 0 to 100. Please note that this measure is for comparative purposes only, and should not necessarily be interpreted as rating of the appropriateness or effectiveness of a country's response.
FEEDBACK
For specific feedback on the analysis or any aspect of the data please fill in this form and submit to the OxCGRT team.
This data is provided free of charge. However, please consider contributing to the COVID-19 Fund for the World Health Organisation.  You can also find out more about supporting the work of the Blavatnik School of Government.
MEDIA
View the full press release about the Oxford COVID-19 Government Response Tracker. For media enquiries contact Giulia Biasibetti.
CONTRIBUTORS
Contributors to the project include: Femi Adebola, Babu Ahamed, Dane Alivarius, Jessica Anania, Isabela Blumm, Michael Chen, Siu Cheng, James Fox, Bronwyn Gavine, Robert Gorwa, Jenna Hand, William Hart, Arkar Hein, Beatriz Kira, Maurice Kirschbaum, Finn Klebe, Dário Kuteev Moreira, Tiphaine Le Corre, Melody Leong Mei San, Pollyana Lima, Zoe Lin, Anindita Listya, Francesca Lovell-Read, Ben Luria, Oksana Mattiash, Chloë Mayoux, Lian Najami, Tim Nusser, Sophie Pearlman, Marcela Reynoso Jurado, Barbara Roggeveen, Olga Romanova, Charlotte Rougier, Lin Shi, Louisa-Madeline Singer, Helen Tatlow, Katherine Tyson, Francesca Valmorbida McSteen, Twan van der Togt, Andrew Wood, Victoria Yang and Tatsuya Yasui."	58	2610	6	ecopoiesis	oxford-covid19-government-response-tracker
1023	1023	china fund data daily	pridiction from china fund data daily	['investing']		53	3311	7	darwinwin	china-fund-data-daily
1024	1024	Bangalore COVID-19 quarantine cases	list of quarantined people from Bangalore	['arts and entertainment', 'exploratory data analysis', 'covid19']	"Context
Government of Karnataka has listen out the people who are quarantined from COVID-19
Content
The dataset contains information about Port of Origin of Journey, the respective house no., street/village, district, date of arrival, Date until quarantined at home.
Acknowledgements
This dataset was taken from page.
Do check it out for details regarding other districts of Karnataka
Inspiration
I was curious in knowing about this dataset, and to see from where most of these cases have arrived"	159	3877	13	dejokz	bangalore-covid19-cases
1025	1025	How to Use Adjectives in your Essay Effectively?		[]		0	3	0	robertnicholson	how-to-use-adjectives-in-your-essay-effectively
1026	1026	Misused Quotes in Writing That Can Cost Grades		[]		0	3	0	robertnicholson	misused-quotes-in-writing-that-can-cost-grades
1027	1027	Add Adjectives to Make your Descriptive Essay Rock		[]		0	5	0	robertnicholson	add-adjectives-to-make-your-descriptive-essay-rock
1028	1028	beirbiencoderresults1		[]		2	13	0	muennighoff	beirbiencoderresults1
1029	1029	Get Rid of Run-On Sentences and Redundancy		[]		0	4	0	robertnicholson	get-rid-of-runon-sentences-and-redundancy
1030	1030	NamedEntityRecognation		[]		0	3	1	yosefeliasshahiso	namedentityrecognation
1031	1031	gan aud2lips v2		[]		0	3	0	theayushanand	gan-aud2lips-v2
1032	1032	Data on COVID-19 (coronavirus) 	Data on COVID-19 (coronavirus) by Our World in Data	['medicine', 'covid19']	"Coronavirus Country Profiles
We built 207 country profiles which allow you to explore the statistics on the coronavirus pandemic for every country in the world.
In a fast-evolving pandemic it is not a simple matter to identify the countries that are most successful in making progress against it. Excess mortality and the rate of the confirmed deaths is what we focus on in the sections below, but for a fuller assessment a wider perspective is useful. For this purpose we track the impact of the pandemic across our publication and we built country profiles for 207 countries to study the statistics on the coronavirus pandemic for every country in the world in depth.
Each profile includes interactive visualizations, explanations of the presented metrics, and the details on the sources of the data.
Every country profile is updated daily."	496	4602	55	tunguz	data-on-covid19-coronavirus
1033	1033	COVID19 data Germany / Deutschland (RKI)	Official data for Germany released by German COVID19 authority RKI	['public health', 'business', 'covid19']	"Context
Official COVID19 data for Germany publicized by Robert Koch Institute
Offizieller Datensatz des Rober-Koch-Instituts zu COVID19-Fällen in Deutschland
I'm just linking the official upload location to Kaggle.
There already is a COVID19 dashboard with a map for Germany, based on that data:
https://npgeo-corona-npgeo-de.hub.arcgis.com/
But there certainly are more statistical questions to be answered.
I also started gathering and adding some additional data (not by RKI).
As for the columns labels in two of the three sets: they are very confusing and they are not even explained on the official upload website. Fortunately @sebastianhelm put some work into researching them:
https://www.kaggle.com/mreverybody/covid19-data-germany-robert-koch-institute/discussion/142140#808487
Content
RKI data is uploaded here (For the actual download link for the CSV
seed download button on the site):
- https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0?selectedAttribute=Datenstand
- https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/ef4b445a53c1406892257fe63129a8ea_0?geometry=-19.734%2C46.270%2C35.989%2C55.886
- https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/917fc37a709542548cc3be077a786c17_0
Additional data:
- Political measures taken and events / incidents:
https://github.com/mafleischer/covid19-robert-koch-data/blob/master/additional_data/covid19_events_measures.csv
Sources:
https://www.deutschland.de/de/news/coronavirus-in-deutschland-informationen#
Acknowledgements
Rober Koch Institute for making the data public
https://www.rki.de/
Inspiration
There are only few official and neutral sources concerning COVID19 cases in Germany, but many false claims and panic going around in the public.
Although the RKI data is publically available it is not propagated well and it is a bit hard to come across."	272	7139	16	mreverybody	covid19-data-germany-robert-koch-institute
1034	1034	Google Covid-19 mobility	Google Covid-19 mobility time series	['united states', 'internet', 'covid19']	Mobility data from Google, showing the frequentation of categories of places among the world.	41	2544	0	achyrogue	covid-mobility-time-series
1035	1035	Contrastive Language Image Pretraining by openai	CLIP: Connecting Text and Images	['social science', 'computer science', 'computer vision', 'deep learning']		11	1684	11	codenamekash	contrastive-language-image-pretraining-by-openai
1036	1036	covid_19 time series		['arts and entertainment']		28	1861	0	siddharthkushwaha	covid-19-time-series
1037	1037	ML Lab-02		['earth and nature']		1	16	1	bunnysriharsh	lab-2
1038	1038	Popular Movies of IMDb	Build Movie Recommender System	['arts and entertainment', 'movies and tv shows', 'nlp', 'data visualization', 'recommender systems']	"Introduction
TMDB.org is a crowd-sourced movie information database used by many film-related consoles, sites and apps, such as XBMC, MythTV and Plex. Dozens of media managers, mobile apps and social sites make use of its API.
TMDb lists some 80,000 films at time of writing, which is considerably fewer than IMDb. While not as complete as IMDb, it holds extensive information for most popular/Hollywood films.
This is dataset of the 10,000 most popular movies across the world has been fetched through the read API.
TMDB's free API provides for developers and their team to programmatically fetch and use TMDb's data.
Their API is  to use as long as you attribute TMDb as the source of the data and/or images. Also, they update their API from time to time.
This data set is fetched using exception handling process so the data set contains some null values as there are missing fields in the tmdb database. Thought it's good for a young analyst to deal with messing value.
Hey  analyst are you all excited?"	1564	15690	83	sankha1998	tmdb-top-10000-popular-movies-dataset
1039	1039	BibleData	The Bible in Structured Data	['religion and belief systems', 'tabular data', 'text data']	"Context
Read by students, scholars, critics, and the curious for millennia, the Holy Bible is the most translated, most widely published, and most examined text in history. Unfortunately, the information in Scripture largely has remained unstructured and not easily parsed, examined, processed, or enriched with modern technology.  This series of datasets is intended to make the information in the Bible accessible to these technologies.
Content
There are several files and, while some are complete (minus any corrections that are identified over time), many are still in development.
BibleData-Reference [complete]
Based on the Unified Scripture XML (USX) labels for each book, this dataset provides unique identifiers for each book, chapter, and verse in the 66 books of the Bible.  The reference IDs in this dataset are used throughout other BibleData datasets.  The dataset is complete (but open to corrections) as of 4/7/2021.
BibleData-Commandments [complete]
This dataset contains detailed information about the (traditionally enumerated) 613 commandments found in the Bible with chapter and verse references (keyed to the BibleData-Reference dataset) and related source texts in Hebrew, Greek, and English to facilitate individual verification and study.  This dataset is complete (but open to corrections) as of 1/31/2021.
HebrewStrongs [complete]
A dataset of all the words used in the Hebrew Bible as organized by James Strong in his Hebrew and Chaldee Dictionary.  Based on the work of Matthias Mueller (https://christthetruth.net/2013/07/15/strongs-goes-excel/), this data is organized by Strong's number, one entry per number.  This dataset is complete (but open for corrections) as of 2/29/2020.
NavesTopicalDictionary [complete]
Naves 1897 Topical Dictionary in structured data format.  This data is alphabetically organized, with one entry per topic.  This dataset is complete (but open for corrections) as of 6/5/2020.
HitchcocksBibleNamesDictionary [complete]
Roswell D. Hitchcock's Bible Names Dictionary (1869) in a structured data format.  This data is alphabetically organized, with one entry per name.  This dataset is complete (but open for corrections) as of 6/5/2020.
The Alamo Polyglot [complete]
This dataset is the work of a Bible student in San Antonio, Texas, USA, seeking to integrate the plain-text (UTF-8) data of ancient Bible manuscripts into one source and make it freely available to others for their own studies.  This dataset is complete (but open for additions or corrections!) as of 8/29/2020.
This single, parallel view of various texts and translations of Scripture includes:
- World English Bible [WEB]
- King James Version [KJV]
- Leningrad Codex [BHS]
- Jewish Publication Society [JPS 1917]
- Codex Alexandrinus
- Brenton's English Translation of Alexandrinus [BET]
- Samaritan Pentateuch [SP]
- Samaritan Pentateuch In English [SPE]
- Targum Onkelos
- Targum Onkelos in English [TOE])
BibleData-Book [in progress]
This dataset contains basic information about each of the 66 books in the Bible: book names in English, Hebrew, and Greek (along with transliterations and meanings of those names), chapter and verse counts, along with details of who wrote each book, when, and where (if known). This dataset is still in development as of 5/16/2020.
BibleData-Person [in progress]
Information about each named individual in the Bible with chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study. This list serves as the foundation for the BibleData-PersonLabel (with English, Hebrew, and Greek labels including chapter and verse) and the BibleData-PersonRelationship (noting parental, marital, or other relationships between individuals in the Bible). This dataset is incomplete (only Genesis 1:1-Psalm 1:1) but still in development as of 12/18/2021.
BibleData-PersonLabel [in progress]
This dataset contains detailed information about the English, Hebrew, and Greek labels (proper names, titles, etc.) given to individuals mentioned in the Bible (keyed to the BibleData-Person dataset) along with their meanings and the chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study. This dataset is incomplete (only Genesis 1:1-Psalm 1:1) but still in development as of 12/18/2021.
BibleData-PersonRelationship [in progress]
This dataset contains information about the relationships between individuals named in the Bible including unique identifiers for each person (keyed to the BibleData-Person dataset), the relationship type (father, son, mother, daughter, wife, husband, etc), and notes along with chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study. This dataset is incomplete (only Genesis 1:1-Psalm 1:1) but still in development as of 12/18/2021.
BibleData-PersonVerse [in progress]
This dataset contains information about the individuals named in each chapter and verse of the Bible including unique identifiers (keyed to the BibleData-Person and BibleData-PersonLabel datasets), and notes along with chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study.  This dataset is incomplete (only Genesis 1:1—Deuteronomy 34:12) but still in development as of 12/18/2021.
BibleData-Epoch [in progress]
This dataset contains information about periods of time (epochs) mentioned in the Bible including the beginning and ending years for the epoch (from anno hominis 1 at Creation), the key individual who defines the epoch (where applicable), and other notes along with chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study. This dataset is still in development as of 5/8/2021.
BibleData-Event [in progress]
This dataset contains information about key events mentioned in the Bible including (when identifiable) the year the event occurred (from anno hominis 1 at Creation), the key individual involved in the event, where the event occurred, and other notes along with chapter and verse references (keyed to the BibleData-Reference dataset) to facilitate individual verification and study. This dataset is still in development as of 5/8/2021.
Acknowledgements
I am thankful for everyone who has bookmarked these datasets on data.world, Git Hub, here, or elsewhere.  Each response represents a measure of encouragement to keep going because others find this information valuable.  A special word of thanks belongs to Mislav Vuletić, who was the first to perform an analysis on the data in 2021, find errors, and provide feedback to make these datasets better for everyone.  Thank you, sir! :)
Inspiration
These datasets are inspired by men like James Strong, Orvil Nave, and Roswell Hitchcock, who, in the 1890s without the benefit of modern technology, took the first steps towards making the information in the Bible more accessible to everyone."	60	1186	16	bradystephenson	bibledata
1040	1040	dataset		[]		0	4	0	daksh2998	dataset
1041	1041	lr csv		[]		0	10	0	sathishkumar12	lr-csv
1042	1042	autogluondata		[]		0	3	0	leonshangguan	autogluondata
1043	1043	whale-tfrecords-768		[]		1	5	0	manojprabhaakr	whale-tfrecords-768
1044	1044	Countries income share of the richest 1 %💸	List of countries by share of income of the richest one percent	['business']		1	8	1	imnoob	countries-income-share-of-the-richest-1
1045	1045	Gold_Daily .csv		[]		0	2	0	biswal127	gold-daily-csv
1046	1046	Flipkart_reviews_for_sentiment_analysis		['websites', 'exploratory data analysis', 'tabular data', 'text data', 'ratings and reviews']		1	24	6	lakhankumawat	flipkart-reviews-for-sentiment-analysis
1047	1047	dependencies		[]		0	5	0	leonshangguan	dependencies
1048	1048	transformers-cache-bigbird		['movies and tv shows']		0	6	1	bobber	transformers-cache-bigbird
1049	1049	Steam and Steam Spy raw datasets	Extracted from Steam Spy and Steam Store APIs	['video games', 'data cleaning', 'tabular data', 'json', 'pandas']	"I followed the work from Nik Davis, with minor diferences in the gathering code.
See https://www.kaggle.com/nikdavis/steam-store-raw
This is a raw data extraction of all GAME ids and relevant information from the Steam Store API and Steam Spy API.
The data extraction will be documented here (not yet): https://github.com/Duerkos/steam_analysis"	12	95	3	vicentearce	steam-and-steam-spy-raw-datasets
1050	1050	2016 SOI Tax Stats	Selected income and tax items. 	['finance', 'demographics', 'economics']	"Context
Learning how to create a data pipeline.
Content
https://www.irs.gov/pub/irs-soi/16zpdoc.doc
Individual Income Tax ZIP Code Data
ZIP Code data show selected income and tax items classified by State, ZIP Code, and size of adjusted gross income. Data are based on individual income tax returns filed with the IRS and are available for Tax Years 2016. The data include items, such as:
-Number of returns, which approximates the number of households
-Number of personal exemptions, which approximates the population
-Adjusted gross income 
-Wages and salaries
-Dividends before exclusion
-Interest received
Acknowledgements
https://www.irs.gov/statistics"	74	3887	0	dmwvanity	2016-soi-tax-stats
1051	1051	Understat Data for Teams + Players (2014- present)	understat.com data for all seasons from 2014- present  in csv format. 	['football', 'sports', 'internet', 'data analytics', 'online communities']	"This dataset holds the relevant understat.com data for all seasons from 2014- present for all the players and teams they play for. The data available is for English Premier League only. As those were the only data I needed for a separate project I am doing. Regardless, if anybody needs data for other leagues as well, raise an issue, I will try to add them as well. Once the season starts I will update the data after each game-week. I will be updating the data for each game-week once the new season starts.
What is available in this dataset:
Data for each player for last 3,5 and 10 GW
Data for individual player for whole season
Data for individual team going from last season up to 2014/15 season based on where they finished in the respective season
Each teams complete data from 2014/15 to 2019/20 season 
Source: understat.com"	386	3963	31	abrarhossainhimself	understat-data-for-teams-players-2014-present
1052	1052	BiLSTMtpu_fold3		[]		0	1	0	mahdibb	bilstmtpu-fold3
1053	1053	2020-21 NBA Player Stats: Per Game🏀	It is about the NBA player wise performance	['basketball']	"You can visit this website for more details 
https://www.basketball-reference.com/leagues/NBA_2021_per_game.html"	3	13	1	imnoob	202021-nba-player-stats-per-game
1054	1054	tilling8x8		[]		1	11	0	truonghuymai	tilling8x8
1055	1055	Spam Email	csv file containing 5571 spam/ham emails.	['categorical data', 'binary classification']		2	14	1	rhitazajana	spam-email
1056	1056	visdrone-label		['business']		0	3	0	limjunhao	visdronelabel
1057	1057	stimuliBrain		[]		2	150	1	saad2714	stimulibrain
1058	1058	wordnet		[]		0	3	0	bobber	wordnet
1059	1059	yolor_pt	Official weight for yolor	[]		1	23	1	wangyuecn	yolor-pt
1060	1060	100K Tinder Swindler Tweets	Tweets regarding the topic Tinder Swindler	['movies and tv shows', 'online communities', 'social networks']	"Image Credits
source
Context
The Tinder Swindler is a viral British true crime documentary film directed by Felicity Morris and was released on Netflix on 2 February 2022. The film tells the story of the Israeli con artist Simon Leviev who uses the dating-application Tinder to locate individuals he emotionally manipulates into providing financial support for his lavish lifestyle.
Dataset
Hi all, so time for some interesting and fun dataset, the Dataset contains tweets in context to the documentary ""The Tinder Swindler"""	81	973	16	deepcontractor	100k-tinder-swindler-tweets
1061	1061	ECE657AW20-ASG4-Coronavirus	Data is the Fuel, Algorithms are the Engine, Knowledge is the Destination 	['earth and nature', 'artificial intelligence', 'computer science', 'deep learning', 'covid19']	"COVID-19 Data for Analysis and Machine Learning
There are lots of datasets online, more growing every day, to help us all get a handle on this pandemic. Here are just a few links to data we've found that students in ECE 657A, and anyone else who finds their way here, can play with and practice their machine learning skills.
The main dataset is the COVID-19 dataset from John Hopkins university.  This data is perfect for time series analysis and Recurrent Neural Networks, the final topic in the course. This dataset will be left public so anyone can see it but to join you must request the link from Prof. Crowley or be in the ECE 657A W20 course at the University of Waterloo. 
For ECE 657A W20 Students
Your bonus grade for assignment 4 comes from creating a kernel from this dataset and writing up some useful analysis and publishing that notebook. You can do any kind of analysis you like but some good places to start are
- Analysis: feature extraction and analysis of the data to look for patterns that aren't evident from the original features (this is hard for the simple spread/infection/death data since there aren't that many features)
- Other Data: utilize any other datasets in your kernels by loading data about the countries themselves (population, density, wealthy etc.) or their responses to the situation. Tip: If you open a New Notebook related to this dataset you can easily add new data available on Kaggle and link that to you analysis. 
   - HOW'S MY FLATTENING COVID19 DATASET - This dataset has a lot more files and includes a lot of what I was talking about, so if you produce good kernels there you can also count them for your asg4 grade. https://www.kaggle.com/howsmyflattening/covid19-challenges
- Predict: make predictions about confirmed cases, deaths, recoveries or other metrics for the future. You can test you models by training on the past and predicting on the following days, then post a prediction for tomorrow or the next few days given ALL the data up to this point. Hopefully the datasets we've linked here will updated automatically so your kernels would update as well.
- Create Tasks: you can make your own ""Tasks"" as part of this kaggle and propose your own solution to it. Then others can try solving it as well.
- Groups: students can do this assignment either in the same groups they had for assignment 3 or individually.
Suggest other datasets
We're happy to add other relevant data to this Kaggle, in particular it would be great to integrate live data on the following:
- Progression of each country/region/city in ""days since X Level"" such as Days since 100 confirmed cases, see the link for a great example such a dataset being plotted. I haven't see a live link to a csv of that data, but we could generate.
- Mitigation Policies enacted by local governments in each city/region/country. These are dates when that region first enacted Level 1, 2, 3, 4 containment, or started encouraging social distancing or the date when they closed different levels of schools, pubs, restaurants etc.
- The hidden positives: this would be a dataset, or method for estimating, as described by Emtiyaz Khan in this twitter thread. The idea is, how many unreported or unconfirmed cases are there in any region, and can we build an estimate of that number using other regions with widespread testing as a baseline and the death rates which are like an observation of a process with a hidden variable or true infection rate.
    - Paper discussing one way to compute this : https://cmmid.github.io/topics/covid19/severity/global_cfr_estimates.html"	295	9911	21	markcrowley	ece657aw20asg4coronavirus
1062	1062	2019 Canadian Federal Election Results	Voting results by riding	['politics']		16	2227	1	ijensen	2019-canadian-federal-election-results
1063	1063	zijing_dataset		[]		0	3	0	zijinggao123	aaaaa
1064	1064	MTAT MFCC	Multilabel Classification | Audio Data	['music', 'categorical data', 'multilabel classification']	"TO BE UPDATED...
Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	10	0	mantasu	mtat-mfcc
1065	1065	272_lec6		[]		0	0	0	chenmei1120	272-lec6
1066	1066	faceattention		[]		0	6	0	luatvytran	faceattention
1067	1067	google_bigbird_roberta_base		[]		0	3	0	shobhitupadhyaya	google-bigbird-roberta-base
1068	1068	Week-6		[]		2	254	0	doclogan	week6
1069	1069	GOOGLE MOBILITY DATA 	Understand how people are moving due to COVID	['data analytics', 'tabular data', 'covid19']	"Context
As global communities respond to COVID-19, we've heard from public health officials that the same type of aggregated, anonymized insights we use in products such as Google Maps could be helpful as they make critical decisions to combat COVID-19.
These Community Mobility Reports aim to provide insights into what has changed in response to policies aimed at combating COVID-19. The reports chart movement trends over time by geography, across different categories of places such as retail and recreation, groceries and pharmacies, parks, transit stations, workplaces, and residential. (https://www.google.com/covid19/mobility/)
Content
The data contains aggregated and anonymised aggregated data per day for each country. For say accessing data for India - the files 2020_IN_Region_Mobility_Report.csv for 2020 data and 2021_IN_Region_Mobility_Report.csv. The aggregated data is not only present at country level, but also at States and district level - as given in sub_region_1 and sub_region_2.
Acknowledgements
This data from report published by Google. https://www.google.com/covid19/mobility/
Inspiration
Some Questions to answer
India is having its Second Wave and one of the major causes is considered to the election rallies held in different parts of the country. How does Mobility Impact the COVID Cases?
Comparing Mobility across different Countries"	178	3339	21	aiswaryaramachandran	google-mobility-data
1070	1070	Coronavirus infection in Finland	information about coronavirus in Finland	['covid19']	"This is data about coronavirus in Finland.
References:
* original article about the data: https://www.hs.fi/kotimaa/art-2000006433221.html
* github with the data itself: https://github.com/HS-Datadesk/koronavirus-avoindata
Lisenssi: MIT-lisenssi
Copyright 2020 Helsingin Sanomat"	157	5895	41	artgor	coronavirus-infection-in-finland
1071	1071	Keras Toolkit		[]		3	724	0	xhlulu	keras-toolkit
1072	1072	Chess Puzzles	Lichess Puzzle Database	['board games', 'puzzles']		12	990	0	reinism	lichess-database-puzzles
1073	1073	Advanced-Statistical-Modelling	Practicing Advanced Statistical Modelling in Python (Machine Learning I)	[]		15	2310	0	rodrigonca	advancedstatisticalmodelling
1074	1074	The BeeAudio Dataset: Clips of Beehive Sounds	Apis mellifera with sound files, temp, humidity, location, date and more labels	['united states', 'earth and nature', 'environment', 'audio data']		0	39	2	annajyang	beehive-sounds
1075	1075	GBR Starfish TFRecords Mini 1Xs 0 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-0-0
1076	1076	GBR Starfish TFRecords Mini 1Xs 0 1		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-0-1
1077	1077	GBR Starfish TFRecords Mini 1Xs 2 1		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-2-1
1078	1078	GBR Starfish TFRecords Mini 1Xs 1 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-1-0
1079	1079	GBR Starfish TFRecords Mini 1Xs 2 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-2-0
1080	1080	US states county wise COVID19 Cases Data		[]		1	4	0	amirtai	us-states-county-wise-covid19-cases-data
1081	1081	GBR Starfish TFRecords Mini 1Xs 1 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-1xs-1-1
1082	1082	GBR Starfish TFRecords Mini 1X 1 2		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-1x-1-2
1083	1083	GBR Starfish TFRecords Mini 1X 2 2		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-1x-2-2
1084	1084	GBR Starfish TFRecords Mini 0_7Xs 0 0		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-0-7xs-0-0
1085	1085	GBR Starfish TFRecords Mini 0_7Xs 2 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-0-7xs-2-0
1086	1086	GBR Starfish TFRecords Mini 0_7Xs 1 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-0-7xs-1-0
1087	1087	GBR Starfish TFRecords Mini 0_5Xs 0 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-0-5xs-0-0
1088	1088	GBR Starfish TFRecords Mini 0_5Xs 2 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-0-5xs-2-0
1089	1089	GBR Starfish TFRecords Mini 0_5Xs 1 0		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-0-5xs-1-0
1090	1090	blurr-master		['arts and entertainment']		0	3	3	bobber	blurr-master
1091	1091	PyTorch-Optimizer	collection of optimizers for PyTorch compatible with optim module	[]		0	858	1	miklgr500	pytorchoptimizer
1092	1092	Car Sale 	Car Sale data from Good car bad car website	[]		5	18	1	rahh2333	car-sale
1093	1093	test-three-class-model		[]		0	3	0	tuanledinh	testthreeclassmodel
1094	1094	Github_top30_dataset( https://github.com/topics)	Fetch top repos and returns all the required info about a repository	['arts and entertainment']		2	25	0	jokervb	github-top30-dataset-httpsgithubcomtopics
1095	1095	tgnrd2		[]		0	36	0	v1olet2	tgnrd2
1096	1096	H&M Competition Data in Parquet Format (merged)	All data is merged and converted to parquet format.	['business', 'retail and shopping']	Original competition data cannot be merged using Kaggle. I downloaded the competition files transactions_train.csv, customers.csv, articles.csv, merged them into a single parquet data file. Have fun and good luck!	4	46	4	impostorengineer	hm-competition-data-in-parquet-format-merged
1097	1097	NVidia - Stock Data - Latest and Updated	Stock Performance since IPO - Latest 	['business', 'science and technology', 'computer science', 'deep learning', 'investing', 'gpu']	"Nvidia Corporation is an American multinational technology company incorporated in Delaware and based in Santa Clara, California.
It designs graphics processing units (GPUs) for the gaming and professional markets, as well as system on a chip units (SoCs) for the mobile computing and automotive market. 
Its primary GPU line, labeled ""GeForce"", is in direct competition with the GPUs of the ""Radeon"" brand by Advanced Micro Devices (AMD). Nvidia expanded its presence in the gaming industry with its handheld game consoles Shield Portable, Shield Tablet, and Shield Android TV and its cloud gaming service GeForce Now. 
Its professional line of GPUs are used in workstations for applications in such fields as architecture, engineering and construction, media and entertainment, automotive, scientific research, and manufacturing design.
In addition to GPU manufacturing, Nvidia provides an application programming interface (API) called CUDA that allows the creation of massively parallel programs which utilize GPUs.They are deployed in supercomputing sites around the world. More recently, it has moved into the mobile computing market, where it produces Tegra mobile processors for smartphones and tablets as well as vehicle navigation and entertainment systems.It recently acquired ARM
Let us analyze the performance of this solid star!"	232	2173	16	kalilurrahman	nvidia-stock-data-latest-and-updated
1098	1098	kaggle feedback comments	Transformed Feature Launch/Update discussion posts and replies 	['computer science']	"Context
This dataset  contains ""Feature Launch/Update"" forum posts and replies  from meta-kaggle dataset which is transformed by this notebook using scheduling notebook feature on kaggle.
Content
contains 2 files - forum_messages.csv , forum_topics.csv
forum_messages.csv
Id
ForumTopicId
PostUserId
PostDate
ReplyToForumMessageId
Message
Medal
MedalAwardDate
forum_topics.csv
Id
ForumId
KernelId
LastForumMessageId
FirstForumMessageId
CreationDate
LastCommentDate
Title
IsSticky
TotalViews
Score
TotalMessages
TotalReplies
feedback_type"	3	359	1	yashvi	kaggle-feedback-comments
1099	1099	Rough Skate Footage	 BATBs cut into clips, and labelled for video classification. And raw SkaterXL.	['arts and entertainment', 'video games', 'sports', 'transportation']	"Context
I had recently shifted to a new city, which completely lacks any skateboarding scene. So, I thought I'll finally realize my long-standing idea for (creating a bot for SkaterXL)[https://github.com/Marceline/SkateBrot]. This dataset got created from my initial tries/thoughts for the same. I quickly realized that labelling data is something that's crucial as well as time consuming. So, I made a habit of doing a little bit of it everyday, even though I knew that my methods might render most of it useless for actual consumption. But hey, you live and learn, and skate!
Content
So, there are two types of data here, real footage of skaters almost all of it from the same park(Berrics). And artificial Skate footage from SkaterXL.
The real footage is made up of short(&lt;10s) clips, consisting of exactly one trick. With the filename being that trick's name. Further, tricks with lots of representation have gotten their own subdirectories. This data is scarce, and is provided with the intention of being used for testing with respect to the real world.
Artificial footage is meant to be trained on, and it contains three phases. The earliest is freeform footage from one of my friends playing SkaterXL. Then comes footage where clips show attempts at a particular line, i.e. they start from the same point in the map. Finally, come the part where each clip is paired with a csv file containing (grabbed) inputs for that clip.
Acknowledgements
The real footage comes from the Battle at the Berrics that I'd downloaded due to my ISPs being flakey.
The SkaterXL footage is made possible by Tanyut Huidrom.
Inspiration
Note that all of the SkaterXL clips are technically self sufficient for training. The instantaneous input is represented by the in-game controller overlay(X) in the bottom-left, and tricks(y) are shown on top-left. But there are three problems. First is, that both overlays are translucent, and depend heavily on the map being played (which in themselves are colorfully diverse). Secondly, the controller-overlay has many different variants (only two in this dataset though). Finally, the in-game trick overlay is often not the label, as it can (very rarely) misidentify, is on-screen for a duration, and contains a history as well.
The following are my task ideas, some of which I'm always trying to implement:
1. Given SkaterXL footage, and starting on the same map at the same location. Extract the controls that would generate it.
2. Do 1., but also validate by replaying the controls in the game for real. Note that SkaterXL only supports Windows (though, it's written in Unity)If you have any idea on how I can do this playing-the-game on cloud, please do tell.
3. Use the model thus trained to watch in-game skaters, and then predict the controls that will generate a different set of tricks, but go through approximately the same region on the game map.
4. Modify the model trained using above stuff to classify tricks in the actual footage. (Modify the above model, because labelling actual footage is hard, while SkaterXL footage has reasonably decent auto-labelling due to the Game)."	15	1368	3	icebearisin	raw-skates
1100	1100	News Articles Corpus	More than 800k news articles	['news']	"Data
The following data is a folder of more than 800k TXT files. Each file contains a news article from a variety of different sources. You can use this data from many tasks, such as training tokenizers or language models!"	13	268	17	sbhatti	news-articles-corpus
1101	1101	213hmm		[]		4	5	0	carlosmartius	213hmm
1102	1102	Canada 2021 Census (Province/City Pops + More)	Province, Territories, Census Divisions, and Census Subdivisions - mult metrics	['social science']	"Content
Canada's 2021 Census Data: Population, Private Dwelling Count, and more! For Provides, Territories Census Divisions and Census Subdivisions. Explore Canada's Wonderful populations centres! Which cities are booming? Which cities are Busting?
Acknowledgements
Data retrieved from: 
Statistics Canada. 2022.Census Profile. 2021 Census. Statistics Canada Catalogue no. 98 - 316 - X2021001.Ottawa.Released February 9, 2022. https://www12.statcan.gc.ca/census-recensement/2021/dp-pd/prof/index.cfm?Lang=E"	0	9	0	jonathanmoore2	canada-2021-census-provincecity-pops-more
1103	1103	Malaysia Vaccination Progress	Malaysia Immunisation Progress, as reported by JKJAV	['public health', 'beginner', 'data visualization', 'data analytics', 'tabular data', 'public safety', 'covid19']	"Data on Malaysia COVID-19 (coronavirus) vaccinations
All data are collected from JKJAV Social Media and updated daily. 
Population Data
The population data are calculated from the daily registration infographic percentage, and rounded to the nearest number.
Registered Data
The registered data consists of daily registered data, starting 7 March 2021. 
Vaccination Data
The vaccination data consists of daily vaccination data, starting 3 March 2021. Every states has two column, namely `dose1_"	605	3919	33	koayhongvin	malaysia-vaccination-progress
1104	1104	HtcViveRawData		[]		1	9	1	woodyloks	htcviverawdata
1105	1105	tilling_cots		[]		10	22	0	truonghuymai	tilling-cots
1106	1106	NLPretext	All the text preprocessing functions you need to ease your NLP project	['software', 'nlp', 'text mining', 'text data']		3	675	14	debarshichanda	nlpretext
1107	1107	Haberman		['education']		2	3	3	sivianil	haberman
1108	1108	bahasa tangan sibi a-j		['movies and tv shows']		0	2	0	tiyokprasetyo	bahasa-tangan-sibi-aj
1109	1109	The 2022 Freedom Convoy on Reddit	/r/Ottawa's sentiment on the ongoing COVID protests	['arts and entertainment', 'text data', 'online communities', 'social networks', 'covid19', 'canada']	"Context
The 2022 trucker strike, also known as the Freedom Convoy 2022, has become an internet sensation overnight. From spawning a slew of memes to gathering major industry mogul support to getting its own Wikipedia page, it has clearly become an internationally significant event.
This dataset aims to provide data scientists with the textual reactions of local redditors occupying the subreddit /r/Ottawa.
Content
This dataset contains all the comments under the ongoing Freedom Convoy megathreads in /r/Ottawa.
The dataset was procured using SocialGrep's API.
To preserve users' anonymity and to prevent targeted harassment, the data does not include usernames.
Acknowledgements
We would like to thank Zetong Li for generously providing the cover image for this dataset.
Inspiration
What are the Ottawa redditor community's views on the ongoing protest? What facets of their sentiment are most prominent?"	24	503	8	pavellexyr	the-2022-trucker-strike-on-reddit
1110	1110	250x250data		[]		0	3	0	fidanmusazade	250x250data
1111	1111	intent-clf-test		[]		0	6	0	gsri30	intentclftest
1112	1112	NTM: Miami-Dade Transit		['music', 'transportation', 'travel']	"NTM: Miami-Dade Transit
Metadata Updated: January 30, 2022
Agency Name: Miami-Dade Transit NTD Name: County of Miami-Dade , dba: Transportation & Public Work NTD ID: 40034 Feed ID: 40034 GTFS: Y GTFS URL: http://www.miamidade.gov/transit/googletransit/current/google_transit.zip Agency URL: http://www.miamidade.gov/transit/ Region: Miami, FL City: Miami State: FL"	0	1	3	qusaybtoush	ntm-miamidade-transit
1113	1113	DYCD after-school programs: Jobs and Internships		['business', 'logistic regression', 'linear regression', 'jobs and career']	"DYCD after-school programs: Jobs and Internships
Facilities in New York City, by agency and site, that offer the following after-school job and internship programs: Summer Youth Employment, In-School Youth Employment (ISY), Out-of-School Youth Employment (OSY), Youth Employment, and Adult Employment Programs for children in age groups 14 to 24, 16 to 21, children in all grades, and adults."	0	3	3	qusaybtoush	dycd-afterschool-programs-jobs-and-internships
1114	1114	Recommended Fishing Lakes and Ponds		['business', 'logistic regression', 'linear regression', 'matplotlib', 'pandas']	"Recommended Fishing Lakes and Ponds
This data displays the locations of top lakes and ponds for fishing in New York State, as determined by fisheries biologists working for the New York State Department of Environmental Conservation. Although every effort has been made to ensure the accuracy of information, errors may be reflected in the data supplied. The user must be aware of data conditions and bear responsibility for the appropriate use of the information with respect to possible errors, original map scale, collection methodology, currency of data, and other conditions."	0	2	3	qusaybtoush	recommended-fishing-lakes-and-ponds
1115	1115	COVID-19 Hospital Data		['logistic regression', 'linear regression', 'hospitals and treatment centers', 'pandas', 'seaborn']	"COVID-19 Hospital Data
Data is from the California COVID-19 State Dashboard at https://covid19.ca.gov/state-dashboard/
Note: Hospitalization counts include all patients diagnosed with COVID-19 during their stay. This does not necessarily mean they were hospitalized because of COVID-19 complications or that they experienced COVID-19 symptoms.
Note: Cumulative totals are not available due to the fact that hospitals report the total number of patients each day (as opposed to new patients)."	3	27	3	qusaybtoush	covid19-hospital-data
1116	1116	Dados csv clientes banco stone		[]		0	1	0	caiodouglas	dados-csv-clientes-banco-stone
1117	1117	Gifts Of Travel		['business', 'logistic regression', 'linear regression', 'pandas', 'plotly']	"Gifts Of Travel
Under San Francisco Campaign & Governmental Conduct Code Section 3.216(d) elected officers are required to file the Gifts of Travel form before accepting a gift of transportation, lodging, or subsistence for any out-of-state travel that is paid for in part by an individual or entity other than the City and County of San Francisco, another governmental body, or a bona-fide educational institution as defined in section 203 of the Revenue and Taxation Code, or that is paid for by the City in whole or in part with funds donated by any of those individuals or entities. An elected officer who reimburses an individual or entity for a gift of transportation, lodging or subsistence related to out-of-state travel must also file this form within 30 days of the reimbursement. For more information visit www.sfethics.org.
Each row on this table represents the contents of a Gifts of Travel form filed with the Ethics Commission. There are two sub-tables of information on the Gifts of Travel form: Accompanying Persons and Contributors. These sub-tables are provided in JSON format in the ""AccompanyingPersons"" and ""Contributors"" fields. The same information may also be accessed via the endpoints provided in ""AccompanyingPersonsSubTableQuery"" and ""ContributorsSubTableQuery"" fields to separate datasets: Accompanying Persons ( https://data.sfgov.org/City-Management-and-Ethics/Gifts-of-Travel-Accompanying-Individuals/gqui-q3n5 )Contributors ( https://data.sfgov.org/City-Management-and-Ethics/Gifts-of-Travel-Contributors/hpj3-vnwh ). Join the this dataset with the sub-tables using the DocuSignID."	0	9	5	qusaybtoush	gifts-of-travel
1118	1118	new data		['internet']		1	13	0	eslamomar	new-data
1119	1119	Spotify Charts	A dataset of all daily hit charts curated by Spotify	['arts and entertainment', 'music', 'education']	"Content
This is a complete dataset of all the ""Top 200"" and ""Viral 50"" charts published globally by Spotify. Spotify publishes a new chart every 2-3 days. This is its entire collection since January 1, 2017.
Note
The value of streams is NULL when the chart column is ""viral50"".
Acknowledgement
Image Credits: Photo by Omid Armin on Unsplash"	2286	20223	98	dhruvildave	spotify-charts
1120	1120	Zoo-PennState_DAAN		[]		0	11	0	kausalyakavuri	zoopennstate-daan
1121	1121	divvy_tripdata	Cycle Transportation Data	['business', 'transportation', 'bigquery']		0	9	0	johnmestyanek	divvy-tripdata
1122	1122	Recommendations		[]		0	0	0	emilinberg	recommendations
1123	1123	PL pay and performance		[]		0	2	1	bhavishsalia	pl-pay-and-performance
1124	1124	Grapevine Leaves Image Dataset	Grapevine Leaves Image Dataset (Ak, Ala Idris, Büzgülü, Dimnit, Nazli)	['computer science', 'computer vision', 'classification', 'image data', 'multiclass classification', 'transfer learning']	"Grapevine Leaves Image Dataset
KOKLU Murat (a), UNLERSEN M. Fahri (b), OZKAN Ilker Ali (a), ASLAN M. Fatih(c), SABANCI Kadir (c)
(a) Department of Computer Engineering, Selcuk University, Turkey, Konya, Turkey
(b) Department of Electrical and Electronics Engineering, Necmettin Erbakan University, Konya, Turkey
(c) Department of Electrical-Electronic Engineering, Karamanoglu Mehmetbey University, Karaman, Turkey
Citation Request :
Koklu, M., Unlersen, M. F., Ozkan, I. A., Aslan, M. F., & Sabanci, K. (2022). A CNN-SVM study based on selected deep features for grapevine leaves classification. Measurement, 188, 110425. Doi:https://doi.org/10.1016/j.measurement.2021.110425
Link: https://doi.org/10.1016/j.measurement.2021.110425
Highlights
• Classification of five classes of grapevine leaves by MobileNetv2 CNN Model.
• Classification of features using SVMs with different kernel functions.
• Implementing a feature selection algorithm for high classification percentage.
• Classification with highest accuracy using CNN-SVM Cubic model.
Abstract: The main product of grapevines is grapes that are consumed fresh or processed. In addition, grapevine leaves are harvested once a year as a by-product. The species of grapevine leaves are important in terms of price and taste. In this study, deep learning-based classification is conducted by using images of grapevine leaves. For this purpose, images of 500 vine leaves belonging to 5 species were taken with a special self-illuminating system. Later, this number was increased to 2500 with data augmentation methods. The classification was conducted with a state-of-art CNN model fine-tuned MobileNetv2. As the second approach, features were extracted from pre-trained MobileNetv2′s Logits layer and classification was made using various SVM kernels. As the third approach, 1000 features extracted from MobileNetv2′s Logits layer were selected by the Chi-Squares method and reduced to 250. Then, classification was made with various SVM kernels using the selected features. The most successful method was obtained by extracting features from the Logits layer and reducing the feature with the Chi-Squares method. The most successful SVM kernel was Cubic. The classification success of the system has been determined as 97.60%. It was observed that feature selection increased the classification success although the number of features used in classification decreased.
Keywords: Deep learning, Transfer learning, SVM, Grapevine leaves, Leaf identification"	13	200	20	mkoklu42	grapevine-leaves-image-dataset
1125	1125	UbiquantFloat16Feather		[]		0	2	0	victoraugustocavalli	ubiquantfloat16feather
1126	1126	Acoustic Extinguisher Fire Dataset	Acoustic Extinguisher Fire Dataset	['categorical data', 'earth and nature', 'artificial intelligence', 'computer science', 'binary classification']	"Yavuz Selim TASPINAR, Murat KOKLU and Mustafa ALTIN
Citation Request :
1: KOKLU M., TASPINAR Y.S.,  (2021).  Determining the Extinguishing Status of Fuel Flames With Sound Wave by Machine Learning Methods.  IEEE Access, 9, pp.86207-86216, Doi: 10.1109/ACCESS.2021.3088612
Link: https://ieeexplore.ieee.org/document/9452168 (Open Access)
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9452168
2: TASPINAR Y.S., KOKLU M., ALTIN M., (2021).  Classification of Flame Extinction Based on Acoustic Oscillations using Artificial Intelligence Methods.  Case Studies in Thermal Engineering, 28, 101561, Doi: 10.1016/j.csite.2021.101561
Link: https://www.sciencedirect.com/science/article/pii/S2214157X21007243  (Open Access) https://www.sciencedirect.com/sdfe/reader/pii/S2214157X21007243/pdf
3: TASPINAR Y.S., KOKLU M., ALTIN M., (2022).  Acoustic-Driven Airflow Flame Extinguishing System Design and Analysis of Capabilities of Low Frequency in Different Fuels.  Fire Technology, Doi: 10.1007/s10694-021-01208-9
Link: https://link.springer.com/content/pdf/10.1007/s10694-021-01208-9.pdf
SHORT DESCRIPTION: The dataset was obtained as a result of the extinguishing tests of four different fuel flames with a sound wave extinguishing system. The sound wave fire-extinguishing system consists of 4 subwoofers with a total power of 4,000 Watt placed in the collimator cabinet. There are two amplifiers that enable the sound come to these subwoofers as boosted. Power supply that powers the system and filter circuit ensuring that the sound frequencies are properly transmitted to the system is located within the control unit. While computer is used as frequency source, anemometer was used to measure the airflow resulted from sound waves during the extinguishing phase of the flame, and a decibel meter to measure the sound intensity. An infrared thermometer was used to measure the temperature of the flame and the fuel can, and a camera is installed to detect the extinction time of the flame. A total of 17,442 tests were conducted with this experimental setup. The experiments are planned as follows:
1. Three different liquid fuels and LPG fuel were used to create the flame.
2. 5 different sizes of liquid fuel cans are used to achieve different size of flames.
3. Half and full gas adjustment is used for LPG fuel.
4. While carrying out each experiment, the fuel container, at 10 cm distance, was moved forward up to 190 cm by increasing the distance by 10 cm each time.
5. Along with the fuel container, anemometer and decibel meter were moved forward in the same dimensions.
6. Fire extinguishing experiments was conducted with 54 different frequency sound waves at each distance and flame size.
Throughout the flame extinguishing experiments, the data obtained from each measurement device was recorded and a dataset was created. The dataset includes the features of fuel container size representing the flame size, fuel type, frequency, decibel, distance, airflow and flame extinction. Accordingly, 6 input features and 1 output feature will be used in models. The explanation of a total of seven features for liquid fuels in the dataset is given in Table 1, and the explanation of 7 features for LPG fuel is given in Table 2.
The status property (flame extinction or non-extinction states) can be predicted by using six features in the dataset. Status and fuel features are categorical, while other features are numerical. 8,759 of the 17,442 test results are the non-extinguishing state of the flame. 8,683 of them are the extinction state of the flame. According to these numbers, it can be said that the class distribution of the dataset is almost equal.""              
KEYWORDS: Fire, Extinguishing System, Sound wave, Machine learning, Fire safety, Low frequency, Acoustic
Data properties and descriptions for liquid fuels           
FEATURES    MIN/MAX VALUES                  UNIT    DESCRIPTIONS
SIZE        7, 12, 14, 16, 20               cm      Recorded as 7 cm=1, 12 cm=2, 14 cm=3, 16 cm=4, 20 cm=5
FUEL        Gasoline, Kerosene, Thinner             Fuel type 
DISTANCE    10 - 190                        cm  
DESIBEL     72 - 113                        dB  
AIRFLOW     0 - 17                          m/s 
FREQUENCY   1-75                            Hz  
STATUS      0, 1                                    0 indicates the non-extinction state, 1 indicates the extinction state
Data properties and descriptions for LPG            
FEATURES    MIN/MAX VALUES                                  UNIT        DESCRIPTIONS
SIZE        Half throttle setting, Full throttle setting                Reocerded as Half throttle setting=6, Full throttle setting=7 
FUEL        LPG                                                         Fuel type 
DISTANCE    10 - 190                                        cm  
DESIBEL     72 - 113                                        dB  
AIRFLOW     0 - 17                                          m/s 
FREQUENCY   1-75                                            Hz  
STATUS      0, 1                                                        0 indicates the non-extinction state, 1 indicates the extinction state"	3	19	15	mkoklu42	acoustic-extinguisher-fire-dataset
1127	1127	Juvenile Arrests by Crime	number of arrests of juveniles for various offenses	['alcohol', 'united states', 'crime', 'advanced', 'tabular data']		2	15	5	andrej0marinchenko	juvenile-arrests-by-crime
1128	1128	Happy Whale And Dolphin TFRecords 348x660		[]		3	12	0	dschettler8845	happy-whale-and-dolphin-tfrecords-348x660
1129	1129	ATV Snowblowers		[]	"ATV snowblowers are versatile machines that can be used to clear snow and ice. The heavy-duty steel construction allows you to clear even the heaviest of snow. This machine can clear 50 inches of snow and is ideal for sidewalks and driveways. However, you might need something smaller for smaller spaces. This machine also includes an electromagnetic clutch, electric controls, and a heavy-duty gear housing.
The Massimo Motors Universal ATV snow blower is a high-quality, durable attachment. It features a thick steel body with heavy-duty steel augers that can cut through heavy snow and icy drifts. It has easy-grip tires and a powerful motor that makes it easier for you to control. If you live in a colder climate, you might want to consider the Bercomac Snow Blower, a high-performance snow blower.
ATV snowblowers have a variety of features and are perfect for use in severe snowfalls. They are powered by their own engine and can work independently from your ATV. Depending on what you need, you can choose a unit that is powerful enough for your needs and fits your budget. It is important to remember that there are a variety of different models available, so you should shop around for the right one.
To do the job, an ATV snowblower should have at least 14 horsepower. You need to choose a machine that has at least 14 horsepower. This is because you will be pushing snow with it. ATV snowblowers that are top-of-the-line will have a snow angel that you can control, so you don't have to do extra work. It is important that you choose the right unit for your ATV.
Heavy snowfall can be handled by ATV snowblowers. These machines can be mounted on your ATV and are ideal for clearing large areas. ATV snowblowers are able to move up to 14 HP, which is the maximum power for a snowblower. The engine power required for ATV snowblowers is generally 4.5 HP. Some models come with an optional accessory kit to allow the user to mount the equipment on their vehicle.
ATV snowblowers are also available. They are ideal for clearing driveways and suburban areas. While single-stage models are easy to handle, they are not recommended for homes with driveways that have large snow blocks. ATV snowblowers can be a great way for winter travel. ATV snowblowers are also a great source of income. You can use an ATV to clean up your driveway.
An ATV snowblower can be attached to an ATV and is an excellent investment. It can be used to clear large areas and can also be used on an ATV. There are many models of ATV snowblowers and many types of models. The most common differences are the size and weight. However, it is important to consider the size of your ATV before purchasing an ATV snowblower. There are several models that are smaller, and the ones that fit on the front of the ATV are the ones that are smaller and larger."	0	2	0	mandyrose	atv-snowblowers
1130	1130	Gold rates (1985 - Present)	Gold rates of six major countries(India, US, UAE, China, Europe, Great Britain)	['business', 'beginner', 'intermediate', 'tabular data', 'currencies and foreign exchange']	"Context
Gold rates differ from place to place, here is the dataset which included major six countries' Gold rates. This data can also be viewed as a Time Series data.
Content
This dataset contains two csv files - 
- Daily gold rates : which includes daily data of gold rates from 1st Jan 1985 to 4th Feb 2022
- Annual average gold rates : this file includes average annual gold rate in their national currency from 1978 to 2021
The rates are in their national currencies and per troy ounce
Currencies included are USD, INR, AED, EUR, GBP, CNY
Acknowledgements
This data was collected from https://www.gold.org/goldhub and then cleaned.
Banner by Unsplash
Inspiration
Things which can be done with this data:
- Time series analysis and prediction
- EDA and Visualization of the gold rates 
- Country that has highest and lowest rate in their national currency (Latest Years)"	511	3071	42	hemil26	gold-rates-1985-jan-2022
1131	1131	Vgg16_model		[]		0	2	0	arun2729	vgg16-model
1132	1132	Images from The Lord of the Rings	2.3k+ picutres from three parts of the legendary trilogy 	['popular culture', 'arts and entertainment', 'literature', 'movies and tv shows', 'image data']	"Context
The Lord of the Rings is a legendary book written by Tolkien that became the basis of the extremely famous and popular film trilogy of the same name directed by Peter Jackson. Despite the passage of time, 3 films from the years 200-2003 enjoy great popularity and recognition in the eyes of viewers and critics, and the series has become an icon of pop culture. The database below presents a series of photos obtained on the largest portal to rate IMDb films from each of the three parts of the film trilogy.
Content
The data is grouped into folders where one folder corresponds to one movies. We have a total of 3, as we took the all 3 parts of trilogy. In each of the folders we can find pictures from pcitures from a specific movie. The number of pictures in each series differ from each other during the data on imdb. Data was obtained using webscraping. Python was used for this process with the ""BeautifulSoup"", ""requests"", ""re"", ""urllib"", and ""os"" packages. For each movie, using the preview of the HTML page, we were able to find direct links to the photos. Then, in a loop, each photo was downloaded in turn and saved to the appropriate folder with the appropriate name based on the link and the title of the part.
Inspiration
The inspiration to create a notebook based on this data may be a comparative analysis of the movies based on the pictures or the creation of a model which, based on the pictures, can predict which part they come from.
<br>
Photo by <a href=""https://unsplash.com/@zoltantasi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Zoltan Tasi</a> on <a href=""https://unsplash.com/s/photos/fantasy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	16	422	7	michau96	images-from-the-lord-of-the-rings
1133	1133	world sql		['business']		0	3	0	beaststudiooo	world
1134	1134	catsvdogs npz		[]		1	6	0	neerajkaroshi	catsvdogs-npz
1135	1135	Covid in South America - Latest Data	Covid-19 Data as on February 09, 2022	['south america', 'health', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of South America as on February 09, 2022
Attribute Information
Country - Name of South American countries, territories and islands
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population- Total Cases per 1 million of the population
Deaths/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests done per 1 million of the population
Population - Population
Source
Link : https://www.worldometers.info/coronavirus/#countries
Please appreciate the effort with an upvote 👍 
Thank You"	405	2689	47	anandhuh	covid-in-south-america-latest-data
1136	1136	catsvdogs		[]		0	4	0	neerajkaroshi	catsvdogs
1137	1137	Covid in North America - Latest Data	Covid-19 Data as on February 09, 2022	['north america', 'health', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
This dataset contains Covid-19 data of North America as on February 09, 2022
Attribute Information
Country - Name of North American countries, territories and islands
Total Cases - Total number of Covid-19 cases
Total Deaths - Total number of Deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of Active cases
Total Cases/1 mil population- Total Cases per 1 million of the population
Deaths/1 mil population - Total Deaths per 1 million of the population
Total Tests - Total number of Covid tests done
Tests/1 mil population - Covid tests done per 1 million of the population
Population - Population
Source
Link : https://www.worldometers.info/coronavirus/#countries
Please appreciate the effort with an upvote 👍 
Thank You"	508	3049	36	anandhuh	covid-in-north-america-latest-data
1138	1138	m d ds		[]		1	3	0	rishabhkhurana3046	m-d-ds
1139	1139	Latest Covid19 Data of Asian Countries	Covid-19 Data of Asian Countries as on February 09, 2022	['asia', 'beginner', 'exploratory data analysis', 'tabular data', 'covid19']	"Content
Covid-19 Data of Asian Countries as on February 09, 2022
Attribute Information
Country - Name of Asian Countries 
Total Cases - Total number of Covid cases
Total Deaths - Total number of Covid deaths
Total Recovered - Total number of recovered cases
Active Cases - Total number of active cases
Total Cases/1M population - Total cases per 1 million population
Deaths per 1M population - Total deaths per 1 million population
Total Tests - Total covid tests done
Tests/1M population - Tests per 1 million of the population
Population - Population in each country
Source
Link : https://www.worldometers.info/coronavirus/#countries
Other updated Covid datasets
Link : https://www.kaggle.com/anandhuh/datasets
If you find it useful, please support by upvoting   👍"	508	2836	51	anandhuh	latest-covid19-data-of-asian-countries
1140	1140	Accenture Stock Price - All time	Stock Performance of Accenture, PLC since it's IPO	['business', 'finance', 'data visualization', 'investing']	"Accenture plc is a Global multinational professional services company that specialises in IT services and consulting. A Fortune Global 500 company,  it reported revenues of $44.33 billion in 2020 and had 569,000 employees. In 2015, the company had about 150,000 employees in India, 48,000 in the US, and 50,000 in the Philippines. Accenture's current clients include 91 of the Fortune Global 100 and more than three-quarters of the Fortune Global 500.
Julie Sweet has served as CEO of Accenture since 1 September 2019.
It has been incorporated in Dublin, Ireland, since 2009.
Context
A lot of info available to visualize and analyze this dataset"	125	1294	15	kalilurrahman	accenture-stock-price-all-time
1141	1141	Cakey Bakey	Delicious Photos of Cakes for Multi-target Image Classification	['culture and humanities', 'image data', 'cooking and recipes', 'food']	"cake classification
Collection of Cake Images and their multi-target classes.
cake_annotated.csv contains the following entries:
1. file_name (str: unique_values) - jpg image file name 
2. cream (str:['yes', 'no'] - yes represents presence of cream and no represents otherwise
3. fruits (str:['yes', 'no'] - yes represents presence of fruits and no represents otherwise
4. sprinkle_toppings (str:['yes', 'no'] - yes represents presence of sprinkled toppings such as candies, rainbow balls and no represents otherwise
Annotations are made manually.
Non-uniform sized images of photographs of cakes. Automatically updated by fetching new collections from the source github repo. 
Acknowledgements
Collected mostly from Unsplash.com with thanks and stored at repo https://github.com/RajkumarGalaxy/cake
Inspiration
Multi-target classification is an interesting computer vision task. It requires a collection of high quality images. This collection of images of cakes and the annotations may serve the purpose."	30	596	4	rajkumarl	cakey-bakey
1142	1142	Wine Dataset		['alcohol']		1	5	0	surajchaudharistar02	wine-dataset
1143	1143	Postsecondary Completion Rates	Dataset from National Center for Education Statistics Statistics	['universities and colleges', 'education']	"About this dataset
&gt; <p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting and analyzing data related to education in the U.S. and other nations. NCES is located within the U.S. Department of Education and the Institute of Education Sciences. NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics on the condition of American education; conduct and publish reports; and review and report on education activities internationally.</p>
<ul>
<li>Table 326.10. Graduation rate from first institution attended for first-time, full-time bachelor's degree-seeking students at 4-year postsecondary institutions, by race/ethnicity, time to completion, sex, control of institution, and acceptance rate: Selected cohort entry years, 1996 through 2008</li>
<li>Table 326.20. Graduation rate from first institution attended within 150 percent of normal time for first-time, full-time degree/certificate-seeking students at 2-year postsecondary institutions, by race/ethnicity, sex, and control of institution: Selected cohort entry years, 2000 through 2011</li>
<li>Table 326.30. Retention of first-time degree-seeking undergraduates at degree-granting postsecondary institutions, by attendance status, level and control of institution, and percentage of applications accepted: Selected years, 2006 to 2014</li>
<li>Table 326.40. Percentage distribution of first-time postsecondary students starting at -2 and 4-year institutions during the 2003-04 academic year, by highest degree attained, enrollment status, and selected characteristics: Spring 2009</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://nces.ed.gov/programs/digest/current_tables.asp"" target=""_blank"" rel=""nofollow"">https://nces.ed.gov/programs/digest/current_tables.asp</a></p>
This dataset was created by National Center for Education Statistics and contains around 100 samples along with Unnamed: 27, Unnamed: 11, technical information and other features such as:
- Unnamed: 21
- Unnamed: 5
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 4 in relation to Unnamed: 34
- Study the influence of Unnamed: 6 on Unnamed: 29
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Center for Education Statistics 
Start A New Notebook!"	60	536	8	yamqwe	postsecondary-completion-ratese
1144	1144	Pedestrian, Cyclist & Vehicle Event-Based Dataset	Pedestrian, Cyclist & Vehicle Event-Based Dataset	[]		0	7	0	cargroup	pedestrian-cyclist-vehicle-eventbased-dataset
1145	1145	linear regression 		[]		0	10	0	sathishkumar12	linear-regression
1146	1146	margin_x		[]		0	20	0	kittenraidrua	margin-x
1147	1147	COVID-19 Indonesia Dataset (Case and Vaccination)	COVID-19 Daily Case and Vaccination Progress  Dataset in Indonesia	['asia', 'public health', 'health', 'data analytics', 'public safety', 'covid19']	"Context
Since the first time the Covid 19 case in Indonesia attacked, which was announced for the first time on March 2, 2020, it has affected many fields such as the economy to education. many companies or businesses are bankrupt and education is not running properly which is very likely to cause a decline in the academic ability of the nation's young generation. In 2021, Indonesia has started efforts to solve this pandemic through one way, namely vaccines. Several vaccines are used, one of which is Sinovac and will then follow the Nusantara Vaccine.
The reason I created this dataset is not only because I am still pursuing my first professional career (im fresher guys😄 )which is still unclear and also provides data which can then be processed as insight to convince the public that this pandemic will end soon🔥 .
Content
This data was scraped from several source ( I'll write it down in the Acknowledgments section) on March, 15 2021 by Thoriqul Aziz👋 
Some data represented all province in Indonesia
The records of each location (province/city/regency) have different date first case or first vaccinated. but the announcement of first case is March, 2 2020 and first vaccination in January, 13 2021
Indonesia Coronavirus daily data Columns Description:
- Date: Means the Date of Observation
- Province: Means Location/ Province which the data was observed
- Daily_Case: Means Daily new number of confirmed case in observed province
- Daily_Death: Means Daily new number of confirmed death in observed province
- Daily_Recovered: Means Daily new number of confirmed recover in observed province
- Active_Case: Means Daily new number of active case such as isolated or threated in hospital (still didnt recover nor die)
- Cumulative_Case: Means total for each day number number of confirmed case of the row's date, for the row's province
- Cumulative_Recovered: Means total for each day number number of confirmed Recover of the row's date, for the row's province
- Cumulative_Death: Means total for each day  number of confirmed death of the row's date, for the row's province
- Cumulative_Active_Case: Means total for each day number of active case of the row's date, for the row's province
Vaccination Data columns Description:
- City_or_Regency: Location which data was observed (regency or city)
- Vaccinated: Total person who get Vaccinated ( first vaccination)
- Fully_Vaccinated: Total person who get Full Vaccinated (second vaccination)
- Total: Total vaccin each day each location
- Date: Date of observation
- URL: Source of Data
Acknowledgements
All data present in this dataset is get or scraped from:
1. covid19.go.id
2. Diskominfo JATIM Twitter Account
Inspiration
From this data, you can makes some visualization from trend in several province in indonesia.
you can also combine this dataset with  other dataset like GDP Each Province or news in some point time like Lebaran, christmas day, or last PILKADA(regional head elections) in order to get insight and effect of these events on the progression of Covid-19 
Source
images cover : Unsplash"	1851	9185	50	riqulaziz	case-vaccination-covid19-indonesia-dataset
1148	1148	wbf_2022		[]		0	4	0	stavrosorfanoudakis	wbf-2022
1149	1149	reef-yolox-m-aug2-v6		[]		0	9	0	sangayb	reef-yolox-m-aug2-v6
1150	1150	Between Our Worlds	A Linked Open Dataset with the focus on anime	['anime and manga']		3	1010	3	pieterheyvaert	between-our-worlds
1151	1151	LVIS_train		[]		2	20	0	zaopolearning	lvis-train
1152	1152	PubMed		['health conditions']		3	11	0	eleldar	pubmed
1153	1153	Melanoma with Diagnosis Label	Melanoma with Diagnosis Label	[]		0	15	0	saschamet	melanoma-diagnosis
1154	1154	gap_1_data		[]		0	4	0	danukatheja	gap-1-data
1155	1155	lvis_val		[]		2	15	0	zaopolearning	lvis-val
1156	1156	Persian Classical Music Recognition	Persian Classical Music Instrument Recognition (PCMIR) Database	['music', 'beginner', 'intermediate', 'advanced', 'audio data']	"Context
Due to the lack of availability of Persian musical instruments-based databases, we decided to make one. Proposed database contains of 7 main Persian musical instruments: Ney, Tar, Santur, Kamancheh, Tonbak, Ud and Setar classes. Classes consist of 89 to 110 samples; each sample 5-10 seconds. Database is developed with the aid of musicians whom we asked to play and we recorded. Some of the pieces were played in natural places such as rooms and music shops and a few in the music studios.
Acknowledgements
Mousavi, Seyed Muhammad Hossein, and VB Surya Prasath. ""Persian Classical Music Instrument Recognition (PCMIR) Using a Novel Persian Music Database."" 2019 9th International Conference on Computer and Knowledge Engineering (ICCKE). IEEE, 2019."	8	527	13	saurabhshahane	persian-classical-music-recognition
1157	1157	dataset-reduction-svd		[]		0	32	0	claudiovaliense	dataset-reduction-svd
1158	1158	model comvis no 3		['clothing and accessories']		2	6	0	edgardjonathan	model-comvis-no-3
1159	1159	random images dataset		[]		2	191	3	soumya9977	random-images-dataset
1160	1160	Review mechanisms labour market discrimination	Dataset related to the review on ethnic labour market discrimination mechanisms	['economics', 'tabular data']	"Context
This dataset is related to the manuscript 'Is labour market discrimination against ethnic minorities better explained by taste or statistics? A systematic review of the empirical evidence'.
Content
This dataset is the fruit of a systematic research of empirical research on ethnic labour market discrimination published between 2000 and 2019.
Authorship
The data were gathered by Louis Lippens. The related paper was written by Louis Lippens, Stijn Baert, Abel Ghekiere, Pieter-Paul Verhaeghe, and Eva Derous."	0	9	1	louislippens	sysreviewmelmd
1161	1161	5 flavors of bert nbme		['standardized testing']	"fold0 bert-base-cased
fold1 albert-base-v2
fold2 google/electra-base-discriminator
fold3 microsoft/deberta-v3-base
fold4 roberta-base
used here: https://www.kaggle.com/nbroad/qa-ner-hybrid-infer-nbme"	6	39	0	nbroad	5-flavors-bert
1162	1162	iterative-stratification		['earth and nature']		0	0	0	wbrgsh	iterative-stratification
1163	1163	stellar classification		['music']		2	22	0	palak2202	stellar-classification
1164	1164	deberta v2_3 fast tokenizer		[]		23	38	2	nbroad	deberta-v2-3-fast-tokenizer
1165	1165	mydata3		[]		0	4	0	dhafermaloucheauc	mydata3
1166	1166	Applications Received For Insurance	Datasets for IAP applications received 	['healthcare', 'finance', 'health', 'insurance']	"About this dataset
&gt; <p><em>Original Title</em>: Applications Received For Insurance Affordability Programs Through Other Eligibility Pathways</p>
<p>The number of Insurance Affordability Programs (IAPs) applications received through other eligibility pathways. Other eligibility pathways include applications transferred through transition from Advanced Premium Tax Credits (APTC) to Medi-Cal, as well as applications submitted through Hospital Presumptive Eligibility (HPE), through Express Lane, or the Medi-Cal Access Program (MCAP) during a reporting period. APTC to Medi-Cal batch data is reported by CalHEERS and MEDS and consists of individuals who are no longer eligible for APTC but are eligible for Medi-Cal. Another eligibility pathway is Express Lane Eligibility (ELE), which is a program that waives the need for a Medi-Cal eligibility determination for 12 months if the individual is enrolled in CalFresh. Hospital Presumptive Eligibility (HPE) applications are submitted through qualified HPE Providers. ELE and HPE data are reported by DHCS and commencing with this report, MCAP applications are reported by MAXIMUS Inc. This dataset is part of public reporting requirements of set forth in the California Welfare and Institutions Code 14102.5.</p>
<p>Source: <a href=""http://www.dhcs.ca.gov"" target=""_blank"" rel=""nofollow"">http://www.dhcs.ca.gov</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2021-03-18<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 100 samples along with Reporting Period, Annotation Description, technical information and other features such as:
- Pathway
- Annotation Code
- and more.
How to use this dataset
&gt; - Analyze Number Of Applications in relation to Unnamed: 6
- Study the influence of Year on Reporting Period
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	43	375	3	yamqwe	applications-received-for-insurance-affordabilite
1167	1167	Louis Vuitton Daily Stock Prices 2000-2022	LVMH Moët Hennessy - Louis Vuitton, Société Européenne	['finance', 'tabular data', 'retail and shopping', 'investing']	"Context
Data were collected from Yahoo Finance. 
Content
Take a look at the stock performance of largest luxury products in the world.
Acknowledgement
Data source: Yahoo Finance (https://finance.yahoo.com/quote/MOH.F?p=MOH.F&.tsrc=fin-srch)"	51	410	7	prasertk	louis-vuitton-daily-stock-prices-20002022
1168	1168	Playing Cards Object Detection Dataset	Synthetically generated playing cards images with bounding boxes	['games', 'card games', 'computer science', 'classification', 'gambling']	"Context
Highly inspired by RAIN MAN 2.0 by Edje Electronics (https://www.youtube.com/watch?v=Nf3zBJ2cDAs) I wanted to create my own playing cards AI for Blackjack and Poker. I didn't have much prior experience to DL so I wanted to jump in straight away and train my own yolov5 model but there was no dataset provided, only the code for generating it. 
Content
First I took 20-30 second videos of all 52 cards under variable light temperature and brightness. The images were processed with open-cv. The DTD dataset (https://www.robots.ox.ac.uk/~vgg/data/dtd/) was used to simulate backgrounds of various textures for our dataset. 
The original generated dataset was in Pascal VOC format. It was uploaded to Roboflow and exported to YOLO v5 PyTorch format. 
Images are 416x416, split into train/test/valid (70/20/10 split). 
Use kaggle_data.yaml if training in kaggle; data.yaml if training on your local machine
Acknowledgements
Thanks to @geaxgx for providing a well documented jupyter notebook on generating this dataset. 
https://github.com/geaxgx/playing-card-detection"	205	1963	12	andy8744	playing-cards-object-detection-dataset
1169	1169	5 LIDARs	PCAP files obtained with 5 LIDAR sensors	[]		7	199	0	rshinkuma	5lidars
1170	1170	TrainedModel		[]		0	1	0	kaziimranahmed	trainedmodel
1171	1171	28x28 DataFrame		[]		0	5	0	bstnst99	28x28-dataframe
1172	1172	🕴 Kickstarter Campaign Success	Data on 20,632 Kickstarter campaigns on the site	['business', 'finance', 'crowdfunding']	"About this dataset
&gt; <p><strong>This project was done by Rachel Downs and Muhammad Ghuari for and MIS elective at UT Austin.</strong></p>
<h2>Project&nbsp;Objectives:</h2>
<p>Kickstarter is an online crowdfunding platform aimed at helping people get their ideas funded while building a community of fans to support their ideas. While Kickstarter publishes many advice and best-practices articles on their blog, over half of campaigns still fail.</p>
<p><em>Why does this matter?</em> Well unlike their competitor, Indiegogo, Kickstarter campaign projects follow an ""all or nothing"" funding model. This means that if a Kickstarter campaign fails, both the project creators are disappointed, as well as the people who did contribute because the project will not be completed in any capacity.</p>
<p>This project's objectives are the following:</p>
<ol>
<li>Understand the marketplace of Kickstarter including timing of campaigns posted, types of projects, location of campaigns, description of campaigns and more</li>
<li>Provide insight into attributes that set campaigns up for a higher rate of success to inform campaign creation in the future</li>
<li>Build a predictive model that allows Kickstarter to identify high-failure-risk campaigns before they fail and provide supplemental advice and material to the creators</li>
<li>Identify a business opportunity for Kickstarter by finding campaigns just below the predicted threshold of success and helping them get to their goal, helping both parties earn revenue</li>
</ol>
<h2>Methodology:</h2>
<p>This dataset contains data on 20,632 Kickstarter campaigns on the site as of February 1st 2017. Important attributes are described below:</p>
<ul>
<li>Project: a finite work with a clear goal that you’d like to bring to life (aka campaign)</li>
<li>Funding goal: amount of money that a creator needs to complete their project</li>
<li>Name: name of project on Kickstarter</li>
<li>Blurb: the short description displayed under the name of your project and on the browse page</li>
<li>Pledged and backers: amount of money that a project has raised and people that have supported it at the point of the API pull</li>
<li>State: successful, failed, cancelled, live or suspended</li>
<li>Deadline, state changed, created at, launched at: deadline given for successful funding, state changed when campaign went to success or failure, time the project was created at, time the project was launched at</li>
<li>Other attributes in this dataset: country, currency, category</li>
</ul>
<p>To attain a deeper understanding of our data and to have more attributes to explore, we also created the following features out of the data for our analysis:</p>
<ul>
<li>Name and blurb (description) length including and excluding “stop words” - name_len_clean, blurb_len_clean</li>
<li>Day of week and hour of the day for creation, launch and deadline date - deadline_weekday, created_at_weekday, lauched_at_weekday, deadline_hour, created_at_hour, launched_at_hour</li>
<li>Days between creation and launch, and days between launch and deadline - create_to_launch, launch_to_deadline</li>
</ul>
<h2><a href=""https://data.world/rdowns26/kickstarter-campaigns/file/Term%20Project%20Report.pdf"">Read our project report.</a></h2>
<p>This data was collected from <a href=""https://webrobots.io/kickstarter-datasets/"" target=""_blank"" rel=""nofollow"">WebRobots.io</a></p>
This dataset was created by Rachel Woods and contains around 20000 samples along with Create To Launch, Created At Month, technical information and other features such as:
- Deadline Hr
- Blurb Len
- and more.
How to use this dataset
&gt; - Analyze Currency Trailing Code in relation to Blurb Len Clean
- Study the influence of Disable Communication on Created At Yr
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Rachel Woods 
Start A New Notebook!"	59	453	4	yamqwe	kickstarter-campaign-successe
1173	1173	datafacedetection		[]		0	0	0	maheenrafi	datafacedetection
1174	1174	timm-master	pytorch image models - master branch	['arts and entertainment']		172	1167	20	abhishek	timmmaster
1175	1175	India Health Dataset		['health']	"Things you have to find out
1.connection between domestic abuse and literacy race(EDA)
2.connection between mother using tobacco and alcohol has an impact on childs health(EDA)
3.the probability of 50%or above housewife has a degree in urban area?
4.make and test a hypothesis fro the dataset.
5.find the outliuer in any column using iqr test"	9	37	1	mehboobmg	india-health-dataset
1176	1176	trainvgg19		[]		0	0	0	oaeliar	vgg19
1177	1177	Yoga_Poses_CGI		['health and fitness', 'exercise', 'intermediate', 'deep learning', 'image data', 'tensorflow']	"Context
This data set is useful for learning  image classification using deep learning techniques.
Data is created by Google AI Advocate - Laurence Moroney and is downloaded from his website at https://laurencemoroney.com/datasets.html
Content
The images are created using Photoreal CGI . 
There are 5 different  type of yoga poses . In a gist - each pose has images with varying skin and hair tones . The images with in the same pose type may differ in the angle at which the pose is displayed and also different positions while completing the pose.
Acknowledgements
Sole credit to Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com)
The dataset is licensed as a CC By 2.0, free  to share and adapt for all uses, commercial or non-commercial. 
Inspiration
Each AI task is an inspiration to inch closer to  mirror the reality."	0	7	0	vidyams	yoga-poses-cgi
1178	1178	Lyft Mini		[]		0	3	0	didiruh	lyft-mini
1179	1179	Bellabeaat fitness tracker device data	This dataset contains information fitness tracker data of various users.	['health and fitness', 'exercise', 'tabular data']	"Context
The data contained in this dataset was collected from fitness tracker devices made for women. 
This dataset contains information collated from bellabeat (fitbit) fitness tracker devices. 
Content
The fitness tracker collects the following data;
Calories burnt per hour, and per day
Distance covered per day
Distance covered (at different activity intensities) per day
Total steps per hour, per day
Sleep time per day
Acknowledgements
My acknowledgement goes to the Google Data Analytics team for sharing this dataset, enabling starters like myself practice and show-off our data analytic skills
Inspiration
The dataset was created for analysis, to enable the Fitbit team gain insights into how their customers currently use the device and enable them create effective marketing strategy to further promote their product."	14	84	5	blessingalabie	bellabeaat-fitness-tracker-device-data
1180	1180	catpreds		[]		1	6	0	aszelsceznyk	catpreds
1181	1181	Airplane Tickets from Moscow, 2022	Airplane oneway tickets price from Moscow to whole world by 2022 year	['business', 'finance', 'aviation', 'tabular data', 'python']	"Context
Dataset include information about airplane ticket price from Moscow for 2022 year. Ticket price changes dynamic and depend on many factors.
This dataset is timestamp on 9 Feb. 
Acknowledgements
The data was obtained from Aviasales (https://www.aviasales.ru). Aviasales collect and provides information about airplane ticket price from all sale company.
Inspiration
You can see a dynamic of price throughout the year, build a report by airlines and find the best price to flight to another city."	51	274	5	francuzovd	airplane-tickets-from-moscow-2022
1182	1182	EDA case study		['education']		0	4	0	epsitabose	eda-case-study
1183	1183	linear_regression		[]		0	5	0	epsitabose	linear-regression
1184	1184	Jogos 2019 a 2022 Campeonato Paulista Série A1	Tabelas com estatísticas, eventos e jogos do Campeonato Paulista da A1	['sports']	"Context
Estes dados contém estatísticas e eventos dos jogos do Campeonato Paulista da Série A1 (Brasil) dos anos de 2019 a 2022 e foram utilizados para ilustrar como estruturar dados de futebol neste vídeo (https://youtu.be/5AwqRsMtkCo)
Content
Os dados estão estruturados da seguinte forma:
**Tabela de Jogos
1 jogo por linha
Dados como o estádio, juiz, resultado, id do jogo, nome dos times, etc.
Base para a união dos dados para as demais tabelas
**Tabela Estatísticas
Pode variar, neste caso são 2 linhas por jogo (1 para o time da casa e 1 para o visitante)
Trazem os dados como posse de bola, chutes a gol, escanteios, cartões, etc.
Com os tratamentos, transformando 1 linha por jogo, geralmente é o input para análises e modelos estatísticos
**Tabela Eventos
1 linha por tipo de evento/time/jogador/tempo
Geralmente traz os principais eventos do jogos: Substituições, cartões, gols
Interessante para montar uma linha do tempo do jogo e entender o efeito destes eventos no resultado. Será que um cartão vermelho acima dos 30 minutos garante que o time com 1 a mais será vencedor? Fica a questão para análises...hehe
Acknowledgements
Estes dados foram extraídos por meio da minha assinatura no serviço api-football.
Inspiration
Destes dados podemos tirar diversos insights interessantes e a idéia é estimular a comunidade a explorá-los. 
- Como torcedor, quando o time adversário perde um jogador logo nos animamos, o grito no estádio aumenta e a esperança de uma goleada logo se acende. Mas, nos números, será que isto é realmente verdade ou estamos sendo enganados pelo nosso senso comum?
- Qual a probabilidade de sair o segundo gol, do mesmo time, após a marcação do primeiro?
- Os gols aos últimos minutos são eventos extremamente raros?
- Um time que ganhou um jogo de virada aumenta a probabilidade de ganhar o próximo jogo? (mudança na motivação)
..."	1	9	0	tiagodatascience	campeonato-paulista-a1
1185	1185	lead score case study		['education']		1	7	0	epsitabose	lead-score-case-study
1186	1186	portrait_shadow_manipulation		[]		0	2	0	dwchen	portrait-shadow-manipulation
1187	1187	Lata Mangeshkar's Songs on Spotify	Songs of a popular Indian singer from Spotify	['arts and entertainment', 'music', 'art', 'linguistics', 'tabular data']	"Context
Lata Mangeshkar was an Indian playback singer and occasional music composer. She is widely considered to have been one of the greatest and most influential singers in India. Her contribution to the Indian music industry in a career spanning seven decades gained her honorific titles such as the Nightingale of India, Voice of the Millennium and Queen of Melody.
Content
Id: Unique value for each song
Name: Name of the song
Album: Name of the album
Release Date: Release Date of the song
Length (ms): Length of the song in milli-seconds
Acousticness: This value describes how acoustic a song is. A score of 1.0 means the song is most likely to be an acoustic one.
Danceability: Danceability is measured using a mixture of song features such as beat strength, tempo stability, and overall tempo. The value returned determines the ease with which a person could dance to a song over the course of the whole song.
Energy: Energy is the sense of forward motion in music, whatever keeps the listener engaged and listening. In loud music, musical energy is easy to identify. We notice the energy more as the drums get busier and play louder, and as the singer sings higher.
Instrumentalness: This value represents the amount of vocals in the song. The closer it is to 1.0, the more instrumental the song is.
Valence: Describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
Liveness: This value describes the probability that the song was recorded with a live audience. A value above 0.8 provides strong likelihood that the track is live
Loudness: Loudness is a way to measure audio levels.
Speechiness: Speechiness detects the presence of spoken words in a track. If the speechiness of a song is above 0.66, it is probably made of spoken words, a score between 0.33 and 0.66 is a song that may contain both music and words, and a score below 0.33 means the song does not have any speech.
Tempo: Tempo is how fast or slow a piece of music is performed. Tempo generally is measured as the number of beats per minute, where the beat is the basic measure of time in music.
Time Signature: The time signature indicates how many counts are in each measure and which type of note will receive one count. The top number is commonly 2, 3, 4, or 6. The bottom number is either 4 or 8.
Popularity: Describes how popular the song is."	13	120	7	rprkh15	lata-mangeshkar-songs-on-spotify
1188	1188	TPS Feb22: Deduped Trainset with Folds	Deduplicated Trainset for TPS Feb'22 Competition with Stratified Folds	['earth and nature', 'tabular data']	"Context
This dataset is meant for the competition TPS Feb'22
Content
Traning Set of the TPS Feb'22 Competition after removing duplicates and adding stratified folds
Inspiration
The original training set for the contest consisted of around 40% duplicate values thus this daaset will help to speed up the training and prevent from overfitting the duplicated values"	2	74	8	ritvik1909	tps-feb22-deduped-trainset-with-folds
1189	1189	ubiquant_models_jw		[]		0	0	0	jackit940	ubiquant-models-jw
1190	1190	Plastic Bottle Waste	Counts of single-use plastic bottle waste detected by Wastebase.	['africa', 'environment', 'pollution', 'business', 'energy', 'beginner', 'tabular data']	"Context
Regularly-collected data about single-use plastic bottle waste found in public spaces, identified to the level of individual commercial products.
Content
This data was collected over the course of 2021 using the Wastebase digital platform for crowdsourced data collection about single-use plastic waste.
Data is collected via an Android app, and then checked and linked to Brand and Product information by Wastebase Data Partners (see below).    Data is collected to the level of the specific product barcode, timestamped and location stamped.   You can search for specific data on the Wastebase reports page.
Data was collected in the following countries:
- GB: United Kingdom
- MZ: Mozambique
- TZ: Tanzania
- ZM: Zambia
Acknowledgements
This data was collected by Wastebase's Data Partners in several countries of sub-saharan Africa as well as individual contributors in Africa and the UK."	294	3035	7	wastebase	plastic-bottle-waste
1191	1191	Anime Offline Database	A JSON anime database containing the meta data from MAL, ANIDB, ANILIST, KITSU	['anime and manga']		7	159	3	vishalkalathil	anime-offline-database
1192	1192	anomailes des poumons		['deep learning']		1	17	1	diopjames	anomailes-des-poumons
1193	1193	US Electric Utility Look	Average residential, commercial and industrial electricity rates by zip code	['business', 'energy', 'electricity']	"About this dataset
&gt; <p>This dataset, compiled by NREL using data from Ventyx and the U.S. Energy Information Administration dataset 861, provides average residential, commercial and industrial electricity rates by zip code for both investor owned utilities (IOU) and non-investor owned utilities. Note: the file includes average rates for each utility, but not the detailed rate structure data found in the OpenEI U.S. Utility Rate Database.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/u-s-electric-utility-companies-and-rates-look-up-by-zipcode-2014"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/u-s-electric-utility-companies-and-rates-look-up-by-zipcode-2014</a></p>
This dataset was created by Department of Energy and contains around 1000 samples along with Wind, Photovoltaic, technical information and other features such as:
- Unnamed: 8
- Unnamed: 16
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 50 in relation to Unnamed: 23
- Study the influence of Utility Characteristics on Unnamed: 61
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Department of Energy 
Start A New Notebook!"	14	216	2	yamqwe	us-electric-utility-look-2014e
1194	1194	../input/world-university-rankings		['universities and colleges']		2	2	1	bestesakar	inputworlduniversityrankings
1195	1195	视频动作分类		[]		2	6	0	sailor666	vedio-action-classification
1196	1196	test-dataset		[]		0	7	1	jingyi244	test-dataset
1197	1197	BiLSTMtpu_fold4		[]		0	2	0	mahdibb	bilstmtpu-fold4
1198	1198	aptos densenet model		[]		1	0	0	abderaoufmoustari	aptos-densenet-model
1199	1199	Weather in Kobe	Daily weather data in Kobe, Japan in 2020	['weather and climate']	"Daily weather data in Kobe city in 2020
kobe-weather-2020.csv
Date
Precipitation (mm/day)
Temparature (°C)
Wind Speed (m/sec)  
Humidity  (%)
Daylight Hours (h/day)
https://www.data.jma.go.jp/gmd/risk/obsdl/
kobe-solar-altitude-azimuth-2020.csv
date
altitude
azimuth 
https://keisan.casio.jp/keisan/service.php"	91	1195	9	stpeteishii	weather-in-kobe
1200	1200	All Mobile Price List dataset with specification.	Mobile price prediction	['mobile and wireless', 'investing']		1	22	0	rahulmishra5	all-mobile-price-list-dataset-with-specification
1201	1201	malware_only_byte		[]		2	11	0	songwonmin	malware-only-byte
1202	1202	tomato		['food']		0	9	0	sabelomuzimsimango	tomato
1203	1203	MMA Fight Predictions by Professional Fighters	Fight Picks Made by Fellow Fighters	['sports', 'martial arts']	"Context
This is a continually updated dataset of professional fighters making fight predictions.
Content
The data is gathered mostly from James Lynch's YouTube channel, where fighters are asked to predict upcoming fights. Currently, the dataset includes 2,552 made predictions of 244 fights since the end of 2015 up to today."	62	1503	5	martj42	mma-fight-predictions-by-professional-fighters
1204	1204	gauge300x300		[]		0	2	0	aithammadiabdellatif	gauge300x300
1205	1205	wagesdata		[]		0	0	0	yaminisanjayrangari	wagesdata
1206	1206	Unicorn Startups	Startups with 1 billion dollars valuation.	['people and society', 'business', 'finance', 'economics']	"""Unicorn"" is a term used in the venture capital industry to describe a privately held startup company with a value of over $1 billion. The term was first popularized by venture capitalist Aileen Lee, founder of Cowboy Ventures, a seed-stage venture capital fund based in Palo Alto, California.
Unicorns can also refer to a recruitment phenomenon within the human resources (HR) sector. HR managers may have high expectations to fill a position, leading them to look for candidates with qualifications that are higher than required for a specific job. In essence, these managers are looking for a unicorn, which leads to a disconnect between their ideal candidate versus who they can hire from the pool of people available.
<img src=""https://64.media.tumblr.com/bac335874ef2027808929dae97be5edc/tumblr_mqseckyDQX1r8q1s0o1_500.gifv"" alt=""Computer man"" style=""width: 600px; height: 300px"">"	517	3861	44	ramjasmaurya	unicorn-startups
1207	1207	for_this_priject		[]		3	10	0	datenjisherpa	for-this-priject
1208	1208	KaggleFold	AlphaFold2 Adapted to kaggle	['genetics', 'biology', 'biotechnology', 'computer science']		0	91	0	victorfernandezalbor	kagglefold
1209	1209	useboodsupervisedresults		[]		11	16	0	muennighoff	useboodsupervisedresults
1210	1210	pbvs-multimodal-dataset		[]		0	3	0	adityakane	pbvsmultimodaldataset
1211	1211	train_covidx9a_balance_5555		[]		0	6	0	tuanledinh	train-covidx9a-balance-5555
1212	1212	edaotkryt		[]		2	7	0	maximvlah	edaotkryt
1213	1213	Mexican Emotional Speech Database (MESD)	Dataset of single-word emotional utterances of adult and child voices	['beginner', 'intermediate', 'advanced', 'audio data']	"Context
The Mexican Emotional Speech Database (MESD) provides single-word utterances for anger, disgust, fear, happiness, neutral, and sadness affective prosodies with Mexican cultural shaping. The MESD has been uttered by both adult  and child non-professional actors: 3 female, 2 male, and 6 child voices are available (female mean age ± SD = 23.33 ± 1.53, male mean age ± SD = 24 ± 1.41, and children mean age ± SD = 9.83 ± 1.17). Words for emotional and neutral utterances come from two corpora: (corpus A) composed of nouns and adjectives that are repeated across emotional prosodies and types of voice (female, male, child), and (corpus B) which consists of words controlled for age-of-acquisition, frequency of use, familiarity, concreteness, valence, arousal, and discrete emotion dimensionality ratings. Particularly, words from corpus B are nouns and adjectives which subjective age of acquisition is under 9-year-old. Neutral-uttered words have valence and arousal ratings strictly greater than 4, but lower than 6 (in a 9-point-scale). Emotional-uttered words have valence and arousal ratings ranging from 1 to 4, or from 6 to 9. Furthermore, ratings for discrete emotional dimension greater than 2.5 (on a 5-point scale) allowed the emotional utterance with the corresponding anger, disgust, fear, happiness, or sadness prosody. Finally, words from corpus B were selected so that emotional prosodies do not differ as regards frequency of use, familiarity, and concreteness dimensions. 
The audio recordings took place in a professional studio with the following materials: (1) a Sennheiser e835 microphone with a flat frequency response (100 Hz to 10 kHz), (2) a Focusrite Scarlett 2i4 audio interface connected to the microphone with an XLR cable and to the computer, and (3) the digital audio workstation REAPER (Rapid Environment for Audio Production, Engineering, and Recording). Audio files were stored as a sequence of 24-bit with a sample rate of 48000Hz. 
Content
Utterances are shared as 864 audio files in WAV format that are named according to the following pattern:"	3	79	6	saurabhshahane	mexican-emotional-speech-database-mesd
1214	1214	Diabetes		['diabetes']		3	20	3	ayesha111	diabetes
1215	1215	CV: Start Farm Distracted Driver Detection		['software']		0	10	0	vinaykakkad	cv-start-farm-distracted-driver-detection
1216	1216	demo.614		['software']		0	0	0	wallacefqq	demo614
1217	1217	Number of Sales and Visits of 5 Different Drugs		['time series analysis', 'drugs and medications', 'datetime']	"There are number of sales and visits of 5 different drugs in data. There are four different columns:
Date: Month period
Product: Name of product
SalesCount: Sales count of product by month
VisitsCount: Doctor or pharmacy visits count for product by month"	19	114	2	recepyilkici	number-of-sales-and-visits-of-5-different-drugs
1218	1218	Redefining the 21st 		[]		0	1	0	oliviaharry	redefining-the-21st
1219	1219	Main Differences and Similarities		[]		0	1	0	oliviaharry	main-differences
1220	1220	Essay Writing for Beginners		[]		0	1	0	oliviaharry	blog2
1221	1221	APA Format: Everything You Need to Know Here		[]		0	1	0	oliviaharry	apa-format-everything-you-need-to-know-here
1222	1222	5 Steps To Writing Yourself A Great Essays		[]		0	2	0	oliviaharry	5-steps-to-writing-yourself-a-great-essays
1223	1223	NBME-MultiPos-TFData		['standardized testing']		0	5	0	tchaye59	nbmemultipostfdata
1224	1224	weights		['exercise']		0	0	0	medetovyerma	weights
1225	1225	CubexSoft MBOX Converter for Mac	Convert MBOX file on Mac OS and Windows OS	['software', 'email and messaging']	"If you are looking to convert MBXO files on any Mac OS based computer, then you are required to utilize the most downloaded software that is the CubexSoft MBOX Converter for Mac. This software helps users to move complete email data into multiple file formats.
It allows users to save files into PST, PDF, EML, MSG, DOC, and so on. The tool also allows users to save email files into multiple cloud apps like MS Office 365, Exchange Server, Gmail, and many others. Users can easily save data from MBOX email files into the required output options with batch email data.
The software allows users to convert batch emails directly into the desired output option without any error or loss issue. You can follow the steps of this amazing software on any Mac OS. The software allows p provides the other edition of the software to help users to convert email files on Windows OS. The Windows OS edition allows users to convert complete email files into more than 10+ options.
The tool helps users to convert email files into PDF, PST, EML, MSG, DOC, RTF, NSF, Office 365, Exchange Server, Gmail, Yahoo Mai, G Suite, etc. You can easily get your email data into these output format b by just following some simple steps of this advanced and best software.
Features of MBOX Converter for Mac
This software works on any Mac OS Edition.
It helps users to convert complete email data including large size email files.
It never disturb the structure of email files so that every user can easily understand and view email files with the same view of email files.
You can understand the procedure step by step without any help from professional users.
The tool allows users to convert MBXO email files with any size of attachments and other email properties like email hyperlinks, email images, email header, and so on.
Free Edition to Check the Process Step by Step
The tool allows users to convert some MBOX email files for absolutely free through its demo version. This edition is helpful for all users who are looking to convert MBOX email files. You can follow the steps of this edition to check the processing of the tool and follow the step by step procedure with the live MBOX file conversion process instantly.
**Read More:
MBOX to PDF Converter
MBOX to Outlook Converter
MBOX to Office 365 Tool
MBOX to Gmail Tool"	0	20	0	cubexsoftmbox	cubexsoft-mbox-converter-for-mac
1226	1226	Person-to-Object Contact Dataset	Actual conditions of contact behaviors with objects during the COVID-19 pandemic	['people and society', 'education', 'tabular data', 'text data', 'japan', 'covid19']	"Summary of this dataset
This dataset has been designed and obtained for discussing control measures during the COVID-19 pandemic. In this study, 1,260 people living in Tokyo and Kanagawa prefectures in Japan participated in the survey. This survey was used to collect participants’ behaviors and the objects that they touched on the days that they went out at 15 types of locations and vehicles.
This dataset is expected to improve our understanding of actual human behavior and contact with objects that could. Although it is impossible to disinfect all objects and spaces, this dataset is expected to contribute to the prioritization of disinfection during periods of widespread infection.
Datasets
1. Study and Dataset Design
The participants living in Tokyo and Kanagawa prefectures in Japan were asked to respond, in detail, to a survey regarding the locations they stayed at for an extended period between December 3 (Thursday) and December 7 (Monday), 2020, and all the items that they touched during this time. Using the locations where clusters of infections were found during April 2020, 12 locations were selected (e.g., medical facilities, including hospitals; restaurants; stores whose main objective was to sell alcohol, such as bars; companies, including the participants’ own companies and the offices of others; and sports facilities such as gyms) and investigated. Similarly, three means of transport, namely trains, buses, and taxis, were selected as spaces where people often crowd together. 
The main survey was conducted with 1,536 subjects during December 3–8. Data from 1,260 subjects who gave valid responses were used for the dataset. To ensure that the respondents could respond while their memories were still fresh, the survey was distributed to each subject on the day of their corresponding behavior. Participants were asked to respond about the locations where they spent most of their time during the corresponding period. They were also asked to detail all the objects they touched (excluding personal objects) during this time. The objects in this study were evaluated using a free-writing description. Typographical errors and differences in expressions were frequently observed (e.g., water closet, toilet, and bathroom). A categorization rule was thus developed to better ascertain the actual status of locations and object contact. The participants’ expressions were modified through visual inspection.
2. Patient and Public Involvement
This survey was conducted after appropriate review by the Ethics Committee of the Graduate School of Engineering, University of Tokyo (examination number: 20-61, approval number: KE20-72). 
Reference
Teruaki Hayashi, Daisuke Hase, Hikaru Suenaga, Yukio Ohsawa, ""The Actual Conditions of Person-to-Object Contact and a Proposal for Prevention Measures During the COVID-19 Pandemic,"" medRxiv, 2021. DOI: https://doi.org/10.1101/2021.04.11.21255290
Acknowledgements
This research project was supported by the “Startup Research Program for Post-Corona Society” of the Academic Strategy Office, School of Engineering, the University of Tokyo, and the “COVID-19 AI and Simulation Project” of the Office for Novel Coronavirus Disease Control, Cabinet Secretariat, Government of Japan. The authors would like to thank PLUG-Inc. for survey design and implementation."	14	279	4	teruakihayashi	person-to-object-contact-dataset
1227	1227	AG_Store	A combination of Apple and Google App Store Information	['games', 'science and technology', 'software', 'exploratory data analysis', 'tidyverse']	"Context
This is the combined dataset of Apple and Google's App Stores I used to make a Tableau Dashboard
Content & Acknowledgements
I borrowed the datasets from Gautham Prakash (https://www.kaggle.com/gauthamp10) & Jithin Koshy (https://www.kaggle.com/jithinkoshy)
The data comes from June & October of 2021, scraped via Python script (Scrapy)
Inspiration
I thought this would be a good first project working with large datasets and then creating an interesting dashboard
Here's the link to my dashboard if you want to have a look:
https://public.tableau.com/views/AppStore_16407564332280/AppStores?:language=en-US&:display_count=n&:origin=viz_share_link
Thanks for stopping by!"	0	11	0	brianblitz	ag-stores
1228	1228	class playlist		['education']		0	2	0	joxu314	class-playlist
1229	1229	DynamicRGB		[]		0	4	0	sarthak4u	dynamicrgb
1230	1230	RidingMowers.csv		['internet']		0	0	1	rezaafsharniakan	ridingmowerscsv
1231	1231	Blog 5		['online communities']		0	3	0	deneenbruss	blog5
1232	1232	Blog 4		['online communities']		0	2	0	deneenbruss	blog4
1233	1233	dataset		[]		4	24	0	bastiennes	dataset
1234	1234	embeddingsexp1		[]		0	0	0	gyanendradas	embeddingsexp1
1235	1235	Blog 2		['online communities']		0	3	0	deneenbruss	blog2
1236	1236	Sorts of Public Speeches		[]		0	2	0	andygray1	sorts-of-public-speeches
1237	1237	Blog 1		['online communities']		0	2	0	deneenbruss	blog1
1238	1238	Writing a Psychology Essay		[]		0	6	0	andygray1	writing-a-psychology-essay
1239	1239	Different Types of Essays		[]		0	2	0	andygray1	different-types-of-essays
1240	1240	How to Begin Your Essay Successfully		[]		0	2	0	andygray1	how-to-begin-your-essay-successfully
1241	1241	Happywhale TFRecords (512x512)		[]		0	5	0	soumikrakshit	happywhale-tfrecords-512x512
1242	1242	How to Format a Thesis Proposal		[]		1	3	0	andygray1	how-to-format-a-thesis-proposal
1243	1243	car/detection/detr		['automobiles and vehicles']		0	1	0	pondurivasanthi	cardetectiondetr
1244	1244	Indonesian Adjective Sentiment (Kata Sifat)	Indonesian Adjective Sentiment (Sentimen Kata Sifat Bahasa Indonesia)	['languages', 'asia', 'categorical data', 'text data']	"Context
Indonesian Adjective Sentiment data. Consist of adjective words from Big Indonesian Language Dictionary (KBBI). 
Content
There are two file for dataset on this project
Dataset\indonesian-adjective-sentiment-raw.csv contain raw information about Indonesian Adjective
Dataset\indonesian-adjective-sentiment-score.csv contain sentiment labels
Data Table
Table from indonesian-adjective-sentiment-raw.csv
| ID | Word | URL | Spelling | Explanation |
| --- | --- | --- | -------- | ----------- |
| 1 | abadi | https://kbbi.kata.web.id/abadi/ | aba.di | kekal; tidak berkesudahan: di dunia ini tidak ada yang abadi |
| 2 | abaktinal | https://kbbi.kata.web.id/abaktinal/ | abak.ti.nal | istilah biologi berkenaan dengan sisi tubuh yang tidak mengandung mulut, seperti pada binatang laut |
| ... | ... | ... | ... | ... |
| ... | ... | ... | ... | ... |
text
Description for each column:
ID = Primary key (UID)
Word = Word in Indonesian Language
URL = URL from kbbi https://kbbi.kata.web.id
Spelling = How to pronounce the word
Explanation = Detail and explanation
Table from indonesian-adjective-sentiment-score.csv
| ID | Score |
| --- | --- |
| 1 | 2 |
| 2 | 0 |
| ... | ... |
| ... | ... |
text
Description for each column:
ID = Foreign key (UID) from indonesian-adjective-sentiment-raw.csv
Score = Sentiment label
Sentiment
There are 5 types of sentiment
| Code | Sentiment |
| ---- | --------- |
| -2 | Very Negative |
| -1 | Negative |
| 0 | Neutral |
| 1 | Positive |
| 2 | Very Positive |"	12	130	7	linkgish	indonesian-adjective-sentiment-kata-sifat
1245	1245	cell_image_stitching		[]		0	0	0	mumbaiyachori	cell-image-stitching
1246	1246	Blog 5		['online communities']		0	3	0	klaraloft	blog-5
1247	1247	Blog 4		['online communities']		0	3	0	klaraloft	blog-4
1248	1248	Blog 3		['online communities']		0	2	0	klaraloft	blog-3
1249	1249	Blog 2		['online communities']		0	2	0	klaraloft	blog-2
1250	1250	Blog 1		['online communities']		0	4	0	klaraloft	blog-1
1251	1251	skelton		['arts and entertainment']		0	2	0	sarthak4u	skelton
1252	1252	9 Process Chart Examples ideas 		[]		0	1	0	frostysilver	9-process-chart-examples-ideas
1253	1253	509 Informative Speech Ideas 		[]		0	1	0	frostysilver	509-informative-speech-ideas
1254	1254	INTERNATIONAL ENGLISH		[]		0	3	0	frostysilver	international-english
1255	1255	How to Write a Great Movie 		[]		0	3	0	frostysilver	how-to-write-a-great-movie
1256	1256	RSNA Ischemia Series		['health conditions']		2	8	0	fereshtej	rsna-ischemia-series
1257	1257	work163		[]		2	11	1	kk256kk	work163
1258	1258	Ethiopian Habesha Dresses		[]		1	2	1	mickiaszerfu	ethiopian-habesha-dresses
1259	1259	road_crack		[]		0	2	0	jirapaweer	road-crack
1260	1260	Road_crack		[]		0	5	0	chomchom07	road-crack
1261	1261	Impact of Radiation on Patient's weight		['asia', 'text data', 'hospitals and treatment centers']		1	5	0	farhanakram5156	impact-of-radiation-on-patients-weight
1262	1262	HappyWhale: Samp		['sports']		0	3	0	iftiben10	happywhale-samp
1263	1263	yolor-main		[]		0	5	2	thomasdubail	yolormain
1264	1264	RSNA Ischemia Test Result		[]		0	5	0	hamedghavami	rsna-ischemia-test-result
1265	1265	datenet		[]		0	1	0	luozheng	datenet
1266	1266	horse_convo		[]		0	3	0	chancecallahan	horse-convo
1267	1267	Python2		['computer science', 'programming']		0	1	0	satriyopudjo	python2
1268	1268	Python1		[]		0	0	0	satriyopudjo	python1
1269	1269	Melbourne Housing Data		['real estate']		0	3	0	taufaneka	melbourne-housing-data
1270	1270	Tugas 1		[]		0	7	0	satriyopudjo	tugas-1
1271	1271	tugas2		[]		0	1	0	rakaumridreftanta	tugas2
1272	1272	EDY2visualisasi		[]		0	0	0	rakaumridreftanta	edy2visualisasi
1273	1273	helloh		[]		0	2	0	eddyju	helloh
1274	1274	mmlab_mmcv		[]		1	4	0	dwchen	mmlab-mmcv
1275	1275	warped_images		[]		0	108	0	khivhonganh	warped-images
1276	1276	Ubiquant Competition Train Data Divided - QS	Split CSV files for the Ubiquant comp. Training Data Scaled - Scalers Included	['finance', 'investing']	"Ubiquant dataset training data split into different csv-files.
Original Training dataset from Ubiquant Competition has been divided into three subsets of Training-Validation-Test
The data has been scaled using QuantileTransformer from ScikitLearn with the following parameters:
output_distribution='normal'
random_state=17
Two pkl-files included in the dataset contains the scalers used for the features and target variables.
Out of the original dataset, 80% has been used to create the Training Dataset, 10% for the Validation and the remaining 10% for the Test set.
The Scikitlearn Train_Test_split function has been used for this purpose."	1	34	3	fabrizio78	ubiquant-competition-train-data-divided-qs
1277	1277	Pytorch1.10+torchvision0.11+torchaudio0.10cu11.3	Pytorch for offline installation	['software', 'pytorch']		0	3	0	tunminhhunh	pytorch110torchvision011torchaudio010cu113
1278	1278	FIFA Dataset		['football']	"Data from: https://www.kaggle.com/kriegsmaschine/soccer-players-values-and-their-statistics
Changes:
Transformed from .csv to .xlsx
Modified certain values:
Values replaced in excel using search and replace function (transfermarkt database 1718 and 1819):
Ã© -- é
Ã³ -- ó
Ã¡ -- á
Ã« -- ë
Ãº -- ú
Ã¯ -- ï
Ã´ -- ô
Ã¼ -- ü
Ã– -- Ö
Ã -- í
í± -- ñ
í¶ -- ö
í² -- ò
í‰ -- É
í“ -- Ó
È™ -- ș
í® -- î
Ä‡ -- ć
í£ -- ã
í‘ -- Ñ
í§ -- ç
Values replaced in excel using search and replace function (transfermarkt database 1920):
Ăş -- ú
Ă© -- é
ĂŻ -- ï
Ã -- í
ÄŤ -- č
í§ -- ç
Ĺˇ -- š
íˇ -- á
í˛ -- ò
í« -- ë
í‰ -- É
í¶ -- ö
í– -- Ö
í§ -- ç
í± -- ñ
íł -- ó
í‘ -- Ñ
í“ -- Ó
í´ -- ô
Ä‡ -- ć
Ã¼ -- ü
í“ -- Ó"	0	6	0	massimodg	fifa-project-dataset
1279	1279	ubiquant	Ubiquant competition data in feather format	[]		1	42	1	sashakuzmin	ubiquant
1280	1280	Single DBFC Dataset	Direct Borohydride Fuel Cell impedance and polarization test	['electronics']	"This dataset includes Direct Borohydride Fuel Cell (DBFC) impedance and polarization test in anode with Pd/C, Pt/C and Pd decorated Ni–Co/rGO catalysts. In fact, different concentration of Sodium Borohydride (SBH), applied voltages and various anode catalysts loading with explanation of experimental details of electrochemical analysis are considered in data. Voltage, power density and resistance of DBFC change as a function of weight percent of SBH (%), applied voltage and amount of anode catalyst loading that are evaluated by polarization and impedance curves with using appropriate equivalent circuit of fuel cell. Can be stated that interpretation of electrochemical behavior changes by the data of related cell is inevitable, which can be useful in simulation, power source investigation and depth analysis in DB fuel cell researches.
Github
Data Paper"	19	1018	3	sepandhaghighi	single-dbfc-dataset
1281	1281	Metacritic best music albums of all time	artist, release date, summary, metacritic score, user score, link, image url	['arts and entertainment', 'music', 'exploratory data analysis']	"Context
Detailed about artist, release date, summary, metacritic score, user score
Acknowledgements
data source credit: https://www.metacritic.com/browse/albums/score/metascore/all/filtered?sort=desc
cover image credit: https://www.pexels.com/photo/retro-turntable-playing-vinyl-disc-in-living-room-4200747/"	115	677	11	prasertk	metacritic-best-albums-of-all-time
1282	1282	yolov5s6_3584		[]		1	5	0	dragonzhang	yolov5s6-3584
1283	1283	autogluon		[]		0	2	0	llin22	autogluon
1284	1284	Rio de Janeiro Bus System	This dataset does not include data from the BRT bus system.	['brazil', 'transportation', 'tabular data']		9	178	2	elioenaigomes	rio-de-janeiro-bus-system
1285	1285	C4_four_class		[]		5	6	0	bwtest	c4-four-class
1286	1286	[RNAseq] Aging, Dementia and TBI	RNA data of hippcampus and cortex in an aged cohort isolated by macrodissection. 	['healthcare', 'genetics', 'biology', 'chemistry', 'biotechnology']	"Dataset
The data set includes 377 RNA-Seq samples collected from hippocampus, temporal cortex, and parietal cortex (both grey and white matter) in 55 aged donors with TBI and their matched controls (107 donors total after QC).  Additional donor meta-data, neuropathology metrics, and IHC image quantifications for each sample are available.  
The sequencing results were aligned and aggregated at the gene level using the RSEM algorithm, and the resulting fpkm values were normalized across all samples within each brain region to account for processing batch and RNA quality (RIN).
Description
fpkm_table_unnormalized.csv : Contains the (row, column) matrix of fpkm values obtained for each (gene, sample) from the RSEM analysis pipeline
The first row contains the unique identifiers of the RNA-seq profiles of the samples (rnaseq_profile_id)
The first column contains the gene unique identifiers (gene_id)
fpkm_table_normalized.csv: Contains the (row, column) matrix of fpkm values obtained for each (gene, sample) after correcting for RIN and batch effects. These are the data displayed in the RNA-Seq page heatmap
The first row contains the unique identifiers of the RNA-seq profiles of the samples (rnaseq_profile_id)
The first column contains the gene unique identifiers (gene_id)
columns-samples.csv: Contains information about the samples profiled with RNA sequencing
rnaseq_profile_id
    Expression profile obtained from aligning the RNA-Seq data to the GRCh38.p2 reference genome.
donor_id and donor_name
    Donor from which the sample was dissected
specimen_id and specimen_name
        Specimen from which the sample was dissected (i.e., a particular brain structure from a particular donor)
rna_well_id
    Unique identifier of the sample.
polygon_id
        Unique identifier of an avg_graphic_object that outlines where the sample was cut from.
structure_id, structure_abbreviation, structure_color, structure_name
        Label that groups samples by brain region (hippocampus, temporal cortex, parietal cortex, and forebrain white matter).
hemisphere
        Hemisphere from which the processed sample was collected
rows-genes.csv: Contains information about the genes for which fpkm values were calculated.
gene_id: Unique identifier for the gene.
chromosome: Chromosome associated with the gene.
gene_entrez_id, gene_symbol, gene_name: entrez_id, NCBI symbol, and name of the gene."	55	648	11	albertozorzetto	rnaseq-aging-dementia-and-tbi
1287	1287	FitBit Fitness Tracker Data		['exercise']		0	9	0	maggieshen007	fitbit-fitness-tracker-data
1288	1288	EfficientNetB7 Model Binary (without final layer)	Compatible with TensorFlow version 2.7	['computer science']		1	8	0	juliangarratt	efficientnetb7-model-binary
1289	1289	Parkinson with replicated acoustic features		[]		1	18	0	bunyaminergen	parkinson-with-replicated-acoustic-features
1290	1290	Ohio COVID Statistics	All the CSV filed published by the state of ohio	['covid19']	"Context
The State of Ohio COVID-19 Dashboard displays the most recent preliminary data reported to the Ohio Department of Health (ODH) about cases, hospitalizations and deaths in Ohio by selected demographics and county of residence. Data for cases and hospitalizations is reported to ODH via the Ohio Disease Reporting System (ODRS), and verified mortality data is reported via the Electronic Death Registration System (EDRS).
Content
Data definitions are published by Ohio.
Acknowledgements
All data is pulled from the state of Ohio COVID-19 Dashboards.  Additional documentation can be found there. 
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	2	25	0	carlfischeriv	ohio-covid-statistics
1291	1291	GP_dataset		[]		0	1	0	waeltarek	gp-dataset
1292	1292	flexible_yolo_1280		[]		1	5	0	timxinxinpeng	flexible-yolo-1280
1293	1293	Bellabeat Case Study		['business']	"About the company
Urška Sršen and Sando Mur founded Bellabeat, a high-tech company that manufactures health-focused smart products.
Sršen used her background as an artist to develop beautifully designed technology that informs and inspires women around
the world. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with
knowledge about their own health and habits. Since it was founded in 2013, Bellabeat has grown rapidly and quickly
positioned itself as a tech-driven wellness company for women.
Characters
○ Urška Sršen: Bellabeat’s cofounder and Chief Creative Officer
○ Sando Mur: Mathematician and Bellabeat’s cofounder; key member of the Bellabeat executive team
○ Bellabeat marketing analytics team: A team of data analysts responsible for collecting, analyzing, and
reporting data that helps guide Bellabeat’s marketing strategy. You joined this team six months ago and have
been busy learning about Bellabeat’’s mission and business goals — as well as how you, as a junior data analyst,
can help Bellabeat achieve them.
Content
I cleaned and analyzed the data via Google Sheets and imported it as an Excel file and created visualizations in Tableau Public. I found trends between Calories, Total Steps, Total Distance and Sedentary Minutes. All is explained in my word document case study.
Acknowledgements
I used data from ""Fitbit Fitness Tracker Data"" by the user Mobius.
Feel free to comment any mistakes made or things you would have done differently. This is my first case study and first time analyzing data. Any feedback will be gladly appreciated."	0	14	0	brianschuman	bellabeat-case-study
1294	1294	C4_two_class		[]		5	6	0	bwtest	c4-two-class
1295	1295	TestData		['business']		0	3	0	jonathanbruennert	testdata
1296	1296	🎾 100 Years of Tennis - WTA 1922 - 2022	WTA Match Results from 1922 to 2022	['tennis', 'sports']	"About this dataset
&gt; <h1>WTA Match Results from 2016 and 2018</h1>
<h2>License</h2>
<p>Tennis databases, files, and algorithms by <a href=""http://www.tennisabstract.com/"" target=""_blank"" rel=""nofollow"">Jeff Sackmann</a> is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License Based on (<a href=""https://github.com/JeffSackmann"" target=""_blank"" rel=""nofollow"">https://github.com/JeffSackmann</a>)</p>
<p>In other words: Attribution is required. Non-commercial use only.</p>
This dataset was created by Jon Loyens and contains around 3000 samples along with Draw Size, W Ace, technical information and other features such as:
- Tourney Name
- Winner Ht
- and more.
How to use this dataset
&gt; - Analyze W Df in relation to Round
- Study the influence of Best Of on Loser Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jon Loyens 
Start A New Notebook!"	19	266	8	yamqwe	wtaresultse
1297	1297	SharkTankDataset	Shark Tank India Dataset taken from Wikipedia. 	['business']		2	37	0	priyankam1709	sharktankdataset
1298	1298	The Lost Journalists: Dataset of journalist deaths	Detailed records on journalist deaths from 1992  compiled by CPJ	['literature', 'politics', 'news']	"Credit for the original dataset goes to CPJ
About this dataset
&gt; <p><code>In-the-News</code>:</p>
<ul>
<li><em>NRT</em>: <a href=""http://www.nrttv.com/EN/Details.aspx?Jimare=11928"" target=""_blank"">AT LEAST 122 MEDIA PROFESSIONALS KILLED GLOBALLY IN 2016</a></li>
<li><em>All Africa</em>: <a href=""http://allafrica.com/stories/201612190149.html"" target=""_blank"">Africa: Journalist Killings Ease From Record Highs As Murders Down, Combat Deaths Up</a></li>
<li><em>BBC</em>: <a href=""http://www.bbc.co.uk/news/entertainment-arts-38359384"" target=""_blank"">The lost journalists of 2016</a></li>
</ul>
<p><img src=""https://data.world/api/journalism/dataset/journalist-deaths/file/raw/journalist_deaths_by_year.png"" alt=""journalist_deaths_by_year.png""></p>
<h2>Methodology</h2>
<p>CPJ began compiling detailed records on journalist deaths in 1992. We apply strict journalistic standards when investigating a death. One important aspect of our research is determining whether a death was work-related. As a result, we classify deaths as ""motive confirmed"" or ""motive unconfirmed.""</p>
<p>We consider a case ""confirmed"" only if we are reasonably certain that a journalist was murdered in direct reprisal for his or her work; was killed in crossfire during combat situations; or was killed while carrying out a dangerous assignment such as coverage of a street protest. We do not include journalists who are killed in accidents such as car or plane crashes.</p>
<p>We include only confirmed cases in the statistical analyses in this database.</p>
<p>When the motive is unclear, but it is possible that a journalist was killed because of his or her work, CPJ classifies the case as ""unconfirmed"" and continues to investigate. We regularly reclassify cases based on our ongoing research.</p>
<p>Our archives include narrative capsules of all journalists killed, including the cases in which the motive is unconfirmed. In cases where the place of death is incidental to the journalist's killing, we have listed the country where the fatal attack occurred to be the place of the journalist's death (for example, in a case where a journalist is hit by shrapnel in one country and evacuated to another, where he or she dies, CPJ lists the country in which he or she was hit as the place of death).</p>
<p>CPJ defines journalists as people who cover news or comment on public affairs through any media -- including in print, in photographs, on radio, on television, and online. We take up cases involving staff journalists, freelancers, stringers, bloggers, and citizen journalists. The combination of daily reporting and statistical data forms the basis of our case-driven and long-term advocacy.</p>
<p>In 2003, CPJ began documenting the deaths of media support workers. We did so in recognition of the vital role these individuals play in newsgathering. These workers include translators, drivers, fixers, and administrative workers.</p>
<p>Our archives include narrative capsules for media workers killed on duty. These cases are not included our statistical analyses.</p>
<h2>About CPJ</h2>
<p>The Committee to Protect Journalists is an independent, nonprofit organization that promotes press freedom worldwide. We defend the right of journalists to report the news without fear of reprisal.</p>
<p><strong>Additional Reading</strong><br>
<a href=""http://sand-kas-ten.org/ijm/Introduction.pdf"" target=""_blank"">Investigative journalism in Africa – “Walking through a minefield at midnight”</a><br>
<a href=""http://www.aljazeera.com/humanrights/2013/04/2013481202781452.html"" target=""_blank"">Iraq: The deadliest war for journalists</a><br>
<a href=""https://www.washingtonpost.com/posteverything/wp/2016/02/18/being-a-journalist-in-mexico-is-getting-even-more-dangerous/?utm_term=.387ad39ec37f"" target=""_blank"">Being a journalist in Mexico is getting even more dangerous</a></p>
<p><em>Source:</em> <a href=""https://cpj.org/killed/"" target=""_blank"">Committee to Protect Journalists</a></p>
This dataset was created by Journalism, News, and Media and contains around 2000 samples along with Date, Unnamed: 18, technical information and other features such as:
- Local/ Foreign
- Unnamed: 20
- and more.
How to use this dataset
&gt; - Analyze Coverage in relation to Taken Captive
- Study the influence of Organization on Unnamed: 21
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Journalism, News, and Media 
Start A New Notebook!"	105	647	13	yamqwe	journalist-deathse
1299	1299	tmax_tmin_precipDataYearly201Cities		['united states', 'tabular data']	"Context
Proccesed data of 201 weather stations within the US.
Content
Two separated datasets city_info.csv contains the positional information of the 201 cities within the US, the other dataset(compiled_data.pkl) is the information by date of the 201 stations
Acknowledgements
This is only a compiled work of the original separated datasets, all the rights are of the authors mentioned.
Inspiration
This is a good dataset for learning mapping and creating animated timelines."	0	14	0	jibo16	tmax-tmin-precipdatayearly201cities
1300	1300	global sales		[]		5	9	0	ahmedallawy	global-sales
1301	1301	Post-storm data for cyclones way back from 1949	A compilation of post-storm data for cyclones ranging all the way back to 1949	['natural disasters']	"About this dataset
&gt; <p><code>In-the-News</code>:</p>
<ul>
<li>NBC News: <a href=""http://www.nbcnews.com/news/us-news/how-hurricanes-have-disrupted-defined-past-elections-n660796"" target=""_blank"" rel=""nofollow"">How Hurricanes Have Disrupted and Defined Past Elections<br>
</a></li>
<li>The New York Times: <a href=""http://www.nytimes.com/2016/10/07/us/hurricane-matthew.html"" target=""_blank"" rel=""nofollow"">Hurricane Matthew, ‘Extremely Dangerous,’<br>
Moves North Near Florida Coast</a></li>
<li>Bustle: <a href=""https://www.bustle.com/articles/188289-will-hurricane-matthew-be-as-bad-as-hurricane-katrina-preparation-could-still-make-a-world-of"" target=""_blank"" rel=""nofollow"">Will Hurricane Matthew Be As Bad As Hurricane Katrina? Preparation Could Still Make A World Of Difference<br>
</a></li>
</ul>
<p><img src=""http://i.imgur.com/7yXqWDy.gif"" alt=""Hurricane Katrina"" title=""Hurricane Katrina"" style=""""><br>
<em>Image from <a href=""https://commons.wikimedia.org/wiki/File:Hurricane_Katrina_LA_landfall_radar.gif"" target=""_blank"" rel=""nofollow"">Wikimedia Commons</a></em>.</p>
<p>HURDAT2 contains the best track data for cyclones ranging all the way back to 1949 for those that originated in the Northeast and North Central Pacific, and 1851 for those that originated in the Atlantic. The dataset is a compilation of the post-storm analyses prepared by The National Hurricane Center and the Central Pacific Hurricane Center.</p>
<p><strong>Notes</strong></p>
<ol>
<li>Basin: This designation refers to where the cyclone formed (typically via tropical depression status.)
<ul>
<li>AL - THe Atlantic</li>
<li>EP - in the North Pacific east of 140 deg W</li>
<li>CP - Between 140 and 180 deg W</li>
</ul>
</li>
<li>Name: Tropical cyclones were not formally named before 1950 and are thus referred to as “UNNAMED” in the database. Systems that were added into the<br>
database after the season (such as AL20 in 2011) also are considered “UNNAMED”. Non-developing tropical depressions formally were given names (actually numbers, such as “TEN”) that were included into the ATCF b-decks starting in 2003. Non-developing tropical depressions before this year are also referred to as “UNNAMED”.</li>
<li>Record identifier: This code is used to identify records that correspond to landfalls or to indicate the reason for inclusion of a record not at the standard synoptic times (0000, 0600, 1200, and 1800 UTC). For the years 1851-1955, 1969’s Camille, and 1991 onward, all continental United States landfalls are marked, while international landfalls are only marked from 1951 to 1955 and 1991 onward. The landfall identifier (L) is the only identifier that will appear with a standard synoptic time record. The remaining identifiers (see table above) are only used with asynoptic records to indicate the reason for their inclusion. Inclusion of asynoptic data is at the discretion of the Hurricane Specialist who performed the post-storm analysis; standards for inclusion or non-inclusion have varied over time. Identification of asynoptic peaks in intensity (either wind or pressure) may represent either system’s lifetime peak or a secondary peak.
<ul>
<li>L – Landfall (center of system crossing a coastline)</li>
<li>P – Minimum in central pressure</li>
<li>I – An intensity peak in terms of both pressure and maximum wind</li>
<li>S – Change of status of the system</li>
<li>T – Provides additional detail on the track (position) of the cyclone</li>
</ul>
</li>
<li>Time: Nearly all HURDAT2 records correspond to the synoptic times of 0000, 0600, 1200, and 1800. Recording best track data to the nearest minute became<br>
available within the b-decks beginning in 1991 and some tropical cyclones since that year have the landfall best track to the nearest minute.</li>
<li>Status: Tropical cyclones with an ending tropical depression status (the dissipating stage) were first used in the best track beginning in 1871, primarily for systems weakening over land. Tropical cyclones with beginning tropical depression (the formation stage) were first included in the best track beginning in 1882.<br>
Subtropical depression and subtropical storm status were first used beginning in 1968 at the advent of routine satellite imagery for the Atlantic basin. The low status – first used in 1987 - is for cyclones that are not tropical cyclone or subtropical cyclones, nor extratropical cyclones. These typically are assigned at the beginning of a system’s lifecycle and/or at the end of a system’s lifecycle. The tropical wave status – first used in 1981 - is almost exclusively for cyclones that degenerate into an open trough for a time, but then redevelop later in time into a tropical cyclone (for example, AL10-DENNIS in 1981 between 13 and 15 August). The disturbance status is similar to tropical wave and was first used in 1980. It should be noted that for tropical wave and disturbance status the location given is the approximate position of the lower tropospheric vorticity center, as the surface center no longer exists for these stages.
<ul>
<li>TD – Tropical cyclone of tropical depression intensity (&lt; 34 knots)</li>
<li>TS – Tropical cyclone of tropical storm intensity (34-63 knots)</li>
<li>HU – Tropical cyclone of hurricane intensity (&gt; 64 knots)</li>
<li>EX – Extratropical cyclone (of any intensity)</li>
<li>SD – Subtropical cyclone of subtropical depression intensity (&lt; 34 knots)</li>
<li>SS – Subtropical cyclone of subtropical storm intensity (&gt; 34 knots)</li>
</ul>
</li>
<li>Maximum sustained surface wind: This is defined as the maximum 1-min average wind associated with the tropical cyclone at an elevation of 10 m with an unobstructed exposure. Values are given to the nearest 10 kt for the years 1851 through 1885 and to the nearest 5 kt from 1886 onward. A value is assigned for every cyclone at every best track time. Note that the non-developing tropical depressions of 1967 did not have intensities assigned to them in the b-decks. These are indicated as “-99” currently, but will be revised and assigned an intensity when the Atlantic hurricane database reanalysis project (Hagen et al. 2012) reaches that hurricane season.</li>
<li>Central Pressure: These values are given to the nearest millibar. Originally, central pressure best track values were only included if there was a specific observation that could be used explicitly. Beginning in 1979, central pressures have been analyzed and included for every best track entry, even if there was not a specific in-situ measurement available.</li>
<li>Wind Radii – These values are available with a resolution to the nearest 5 nm. Note that occasionally when there is a non-synoptic time best track entry included for either landfall or peak intensity, that the wind radii best tracks were not provided.</li>
</ol>
<p>Source: <a href=""http://www.nhc.noaa.gov/data/"" target=""_blank"" rel=""nofollow"">NOAA</a></p>
This dataset was created by National Oceanic and Atmospheric Administration (NOAA) and contains around 30000 samples along with 34kt Ne, 64kt Sw, technical information and other features such as:
- 64kt Nw
- Num Of Track Entries
- and more.
How to use this dataset
&gt; - Analyze 50kt Se in relation to Longitude
- Study the influence of Latitude on Basin
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Oceanic and Atmospheric Administration (NOAA) 
Start A New Notebook!"	49	219	6	yamqwe	hurdat2e
1302	1302	listlisttest.txt		[]		0	0	0	goonmyamya	list02
1303	1303	mmtracking		['earth and nature']	"Description
This datasets contains mmtracking package and its dependencies for an offline installation. Use mmdetection dataset to install mmdetection (it's also required by mmtracking).
What's MMTracking?
OpenMMLab Video Perception Toolbox. It supports Video Object Detection (VID), Multiple Object Tracking (MOT), Single Object Tracking (SOT), Video Instance Segmentation (VIS) with a unified framework.
Source
https://github.com/open-mmlab/mmtracking
Documentation
https://mmtracking.readthedocs.io/"	0	17	1	atamazian	mmtracking
1304	1304	COVID-19 - Italy Data	Evolution of Coronavirus in Italy with daily data from the italian government 	['health', 'covid19']	"Public data - Covid-19 daily evolution in Italy
This is a 1:1 pull from the official government github page: https://github.com/pcm-dpc/COVID-19
Please use this data for good and help us (and your home country!) in the fight to Covid-19.
For a full guide on data, please refer to (italian): https://github.com/pcm-dpc/COVID-19/blob/master/README.md
In each folder you will find all the single daily input .csv files as a single row, and a master file which contains all the daily data since 2020-02-24.
Dati-andamento-nazionale folder contains all daily data at the national level.
Master file: ""/kaggle/input/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv""**
Dati-regioni is a folder at Regional level.
Master file: ""dpc-covid19-ita-regioni.csv""
Finally, Dati-province is at city level.
Master file:  ""dpc-covid19-ita-province.csv"""	167	7802	12	lorenzopagliaro01	coronavirus-italian-data
1305	1305	Mobile Food Facility Permit	Including name of vendor, location, type of food sold and status of permit.	['business', 'plotly', 'seaborn', 'sklearn']		2	7	4	qusaybtoush	mobile-food-facility-permit
1306	1306	😷 Community Health Status Indicators (CHSI)	Community health data from the CHSI report	['healthcare', 'public health', 'health', 'health conditions', 'cancer', 'news']	"About this dataset
&gt; <p>Community Health Status Indicators (CHSI) to combat obesity, heart disease, and cancer are major components of the Community Health Data Initiative. This dataset provides key health indicators for local communities and encourages dialogue about actions that can be taken to improve community health (e.g., obesity, heart disease, cancer). The CHSI report and dataset was designed not only for public health professionals but also for members of the community who are interested in the health of their community. The CHSI report contains over 200 measures for each of the 3,141 United States counties. Although CHSI presents indicators like deaths due to heart disease and cancer, it is imperative to understand that behavioral factors such as obesity, tobacco use, diet, physical activity, alcohol and drug use, sexual behavior and others substantially contribute to these deaths.</p>
<ul>
<li>DATA_ELEMENT_DESCRIPTION.csv defines each data element and indicates where its description is found in Data Sources, Definitions, and Notes.</li>
<li>DEFINED_DATA_VALUE.csv defines the meaning of specific values (such as missing or suppressed data).</li>
<li>HEALTHY_PEOPLE_2010.csv identifies the Healthy People 2010 Targets and the U.S. Percentages or Rates.</li>
<li>DEMOGRAPHICS.csv identifies the data elements and values in the Demographics indicator domain.</li>
<li>LEADING_CAUSES_OF_DEATH.csv identifies the data elements and values in the Leading Causes of Death indicator domain.</li>
<li>SUMMARY_MEASURES_OF_HEALTH.csv identifies the data elements and values in the Summary Measures of Health indicator domain.</li>
<li>MEASURES_OF_BIRTH_AND_DEATH.csv identifies the data elements and values in the Measures of Birth and Death indicator domain.</li>
<li>RELATIVE_HEALTH_IMPORTANCE.csv identifies the data elements and values in the Relative Health Importance indicator domain.</li>
<li>VULNERABLE_POPS_AND_ENV_HEALTH.csv identifies the data elements and values in the Vulnerable Populations and Environmental Health indicator domain.</li>
<li>PREVENTIVE_SERVICES_USE.csv identifies the data elements and values in the Preventive Services indicator domain.</li>
<li>RISK_FACTORS_AND_ACCESS_TO_CARE.csv identifies the data elements and values in the Risk Factors and Access to Care indicator domain.</li>
</ul>
<p>Source: <a href=""https://catalog.data.gov/dataset/community-health-status-indicators-chsi-to-combat-obesity-heart-disease-and-cancer#sec-dates"" target=""_blank"">https://catalog.data.gov/dataset/community-health-status-indicators-chsi-to-combat-obesity-heart-disease-and-cancer#sec-dates</a></p>
This dataset was created by Data Society and contains around 3000 samples along with Ci Min E Hi Cancer, Ci Max D Wh Suicide, technical information and other features such as:
- Ci Min D Bl Heart Dis
- Ci Min D Wh Injury
- and more.
How to use this dataset
&gt; - Analyze Ci Max E Hi Cancer in relation to D Bl Injury
- Study the influence of Ci Min D Ot Hiv on C Ot Homicide
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	31	297	6	yamqwe	health-status-indicatorse
1307	1307	Violent Crime & Property Crime Statewide Totals	Violent Crime & Property Crime Statewide Totals: 1975 to Present	['crime', 'logistic regression', 'linear regression', 'pandas', 'seaborn']	"Violent Crime & Property Crime Statewide Totals: 1975 to Present
The data are provided are the Maryland Statistical Analysis Center (MSAC), within the Governor's Office of Crime Control and Prevention (GOCCP). MSAC, in turn, receives these data from the FBI's annual Uniform Crime Reports."	0	9	4	qusaybtoush	violent-crime-property-crime-statewide-totals
1308	1308	Most Popular Baby Names 	Most Popular Baby Names 2008-2017 City Of Austin	['matplotlib', 'numpy', 'pandas', 'plotly', 'seaborn']	"Most Popular Baby Names 2008-2017 City Of Austin
Provides the most popular names used for children born in the City of Austin. An asterisk (*) denotes names with equal number of occurrences"	1	17	8	qusaybtoush	most-popular-baby-names
1309	1309	 NYC Healthy Relationship	Survey Findings from the NYC Healthy Relationship Training Academy 	['logistic regression', 'linear regression', 'matplotlib', 'pandas', 'seaborn']	"NYC Healthy Relationship
Survey Findings from the NYC Healthy Relationship Training Academy 
This data set contains pre and post workshop survey level data of TechnoLOVE workshops conducted by the New York City Healthy Relationship Academy between 2010 and 2018."	0	4	3	qusaybtoush	nyc-healthy-relationship
1310	1310	DPD Scout Car Areas	From the Detroit Police Department.	['automobiles and vehicles', 'logistic regression', 'linear regression', 'matplotlib', 'pandas']	"DPD Scout Car Areas
From the Detroit Police Department.
Scout Car Areas are subdivisions of precincts to which officers can be assigned for patrol."	1	5	6	qusaybtoush	dpd-scout-car-areas
1311	1311	One Million Reddit Confessions	One million confessions from Reddit's users and past 30 years.	['nlp', 'online communities', 'social networks']	"About this dataset
&gt; <h2>NOTICE</h2>
<p>Due to the platform's limitations, we can only provide a sample of this dataset. Please download the full version (free, no registration) from <a href=""https://socialgrep.com/datasets?utm_source=dataworld&amp;utm_medium=link&amp;utm_campaign=onemillionconfessions#one-million-reddit-confessions"" target=""_blank"" rel=""nofollow"">SocialGrep</a>.</p>
<h3>Context</h3>
<p>For one reason or another, people are compelled to be frank with strangers. Whether it's making a fast friend on a train ride, or posting an anonymous confession online, we just tend to find it easier to let our secrets out to someone we'll never know again. A brief, beautiful window of candid honesty is somewhere in there. That's what this dataset was inspired by.</p>
<h3>Content</h3>
<p>The following dataset comprises a million confession posts from Sep 30 2021 and backwards, proportionally taken from the following subreddits:</p>
<ul>
<li>/r/trueoffmychest</li>
<li>/r/confession</li>
<li>/r/confessions</li>
<li>/r/offmychest</li>
</ul>
<p>All the posts are annotated with their score.</p>
<p>The dataset was procured using <a href=""https://socialgrep.com/datasets?utm_source=dataworld&amp;utm_medium=link&amp;utm_campaign=onemillionconfessions#one-million-reddit-confessions"" target=""_blank"" rel=""nofollow"">SocialGrep</a>.</p>
<p>To preserve users' anonymity and to prevent targeted harassment, the data does not include usernames.</p>
<h3>Inspiration</h3>
<p>In this dataset, we wanted to explore the nature of sympathy. Which confessions are met with forgiveness? Which aren't? It's our most candid corpus to date.</p>
This dataset was created by SocialGrep and contains around 100 samples along with Subreddit.nsfw, Domain, technical information and other features such as:
- Subreddit.name
- Subreddit.id
- and more.
How to use this dataset
&gt; - Analyze Type in relation to Score
- Study the influence of Selftext on Url
- More datasets
Acknowledgements
If you use this dataset in your research, please credit SocialGrep 
Start A New Notebook!"	66	845	7	yamqwe	one-million-reddit-confessions-samplee
1312	1312	⭐ Hotel Reviews	A list of 1,000 hotels and their online reviews.	['business', 'travel', 'hotels and accommodations']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of 1,000 hotels and their reviews provided by <a href=""https://datafiniti.co/products/business-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Business Database</a>.  The dataset includes hotel location, name, rating, review data, title, username, and more.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with This Data</h1>
<p>You can use this data to <a href=""https://datafiniti.co/state-state-comparison-hotel-reviews/"" target=""_blank"" rel=""nofollow"">compare hotel reviews on a state-by-state basis</a>; experiment with sentiment scoring and other natural language processing techniques. The review data lets you correlate keywords in the review text with ratings. E.g.:</p>
<ul>
<li>What are the bottom and top states for hotel reviews by average rating?</li>
<li>What is the correlation between a state’s population and their number of hotel reviews?</li>
<li>What is the correlation between a state’s tourism budget and their number of hotel reviews?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/business-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>You can access the full dataset by running the following query with <a href=""https://developer.datafiniti.co/docs/getting-started-with-business-data"" target=""_blank"" rel=""nofollow"">Datafiniti’s Business API</a>.</p>
<p><code>{ ""query"": ""dateUpdated:[2018-01-01 TO *] AND categories:(Hotel OR Hotels) AND country:US* AND name:* AND reviews:* AND sourceURLs:*"", ""format"": ""csv"", ""download"": true, ""view"": ""datasets_hotel_reviews"" }</code></p>
<p>*<em>This query generated 61,544 records as of December 10, 2018. The total number of results may vary.</em></p>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/business-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 10000 samples along with Date Updated, Country, technical information and other features such as:
- Name
- Reviews.username
- and more.
How to use this dataset
&gt; - Analyze Reviews.date Seen in relation to Province
- Study the influence of Address on Reviews.date Added
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	56	537	7	yamqwe	hotel-reviewse
1313	1313	🚌 Mayors of Top 100 US Cities	Datasets of major cities including population, mayor names, and city budgest	['cities and urban areas', 'government', 'politics']	"About this dataset
&gt; <p>Information on America's largest cities including population, mayor names, and city budgest.</p>
<p>Source: <a href=""https://ballotpedia.org/Largest_cities_in_the_United_States_by_population"" target=""_blank"" rel=""nofollow"">Ballotpedia, America's largest cities</a></p>
<p>Data Last Collected 4/6/2017</p>
This dataset was created by Government and contains around 100 samples along with Budget, Elections In 2017?, technical information and other features such as:
- Budget
- Elections In 2017?
- and more.
How to use this dataset
&gt; - Analyze Budget in relation to Elections In 2017?
- Study the influence of Budget on Elections In 2017?
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Government 
Start A New Notebook!"	27	168	4	yamqwe	mayors-of-top-100-us-citiese
1314	1314	Augmented MNIST 3D	3D version of MNIST with augmented features like rotation and colors	['artificial intelligence', 'computer science', 'computer vision']	"Context
This dataset consists of an augmented 3D version of MNIST handwritten digit dataset.
GIthub repo: https://github.com/doleron/augmented_3d_mnist
medium story: https://medium.com/@doleron/start-using-3d-images-today-845f959a6b0b
Augmentation
The following transformations are applied in order to generate a 3D digit from the a 2D image:
1 - dilating: 
2 - noise: 
3 - coloring: 
4 - rotation: 
Inspiration
This dataset is inspired on 3D MNIST dataset which provided good insights. Anyway, I prefered to build a new version of 3D MNIST to allow users to fine control augmented features such as rotation and colors."	0	82	2	doleron	augmented-mnist-3d
1315	1315	Employee		[]		0	3	0	pujandey	employee
1316	1316	Ubiquant Pickle Data	Ubiquant_Pickle_Data	[]	"Code to create Pickle Files
[transform_csv2pickle] referred to https://www.kaggle.com/columbia2131/speed-up-reading-csv-to-pickle
First, create train.pkl with ""transform_csv2pickle""
train.pkl is whole train.csv
df = pd.read_pickle(""../input/ubiquant-pickle-data/train.pkl"")
display(df.info())
display(df.dtypes)
display(df)
df_train = df.loc[df[""time_id""] &lt; 1000]
df_train.to_pickle(""df_train.pkl"")
df_test = df.loc[df[""time_id""] &gt;= 1000]
df_test.to_pickle(""df_test.pkl"")
train_df = df_train.loc[df_train[""time_id""] &lt; 800]
train_df.to_pickle(""train_df.pkl"")
valid_df = df_train.loc[df_train[""time_id""] &gt;= 800]
valid_df.to_pickle(""valid_df.pkl"")"	0	38	0	alphamasterss	ubiquant-pickle-data
1317	1317	Global Male Circumcision (2021 estimation)	Male Circumcision by country and region	['global', 'gender', 'health']	"About this dataset
&gt; <h1>Original Visualization</h1>
<p>​</p>
<p><img src=""https://mediauploads.data.world/9e23df1322888ee64f146e95209b842a3e7313119be226a77aff8f287ffd4af4_ij5bedwq52j71.png"" alt="""" style=""display: none;""></p>
<p>​</p>
<h1>Resources</h1>
<p>Original viz: <a href=""https://www.reddit.com/r/MapPorn/comments/p9umhy/percentage_of_males_who_are_circumcised_in_each/"" target=""_blank"" rel=""nofollow"">Reddit</a><br>
Data Source &amp; Citation:<br>
Morris, B.J., Wamai, R.G., Henebeng, E.B. <em>et al.</em> Estimation of country-specific and global prevalence of male circumcision. <em>Population Health Metrics</em> <strong>14,</strong> 4 (2016). <a href=""https://doi.org/10.1186/s12963-016-0073-5"" target=""_blank"" rel=""nofollow"">https://doi.org/10.1186/s12963-016-0073-5</a></p>
This dataset was created by Andy Kriebel and contains around 200 samples along with Sub Region, Alpha 2 Country Code, technical information and other features such as:
- Male Circumcision %
- Basis
- and more.
How to use this dataset
&gt; - Analyze Region in relation to Intermediate Region
- Study the influence of Sub Region on Alpha 2 Country Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	17	287	2	yamqwe	2021-w35-estimation-of-global-male-circumcisione
1318	1318	🚀 Kepler Confirmed Exoplanets	Confirmed Exoplanets Discovered by the Kepler Telescope	['astronomy', 'science and technology']	"About this dataset
&gt; <h2>Kepler Telescope</h2>
<p>Updates!<br>
Over 100 <strong>confirmed</strong> exoplanets were found during Kepler's K2 mission.</p>
<p><a href=""https://data.world/markmarkoh/kepler-confirmed-planets/query?query=%23+Kepler+planets+discovered+during+K2+mission%0A%0APREFIX+planets%3A+%3Chttp%3A%2F%2Fdata.world%2Fmarkmarkoh%2Fkepler-confirmed-planets%2Fplanets.csv%2Fplanets%23%3E%0APREFIX+rdf%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%3E%0APREFIX+xsd%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2FXMLSchema%23%3E%0A%0ASELECT+%3Frowupdate+%3Frowid+%3Fpl_hostname+%3Fpl_letter+%3Fpl_discmethod+%3Fpl_pnum+%3Fpl_orbper%0AWHERE+%7B%0A%3Fs+rdf%3Atype+%3Chttp%3A%2F%2Fdata.world%2Fmarkmarkoh%2Fkepler-confirmed-planets%2Fplanets.csv%2Fplanets%23%3E.%0A+%3Fs+planets%3Arowid+%3Frowid.++%0A+%3Fs+planets%3Apl_hostname+%3Fpl_hostname.++%0A+%3Fs+planets%3Apl_letter+%3Fpl_letter.+%0A+%3Fs+planets%3Apl_discmethod+%3Fpl_discmethod.%0A+%3Fs+planets%3Apl_pnum+%3Fpl_pnum.%0A+%3Fs+planets%3Apl_orbper+%3Fpl_orbper.%0A+%3Fs+planets%3Arowupdate+%3Frowupdate.%0A+FILTER+REGEX%28%3Fpl_hostname%2C+%22K2-%22%2C+%22i%22%29%0A%7D%0A"">Check out the new planets here</a>, and read the <a href=""https://uanews.arizona.edu/story/ualed-team-confirms-100plus-exoplanets-keplers-k2-mission"" target=""_blank"" rel=""nofollow"">K2 Mission result announcement</a>.</p>
<p>The scientific objective of the Kepler Mission is to explore the structure and diversity of planetary systems. This is achieved by surveying a large sample of stars to:</p>
<ol>
<li>Determine the abundance of terrestrial and larger planets in or near the habitable zone of a wide variety of stars;</li>
<li>Determine the distribution of sizes and shapes of the orbits of these planets;</li>
<li>Estimate how many planets there are in multiple-star systems;</li>
<li>Determine the variety of orbit sizes and planet reflectivities, sizes, masses and densities of short-period giant planets;</li>
<li>Identify additional members of each discovered planetary system using other techniques; and</li>
<li>Determine the properties of those stars that harbor planetary systems.</li>
</ol>
<p>The Kepler Mission also supports the objectives of future NASA Origins theme missions Space Interferometry Mission (SIM) and Terrestrial Planet Finder (TPF),</p>
<ol>
<li>By identifying the common stellar characteristics of host stars for future planet searches,</li>
<li>By defining the volume of space needed for the search and</li>
<li>By allowing SIM to target systems already known to have terrestrial planets.</li>
</ol>
<p>(<a href=""http://kepler.nasa.gov/Mission/QuickGuide/"" target=""_blank"" rel=""nofollow"">Source</a>)</p>
<h2>Helpful links</h2>
<p><a href=""http://kepler.nasa.gov/Mission/QuickGuide/"" target=""_blank"" rel=""nofollow"">http://kepler.nasa.gov/Mission/QuickGuide/</a></p>
<p><a href=""http://exoplanetarchive.ipac.caltech.edu/index.html"" target=""_blank"" rel=""nofollow"">http://exoplanetarchive.ipac.caltech.edu/index.html</a></p>
<p><a href=""http://kepler.nasa.gov/"" target=""_blank"" rel=""nofollow"">http://kepler.nasa.gov/</a></p>
<p><a href=""http://www.nytimes.com/2015/07/24/science/space/kepler-data-reveals-what-might-be-best-goldilocks-planet-yet.html"" target=""_blank"" rel=""nofollow"">http://www.nytimes.com/2015/07/24/science/space/kepler-data-reveals-what-might-be-best-goldilocks-planet-yet.html</a></p>
<h2>Schema</h2>
<p>This file was produced by the NASA Exoplanet Archive  <a href=""http://exoplanetarchive.ipac.caltech.edu"" target=""_blank"" rel=""nofollow"">http://exoplanetarchive.ipac.caltech.edu</a></p>
<pre><code>COLUMN pl_hostname:    Host Name
COLUMN pl_letter:      Planet Letter
COLUMN pl_discmethod:  Discovery Method
COLUMN pl_pnum:        Number of Planets in System
COLUMN pl_orbper:      Orbital Period [days]
COLUMN pl_orbpererr1:  Orbital Period Upper Unc. [days]
COLUMN pl_orbpererr2:  Orbital Period Lower Unc. [days]
COLUMN pl_orbperlim:   Orbital Period Limit Flag
COLUMN pl_orbsmax:     Orbit Semi-Major Axis [AU]
COLUMN pl_orbsmaxerr1: Orbit Semi-Major Axis Upper Unc. [AU]
COLUMN pl_orbsmaxerr2: Orbit Semi-Major Axis Lower Unc. [AU]
COLUMN pl_orbsmaxlim:  Orbit Semi-Major Axis Limit Flag
COLUMN pl_orbeccen:    Eccentricity
COLUMN pl_orbeccenerr1: Eccentricity Upper Unc.
COLUMN pl_orbeccenerr2: Eccentricity Lower Unc.
COLUMN pl_orbeccenlim: Eccentricity Limit Flag
COLUMN pl_orbincl:     Inclination [deg]
COLUMN pl_orbinclerr1: Inclination Upper Unc. [deg]
COLUMN pl_orbinclerr2: Inclination Lower Unc. [deg]
COLUMN pl_orbincllim:  Inclination Limit Flag
COLUMN pl_bmassj:      Planet Mass or M*sin(i)[Jupiter mass]
COLUMN pl_bmassjerr1:  Planet Mass or M*sin(i)Upper Unc. [Jupiter mass]
COLUMN pl_bmassjerr2:  Planet Mass or M*sin(i)Lower Unc. [Jupiter mass]
COLUMN pl_bmassjlim:   Planet Mass or M*sin(i)Limit Flag
COLUMN pl_bmassprov:   Planet Mass or M*sin(i) Provenance
COLUMN pl_radj:        Planet Radius [Jupiter radii]
COLUMN pl_radjerr1:    Planet Radius Upper Unc. [Jupiter radii]
COLUMN pl_radjerr2:    Planet Radius Lower Unc. [Jupiter radii]
COLUMN pl_radjlim:     Planet Radius Limit Flag
COLUMN pl_dens:        Planet Density [g/cm**3]
COLUMN pl_denserr1:    Planet Density Upper Unc. [g/cm**3]
COLUMN pl_denserr2:    Planet Density Lower Unc. [g/cm**3]
COLUMN pl_denslim:     Planet Density Limit Flag
COLUMN pl_ttvflag:     TTV Flag
COLUMN pl_kepflag:     Kepler Field Flag
COLUMN pl_k2flag:      K2 Mission Flag
COLUMN pl_nnotes:      Number of Notes
COLUMN ra_str:         RA [sexagesimal]
COLUMN ra:             RA [sexagesimal]
COLUMN dec_str:        Dec [sexagesimal]
COLUMN dec:            Dec [sexagesimal]
COLUMN st_dist:        Distance [pc]
COLUMN st_disterr1:    Distance Upper Unc. [pc]
COLUMN st_disterr2:    Distance Lower Unc. [pc]
COLUMN st_distlim:     Distance Limit Flag
COLUMN st_optmag:      Optical Magnitude [mag]
COLUMN st_optmagerr:   Optical Magnitude Unc. [mag]
COLUMN st_optmaglim:   Optical Magnitude Limit Flag
COLUMN st_optmagblend: Optical Magnitude Blend Flag
COLUMN st_optband:     Optical Magnitude Band
COLUMN st_teff:        Effective Temperature [K]
COLUMN st_tefferr1:    Effective Temperature Upper Unc. [K]
COLUMN st_tefferr2:    Effective Temperature Lower Unc. [K]
COLUMN st_tefflim:     Effective Temperature Limit Flag
COLUMN st_teffblend:   Effective Temperature Blend Flag
COLUMN st_mass:        Stellar Mass [Solar mass]
COLUMN st_masserr1:    Stellar Mass Upper Unc. [Solar mass]
COLUMN st_masserr2:    Stellar Mass Lower Unc. [Solar mass]
COLUMN st_masslim:     Stellar Mass Limit Flag
COLUMN st_massblend:   Stellar Mass Blend Flag
COLUMN st_rad:         Stellar Radius [Solar radii]
COLUMN st_raderr1:     Stellar Radius Upper Unc. [Solar radii]
COLUMN st_raderr2:     Stellar Radius Lower Unc. [Solar radii]
COLUMN st_radlim:      Stellar Radius Limit Flag
COLUMN st_radblend:    Stellar Radius Blend Flag
COLUMN rowupdate:      Date of Last Update    
</code></pre>
This dataset was created by Mark Di Marco and contains around 3000 samples along with Pl Orbperlim, Pl Orbsmaxerr2, technical information and other features such as:
- Pl Orbpererr1
- Pl Orbeccenerr2
- and more.
How to use this dataset
&gt; - Analyze St Optband in relation to Pl Radj
- Study the influence of Pl Denserr1 on St Masslim
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Mark Di Marco 
Start A New Notebook!"	13	143	6	yamqwe	kepler-confirmed-planetse
1319	1319	Notebooks of the Week - Hidden Gems	Complete collection of the weekly series showcasing underrated Kaggle Notebooks	['exploratory data analysis', 'data visualization', 'online communities']	"Context
On Kaggle, many great Notebooks (aka ""Kernels"") can be found that have a large number of well-deserved upvotes. I'm publishing a weekly series of ""Hidden Gems"" in which I aspire to find and showcase underrated Notebooks on Kaggle in an effort to illuminate the diversity and creativity of our community.
This series looks beyond those well-recognised works to find Notebooks which I personally like and which, somehow, didn't receive the attention that I think they deserve. My selections are always highly subjective; reflecting my own views and preferences. I have managed to publish an episode of ""Hidden Gems"" every week since the first post back in May 2020 and I hope to be able to keep this weekly schedule for the foreseeable future.
Each episode presents 3 Notebooks, which can be recent or older works. These episodes are first posted to the Kaggle Forum (later on Twitter/LinkedIn), and I always encourage readers to contribute any other underestimated Notebooks in the comments of the forum posts. The title and link to the Notebook are accompanied by the author's user name and a brief description on what I like about this work.
This dataset contains all published episodes, with the goal to make the archive of ""Hidden Gems"" better accessible and to enable meta analysis.
Content
The file of interest is kaggle_hidden_gems.csv, containing the following columns:
vol and date: The consecutive number of the Hidden Gems episode and when it was first published.
link_forum and link_twitter: The hyperlinks to the Kaggle Forum post and Twitter post for the episode.
notebook and author: The hyperlinks to the Notebook itself, as well as to the Kaggle profile of the author.
title: The Notebook title as a string.
review: My brief review of the Notebook.
author_name: The name of the Notebook author as listed on their Kaggle profile at the time the episode was published.
author_twitter and author_linkedin: The social media links of the author, if listed on their Kaggle profile.
notes: Notes about special episodes.
Acknowledgements
I would like to thank all current and future Notebook authors for their inspiring contributions. Your hard work and brilliance is what makes this community so amazing.
Cover image by Shachar Harshuv.
Inspiration
My starter Notebook contains a number of suggestions on what to explore. Linking this dataset to Meta Kaggle will provide even more opportunities for extracting insights. Have fun!"	85	5169	84	headsortails	notebooks-of-the-week-hidden-gems
1320	1320	output_forex_test		[]		0	4	0	rammellutz	output-forex-test
1321	1321	⚡ Household Power Consumption Timeseries 	260,640 Measurements gathered for predicting household power consumption	['business', 'energy', 'electricity']	"About this dataset
&gt; <h2>Data Set Information</h2>
<p>This household electricity consumption dataset contains 260,640 measurements gathered between January 2007 and June 2007 (6 months). It is a subset of a larger, original <a href=""https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption"" target=""_blank"" rel=""nofollow"">archive</a> that contains 2,075,259 measurements gathered between December 2006 and November 2010 (47 months).</p>
<h2>Attribute Information</h2>
<ol>
<li>date: Date in format dd/mm/yyyy</li>
<li>time: time in format hh:mm:ss</li>
<li>global_active_power: household global minute-averaged active power (in kilowatt)</li>
<li>global_reactive_power: household global minute-averaged reactive power (in kilowatt)</li>
<li>voltage: minute-averaged voltage (in volt)</li>
<li>global_intensity: household global minute-averaged current intensity (in ampere)</li>
<li>sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).</li>
<li>sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.</li>
<li>sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.</li>
</ol>
<h2>Notes</h2>
<ol>
<li>(global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3.</li>
<li>The dataset contains some missing values in the measurements (nearly 1.25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.</li>
</ol>
<h2>Original Source</h2>
<p>Georges Hébrail, (georges.hebrail '@' <a href=""http://edf.fr"" target=""_blank"" rel=""nofollow"">edf.fr</a>), Senior Researcher, EDF R&amp;D, Clamart, France<br>
Alice Bérard, TELECOM ParisTech Master of Engineering Internship at EDF R&amp;D, Clamart, France</p>
<h2>Provider</h2>
<p>This dataset was downloaded from the UCI Machine Learning Repository on 11/8/2016</p>
This dataset was created by Jonathan Ortiz and contains around 300000 samples along with Voltage, Global Intensity, technical information and other features such as:
- Sub Metering 3
- Global Active Power
- and more.
How to use this dataset
&gt; - Analyze Date in relation to Sub Metering 2
- Study the influence of Global Reactive Power on Time
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jonathan Ortiz 
Start A New Notebook!"	41	363	3	yamqwe	household-power-consumptione
1322	1322	Weather classification J Component		[]		0	22	0	saikiran19mis0226	weather-classification-j-component
1323	1323	ProjectDataSet	data set of minery and deforestation	[]		1	17	0	alcionegomez	projectdataset
1324	1324	🍷 Alcohol vs Life Expectancy	Life expectancy by country vs Alcohol consumption by country	['alcohol', 'health']	"Credit:  This dataset was created by Jonathan Ortiz! All credits for the original go to the original author!
About this dataset
<h1>Intro</h1>
2016""&gt;https://data.world/uncc-dsba/dsba-6100-fall-2016<p></p>
<h1>Findings</h1>
<p>There is a surprising relationship between alcohol consumption and life expectancy. In fact, the data suggest that life expectancy and alcohol consumption are positively correlated - 1.2 additional years for every 1 liter of alcohol consumed annually. This is, of course, a spurious finding, because the correlation of this relationship is very low - 0.28. This indicates that other factors in those countries where alcohol consumption is comparatively high or low are contributing to differences in life expectancy, and further analysis is warranted.</p>
<p><img src=""https://data.world/api/databeats/dataset/alcohol-vs-life-expectancy/file/raw/LifeExpectancy_v_AlcoholConsumption_Plot.jpg"" alt=""LifeExpectancy_v_AlcoholConsumption_Plot.jpg""></p>
<h1>Methods</h1>
<h2>1. Addressing Missing Values</h2>
<p>The original drinks.csv file in the <a href=""https://data.world/uncc-dsba/dsba-6100-fall-2016"">UNCC/DSBA-6100</a> dataset was missing values for The Bahamas, Denmark, and Macedonia for the wine, spirits, and beer attributes, respectively. Drinks_solution.csv shows these values filled in, for which I used the Mean of the rest of the data column.</p>
<p>Other methods were considered and ruled out:</p>
<ul>
<li><strong>Deleting the Bahamas, Denmark, and Macedonia instances altogether</strong> - This is a possible route, but the data file itself is just under 200 rows, and there is only one observation for each country. Because the dataset is relatively small by number of instances, removal should be avoided in order to give the model more data to use.</li>
<li><strong>Imputing missing values with k-Nearest Neighbors</strong> - Another possible route, knn impute can yield higher accuracy in certain cases when the dataset is fairly large. However, this particular dataset only contains 3 attributes, all of which seem unrelated to each other. If we had more columns with more data like availability, annual sales, preferences, etc. of the different drinks, it would be possible to predict these values with knn, but this approach should be avoided given the data we have.</li>
<li><strong>Filling missing values with a MODE</strong> - By visualizing the data, it is easy to see that each column is fairly skewed, with many countries reporting 0 in one or more of the servings columns. Using the MODE would fill these missing entries with 0 for all three (<code>beer_servings</code>, <code>spirit_servings</code>, and <code>wine_servings</code>), and upon reviewing the Bahamas, Denmark, and Macedonia more closely, it is apparent that 0 would be a poor choice for the missing values, as all three countries clearly consume alcohol.</li>
<li><strong>Filling missing values with MEDIAN</strong> - Due to the skewness mentioned just above in the MODE section, using a MEDIAN of the whole column would also be a poor choice, as the MEDIAN is pulled down by several countries reporting 0 or 1. A MEDIAN of only the observations reporting 1 or more servings--or another cutoff--could be used, however, and this would be acceptable.</li>
</ul>
<p><strong>Filling missing values with MEAN</strong> - In the case of the drinks dataset, this is the best approach. The MEAN averages for the columns happen to be very close to the actual data from where we sourced this exercise. In addition, the MEAN will not skew the data, which the prior approaches would do.</p>
<h2>2. Calculating New Attributes</h2>
<p>The original drinks.csv dataset also had an empty data column: <code>total_litres_of_pure_alcohol</code>. This column needed to be calculated in order to do a simple 2D plot and trendline. It would have been possible to instead run a multi-variable regression on the data and therefore skip this step, but this adds an extra layer of complication to understanding the analysis - not to mention the point of the exercise is to go through an example of calculating new attributes (or ""feature engineering"") using domain knowledge.</p>
<p>The graphic found at the <a href=""https://en.wikipedia.org/wiki/Standard_drink"" target=""_blank"">Wikipedia / Standard Drink</a> page shows the following breakdown:</p>
<ul>
<li>Beer - 12 fl oz per serving - 5% average ABV</li>
<li>Wing - 5 fl oz - 12% ABV</li>
<li>Spirits - 1.5 fl oz - 40% ABV</li>
</ul>
<p>The conversion factor from fl oz to L is 1 fl oz : 0.0295735 L</p>
<p>Therefore, the following formula was used to compute the empty column:<br>
<code>total_litres_of_pure_alcohol</code><br>
<code>=</code><br>
<code>(beer_servings * 12 fl oz per serving * 0.05 ABV + spirit_servings * 1.5 fl oz * 0.4 ABV + wine_servings * 5 fl oz * 0.12 ABV) * 0.0295735 liters per fl oz</code></p>
<h2>3. Joining To External Data</h2>
<p>The lifeexpectancy.csv datafile in the <a href=""https://data.world/uncc-dsba/dsba-6100-fall-2016"">https://data.world/uncc-dsba/dsba-6100-fall-2016</a> dataset contains life expectancy data for each country. The following query will join this data to the cleaned drinks.csv data file:</p>
<pre><code># Life Expectancy vs Alcohol Consumption
</code></pre>
<h1>Life Expectancy vs. Alcohol Consumption with countryTable</h1>
<pre><code>PREFIX drinks: &lt;http://data.world/databeats/alcohol-vs-life-expectancy/drinks_solution.csv/drinks_solution#&gt;
PREFIX life: &lt;http://data.world/uncc-dsba/dsba-6100-fall-2016/lifeexpectancy.csv/lifeexpectancy#&gt;
PREFIX countries: &lt;http://data.world/databeats/alcohol-vs-life-expectancy/countryTable.csv/countryTable#&gt;

SELECT ?country ?alc ?years
WHERE {
    SERVICE &lt;https://query.data.world/sparql/databeats/alcohol-vs-life-expectancy&gt; {
        ?r1 drinks:total_litres_of_pure_alcohol ?alc .
        ?r1 drinks:country ?country .
        ?r2 countries:drinksCountry ?country .
        ?r2 countries:leCountry ?leCountry .
    }

    SERVICE &lt;https://query.data.world/sparql/uncc-dsba/dsba-6100-fall-2016&gt; {
        ?r3 life:CountryDisplay ?leCountry .
        ?r3 life:GhoCode ?gho_code .
        ?r3 life:Numeric ?years .
        ?r3 life:YearCode ?reporting_year .
        ?r3 life:SexDisplay ?sex .
    }
FILTER ( ?gho_code = ""WHOSIS_000001"" && ?reporting_year = 2013 && ?sex = ""Both sexes"" )
}
ORDER BY ?country
</code></pre>
<h2>4. Plotting</h2>
<p>The resulting joined data can then be saved to local disk and imported into any analysis tool like Excel, Numbers, R, etc. to make a simple scatterplot. A trendline and R^2 should be added to determine the relationship between Alcohol Consumption and Life Expectancy (if any).</p>
<p><img src=""https://data.world/api/databeats/dataset/alcohol-vs-life-expectancy/file/raw/LifeExpectancy_v_AlcoholConsumption_Plot.jpg"" alt=""LifeExpectancy_v_AlcoholConsumption_Plot.jpg""></p>
This dataset was created by Jonathan Ortiz and contains around 200 samples along with Beer Servings, Spirit Servings, technical information and other features such as:
- Total Litres Of Pure Alcohol
- Wine Servings
- and more.
How to use this dataset
&gt; - Analyze Beer Servings in relation to Spirit Servings
- Study the influence of Total Litres Of Pure Alcohol on Wine Servings
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jonathan Ortiz 
Start A New Notebook!"	219	1508	3	yamqwe	alcohol-vs-life-expectancye
1325	1325	🚓 Every Reported Instance Of A Crime In Chicago	Every reported instance of a crime in the city of Chicago	['crime', 'public safety']	"About this dataset
&gt; <p>This dataset contains every reported instance of a crime in the city of Chicago from 01/01/2014 to 10/24/2016.</p>
<p>As of November 1st, 2016, there have been 605 murders and 3,003 shootings year-to-date, with 78 and 353, respectively, in October.</p>
<p><code>In-the-News</code>:</p>
<ul>
<li>ABC7: <a href=""http://abc7chicago.com/news/chicago-crime-october-was-2nd-deadliest-month-of-2016-police-say/1583397/"" target=""_blank"" rel=""nofollow"">October was 2nd Deadliest Month of 2016, Police Say</a></li>
<li>DNAinfo: <a href=""https://www.dnainfo.com/chicago/20161101/logan-square/fiddle-theft-logan-square-abby-lee-hood"" target=""_blank"" rel=""nofollow"">Thief Steals $1,500 Fiddle From Music Teacher's Car In Logan Square</a></li>
<li>NBCNews: <a href=""http://www.nbcnews.com/news/us-news/chicago-battles-violent-crime-rise-heroin-epidemic-n649541"" target=""_blank"" rel=""nofollow"">Chicago Battles Violent Crime Rise and Heroin Epidemic</a></li>
<li>US News: <a href=""http://www.usnews.com/opinion/articles/2016-10-07/chicagos-predictive-policing-program-isnt-a-cure-all-for-violent-crime"" target=""_blank"" rel=""nofollow"">Pitfalls of Predictive Policing</a></li>
<li>The Washington Post: <a href=""https://www.washingtonpost.com/news/post-nation/wp/2016/11/01/chicago-surpasses-600-homicides-in-2016-and-is-on-pace-to-have-its-deadliest-year-in-two-decades/?utm_term=.29d71bfec803"" target=""_blank"" rel=""nofollow"">Chicago surpasses 600 homicides in 2016 and is on pace to have its deadliest year in two decades</a></li>
</ul>
<h1>Footnotes</h1>
<ul>
<li>Location data is only available to the block level to protect the privacy of the victims.</li>
<li>Certain reports may be based on preliminary information and are subject to change.</li>
<li>Data is extracted from the Chicago Police Department's CLEAR (Citizen Law Enforcement Analysis and Reporting) system and made available <a href=""https://data.cityofchicago.org/d/ijzp-q8t2"" target=""_blank"" rel=""nofollow"">here</a>.</li>
</ul>
This dataset was created by Public Safety and contains around 300000 samples along with Ward, Community Area, technical information and other features such as:
- Description
- District
- and more.
How to use this dataset
&gt; - Analyze Arrest in relation to Case Number
- Study the influence of Domestic on Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Public Safety 
Start A New Notebook!"	91	775	10	yamqwe	chicago-crimee
1326	1326	  🚴 Bike Sharing Dataset	This dataset is a dataset of rental bike sharing systems	['cycling', 'regression']	"About this dataset
&gt; <h1>Source:</h1>
<p>Hadi Fanaee-T<br>
Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of PortoINESC Porto, Campus da FEUPRua Dr. Roberto Frias, 3784200 - 465 Porto, Portugal<br>
Original Source:<br>
<a href=""http://capitalbikeshare.com/system-data"" target=""_blank"" rel=""nofollow"">http://capitalbikeshare.com/system-data</a><br>
Weather Information:<br>
<a href=""http://www.freemeteo.com"" target=""_blank"" rel=""nofollow"">http://www.freemeteo.com</a><br>
Holiday Schedule:<br>
<a href=""http://dchr.dc.gov/page/holiday-schedule"" target=""_blank"" rel=""nofollow"">http://dchr.dc.gov/page/holiday-schedule</a></p>
<h1>Data Set Information:</h1>
<p>Bike sharing systems are new generation of traditional bike rentals where whole process from membership, rental and return back has become automatic. Through these systems, user is able to easily rent a bike from a particular position and return back at another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousands bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.<br>
Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.</p>
<h1>Attribute Information:</h1>
<p>Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv<br>
- instant: record index - dteday : date - season : season (1:springer, 2:summer, 3:fall, 4:winter)  - yr : year (0: 2011, 1:2012)   - mnth : month ( 1 to 12)   - hr : hour (0 to 23)   - holiday : weather day is holiday or not (extracted from ) - weekday : day of the week - workingday : if day is neither weekend nor holiday is 1, otherwise is 0.  + weathersit :<br>
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy        - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist       - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds        - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog - temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)  - atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale) - hum: Normalized humidity. The values are divided to 100 (max) - windspeed: Normalized wind speed. The values are divided to 67 (max)  - casual: count of casual users - registered: count of registered users - cnt: count of total rental bikes including both casual and registered</p>
<h1>Relevant Papers:</h1>
<ol>
<li>Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, .</li>
</ol>
<h1>Citation Request:</h1>
<p>Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, .<br>
<a href=""/article"" target=""_blank"" rel=""nofollow"">@article</a>{ year={2013},    issn={2192-6352},   journal={Progress in Artificial Intelligence},  doi={10.1007/s13748-013-0040-3},    title={Event labeling combining ensemble detectors and background knowledge},   url={ },    publisher={Springer Berlin Heidelberg}, keywords={Event labeling; Event detection; Ensemble learning; Background knowledge},    author={Fanaee-T, Hadi and Gama, Joao}, pages={1-15}}</p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a></p>
This dataset was created by UCI and contains around 20000 samples along with Dteday, Windspeed, technical information and other features such as:
- Registered
- Cnt
- and more.
How to use this dataset
&gt; - Analyze Weekday in relation to Casual
- Study the influence of Season on Holiday
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	41	370	5	yamqwe	bike-sharing-datasete
1327	1327	Deaths in 122 U.S. cities	Deaths in 122 U.S. cities by age (years)	['health', 'mortality', 'covid19']	"About this dataset
&gt; <p>TABLE III. Deaths in 122 U.S. cities â 2016. 122 Cities Mortality Reporting System â Each week, the vital statistics offices of 122 cities across the United States report the total number of death certificates processed and the number of those for which pneumonia or influenza was listed as the underlying or contributing cause of death by age group (Under 28 days, 28 days â1 year, 1-14 years, 15-24 years, 25-44 years, 45-64 years, 65-74 years, 75-84 years, and â¥ 85 years).</p>
<p>FOOTNOTE: U: Unavailable. â: No reported cases. * Mortality data in this table are voluntarily reported from 122 cities in the United States, most of which have populations of 100,000 or more. A death is reported by the place of its occurrence and by the week that the death certificate was filed. Fetal deaths are not included.</p>
<p>â  Pneumonia and influenza.</p>
<p>Â§ Total includes unknown ages.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/table-iii-deaths-in-122-u-s-cities"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/table-iii-deaths-in-122-u-s-cities</a></p>
This dataset was created by Health and contains around 5000 samples along with Reporting Area, All Causes, By Age (years), All Ages, technical information and other features such as:
- All Causes, By Age (years), All Ages, Flag
- All Causes, By Age (years), Lt 1, Flag
- and more.
How to use this dataset
&gt; - Analyze All Causes, By Age (years), ≥65, Flag in relation to All Causes, By Age (years), ≥65
- Study the influence of P&i† Total on All Causes, By Age (years), 25–44, Flag
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	106	502	3	yamqwe	deaths-in-122-u-s-citiese
1328	1328	🛟 Breast Cancer	3 medical domains with 9 attributes and one class of another class	['earth and nature', 'health', 'computer science', 'classification', 'cancer']	"About this dataset
&gt; <h1>Source:</h1>
<p>Creators:<br>
Matjaz Zwitter &amp; Milan Soklic (physicians)<br>
Institute of Oncology University Medical Center<br>
Ljubljana, Yugoslavia</p>
<p>Donors:<br>
Ming Tan and Jeff Schlimmer (Jeffrey.Schlimmer '@' <a href=""http://a.gp.cs.cmu.edu"" target=""_blank"" rel=""nofollow"">a.gp.cs.cmu.edu</a>)</p>
<h1>Data Set Information:</h1>
<p>This is one of three domains provided by the Oncology Institute that has repeatedly appeared in the machine learning literature. (See also lymphography and primary-tumor.)<br>
This data set includes 201 instances of one class and 85 instances of another class. The instances are described by 9 attributes, some of which are linear and some are nominal.</p>
<h1>Attribute Information:</h1>
<ol>
<li>Class: no-recurrence-events, recurrence-events</li>
<li>age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.</li>
<li>menopause: lt40, ge40, premeno.</li>
<li>tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59.</li>
<li>inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26, 27-29, 30-32, 33-35, 36-39.</li>
<li>node-caps: yes, no.</li>
<li>deg-malig: 1, 2, 3.</li>
<li>breast: left, right.</li>
<li>breast-quad: left-up, left-low, right-up,   right-low, central.</li>
<li>irradiat:   yes, no.</li>
</ol>
<h1>Relevant Papers:</h1>
<p>Michalski,R.S., Mozetic,I., Hong,J., &amp; Lavrac,N. (1986). The Multi-Purpose Incremental Learning System AQ15 and its Testing Application to Three Medical Domains. In Proceedings of the Fifth National Conference on Artificial Intelligence, 1041-1045, Philadelphia, PA: Morgan Kaufmann.<br>
Clark,P. &amp; Niblett,T. (1987). Induction in Noisy Domains. In Progress in Machine Learning (from the Proceedings of the 2nd European Working Session on Learning), 11-30, Bled, Yugoslavia: Sigma Press.<br>
Tan, M., &amp; Eshelman, L. (1988). Using weighted networks to represent classification knowledge in noisy domains. Proceedings of the Fifth International Conference on Machine Learning, 121-134, Ann Arbor, MI.<br>
Cestnik,G., Konenenko,I, &amp; Bratko,I. (1987). Assistant-86: A Knowledge-Elicitation Tool for Sophisticated Users. In I.Bratko &amp; N.Lavrac (Eds.) Progress in Machine Learning, 31-45, Sigma Press.</p>
<h1>Papers That Cite This Data Set1:</h1>
<ul>
<li>Igor Fischer and Jan Poland. Amplifying the Block Matrix Structure for Spectral Clustering. Telecommunications Lab. 2005.</li>
<li>Saher Esmeir and Shaul Markovitch. Lookahead-based algorithms for anytime induction of decision trees. ICML. 2004.</li>
<li>Gavin Brown. Diversity in Neural Network Ensembles. The University of Birmingham. 2004.</li>
<li>Kaizhu Huang and Haiqin Yang and Irwin King and Michael R. Lyu and Laiwan Chan. Biased Minimax Probability Machine for Medical Diagnosis. AMAI. 2004.</li>
<li>Qingping Tao Ph. D. MAKING EFFICIENT LEARNING ALGORITHMS WITH EXPONENTIALLY MANY FEATURES. Qingping Tao A DISSERTATION Faculty of The Graduate College University of Nebraska In Partial Fulfillment of Requirements. 2004.</li>
<li>Krzysztof Grabczewski and Wl/odzisl/aw Duch. Heterogeneous Forests of Decision Trees. ICANN. 2002.</li>
<li>Hussein A. Abbass. An evolutionary artificial neural networks approach for breast cancer diagnosis. Artificial Intelligence in Medicine, 25. 2002.</li>
<li>Fei Sha and Lawrence K. Saul and Daniel D. Lee. Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines. NIPS. 2002.</li>
<li>Kristin P. Bennett and Ayhan Demiriz and Richard Maclin. Exploiting unlabeled data in ensemble methods. KDD. 2002.</li>
<li>Baback Moghaddam and Gregory Shakhnarovich. Boosted Dyadic Kernel Discriminants. NIPS. 2002.</li>
<li>András Antos and Balázs Kégl and Tamás Linder and Gábor Lugosi. Data-dependent margin-based generalization bounds for classification. Journal of Machine Learning Research, 3. 2002.</li>
<li>Michael G. Madden. Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm. CoRR, csLG/0211003. 2002.</li>
<li>Yongmei Wang and Ian H. Witten. Modeling for Optimal Probability Prediction. ICML. 2002.</li>
<li>Remco R. Bouckaert. Accuracy bounds for ensembles under 0 { 1 loss. Xtal Mountain Information Technology &amp; Computer Science Department, University of Waikato. 2002.</li>
<li>Nikunj C. Oza and Stuart J. Russell. Experimental comparisons of online and batch versions of bagging and boosting. KDD. 2001.</li>
<li>Bernhard Pfahringer and Geoffrey Holmes and Richard Kirkby. Optimizing the Induction of Alternating Decision Trees. PAKDD. 2001.</li>
<li>Robert Burbidge and Matthew Trotter and Bernard F. Buxton and Sean B. Holden. STAR - Sparsity through Automated Rejection. IWANN (1). 2001.</li>
<li>Bernhard Pfahringer and Geoffrey Holmes and Gabi Schmidberger. Wrapping Boosters against Noise. Australian Joint Conference on Artificial Intelligence. 2001.</li>
<li>W. Nick Street and Yoo-Hyon Kim. A streaming ensemble algorithm (SEA) for large-scale classification. KDD. 2001.</li>
<li>Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Improved Generalization Through Explicit Optimization of Margins. Machine Learning, 38. 2000.</li>
<li>Endre Boros and Peter Hammer and Toshihide Ibaraki and Alexander Kogan and Eddy Mayoraz and Ilya B. Muchnik. An Implementation of Logical Analysis of Data. IEEE Trans. Knowl. Data Eng, 12. 2000.</li>
<li>P. S and Bradley K. P and Bennett A. Demiriz. Constrained K-Means Clustering. Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys. 2000.</li>
<li>Sally A. Goldman and Yan Zhou. Enhancing Supervised Learning with Unlabeled Data. ICML. 2000.</li>
<li>Justin Bradley and Kristin P. Bennett and Bennett A. Demiriz. Constrained K-Means Clustering. Microsoft Research Dept. of Mathematical Sciences One Microsoft Way Dept. of Decision Sciences and Eng. Sys. 2000.</li>
<li>Yuh-Jeng Lee. Smooth Support Vector Machines. Preliminary Thesis Proposal Computer Sciences Department University of Wisconsin. 2000.</li>
<li>Petri Kontkanen and Petri Myllym and Tomi Silander and Henry Tirri and Peter Gr. On predictive distributions and Bayesian networks. Department of Computer Science, Stanford University. 2000.</li>
<li>Kristin P. Bennett and Ayhan Demiriz and John Shawe-Taylor. A Column Generation Algorithm For Boosting. ICML. 2000.</li>
<li>Matthew Mullin and Rahul Sukthankar. Complete Cross-Validation for Nearest Neighbor Classifiers. ICML. 2000.</li>
<li>David W. Opitz and Richard Maclin. Popular Ensemble Methods: An Empirical Study. J. Artif. Intell. Res. (JAIR, 11. 1999.</li>
<li>Chun-Nan Hsu and Hilmar Schuschel and Ya-Ting Yang. The ANNIGMA-Wrapper Approach to Neural Nets Feature Selection for Knowledge Discovery and Data Mining. Institute of Information Science. 1999.</li>
<li>David M J Tax and Robert P W Duin. Support vector domain description. Pattern Recognition Letters, 20. 1999.</li>
<li>Kai Ming Ting and Ian H. Witten. Issues in Stacked Generalization. J. Artif. Intell. Res. (JAIR, 10. 1999.</li>
<li>Ismail Taha and Joydeep Ghosh. Symbolic Interpretation of Artificial Neural Networks. IEEE Trans. Knowl. Data Eng, 11. 1999.</li>
<li>Lorne Mason and Jonathan Baxter and Peter L. Bartlett and Marcus Frean. Boosting Algorithms as Gradient Descent. NIPS. 1999.</li>
<li>Iñaki Inza and Pedro Larrañaga and Basilio Sierra and Ramon Etxeberria and Jose Antonio Lozano and Jos Manuel Peña. Representing the behaviour of supervised classification learning algorithms by Bayesian networks. Pattern Recognition Letters, 20. 1999.</li>
<li>Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Direct Optimization of Margins Improves Generalization in Combined Classifiers. NIPS. 1998.</li>
<li>Richard Maclin. Boosting Classifiers Regionally. AAAI/IAAI. 1998.</li>
<li>Huan Liu and Hiroshi Motoda and Manoranjan Dash. A Monotonic Measure for Optimal Feature Selection. ECML. 1998.</li>
<li>Yk Huhtala and Juha Kärkkäinen and Pasi Porkka and Hannu Toivonen. Efficient Discovery of Functional and Approximate Dependencies Using Partitions. ICDE. 1998.</li>
<li>W. Nick Street. A Neural Network Model for Prognostic Prediction. ICML. 1998.</li>
<li>Kristin P. Bennett and Erin J. Bredensteiner. A Parametric Optimization Method for Machine Learning. INFORMS Journal on Computing, 9. 1997.</li>
<li>Pedro Domingos. Control-Sensitive Feature Selection for Lazy Learners. Artif. Intell. Rev, 11. 1997.</li>
<li>Rudy Setiono and Huan Liu. NeuroLinear: From neural networks to oblique decision rules. Neurocomputing, 17. 1997.</li>
<li>. Prototype Selection for Composite Nearest Neighbor Classifiers. Department of Computer Science University of Massachusetts. 1997.</li>
<li>Erin J. Bredensteiner and Kristin P. Bennett. Feature Minimization within Decision Trees. National Science Foundation. 1996.</li>
<li>Ismail Taha and Joydeep Ghosh. Characterization of the Wisconsin Breast cancer Database Using a Hybrid Symbolic-Connectionist System. Proceedings of ANNIE. 1996.</li>
<li>Kamal Ali and Michael J. Pazzani. Error Reduction through Learning Multiple Descriptions. Machine Learning, 24. 1996.</li>
<li>Jennifer A. Blue and Kristin P. Bennett. Hybrid Extreme Point Tabu Search. Department of Mathematical Sciences Rensselaer Polytechnic Institute. 1996.</li>
<li>Pedro Domingos. Unifying Instance-Based and Rule-Based Induction. Machine Learning, 24. 1996.</li>
<li>Geoffrey I. Webb. OPUS: An Efficient Admissible Algorithm for Unordered Search. J. Artif. Intell. Res. (JAIR, 3. 1995.</li>
<li>Christophe Giraud and Tony Martinez and Christophe G. Giraud-Carrier. University of Bristol Department of Computer Science ILA: Combining Inductive Learning with Prior Knowledge and Reasoning. 1995.</li>
<li>Ron Kohavi. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. IJCAI. 1995.</li>
<li>M. A. Galway and Michael G. Madden. DEPARTMENT OF INFORMATION TECHNOLOGY technical report NUIG-IT-011002 Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm. Department of Information Technology National University of Ireland, Galway.</li>
<li>John G. Cleary and Leonard E. Trigg. Experiences with OB1, An Optimal Bayes Decision Tree Learner. Department of Computer Science University of Waikato.</li>
<li>Wl/odzisl/aw Duch and Rafal/ Adamczak Email:duchraad@phys. uni. torun. pl. Statistical methods for construction of neural networks. Department of Computer Methods, Nicholas Copernicus University.</li>
<li>Rong-En Fan and P. -H Chen and C. -J Lin. Working Set Selection Using the Second Order Information for Training SVM. Department of Computer Science and Information Engineering National Taiwan University.</li>
<li>Rong Jin and Yan Liu and Luo Si and Jaime Carbonell and Alexander G. Hauptmann. A New Boosting Algorithm Using Input-Dependent Regularizer. School of Computer Science, Carnegie Mellon University.</li>
<li>David Kwartowitz and Sean Brophy and Horace Mann. Session S2D Work In Progress: Establishing multiple contexts for student's progressive refinement of data mining.</li>
<li>Geoffrey I Webb. Generality is more significant than complexity: Toward an alternative to Occam's Razor. School of Computing and Mathematics Deakin University.</li>
<li>Karthik Ramakrishnan. UNIVERSITY OF MINNESOTA.</li>
<li>Geoffrey I Webb. Learning Decision Lists by Prepending Inferred Rules. School of Computing and Mathematics Deakin University.</li>
<li>Adil M. Bagirov and Alex Rubinov and A. N. Soukhojak and John Yearwood. Unsupervised and supervised data classification via nonsmooth and global optimization. School of Information Technology and Mathematical Sciences, The University of Ballarat.</li>
<li>M. V. Fidelis and Heitor S. Lopes and Alex Alves Freitas. Discovering Comprehensible Classification Rules with a Genetic Algorithm. UEPG, CPD CEFET-PR, CPGEI PUC-PR, PPGIA Praa Santos Andrade, s/n Av. Sete de Setembro.</li>
<li>Chris Drummond and Robert C. Holte. C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling. Institute for Information Technology, National Research Council Canada.</li>
<li>Wl odzisl/aw Duch and Rudy Setiono and Jacek M. Zurada. Computational intelligence methods for rule-based data understanding.</li>
<li>Maria Salamo and Elisabet Golobardes. Analysing Rough Sets weighting methods for Case-Based Reasoning Systems. Enginyeria i Arquitectura La Salle.</li>
<li>G. Ratsch and B. Scholkopf and Alex Smola and K. -R Muller and T. Onoda and Sebastian Mika. Arc: Ensemble Learning in the Presence of Outliers. GMD FIRST.</li>
<li>D. Randall Wilson and Roel Martinez. Improved Center Point Selection for Probabilistic Neural Networks. Proceedings of the International Conference on Artificial Neural Networks and Genetic Algorithms.</li>
<li>Chiranjib Bhattacharyya. Robust Classification of noisy data using Second Order Cone Programming approach. Dept. Computer Science and Automation, Indian Institute of Science.</li>
<li>K. A. J Doherty and Rolf Adams and Neil Davey. Unsupervised Learning with Normalised Data and Non-Euclidean Norms. University of Hertfordshire.</li>
<li>Adam H. Cannon and Lenore J. Cowen and Carey E. Priebe. Approximate Distance Classification. Department of Mathematical Sciences The Johns Hopkins University.</li>
<li>G. Ratsch and B. Scholkopf and Alex Smola and Sebastian Mika and T. Onoda and K. -R Muller. Robust Ensemble Learning for Data Mining. GMD FIRST, Kekul#estr.</li>
<li>Andrew I. Schein and Lyle H. Ungar. A-Optimality for Active Learning of Logistic Regression Classifiers. Department of Computer and Information Science Levine Hall.</li>
<li>Huan Liu. A Family of Efficient Rule Generators. Department of Information Systems and Computer Science National University of Singapore.</li>
<li>Alexander K. Seewald. Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgefuhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften.</li>
<li>Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas. PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery. CEFET-PR, Curitiba.</li>
<li>Paul D. Wilson and Tony R. Martinez. Combining Cross-Validation and Confidence to Measure Fitness. fonix corporation Brigham Young University.</li>
<li>Charles Campbell and Nello Cristianini. Simple Learning Algorithms for Training Support Vector Machines. Dept. of Engineering Mathematics.</li>
<li>Nikunj C. Oza and Stuart J. Russell. Online Bagging and Boosting. Computer Science Division University of California.</li>
<li>Michael R. Berthold and Klaus*Peter Huber. From Radial to Rectangular Basis Functions: A new Approach for Rule Learning from Large Datasets. Institut fur Rechnerentwurf und Fehlertoleranz (Prof. D. Schmid) Universitat Karlsruhe.</li>
<li>Bart Baesens and Stijn Viaene and Tony Van Gestel and J. A. K Suykens and Guido Dedene and Bart De Moor and Jan Vanthienen and Katholieke Universiteit Leuven. An Empirical Assessment of Kernel Type Performance for Least Squares Support Vector Machine Classifiers. Dept. Applied Economic Sciences.</li>
<li>Rudy Setiono and Huan Liu. Neural-Network Feature Selector. Department of Information Systems and Computer Science National University of Singapore.</li>
<li>Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas. An Ant Colony Based System for Data Mining: Applications to Medical Data. CEFET-PR, CPGEI Av. Sete de Setembro, 3165.</li>
<li>Wl odzisl and Rafal Adamczak and Krzysztof Grabczewski and Grzegorz Zal. A hybrid method for extraction of logical rules from data. Department of Computer Methods, Nicholas Copernicus University.</li>
<li>Rudy Setiono. Extracting M-of-N Rules from Trained Neural Networks. School of Computing National University of Singapore.</li>
<li>Jarkko Salojarvi and Samuel Kaski and Janne Sinkkonen. Discriminative clustering in Fisher metrics. Neural Networks Research Centre Helsinki University of Technology.</li>
<li>Ayhan Demiriz and Kristin P. Bennett and John Shawe and I. Nouretdinov V.. Linear Programming Boosting via Column Generation. Dept. of Decision Sciences and Eng. Systems, Rensselaer Polytechnic Institute.</li>
<li>Liping Wei and Russ B. Altman. An Automated System for Generating Comparative Disease Profiles and Making Diagnoses. Section on Medical Informatics Stanford University School of Medicine, MSOB X215.</li>
<li>Chotirat Ann and Dimitrios Gunopulos. Scaling up the Naive Bayesian Classifier: Using Decision Trees for Feature Selection. Computer Science Department University of California.</li>
<li>Sherrie L. W and Zijian Zheng. A BENCHMARK FOR CLASSIFIER LEARNING. Basser Department of Computer Science The University of Sydney.</li>
<li>John W. Chinneck. Fast Heuristics for the Maximum Feasible Subsystem Problem. Systems and Computer Engineering, Carleton University.</li>
</ul>
<h1>Citation Request:</h1>
<p>This breast cancer domain was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data. Please include this citation if you plan to use this database.[1] Papers were automatically harvested and associated with this data set, in collaborationwith <a href=""http://rexa.info/"" target=""_blank"" rel=""nofollow"">Rexa.info</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Breast+Cancer"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Breast+Cancer</a></p>
This dataset was created by UCI and contains around 0 samples along with Please Send A, Please Send A, technical information and other features such as:
- Please Send A
- Please Send A
- and more.
How to use this dataset
&gt; - Analyze Please Send A in relation to Please Send A
- Study the influence of Please Send A on Please Send A
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	84	942	7	yamqwe	breast-cancere
1329	1329	World Development Indicators	World Bank collection of development indicators	['finance']	"About this dataset
&gt; <p>The primary World Bank collection of development indicators, compiled from officially-recognized international sources. It presents the most current and accurate global development data available, and includes national, regional and global estimates. This static dataset covers 1960 - 2016.</p>
<p><strong>Type:</strong> Time series</p>
<p><strong>Economy Coverage:</strong> WLD, EAP, ECA, LAC, MNA, SAS, SSA, HIC, LMY, IBRD, IDA</p>
<p><strong>Granularity:</strong> National, Regional</p>
<p><strong>Number of Economies:</strong> 217</p>
<p><strong>Attribution:</strong> <a href=""http://data.worldbank.org/data-catalog/world-development-indicators"">World Development Indicators, The World Bank</a>. Updated quarterly at The World Bank.</p>
<p><a href=""http://web.worldbank.org/WBSITE/EXTERNAL/0,,contentMDK:22547097~pagePK:50016803~piPK:50016805~theSitePK:13,00.html"" target=""_blank"" rel=""nofollow"">World Bank Data Catalog Terms of Use</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://data.worldbank.org/data-catalog/world-development-indicators"">http://data.worldbank.org/data-catalog/world-development-indicators</a></p>
This dataset was created by World Bank and contains around 400000 samples along with 2001, 2010, technical information and other features such as:
- 1993
- 1982
- and more.
How to use this dataset
&gt; - Analyze 1962 in relation to Country Name
- Study the influence of 2006 on Indicator Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit World Bank 
Start A New Notebook!"	21	250	3	yamqwe	world-development-indicatorse
1330	1330	International Educational Attainment by Year & Age	Datasets containing information about education in the United States.	['universities and colleges', 'earth and nature', 'education']	"About this dataset
&gt; <p>The National Center for Education Statistics (NCES) is the primary federal entity for collecting and analyzing data related to education in the U.S. and other nations. NCES is located within the U.S. Department of Education and the Institute of Education Sciences. NCES fulfills a Congressional mandate to collect, collate, analyze, and report complete statistics on the condition of American education; conduct and publish reports; and review and report on education activities internationally.</p>
<ul>
<li>Table 603.10. Percentage of the population 25 to 64 years old who completed high school, by age group and country: Selected years, 2001 through 2012</li>
<li>Table 603.20. Percentage of the population 25 to 64 years old who attained selected levels of postsecondary education, by age group and country: 2001 and 2012</li>
<li>Table 603.30. Percentage of the population 25 to 64 years old who attained a bachelor's or higher degree, by age group and country: Selected years, 1999 through 2012</li>
<li>Table 603.40 Percentage of the population 25 to 64 years old who attained a postsecondary vocational degree, by age group and country: Selected years, 1999 through 2012</li>
<li>Table 603.50 Number of bachelor's degree recipients per 100 persons at the typical minimum age of graduation, by sex and country: Selected years, 2005 through 2012</li>
<li>Table 603.60. Percentage of postsecondary degrees awarded to women, by field of study and country: 2013</li>
<li>Table 603.70. Percentage of bachelor's or equivalent degrees awarded in mathematics, science, and engineering, by field of study and country: 2013</li>
<li>Table 603.80. Percentage of master's or equivalent degrees and of doctoral or equivalent degrees awarded in mathematics, science, and engineering, by field of study and country: 2013</li>
<li>Table 603.90. Employment to population ratios of -25 to 64-year-olds, by sex, highest level of educational attainment, and country: 2014</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://nces.ed.gov/programs/digest/current_tables.asp"" target=""_blank"" rel=""nofollow"">https://nces.ed.gov/programs/digest/current_tables.asp</a></p>
This dataset was created by National Center for Education Statistics and contains around 100 samples along with Unnamed: 20, Unnamed: 24, technical information and other features such as:
- Unnamed: 11
- Unnamed: 16
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 15 in relation to Unnamed: 6
- Study the influence of Unnamed: 1 on Unnamed: 10
- More datasets
Acknowledgements
If you use this dataset in your research, please credit National Center for Education Statistics 
Start A New Notebook!"	47	310	3	yamqwe	international-comp-attainmente
1331	1331	🍷 Wine, Beer, and Liquor Reviews	Over 2,000 reviews of beer, liquor, and wine sold online.	['alcohol', 'business', 'food']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of over 2,000 reviews for beer, liquor, and wine sold online provided by <a href=""https://datafiniti.co/products/product-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Product Database</a>.  The dataset includes address, city, state, business name, business categories, menu data, phone numbers, and more.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do With This Data</h1>
<p>You can use this data to discover insights into how consumers review alcoholic beverages.  E.g.:</p>
<ul>
<li>Which brands have the best reviews?</li>
<li>Does white wine or red wine get better reviews?</li>
<li>What words are most commonly associated with each beverage type?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/product-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 1000 samples along with Reviews.rating, Descriptions, technical information and other features such as:
- Manufacturer Number
- Reviews.username
- and more.
How to use this dataset
&gt; - Analyze Reviews.date in relation to Id
- Study the influence of Source Ur Ls on Primary Categories
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	48	466	3	yamqwe	wine-beer-and-liquor-reviewse
1332	1332	🩰 Women's Shoe Prices	A list of 10,000 women's shoes and their product information	['clothing and accessories', 'business', 'marketing', 'retail and shopping']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of 10,000 women's shoes and their product information provided by <a href=""https://datafiniti.co/products/product-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Product Database</a>.</p>
<p>The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with This Data</h1>
<p>You can use this data to <a href=""https://datafiniti.co/cost-of-designer-label/"" target=""_blank"" rel=""nofollow"">determine brand markups, pricing strategies, and trends for luxury shoes</a>. E.g.:</p>
<ul>
<li>What is the average price of each distinct brand listed?</li>
<li>Which brands have the highest prices?</li>
<li>Which ones have the widest distribution of prices?</li>
<li>Is there a typical price distribution (e.g., normal) across brands or within specific brands?</li>
</ul>
<p>Further processing data would also let you:</p>
<ul>
<li>Correlate specific product features with changes in price.</li>
<li>You can cross-reference this data with a sample of our <a href=""https://data.world/datafiniti/mens-shoe-prices"">Men's Shoe Prices</a> to see if there are any differences between women's brands and men's brands.</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/product-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>You can access the full dataset by running the following query with <a href=""https://developer.datafiniti.co/docs/getting-started-with-product-data"" target=""_blank"" rel=""nofollow"">Datafiniti’s Product API</a>.</p>
<p><code>{ ""query"": ""dateUpdated:[2018-01-01 TO *] AND brand:* AND categories:(Women's Shoes OR All Women's Shoes OR Womens Shoes)* AND primaryCategories:(Shoes)* AND name:* AND prices:* AND sizes:* AND sourceURLs:*"", ""format"": ""csv"", ""download"": true, ""view"": ""datasets_womens_shoes"" }</code></p>
<p>*<em>This query generated 129,302 records as of December 10, 2018. The total number of results may vary.</em></p>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 20000 samples along with Colors, Prices.amount Max, technical information and other features such as:
- Prices.availability
- Manufacturer
- and more.
How to use this dataset
&gt; - Analyze Prices.count in relation to Features
- Study the influence of Descriptions on Id
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	107	737	5	yamqwe	women-s-shoe-pricese
1333	1333	Electronic Products Reviews from Amazon, Bestbuy	7K online reviews of 50 electronic products by store and time 	[]	"About this dataset
<p>This is a list of over 7,000 online reviews for 50 electronic products from websites like <a href=""https://www.amazon.com/"" target=""_blank"">Amazon</a> and <a href=""https://www.bestbuy.com/"" target=""_blank"">Best Buy</a> provided by <a href=""https://datafiniti.co/products/product-data/"" target=""_blank"">Datafiniti's Product Database</a>. The dataset includes the review date, source, rating, title, reviewer metadata, and more.</p>

<h1>What You Can Do With This Data</h1>
<p>You can use this data to identify how <a href=""https://datafiniti.co/use-case/major-electronics-company/"" target=""_blank"">consumer feedback impacts the product buying process</a>. E.g.:</p>
<ul>
<li>What are the different trends for electronic products?</li>
<li>What is the correlation between star ratings and positive reviews?</li>
<li>What is the online reputation of different brands?</li>
</ul>

<h1>Data Schema</h1>
<p>A full schema for the data is available in datafinity's <a href=""https://datafiniti-api.readme.io/docs/product-data-schema"" target=""_blank"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. <a href=""https://datafiniti.co/"" target=""_blank"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"">requesting a demo</a>.</p>
This dataset was created by Nicholle and contains around 7000 samples along with Reviews.source Ur Ls, Reviews.title, technical information and other features such as:
- Colors
- Reviews.text
- and more.
How to use this dataset
&gt; - Analyze Image Ur Ls in relation to Reviews.num Helpful
- Study the influence of Upc on Date Added
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Nicholle 
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>

Start A New Notebook!"	18	197	3	yamqwe	amazon-and-best-buy-electronicse
1334	1334	Hate Crime Statistics	Datasets for single-bias and multiple-bias hate crimes	['crime', 'public safety']	"About this dataset
&gt; <p>The Uniform Crime Reporting Program collects data about both single-bias and multiple-bias hate crimes. For each offense type reported, law enforcement must indicate at least one bias motivation. A single-bias incident is defined as an incident in which one or more offense types are motivated by the same bias. As of 2013, a multiple-bias incident is defined as an incident in which one or more offense types are motivated by two or more biases.  <strong>Overview</strong><br>
In 2014, 15,494 law enforcement agencies participated in the Hate Crime Statistics Program. Of these agencies, 1,666 reported 5,479 hate crime incidents involving 6,418 offenses.<br>
There were 5,462 single-bias incidents that involved 6,385 offenses, 6,681 victims, and 5,176 known offenders.<br>
The 17 multiple-bias incidents reported in 2014 involved 33 offenses, 46 victims, and 16 offenders. (See Tables 1 and 12.) <strong>Source:</strong> <a href=""https://ucr.fbi.gov/hate-crime/2014"" target=""_blank"">FBI Hate Crime Statistics</a> and more <a href=""https://ucr.fbi.gov/hate-crime/2014/resource-pages/about-hate-crime.pdf"" target=""_blank"">about the Hate Crime Statistics</a></p>
<p>Source: <a href=""https://ucr.fbi.gov/about-us/cjis/ucr/hate-crime/2014/resource-pages/download-files"" target=""_blank"">https://ucr.fbi.gov/about-us/cjis/ucr/hate-crime/2014/resource-pages/download-files</a></p>
This dataset was created by Uniform Crime Reports and contains around 0 samples along with Unnamed: 13, Unnamed: 3, technical information and other features such as:
- Unnamed: 12
- Unnamed: 5
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 14 in relation to Unnamed: 9
- Study the influence of Unnamed: 15 on Unnamed: 4
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Uniform Crime Reports 
Start A New Notebook!"	37	395	1	yamqwe	2014-hate-crime-statisticse
1335	1335	Hospital Inpatient Mortality Rates	California Hospital Inpatient Mortality Rates and Quality Ratings	['healthcare', 'health', 'heart conditions', 'hospitals and treatment centers']	"About this dataset
&gt; <p><em>Original Title</em>: California Hospital Inpatient Mortality Rates and Quality Ratings</p>
<p>The dataset contains risk-adjusted mortality rates, quality ratings, and number of deaths and cases for 6 medical conditions treated (Acute Stroke, Acute Myocardial Infarction, Heart Failure, Gastrointestinal Hemorrhage, Hip Fracture and Pneumonia) and 6 procedures performed (Abdominal Aortic Aneurysm Repair, Carotid Endarterectomy, Craniotomy, Esophageal Resection, Pancreatic Resection, Percutaneous Coronary Intervention) in California hospitals. The 2014 and 2015 IMIs were generated using AHRQ Version 5.0, while the 2012 and 2013 IMIs were generated using AHRQ Version 4.5. The differences in the statistical method employed and inclusion and exclusion criteria using different versions can lead to different results. Users should not compare trends of mortality rates over time. However, many hospitals showed consistent performance over years; “better” performing hospitals may perform better and “worse” performing hospitals may perform worse consistently across years. This dataset does not include conditions treated or procedures performed in outpatient settings. Please refer to statewide table for California overall rates. <a href=""https://chhs.data.ca.gov/Healthcare/California-Statewide-Inpatient-Mortality-Rates-201/nmyg-79m3"" target=""_blank"" rel=""nofollow"">https://chhs.data.ca.gov/Healthcare/California-Statewide-Inpatient-Mortality-Rates-201/nmyg-79m3</a></p>
<p>Source: <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2019-08-03<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 300 samples along with Unnamed: 21, Unnamed: 4, technical information and other features such as:
- Unnamed: 38
- Unnamed: 36
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 50 in relation to Unnamed: 35
- Study the influence of Unnamed: 2 on California Hospital Risk Adjusted Mortality Rates (ramr), Total Deaths And Cases, And Quality Ratings For Inpatient Mortality Indicators, 2010
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	155	1397	11	yamqwe	california-hospital-inpatient-mortality-rates-ane
1336	1336	2022 Winter Olympics Beijing	Data about Athletes, Teams, Coaches, Events	['china ', 'sports', 'beginner', 'exploratory data analysis', 'tabular data']	"Details
This contains the details of over 2,800 athletes, with 15 disciplines, along with 91 Nations taking part in the 2022 Winter Beijing Olympics.
This dataset contains the details of the Athletes, Coaches, Technical Coaches, Teams participating as well as the Entries by gender. It contains their names, countries represented, discipline, gender of competitors, name of the coaches.
Will update the medals every 2-3 days till the end date, as well as upload the medalists' details & multi-medalist details.
Credits
Source: Beijing 2022 Winter Olympics Website"	149	1252	18	arjunprasadsarkhel	2022-winter-olympics-beijing
1337	1337	🗳 Partisan Lean	50 collected samples, technical information	['business', 'beginner']	"About this dataset
&gt; <p>This directory contains the data for FiveThirtyEight's partisan lean, which is used in our <a href=""https://projects.fivethirtyeight.com/2018-midterm-election-forecast/house"" target=""_blank"" rel=""nofollow"">House</a>, <a href=""https://projects.fivethirtyeight.com/2018-midterm-election-forecast/senate"" target=""_blank"" rel=""nofollow"">Senate</a> and <a href=""https://projects.fivethirtyeight.com/2018-midterm-election-forecast/governor/"" target=""_blank"" rel=""nofollow"">Governor</a> forecasts.</p>
<p>Partisan lean is the average difference between how a state or district votes and how the country votes overall, with 2016 presidential election results weighted 50 percent, 2012 presidential election results weighted 25 percent and results from elections for the state legislature weighted 25 percent.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by data.world's Admin and contains around 0 samples along with Pvi 538, Pvi 538, technical information and other features such as:
- Pvi 538
- Pvi 538
- and more.
How to use this dataset
&gt; - Analyze Pvi 538 in relation to Pvi 538
- Study the influence of Pvi 538 on Pvi 538
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	7	63	1	yamqwe	partisan-leane
1338	1338	How 'Qi' And 'Za' Changed Scrabble	1500000 collected samples, technical information	['games', 'beginner']	"About this dataset
&gt; <h3>Scrabble tournament games data</h3>
<p>The raw data behind the story <a href=""https://fivethirtyeight.com/features/how-qi-and-za-changed-scrabble/"" target=""_blank"" rel=""nofollow"">How 'Qi' And 'Za' Changed Scrabble</a></p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>gameid</code></td>
<td>A numerical game ID</td>
</tr>
<tr>
<td><code>tourneyid</code></td>
<td>A numerical tournament ID</td>
</tr>
<tr>
<td><code>tie</code></td>
<td>A binary variable indicating if the game ended in a tie</td>
</tr>
<tr>
<td><code>winnerid</code></td>
<td>A numerical ID for the winning player</td>
</tr>
<tr>
<td><code>winnername</code></td>
<td>The name of the winning player</td>
</tr>
<tr>
<td><code>winnerscore</code></td>
<td>The score of the winning player</td>
</tr>
<tr>
<td><code>winneroldrating</code></td>
<td>The winner‰Ûªs rating before the game</td>
</tr>
<tr>
<td><code>winnernewrating</code></td>
<td>The winner‰Ûªs rating after the game</td>
</tr>
<tr>
<td><code>winnerpos</code></td>
<td>The winner‰Ûªs position in the tournament</td>
</tr>
<tr>
<td><code>loserid</code></td>
<td>A numerical ID for the losing player</td>
</tr>
<tr>
<td><code>loserscore</code></td>
<td>The score of the losing player</td>
</tr>
<tr>
<td><code>loseroldrating</code></td>
<td>The loser‰Ûªs rating before the game</td>
</tr>
<tr>
<td><code>losernewrating</code></td>
<td>The loser‰Ûªs rating after the game</td>
</tr>
<tr>
<td><code>loserpos</code></td>
<td>The loser‰Ûªs position in the tournament</td>
</tr>
<tr>
<td><code>round</code></td>
<td>The round of the tournament in which the game took place</td>
</tr>
<tr>
<td><code>division</code></td>
<td>The division of the tournament in which the game took place</td>
</tr>
<tr>
<td><code>date</code></td>
<td>The date of the game</td>
</tr>
<tr>
<td><code>lexicon</code></td>
<td>A binary variable indicating if the game‰Ûªs lexicon was the main North American lexicon (<code>False</code>) or the international lexicon (<code>True</code>)</td>
</tr>
</tbody>
</table></div>
<p>Source: <a href=""http://cross-tables.com"" target=""_blank"" rel=""nofollow"">cross-tables.com</a></p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data/tree/master/scrabble-games"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/scrabble-games</a></p>
This dataset was created by FiveThirtyEight and contains around 1500000 samples along with Round, Winnerpos, technical information and other features such as:
- Loserpos
- Winnerid
- and more.
How to use this dataset
&gt; - Analyze Lexicon in relation to Winnernewrating
- Study the influence of Losername on Winnername
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	10	102	1	yamqwe	scrabble-gamese
1339	1339	 The Complete History Of The NBA 	70000 collected samples, technical information	['games', 'basketball', 'people', 'people and society', 'beginner']	"About this dataset
&gt; <p>Data behind <a href=""https://projects.fivethirtyeight.com/complete-history-of-the-nba/"" target=""_blank"" rel=""nofollow"">The Complete History Of The NBA</a> and our <a href=""https://projects.fivethirtyeight.com/2018-nba-predictions/"" target=""_blank"" rel=""nofollow"">NBA Predictions</a>.</p>
<p><code>nba_elo.csv</code> contains game-by-game Elo ratings and forecasts back to 1946.</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 70000 samples along with Playoff, Score2, technical information and other features such as:
- Carmelo Prob2
- Elo2 Pre
- and more.
How to use this dataset
&gt; - Analyze Elo1 Post in relation to Elo Prob1
- Study the influence of Date on Elo2 Post
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	129	815	5	yamqwe	nba-carmeloe
1340	1340	🧏 Language: Certainty of Events	10000 collected samples, technical information	['business', 'linguistics', 'nlp']	"About this dataset
&gt; <p>A linguistic data set concerning the certainty an author has about a specific word. For example, in the following sentence: ""The dog ran out the door,"" if the word ""ran"" was asked about, the certainty that the event did or will happen would be high.   Added: February 6, 2015 by CrowdFlower | Data Rows: 13386 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 10000 samples along with Golden, Event Question Gold, technical information and other features such as:
- Brstart
- Document
- and more.
How to use this dataset
&gt; - Analyze Bgend in relation to Event
- Study the influence of Bgstart on Trusted Judgments
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	6	130	1	yamqwe	language-certainty-of-eventse
1341	1341	🗳 Women with 25 Years or More House Service	16 collected samples, technical information	['government']	"About this dataset
&gt; <p>Last Updated: January 3, 2019<br>
Years are determined by using 365.2425 days per year, the average length of a year on the Gregorian calendar.</p>
<p><strong><em>Source:</em></strong> <a href=""https://history.house.gov/Exhibitions-and-Publications/WIC/Historical-Data/Women-Who-Have-Served-More-Than-25-Years/"" target=""_blank"" rel=""nofollow"">https://history.house.gov/Exhibitions-and-Publications/WIC/Historical-Data/Women-Who-Have-Served-More-Than-25-Years/</a></p>
This dataset was created by data.world's Admin and contains around 0 samples along with Party, Start Date, technical information and other features such as:
- Years In Office
- State
- and more.
How to use this dataset
&gt; - Analyze Days In Office in relation to Party
- Study the influence of Start Date on Years In Office
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	12	122	1	yamqwe	women-with-25-years-or-more-house-servicee
1342	1342	🧫 GLOBE Project	100 collected samples, technical information	['beginner']	"About this dataset
&gt; <p>The GLOBE study is similar to Geert Hofstede's cultural dimensions, but build's upon the work that he did.</p>
<p><em>""The GLOBE instruments are intended primarily for scholarly, nonprofit, noncommercial purposes.The GLOBE instruments can also be made available to individuals or organizations for commercial (i.e., nonacademic) purposes. In such instances, a representative of the GLOBE Foundation will negotiate with the individual or organization requesting use of the GLOBE instruments with respect to terms of conditions for the use of the instruments.""</em></p>
<p>The GLOBE study also uses different categories of traits, such as:</p>
<p><strong>Charismatic/Value-Based Leadership</strong>: Reflects the ability to inspire, motivate, and expect high performance outcomes from others based on firmly held core values. It includes the following six primary leadership dimensions: (a) visionary, (b) inspirational, (c) self-sacrifice, (d) integrity, (e) decisive and (f) performance oriented.</p>
<p><strong>Team-Oriented Leadership</strong>: Emphasizes effective team building and implementation of a common purpose or goal among team members. It includes the following five primary leadership dimensions: (a) collaborative team orientation, (b) team integrator, (c) diplomatic, (d) malevolent (reverse scored), and (e) administratively competent.</p>
<p><strong>Participative Leadership</strong>: Reflects the degree to which managers involve others in making and implementing decisions. It includes two primary leadership dimensions labeled (a) nonparticipative and (b) autocratic (both reverse scored).</p>
<p><strong>Humane-Oriented Leadership</strong>: Reflects supportive and considerate leadership and includes compassion and generosity. This leadership dimension includes two primary leadership dimensions labeled (a) modesty and (b) humane orientation.</p>
<p><strong>Autonomous Leadership</strong>: Refers to independent and individualistic leadership attributes. It is measured by a single primary leadership dimension labeled autonomous leadership, consisting of individualistic, independence, autonomous, and unique attributes.</p>
<p><strong>Self-Protective Leadership</strong>: Focuses on ensuring the safety and security of the individual and group through status enhancement and face saving. It includes five primary leadership dimensions labeled (a) self-centered, (b) status conscious, (c) conflict inducer, (d) face saver, and (e) procedural.</p>
<p><em>Source: <a href=""http://globeproject.com/study_2004_2007#data"" target=""_blank"" rel=""nofollow"">http://globeproject.com/study_2004_2007#data</a></em><br>
<em>Data Source: <a href=""http://globeproject.com/study_2004_2007#data"" target=""_blank"" rel=""nofollow"">http://globeproject.com/study_2004_2007#data</a></em></p>
This dataset was created by Adam Helsinger and contains around 100 samples along with Country Cluster, Country Cluster, technical information and other features such as:
- Country Cluster
- Country Cluster
- and more.
How to use this dataset
&gt; - Analyze Country Cluster in relation to Country Cluster
- Study the influence of Country Cluster on Country Cluster
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	19	226	1	yamqwe	globe-projecte
1343	1343	IBRD Statement Cash Flows 2011	39 collected samples, technical information	['business', 'finance', 'banking', 'economics', 'investing']	"About this dataset
&gt; <p>Provides data from the IBRD Statement of Cash Flows for the fiscal years ended June 30, 2011, June 30, 2010 and June 30, 2009. Sum of all cash flows represent the net changes in unrestricted cash. The values are expressed in millions of U.S. Dollars. Where applicable, changes have been made to certain line items on FY 2010 statement of cash flows to conform with the current year's presentation, but the comparable prior years' data sets have not been adjusted to reflect the reclassification impact of those changes.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/zyqx-8e4a"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/zyqx-8e4a</a></p>
This dataset was created by Finance and contains around 0 samples along with Source, Amount (us$, Millions), technical information and other features such as:
- Fiscal Year
- Line Item
- and more.
How to use this dataset
&gt; - Analyze Source in relation to Amount (us$, Millions)
- Study the influence of Fiscal Year on Line Item
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	2	65	1	yamqwe	ibrd-statement-cash-flows-2011e
1344	1344	💊 A bird’s-eye view of clinical trials	10000 collected samples, technical information	['business', 'health']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/OVjEAs3JTKmMA2gK9s7F_Screenshot%202019-08-10%20at%2010.47.09%20am.png"" alt=""https://media.data.world/OVjEAs3JTKmMA2gK9s7F_Screenshot%202019-08-10%20at%2010.47.09%20am.png"" style=""""></p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE ARTICLE: <a href=""https://www.statnews.com/2019/07/18/clinical-trials-birds-eye-view-drug-development/"" target=""_blank"" rel=""nofollow"">A bird’s-eye view of clinical trials</a><br>
DATA SOURCE/ORIGINAL VIZ: <a href=""https://www.aerodatalab.org/birds-eye-view-of-research-landscape"" target=""_blank"" rel=""nofollow"">Aero Data Lab</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
</ul>
This dataset was created by Andy Kriebel and contains around 10000 samples along with Start Year, Phase, technical information and other features such as:
- Status
- Condition
- and more.
How to use this dataset
&gt; - Analyze Title in relation to Start Month
- Study the influence of Sponsor on Enrollment
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	17	249	1	yamqwe	2019-w33-a-birds-eye-view-of-clinical-trialse
1345	1345	🩱 Sports Illustrated Covers	100 collected samples, technical information	['sports']	"About this dataset
&gt; <p>A data set listing the sports that have been on the cover of Sports Illustrated since 1955. Covers are grouped by year. You can see the related <a href=""https://www.crowdflower.com/its-hockey-weekend-in-america-but-does-anyone-care/"" target=""_blank"" rel=""nofollow"">blog post here</a>.   Added: February 12, 2015 by CrowdFlower | Data Rows: 32000 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 100 samples along with Other Individual Sports, Surfing, technical information and other features such as:
- Bowling
- Boxing
- and more.
How to use this dataset
&gt; - Analyze Horse Racing in relation to Figure Skating
- Study the influence of Skiing on Golf
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	10	83	1	yamqwe	sports-illustrated-coverse
1346	1346	✨ How ISIS Uses Twitter	20000 collected samples, technical information	['religion and belief systems']	"About this dataset
&gt; <p>Over 17,000 tweets scrapped from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. To help develop effective counter-messaging measures against violent extremists at home and abroad.</p>
<p>The dataset includes the following:</p>
<ul>
<li>Name</li>
<li>Username</li>
<li>Description</li>
<li>Location</li>
<li>Number of followers at the time the tweet was downloaded</li>
<li>Number of statuses by the user when the tweet was downloaded</li>
<li>Date and timestamp of the tweet</li>
<li>The tweet itself</li>
</ul>
<p>Based on this data, here are some useful ways of deriving insights and analysis:</p>
<ul>
<li>Social Network Cluster Analysis: Who are the major players in the pro-ISIS twitter network? Ideally, we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers.</li>
<li>Keyword Analysis: Which keywords derived from the name, username, description, location, and tweets were the most commonly used by ISIS fanboys? Examples include: ""baqiyah"", ""dabiq"", ""wilayat"", ""amaq""</li>
<li>Data Categorization of Links: Which websites are pro-ISIS fanboys linking to? Categories include: Mainstream Media, Altermedia, Jihadist Websites, Image Upload, Video Upload,</li>
<li>Sentiment Analysis: Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive, negative, or neutral and if negative, include the reasons why. Examples of clergy they like the most: ""Anwar Awlaki"", ""Ahmad Jibril"", ""Ibn Taymiyyah"", ""Abdul Wahhab"". Examples of clergy that they hate the most: ""Hamza Yusuf"", ""Suhaib Webb"", ""Yaser Qadhi"", ""Nouman Ali Khan"", ""Yaqoubi"".</li>
<li>Timeline View: Visualize all the tweets over a timeline and identify peak moments</li>
</ul>
<p>Source: Kaggle</p>
<p><a href=""https://www.kaggle.com/kzaman/how-isis-uses-twitter"" target=""_blank"" rel=""nofollow"">https://www.kaggle.com/kzaman/how-isis-uses-twitter</a></p>
This dataset was created by Data Society and contains around 20000 samples along with Followers, Username, technical information and other features such as:
- Time
- Name
- and more.
How to use this dataset
&gt; - Analyze Description in relation to Location
- Study the influence of Numberstatuses on Followers
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	21	259	1	yamqwe	how-isis-uses-twittere
1347	1347	🫂 Comparing Pictures of People	60000 collected samples, technical information	['people', 'computer vision', 'image data']	"About this dataset
&gt; <p>In this job, contributors viewed two pictures of people walking through the same room and were then asked to compare the person on the left to the person on the right. Questions center on observable traits (like skin color, hair length, muscularity, etc.). An example: For “Weight”, the person on the left is:<br>
Much more heavy<br>
More heavy<br>
Same<br>
More light<br>
Much more light<br>
Data set contains nearly 60,000 rows of judgments; all images are relative URLs based on the following structure <a href=""http://users.ecs.soton.ac.uk/dmc1g14/biot/frames/%5Bcamera%5D/%5Bimg_path%5D"" target=""_blank"" rel=""nofollow"">http://users.ecs.soton.ac.uk/dmc1g14/biot/frames/[camera]/[img_path]</a>   For ""Weight"", the person on the left is:</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 60000 samples along with Question:confidence, Img Path0, technical information and other features such as:
- Uid1
- Nil
- and more.
How to use this dataset
&gt; - Analyze Img Path1 in relation to Sample1
- Study the influence of Orig Question on Uid0
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	9	226	1	yamqwe	comparing-pictures-of-peoplee
1348	1348	📺  Fox News Facebook Shares vs Likes	1000 collected samples, technical information	['internet', 'news', 'online communities', 'social networks']	"About this dataset
&gt; <h1>Background</h1>
<p>Most people presume Facebook shares increase post likes, but most people aren’t aware at what rate this occurs. For this study, we will analyze the impact a single Facebook  share has on a Facebook post. Data for this study was pulled from two news outlets and two universities via Facebook’s graph API, in order to analyze two different types of Facebook pages.</p>
<h1>Methodology</h1>
<p>In order to have a representative sample, two data sets will be pulled from the same topic. For example, we will pull posts from two news outlets with 1,000 posts each.</p>
<h1>Source</h1>
<p>The source of this data and study can be found at <a href=""http://theconceptcenter.com/simple-research-study-impact-of-facebook-shares/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 1000 samples along with Unnamed: 4, Unnamed: 3, technical information and other features such as:
- Likes
- Shares
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 4 in relation to Unnamed: 3
- Study the influence of Likes on Shares
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	17	179	1	yamqwe	fox-news-facebook-shares-vs-likese
1349	1349	🌡 Weather sentiment evaluated	1000 collected samples, technical information	['healthcare', 'health', 'drugs and medications']	"About this dataset
&gt; <p>Here, contributors were asked if the crowd graded the sentiment of a particular tweet relating to the weather correctly. The original job (above this one, called simply ""Weather sentiment"") involved 20 contributors noting the sentiment of weather-related tweets. In this job, we asked 10 contributors to check that original sentiment evaluation for accuracy.   The button to the right is the aggregated data set. You can also download the non-aggregated, full data set.</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 1000 samples along with Orig Trusted Judgments, Trusted Judgments, technical information and other features such as:
- Orig Unit Id
- Canary
- and more.
How to use this dataset
&gt; - Analyze Last Judgment At in relation to Orig Canary
- Study the influence of Orig Unit State on Is The Category Correct For This Tweet:confidence
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	15	291	1	yamqwe	weather-sentiment-evaluatede
1350	1350	yearly		[]		0	0	0	mudialq	yearly
1351	1351	Resumen Bono Blindaje 6 meses		[]		0	5	0	fisinauta	resumen-bono-blindaje-6-meses
1352	1352	How Trump Least Enthusiastic Voters Feel Now?	200 collected samples, technical information	['politics', 'beginner']	"About this dataset
&gt; <h1>SurveyMonkey Survey of Reluctant Trump Voters</h1>
<p>This directory contains the raw data behind <a href=""https://fivethirtyeight.com/features/how-trumps-least-enthusiastic-voters-feel-about-him-now/"" target=""_blank"" rel=""nofollow"">How Trump‰Ûªs Least Enthusiastic Voters Feel About Him Now</a></p>
<ul>
<li><code>may_survey_questionnaire.pdf</code> contains the questionnaire of the SurveyMonkey News Survey conducted in partnership with FiveThirtyEight.</li>
<li><code>may_survey_responses.xlsx</code> contains the responses to interviews conducted from May 26 to June 4, 2017 including crosstabs of Trump voters who were excited to vote for Trump versus those who were not.</li>
<li><code>april_survey_responses.xlsx</code> contains the responses to interviews conducted from April 18 to April 23, 2017 including crosstabs of Trump voters who were excited to vote for Trump versus those who were not.</li>
<li><code>march_survey_responses.xlsx</code> contains the responses to interviews conducted from March 31 to April 7, 2017 including crosstabs of Trump voters who were excited to vote for Trump versus those who were not.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data/tree/master/reluctant-trump"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/reluctant-trump</a></p>
This dataset was created by FiveThirtyEight and contains around 200 samples along with Unnamed: 8, Unnamed: 33, technical information and other features such as:
- Unnamed: 37
- Unnamed: 15
- and more.
How to use this dataset
&gt; - Analyze 5 Point Party Id in relation to Unnamed: 22
- Study the influence of Unnamed: 4 on Unnamed: 28
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	11	249	4	yamqwe	reluctant-trumpe
1353	1353	🗳 Primary Candidates	800 collected samples, technical information	['politics', 'beginner']	"About this dataset
&gt; <p>This folder contains the data behind the stories:</p>
<ul>
<li><a href=""https://fivethirtyeight.com/features/democrats-primaries-candidates-demographics/"" target=""_blank"">We Researched Hundreds Of Races. Here’s Who Democrats Are Nominating.</a></li>
<li><a href=""https://fivethirtyeight.com/features/the-establishment-is-beating-the-progressive-wing-in-democratic-primaries-so-far/"" target=""_blank"">How’s The Progressive Wing Doing In Democratic Primaries So Far?</a></li>
<li><a href=""https://fivethirtyeight.com/features/we-looked-at-hundreds-of-endorsements-heres-who-republicans-are-listening-to/"" target=""_blank"">We Looked At Hundreds Of Endorsements. Here’s Who Republicans Are Listening To.</a></li>
</ul>
<p>This project looks at patterns in open Democratic and Republican primary elections for the U.S. Senate, U.S. House and governor in 2018.</p>
<p><code>dem_candidates.csv</code> contains information about the 811 candidates who have appeared on the ballot this year in Democratic primaries for Senate, House and governor, not counting races featuring a Democratic incumbent, as of August 7, 2018.</p>
<p><code>rep_candidates.csv</code> contains information about the 774 candidates who have appeared on the ballot this year in Republican primaries for Senate, House and governor, not counting races featuring a Republican incumbent, through September 13, 2018.</p>
<p>Here is a description and source for each column in the accompanying datasets.</p>
<p><code>dem_candidates.csv</code> and <code>rep_candidates.csv</code> include:</p>
<div><table><thead>
<tr>
<th>Column</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Candidate</code></td>
<td>All candidates who received votes in 2018’s Democratic primary elections for U.S. Senate, U.S. House and governor in which no incumbent ran. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>State</code></td>
<td>The state in which the candidate ran. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>District</code></td>
<td>The office and, if applicable, congressional district number for which the candidate ran. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Office Type</code></td>
<td>The office for which the candidate ran. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Race Type</code></td>
<td>Whether it was a “regular” or “special” election. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Race Primary Election Date</code></td>
<td>The date on which the primary was held. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Primary Status</code></td>
<td>Whether the candidate lost (“Lost”) the primary or won/advanced to a runoff (“Advanced”). Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Primary Runoff Status</code></td>
<td>“None” if there was no runoff; “On the Ballot” if the candidate advanced to a runoff but it hasn’t been held yet; “Advanced” if the candidate won the runoff; “Lost” if the candidate lost the runoff. Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>General Status</code></td>
<td>“On the Ballot” if the candidate won the primary or runoff and has advanced to November; otherwise, “None.” Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Primary %</code></td>
<td>The percentage of the vote received by the candidate in his or her primary. In states that hold runoff elections, we looked only at the first round (the regular primary). In states that hold all-party primaries (e.g., California), a candidate’s primary percentage is the percentage of the total Democratic vote they received. Unopposed candidates and candidates nominated by convention (not primary) are given a primary percentage of 100 but were excluded from our analysis involving vote share. Numbers come from official results posted by the secretary of state or local elections authority; if those were unavailable, we used unofficial election results from the New York Times.</td>
</tr>
<tr>
<td><code>Won Primary</code></td>
<td>“Yes” if the candidate won his or her primary and has advanced to November; “No” if he or she lost.</td>
</tr>
</tbody>
</table></div>
<p><code>dem_candidates.csv</code> includes:</p>
<div><table><thead>
<tr>
<th>Column</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Gender</code></td>
<td>“Male” or “Female.” Supplied by Ballotpedia.</td>
</tr>
<tr>
<td><code>Partisan Lean</code></td>
<td>The FiveThirtyEight partisan lean of the district or state in which the election was held. Partisan leans are calculated by finding the average difference between how a state or district voted in the past two presidential elections and how the country voted overall, with 2016 results weighted 75 percent and 2012 results weighted 25 percent.</td>
</tr>
<tr>
<td><code>Race</code></td>
<td>“White” if we identified the candidate as non-Hispanic white; “Nonwhite” if we identified the candidate as Hispanic and/or any nonwhite race; blank if we could not identify the candidate’s race or ethnicity. To determine race and ethnicity, we checked each candidate’s website to see if he or she identified as a certain race. If not, we spent no more than two minutes searching online news reports for references to the candidate’s race.</td>
</tr>
<tr>
<td><code>Veteran?</code></td>
<td>If the candidate’s website says that he or she served in the armed forces, we put “Yes.” If the website is silent on the subject (or explicitly says he or she didn’t serve), we put “No.” If the field was left blank, no website was available.</td>
</tr>
<tr>
<td><code>LGBTQ?</code></td>
<td>If the candidate’s website says that he or she is LGBTQ (including indirect references like to a same-sex partner), we put “Yes.” If the website is silent on the subject (or explicitly says he or she is straight), we put “No.” If the field was left blank, no website was available.</td>
</tr>
<tr>
<td><code>Elected Official?</code></td>
<td>We used Ballotpedia, VoteSmart and news reports to research whether the candidate had ever held elected office before, at any level. We put “Yes” if the candidate has held elected office before and “No” if not.</td>
</tr>
<tr>
<td><code>Self-Funder?</code></td>
<td>We used Federal Election Committee fundraising data (for federal candidates) and state campaign-finance data (for gubernatorial candidates) to look up how much each candidate had invested in his or her own campaign, through either donations or loans. We put “Yes” if the candidate donated or loaned a cumulative $400,000 or more to his or her own campaign before the primary and “No” for all other candidates.</td>
</tr>
<tr>
<td><code>STEM?</code></td>
<td>If the candidate identifies on his or her website that he or she has a background in the fields of science, technology, engineering or mathematics, we put “Yes.” If not, we put “No.” If the field was left blank, no website was available.</td>
</tr>
<tr>
<td><code>Obama Alum?</code></td>
<td>We put “Yes” if the candidate mentions working for the Obama administration or campaign on his or her website, or if the candidate shows up on this list of Obama administration members and campaign hands running for office. If not, we put “No.”</td>
</tr>
<tr>
<td><code>Dem Party Support?</code></td>
<td>“Yes” if the candidate was placed on the DCCC’s Red to Blue list before the primary, was endorsed by the DSCC before the primary, or if the DSCC/DCCC aired pre-primary ads in support of the candidate. (Note: according to the DGA’s press secretary, the DGA does not get involved in primaries.) “No” if the candidate is running against someone for whom one of the above things is true, or if one of those groups specifically anti-endorsed or spent money to attack the candidate. If those groups simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Emily Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Emily’s List before the primary. “No” if the candidate is running against an Emily-endorsed candidate or if Emily’s List specifically anti-endorsed or spent money to attack the candidate. If Emily’s List simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Gun Sense Candidate?</code></td>
<td>“Yes” if the candidate received the Gun Sense Candidate Distinction from Moms Demand Action/Everytown for Gun Safety before the primary, according to media reports or the candidate’s website. “No” if the candidate is running against an candidate with the distinction. If Moms Demand Action simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Biden Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Joe Biden before the primary. “No” if the candidate is running against a Biden-endorsed candidate or if Biden specifically anti-endorsed the candidate. If Biden simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Warren Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Elizabeth Warren before the primary. “No” if the candidate is running against a Warren-endorsed candidate or if Warren specifically anti-endorsed the candidate. If Warren simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Sanders Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Bernie Sanders before the primary. “No” if the candidate is running against a Sanders-endorsed candidate or if Sanders specifically anti-endorsed the candidate. If Sanders simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Our Revolution Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Our Revolution before the primary, according to the Our Revolution website. “No” if the candidate is running against an Our Revolution-endorsed candidate or if Our Revolution specifically anti-endorsed or spent money to attack the candidate. If Our Revolution simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Justice Dems Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Justice Democrats before the primary, according to the Justice Democrats website, candidate website or news reports. “No” if the candidate is running against a Justice Democrats-endorsed candidate or if Justice Democrats specifically anti-endorsed or spent money to attack the candidate. If Justice Democrats simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>PCCC Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Progressive Change Campaign Committee before the primary, according to the PCCC website, candidate website or news reports. “No” if the candidate is running against a PCCC-endorsed candidate or if the PCCC specifically anti-endorsed or spent money to attack the candidate. If the PCCC simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Indivisible Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Indivisible before the primary, according to the Indivisible website, candidate website or news reports. “No” if the candidate is running against an Indivisible-endorsed candidate or if Indivisible specifically anti-endorsed or spent money to attack the candidate. If Indivisible simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>WFP Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Working Families Party before the primary, according to the WFP website, candidate website or news reports. “No” if the candidate is running against a WFP-endorsed candidate or if the WFP specifically anti-endorsed or spent money to attack the candidate. If the WFP simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>VoteVets Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by VoteVets before the primary, according to the VoteVets website, candidate website or news reports. “No” if the candidate is running against a VoteVets-endorsed candidate or if VoteVets specifically anti-endorsed or spent money to attack the candidate. If VoteVets simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>No Labels Support?</code></td>
<td>“Yes” if a No Labels-affiliated group (Citizens for a Strong America Inc., Forward Not Back, Govern or Go Home, United for Progress Inc. or United Together) spent money in support of the candidate in the primary. “No” if the candidate is running against an candidate supported by a No Labels-affiliated group or if a No Labels-affiliated group specifically anti-endorsed or spent money to attack the candidate. If No Labels simply did not weigh in on the race, we left the cell blank.</td>
</tr>
</tbody>
</table></div>
<p><code>rep_candidates.csv</code> includes:</p>
<div><table><thead>
<tr>
<th>Column</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Rep Party Support?</code></td>
<td>“Yes” if the candidate was named to the NRCC’s Young Guns list (any tier) before the primary, was endorsed by the NRSC before the primary, was endorsed by the RGA before the primary or if the Senate Leadership Fund or Congressional Leadership Fund made independent expenditures in support of the candidate in the primary.  “No” if the candidate is running against someone for whom one of the above things is true, or if one of those groups specifically anti-endorsed or spent money to attack the candidate. If those groups simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Trump Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by President Trump before the primary. “No” if the candidate is running against a Trump-endorsed candidate or if Trump specifically anti-endorsed the candidate. If Trump simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Bannon Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Steve Bannon before the primary. “No” if the candidate is running against a Bannon-endorsed candidate or if Bannon specifically anti-endorsed the candidate. If Bannon simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Great America Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Great America Alliance before the primary, according to the Great America Alliance website, candidate website or news reports, or if Great America PAC spent money in the primary on the candidate’s behalf. “No” if the candidate is running against a Great America-endorsed candidate or if Great America specifically anti-endorsed or spent money to attack the candidate. If Great America simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>NRA Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the National Rifle Association before the primary, according to the NRA website, candidate website or news reports, or if the NRA made independent expenditures in the primary on the candidate’s behalf. “No” if the candidate is running against an NRA-endorsed candidate or if the NRA specifically anti-endorsed or spent money to attack the candidate. If the NRA simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Right to Life Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the National Right to Life Committee or a local affiliate before the primary, according to the organization’s website, candidate website or news reports. “No” if the candidate is running against a Right to Life-endorsed candidate(s) or if Right to Life specifically anti-endorsed or spent money to attack the candidate. If Right to Life simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Susan B. Anthony Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by Susan B. Anthony List before the primary, according to the SBA List website, candidate website or news reports. “No” if the candidate is running against a SBA List-endorsed candidate or if SBA List specifically anti-endorsed or spent money to attack the candidate. If SBA List simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Club for Growth Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Club for Growth before the primary, according to the Club for Growth website, candidate website or news reports, or if Club for Growth Action made independent expenditures in the primary on the candidate’s behalf. “No” if the candidate is running against a Club for Growth-endorsed candidate or if the Club for Growth specifically anti-endorsed or spent money to attack the candidate. If the Club for Growth simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Koch Support?</code></td>
<td>“Yes” if the candidate was supported by the Koch brothers’ political network in the primary. This could include an endorsement from David or Charles Koch, financial support from groups like Americans for Prosperity (including a state affiliate), Freedom Partners Action Fund or KochPAC or an endorsement from one of those groups. “No” if the candidate is running against a Koch-endorsed candidate or if the Koch brothers’ political network specifically anti-endorsed or spent money to attack the candidate. If the Koch brothers’ political network simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>House Freedom Support?</code></td>
<td>“Yes” if the candidate was supported by the House Freedom Fund in the primary. This could include an endorsement, a donation or an independent expenditure on the candidate’s behalf. “No” if the candidate is running against a House Freedom Fund-endorsed candidate or if the House Freedom Fund specifically anti-endorsed or spent money to attack the candidate. If the House Freedom Fund simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Tea Party Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Tea Party Express before the primary, according to the Tea Party Express website, candidate website or news reports. “No” if the candidate is running against a Tea Party Express-endorsed candidate or if Tea Party Express specifically anti-endorsed or spent money to attack the candidate. If Tea Party Express simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Main Street Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the Republican Main Street Partnership before the primary, according to the candidate website or news reports. “No” if the candidate is running against a Main Street-endorsed candidate or if Main Street specifically anti-endorsed or spent money to attack the candidate. If Main Street simply did not weigh in on the race, we left the cell blank.</td>
</tr>
<tr>
<td><code>Chamber Endorsed?</code></td>
<td>“Yes” if the candidate was endorsed by the U.S. Chamber of Commerce or a state-level chamber of commerce before the primary, according to the candidate website or news reports, or if the national or state chamber of commerce made independent expenditures for the candidate during the primary. “No” if the candidate is running against a Chamber of Commerce-endorsed candidate or if the Chamber of Commerce specifically anti-endorsed or spent money to attack the candidate. If the Chamber of Commerce simply did not weigh in on the race, we left the cell blank.</td>
</tr>
</tbody>
</table></div>
<h4>Updates</h4>
<ul>
<li>Post data for elections through Aug 7, 2018. [<a href=""https://github.com/fivethirtyeight/private-data/commit/36007c25c5a975f37958ae6d72aabadfcdd273a8"" target=""_blank"">commit</a>]</li>
<li>Fix record for David Trone. [<a href=""https://github.com/fivethirtyeight/private-data/commit/d43ed51823b71966ce1de330684ecbc0e520cc56"" target=""_blank"">commit</a>]</li>
<li>Add new columns and update and added the result in Washington's 8th Congressional District (called Aug. 13). [<a href=""https://github.com/fivethirtyeight/data/commit/27166710f84ade6a59d9b7cec4cd9d31cf0ead03"" target=""_blank"">commit</a>]</li>
<li>Add Republican Candidates. [<a href=""https://github.com/fivethirtyeight/data/commit/8d6995e6785a2c912101f5f9c18022530f3783d6"" target=""_blank"">commit</a>]</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by data.world's Admin and contains around 800 samples along with Justice Dems Endorsed?, Vote Vets Endorsed?, technical information and other features such as:
- Self Funder?
- Warren Endorsed?
- and more.
How to use this dataset
&gt; - Analyze Biden Endorsed? in relation to Race Primary Election Date
- Study the influence of Our Revolution Endorsed? on Primary Runoff Status
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	12	96	2	yamqwe	primary-candidatese
1354	1354	✨ Twitter Transparency Reports	47 collected samples, technical information	['politics', 'internet', 'news', 'social networks']	"About this dataset
&gt; <p><code>In-the-News</code>:</p>
<ul>
<li><em>CNET</em>: <a href=""https://www.cnet.com/news/governments-keep-dming-twitter-for-information-requests/"" target=""_blank"" rel=""nofollow"">Governments keep DM'ing Twitter for information requests</a></li>
<li><em>WIRED</em>: <a href=""https://www.wired.com/2016/09/government-requests-twitter-info-keep-growing/"" target=""_blank"" rel=""nofollow"">Government Requests For Twitter Info Keep Growing</a></li>
<li><em>The Christian Science Monitor</em>: <a href=""http://www.csmonitor.com/World/Passcode/2016/0921/With-terror-in-spotlight-government-requests-for-Twitter-data-surge"" target=""_blank"" rel=""nofollow"">With terror in spotlight, government requests for Twitter data surge</a></li>
<li><em>TechCrunch</em>: <a href=""https://techcrunch.com/2016/09/21/twitters-new-transparency-report-is-disclosure-done-right/"" target=""_blank"" rel=""nofollow"">Twitter’s new transparency report is disclosure done right</a></li>
<li><em>Vice</em>: <a href=""http://motherboard.vice.com/read/government-requests-for-twitter-user-data-continue-to-skyrocket"" target=""_blank"" rel=""nofollow"">Government Requests for Twitter User Data Continue to Skyrocket</a></li>
</ul>
<p>Twitter and other social networking sites like <a href=""https://data.world/dash/reddit-transparency-reports"">Reddit</a> publish regular reports that inform their communities of government requests for information. Twitter defines government requests as, ""requests issued by law enforcement and other government agencies."" The files in this dataset chronicle how the rate of requests vary from country to country and year to year.</p>
<p><code>WIP</code>: Currently contains the information requests portion of the Transparency Report only.</p>
<blockquote>
<p>First published on July 2, 2012, our biannual Twitter Transparency Report highlights trends in legal requests we've received, intellectual property-related requests, and email privacy best practices. The report also provides insight into whether or not we take action on these requests.</p>
</blockquote>
<blockquote>
<p>Information requests include worldwide government requests we’ve received for account information, typically in connection with criminal investigations.<br>
The latest report includes the number of government and non-government requests received for account information, as well as the percentage of requests we complied with in some manner. We also mark the countries from which we have only received emergency disclosure requests with an asterisk (*).</p>
</blockquote>
<p><img src=""http://i.imgur.com/jEFTt2x.png"" alt=""alt text"" style=""""></p>
<p><em>Text and Image from the <a href=""https://transparency.twitter.com/"" target=""_blank"" rel=""nofollow"">https://transparency.twitter.com/</a></em></p>
<h1>Files</h1>
<ul>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jan-jun-2012.csv"">Jan - Jun 2012</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jul-dec-2012.csv"">Jul - Dec 2012</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jan-jun-2013.csv"">Jan - Jun 2013</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jul-dec-2013.csv"">Jul - Dec 2013</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jan-jun-2014.csv"">Jan - Jun 2014</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jul-dec-2014.csv"">Jul - Dec 2014</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jan-jun-2015.csv"">Jan - Jun 2015</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jul-dec-2015.csv"">Jul - Dec 2015</a></li>
<li><a href=""https://data.world/dash/twitter-transparency-reports/file/information-requests-report-jan-jun-2016.csv"">Jan - Jun 2016</a></li>
</ul>
<p>Attribution: <a href=""https://transparency.twitter.com/en/information-requests.html"" target=""_blank"" rel=""nofollow"">Twitter Information Requests</a></p>
This dataset was created by Dashiel Lopez Mendez and contains around 0 samples along with Accounts Specified, Percentage Where Some Information Produced, technical information and other features such as:
- Emergency Disclosure Request Only
- Account Information Requests
- and more.
How to use this dataset
&gt; - Analyze Accounts Specified in relation to Percentage Where Some Information Produced
- Study the influence of Emergency Disclosure Request Only on Account Information Requests
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Dashiel Lopez Mendez 
Start A New Notebook!"	12	230	1	yamqwe	twitter-transparency-reportse
1355	1355	🗳 Pollster Ratings	10000 collected samples, technical information	['politics', 'beginner']	"About this dataset
&gt; <p>This dataset contains the data behind FiveThirtyEight's pollster ratings.</p>
<ul>
<li><a href=""https://projects.fivethirtyeight.com/pollster-ratings/"" target=""_blank"" rel=""nofollow"">FiveThirtyEight's Pollster Ratings</a></li>
<li><a href=""https://fivethirtyeight.com/features/the-state-of-the-polls-2019/"" target=""_blank"" rel=""nofollow"">The State Of The Polls, 2019</a></li>
<li><a href=""https://fivethirtyeight.com/features/the-polls-are-all-right/"" target=""_blank"" rel=""nofollow"">The Polls Are All Right</a></li>
<li><a href=""https://fivethirtyeight.com/features/the-state-of-the-polls-2016/"" target=""_blank"" rel=""nofollow"">The State Of The Polls, 2016</a></li>
<li><a href=""https://fivethirtyeight.com/features/how-fivethirtyeight-calculates-pollster-ratings/"" target=""_blank"" rel=""nofollow"">How FiveThirtyEight Calculates Pollster Ratings</a></li>
</ul>
<p><code>pollster-stats-full</code> contains a spreadsheet with all of the summary data and calculations involved in determining the pollster ratings as well as descriptions for each column.<br>
<code>pollster-ratings</code> has ratings and calculations for each pollster. A copy of this data and descriptions for each column can also be found in pollster-stats-full.<br>
<code>raw-polls</code> contains all of the polls analyzed to give each pollster a grade</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
<p><em><strong>License:</strong></em> The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a>. If you find it useful, please <a href=""mailto:data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><em><strong>Updated:</strong></em> Pollster-ratings and raw-polls synced from source weekly.</p>
This dataset was created by FiveThirtyEight and contains around 10000 samples along with Cand2 Id, Pollster, technical information and other features such as:
- Samplesize
- Partisan
- and more.
How to use this dataset
&gt; - Analyze Cand2 Party in relation to Race Id
- Study the influence of Margin Poll on Cand1 Actual
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	16	141	2	yamqwe	pollster-ratingse
1356	1356	🧑 FDA Orange Book Drug Data July 2017	900 collected samples, technical information	['beginner', 'drugs and medications']	"About this dataset
&gt; <p>This data set has drug information from the FDA Orange Book, with data files updated as of July 10, 2017. This data set can be used to view drugs by pharma, along with drug status, exclusivity, and patent information.</p>
<p><strong>Source:</strong> <a href=""https://www.fda.gov/Drugs/InformationOnDrugs/ucm129689.htm"" target=""_blank"" rel=""nofollow"">FDA Orange Book</a></p>
<h3>Quick References</h3>
<h4>Exclusivity Code Overview</h4>
<ul>
<li><strong>D:</strong> New Dosing Schedule (NCI*)</li>
<li><strong>GAIN:</strong> Generating Antibiotic Incentives Now Act (GAIN Act)  - additional 5 years</li>
<li><strong>I:</strong> New Indication (NCI*)</li>
<li><strong>M:</strong> Miscellaenous</li>
<li><strong>NC:</strong> New Combination (NCI*)</li>
<li><strong>NCE:</strong> New Chemical - 5 years</li>
<li><strong>NDF:</strong> New Dosage Form (NCI*)</li>
<li><strong>NE:</strong> New Ester Or Salt (NCI*)</li>
<li><strong>NP:</strong> New Product (NCI*)</li>
<li><strong>NPP:</strong> New Patient Population (NCI*)</li>
<li><strong>NR:</strong> New Route (NCI*)</li>
<li><strong>NS:</strong> New Strength (NCI*)</li>
<li><strong>ODE:</strong> Orphan Drug Exclusivity - 7 years</li>
<li><strong>PC:</strong> Patent Challenge - 180 day exclusivity for Abbreviated New Drug Applications (ANDAs)</li>
<li><strong>PED:</strong> Pediatric Exclusivity - 6 months added to existing patent / exclusivity</li>
<li><strong>RTO:</strong> Rx to OTC switch</li>
<li><strong>W:</strong> Exclusivity waived by sponsor</li>
</ul>
<p><strong>Sources:</strong></p>
<ul>
<li><a href=""https://books.google.com/books?id=STZ4DAAAQBAJ"" target=""_blank"" rel=""nofollow"">Approved Drug Products With Therapeutic Equivalance Evaluations</a></li>
<li><a href=""https://books.google.com/books?id=1XCmCwAAQBAJ"" target=""_blank"" rel=""nofollow"">Pharmaceutical Product Development: Insights Into Pharmaceutical Processes, Management and Regulatory Affairs</a></li>
</ul>
This dataset was created by Basil Hayek and contains around 900 samples along with Code Meaning, Code Meaning, technical information and other features such as:
- Code Meaning
- Code Meaning
- and more.
How to use this dataset
&gt; - Analyze Code Meaning in relation to Code Meaning
- Study the influence of Code Meaning on Code Meaning
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Basil Hayek 
Start A New Notebook!"	9	74	2	yamqwe	fda-orange-book-drug-data-july-2017e
1357	1357	🧑‍🤝‍🧑 Geert Hofstede Cultural Dimension	100 collected samples, technical information	['beginner']	"About this dataset
&gt; <p>Geert Hofstede's cultural dimensions theory proposes a method of analyzing cultures based on a handful of continuums.</p>
<p><strong>Power distance index (PDI)</strong>: The power distance index is defined as “the extent to which the less powerful members of organizations and institutions (like the family) accept and expect that power is distributed unequally.”</p>
<p><strong>Individualism vs. collectivism (IDV)</strong>: This index explores the “degree to which people in a society are integrated into groups.”</p>
<p><strong>Uncertainty avoidance index (UAI)</strong>: The uncertainty avoidance index is defined as “a society's tolerance for ambiguity,” in which people embrace or avert an event of something unexpected, unknown, or away from the status quo.</p>
<p><strong>Masculinity vs. femininity (MAS)</strong>: In this dimension, masculinity is defined as “a preference in society for achievement, heroism, assertiveness and material rewards for success.”</p>
<p><strong>Long-term orientation vs. short-term orientation (LTO)</strong>: This dimension associates the connection of the past with the current and future actions/challenges.</p>
<p><strong>Indulgence vs. restraint (IND)</strong>: This dimension is essentially a measure of happiness; whether or not simple joys are fulfilled.</p>
<p><em>Description Source: <a href=""https://en.wikipedia.org/wiki/Hofstede's_cultural_dimensions_theory"" target=""_blank"" rel=""nofollow"">https://en.wikipedia.org/wiki/Hofstede's_cultural_dimensions_theory</a></em></p>
This dataset was created by Adam Helsinger and contains around 100 samples along with Uai, Pdi, technical information and other features such as:
- Ivr
- Idv
- and more.
How to use this dataset
&gt; - Analyze Ltowvs in relation to Mas
- Study the influence of Uai on Pdi
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	16	74	2	yamqwe	geerthofstedeculturaldimensione
1358	1358	AHCA Polls	0 collected samples, technical information	['politics', 'beginner']	"About this dataset
&gt; <h1>American Health Care Act Polls</h1>
<p>The raw data behind the story <a href=""https://fivethirtyeight.com/features/why-the-gop-is-so-hell-bent-on-passing-an-unpopular-health-care-bill"" target=""_blank"" rel=""nofollow"">Why The GOP Is So Hell-Bent On Passing An Unpopular Health Care Bill</a></p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data/tree/master/ahca-polls"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/ahca-polls</a></p>
This dataset was created by FiveThirtyEight and contains around 0 samples along with End, Start, technical information and other features such as:
- Text
- Favor
- and more.
How to use this dataset
&gt; - Analyze Pollster in relation to Oppose
- Study the influence of End on Start
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	21	231	2	yamqwe	ahca-pollse
1359	1359	📛 Citibike Tripdata -JC- August 2016	30000 collected samples, technical information	['business']	"About this dataset
&gt; <p>Where do Citi Bikers ride? When do they ride? How far do they go? Which stations are most popular? What days of the week are most rides taken on? This data helps you discover the answers to these questions and more.</p>
<p>The data includes:</p>
<p>Trip Duration (seconds)<br>
Start Time and Date<br>
Stop Time and Date<br>
Start Station Name<br>
End Station Name<br>
Station ID<br>
Station Lat/Long<br>
Bike ID<br>
User Type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)<br>
Gender (Zero=unknown; 1=male; 2=female)<br>
Year of Birth</p>
<p>Data notes:</p>
<p>Trip count and milage estimates include trips with a duration of greater than one minute.<br>
Milage estimates are calculated using an assumed speed of 7.456 miles per hour, up to two hours. Trips over two hours max-out at 14.9 miles.<br>
To learn more about Trip Data visit - <a href=""https://www.citibikenyc.com"" target=""_blank"" rel=""nofollow"">https://www.citibikenyc.com</a></p>
<p><strong><em>Source:</em></strong> <a href=""https://s3.amazonaws.com/tripdata/index.html"" target=""_blank"" rel=""nofollow"">https://s3.amazonaws.com/tripdata/index.html</a></p>
This dataset was created by Selene Arrazolo and contains around 30000 samples along with User Type, Bike Id, technical information and other features such as:
- Start Time
- Stop Time
- and more.
How to use this dataset
&gt; - Analyze Start Station Latitude in relation to End Station Latitude
- Study the influence of End Station Name on Start Station Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	18	139	1	yamqwe	citibike-tripdata-jc-august-2016e
1360	1360	💼 Corporate Misconducts	Datasets containing 112,000 incidents of corporate misconduct	['business', 'health']	"About this dataset
&gt; <p><code>In-the-News</code>:</p>
<ul>
<li><em>Good Jobs First Press Release</em>: <a href=""http://www.goodjobsfirst.org/news/releases/violation-tracker-expansion-highlights-fraud-healthcare-companies-and-banks"" target=""_blank"" rel=""nofollow"">Violation Tracker Expansion Highlights Fraud by Healthcare Companies and Banks</a></li>
<li><em>Politico</em>: <a href=""http://www.politico.com/tipsheets/morning-money/2016/06/brexit-keeps-pounding-markets-215050"" target=""_blank"" rel=""nofollow"">Brexit keeps pounding markets</a></li>
<li><em>AFL-CIO</em>: <a href=""http://www.aflcio.org/Blog/Corporate-Greed/New-Online-Database-Makes-It-Easier-to-Fight-Corporate-Crime"" target=""_blank"" rel=""nofollow"">New Online Database Makes It Easier to Fight Corporate Crime</a></li>
<li><em>Corporate Crime Reporter</em>: <a href=""http://www.corporatecrimereporter.com/news/200/160-billion-in-penalties-against-banks-in-last-six-years/9"" target=""_blank"" rel=""nofollow"">Penalties Against Banks In Last Six Years Totals More Than $160 Billion</a></li>
<li><em>Albany Times Union</em>: <a href=""http://blog.timesunion.com/rogergreen/corporate-crime-database/5429"" target=""_blank"" rel=""nofollow"">Corporate Crime Database</a></li>
</ul>
<p>Good Jobs First maintains a database of ~112,000 incidents of corporate misconduct, as well as the associated penalties, $300 billion in all. The data is sourced from 30 federal regulatory agencies ranging from the Justice Department to the Nuclear Regulatory Commission.</p>
<p>Source: <a href=""http://www.goodjobsfirst.org/violation-tracker"" target=""_blank"" rel=""nofollow"">Good Jobs First</a><br>
Downloaded on October 7, 2016.</p>
<p><strong>Visualizations</strong><br>
<img src=""http://i.imgur.com/FrvXqIW.png"" alt=""Imgur"" style=""""></p>
<p><img src=""http://i.imgur.com/n6dQxYS.png"" alt=""Imgur"" style=""""></p>
This dataset was created by Dashiel Lopez Mendez and contains around 100000 samples along with Ownership Structure, Record Date, technical information and other features such as:
- Penalty Amount Adjusted For Eliminating Multiple Counting
- Civil/ Criminal
- and more.
How to use this dataset
&gt; - Analyze Penalty Amount in relation to Industry In Record
- Study the influence of Facility State on Primary Offense
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Dashiel Lopez Mendez 
Start A New Notebook!"	14	158	1	yamqwe	violation-trackere
1361	1361	💣 Historic Battles by Casualties	41 battles, technical information	['history', 'beginner']	"About this dataset
&gt; <p>List of battles by casualties and geolocation.<br>
Map <a href=""https://public.tableau.com/profile/tomek7068#!/vizhome/BattlebyCasualties/Sheet1"" target=""_blank"" rel=""nofollow"">https://public.tableau.com/profile/tomek7068#!/vizhome/BattlebyCasualties/Sheet1</a></p>
<p>The following is a list of the casualties count in battles in world history. The list includes both sieges (not technically battles but usually yielding similar combat-related deaths) and civilian casualties during the battles.</p>
<p>Source <a href=""https://en.wikipedia.org/wiki/List_of_battles_by_casualties"" target=""_blank"" rel=""nofollow"">https://en.wikipedia.org/wiki/List_of_battles_by_casualties</a></p>
This dataset was created by Tomek and contains around 0 samples along with Lon, Siege, technical information and other features such as:
- Year
- Conflict
- and more.
How to use this dataset
&gt; - Analyze Lat in relation to Casualties
- Study the influence of Lon on Siege
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tomek 
Start A New Notebook!"	19	238	2	yamqwe	list-of-battles-by-casualtiese
1362	1362	How Every NFL Team’s Fans Lean Politically?	34 collected samples, technical information	['politics', 'beginner']	"About this dataset
&gt; <p>Data behind the story <a href=""https://fivethirtyeight.com/features/how-every-nfl-teams-fans-lean-politically"" target=""_blank"" rel=""nofollow"">How Every NFL Team’s Fans Lean Politically</a>.</p>
<p><strong>Google Trends Data</strong></p>
<p>Google Trends data was derived from comparing 5-year search traffic for the 7 sports leagues we analyzed:</p>
<p><a href=""https://g.co/trends/5P8aa"" target=""_blank"" rel=""nofollow"">https://g.co/trends/5P8aa</a></p>
<p>Results are listed by designated market area (DMA).</p>
<p>The percentages are the approximate percentage of major-sports searches that were conducted for each league.</p>
<p>Trump's percentage is his share of the vote within the DMA in the 2016 presidential election.</p>
<p><strong>SurveyMonkey Data</strong></p>
<p>SurveyMonkey data was derived from a poll of American adults ages 18 and older, conducted between Sept. 1-7, 2017.</p>
<p>Listed numbers are the raw totals for respondents who ranked a given NFL team among their three favorites, and how many identified with a given party (further broken down by race). We also list the percentages of the entire sample that identified with each party, and were of each race.</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 0 samples along with Unnamed: 10, Unnamed: 4, technical information and other features such as:
- Unnamed: 3
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 13 in relation to Unnamed: 21
- Study the influence of Unnamed: 7 on Unnamed: 12
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	67	637	7	yamqwe	nfl-fandome
1363	1363	US Customs and Border Protection child detentions	600000 collected samples, technical information	['crime', 'government', 'public safety']	"About this dataset
&gt; <p>Data from US Customs and Border Protection obtained and published by <a href=""https://www.themarshallproject.org/"" target=""_blank"" rel=""nofollow"">The Marshall Project</a> and used in <a href=""https://www.themarshallproject.org/2020/10/30/500-000-kids-30-million-hours-trump-s-vast-expansion-of-child-detention"" target=""_blank"" rel=""nofollow"">500,000 Kids, 30 Million Hours: Trump’s Vast Expansion of Child Detention</a>.</p>
<p>The data is a combination of two sources:</p>
<ul>
<li>CBP Office of Field Operations (OFO) child detentions: Detentions of children at ports of entry by the U.S. Customs and Border Protection Border Patrol between mid-January of 2017 and late January of 2020.</li>
<li>CBP Border Patrol (BP) child detentions: Detentions of children between ports of entry by the U.S. Customs and Border Protection Office of Field Operations between mid-January of 2017 and mid-June of 2020.</li>
</ul>
<p>See <a href=""https://observablehq.com/@themarshallproject/cbp-child-detentions-2017-to-2020"" target=""_blank"" rel=""nofollow"">https://observablehq.com/@themarshallproject/cbp-child-detentions-2017-to-2020</a> for more details.</p>
<p><em><strong>Source:</strong></em> <a href=""https://github.com/themarshallproject/cbp-migrantchildren-detention-data"" target=""_blank"" rel=""nofollow"">https://github.com/themarshallproject/cbp-migrantchildren-detention-data</a></p>
<p><em><strong>Updated:</strong></em> synced from source weekly</p>
<p><em><strong>License:</strong></em> <a href=""https://github.com/themarshallproject/cbp-migrantchildren-detention-data/blob/main/LICENSE"" target=""_blank"" rel=""nofollow"">MIT License</a></p>
<p>Copyright (c) 2020 The Marshall Project</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sellcopies of the Software, and to permit persons to whom the Software isfurnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in allcopies or substantial portions of the Software.</p>
<p>THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS ORIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>
<p>​</p>
This dataset was created by Liz Friedman and contains around 600000 samples along with Age Group, Sector, technical information and other features such as:
- Source
- Date In
- and more.
How to use this dataset
&gt; - Analyze Gender in relation to Hours In Custody
- Study the influence of Date Out on Citizenship
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Liz Friedman 
Start A New Notebook!"	10	85	2	yamqwe	us-customs-and-border-protection-child-detentione
1364	1364	R2D2 Training Data	Natural Images for UnSupervised tasks	['biology', 'beginner', 'image data']	Some Natural Image for UnSupervied Learning Tasks	3	10	2	javidtheimmortal	naturalimages
1365	1365	🏈 NFL Predictions	300 collected samples, technical information	['beginner']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>This file contains links to the data behind <a href=""https://projects.fivethirtyeight.com/complete-history-of-the-nfl/"" target=""_blank"" rel=""nofollow"">The Complete History Of The NFL</a> and our <a href=""https://projects.fivethirtyeight.com/2020-nfl-predictions/"" target=""_blank"" rel=""nofollow"">NFL Predictions</a>.</p>
<p><code>nfl_elo.csv</code> contains game-by-game Elo ratings and forecasts back to 1920.<br>
<code>nfl_elo_latest.csv</code> contains game-by-game Elo ratings and forecasts for only the latest season.</p>
<h2><strong>License</strong></h2>
<p>Data released under <a href=""https://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 License</a></p>
<h2><strong>Source</strong></h2>
<p><a href=""https://github.com/fivethirtyeight/data/tree/master/nfl-elo"" target=""_blank"" rel=""nofollow"">GitHub</a></p>
This dataset was created by data.world's Admin and contains around 300 samples along with Importance, Qb1 Game Value, technical information and other features such as:
- Qbelo2 Post
- Score1
- and more.
How to use this dataset
&gt; - Analyze Qb2 Value Pre in relation to Neutral
- Study the influence of Date on Team2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	21	329	2	yamqwe	nfl-predictionse
1366	1366	🛐 US Places of Worship	50000 collected samples, technical information	['religion and belief systems']	"About this dataset
&gt; <p>This dataset contains almost 50,000 Buddhist, Christian, Hindu, Islamic, Judaic, and Sikh places of worship across the US. Lat/lon coordinates allow for geographic visualization along with a variety of other attributes.</p>
<p>​</p>
<p><strong>Source:</strong>  <a href=""https://hifld-geoplatform.opendata.arcgis.com/datasets/51cd2ffaea2e4579aace5a1d4f0de71f_0"" target=""_blank"" rel=""nofollow"">HIFLD Open GP - Public Venues</a></p>
This dataset was created by Chris Awram and contains around 50000 samples along with X, Subtype, technical information and other features such as:
- State Id
- Cath
- and more.
How to use this dataset
&gt; - Analyze Directions in relation to Naicscode
- Study the influence of State on Naicsdescr
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chris Awram 
Start A New Notebook!"	18	146	2	yamqwe	us-places-of-worshipe
1367	1367	Is a certain body part a part of an other part? 🤔 	Body Part Relationships quiz results 	['government', 'politics']	"About this dataset
&gt; <p>A data set where contributors classified if certain body parts were part of other parts. Questions were phrased like so: ""[Part 1] is a part of [part 2],"" or, by way of example, ""Nose is a part of spine"" or ""Ear is a part of head.""   Added: February 4, 2015 by CrowdFlower | Data Rows: 1892 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 2000 samples along with True Or False Gold, Part 2, technical information and other features such as:
- Last Judgment At
- Trusted Judgments
- and more.
How to use this dataset
&gt; - Analyze True Or False:confidence in relation to True Or False
- Study the influence of Part 1 on Unit State
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	6	116	2	yamqwe	body-part-relationshipse
1368	1368	💰 US Median Weekly Earnings by Year	Weekly Earnings for full-time employed workers in US	['business', 'economics']	"About this dataset
&gt; <p>Adjusted to be in constant (1982-1984) dollars using CPI-U.  For full-time employed workers only.</p>
<p><strong>Source:</strong> US Bureau of Labor Statistics</p>
<p>​</p>
This dataset was created by Chris Awram and contains around 0 samples along with Median Weekly Earnings, Year, technical information and other features such as:
- Median Weekly Earnings
- Year
- and more.
How to use this dataset
&gt; - Analyze Median Weekly Earnings in relation to Year
- Study the influence of Median Weekly Earnings on Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chris Awram 
Start A New Notebook!"	47	368	1	yamqwe	us-median-weekly-earnings-by-yeare
1369	1369	👣 Ecological Footprint per capita 	Ecological Footprint per capita data for years 1961-2012 in global hectares	['environment', 'business']	"About this dataset
&gt; <h1>National Footprint Accounts 2016 Edition</h1>
<p>Dataset provides Ecological Footprint per capita data for years 1961-2012 in global hectares (gha).</p>
<p><strong>Ecological Footprint</strong> is a measure of how much area of biologically productive land and water an individual, population, or activity requires to produce all the resources it consumes and to absorb the waste it generates, using prevailing technology and resource management practices. The Ecological Footprint is measured in global hectares. Because trade is global, an individual or country's Footprint includes land or sea from all over the world. Without further specification, Ecological Footprint generally refers to the Ecological Footprint of consumption. Ecological Footprint is often referred to in short form as Footprint.</p>
<p></p>
<div class=""""><iframe class=""embed-responsive-item"" id=""youtubeplayer"" type=""text/html"" src=""//www.youtube.com/embed/_T5M3MiPfW4"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen="""" width=""640"" height=""390"" frameborder=""0""></iframe></div>
<p></p>
This dataset was created by Global Footprint Network and contains around 8000 samples along with Quality Score, Year, technical information and other features such as:
- Country Code
- Ef Percap
- and more.
How to use this dataset
&gt; - Analyze Country in relation to Quality Score
- Study the influence of Year on Country Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Global Footprint Network 
Start A New Notebook!"	38	353	1	yamqwe	nfa-2016-editione
1370	1370	Political Donation History Of Wealthy Sports	3000 collected samples, technical information	['sports', 'history', 'politics', 'beginner', 'investing', 'social issues and advocacy']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>This dataset contains the data behind the story <a href=""https://fivethirtyeight.com/features/inside-the-political-donation-history-of-wealthy-sports-owners/"" target=""_blank"" rel=""nofollow"">Inside The Political Donation History Of Wealthy Sports Owners</a>.</p>
<p><code>sports-political-donations.csv</code> contains every confirmed partisan political contribution from team owners and commissioners in the NFL, NBA, WNBA, NHL, MLB and NASCAR.</p>
<h2><strong>Caveats</strong></h2>
<p>Only contributions while owners were involved with the team are included. The data is from the Federal Election Commission and OpenSecrets.</p>
<h2><strong>License</strong></h2>
<p>Dataset released under <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 License</a></p>
<h2><strong>Source</strong></h2>
<p><a href=""https://github.com/fivethirtyeight/data/tree/master/sports-political-donations"" target=""_blank"" rel=""nofollow"">GitHub</a></p>
This dataset was created by data.world's Admin and contains around 3000 samples along with Party, Amount, technical information and other features such as:
- Election Year
- Recipient
- and more.
How to use this dataset
&gt; - Analyze Team in relation to League
- Study the influence of Owner on Party
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	80	677	9	yamqwe	political-donation-history-of-wealthy-sports-owne
1371	1371	✨ Classification of Pol Social	5000 collected samples, technical information	['social networks']	"About this dataset
&gt; <p>Contributors looked at thousands of social media messages from US Senators and other American politicians to classify their content. Messages were broken down into audience (national or the tweeter's constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)   Added: August 5, 2015 by CrowdFlower | Data Rows: 5000 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 5000 samples along with Audience, Message:confidence, technical information and other features such as:
- Last Judgment At
- Id
- and more.
How to use this dataset
&gt; - Analyze Trusted Judgments in relation to Label
- Study the influence of Bias Gold on Bias:confidence
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	15	163	3	yamqwe	classification-of-pol-sociale
1372	1372	Adverse Events Total Hip Replacement 2014	In-depth Dataset for Hip Replacement Projects	['healthcare', 'health']	"About this dataset
&gt; <p>This dataset identifies adverse events associated with medical devices for total hip replacement. This dataset includes reports submitted from January 2014 through December 2014.</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with Event Type, Manufacturer, technical information and other features such as:
- Event Date
- Date Received
- and more.
How to use this dataset
&gt; - Analyze Product Code in relation to Brand Name
- Study the influence of Event Type on Manufacturer
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	17	136	2	yamqwe	adverse-events-total-hip-replacement-2014e
1373	1373	🏈 NFL Favorite Team	32 collected samples, technical information	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 0 samples along with Cch, Slp, technical information and other features such as:
- Nyp
- Beh
- and more.
How to use this dataset
&gt; - Analyze Cch in relation to Slp
- Study the influence of Nyp on Beh
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	16	147	2	yamqwe	nfl-favorite-teame
1374	1374	Interdependence NGOs by Funding, Reach	Data on 163 nonprofit organizations across the philanthropic landscape	['business', 'finance', 'government']	"About this dataset
&gt; <h3>SUMMARY</h3>
<p>This dataset contains information on 163 different nonprofit organizations across the philanthropic landscape whose interventions focus on efforts to promote interdependence, mutual humanity, and combat growing polarization in Western society.</p>
<h3>PURPOSE</h3>
<p>This data was collected manually by the user to help an impact investing nonprofit shed light on the burgeoning field of nonprofit interventions in the realm of interdependence.</p>
<h3>INSIGHTS</h3>
<p></p>
This dataset was created by Carl V. Lewis and contains around 200 samples along with Polarization, Three Year Funding, technical information and other features such as:
- Funding (2015)
- Funding (2016)
- and more.
How to use this dataset
&gt; - Analyze Tags in relation to Description
- Study the influence of Fb Likes on Interdependence
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Carl V. Lewis 
Start A New Notebook!"	19	136	1	yamqwe	interdependence-ngos-by-funding-reache
1375	1375	🇺🇸 US Economic Performance Quiz	Is a sentence an indication of the U.S. economy's health?	['business', 'health', 'news']	"About this dataset
&gt; <p>Contributors viewed a new article headline and a short, bolded excerpt of a sentence or two from the attendant article. Next, they decided if the sentence in question provided an indication of the U.S. economy's health, then rated the indication on a scale of 1-9, with 1 being negative and 9 being positive.   Added: June 25, 2015 by CrowdFlower | Data Rows: 5000 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 5000 samples along with Previous Sentence, Articleid, technical information and other features such as:
- Relevance:confidence
- Trusted Judgments
- and more.
How to use this dataset
&gt; - Analyze Headline in relation to Next Sentence
- Study the influence of Orig Golden on Last Judgment At
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	26	194	2	yamqwe	us-economic-performancee
1376	1376	🕴 Historical IFC Reach Data	9000 collected samples, technical information	['business', 'finance', 'banking', 'investing']	"About this dataset
&gt; <p>Lack of access to financial services is a key barrier to the growth of micro, small, medium enterprises (MSMEs). IFC is working to develop solutions to close the MSME financing gap. By partnering with many types of financial intermediaries, including microfinance institutions (MFIs), commercial banks, leasing companies, and private equity funds, IFC reaches many more small and medium enterprises (SMEs) than it could directly.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/cedv-xjcw"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/cedv-xjcw</a></p>
This dataset was created by Finance and contains around 9000 samples along with Financial Institution Type, Indicator, technical information and other features such as:
- Region
- Unit
- and more.
How to use this dataset
&gt; - Analyze Indicator Category in relation to Loan Size
- Study the influence of Value on Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	11	132	2	yamqwe	historical-ifc-reach-datae
1377	1377	Myers-Briggs Personality Type Frequency 	16 collected samples, technical information	['united states', 'psychology']	"About this dataset
&gt; <h1>Original Visualization</h1>
<p><img src=""http://www.myersbriggs.org/_images/estimated_frequency_table.gif"" alt=""supporting image or chart"" style=""""></p>
<h1>Data Source</h1>
<ul>
<li><a href=""http://www.myersbriggs.org/my-mbti-personality-type/my-mbti-results/how-frequent-is-my-type.htm"" target=""_blank"" rel=""nofollow"">Myers-Briggs.org</a></li>
</ul>
This dataset was created by Andy Kriebel and contains around 0 samples along with (s)ensing/i(n)tuition, (t)hinking/(f)eeling, technical information and other features such as:
- (j)udging/(p)erceiving
- (e)xtroversion/(i)ntroversion
- and more.
How to use this dataset
&gt; - Analyze (s)ensing/i(n)tuition in relation to (t)hinking/(f)eeling
- Study the influence of (j)udging/(p)erceiving on (e)xtroversion/(i)ntroversion
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	21	315	2	yamqwe	2017-w43-how-frequent-is-my-myers-briggs-typee
1378	1378	👍 CNN Facebook Shares vs Likes	1000 collected samples, technical information	['arts and entertainment', 'internet', 'online communities', 'social networks']	"About this dataset
&gt; <h1>Background</h1>
<p>Most people presume Facebook shares increase post likes, but most people aren’t aware at what rate this occurs. For this study, we will analyze the impact a single Facebook  share has on a Facebook post. Data for this study was pulled from two news outlets and two universities via Facebook’s graph API, in order to analyze two different types of Facebook pages.</p>
<h1>Methodology</h1>
<p>In order to have a representative sample, two data sets will be pulled from the same topic. For example, we will pull posts from two news outlets with 1,000 posts each.</p>
<h1>Source</h1>
<p>The source of this data and study can be found at <a href=""http://theconceptcenter.com/simple-research-study-impact-of-facebook-shares/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 1000 samples along with Likes, Unnamed: 3, technical information and other features such as:
- Shares
- Unnamed: 4
- and more.
How to use this dataset
&gt; - Analyze Likes in relation to Unnamed: 3
- Study the influence of Shares on Unnamed: 4
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	28	283	2	yamqwe	cnn-facebook-shares-vs-likese
1379	1379	ACO Assigned Beneficiaries by County 2018	Prediction of Aged Non-Deed Assigned Beneficiary by County	['finance', 'health', 'insurance']	"About this dataset
&gt; <p>This dataset shows the ACO (Accountable Care Organization) Beneficiaries by County for the year 2018. Aged beneficiaries are identified as dually-eligible if they are identified in CMS (Centers for Medicare and Medicaid Services) data systems by dual status.</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with Total Assigned Beneficiary Person Years, Aco Number, technical information and other features such as:
- State Id
- Disabled Assigned Beneficiary
- and more.
How to use this dataset
&gt; - Analyze Aged Dual Assigned Beneficiary in relation to Esrd Assigned Beneficiary
- Study the influence of Aged Non Dual Assigned Beneficiary on State
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	4	89	2	yamqwe	aco-assigned-beneficiaries-by-county-2018e
1380	1380	🌎 National Footprint Accounts 2017 	200 collected samples, technical information	['global', 'earth and nature', 'environment', 'government']	"About this dataset
&gt; <p>Our National Footprint Accounts (NFAs) measure the ecological resource use and resource capacity of nations from 1961 to 2013.</p>
<p>The calculations in the National Footprint Accounts are primarily based on United Nations data sets, including those published by the Food and Agriculture Organization, United Nations Commodity Trade Statistics Database, and the UN Statistics Division, as well as the International Energy Agency.</p>
<h1>National Footprint Accounts 2017 Edition</h1>
<p>To visualize our data in our data explorer click here <a href=""http://data.footprintnetwork.org/"" target=""_blank"" rel=""nofollow"">http://data.footprintnetwork.org/</a></p>
<p>Dataset provides Ecological Footprint per capita data for years 1961-2013 in global hectares (gha).</p>
<p><strong>Ecological Footprint</strong> is a measure of how much area of biologically productive land and water an individual, population, or activity requires to produce all the resources it consumes and to absorb the waste it generates, using prevailing technology and resource management practices. The Ecological Footprint is measured in global hectares. Because trade is global, an individual or country's Footprint includes land or sea from all over the world. Without further specification, Ecological Footprint generally refers to the Ecological Footprint of consumption. Ecological Footprint is often referred to in short form as Footprint.</p>
<p></p>
<div class=""""><iframe class=""embed-responsive-item"" id=""youtubeplayer"" type=""text/html"" src=""//www.youtube.com/embed/_T5M3MiPfW4"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen="""" width=""640"" height=""390"" frameborder=""0""></iframe></div>
<p></p>
<hr>
<h2>Visual Exploratory Analysis: Ecological Footprint vis-a-vis GDP</h2>
<p>We created this analysis to begin the process of better understanding the extent to which economic growth (GDP) was coupled with consumption of natural resources (EF; Ecological Footprint) over the interval from 2009-2013. What do you see in the results? We'd love to hear your thoughts in the dicussions. GDP data obtained from the <a href=""https://data.world/worldbank/world-development-indicators"">WorldBank World Development Indicator dataset</a>.</p>
<p><img src=""https://data.world/api/footprint/dataset/nfa-2017-edition/file/raw/EF_GDP.png"" alt=""EF_GDP.png"" style=""""></p>
<p><em>x-axis: GDP growth 2009-2013 (as percent of 2009 level) - Ordinal scale (rank-ordered)</em><br>
<em>y-axis: EF growth 2009-2013 (as percent of 2009 level) - Ordinal scale (rank-ordered)</em><br>
<em>Markersize = GDP2013 (Constant 2010 US Dollar)</em><br>
<em>Dashed lines = zero</em><br>
<em>The bottom-right region represents <strong>absolute decoupling</strong> -- (+) GDP, (-) EF</em><br>
<em>For higher-resolution, the chart can be viewed below as a png file</em></p>
<hr>
<h2>Nation Rankings:</h2>
<h3>Degree of Decoupling 2009-2013</h3>
<p><em>DegreeOfDecoupling (DDelta_P)= Percent growth in GDP minus percent growth in Ecological Footprint (GDPDelta_P minus EFDelta_P)</em><br>
<em>Absolute Decoupling = 1 if GDP growth &gt; 0 and Ecological Footprint growth &lt; 0 ( (GDPDelta &gt; 0) &amp; (EFDelta &lt; 0)) else 0</em></p>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/ddelta_gdp.png"" alt=""ddelta_gdp.png"" style=""""></p>
<h3>Degree of Decoupling 2009-2013 -- All 36 nations which achieved absolute decoupling (Increased GDP and Decreased Ecological Footprint)</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/ddc.png"" alt=""ddc.png"" style=""""></p>
<h3>Smallest Total Ecological Footprint 2013</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.08.29%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.08.29 PM.png"" style=""""></p>
<h3>Largest Total Ecological Footprint 2013</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.13.29%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.13.29 PM.png"" style=""""></p>
<h3>Smallest GDP 2013 (2010 USD (billions))</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.14.54%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.14.54 PM.png"" style=""""></p>
<h3>Largest GDP 2013 (2010 USD (billions))</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.16.58%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.16.58 PM.png"" style=""""></p>
<h3>Smallest Footprint Growth 2009-2013 (Percent; Negative values = reduced footprint)</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.18.47%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.18.47 PM.png"" style=""""></p>
<h3>Largest Footprint Growth 2009-2013 (Percent)</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.20.50%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.20.50 PM.png"" style=""""></p>
<h3>Smallest GDP Growth 2009-2013 (Percent; Negative values = reduced GDP)</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.22.24%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.22.24 PM.png"" style=""""></p>
<h3>Largest GDP Growth 2009-2013 (Percent)</h3>
<p><img src=""https://data.world/api/nrippner/dataset/my-footprint-data/file/raw/Screen%20Shot%202017-04-03%20at%207.24.29%20PM.png"" alt=""Screen Shot 2017-04-03 at 7.24.29 PM.png"" style=""""></p>
This dataset was created by Global Footprint Network and contains around 200 samples along with Country, Dec Flag, technical information and other features such as:
- Country
- Dec Flag
- and more.
How to use this dataset
&gt; - Analyze Country in relation to Dec Flag
- Study the influence of Country on Dec Flag
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Global Footprint Network 
Start A New Notebook!"	26	393	2	yamqwe	nfa-2017-editione
1381	1381	IBRD Statement Cash Flows 2010	39 collected samples, technical information	['business']	"About this dataset
&gt; <p>Provides data from the IBRD Statement of Cash Flows for the fiscal years ended June 30, 2010, June 30, 2009 and June 30, 2008. Sum of all cash flows represent the net changes in unrestricted cash. The values are expressed in millions of U.S. Dollars.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/v84d-dq44"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/v84d-dq44</a></p>
This dataset was created by Finance and contains around 0 samples along with Source, Fiscal Year, technical information and other features such as:
- Line Item
- Amount (us$, Millions)
- and more.
How to use this dataset
&gt; - Analyze Source in relation to Fiscal Year
- Study the influence of Line Item on Amount (us$, Millions)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	28	169	3	yamqwe	ibrd-statement-cash-flows-2010e
1382	1382	IDA Statement Cash Flows 2011	15 collected samples, technical information	['business']	"About this dataset
&gt; <p>Provides data from the IDA Statement of Cash Flows for the fiscal years ended June 30, 2011, June 30, 2010 and June 30, 2009. Sum of all cash flows represent the net changes in unrestricted cash.The values are expressed in millions of U.S. Dollars. Amount in millions of US Dollars, rounded.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/gt7f-f7vc"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/gt7f-f7vc</a></p>
This dataset was created by Finance and contains around 0 samples along with Source, Amount (us$, Millions), technical information and other features such as:
- Fiscal Year
- Line Item
- and more.
How to use this dataset
&gt; - Analyze Source in relation to Amount (us$, Millions)
- Study the influence of Fiscal Year on Line Item
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	12	141	3	yamqwe	ida-statement-cash-flows-2011e
1383	1383	🏀 NBA Draft 2015	1000 collected samples, technical information	['basketball', 'beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 1000 samples along with Position, Draft Year, technical information and other features such as:
- Player
- Superstar
- and more.
How to use this dataset
&gt; - Analyze Position in relation to Draft Year
- Study the influence of Player on Superstar
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	22	272	3	yamqwe	nba-draft-2015e
1384	1384	🚴 Citibike Tripdata -JC- October 2017	30000 collected samples, technical information	['business']	"About this dataset
&gt; <p>Where do Citi Bikers ride? When do they ride? How far do they go? Which stations are most popular? What days of the week are most rides taken on? This data helps you discover the answers to these questions and more.</p>
<p>The data includes:</p>
<p>Trip Duration (seconds)<br>
Start Time and Date<br>
Stop Time and Date<br>
Start Station Name<br>
End Station Name<br>
Station ID<br>
Station Lat/Long<br>
Bike ID<br>
User Type (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)<br>
Gender (Zero=unknown; 1=male; 2=female)<br>
Year of Birth</p>
<p>Data notes:</p>
<p>Trip count and milage estimates include trips with a duration of greater than one minute.<br>
Milage estimates are calculated using an assumed speed of 7.456 miles per hour, up to two hours. Trips over two hours max-out at 14.9 miles.<br>
To learn more about Trip Data visit - <a href=""https://www.citibikenyc.com"" target=""_blank"" rel=""nofollow"">https://www.citibikenyc.com</a></p>
<p><strong><em>Source:</em></strong> <a href=""https://s3.amazonaws.com/tripdata/index.html"" target=""_blank"" rel=""nofollow"">https://s3.amazonaws.com/tripdata/index.html</a></p>
This dataset was created by Selene Arrazolo and contains around 30000 samples along with Tripduration, Stoptime, technical information and other features such as:
- Start Station Id
- Bikeid
- and more.
How to use this dataset
&gt; - Analyze Start Station Latitude in relation to End Station Name
- Study the influence of Gender on End Station Id
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	29	208	3	yamqwe	citibike-tripdata-jc-october-2017e
1385	1385	Special Economic Zones By Country	400 collected samples, technical information	['economics']	"About this dataset
&gt; <h1>About this project</h1>
<p>WORK IN PROGRESS:<br>
Trying to collect all the Special Economic Zones (SEZ) of the world into one central databases to be used for economic analysis.</p>
<h1>Objectives:</h1>
<h2>Data Collection</h2>
<ul>
<li>SEZgraphics like size, name, location  (latitude / longitude) for SEZs in each country</li>
<li>Annual SEZ value (in current $USD or local currency)</li>
<li>Classification &amp; generalization of tax incentives located in each SEZ</li>
</ul>
<h2>Data Verification</h2>
<ul>
<li>Validate locations</li>
<li>Validate size and value estimations</li>
<li>Compare tax incentives</li>
</ul>
<h2>Data Analysis &amp; Visualization</h2>
<ul>
<li>Correlate certain tax provisions with SEZ value</li>
<li>Assess success of SEZs by making global comparisons</li>
</ul>
<h1>Background</h1>
<p>This project was conceived as a result of a graduate economics paper on the topic of Special Economic Zones: Case of China, Nigeria, India and UAE. To authors' great surprise, there was no central location to view the world's special economic zones, making the research for even four countries SEZs painstakingly long. This project aims to fix that by creating a central repository of all special economic zones, yielding better assessment in the future.</p>
<h1>Get involved</h1>
<p>This project is in need of:</p>
<ul>
<li>Data collection: This info is out there, it's just not organized. Help with web crawling, web scraping and plain research very welcome. Contact me for more details.</li>
</ul>
<h1>External resources</h1>
This dataset was created by Diana Kontsevaia and contains around 400 samples along with Year, Month, technical information and other features such as:
- Type
- Date
- and more.
How to use this dataset
&gt; - Analyze Day in relation to Longitude
- Study the influence of Size on Province
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Diana Kontsevaia 
Start A New Notebook!"	32	295	3	yamqwe	special-economic-zones-by-countrye
1386	1386	😐 Coachella 2015 Twitter	4000 collected samples, technical information	['arts and entertainment']	"About this dataset
&gt; <p>A sentiment analysis job about the lineup of Coachella 2015. We wrote about it <a href=""http://www.crowdflower.com/blog/coachella-2015-lineup"" target=""_blank"" rel=""nofollow"">here</a>. An additional, thousand-row data set about which artists fans were most excited about can be <a href=""http://cdn2.hubspot.net/hub/346378/file-2572886395-csv/DFE_CSVs/Coachella-2015-1-DFE.csv?t=1457728181112"" target=""_blank"" rel=""nofollow"">found here</a>. The button to the right concerns sentiment about the festival overall.   Added: February 4, 2015 by CrowdFlower | Data Rows: 3847 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 4000 samples along with Retweet Count, Tweet Created, technical information and other features such as:
- Tweet Id
- Name
- and more.
How to use this dataset
&gt; - Analyze Coachella Sentiment in relation to Tweet Location
- Study the influence of User Timezone on Tweet Coord
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	17	219	4	yamqwe	coachella-2015-twittere
1387	1387	🎓 Business Finance Courses From Udemy	1000 collected samples, technical information	['business', 'education', 'computer science']	"About this dataset
&gt; <h1>Background</h1>
<p>Udemy is a massive online open course (MOOC) web application. Within Udemy, a student can learn nearly anything. You may wonder, why would anyone take one of these courses? If you use Google’s Trends app, you can enter in different search terms and compare the world-wide volume of searches for that search term.</p>
<p>For example, I put in the terms, who, what, when, where, why and how. In addition, I furthered the comparison and added the terms, how to, what are, who is, why are, when do.</p>
<p>According to the trends on Google, obviously the worlds wants to know how to do things and this is exactly what Udemy does. It teaches people how to do things.</p>
<h1>Methodology</h1>
<p>I scraped the Udemy website and pulled many published courses for the topics of Graphic Design, Business Finance, Web Development and Musical Instruments.</p>
<h1>Source</h1>
<p>For the full study, see <a href=""http://theconceptcenter.com/simple-research-study-udemy-courses/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 1000 samples along with Content Info, Url, technical information and other features such as:
- Num Reviews
- Unnamed: 11
- and more.
How to use this dataset
&gt; - Analyze Instructional Level in relation to Published Time
- Study the influence of Column1 on Is Paid
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	19	160	2	yamqwe	business-finance-courses-from-udemye
1388	1388	Austin's data portal activity metrics	100 collected samples, technical information	['business']	"About this dataset
&gt; <h2>Background</h2>
<p>Austin's <a href=""https://data.austintexas.gov"" target=""_blank"">open data portal</a> provides lots of public data about the City of Austin. It also provides portal administrators with behind-the-scenes information about how the portal is used... but that data is mysterious, hard to handle in a spreadsheet, and not located all in one place.</p>
<p>Until now! Authorized city staff used admin credentials to grab this usage data and share it the public. The City of Austin wants to use this data to inform the development of its open data initiative and manage the open data portal more effectively.</p>
<p>This project contains related datasets for anyone to explore. These include site-level metrics, dataset-level metrics, and department information for context. A detailed detailed description of how the files were prepared (along with code) can be found on github <a href=""https://github.com/cityofaustin/data-portal-analytics/tree/master/soc-metrics-api-deep-dive"" target=""_blank"">here</a>.</p>
<h2>Example questions to answer about the data portal</h2>
<ol>
<li>What parts of the open data portal do people seem to value most?</li>
<li>What can we tell about who our users are?</li>
<li>How are our data publishers doing?</li>
<li>How much data is published programmatically vs manually?</li>
<li>How data is super fresh? Super stale?</li>
<li>Whatever you think we should know...</li>
</ol>
<h2>About the files</h2>
<h4><code>all_views_20161003.csv</code></h4>
<p>There is a resource available to portal administrators called ""Dataset of datasets"". This is the export of that resource, and it was captured on Oct 3, 2016. It contains a summary of the assets available on the data portal. While this file contains over 1400 resources (such as views, charts, and binary files), only 363 are actual tabular datasets.</p>
<h4><code>table_metrics_ytd.csv</code></h4>
<p>This file contains information about the 363 tabular datasets on the portal. Activity metrics for an individual dataset can be accessed by calling Socrata's views/metrics API and passing along the dataset's unique ID, a time frame, and admin credentials. The process of obtaining the 363 identifiers, calling the API, and staging the information can be reviewed in the python notebook <a href=""https://github.com/cityofaustin/data-portal-analytics/blob/master/soc-metrics-api-deep-dive/dataset-metrics.ipynb"" target=""_blank"">here</a>.</p>
<h4><code>site_metrics.csv</code></h4>
<p>This file is the export of site-level stats that Socrata generates using a given time frame and grouping preference. This file contains records about site usage each month from Nov 2011 through Sept 2016. By the way, it contains 285 columns... and we don't know what many of them mean. But we are determined to find out!! For a preliminary exploration of the columns and what portal-related business processes to which they might relate, check out the notes in this python notebook <a href=""https://github.com/cityofaustin/data-portal-analytics/blob/master/soc-metrics-api-deep-dive/site_metrics.ipynb"" target=""_blank"">here</a></p>
<h4><code>city_departments_in_current_budget.csv</code></h4>
<p>This file contains a list of all City of Austin departments according to how they're identified in the most recently approved budget documents. Could be helpful for getting to know more about who the publishers are.</p>
<h4><code>crosswalk_to_budget_dept.csv</code></h4>
<p>The City is in the process of standardizing how departments identify themselves on the data portal. In the meantime, here's a crosswalk from the department values observed in <code>all_views_20161003.csv</code> to the department names that appear in the City's budget</p>
This dataset was created by Hailey Pate and contains around 100 samples along with Di Sync Success, Browser Firefox 19, technical information and other features such as:
- Browser Firefox 33
- Di Sync Failed
- and more.
How to use this dataset
&gt; - Analyze Sf Query Error User in relation to Js Page View Admin
- Study the influence of Browser Firefox 37 on Datasets Created
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Hailey Pate 
Start A New Notebook!"	5	82	3	yamqwe	data-portal-activity-metricse
1389	1389	🕴 New Business Filings from 1890 - 2017	39 collected samples, technical information	['business']	"About this dataset
&gt; <h1>Background</h1>
<p>Being a resident of Rexburg, Idaho, I thought it would be fascinating to see stats on businesses that started, or at least were filed, within the city of Rexburg. In the year 2000, President Gordon B. Hinkley announced the 2-year university previously called Ricks College to become a 4-year university. The change of the university would change the city of Rexburg for many years.</p>
<h1>Methodology</h1>
<p>The main data source for this study was at Idaho Free Public Records Directory. All businesses filed to the Secretary of State will be posted in this webpage. This webpage was crawled to extract the information for this micro-research study.</p>
<h1>Source</h1>
<p>The published article with analyzed data can be found at <a href=""http://theconceptcenter.com/simple-research-study-new-business-filings-in-rexburg/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 0 samples along with Unnamed: 1, Unnamed: 0, technical information and other features such as:
- Unnamed: 1
- Unnamed: 0
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 1 in relation to Unnamed: 0
- Study the influence of Unnamed: 1 on Unnamed: 0
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	13	160	2	yamqwe	new-business-filings-from-1890-2017e
1390	1390	🍐 FDIC Failed Bank List	500 collected samples, technical information	['banking']	"About this dataset
&gt; <p>The FDIC is often appointed as receiver for failed banks. This list includes banks which have failed since October 1, 2000.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/fdic-failed-bank-list"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/fdic-failed-bank-list</a></p>
This dataset was created by Finance and contains around 500 samples along with Acquiring Institution, Bank Name, technical information and other features such as:
- Updated Date
- St
- and more.
How to use this dataset
&gt; - Analyze Closing Date in relation to City
- Study the influence of Acquiring Institution on Bank Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	20	184	4	yamqwe	fdic-failed-bank-liste
1391	1391	Greece Largest Companies	Dataset of the world's 2000 largest publicly listed corporations	['business', 'finance', 'economics', 'investing']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p>​Source: <a href=""%E2%80%8Bhttp://www.economywatch.com/companies/forbes-list/"" target=""_blank"" rel=""nofollow"">Economy Watch</a></p>
This dataset was created by Finance and contains around 0 samples along with Market Value ($billion), Assets ($billion), technical information and other features such as:
- Profits ($billion)
- Market Value ($billion)
- and more.
How to use this dataset
&gt; - Analyze Assets ($billion) in relation to Profits ($billion)
- Study the influence of Market Value ($billion) on Assets ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	33	276	3	yamqwe	greece-largest-companiese
1392	1392	🌡 Weather Check	900 collected samples, technical information	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 900 samples along with What Is Your Gender?, Age, technical information and other features such as:
- How Much Total Combined Money Did All Members Of Your Household Earn Last Year?
- If You Had A Smartwatch (like The Soon To Be Released Apple Watch), How Likely Or Unlikely Would You Be To Check The Weather On That Device?
- and more.
How to use this dataset
&gt; - Analyze How Do You Typically Check The Weather? in relation to Do You Typically Check A Daily Weather Report?
- Study the influence of Us Region on A Specific Website Or App (please Provide The Answer)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	67	617	5	yamqwe	weather-checke
1393	1393	🖨 React, Angular and Vue Reported Issues 	5000 collected samples, technical information	['earth and nature', 'beginner']	"About this dataset
&gt; <h1>Background</h1>
<p>In the web development industry technology changes rapidly. Within the past 7 years, JavaScript frameworks, in particular, have developed and become more popular. In this study, I’d like to examine React, Angular and Vue on a project level, to determine which framework is most widely accepted in the industry.</p>
<h1>Sources</h1>
<p>Thanks to <a href=""http://GitHub.com"" target=""_blank"" rel=""nofollow"">GitHub.com</a> and <a href=""http://theconceptcenter.com/simple-research-study-choosing-a-front-end-framework/"" target=""_blank"" rel=""nofollow"">The Concept Center</a> for pulling the data and doing a simple research study</p>
This dataset was created by Chase Willden and contains around 5000 samples along with Body, Day.1, technical information and other features such as:
- Day.2
- Userid
- and more.
How to use this dataset
&gt; - Analyze Year.2 in relation to Month.2
- Study the influence of Assignee on Milestone
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	11	293	4	yamqwe	react-angular-and-vue-reported-issues-and-release
1394	1394	🇦 UNDP Gender Inequality Index	13 collected samples, technical information	['beginner']	"About this dataset
&gt; <p>A composite measure reflecting inequality in achievements between women and men in three dimensions: reproductive health, empowerment and the labour market.</p>
<p>More information is available at <a href=""http://docs.hdx.rwlabs.org/"" target=""_blank"" rel=""nofollow"">http://docs.hdx.rwlabs.org/</a></p>
<p>Creative Commons Attribution for Intergovernmental Org</p>
This dataset was created by Adam Helsinger and contains around 0 samples along with This Document Is An Extract Of Data Compiled By Automated Extraction Of Data From A Variety Of Online Sources And Manually Compiled Sources., Unnamed: 1, technical information and other features such as:
- This Document Is An Extract Of Data Compiled By Automated Extraction Of Data From A Variety Of Online Sources And Manually Compiled Sources.
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze This Document Is An Extract Of Data Compiled By Automated Extraction Of Data From A Variety Of Online Sources And Manually Compiled Sources. in relation to Unnamed: 1
- Study the influence of This Document Is An Extract Of Data Compiled By Automated Extraction Of Data From A Variety Of Online Sources And Manually Compiled Sources. on Unnamed: 1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	31	253	4	yamqwe	undp-gender-inequality-indexe
1395	1395	📰 Economic News Article Tone	Dataset of judgments based on a 9 point scale from 1951 to 2014.	['business', 'economics', 'news']	"About this dataset
&gt; <p>Contributors read snippets of news articles. They then noted if the article was relevant to the US economy and, if so, what the tone of the article was. Tone was judged on a 9 point scale (from 1 to 9, with 1 representing the most negativity). Dataset contains these judgments as well as the dates, source titles, and text. Dates range from 1951 to 2014.   Added: December 8, 2015 by CrowdFlower | Data Rows: 8000 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 8000 samples along with Unit State, Positivity, technical information and other features such as:
- Last Judgment At
- Text
- and more.
How to use this dataset
&gt; - Analyze Golden in relation to Date
- Study the influence of Relevance on Positivity Gold
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	29	326	2	yamqwe	economic-news-article-tonee
1396	1396	IBRD Statement Of Income 2011	36 collected samples, technical information	['business', 'finance', 'economics', 'investing']	"About this dataset
&gt; <p>Provides data from the IBRD Statement of Income for the fiscal years ended June 30, 2011, June 30, 2010 and June 30, 2009. The values are expressed in millions of U.S. Dollars. Where applicable, changes have been made to certain line items on FY 2010 income statement to conform with the current year's presentation, but the comparable prior years' data sets have not been adjusted to reflect the reclassification impact of those changes.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/eycy-ub35"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/eycy-ub35</a></p>
This dataset was created by Finance and contains around 0 samples along with Line Item, Amount (us$, Millions), technical information and other features such as:
- Category
- Fiscal Year
- and more.
How to use this dataset
&gt; - Analyze Line Item in relation to Amount (us$, Millions)
- Study the influence of Category on Fiscal Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	22	143	3	yamqwe	ibrd-statement-of-income-2011e
1397	1397	🏀 NBA Elo	100000 collected samples, technical information	['basketball', 'beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 100000 samples along with Elo I, Is Playoffs, technical information and other features such as:
- Notes
- Gameorder
- and more.
How to use this dataset
&gt; - Analyze Win Equiv in relation to Forecast
- Study the influence of Game Result on Iscopy
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	7	140	5	yamqwe	nba-eloe
1398	1398	Scientists Donations To Politicians	900000 collected samples, technical information	['earth and nature', 'politics', 'beginner']	"About this dataset
&gt; <h3>Political Donations From the Scientific Community</h3>
<p>This directory contains the raw data behind <a href=""https://fivethirtyeight.com/features/when-scientists-donate-to-politicians-its-usually-to-democrats"" target=""_blank"" rel=""nofollow"">When Scientists Donate To Politicians, It‰Ûªs Usually To Democrats</a></p>
<p>The data in <code>science_federal_giving.csv</code> is from the Federal Election Commission, filtered to donations form the following occupations from 2007 to 2016:</p>
<ul>
<li>computer and information scientists</li>
<li>mathematical scientists</li>
<li>agricultural and food scientists</li>
<li>biological and medical scientists</li>
<li>environmental life scientists</li>
<li>chemists</li>
<li>Earth scientists, geologists and oceanographers</li>
<li>physicists and astronomers</li>
<li>other physical and related scientists</li>
<li>aerospace, aeronautical or astronautical engineers</li>
<li>chemical engineers</li>
<li>civil, architectural or sanitary engineers</li>
<li>electrical or computer hardware engineers</li>
<li>industrial engineers</li>
<li>mechanical engineers</li>
<li>other engineers</li>
<li>statisticians</li>
</ul>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cmte_nm</code></td>
<td>Committee name</td>
</tr>
<tr>
<td><code>cmte_id</code></td>
<td>Committee identification, assigned</td>
</tr>
<tr>
<td><code>cmte_tp</code></td>
<td>Committee type. <a href=""http://www.fec.gov/finance/disclosure/metadata/CommitteeTypeCodes.shtml"" target=""_blank"" rel=""nofollow"">List of committee type codes</a></td>
</tr>
<tr>
<td><code>cmte_pty</code></td>
<td>Committee party. <a href=""http://www.fec.gov/finance/disclosure/metadata/DataDictionaryPartyCodeDescriptions.shtml"" target=""_blank"" rel=""nofollow"">List of party codes</a></td>
</tr>
<tr>
<td><code>cand_name</code></td>
<td>Candidate name</td>
</tr>
<tr>
<td><code>cand_pty_affiliation</code></td>
<td>Political party affiliation reported by the candidate</td>
</tr>
<tr>
<td><code>cand_office_st</code></td>
<td>Candidate state</td>
</tr>
<tr>
<td><code>cand_office</code></td>
<td>Candidate office. H = House, P = President, S = Senate</td>
</tr>
<tr>
<td><code>cand_office_district</code></td>
<td>Candidate district</td>
</tr>
<tr>
<td><code>cand_status</code></td>
<td>Candidate status. C = Statutory candidate, F = Statutory candidate for future election, N = Not yet a statutory candidate, P = Statutory candidate in prior cycle</td>
</tr>
<tr>
<td><code>rpt_tp</code></td>
<td>Report type. <a href=""http://www.fec.gov/finance/disclosure/metadata/ReportTypeCodes.shtml"" target=""_blank"" rel=""nofollow"">Report type codes</a></td>
</tr>
<tr>
<td><code>transaction_pgi</code></td>
<td>The code for which the contribution was made. EYYYY (election plus election year). P = Primary, G = General, O = Other, C = Convention, R = Runoff, S = Special, E = Recount</td>
</tr>
<tr>
<td><code>transaction_tp</code></td>
<td>Transaction type. <a href=""http://www.fec.gov/finance/disclosure/metadata/DataDictionaryTransactionTypeCodes.shtml"" target=""_blank"" rel=""nofollow"">Type codes</a></td>
</tr>
<tr>
<td><code>entity_tp</code></td>
<td>Entity type. Only valid for electronic filings received after April 2002. CAN = Candidate, CCM = Candidate Committee, COM = Committee, IND = Individual (a person), ORG = Organization (not a committee and not a person), PAC = Political Action Committee, PTY = Party Organization</td>
</tr>
<tr>
<td><code>cleaned_name</code></td>
<td>Contributor/lender/transfer name</td>
</tr>
<tr>
<td><code>city</code></td>
<td>City/town</td>
</tr>
<tr>
<td><code>state</code></td>
<td>State</td>
</tr>
<tr>
<td><code>zip_code</code></td>
<td>Zip code</td>
</tr>
<tr>
<td><code>employer</code></td>
<td>Employer</td>
</tr>
<tr>
<td><code>cleaned_occupation</code></td>
<td>Occupation</td>
</tr>
<tr>
<td><code>classification</code></td>
<td>Classification of occupation</td>
</tr>
<tr>
<td><code>transaction_dt</code></td>
<td>Transaction date (MMDDYYYY)</td>
</tr>
<tr>
<td><code>cycle</code></td>
<td>Election cycle</td>
</tr>
<tr>
<td><code>transaction_amt</code></td>
<td>Transaction amount</td>
</tr>
<tr>
<td><code>2016_dollars</code></td>
<td>Transation amount adjusted for inflation</td>
</tr>
<tr>
<td><code>other_id</code></td>
<td>Other identification number. For contributions from individuals this column is null. For contributions from candidates or other committees this column will contain that contributor's FEC ID.</td>
</tr>
<tr>
<td><code>tran_id</code></td>
<td>Transaction ID</td>
</tr>
<tr>
<td><code>file_num</code></td>
<td>A unique identifier associated with each itemization or transaction appearing in an FEC electronic file. Only valid for electronic filings.</td>
</tr>
<tr>
<td><code>memo_cd</code></td>
<td>'X' indicates that the amount is not to be included in the itemization total.</td>
</tr>
<tr>
<td><code>memo_text</code></td>
<td>A description of the activity.</td>
</tr>
<tr>
<td><code>sub_id</code></td>
<td>FEC record number</td>
</tr>
</tbody>
</table></div>
<p>Source: <a href=""http://www.fec.gov/"" target=""_blank"" rel=""nofollow"">Federal Election Commission</a></p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data/tree/master/science-giving"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/science-giving</a></p>
This dataset was created by FiveThirtyEight and contains around 900000 samples along with Tran Id, Cand Office, technical information and other features such as:
- Transaction Pgi
- City
- and more.
How to use this dataset
&gt; - Analyze 2016 Dollars in relation to Zip Code
- Study the influence of Employer on Cleaned Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	41	481	7	yamqwe	science-givinge
1399	1399	Amazon Product Listing Data	This dataset includes product listing data from Amazon	['earth and nature']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 143949  Domain Name : amazon.com  Date Range : 01st Apr 2021 - 30th Apr 2021   File Extension : csv
Available Fields : Uniq Id, Crawl Timestamp, Pageurl, Website, Title, Num Of Reviews, Average Rating, Number Of Ratings, Model Num, Sku, Upc, Manufacturer, Model Name, Price, Monthly Price, Stock, Carrier, Color Category, Internal Memory, Screen Size, Specifications, Five Star, Four Star, Three Star, Two Star, One Star, Broken Link, Discontinued    
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	39	258	6	promptcloud	amazon-product-listing-data
1400	1400	 🌋 Disaster Relief Terms	8000 collected samples, technical information	['earth and nature', 'earth science', 'animals', 'business', 'nlp']	"About this dataset
&gt; <p>Contributors viewed a topic and a term and rated the relevancy of the latter to the former on a five point scale (1 being very irrelevant, 5 being very relevant). The topics all center around humanitarian aid or disaster relief and each topic was defined for contributors. They were also asked if the term was a specific person or place and whether it was misspelled.   Added: January 28, 2015 by CrowdFlower | Data Rows: 7566 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 8000 samples along with #name?.3, Unnamed: 21, technical information and other features such as:
- Unnamed: 28
- Term
- and more.
How to use this dataset
&gt; - Analyze Len in relation to Unnamed: 39
- Study the influence of Unnamed: 29 on Relevance:variance
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	23	232	4	yamqwe	disaster-relief-termse
1401	1401	🐕 Intelligence of Dogs	100 collected samples, technical information	['animals']	"About this dataset
&gt; <p>This dataset is based on research by Stanley Coren, a professor of canine psychology at the University of British Columbia. When Coren first published his book in 1994, there was a high degree of dispute of his analysis, though over time his work has been largely accepted.</p>
<p>Data Dictionary</p>
<ul>
<li>obey: probability that the breed obeys the first command (figure is lower bound)</li>
<li>reps_lower: lower limit of repetitions to understand new commands</li>
<li>reps_upper: upper limit of repetitions to understand new commands</li>
</ul>
<p>Source: <a href=""https://en.m.wikipedia.org/wiki/The_Intelligence_of_Dogs#cite_ref-ReferenceA_18-0"" target=""_blank"" rel=""nofollow"">https://en.m.wikipedia.org/wiki/The_Intelligence_of_Dogs#cite_ref-ReferenceA_18-0</a></p>
This dataset was created by len fishman and contains around 100 samples along with Obey, Reps Lower, technical information and other features such as:
- Reps Upper
- Classification
- and more.
How to use this dataset
&gt; - Analyze Obey in relation to Reps Lower
- Study the influence of Reps Upper on Classification
- More datasets
Acknowledgements
If you use this dataset in your research, please credit len fishman 
Start A New Notebook!"	36	583	3	yamqwe	intelligence-of-dogse
1402	1402	  🍔 Fast Food Restaurants Across America	A list of 10,000 fast food restaurants provided by Datafiniti.co	['business', 'food', 'restaurants']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of 10,000 fast food restaurants provided by <a href=""https://datafiniti.co/products/business-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Business Database</a>. The dataset includes the restaurant's address, city, latitude and longitude coordinates, name, and more.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do With This Data</h1>
<p>You can use this data to <a href=""https://datafiniti.co/fast-food-restaurants-america/"" target=""_blank"" rel=""nofollow"">rank cities with the most and least fast food restaurants across the U.S.</a> E.g.:</p>
<ul>
<li>Cities with the most and least McDonald's per capita</li>
<li>Fast food restaurants per capita for all states</li>
<li>Fast food restaurants with the most locations nationally</li>
<li>Major cities with the most and least fast food restaurants per capita</li>
<li>Small cities with the most fast food restaurants per capita</li>
<li>States with the most and least fast food restaurants per capita</li>
<li>The number of fast food restaurants per capita</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://developer.datafiniti.co/docs/business-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. <a href=""https://datafiniti.co/"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>You can access the full dataset by running the following query with <a href=""https://developer.datafiniti.co/docs/getting-started-with-business-data"" target=""_blank"" rel=""nofollow"">Datafiniti’s Business API</a>.</p>
<p><code>{   ""query"": ""dateUpdated:[2018-04-01 TO *] AND categories:\""Fast Food\"" AND country:US* AND name:*"", ""format"": ""csv"", ""download"": true, ""view"": ""datasets_fast_food_restaurants"" }</code></p>
<p>*<em>This query generated 97,965 records as of December 10, 2018. The total number of results may vary.</em></p>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/business-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 10000 samples along with Websites, Postal Code, technical information and other features such as:
- Date Updated
- Primary Categories
- and more.
How to use this dataset
&gt; - Analyze Country in relation to Date Added
- Study the influence of Name on Longitude
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	135	1164	4	yamqwe	fast-food-restaurants-across-americae
1403	1403	🦟 Zika Virus Epidemic	100000 collected samples, technical information	['public health', 'regression']	"About this dataset
&gt; <p>The dataset includes the following fields:</p>
<p>report_date - The report date is the date that the report was published. The date should be specified in standard ISO format (YYYY-MM-DD).</p>
<p>location - A location is specified for each observation following the specific names specified in the country place name database. This may be any place with a 'location_type' as listed below, e.g. city, state, country, etc. It should be specified at up to three hierarchical levels in the following format: [country]-[state/province]-[county/municipality/city], always beginning with the country name. If the data is for a particular city, e.g. Salvador, it should be specified: Brazil-Bahia-Salvador.</p>
<p>location_type - A location code is included indicating: city, district, municipality, county, state, province, or country. If there is need for an additional 'location_type', open an Issue to create a new 'location_type'.</p>
<p>data_field - The data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes.</p>
<p>data_field_code - This code is defined in the country data guide. It includes a two letter country code (ISO-3166 alpha-2, list), followed by a 4-digit number corresponding to a specific report type and data type.</p>
<p>time_period - Optional. If the data pertains to a specific period of time, for example an epidemiological week, that number should be indicated here and the type of time period in the 'time_period_type', otherwise it should be NA.</p>
<p>time_period_type - Required only if 'time_period' is specified. Types will also be specified in the country data guide. Otherwise should be NA.</p>
<p>value - The observation indicated for the specific 'report_date', 'location', 'data_field' and when appropriate, 'time_period'.</p>
<p>unit - The unit of measurement for the 'data_field'. This should conform to the 'data_field' unit options as described in the country-specific data guide.</p>
<p>Source: Kaggle</p>
<p><a href=""https://www.kaggle.com/cdc/zika-virus-epidemic"" target=""_blank"" rel=""nofollow"">https://www.kaggle.com/cdc/zika-virus-epidemic</a></p>
This dataset was created by Data Society and contains around 100000 samples along with Location, Time Period, technical information and other features such as:
- Data Field
- Value
- and more.
How to use this dataset
&gt; - Analyze Location Type in relation to Time Period Type
- Study the influence of Unit on Data Field Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	37	273	4	yamqwe	zika-virus-epidemice
1404	1404	🗳 VEP Turnout	National and state level election turnout rates by race and ethnicity	['government', 'politics', 'social science', 'retail and shopping']	"About this dataset
&gt; <h2>Files:</h2>
<p>National level</p>
<ul>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=U.S.+VEP+Turnout+1789-Present+-+Statistics.csv"">U.S. VEP Turnout 1789-Present-Statistics</a> - The complete time series of national presidential and midterm general election turnout rates from 1787-present.</li>
</ul>
<p>National and state level</p>
<ul>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=1980-2014+November+General+Election+-+Turnout+Rates.csv"">1980-2014 November General Election - Turnout Rates</a></li>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=2016+November+General+Election+-+Turnout+Rates.csv"">2016 November General Election - Turnout Rates</a></li>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=2018+November+General+Election+-+Turnout+Rates.csv"">2018 November General Election - Turnout Rates</a></li>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=2020+November+General+Election+-+Turnout+Rates.csv"">2020 November General Election - Turnout Rates</a></li>
</ul>
<p>Turnout rates by demographic breakdown, 1986-2018, from the Census Bureau's Current Population Survey, November Voting and Registration Supplement (or CPS for short). These tables are corrected for vote overreporting bias. For uncorrected weights see the source link.</p>
<ul>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=Turnout+Rate+1986-2018+by+Age.csv"">Turnout Rate 1986-2018 by Age</a></li>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=Turnout+Rate+1986-2018+byEducation.csv"">Turnout Rate 1986-2018 by Education</a></li>
<li><a href=""https://data.world/government/vep-turnout/workspace/file?filename=Turnout+Rate+1986-2018+by+Race+and+ethnicity.csv"">Turnout Rate 1986-2018 by Race and Ethnicity</a></li>
</ul>
<p>For more information on these files see the source link below.</p>
<p><em><strong>Source:</strong></em> Data prepared and maintained by Dr. Michael P. McDonald at the University of Florida, at <a href=""http://www.electproject.org/early_2016"" target=""_blank"" rel=""nofollow"">electproject.org</a></p>
<p><em><strong>Updated:</strong></em> synced from source weekly</p>
<p><em><strong>License:</strong></em> <a href=""https://creativecommons.org/licenses/by/3.0/"" target=""_blank"" rel=""nofollow"">CC-BY</a></p>
This dataset was created by Government and contains around 100 samples along with Unnamed: 7, Denominators, technical information and other features such as:
- Unnamed: 4
- Unnamed: 5
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 16 in relation to Unnamed: 14
- Study the influence of Unnamed: 12 on Unnamed: 9
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Government 
Start A New Notebook!"	6	55	3	yamqwe	vep-turnoute
1405	1405	Amtrak Passenger Rail Fuel Consumption and Travel	Energy Consumption By Train - Million Miles Traveled By MC	['business', 'energy', 'transportation', 'rail transport']	"About this dataset
&gt; <p>The dataset gives information on the total energy consumed and distance traveled in National Passenger Railroad Corporation (Amtrak) modes of transportation.</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with Diesel Energy Consumption By Train In Million Gallons, Million Miles Traveled By Train, technical information and other features such as:
- Average Miles Traveled Per Car In Thousands
- Diesel Energy Consumption By Train In Million Gallons
- and more.
How to use this dataset
&gt; - Analyze Million Miles Traveled By Train in relation to Average Miles Traveled Per Car In Thousands
- Study the influence of Diesel Energy Consumption By Train In Million Gallons on Million Miles Traveled By Train
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	29	304	2	yamqwe	amtrak-passenger-rail-fuel-consumption-and-travee
1406	1406	🦫 2017/W45: Life Expectancy at Birth by Country	10000 collected samples, technical information	['health', 'social science']	"About this dataset
&gt; <h1>Makeover Monday Week 45</h1>
<p>The data is sourced from <a href=""https://data.worldbank.org/indicator/SP.DYN.LE00.IN?end=2015&amp;start=1960&amp;view=chart"">The World Bank</a> and allows you to explore life expectancy at birth by year and country for 55 years.</p>
<p><img src=""https://media.data.world/9pzSKznRTRCxYmnJGklw_Life%20Expectancy%20by%20Country.png"" alt=""Life Expectancy by Country.png"" style=""""></p>
<h1>Background</h1>
<p>Via The World Bank -<br>
Derived from male and female life expectancy at birth from sources such as: ( 1 ) United Nations Population Division. World Population Prospects, ( 2 ) Census reports and other statistical publications from national statistical offices, ( 3 ) Eurostat: Demographic Statistics, ( 4 ) United Nations Statistical Division. Population and Vital Statistics Reprot ( various years ), ( 5 ) U.S. Census Bureau: International Database, and ( 6 ) Secretariat of the Pacific Community: Statistics and Demography Programme.</p>
<h1>Get involved</h1>
<p>How can you contribute?</p>
<ul>
<li>What do you like?</li>
<li>What could be improved?</li>
<li>Share your own version via the Discussion section at the top and include a link.</li>
</ul>
This dataset was created by Andy Kriebel and contains around 10000 samples along with Life Expectancy, Income Group, technical information and other features such as:
- Country
- Country Code
- and more.
How to use this dataset
&gt; - Analyze Year in relation to Region
- Study the influence of Life Expectancy on Income Group
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	46	299	1	yamqwe	2017-w45-life-expectancy-at-birth-by-country-196e
1407	1407	🚓 Police Locals	100 collected samples, technical information	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 100 samples along with Non White, Hispanic, technical information and other features such as:
- All
- Black
- and more.
How to use this dataset
&gt; - Analyze Police Force Size in relation to Asian
- Study the influence of Non White on Hispanic
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	40	451	6	yamqwe	police-localse
1408	1408	🧏 2015 New Year's Resolutions	5000 collected samples, technical information	['holidays and cultural events']	"About this dataset
&gt; <p>A Twitter sentiment analysis of users' 2015 New Year's resolutions. Contains demographic and geographical data of users and resolution categorizations. <a href=""http://www.crowdflower.com/blog/new-years-resolutions-2015"" target=""_blank"" rel=""nofollow"">We wrote about it and produced an infographic here</a>.   Added: January 3, 2015 by CrowdFlower | Data Rows: 5011 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 5000 samples along with Resolution Category, Resolution Topics, technical information and other features such as:
- Tweet Region
- User Timezone
- and more.
How to use this dataset
&gt; - Analyze Tweet Coord in relation to Retweet Count
- Study the influence of Tweet Created on Other Topic
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	50	365	5	yamqwe	2015-new-year-s-resolutionse
1409	1409	 Datafiniti's Electronic Products Database	7000 collected samples, technical information	['business', 'internet', 'electronics', 'retail and shopping', 'e-commerce services']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of over 7,000 electronic products with pricing information across 10 unique fields provided by <a href=""https://datafiniti.co/products/product-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Product Database</a>. The dataset also includes the brand, category, merchant, name, source, and more.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do With This Data</h1>
<p>You can use this data to <a href=""https://datafiniti.co/use-case/trov/"" target=""_blank"" rel=""nofollow"">identify retail industry trends in pricing strategies</a>. E.g.:</p>
<ul>
<li>How does the prices.condition affect the pricing strategy of a product?</li>
<li>Is there a correlation between the prices.dateSeen of a product and its dynamic pricing across merchants?</li>
<li>What is the competitive pricing strategy for the same product from different merchants?</li>
<li>What role does a product’s category play in its listing price?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/product-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business, product, and property information. <a href=""https://datafiniti.co/"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Nicholle and contains around 7000 samples along with Date Updated, Ean, technical information and other features such as:
- Primary Categories
- Image Ur Ls
- and more.
How to use this dataset
&gt; - Analyze Weight in relation to Unnamed: 27
- Study the influence of Unnamed: 28 on Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Nicholle 
Start A New Notebook!"	43	309	4	yamqwe	electronic-products-and-pricing-datae
1410	1410	👌 Hate Speech and Offensive Language	20000 collected samples, technical information	['earth and nature', 'social networks']	"About this dataset
&gt; <h1>Automated Hate Speech Detection and the Problem of Offensive Language</h1>
<p>Repository for Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. ""Automated Hate Speech Detection and the Problem of Offensive Language."" ICWSM. You read the paper <a href=""https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15665"" target=""_blank"" rel=""nofollow"">here</a> or our pre-print on <a href=""https://arxiv.org/abs/1703.04009"" target=""_blank"" rel=""nofollow"">Arxiv</a>. You can find more information on our <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language"" target=""_blank"" rel=""nofollow"">Github page</a>.</p>
<p><em><strong>WARNING: The data, lexicons, and notebooks all contain content that is racist, sexist, homophobic, and offensive in many other ways.</strong></em></p>
<p><em><strong>Please cite our paper in any published work.</strong></em></p>
<pre><code>@inproceedings{hateoffensive, 
  title={Automated Hate Speech Detection and the Problem of Offensive Language}, 
  author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, 
  booktitle={Proceedings of the 11th International AAAI Conference on Weblogs and Social Media}, 
  series={ICWSM '17}, 
  year={2017}, 
  location = {Montreal, Canada} 
  }
</code></pre>
<h1>Data guide:</h1>
<p>The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns:</p>
<p><code>count</code> = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).</p>
<p><code>hate_speech</code> = number of CF users who judged the tweet to be hate speech.</p>
<p><code>offensive_language</code> = number of CF users who judged the tweet to be offensive.</p>
<p><code>neither</code> = number of CF users who judged the tweet to be neither offensive nor non-offensive.</p>
<p><code>class</code> = class label for majority of CF users.<br>
0 - hate speech,<br>
1 - offensive  language,<br>
2 - neither</p>
This dataset was created by Tom Davidson and contains around 20000 samples along with Count, Hate Speech, technical information and other features such as:
- Class
- Neither
- and more.
How to use this dataset
&gt; - Analyze Offensive Language in relation to Count
- Study the influence of Hate Speech on Class
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tom Davidson 
Start A New Notebook!"	54	644	3	yamqwe	hate-speech-and-offensive-languagee
1411	1411	💣 Global Terrorism Database (GTD)	Historica Terrorists Attacks, Technical information	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 0 samples along with Ireland, Denmark, technical information and other features such as:
- Greece
- Luxembourg
- and more.
How to use this dataset
&gt; - Analyze Germany in relation to Italy
- Study the influence of France on United Kingdom
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	88	1106	10	yamqwe	terrorisme
1412	1412	🚂 Amtrak Stations	Amtrak intercity railroad passenger terminals in the United States and Canada	['transportation', 'rail transport', 'travel']	"About this dataset
&gt; <p>This dataset represents Amtrak intercity railroad passenger terminals in the United States and Canada. Attribute data include services and passenger amenities provided at the station. This is an updated database of the <strong>Federal Railroad Administration's (FRA) Amtrak Station</strong> database. This data is used for national and network analysis applications.</p>
<p><a href=""https://hifld-dhs-gii.opendata.arcgis.com/datasets/232050afe5744a3eae56e1508fc287f1_0"" target=""_blank"" rel=""nofollow"">SOURCE</a></p>
This dataset was created by Homeland Infrastructure Foundation and contains around 900 samples along with Stntype, City, technical information and other features such as:
- Stfips
- State
- and more.
How to use this dataset
&gt; - Analyze Stnname in relation to Address2
- Study the influence of Zip on Address1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Homeland Infrastructure Foundation 
Start A New Notebook!"	39	388	6	yamqwe	amtrak-stationse
1413	1413	🐏 Beautiful Image of Animals	4000 collected samples, technical information	['arts and entertainment', 'animals', 'image data']	"About this dataset
&gt; <p>Here, contributors were asked to rate image quality (as opposed to how adorable the animals in the images actually are). They were given a five-point scale, from ""unacceptable"" (blurry photos of pets) to ""exceptional"" (hi-res photos that might appear in text books or magazines) and ranked a series of images based on that criteria.</p>
<p>Data set includes a URL for each image, an averaged score (of 1-5) for image quality, and a variance rating accounting for subjective, contributor disagreements.</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">CrowdFlower Data for Everyone</a></p>
This dataset was created by CrowdFlower and contains around 4000 samples along with How Beautiful Is This Picture Gold Reason, How Beautiful Is This Picture Gold, technical information and other features such as:
- Unit State
- How Beautiful Is This Picture:variance
- and more.
How to use this dataset
&gt; - Analyze Trusted Judgments in relation to Pid
- Study the influence of Type on Last Judgment At
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	31	501	4	yamqwe	beautiful-image-animalse
1414	1414	IBRD Balance Sheet 2012	100 collected samples, technical information	['business', 'finance', 'banking', 'economics', 'investing']	"About this dataset
&gt; <p>Provides data from the IBRD Balance Sheet for the fiscal years ended June 30, 2012 and June 30, 2011. The values are expressed in millions of U.S. Dollars. Where applicable, changes have been made to certain line items on the June 30, 2011 balance sheet to conform with the current year's presentation, but the comparable prior years' data sets have not been adjusted to reflect the reclassification impact of those changes.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/uni3-qwsg"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/uni3-qwsg</a></p>
This dataset was created by Finance and contains around 100 samples along with Line Item, Category, technical information and other features such as:
- Amount (us$, Millions)
- Fiscal Year
- and more.
How to use this dataset
&gt; - Analyze Line Item in relation to Category
- Study the influence of Amount (us$, Millions) on Fiscal Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	22	209	5	yamqwe	ibrd-balance-sheet-2012e
1415	1415	🏛️ @POTUS Tweets: Obama White House Social Media	Tweets from President Obama's Twitter account during the Obama Administration	['politics', 'internet', 'email and messaging', 'online communities', 'social networks']	"About this dataset
&gt; <p>Tweets from President Obama's Twitter account during the Obama Administration</p>
<hr>
<p>In the <code>data</code> folder, your Twitter archive is present in two formats: JSON and CSV exports by month and year.</p>
<ul>
<li>CSV is a generic format that can be imported into many data tools, spreadsheet applications, or consumed simply using a programming language.</li>
</ul>
<h2>JSON for Developers</h2>
<ul>
<li>The JSON export contains a full representation of your Tweets as returned by v1.1 of the Twitter API. See <a href=""https://dev.twitter.com/docs/api/1.1"" target=""_blank"" rel=""nofollow"">https://dev.twitter.com/docs/api/1.1</a> for more information.</li>
<li>The JSON export is also used to power the archive browser interface (index.html).</li>
<li>To consume the export in a generic JSON parser in any language, strip the first and last lines of each file.</li>
</ul>
<p>To provide feedback, ask questions, or share ideas with other Twitter developers, join the discussion forums on <a href=""https://dev.twitter.com"" target=""_blank"" rel=""nofollow"">https://dev.twitter.com</a>.</p>
<p>Source: <a href=""https://archive.org/details/ObamaWhiteHouseSocialMediaArchive"" target=""_blank"" rel=""nofollow"">Obama White House Social Media Archive</a></p>
This dataset was created by Social Media Data and contains around 300 samples along with Source, Retweeted Status Timestamp, technical information and other features such as:
- Retweeted Status Id
- Retweeted Status User Id
- and more.
How to use this dataset
&gt; - Analyze In Reply To User Id in relation to Expanded Urls
- Study the influence of In Reply To Status Id on Source
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Social Media Data 
Start A New Notebook!"	8	71	3	yamqwe	obama-white-house-social-media-potus-tweetse
1416	1416	Fannie Mae Mortgage Rates (1985 - 2017)	400 collected samples, technical information	['beginner']	"About this dataset
&gt; <h1>Introduction</h1>
<p>Fannie Mae Mortgage company is one of the more popular mortgage loan companies in America. The rates are fluctuated by the federal reserve rates.</p>
This dataset was created by Chase Willden and contains around 400 samples along with Net Yield, Net Yield, technical information and other features such as:
- Net Yield
- Net Yield
- and more.
How to use this dataset
&gt; - Analyze Net Yield in relation to Net Yield
- Study the influence of Net Yield on Net Yield
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	25	251	5	yamqwe	fannie-mae-mortgage-rates-1985-2017e
1417	1417	🗳 Federal Elections 2016	39 collected samples, technical information	['politics']	"About this dataset
&gt; <p>Source: State Elections Offices</p>
<p>Compiled and Published by:<br>
Federal Election Commission<br>
Public Disclosure and Media Relations Division<br>
Office of Communications<br>
999 E Street, NW<br>
Washington, DC  20463<br>
800/424-9530 (press 2) or 202/694-1120<br>
E-mail: <a href=""mailto:pubrec@fec.gov"" target=""_blank"" rel=""nofollow"">pubrec@fec.gov</a></p>
<p>Source link: <a href=""https://transition.fec.gov/general/FederalElections2016.shtml"" target=""_blank"" rel=""nofollow"">https://transition.fec.gov/general/FederalElections2016.shtml</a></p>
This dataset was created by Government and contains around 0 samples along with Excel Tabs:, Unnamed: 1, technical information and other features such as:
- Excel Tabs:
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze Excel Tabs: in relation to Unnamed: 1
- Study the influence of Excel Tabs: on Unnamed: 1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Government 
Start A New Notebook!"	29	198	7	yamqwe	federal-elections-2016e
1418	1418	😒 Emotions About Nuclear Energy	200 collected samples, technical information	['energy']	"About this dataset
&gt; <p>This dataset is a collection of tweets related to nuclear energy along with the crowd's evaluation of the tweet's sentiment. The possible sentiment categories are: ""Positive,"" ""Negative,"" ""Neutral/author is just sharing information,"" ""Tweet NOT related to nuclear energy,"" and ""I can't tell."" We also provide an estimation of the crowds' confidence that each category is correct which can be used to identify tweets whose sentiment may be unclear.   Added: August 30, 2013 by CrowdFlower | Data Rows: 190 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 200 samples along with Sentiment Confidence Summary, Sentiment, technical information and other features such as:
- Sentiment Confidence Summary
- Sentiment
- and more.
How to use this dataset
&gt; - Analyze Sentiment Confidence Summary in relation to Sentiment
- Study the influence of Sentiment Confidence Summary on Sentiment
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	38	712	6	yamqwe	emotions-about-nuclear-energye
1419	1419	Adverse Reaction with Preferred Term and System	In-depth Dataset for predicting adverse drug reactions	['healthcare', 'public health', 'health', 'drugs and medications']	"About this dataset
&gt; <p>The database includes adverse drug reaction that is associated with the product and substance with MedDRA Preferred Term (PT), PT codes, System Organ Class (SOC) codes etc. This structured database can be used to know whether or not adverse reactions are recorded in the European Summary of Product Characteristics (SPC).</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with Hlgt If No Matching Pt, Age Group, technical information and other features such as:
- Soc If No Matching Pt
- Is Clinical Trial
- and more.
How to use this dataset
&gt; - Analyze Causality in relation to Llt If No Matching Pt
- Study the influence of Is Class Warning on Frequency
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	14	275	5	yamqwe	adverse-reaction-with-preferred-term-and-system-e
1420	1420	✨ Tweets About US Airline	10000 collected samples, technical information	['business', 'text mining', 'classification', 'online communities', 'social networks']	"About this dataset
&gt; <p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as ""late flight"" or ""rude service"").</p>
<p>Source: Kaggle</p>
<p><a href=""https://www.kaggle.com/crowdflower/twitter-airline-sentiment"" target=""_blank"" rel=""nofollow"">https://www.kaggle.com/crowdflower/twitter-airline-sentiment</a></p>
This dataset was created by Data Society and contains around 10000 samples along with Negativereason Confidence, Negativereason, technical information and other features such as:
- Airline Sentiment Gold
- Negativereason Gold
- and more.
How to use this dataset
&gt; - Analyze Airline Sentiment in relation to Name
- Study the influence of User Timezone on Airline
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	38	379	5	yamqwe	twitters-about-us-airlinee
1421	1421	Fin Intermediary Funds Cash	20000 collected samples, technical information	['business', 'finance', 'banking', 'investing']	"About this dataset
&gt; <p>Financial Intermediary Funds (FIFs) are multilateral financing arrangements for which the World Bank provides Trustee services that include committing and transferring funds to project implementers (generally international organizations such as multilateral development banks or UN agencies). In all cases the World Bank as Trustee is required to act in accordance with instructions of independent governing bodies. In fulfilling its responsibilities, the World Bank as Trustee complies with all sanctions applicable to World Bank transactions.This dataset provides data on FIFs cash transfers at the transaction detail level. Funds are channeled in a coordinated manner to a range of recipients in the public and private sectors through a variety of arrangements. FIF trusteeship involves holding, investing and transferring funds as directed by the FIF governing body. Trusteeship does not involve overseeing or supervising the use of funds; this is the role of other agencies that receive the funding and who are responsible for project or program implementation. Transfers are generally made by the Trustee to external agencies (other MDBs, UN agencies, etc.) for the implementation of activities.</p>
<p>Source: <a href=""http://finances.worldbank.org/d/h4s8-nwev"" target=""_blank"" rel=""nofollow"">http://finances.worldbank.org/d/h4s8-nwev</a></p>
This dataset was created by Finance and contains around 20000 samples along with Calendar Year, Sector/ Theme, technical information and other features such as:
- Amount In Usd
- Principal Recipient
- and more.
How to use this dataset
&gt; - Analyze Transfer Quarter in relation to Fund Name
- Study the influence of As Of Date on Calendar Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	22	229	7	yamqwe	fin-intermediary-funds-cashe
1422	1422	📈 Pension Insurance Data	100 collected samples, technical information	['classification', 'clustering']	"About this dataset
&gt; <p>The tables include statistics on the people and pensions that PBGC protects, including how many Americans are in PBGC-insured pension plans, how many get PBGC benefits, and where they live.</p>
<p>Note: Links in the first sheet associated with each table following.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/pension-insurance-data-tables"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/pension-insurance-data-tables</a></p>
This dataset was created by Data Society and contains around 100 samples along with Data Book Listing, Table, technical information and other features such as:
- Data Book Listing
- Table
- and more.
How to use this dataset
&gt; - Analyze Data Book Listing in relation to Table
- Study the influence of Data Book Listing on Table
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	64	970	5	yamqwe	pension-insurance-datae
1423	1423	🔎 Mobile Search Relevance	600 collected samples, technical information	['mobile and wireless', 'nlp']	"About this dataset
&gt; <p>Contributors viewed a variety of searches for mobile apps and determined if the intent of those searches was matched. One was a short query like ""music player""; the other, a much longer one like ""I would like to download an app that plays the music on the phone from multiple sources like Spotify and Pandora and my library.""   Added: May 13, 2015 by CrowdFlower | Data Rows: 647 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 600 samples along with Trusted Judgments, Relevant:variance, technical information and other features such as:
- Golden
- Last Judgment At
- and more.
How to use this dataset
&gt; - Analyze Relevant Gold in relation to Query Description
- Study the influence of Query on Relevant Gold Reason
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	58	527	6	yamqwe	mobile-search-relevancee
1424	1424	🏀 NBA Tattoos	600 collected samples, technical information	['basketball', 'beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please let us know](<a href=""mailto:andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">andrei.scheinkman@fivethirtyeight.com</a>).</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 600 samples along with Tattoos Yes/no, Tattoos Yes/no, technical information and other features such as:
- Tattoos Yes/no
- Tattoos Yes/no
- and more.
How to use this dataset
&gt; - Analyze Tattoos Yes/no in relation to Tattoos Yes/no
- Study the influence of Tattoos Yes/no on Tattoos Yes/no
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	24	347	8	yamqwe	nba-tattoose
1425	1425	😮 Marijuana Laws by State	50 collected samples, technical information	['health', 'law']	"About this dataset
&gt; <p>States have been divided into three Legalization Status categories, based on a map created by <a href=""http://Governing.com"" target=""_blank"" rel=""nofollow"">Governing.com</a>:</p>
<ul>
<li>Medical marijuana legalized</li>
<li>Marijuana legalized for recreational use</li>
<li>No laws legalizing marijuana</li>
</ul>
<blockquote>
<p>Seven states and the District of Columbia have adopted more expansive laws legalizing marijuana for recreational use. Most recently, California, Massachusetts and Nevada all passed measures in November legalizing recreational marijuana. California’s Prop. 64 measure allows adults 21 and older to now possess up to one ounce of marijuana and grow up to six plants in their homes. Other tax and licensing provisions of the law will not take effect until January 2018. In Nevada, adults will be able to legally possess up to one ounce of marijuana beginning Jan. 1. A similar ballot measure in Massachusetts allows for possession of pot starting on Dec. 15.</p>
</blockquote>
<blockquote>
<p>A number of states have also decriminalized the possession of small amounts of marijuana. Other states have passed medical marijuana laws allowing for limited use of cannabis. Some medical marijuana laws are broader than others, with types of medical conditions that allow for treatment varying from state to state. Others states (not shown on the map below) have passed laws allowing residents to possess cannabis oil if they suffer from certain medical illnesses.<br>
<a href=""http://www.governing.com/gov-data/state-marijuana-laws-map-medical-recreational.html"" target=""_blank"" rel=""nofollow"">State Marijuana Laws in 2016 Map</a></p>
</blockquote>
<p><img src=""http://i.imgur.com/lL00Db4.png"" alt=""alttext"" style=""""></p>
<p><em>Some states, such as Virginia, enacted laws decades ago allowing for the possession of marijuana if individuals received prescriptions from doctors. Federal law, however, prohibits doctors from prescribing marijuana, rendering those laws invalid. Doctors can only write a recommendation for medical marijuana, which is different than a prescription.</em></p>
<p>Source: <a href=""http://www.governing.com/gov-data/state-marijuana-laws-map-medical-recreational.html"" target=""_blank"" rel=""nofollow"">Governing Data</a></p>
This dataset was created by Selene Arrazolo and contains around 0 samples along with Medical Marijuana Legalized, Marijuana Legalized For Recreational Use, technical information and other features such as:
- No Laws Legalizing Marijuana
- Medical Marijuana Legalized
- and more.
How to use this dataset
&gt; - Analyze Marijuana Legalized For Recreational Use in relation to No Laws Legalizing Marijuana
- Study the influence of Medical Marijuana Legalized on Marijuana Legalized For Recreational Use
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	47	354	2	yamqwe	marijuana-laws-by-statee
1426	1426	🧂 Austin Restaurant Inspections	30000 collected samples, technical information	['health', 'restaurants']	"About this dataset
&gt; <p>Provides restaurant scores for inspections performed within the last three years. Online search of this data set also available at: <a href=""http://www.ci.austin.tx.us/health/restaurant/search.cfm"" target=""_blank"" rel=""nofollow"">http://www.ci.austin.tx.us/health/restaurant/search.cfm</a></p>
<p>CSV - <a href=""https://data.austintexas.gov/api/views/ecmv-9xxi/rows.csv?accessType=DOWNLOAD"" target=""_blank"" rel=""nofollow"">https://data.austintexas.gov/api/views/ecmv-9xxi/rows.csv?accessType=DOWNLOAD</a></p>
<p>JSON - <a href=""https://data.austintexas.gov/api/views/ecmv-9xxi/rows.json?accessType=DOWNLOAD"" target=""_blank"" rel=""nofollow"">https://data.austintexas.gov/api/views/ecmv-9xxi/rows.json?accessType=DOWNLOAD</a></p>
<p>XML - <a href=""https://data.austintexas.gov/api/views/ecmv-9xxi/rows.xml?accessType=DOWNLOAD"" target=""_blank"" rel=""nofollow"">https://data.austintexas.gov/api/views/ecmv-9xxi/rows.xml?accessType=DOWNLOAD</a></p>
This dataset was created by Adam Helsinger and contains around 30000 samples along with Restaurant Name, Inspection Date, technical information and other features such as:
- Zip Code
- Score
- and more.
How to use this dataset
&gt; - Analyze Address in relation to Facility Id
- Study the influence of Process Description on Restaurant Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	31	286	6	yamqwe	austin-restaurant-inspectionse
1427	1427	Brazil Largest Companies	A dataset of the world's 2000 largest publicly listed corporations	['business', 'finance', 'economics', 'investing']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p>​Source: <a href=""%E2%80%8Bhttp://www.economywatch.com/companies/forbes-list/"" target=""_blank"" rel=""nofollow"">Economy Watch</a></p>
This dataset was created by Finance and contains around 0 samples along with Assets ($billion), Sales ($billion), technical information and other features such as:
- Profits ($billion)
- Market Value ($billion)
- and more.
How to use this dataset
&gt; - Analyze Assets ($billion) in relation to Sales ($billion)
- Study the influence of Profits ($billion) on Market Value ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	48	284	6	yamqwe	brazil-largest-companiese
1428	1428	🧑 Gender Classifier Data	Predicting Profile of a male or woman	['earth and nature', 'gender', 'online communities', 'social networks']	"About this dataset
&gt; <p>This data set was used to train a CrowdFlower AI gender predictor. You can read all about the project here. Contributors were asked to simply view a Twitter profile and judge whether the user was a male, a female, or a brand (non-individual). The dataset contains 20,000 rows, each with a user name, a random tweet, account profile and image, location, and even link and sidebar color. More about this project <a href=""https://www.crowdflower.com/using-machine-learning-to-predict-gender/"" target=""_blank"" rel=""nofollow"">here</a>   Added: November 15, 2015 by CrowdFlower | Data Rows: 20000 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 20000 samples along with Gender Gold, Unit State, technical information and other features such as:
- Gender:confidence
- Last Judgment At
- and more.
How to use this dataset
&gt; - Analyze Name in relation to Golden
- Study the influence of Profile Yn Gold on Sidebar Color
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	99	889	4	yamqwe	gender-classifier-datae
1429	1429	🌏 Asia-Pacific Economic Outlook	400 collected samples, technical information	['asia', 'clustering', 'regression']	"About this dataset
&gt; <p>Dataset included and related document for background knowledge.</p>
<p>Temporal Coverage: Annual data starting in year of publication - 3 and projections up to the year of publication + 5.</p>
<p>Source: IMF</p>
<p><a href=""http://data.imf.org/?sk=ABFF6C02-73A8-475C-89CC-AD515033E662"" target=""_blank"" rel=""nofollow"">http://data.imf.org/?sk=ABFF6C02-73A8-475C-89CC-AD515033E662</a></p>
This dataset was created by Data Society and contains around 400 samples along with 2012, 2020, technical information and other features such as:
- Indicator Code
- Unnamed: 16
- and more.
How to use this dataset
&gt; - Analyze Base Year in relation to 2017
- Study the influence of 2016 on Country Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	44	334	10	yamqwe	asia-pacific-economic-outlooke
1430	1430	2012 Annual Survey of Manufactures	A dataset containing around 2000 samples from the U.S. Census Bureau	['social science', 'manufacturing']	"About this dataset
&gt; <p>Source: <a href=""https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ASM_2015_31GS101&amp;prodType=table"" target=""_blank"" rel=""nofollow"">https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ASM_2015_31GS101&amp;prodType=table</a></p>
 This dataset was created by U.S. Census Bureau and contains around 2000 samples along with Invtotb, Invfinb, technical information and other features such as:
- Rcptot
- Invtotb S
- and more.
How to use this dataset
&gt; - Analyze Naics.id in relation to Geo.display Label
- Study the influence of Invwipb on Invmate S
- More datasets
Acknowledgements
If you use this dataset in your research, please credit U.S. Census Bureau 
Start A New Notebook!"	33	341	3	yamqwe	2012-annual-survey-of-manufacturese
1431	1431	🏴 California SAT Report 2011-2012	SAT Reports from the California Department of Education (CDE)	['universities and colleges', 'education', 'primary and secondary schools', 'standardized testing']	"About this dataset
&gt; <p>Data were aggregated to produce school, district, county, and state-level reports.</p>
<h1>Background</h1>
<p>This document is meant to accompany the SAT Report. The California Department of Education (CDE) receives SAT data at the student level from the College Board. The CDE then aggregates the data by school, district, county, and state in order to produce the SAT Report. The SAT Report includes only California public schools with one or more students enrolled in grade twelve according to data submitted by local educational agencies (LEAs) and maintained in the California Basic Educational Data System (CBEDS). Scores for schools that had fewer than eleven students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</p>
<p><a href=""http://www.cde.ca.gov/ds/sp/ai/glossarysat2012.asp"" target=""_blank"" rel=""nofollow"">Link to Glossary</a></p>
<h1>What Is the SAT?</h1>
<p>Description of the SAT test program developed by the College Board.The SAT Reasoning Test is a standardized test that assesses the critical reading, mathematics, and writing skills that students need to be successful in college. Each of the three sections that comprise the SAT Reasoning Test has a possible score of 800 points. Prior to 2005, the SAT test included only two sections, the verbal section (now referred to as the critical reading section) and the math section, each having possible scores of 800 points. SAT test results represent one factor considered by many colleges and universities in making admissions decisions.</p>
<p>The SAT is owned, published, and developed by the College Board, a non-profit organization in the United States. The California Department of Education does not have results that identify individual students. Please direct questions about individual scores, including requests for transcripts, to the <a href=""https://www.collegeboard.org/"" target=""_blank"" rel=""nofollow"">College Board</a>.</p>
<p>Questions: SAT, ACT, AP Test Results Team | <a href=""mailto:SATACTAP@cde.ca.gov"" target=""_blank"" rel=""nofollow"">SATACTAP@cde.ca.gov</a> | 916-319-0869</p>
<p>Last Reviewed: Thursday, October 20, 2016</p>
<h1>Record Layout for SAT Test Results 2015‰ÛÒ16</h1>
<p>Provides field definitions for standardized test results from SAT Test.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Field #</th>
<th>Field Name</th>
<th>Type</th>
<th>Width</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>CDS</td>
<td>Character</td>
<td>14</td>
<td>County/District/School code</td>
</tr>
<tr>
<td>2</td>
<td>RTYPE</td>
<td>Character</td>
<td>1</td>
<td>Record Type: C=County, D=District, S=School, X=State</td>
</tr>
<tr>
<td>3</td>
<td>SNAME</td>
<td>Character</td>
<td>50</td>
<td>School Name</td>
</tr>
<tr>
<td>4</td>
<td>DNAME</td>
<td>Character</td>
<td>50</td>
<td>District Name</td>
</tr>
<tr>
<td>5</td>
<td>CNAME</td>
<td>Character</td>
<td>15</td>
<td>County Name</td>
</tr>
<tr>
<td>6</td>
<td>ENROLL12</td>
<td>Character</td>
<td>7</td>
<td>Enrollment of Grade 12</td>
</tr>
<tr>
<td>7</td>
<td>NUMTSTTAKR</td>
<td>Character</td>
<td>7</td>
<td>Number of Test Takers</td>
</tr>
<tr>
<td>8</td>
<td>AVGSCRREAD</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Critical Reading, * = Scores for schools that had fewer than 15 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>9</td>
<td>AVGSCRMATH</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Math, * = Scores for schools that had fewer than 15 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>10</td>
<td>AVGSCRWRITE</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Writing, * = Scores for schools that had fewer than 15 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>11</td>
<td>NUMGE1500</td>
<td>Character</td>
<td>6</td>
<td>Number of Test Takers Whose Total SAT Scores Are Greater or Equal to 1500, * = Scores for schools that had fewer than 15 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>12</td>
<td>PCTGE1500</td>
<td>Character</td>
<td>5</td>
<td>Percent of Test Takers Whose Total SAT Scores Are Greater or Equal to 1500, * = Scores for schools that had fewer than 15 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
</tbody>
</table></div>
<p>Questions: SAT, ACT, AP Test Results Team | <a href=""mailto:SATACTAP@cde.ca.gov"" target=""_blank"" rel=""nofollow"">SATACTAP@cde.ca.gov</a> | 916-319-0869</p>
<p>Last Reviewed: Friday, June 16, 2017</p>
<h1>Record Layout for SAT Test Results for 2013‰ÛÒ14 and 2014‰ÛÒ15</h1>
<p>Provides field definitions for standardized test results from SAT Test.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Field #</th>
<th>Field Name</th>
<th>Type</th>
<th>Width</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>CDS</td>
<td>Character</td>
<td>14</td>
<td>County/District/School code</td>
</tr>
<tr>
<td>2</td>
<td>RTYPE</td>
<td>Character</td>
<td>1</td>
<td>Record Type: C=County, D=District, S=School, X=State</td>
</tr>
<tr>
<td>3</td>
<td>SNAME</td>
<td>Character</td>
<td>50</td>
<td>School Name</td>
</tr>
<tr>
<td>4</td>
<td>DNAME</td>
<td>Character</td>
<td>50</td>
<td>District Name</td>
</tr>
<tr>
<td>5</td>
<td>CNAME</td>
<td>Character</td>
<td>15</td>
<td>County Name</td>
</tr>
<tr>
<td>6</td>
<td>ENROLL12</td>
<td>Character</td>
<td>7</td>
<td>Enrollment of Grade 12</td>
</tr>
<tr>
<td>7</td>
<td>NUMTSTTAKR</td>
<td>Character</td>
<td>7</td>
<td>Number of Test Takers</td>
</tr>
<tr>
<td>8</td>
<td>AVGSCRREAD</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Critical Reading, * = Scores for schools that had fewer than 11 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>9</td>
<td>AVGSCRMATH</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Math, * = Scores for schools that had fewer than 11 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>10</td>
<td>AVGSCRWRITE</td>
<td>Character</td>
<td>3</td>
<td>Average of SAT Score for Writing, * = Scores for schools that had fewer than 11 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>11</td>
<td>NUMGE1500</td>
<td>Character</td>
<td>6</td>
<td>Number of Test Takers Whose Total SAT Scores Are Greater or Equal to 1500, * = Scores for schools that had fewer than 11 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
<tr>
<td>12</td>
<td>PCTGE1500</td>
<td>Character</td>
<td>5</td>
<td>Percent of Test Takers Whose Total SAT Scores Are Greater or Equal to 1500, * = Scores for schools that had fewer than 11 students taking the SAT are not shown on the SAT Report in order to preserve the anonymity of the students.</td>
</tr>
</tbody>
</table></div>
<p>Questions: SAT, ACT, AP Test Results Team | <a href=""mailto:SATACTAP@cde.ca.gov"" target=""_blank"" rel=""nofollow"">SATACTAP@cde.ca.gov</a> | 916-319-0869</p>
<p>Last Reviewed: Tuesday, March 14, 2017</p>
<p><strong><em>Source:</em></strong> <a href=""http://www.cde.ca.gov/ds/sp/ai/"" target=""_blank"" rel=""nofollow"">http://www.cde.ca.gov/ds/sp/ai/</a></p>
This dataset was created by Education and contains around 2000 samples along with Unnamed: 12, Unnamed: 1, technical information and other features such as:
- Unnamed: 11
- Unnamed: 8
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 14 in relation to Unnamed: 13
- Study the influence of Unnamed: 9 on California Department Of Education Analysis, Measurement, And Accountability Reporting Division
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Education 
Start A New Notebook!"	56	523	3	yamqwe	california-sat-report-2011-2012e
1432	1432	Legislative data on e-cigarette use	State-level legislative data on tobacco use prevention and control policies	['public health', 'health']	"About this dataset
&gt; <p>1995-2016. Centers for Disease Control and Prevention (CDC). State Tobacco Activities Tracking and Evaluation (STATE) System. E-Cigarette LegislationâLicensure. The STATE System houses current and historical state-level legislative data on tobacco use prevention and control policies. Data are reported on a quarterly basis. Data include information related to requirements, restrictions and penalties associated with holding a retail license to sell e-cigarettes over-the-counter and through vending machines.</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/cdc-state-system-e-cigarette-legislation-licensure-8f90c"" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/cdc-state-system-e-cigarette-legislation-licensure-8f90c</a></p>
This dataset was created by Health and contains around 70000 samples along with Location Abbr, Year, technical information and other features such as:
- Provision Alt Value
- Topic Id
- and more.
How to use this dataset
&gt; - Analyze Measure Desc in relation to Display Order
- Study the influence of Provision Desc on Provision Group Desc
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	19	190	4	yamqwe	cdc-state-e-cigarette-lege
1433	1433	💳 Failed Banks	This list includes banks which have failed since October 1, 2000.	['business', 'finance', 'banking']	"About this dataset
&gt; <p>List of Failed Banks from <a href=""https://www.fdic.gov/bank/individual/failed/banklist.html"" target=""_blank"" rel=""nofollow"">https://www.fdic.gov/bank/individual/failed/banklist.html</a>  , as of 8/12/16</p>
 This dataset was created by Drew Dodson and contains around 500 samples along with Closing Date, Updated Date, technical information and other features such as:
- City
- Acquiring Institution
- and more.
How to use this dataset
&gt; - Analyze Bank Name in relation to St
- Study the influence of Closing Date on Updated Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Drew Dodson 
Start A New Notebook!"	37	440	4	yamqwe	failedbankdatae
1434	1434	🏦 US Retail Sales Per Capita by Store Type	Global Growth Rate (CGR) of per capita spending from 2000 to 2015	['business', 'economics']	"About this dataset
&gt; <p>I have added a column on the  right that shows the compound annual growth rate (CGR) of per capita spending from 2000 to 2015.</p>
<p>source:</p>
This dataset was created by Gary Hoover and contains around 0 samples along with Unnamed: 15, Unnamed: 9, technical information and other features such as:
- Unnamed: 18
- Unnamed: 12
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 4 in relation to Unnamed: 10
- Study the influence of Unnamed: 14 on Unnamed: 1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	36	283	4	yamqwe	us-retail-sales-per-capita-by-store-type-2000-20e
1435	1435	🇺🇸 US Population: 2060 Projection by age	Dataset that contains information on cropped and high grade epochs	['united states', 'social science']	"About this dataset
&gt; <p>The US is headed toward a shortage of people of working age, and we need more immigrants or more babies.  Our rapidly growing senior population will put more stress on the federal budget and the overall economy, without a sufficient share of people of working age.</p>
<p>Source: Table 9 at: <a href=""http://www.census.gov/population/projections/data/national/2014/summarytables.html"" target=""_blank"">http://www.census.gov/population/projections/data/national/2014/summarytables.html</a></p>

Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	35	200	5	yamqwe	us-population-2060-by-agee
1436	1436	Eryhemato-Squamous Disease type Classification	Discovering the type of Eryhemato-Squamous Disease	['health', 'classification', 'health conditions']	"About this dataset
&gt; <p>Aim for this dataset is to determine the type of Eryhemato-Squamous Disease.# Source:<br>
Original Owners:</p>
<ol>
<li>Nilsel Ilter, M.D., Ph.D., Gazi University, School of Medicine06510 Ankara, TurkeyPhone: +90 (312) 214 1080</li>
<li>H. Altay Guvenir, PhD., Bilkent University,Department of Computer Engineering and Information Science,06533 Ankara, TurkeyPhone: +90 (312) 266 4133Email:<br>
guvenir '@' <a href=""http://cs.bilkent.edu.tr"" target=""_blank"" rel=""nofollow"">cs.bilkent.edu.tr</a><br>
Donor:<br>
H. Altay Guvenir,Bilkent University,Department of Computer Engineering and Information Science,06533 Ankara, TurkeyPhone: +90 (312) 266 4133Email:<br>
guvenir '@' <a href=""http://cs.bilkent.edu.tr"" target=""_blank"" rel=""nofollow"">cs.bilkent.edu.tr</a></li>
</ol>
<h1>Data Set Information:</h1>
<p>This database contains 34 attributes, 33 of which are linear valued and one of them is nominal.<br>
The differential diagnosis of erythemato-squamous diseases is a real problem in dermatology. They all share the clinical features of erythema and scaling, with very little differences. The diseases in this group are psoriasis, seboreic dermatitis, lichen planus, pityriasis rosea, cronic dermatitis, and pityriasis rubra pilaris. Usually a biopsy is necessary for the diagnosis but unfortunately these diseases share many histopathological features as well. Another difficulty for the differential diagnosis is that a disease may show the features of another disease at the beginning stage and may have the characteristic features at the following stages. Patients were first evaluated clinically with 12 features. Afterwards, skin samples were taken for the evaluation of 22 histopathological features. The values of the histopathological features are determined by an analysis of the samples under a microscope.<br>
In the dataset constructed for this domain, the family history feature has the value 1 if any of these diseases has been observed in the family, and 0 otherwise. The age feature simply represents the age of the patient. Every other feature (clinical and histopathological) was given a degree in the range of 0 to 3. Here, 0 indicates that the feature was not present, 3 indicates the largest amount possible, and 1, 2 indicate the relative intermediate values.<br>
The names and id numbers of the patients were recently removed from the database.</p>
<h1>Attribute Information:</h1>
<p>Clinical Attributes: (take values 0, 1, 2, 3, unless otherwise indicated)   1: erythema   2: scaling   3: definite borders   4: itching   5: koebner phenomenon   6: polygonal papules   7: follicular papules   8: oral mucosal involvement   9: knee and elbow involvement   10: scalp involvement   11: family history, (0 or 1)   34: Age (linear)<br>
Histopathological Attributes: (take values 0, 1, 2, 3)   12: melanin incontinence   13: eosinophils in the infiltrate   14: PNL infiltrate   15: fibrosis of the papillary dermis   16: exocytosis   17: acanthosis   18: hyperkeratosis   19: parakeratosis   20: clubbing of the rete ridges   21: elongation of the rete ridges   22: thinning of the suprapapillary epidermis   23: spongiform pustule   24: munro microabcess   25: focal hypergranulosis   26: disappearance of the granular layer   27: vacuolisation and damage of basal layer   28: spongiosis   29: saw-tooth appearance of retes   30: follicular horn plug   31: perifollicular parakeratosis   32: inflammatory monoluclear inflitrate   33: band-like infiltrate</p>
<h1>Relevant Papers:</h1>
<p>G. Demiroz, H. A. Govenir, and N. Ilter, ""Learning Differential Diagnosis of Eryhemato-Squamous Diseases using Voting Feature Intervals"", Aritificial Intelligence in Medicine</p>
<h1>Papers That Cite This Data Set1:</h1>
<p>Vassilis Athitsos and Stan Sclaroff. Boosting Nearest Neighbor Classifiers for Multiclass Recognition. Boston University Computer Science Tech. Report No, 2004-006. 2004.</p>
<ul>
<li>Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner. Attribute Selection with a Multi-objective Genetic Algorithm. SBIA. 2002.</li>
<li>Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas. An Ant Colony Based System for Data Mining: Applications to Medical Data. CEFET-PR, CPGEI Av. Sete de Setembro, 3165.</li>
<li>Gisele L. Pappa and Alex Alves Freitas and Celso A A Kaestner. AMultiobjective Genetic Algorithm for Attribute Selection. Computing Laboratory Pontificia Universidade Catolica do Parana University of Kent at Canterbury.</li>
<li>Perry Moerland. Mixtures of latent variable models for density estimation and classification. E S E A R C H R E P R O R T I D I A P D a l l e M o l l e I n s t i t u t e f o r Pe r cep t ua l A r t i f i c i a l Intelligence .</li>
<li>H. Altay Guvenir. A Classification Learning Algorithm Robust to Irrelevant Features. Bilkent University, Department of Computer Engineering and Information Science.</li>
<li>M. V. Fidelis and Heitor S. Lopes and Alex Alves Freitas. Discovering Comprehensible Classification Rules with a Genetic Algorithm. UEPG, CPD CEFET-PR, CPGEI PUC-PR, PPGIA Praa Santos Andrade, s/n Av. Sete de Setembro.</li>
<li>Rafael S. Parpinelli and Heitor S. Lopes and Alex Alves Freitas. PART FOUR: ANT COLONY OPTIMIZATION AND IMMUNE SYSTEMS Chapter X An Ant Colony Algorithm for Classification Rule Discovery. CEFET-PR, Curitiba.</li>
</ul>
<h1>Citation Request:</h1>
<p>Please refer to the Machine Learning <a href=""http://archive.ics.uci.edu/ml/citation_policy.html"" target=""_blank"" rel=""nofollow"">Repository's citation policy.</a><br>
[1] Papers were automatically harvested and associated with this data set, in collaborationwith <a href=""http://rexa.info/"" target=""_blank"" rel=""nofollow"">Rexa.info</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Dermatology"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Dermatology</a></p>
This dataset was created by UCI and contains around 400 samples along with 0.1, 0.6, technical information and other features such as:
- 1.1
- 0.8
- and more.
How to use this dataset
&gt; - Analyze 0.2 in relation to 0.10
- Study the influence of 0.23 on 0.16
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	20	273	3	yamqwe	dermatologye
1437	1437	China Largest Companies	100 collected samples, technical information	['business']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p>​Source: <a href=""%E2%80%8Bhttp://www.economywatch.com/companies/forbes-list/"" target=""_blank"" rel=""nofollow"">Economy Watch</a></p>
This dataset was created by Finance and contains around 100 samples along with Profits ($billion), Market Value ($billion), technical information and other features such as:
- Sales ($billion)
- Assets ($billion)
- and more.
How to use this dataset
&gt; - Analyze Global Rank in relation to Profits ($billion)
- Study the influence of Market Value ($billion) on Sales ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	113	714	6	yamqwe	china-largest-companiese
1438	1438	Vehicle Speed Data - NREL 	Vehicle Speed Data collected from 486 vehicles over a span of 4705 days.	['business', 'automobiles and vehicles']	"About this dataset
&gt; <p>This data represents fleet DNA data collected between 07/2008 to 07/2014 by NREL. Commercial fleet vehicle operating data including vehicle speed distribution, starts, stops, braking, idling time, kinetic intensity, etc., are collected from 486 vehicles over a span of 4705 days.</p>
<p>Source: <a href=""https://www.nrel.gov/transportation/fleettest-fleet-dna.html#"" target=""_blank"" rel=""nofollow"">https://www.nrel.gov/transportation/fleettest-fleet-dna.html#</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2017-01-15</p>
This dataset was created by Kelly Garrett and contains around 1000 samples along with Spd Cat 8 Ttl, Pid, technical information and other features such as:
- Total Median Speed
- Twenty Five Thirty Seconds
- and more.
How to use this dataset
&gt; - Analyze Percent Thirty Five Fourty in relation to Acceleration Event Duration 25th Percentile
- Study the influence of Standard Deivation Rolling Power Density Regen on Mt Count
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	25	223	3	yamqwe	nrel-fleet-dna-project-truck-platooning-data-come
1439	1439	🐄 Walmart Product Listings 2020	A dataset of the product listings of Walmart	['business', 'retail and shopping']	"About this dataset
&gt; <h3>Context</h3>
<p>This dataset was created by our in house teams at PromptCloud(<a href=""https://www.promptcloud.com/"" target=""_blank"" rel=""nofollow"">https://www.promptcloud.com/</a>) and DataStock(<a href=""https://datastock.shop/"" target=""_blank"" rel=""nofollow"">https://datastock.shop/</a>). This is a sample dataset that contains 30K records in it. You can download the full dataset here(<a href=""https://app.datastock.shop/?site_name=Walmart_Products_Data_Listing_2020"" target=""_blank"" rel=""nofollow"">https://app.datastock.shop/?site_name=Walmart_Products_Data_Listing_2020</a>).</p>
<h3>Content</h3>
<p>This dataset contains the following:</p>
<p>Total Records Count : 408504  Domain Name : <a href=""http://walmart.com"" target=""_blank"" rel=""nofollow"">walmart.com</a>  Date Range : 01st Jan 2020 - 31st Mar 2020   File Extension : csv</p>
<p>Available Fields: Uniq Id, Crawl Timestamp, Pageurl, Website, Title, Rating, Review, Reviewer Name, Review Upvotes, Review Downvotes, Verified Purchaser, Recommended Purchase, Review Date, Five Star, Four Star, Three Star, Two Star, One Star</p>
<h3>Acknowledgements</h3>
<p>We wouldn't be here without the help of our in house web scraping teams at PromptCloud and DataStock. Without them, this would have never been possible.</p>
This dataset was created by PromptCloud and contains around 30000 samples along with Four Star, Pageurl, technical information and other features such as:
- Crawl Timestamp
- Rating
- and more.
How to use this dataset
&gt; - Analyze Review Date in relation to Reviewer Name
- Study the influence of Two Star on Review
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	76	539	2	yamqwe	walmart-product-listing-data-2020e
1440	1440	🇮🇳 Country Partnership Strategy India	technical and technical information for relevant IBRD and IDA projects	['business']	"About this dataset
&gt; <p>An aggregation of result indictors that measure progress against the project development objectives (PDOs) for relevant IBRD and IDA funded projects which are tracked under the World Bank Group Country Partnership Strategy for India for the period 2013-2017. A project development objective conveys what the project intends to achieve and these accompanying results indicators are used to monitor implementation progress.</p>
<p>Source: <a href=""http://data.worldbank.org/data-catalog/india-cps-project-result-indicators"">http://data.worldbank.org/data-catalog/india-cps-project-result-indicators</a></p>
This dataset was created by Finance and contains around 700 samples along with Ind Custom Code, Final Target Value Comments, technical information and other features such as:
- Most Rec Act Value Comments
- Baseline Date
- and more.
How to use this dataset
&gt; - Analyze Unit Of Measure in relation to Final Target Date
- Study the influence of Indicator Level on Indicator Name
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	13	112	4	yamqwe	country-prtnrshp-strtgy-indiae
1441	1441	🐉 List of Medieval Battles	600 collected samples, technical information	['military']	"About this dataset
&gt; <p>The CSV file is similar to the XLS file, but has the battle place column filled. The battle place was extracted from the battle name, and assumes that if a battle was named 'The Battle of Turin,' it was fought in Turin.</p>
 This dataset was created by Adam Helsinger and contains around 600 samples along with Year, Battle Name, technical information and other features such as:
- Battle Name.1
- Battle Description
- and more.
How to use this dataset
&gt; - Analyze Battle Place in relation to Year
- Study the influence of Battle Name on Battle Name.1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	47	469	10	yamqwe	list-of-medieval-battlese
1442	1442	⚓ Lowe's stores location data	Location Data for all Lowe's Stores in the US and Canada	['business', 'retail and shopping']	"About this dataset
&gt; <h3>Lowes Store Location Data</h3>
<p>Lowes an American retail company specializing in home improvement. Headquartered in Mooresville, North Carolina, the company operates a chain of retail stores in the United States and Canada. This is a complete list of all Lowe's store locations, along with their geographic coordinates, Street addresses, City, State, ZIP code etc.</p>
<h3><strong>Get data for free</strong></h3>
<p>Contact Datahut (<a href=""https://datahut.co/"" target=""_blank"">https://datahut.co/</a>) for more information and a fresh data set. We give this data for free for startups, journalists bloggers, researchers, analysts, etc.</p>
<p>​</p>
This dataset was created by Tony Paul and contains around 2000 samples along with Phone Number 1, Email 2, technical information and other features such as:
- City
- Facebook
- and more.
How to use this dataset
&gt; - Analyze Email 1 in relation to Website
- Study the influence of State on Pinterest
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tony Paul 
Start A New Notebook!"	49	367	4	yamqwe	lowe-s-stores-location-datae
1443	1443	time_group		[]		0	3	0	zhouyiheng	time-group
1444	1444	motocicletas_empinando		[]		0	3	0	leonardommarques	motocicletas-empinando
1445	1445	nytimes		['politics']		0	2	0	enkrish259	nytimes
1446	1446	Covid-19 Weekly Trends In Europe - Latest Data	Weekly trends of Covid-19 as on February 08, 2022	['news']	"Content
This dataset contains data of weekly trend of Covid-19 in Europe (February 01 - February 08, 2022)
Attribute Information
Country/Other
Cases in the last 7 days
Cases in the preceding 7 days
Weekly Case % Change
Cases in the last 7 days/1M pop
Deaths in the last 7 days
Deaths in the preceding 7 days
Weekly Death % Change 
Deaths in the last 7 days/1M pop
Population
Source
Link : https://www.worldometers.info/coronavirus/weekly-trends/#weekly_table
Other Updated Covid Datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍
Thank You"	385	2466	32	anandhuh	covid19-weekly-trends-in-europe-latest-data
1447	1447	ZEW Data Purchasing Challenge 2022	https://www.aicrowd.com/challenges/data-purchasing-challenge-2022	['business', 'computer science', 'advanced', 'computer vision', 'classification', 'deep learning', 'image data']	"Dataset Source: https://www.aicrowd.com/challenges/data-purchasing-challenge-2022
🕵️ Introduction
Data for machine learning tasks usually does not come for free but has to be purchased. The costs and benefits of data have to be weighed against each other. This is challenging. First, data usually has combinatorial value. For instance, different observations might complement or substitute each other for a given machine learning task. In such cases, the decision to purchase one group of observations has to be made conditional on the decision to purchase another group of observations. If these relationships are high-dimensional, finding the optimal bundle becomes computationally hard. Second, data comes at different quality, for instance, with different levels of noise. Third, data has to be acquired under the assumption of being valuable out-of-sample. Distribution shifts have to be anticipated.
In this competition, you face these data purchasing challenges in the context of an multi-label image classification task in a quality control setting.
📑 Problem Statement
In short: You have to classify images. Some images in your training set are labelled but most of them aren't. How do you decide which images to label if you have a limited budget to do so?
In more detail: You face a multi-label image classification task. The dataset consists of synthetically generated images of painted metal sheets. A classifier is meant to predict whether the sheets have production damages and if so which ones. You have access to a set of images, a subset of which are labelled with respect to production damages. Because labeling is costly and your budget is limited, you have to decide for which of the unlabelled images labels should be purchased in order to maximize prediction accuracy.
Each of the images have a 4 dimensional label representing the presence or the absence of ['scratch_small', 'scratch_large', 'dent_small', 'dent_large'] in the images.
You are required to submit code, which can be run in three different phases:
Pre-Training Phase
In the Pre-Training Phase, your code will have access to 5,000 labelled images on a multi-label image classification task with 4 classes.
It is up to you, how you wish to use this data. For instance, you might want to pre-train a classification model.
Purchase Phase
In the Purchase Phase, your code, after going through the Pre-Training Phase will have access to an unlabelled dataset of 10,000 images.
You will have a budget of 3,000 label purchases, that you can freely use across any of the images in the unlabelled dataset to obtain their labels.
You are tasked with designing your own approach on how to select the optimal subset of 3,000 images in the unlabelled dataset, which would help you optimize your model's performance on the prediction task.
You can then continue training your model (which has been pre-trained in the pre-training phase) using the newly purchased labels.
Prediction Phase
In the Prediction Phase, your code will have access to a test set of 3,000 unlabelled images, for which you have to generate and submit predictions.
Your submission will be evaluated based on the performance of your predictions on this test set.
Your code will have access to a node with 4 CPUS, 16 GB RAM, 1 NVIDIA T4 GPU and 3 hours of runtime per submission. In the final round of this challenge, your code will be evaluated across multiple budget-runtime constraints.
💾 Dataset
The datasets for this challenge can be accessed in the Resources Section.
training.tar.gz: The training set containing 5,000 images with their associated labels. During your local experiments you are allowed to use the data as you please.
unlabelled.tar.gz: The unlabelled set containing 10,000 images, and their associated labels. During your local experiments you are only allowed to access the labels through the provided purchase_label function.
validation.tar.gz: The validation set containing 3,000 images, and their associated labels. During your local experiments you are only allowed to use the labels of the validation set to measure the performance of your models and experiments.
debug.tar.gz.: A small set of 100 images with their associated labels, that you can use for integration testing, and for trying out the provided starter kit.
NOTE While you run your local experiments on this dataset, your submissions will be evaluated on a dataset which might be sampled from a different distribution, and is not the same as this publicly released version.
👥 Participation
🖊 Evaluation Criteria
The challenge will use the Accuracy Score, Hamming Loss and the Exact Match Ratio during evaluation. The primary score will be the Accuracy Score.
📅 Timeline
This challenge has two Rounds.
Round 1 : Feb 4th – Feb 28th, 2022
The first round submissions will be evaluated based on one budget-compute constraint pair (max. of 3,000 images to be labelled and 3 hours runtime).
Labelled Dataset : 5,000 images
Unlabelled Dataset : 10,000 images
Labelling Budget : 3,000 images
Test Set : 3,000 images
GPU Runtime : 3 hours
Round 2 : March 1st – April 5th, 2022
The second round submissions will be evaluated based on multiple budget-compute constraint pairs (different numbers of images to be labelled and different runtime limits).
Details to be announced.
🏆 Prizes
This challenge has both Leaderboard Prizes and Community Contribution Prizes.
LEADERBOARD PRIZES
These prizes will be given away to the top performing teams/participants of the second round of this challenge.
1st Place : USD 6,000
2nd Place : USD 4,500
3rd Place : USD 3,000
The Community Contribution Prizes will be awarded based on the discretion of the organizers, and the popularity of the posts (or activity) in the community (based on the number of likes ❤️) - so share your post widely to spread the word!
The prizes typically go to individuals or teams who are extremely active in the community, share resources - or even answer questions - that benefit the whole community greatly!
You can make multiple submissions, but you are only eligible for the Community Contribution Prize once. In case of resources that are created, your work needs to be published under a license of your choice, and on a platform that allows other participants to access and use it.
Notebooks, Blog Posts, Tutorials, Screencasts, Youtube Videos, or even your active responses on the challenge forums - everything is eligible for the Community Contribution Prizes. We are looking forward to see everything you create!
🔗 Links
💪 Challenge Page: https://www.aicrowd.com/challenges/data-purchasing-challenge-2022
🗣️ Discussion Forum: https://discourse.aicrowd.com/c/data-purchasing-challenge-2022/2136
🏆 Leaderboard: https://www.aicrowd.com/challenges/data-purchasing-challenge-2022/leaderboards
📱 Contact
Dominik Rehse
Sebastian Valet
Johannes Walter
Sharada Mohanty
🤝 Organizers
ZEW - Leibniz Centre for European Economic Research"	1	25	0	manishtripathi86	zew-data-purchasing-challenge-2022
1448	1448	🪅 National Toy Hall of Fame	A wonderful collection of toys from the past and present	['games']	"From Wikipedia
The National Toy Hall of Fame is a U.S. hall of fame that recognizes the contributions of toys and games that have sustained their popularity for many years. Criteria for induction include: icon status (the toy is widely recognized, respected, and remembered); longevity (more than a passing fad); discovery (fosters learning, creativity, or discovery); and innovation (profoundly changed play or toy design).1 Established in 1998 under the direction of Ed Sobey, it was originally housed at A. C. Gilbert's Discovery Village in Salem, Oregon, United States, but was moved to the Strong National Museum of Play (now The Strong) in Rochester, New York, in 2002 after it outgrew its original home.
About this dataset
&gt; <p>A wonderfullyl eclectic mix of toys from the past and present which reflects the diversity of play (sticks, swings, and cardboard boxes to the hottest selling toys).  But where are Chutes and Ladders and Clue?</p>
<p>This hall of fame is operated by the wonderful Strong Museum in Rochester, NY, one of the biggest toy collections on earth and well worth a visit.</p>
<p>Source: <a href=""https://en.wikipedia.org/wiki/National_Toy_Hall_of_Fame"" target=""_blank"">https://en.wikipedia.org/wiki/National_Toy_Hall_of_Fame</a></p>
<p>or</p>
<p><a href=""http://www.toyhalloffame.org/"" target=""_blank"">http://www.toyhalloffame.org/</a></p>
This dataset was created by Gary Hoover and contains around 100 samples along with Year Inducted, Year Inducted, technical information and other features such as:
- Year Inducted
- Year Inducted
- and more.
How to use this dataset
&gt; - Analyze Year Inducted in relation to Year Inducted
- Study the influence of Year Inducted on Year Inducted
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	8	97	3	yamqwe	national-toy-hall-of-famee
1449	1449	💳 CFPB Credit Card History	300 collected samples, technical information	['finance', 'lending']	"About this dataset
&gt; <p><strong>Lending levels</strong><br>
Monitoring developments in overall activity helps us identify new developments in the markets we regulate. These graphs show the number and aggregate dollar volume of new credit cards opened each month. Aggregated monthly originations are displayed along with a seasonally-adjusted series, which adjust for expected seasonal variation in lending activity.</p>
<p><strong>Year-over-year changes</strong><br>
These graphs show the percentage change in the number of new credit cards originated in the month, compared to lending activity from one year ago. Positive changes indicate that lending activity is higher than it was last year and negative values indicate that lending has declined.</p>
<p><strong>Geographic changes</strong><br>
This map shows the percentage change in the volume of new credit cards originated in each state, compared to lending activity from one year ago. Positive changes mean that the volume of credit cards originated in the state during the month are higher than they were one year ago and negative values indicate that the volume of credit cards has declined.</p>
<p>Source: <a href=""https://www.consumerfinance.gov/data-research/consumer-credit-trends/credit-cards/origination-activity/#anchor_geographic-changes"" target=""_blank"" rel=""nofollow"">https://www.consumerfinance.gov/data-research/consumer-credit-trends/credit-cards/origination-activity/#anchor_geographic-changes</a></p>
This dataset was created by Adam Helsinger and contains around 300 samples along with Group, Month, technical information and other features such as:
- Group
- Month
- and more.
How to use this dataset
&gt; - Analyze Group in relation to Month
- Study the influence of Group on Month
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	51	657	10	yamqwe	cfpb-credit-card-historye
1450	1450	⭐ McDonalds Review Sentiment	2000 collected samples, technical information	['nlp', 'restaurants']	"About this dataset
&gt; <p>A sentiment analysis of negative McDonald's reviews. Contributors were given reviews culled from low-rated McDonald's from random metro areas and asked to classify why the locations received low reviews. Options given were: * Rude Service</p>
<ul>
<li>Slow Service</li>
<li>Problem with Order</li>
<li>Bad Food</li>
<li>Bad Neighborhood</li>
<li>Dirty Location</li>
<li>Cost</li>
<li>Missing Item     Added: March 6, 2015 by CrowdFlower | Data Rows: 1500 Download Now</li>
</ul>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 2000 samples along with Unit State, Policies Violated, technical information and other features such as:
- Review
- Policies Violated Gold
- and more.
How to use this dataset
&gt; - Analyze Policies Violated:confidence in relation to City
- Study the influence of Last Judgment At on Trusted Judgments
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	112	933	4	yamqwe	mcdonalds-review-sentimente
1451	1451	🏠 Construction Price Indexes	17, Price Indexes Of New Single Family Houses Sold Including Lot Value	['housing', 'business', 'real estate']	"About this dataset
&gt; <p>The Construction Price Indexes provide price indexes for single-family houses sold and for single-family houses under construction. The houses sold index incorporates the value of the land and is available quarterly at the national level and annually by region. The indexes for houses under construction are available monthly at the national level. The indexes are based on data funded by HUD and collected in the Survey of Construction (SOC).</p>
<p>Source: <a href=""https://catalog.data.gov/dataset/construction-price-indexes"" target=""_blank"">https://catalog.data.gov/dataset/construction-price-indexes</a></p>
This dataset was created by Finance and contains around 100 samples along with Unnamed: 17, Price Indexes Of New Single Family Houses Sold Including Lot Value, technical information and other features such as:
- Unnamed: 9
- Unnamed: 1
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 14 in relation to Unnamed: 16
- Study the influence of Unnamed: 7 on Unnamed: 5
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	66	666	5	yamqwe	construction-price-indexese
1452	1452	🍕 Pizza restaurants and Pizzas on their Menus	10000 collected samples, technical information	['business', 'food', 'restaurants']	"About this dataset
&gt; <h1>About this Data</h1>
<p>This is a list of over 3,500 pizzas from multiple restaurants provided by <a href=""http://datafiniti.co/products/business-data/"" target=""_blank"" rel=""nofollow"">Datafiniti's Business Database</a>.  The dataset includes the category, name, address, city, state, menu information, price range, and more for each pizza restaurant.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with this Data</h1>
<p>You can use this data to discover how much <a href=""https://datafiniti.co/price-slice-pizza-across-america/"" target=""_blank"" rel=""nofollow"">you can expect to pay for pizza across the country</a>. E.g.:</p>
<ul>
<li>What are the least and most expensive cities for pizza?</li>
<li>What is the number of restaurants serving pizza per capita (100,000 residents) across the U.S.?</li>
<li>What is the median price of a large plain pizza across the U.S.?</li>
<li>Which cities have the most restaurants serving pizza per capita (100,000 residents)?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/business-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/business-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 10000 samples along with Longitude, Price Range Max, technical information and other features such as:
- Date Updated
- Categories
- and more.
How to use this dataset
&gt; - Analyze Date Added in relation to Province
- Study the influence of Price Range Min on Address
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	109	991	6	yamqwe	pizza-restaurants-and-pizzas-on-their-menuse
1453	1453	plant-seedling		['biology']		0	3	0	vincentyong97	plantseedling
1454	1454	College Football Bowl Games	Over 20000 samples along with Receiving Receivging Yards, Kicking Detection	['football', 'sports']	"About this dataset
&gt; <h1>Background</h1>
<p>Home field advantage is always the most desirable, but does data back it up? I’ve pulled stats on college football bowl games to see if having the home field advantage is all it is cracked up to be.</p>
<h1>Methodology</h1>
<p>The data collected was scraped from <a href=""http://www.foxsports.com"" target=""_blank"" rel=""nofollow"">www.foxsports.com</a>.</p>
<h1>Source</h1>
<p>The research and blog post can be found at <a href=""http://theconceptcenter.com/simple-research-study-college-football-bowl-games/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 20000 samples along with Receiving Receiving Yards, Kicking Pat Made, technical information and other features such as:
- Kick Return Kick Return Touchdowns
- Passing Completions
- and more.
How to use this dataset
&gt; - Analyze Kick Return Kick Return Avg in relation to Punt Return Punt Return Long
- Study the influence of Kicking Kicking Points on Kick Return Kick Return Long
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	90	730	5	yamqwe	college-football-bowl-gamese
1455	1455	🐕 Cat VS Dog popularity per state	49 collected samples, technical information	['animals']	"About this dataset
&gt; <p><img src=""http://i.imgur.com/LGI7wTt.png"" alt=""Imgur"" style=""""></p>
 This dataset was created by Andrew Duff and contains around 0 samples along with Percentage Of Cat Owners, Mean Number Of Dogs Per Household, technical information and other features such as:
- Percentage Of Households With Pets
- Mean Number Of Cats
- and more.
How to use this dataset
&gt; - Analyze Percentage Of Dog Owners in relation to Number Of Pet Households (in 1000)
- Study the influence of Percentage Of Cat Owners on Mean Number Of Dogs Per Household
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andrew Duff 
Start A New Notebook!"	124	824	4	yamqwe	cat-vs-dog-popularity-in-u-se
1456	1456	👷 US Reported Worker Fatalities	Over 4000 samples of Occupational Safety and Health Administration fatalities	['employment', 'health', 'public safety']	"About this dataset
&gt; <p>The data below provides summaries of all work-related fatalities reported to the Occupational Safety and Health Administration (OSHA), which must be done by employers within 8 hours of learning of the event.</p>
<p><strong>Source:</strong> <a href=""https://www.osha.gov/dep/fatcat/dep_fatcat.html"" target=""_blank"" rel=""nofollow"">https://www.osha.gov/dep/fatcat/dep_fatcat.html</a></p>
<p><em>Originally compiled by Casey Miller</em></p>
This dataset was created by Chris Awram and contains around 4000 samples along with Date, Address, technical information and other features such as:
- State
- Type
- and more.
How to use this dataset
&gt; - Analyze Longitude in relation to Description
- Study the influence of Latitude on Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chris Awram 
Start A New Notebook!"	18	158	4	yamqwe	us-reported-worker-fatalitiese
1457	1457	📈 Daily Top Trends Worldwide 201X - Grow in	3000 collected samples, technical information	['languages', 'people', 'business', 'internet', 'news', 'social networks']	"About this dataset
&gt; <h2>Context</h2>
<p>Daily top trends to see what was lit around the world</p>
<p>One of the original aim was to make it able to get ideas (of new apps/businesses) based on some potential upcoming buzz. Something along the lines of being more creative or give ideas, basically. (For instance, you must have seen seen how apps had sprouted on the App Store when some buzz subjects came out such as Zika virus, Donald Trump's first campaign for president, ...)</p>
<h2>Content</h2>
<p>The data is a CSV dump of all the articles published on <a href=""https://primearchive.blogspot.com/"" target=""_blank"" rel=""nofollow"">a blog</a> whose aim was to analyze search result trends for different countries. The blog itself has been discontinued but the data it contains is still valuable.</p>
<h2>Inspiration</h2>
<ul>
<li><strong>Business</strong> – <strong>Building better products</strong> tailored to the desires of the population.</li>
<li><strong>Business</strong> – <strong>International business perspectives</strong>. Finding good partners around the world is important in the era of globalisation. It has always been easier to build commercial relationships with people who share the same interests as you or people who are curious about things you have in your country. (It can even serve as small talk material). <em>Using this dataset</em> you can look for the countries that have <strong>the most affinities</strong> <em>with your country or your passion</em>. For instance, you may know NOT to talk about <em>this</em> and rather talk about <em>that</em> to avoid foreigners eyeing you strangely.</li>
<li><strong>Studies</strong> – <strong>Studying buzz and trends</strong>. Having a track record of trend hotness over several countries and several days could help study <strong>how buzz are formed and how much they last</strong>. Since there are trends <em>for every domain</em>, you can get a better understanding  of both broad and domain-specific trend / buzz behaviour.</li>
<li><strong>Marketing</strong> / Studies – <strong>How to create a good buzz ?</strong> Looking at the different trending topics and how much people searched them could help you understand crowd phenomenon or crowd behaviour better.</li>
<li><strong>Social</strong> – <strong>Small talk material</strong> when talking with <em>foreigners</em> :) <em>Do not mess up</em> by taking the opposite side of your foreign acquaintance by looking at the hottest subject and whether or not they are mostly <em>hated</em> or loved.</li>
<li><strong>Studies</strong> – <strong><em>Census</em> and segmentation of population</strong>. Depending what people searched the most, you may get insights about the habits of people from a given country. For instance, using search terms that are specific to some segments of the population, such as <em>The Young and the Restles</em> ( <em>Les Feux de l'Amour</em> in french), which is a famous TV program for people in the age of XX.</li>
<li><strong>Marketing</strong> – <strong>Content ideas</strong> for <strong>Marketing agencies</strong> Again, if you want to design ads or deploy ads for your customers, you may be better inquiring about the culture of the people your campaign will target. Depending on what <strong>your analytics</strong> tell you about the countries which saw your campaign the most, you may want to think about their tastes/behaviour to further improve the quality of your campaign.</li>
<li><strong>Social</strong> – <strong>Fans of foreign celebrities</strong>. Since search trends are segmented by country, you can get an idea of how the world see your country's celebrities or politicians, and who care about it the most.</li>
</ul>
<p>You have other use cases or ideas in mind ? Put a note in the comments section to get the power of a brainstorm about it ;) Think of it as a diamond: it will shine a different light and color to everyone, depending on where they are and their point of views.</p>
<h2>Tools / Apps for CSV files</h2>
<p>If you're looking for tools to work with CSV files, I can recommend:</p>
<ul>
<li><a href=""https://apps.apple.com/us/app/csview/id1167193104?mt=12"" target=""_blank"" rel=""nofollow"">CSView</a> on <em>Mac</em> if you want to quickly <strong>visualise</strong> CSV files, which is a fast app that allows you to browse the data quickly in a table.</li>
<li>If you want to <strong>work</strong> with the data, then you probably already have tools for that. But if you don't you may start with <a href=""https://www.libreoffice.org/"" target=""_blank"" rel=""nofollow"">LibreOffice</a> (all computers).  It can handle CSV files that some other similar software cannot.</li>
</ul>
<h2>Credits</h2>
<p>The data was scraped from <a href=""https://primearchive.blogspot.com/"" target=""_blank"" rel=""nofollow"">https://primearchive.blogspot.com/</a>. The blog does not mention the original author. With no contact information whatsoever on the blog, it was impossible to contact them to get permission. I formatted the data to make it more easily consumable for automation or machine learning algorithms. Big shout-out to the blog author(s) who aggregated this wonderful source of data.</p>
<p>If you are the original author of the data stored <a href=""https://primearchive.blogspot.com/"" target=""_blank"" rel=""nofollow"">https://primearchive.blogspot.com/</a> you can contact me.</p>
<p>As mentioned previously, I am not the author of the measurements data of the blog (nor can I confirm its measurement accuracy), thus the data is PROVIDED AS IS.</p>
<h2>License</h2>
<p>Cannot give a license. But the data was (and is still) available publicly on the blog.</p>
This dataset was created by Jeffrey Mvutu Mabilama and contains around 3000 samples along with Traffic, News Source, technical information and other features such as:
- News Link
- Name
- and more.
How to use this dataset
&gt; - Analyze Type in relation to Title
- Study the influence of No on Country
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jeffrey Mvutu Mabilama 
Start A New Notebook!"	72	684	6	yamqwe	daily-top-trends-worldwide-201x-grow-in-businesse
1458	1458	Germany Largest Companies	50 collected samples, technical information	['business']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p>​Source: <a href=""%E2%80%8Bhttp://www.economywatch.com/companies/forbes-list/"" target=""_blank"" rel=""nofollow"">Economy Watch</a></p>
This dataset was created by Finance and contains around 0 samples along with Profits ($billion), Assets ($billion), technical information and other features such as:
- Sales ($billion)
- Market Value ($billion)
- and more.
How to use this dataset
&gt; - Analyze Global Rank in relation to Profits ($billion)
- Study the influence of Assets ($billion) on Sales ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	77	459	10	yamqwe	germany-largest-companiese
1459	1459	🎞 Films Criticized for Violent content in the UK	Video Recordings at the UK, including title, director and more	['arts and entertainment', 'movies and tv shows', 'music']	"About this dataset
<p>Video nasty is a colloquial term in the United Kingdom to refer to a number of films distributed on video cassette that were criticised for their violent content by the press, social commentators and various religious organizations. The term was popularised by the National Viewers' and Listeners' Association (NVALA) in the UK in the early 1980s.</p>
<p>These video releases were not brought before the British Board of Film Classification (BBFC), which could have censored or banned many of the films, due to a loophole in film classification laws. As a result, this produced a glut of potentially censorable video releases, which led to public debate concerning the availability of these films to children due to the unregulated nature of the market.</p>
<p>Following a moral campaign led by Mary Whitehouse and the NVALA, local jurisdictions began to prosecute certain video releases for obscenity. To assist local authorities in identifying obscene films, the Director of Public Prosecutions released a list of 72 films the office believed to violate the Obscene Publications Act 1959. This list included films that had been acquitted of obscenity in certain jurisdictions or that had already obtained BBFC certification. The subsequent revisions to the list and confusion regarding what constituted obscene material led to Parliament passing the Video Recordings Act 1984, which forced all video releases to appear before the BBFC for certification.</p>
<p>The implementation of the Video Recording Act imposed a stricter code of censorship on videos than was required for cinema release. Several major studio productions were banned on video, as they fell within the scope of legislation designed to control the distribution of video nasties. In recent years, the stricter requirements have been relaxed, as numerous films once considered video nasties have obtained certification uncut or with minimal edits. Due to a legislative mistake discovered in August 2009, the Video Recordings Act 1984 was repealed and re-enacted without change by the Video Recordings Act 2010.</p>
<p>From <a href=""https://en.wikipedia.org/wiki/Video_nasty"" target=""_blank"">Wikipedia - Video Nasty</a></p>
This dataset was created by Carolee Mitchell and contains around 100 samples along with Year, Status, technical information and other features such as:
- Original/alternate Titles
- Director
- and more.
How to use this dataset
&gt; - Analyze Year in relation to Status
- Study the influence of Original/alternate Titles on Director
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Carolee Mitchell 
Start A New Notebook!"	38	256	4	yamqwe	video-nastiese
1460	1460	ℹ National Park Locations	Official URLs of United States National and State Parks	['websites']	"About this dataset
&gt; <p>A large data set containing the official URLs of United States national and state parks.   Added: June 14, 2014 by CrowdFlower | Data Rows: 323 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 300 samples along with Google1 Correct Website Found:confidence, Info About Park On A List Website Gold, technical information and other features such as:
- Canary
- Google1 Correct Website Found
- and more.
How to use this dataset
&gt; - Analyze Google1 Correct Website Worker Input:confidence in relation to Info About Park On A List Website
- Study the influence of Info About Park On A List Website Worker Input:confidence on Google1 Correct Website Found Gold
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	57	427	5	yamqwe	national-park-locationse
1461	1461	🥝 Food Nutrition Information	100 collected samples, technical information	['nutrition', 'beginner']	"About this dataset
&gt; <p><em>Fruit:</em><br>
Raw, edible weight portion. Percent Daily Values (%DV) are based on a 2,000 calorie diet.\</p>
<p><em>Vegetables:</em><br>
Raw, edible weight portion. Percent Daily Values (%DV) are based on a 2,000 calorie diet.</p>
<p><em>Seafood:</em><br>
Cooked (by moist or dry heat with no added ingredients), edible weight portion.<br>
Percent Daily Values (%DV) are based on a 2,000 calorie diet.</p>
<p>Sources:<br>
<a href=""https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm063367.htm"" target=""_blank"" rel=""nofollow"">https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm063367.htm</a><br>
Fruits: <a href=""https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm063482.htm"" target=""_blank"" rel=""nofollow"">https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm063482.htm</a><br>
Vegetables: <a href=""https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm114222.htm"" target=""_blank"" rel=""nofollow"">https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm114222.htm</a><br>
Seafood: <a href=""https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm114223.htm"" target=""_blank"" rel=""nofollow"">https://www.fda.gov/Food/IngredientsPackagingLabeling/LabelingNutrition/ucm114223.htm</a></p>
This dataset was created by Adam Helsinger and contains around 100 samples along with Chole Sterol.1, Food Type, technical information and other features such as:
- Sugars
- Potassium
- and more.
How to use this dataset
&gt; - Analyze Êê Ironêê in relation to Total Carbo Hydrate.1
- Study the influence of Calcium on Saturated Fat
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	147	1145	5	yamqwe	food-nutrition-informatione
1462	1462	 👨‍💻 Top Starred Open Source Projects on GitHub	Dataset of Top starred projects on GitHub	['social science', 'computer science', 'programming']	"About this dataset
&gt; <h1>Background</h1>
<p>GitHub is the leader in hosting open source projects. For those who are not familiar with open source projects, a group of developers share and contribute to common code to develop software. Example open source projects include, Chromium (which makes Google Chrome), WordPress, and Hadoop. Open source projects are said to have disrupted the software industry (<a href=""http://s3.cleverelephant.ca/2008-kansas-keynote.pdf"" target=""_blank"" rel=""nofollow"">2008 Kansas Keynote</a>).</p>
<h1>Methodology</h1>
<p>For this study, I crawled the leader in hosting open source projects, <a href=""http://GitHub.com"" target=""_blank"" rel=""nofollow"">GitHub.com</a> and extracted a list of the top starred open source projects. On GitHub, a user may choose the star a repository representing that they “like” the project. For each project, I gathered the repository username or Organization the project resided in, the repository name, a description, the last updated date, the language of the project, the number of stars, any tags, and finally the url of the project.</p>
<h1>Source</h1>
<p>The micro-research study using this dataset can be found at <a href=""http://theconceptcenter.com/simple-research-study-top-starred-open-source-projects/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 1000 samples along with Language, Last Update Date, technical information and other features such as:
- Username
- Url
- and more.
How to use this dataset
&gt; - Analyze Number Of Stars in relation to Repository Name
- Study the influence of Description on Tags
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	59	803	8	yamqwe	top-starred-open-source-projects-on-githube
1463	1463	🏫 Musical Instrument Courses from Udemy	700 collected samples, technical information	['education']	"About this dataset
&gt; <h1>Background</h1>
<p>Udemy is a massive online open course (MOOC) web application. Within Udemy, a student can learn nearly anything. You may wonder, why would anyone take one of these courses? If you use Google’s Trends app, you can enter in different search terms and compare the world-wide volume of searches for that search term.</p>
<p>For example, I put in the terms, who, what, when, where, why and how. In addition, I furthered the comparison and added the terms, how to, what are, who is, why are, when do.</p>
<p>According to the trends on Google, obviously the worlds wants to know how to do things and this is exactly what Udemy does. It teaches people how to do things.</p>
<h1>Methodology</h1>
<p>I scraped the Udemy website and pulled many published courses for the topics of Graphic Design, Business Finance, Web Development and Musical Instruments.</p>
<h1>Source</h1>
<p>For the full study, see <a href=""http://theconceptcenter.com/simple-research-study-udemy-courses/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 700 samples along with Unnamed: 12, Total, technical information and other features such as:
- Num Subscribers
- Price
- and more.
How to use this dataset
&gt; - Analyze Column1 in relation to Title
- Study the influence of Content Info on Is Paid
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	71	585	11	yamqwe	musical-instrument-courses-from-udemye
1464	1464	Ξ Cryptocurrency Price by Day/Hr (2013 - 2018)	2000 collected samples, technical information	['beginner', 'currencies and foreign exchange']	"About this dataset
&gt; <h1>Introduction</h1>
<p>For those who are new to cryptocurrencies, you may ask, what are cryptocurrencies? According to Google, a cryptocurrency is ""a digital currency in which encryption techniques are used to regulate the generation of units of currency and verify the transfer of funds, operating independently of a central bank.""1</p>
<p>If you read the news or keep up on market trends, you may have heard Bitcoin. Bitcoin is a cryptocurrency.&nbsp;Over the past year, cryptocurrencies have become extremely popular. The graph below shows the total cryptocurrencies that have surfaced since quarter 2 of 2013. It wasn't until 2017 that the rise of cryptocurrencies have taken off.</p>
<h1>Data</h1>
<p>Date Pulled: 2/09/2018 12:10pm</p>
<p>Data Pulled From:&nbsp;<a href=""https://coinmarketcap.com"" target=""_blank"" rel=""nofollow"">https://coinmarketcap.com</a></p>
<h1>Resources</h1>
<p>A special thanks to <a href=""https://theconceptcenter.com/simple-research-study-cryptocurrencies"" target=""_blank"" rel=""nofollow"">The Concept Center</a> for pulling the data and writing a simple research study</p>
This dataset was created by Chase Willden and contains around 2000 samples along with Volume (24hr), Symbol, technical information and other features such as:
- Price
- Name
- and more.
How to use this dataset
&gt; - Analyze Market Cap in relation to Circulating Supply
- Study the influence of 24h on 7d
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	174	1224	10	yamqwe	ks-cryptocurrency-price-by-day-hr-2013-2018-w-bie
1465	1465	Visualizing Conflicts Geolocations	Dataset contains images of a fictional entity - Starry Murray	['earth and nature']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/Y6h5quiRRUGL0UTWAq6m_original%20ACLED.png"" alt=""Original Visualization"" style=""""></p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE: <a href=""https://www.acleddata.com/dashboard/"" target=""_blank"" rel=""nofollow"">ACLED</a></p>
<p>Data Source: <a href=""https://www.acleddata.com/data/"" target=""_blank"" rel=""nofollow"">DATA</a></p>
<p>Note: Please include the hashtage #ACLEDconflictviz on Twitter</p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
This dataset was created by Eva Murray and contains around 100000 samples along with Interaction, Region, technical information and other features such as:
- Notes
- Event Id No Cnty
- and more.
How to use this dataset
&gt; - Analyze Latitude in relation to Fatalities
- Study the influence of Time Precision on Country
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Eva Murray 
Start A New Notebook!"	36	420	8	yamqwe	2018-w34-acled-visualizing-conflicte
1466	1466	Covid-19 Weekly Trends In Asia - Latest Data	Weekly trends of Covid-19 as on February 08, 2022	['asia', 'public health', 'exploratory data analysis', 'tabular data', 'news', 'covid19']	"Content
This dataset contains data of weekly trend of Covid-19 in Asia (February 01 - February 08, 2022)
Attribute Information
Country/Other
Cases in the last 7 days
Cases in the preceding 7 days
Weekly Case % Change
Cases in the last 7 days/1M pop
Deaths in the last 7 days
Deaths in the preceding 7 days
Weekly Death % Change 
Deaths in the last 7 days/1M pop
Population
Source
Link : https://www.worldometers.info/coronavirus/weekly-trends/#weekly_table
Other Updated Covid Datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍
Thank You"	431	2408	34	anandhuh	covid19-weekly-trends-in-asia-latest-data
1467	1467	⚽ 2017-2018 Premier League matches	Dataset with 400 samples along with technical information	['football', 'earth and nature']	"About this dataset
&gt; Do whatever with the results of the Prem season This dataset was created by Chas Peacock and contains around 400 samples along with Hy, Home Team, technical information and other features such as:
- Fthg
- Hs
- and more.
How to use this dataset
&gt; - Analyze Hst in relation to As
- Study the influence of Ast on Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chas Peacock 
Start A New Notebook!"	114	882	5	yamqwe	2017-2018-premier-league-matchese
1468	1468	🎗️ Cancer Rates by U.S. State	100 collected samples, technical information	['health', 'cancer']	"About this dataset
&gt; <p>In the following maps, the U.S. states are divided into groups based on the rates at which people developed or died from cancer in 2013, the most recent year for which incidence data are available.</p>
<p>The rates are the numbers out of 100,000 people who developed or died from cancer each year.</p>
<p><strong>Incidence Rates by State</strong><br>
<em>The number of people who get cancer is called cancer incidence. In the United States, the rate of getting cancer varies from state to state.</em></p>
<ul>
<li>
<p>*Rates are per 100,000 and are age-adjusted to the 2000 U.S. standard population.</p>
</li>
<li>
<p>‡Rates are not shown if the state did not meet USCS publication criteria or if the state did not submit data to CDC.</p>
</li>
<li>
<p>†Source: U.S. Cancer Statistics Working Group. United States Cancer Statistics: 1999–2013 Incidence and Mortality Web-based Report. Atlanta (GA): Department of Health and Human Services, Centers for Disease Control and Prevention, and National Cancer Institute; 2016. Available at: <a href=""http://www.cdc.gov/uscs"" target=""_blank"" rel=""nofollow"">http://www.cdc.gov/uscs</a>.</p>
</li>
</ul>
<p><strong>Death Rates by State</strong><br>
<em>Rates of dying from cancer also vary from state to state.</em></p>
<ul>
<li>
<p>*Rates are per 100,000 and are age-adjusted to the 2000 U.S. standard population.</p>
</li>
<li>
<p>†Source: U.S. Cancer Statistics Working Group. United States Cancer Statistics: 1999–2013 Incidence and Mortality Web-based Report. Atlanta (GA): Department of Health and Human Services, Centers for Disease Control and Prevention, and National Cancer Institute; 2016. Available at: <a href=""http://www.cdc.gov/uscs"" target=""_blank"" rel=""nofollow"">http://www.cdc.gov/uscs</a>.</p>
</li>
</ul>
<p>Source: <a href=""https://www.cdc.gov/cancer/dcpc/data/state.htm"" target=""_blank"" rel=""nofollow"">https://www.cdc.gov/cancer/dcpc/data/state.htm</a></p>
This dataset was created by Adam Helsinger and contains around 100 samples along with Range, Rate, technical information and other features such as:
- Range
- Rate
- and more.
How to use this dataset
&gt; - Analyze Range in relation to Rate
- Study the influence of Range on Rate
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Helsinger 
Start A New Notebook!"	110	853	8	yamqwe	cancer-rates-by-u-s-statee
1469	1469	🐄 Walmart Product Listings 2021	This dataset includes product listing data from Walmart	['business', 'retail and shopping']	"About this dataset
&gt; <p><img src=""https://mediauploads.data.world/7ccda64e47f78a5784a1a2cd3d5067f883892e2a8282ab2942311888aebb5de3_Referral_Banner_Jan_2022.png"" alt=""""></p>
<h3>Context</h3>
<p>This dataset was created by teams at <a href=""https://www.promptcloud.com/"" target=""_blank"">PromptCloud</a>  and <a href=""https://datastock.shop/"" target=""_blank"">DataStock</a> . You can download the full dataset here. This sample contains 30K records. You can download the full dataset <a href=""https://app.datastock.shop/?site_name=Walmart_Product_Listing"" target=""_blank"">here</a></p>
<h3>Content</h3>
<p>Total Records Count : 439956  Domain Name : <a href=""http://walmart.com"" target=""_blank"">walmart.com</a>  Date Range : 01st Apr 2021 - 30th Apr 2021   File Extension : csv</p>
<p>Available Fields : Uniq Id, Crawl Timestamp, Pageurl, Website, Title, Num Of Reviews, Average Rating, Number Of Ratings, Model Num, Sku, Upc, Manufacturer, Model Name, Price, Monthly Price, Stock, Carrier, Color Category, Internal Memory, Screen Size, Specifications, Five Star, Four Star, Three Star, Two Star, One Star, Discontinued, Broken Link, Joining Key</p>
<h3>Acknowledgements</h3>
<p>We wouldn't be here without the help of our in house web scraping and data mining teams at <a href=""https://www.promptcloud.com/"" target=""_blank"">PromptCloud</a>  and <a href=""https://datastock.shop/"" target=""_blank"">DataStock</a> .</p>
<h3>Inspiration</h3>
<p>This dataset was created keeping in mind our data scientists and researchers across the world.</p>
This dataset was created by PromptCloud and contains around 30000 samples along with Joining Key, Title, technical information and other features such as:
- Screen Size
- Model Num
- and more.
How to use this dataset
&gt; - Analyze Price in relation to Average Rating
- Study the influence of Two Star on Broken Link
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	53	419	5	yamqwe	product-listing-walmarte
1470	1470	Pumpkin Seeds Dataset	Pumpkin Seeds Dataset, 12 Morphological features, (Urgup Sivrisi and Cercevelik)	['categorical data', 'agriculture', 'artificial intelligence', 'computer science', 'binary classification']	"Citation Request :
KOKLU, M., SARIGIL, S., & OZBEK, O. (2021). The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.). Genetic Resources and Crop Evolution, 68(7), 2713-2726. Doi: https://doi.org/10.1007/s10722-021-01226-0
https://link.springer.com/article/10.1007/s10722-021-01226-0
https://link.springer.com/content/pdf/10.1007/s10722-021-01226-0.pdf
Abstract : Pumpkin seeds are frequently consumed as confection worldwide because of their adequate amount of protein, fat, carbohydrate, and mineral contents. This study was carried out on the two most important and quality types of pumpkin seeds, “Ürgüp Sivrisi” and “Çerçevelik”, generally grown in Ürgüp and Karacaören regions in Turkey. However, morphological measurements of 2500 pumpkin seeds of both varieties were made possible by using the gray and binary forms of threshold techniques. Considering morphological features, all the data were modeled with five different machine learning methods: Logistic Regression (LR), Multilayer Perceptrons (MLP), Support Vector Machine (SVM) and Random Forest (RF), and k-Nearest Neighbor (k-NN), which further determined the most successful method for classifying pumpkin seed varieties. However, the performances of the models were determined with the help of the 10 k-fold cross-validation method. The accuracy rates of the classifiers were obtained as LR 87.92 percent, MLP 88.52 percent, SVM 88.64 percent, RF 87.56 percent, and k-NN 87.64 percent.
Keywords: Pumpkin seed  Logistic regression, Multilayer peceptrons,  Random forest, Classification,  Support vector machine,  Thresholding"	4	61	15	mkoklu42	pumpkin-seeds-dataset
1471	1471	Vehicle Miles Traveled During Covid-19 Lock-Downs 	Vehicle miles traveled in the US during the pandemic	['united states', 'transportation', 'covid19']	"About this dataset
&gt; <p>This data set was last updated 3:30 PM ET Monday, January 4, 2021. The last date of data in this dataset is December 31, 2020.  </p>
<h2>Overview</h2>
<p>Data shows that mobility declined nationally since states and localities began shelter-in-place strategies to stem the spread of COVID-19. The numbers began climbing as more people ventured out and traveled further from their homes, but in parallel with the rise of COVID-19 cases in July, travel declined again.</p>
<p>This distribution contains county level data for vehicle miles traveled (VMT) from <a href=""https://www.streetlightdata.com"" target=""_blank"" rel=""nofollow"">StreetLight Data, Inc</a>, updated three times a week. This data offers a detailed look at estimates of how much people are moving around in each county.</p>
<p>Data available has a two day lag - the most recent data is from two days prior to the update date. Going forward, this dataset will be updated by AP at 3:30pm ET on Monday, Wednesday and Friday each week.</p>
<p><strong>This data has been made available to members of AP’s Data Distribution Program. To inquire about access for your organization - publishers, researchers, corporations, etc. - please click <em>Request Access</em> in the upper right corner of the page or email <a href=""mailto:kromano@ap.org"" target=""_blank"" rel=""nofollow"">kromano@ap.org</a>. Be sure to include your contact information and use case.</strong></p>
<h2>Findings</h2>
<ul>
<li>Nationally, data shows that vehicle travel in the US has doubled compared to the seven-day period ending April 13, which was the lowest VMT since the COVID-19 crisis began. In early December, travel reached a low not seen since May, with a small rise leading up to the Christmas holiday.</li>
<li>Average vehicle miles traveled continues to be below what would be expected without a pandemic - down 38% compared to January 2020. September 4 reported the largest single day estimate of vehicle miles traveled since March 14.</li>
<li>New Jersey, Michigan and New York are among the states with the largest relative uptick in travel at this point of the pandemic - they report almost two times the miles traveled compared to their lowest seven-day period. However, travel in New Jersey and New York is still much lower than expected without a pandemic. Other states such as New Mexico, Vermont and West Virginia have rebounded the least.</li>
</ul>
<h2>About This Data</h2>
<p>The county level data is provided by <a href=""https://www.streetlightdata.com"" target=""_blank"" rel=""nofollow"">StreetLight Data, Inc</a>, a transportation analysis firm that measures travel patterns across the U.S.. The data is from their Vehicle Miles Traveled (VMT) Monitor which uses anonymized and aggregated data from smartphones and other GPS-enabled devices to provide county-by-county VMT metrics for more than 3,100 counties. The VMT Monitor provides an estimate of total vehicle miles travelled by residents of each county, each day since the COVID-19 crisis began (March 1, 2020), as well as a change from the baseline average daily VMT calculated for January 2020. Additional columns are calculations by AP.</p>
<h2>Included Data</h2>
<p><em>01_vmt_nation.csv</em> - Data summarized to provide a nationwide look at vehicle miles traveled. Includes single day VMT across counties, daily percent change compared to January and seven day rolling averages to smooth out the trend lines over time.</p>
<p><em>02_vmt_state.csv</em> - Data summarized to provide a statewide look at vehicle miles traveled. Includes single day VMT across counties, daily percent change compared to January and seven day rolling averages to smooth out the trend lines over time.</p>
<p><em>03_vmt_county.csv</em> - Data providing a county level look at vehicle miles traveled. Includes VMT estimate, percent change compared to January and seven day rolling averages to smooth out the trend lines over time.</p>
<h2>Additional Data Queries</h2>
<p>* <a href=""https://data.world/associatedpress/vehicle-miles-traveled/workspace/query?queryid=cb0260e2-3175-40e7-b164-a00e89143fb1"">Filter for specific state</a>  - filters <code>02_vmt_state.csv</code> daily data for specific state.</p>
<p>* <a href=""https://data.world/associatedpress/vehicle-miles-traveled/workspace/query?queryid=f6b8f8cb-68f0-4f39-bf7d-13056f8c24d0"">Filter counties by state</a>  - filters <code>03_vmt_county.csv</code> daily data for counties in specific state.</p>
<p>* <a href=""https://data.world/associatedpress/vehicle-miles-traveled/workspace/query?queryid=97cc1e7f-a805-4562-a98a-fcaa6c9f36ec"">Filter for specific county</a>  - filters <code>03_vmt_county.csv</code> daily data for specific county.</p>
<h2>Interactive</h2>
<p>The AP has designed an interactive map to show percent change in vehicle miles traveled by county since each counties lowest point during the pandemic:</p>
<p></p>
This dataset was created by Angeliki Kastanis and contains around 0 samples along with Date At Low, Mean7 County Vmt At Low, technical information and other features such as:
- County Name
- County Fips
- and more.
How to use this dataset
&gt; - Analyze State Name in relation to Baseline Jan Vmt
- Study the influence of Date At Low on Mean7 County Vmt At Low
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Angeliki Kastanis 
Start A New Notebook!"	38	437	3	yamqwe	vehicle-miles-travelede
1472	1472	 Zillow Housing Aspirations Report	Housing and Housing Data available from Zillow Website	['housing', 'business', 'real estate', 'economics']	"About this dataset
&gt; <h2>Additional Data Products</h2>
<p>Product: <a href=""https://www.zillow.com/research/tag/zillow-housing-aspirations-report/"" target=""_blank"" rel=""nofollow"">Zillow Housing Aspirations Report</a></p>
<p>Date: April 2017</p>
<h2>Definitions</h2>
<p><strong>Home Types and Housing Stock</strong></p>
<ul>
<li>All Homes: Zillow defines all homes as single-family, condominium and co-operative homes with a county record. Unless specified, all series cover this segment of the housing stock.</li>
<li>Condo/Co-op: Condominium and co-operative homes.</li>
<li>Multifamily 5+ units: Units in buildings with 5 or more housing units, that are not a condominiums or co-ops.</li>
<li>Duplex/Triplex: Housing units in buildings with 2 or 3 housing units.</li>
</ul>
<h2>Additional Data Products</h2>
<ul>
<li>Zillow Home Value Forecast (ZHVF): The ZHVF is the one-year forecast of the ZHVI. Our forecast methodology is <a href=""http://www.zillow.com/research/2013/01/24/zillow-home-value-forecast-methodology-2/"" target=""_blank"" rel=""nofollow"">methodology post</a>.</li>
<li>Zillow creates our negative equity data using our own data in conjunction with data received through our partnership with TransUnion, a leading credit bureau. We match estimated home values against actual outstanding home-related debt amounts provided by TransUnion. To read more about how we calculate our negative equity metrics, please see our <a href=""http://www.zillow.com/research/methodology-negative-equity-3180/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>Cash Buyers: The share of homes in a given area purchased without financing/in cash. To read about how we calculate our cash buyer data, please see our <a href=""http://www.zillow.com/research/top-markets-for-cash-purchases-9696/"" target=""_blank"" rel=""nofollow"">research brief</a>.</li>
<li>Mortgage Affordability, Rental Affordability, Price-to-Income Ratio, Historical ZHVI, Historical ZHVI and Houshold Income are calculated as a part of Zillow’s quarterly Affordability Indices. To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment). Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. For quarters where median income is not available from the U.S. Census Bureau, we calculate future quarters of median household income by estimating it using the Bureau of Labor Statistics’ Employment Cost Index. The affordability forecast is calculated similarly to the current affordability index but uses the one year Zillow Home Value Forecast instead of the current Zillow Home Value Index and a specified interest rate in lieu of PMMS. It also assumes a 20 percent down payment. We calculate rent affordability similarly to mortgage affordability; however we use the Zillow Rent Index, which tracks the monthly median rent in particular geographical regions, to capture rental prices. Rents are chained back in time by using U.S. Census Bureau American Community Survey data from 2006 to the start of the Zillow Rent Index, and Decennial Census for all other years.</li>
<li>The mortgage rate series is the average mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate mortgage in 15-minute increments during business hours, 6:00 AM to 5:00 PM Pacific. It does not include quotes for jumbo loans, FHA loans, VA loans, loans with mortgage insurance or quotes to consumers with credit scores below 720. Federal holidays are excluded. The jumbo mortgage rate series is the average jumbo mortgage rate quoted on Zillow Mortgages for a 30-year, fixed-rate, jumbo mortgage in one-hour increments during business hours, 6:00 AM to 5:00 PM Pacific Time. It does not include quotes to consumers with credit scores below 720. Traditional federal holidays and hours with insufficient sample sizes are excluded.</li>
</ul>
<h2>About Zillow Data (and Terms of Use Information)</h2>
<ul>
<li>Zillow is in the process of transitioning some data sources with the goal of producing published data that is more comprehensive, reliable, accurate and timely. As this new data is incorporated, the publication of select metrics may be delayed or temporarily suspended. We look forward to resuming our usual publication schedule for all of our established datasets as soon as possible, and we apologize for any inconvenience. Thank you for your patience and understanding.</li>
<li>All data accessed and downloaded from this page is free for public use by consumers, media, analysts, academics etc., consistent with our published <a href=""http://www.zillow.com/corp/Terms.htm"" target=""_blank"" rel=""nofollow"">Terms of Use</a>. Proper and clear attribution of all data to Zillow is required.</li>
<li>For other data requests or inquiries for Zillow Real Estate Research, contact us <a href=""http://www.zillow.com/research/contact-us/"" target=""_blank"" rel=""nofollow"">here</a>.</li>
<li>All files are time series unless noted otherwise.</li>
<li>To download all Zillow metrics for specific levels of geography, click <a href=""https://data.world/zillow-data/all-zillow-metrics-by-geography"">here</a>.</li>
<li>To download a crosswalk between Zillow regions and federally defined regions for counties and metro areas, click <a href=""https://data.world/zillow-data/crosswalk-between-zillow-and-federally-defined-regions"">here</a>.</li>
<li>Unless otherwise noted, all series cover single-family residences, condominiums and co-op homes only.</li>
</ul>
<p><strong><em>Source:</em></strong> <a href=""https://www.zillow.com/research/data/"" target=""_blank"" rel=""nofollow"">https://www.zillow.com/research/data/</a></p>
This dataset was created by Zillow Data and contains around 200 samples along with Unnamed: 1, Unnamed: 0, technical information and other features such as:
- Unnamed: 1
- Unnamed: 0
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 1 in relation to Unnamed: 0
- Study the influence of Unnamed: 1 on Unnamed: 0
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Zillow Data 
Start A New Notebook!"	77	795	11	yamqwe	zillow-housing-aspirations-reporte
1473	1473	🥫 Food Pantry and User Data 	Locations of food pantry service providers, food insecure people concentrations	['business', 'food']	"About this dataset
&gt; <p>This dataset identifies locations of food pantry service providers, food insecure people concentrations, and seasonal service demand variability.</p>
<p>Source: <a href=""https://handsoncentralohio.org/"" target=""_blank"" rel=""nofollow"">https://handsoncentralohio.org/</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2018-05-08</p>
This dataset was created by Kelly Garrett and contains around 1000 samples along with Update User, Asl, technical information and other features such as:
- Location Id
- Add Date
- and more.
How to use this dataset
&gt; - Analyze Director in relation to Printcol1
- Study the influence of Special Event Flag on Print Labels Flag
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	59	611	7	yamqwe	food-pantry-and-user-data-agencye
1474	1474	US Law Enforcement Geo-locations	Data from the US Dept of Homeland Security (DOJ-BJS)	['crime', 'government', 'law', 'public safety']	"About this dataset
&gt; <p>This dataset is sourced from the HIFLD open data portal, from the US Dept of Homeland Security.</p>
<p>""The local law enforcement locations feature class/ shapefile contains point location and tabular information pertaining to a wide range of law enforcement entities in the United States. Law Enforcement agencies ""are publicly funded and employ at least one full-time or part-time sworn officer with general arrest powers"". This is the definition used by the US Department of Justice - Bureau of Justice Statistics (DOJ-BJS) for their Census of State and Local Law Enforcement Agencies (CSLLEA). Unlike the previous version of this dataset, published in 2009, federal level law enforcement agencies are excluded from this effort. Data fusion techniques are utilized to synchronize overlapping yet disparate source data. The primary sources for this effort are the DOJ-BJS CSLLEA from 2008 and the previously mentioned 2009 feature class from Homeland Security Infrastructure Foundation-Level Data (HIFLD). This feature class contains data for agencies across all 50 U.S. states, Washington D.C. and Puerto Rico."" (This description is copied from the source.)</p>
<p><strong><em>Source:</em></strong> <a href=""https://hifld-geoplatform.opendata.arcgis.com/datasets/local-law-enforcement-locations"" target=""_blank"" rel=""nofollow"">https://hifld-geoplatform.opendata.arcgis.com/datasets/local-law-enforcement-locations</a></p>
<p><strong><em>License:</em></strong> public domain</p>
<p><em><strong>Updated:</strong></em> synced from source weekly, though as of 4/27/20 the source was last updated on 9/23/2019</p>
This dataset was created by Liz Friedman and contains around 20000 samples along with X, Countyfips, technical information and other features such as:
- Nummobile
- Ptciv
- and more.
How to use this dataset
&gt; - Analyze City in relation to Tribal
- Study the influence of Type on Numfixsub
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Liz Friedman 
Start A New Notebook!"	62	480	11	yamqwe	us-law-enforcement-locationse
1475	1475	Work and Employment	200 collected samples, technical information	['employment', 'social science', 'beginner', 'social issues and advocacy', 'social networks']	"About this dataset
&gt; <p>Work and employment</p>
<p>Methodology - Other</p>
<p>Source: <a href=""https://data.humdata.org/dataset/work-and-employment"" target=""_blank"" rel=""nofollow"">https://data.humdata.org/dataset/work-and-employment</a><br>
Last updated at <a href=""https://data.humdata.org/organization/undp-human-development-reports-office"" target=""_blank"" rel=""nofollow"">https://data.humdata.org/organization/undp-human-development-reports-office</a> : 2021-09-23</p>
<p>License -<br>
Creative Commons Attribution for Intergovernmental Organisations</p>
This dataset was created by Humanitarian Data Exchange and contains around 200 samples along with Output Per Worker (2011 Ppp $) 2005 2012, Long Term Unemployment (% Of Labour Force) 2008 2013, technical information and other features such as:
- Location
- Labour Force Participation Rate (% Ages 15 And Older) 2013
- and more.
How to use this dataset
&gt; - Analyze Labour Force With Tertiary Education (%) 2007 2012 in relation to Vulnerable Employment (% Of Total Employment) 2008 2013
- Study the influence of Employment In Agriculture (% Of Total Employment) 1990 on Employment In Services (% Of Total Employment) 1990
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Humanitarian Data Exchange"	708	4617	22	yamqwe	work-and-employmente
1476	1476	"🏫 Teens Poll: ""Are you getting a good education?"""	"Wide poll of teens answering the question ""Are you getting a good education?"""	['education', 'primary and secondary schools']	"About this dataset
&gt; <p>Over 54,000 high school students responded to a recent one-day poll on the After School App that asked, “Are you getting a good education?” Nearly 58% said yes, while 42% said no.</p>
<p>Students were also asked why they felt they either were or were not getting a good education in their current high school. State by state information and breakdown are provided in the attached data.</p>
This dataset was created by After School and contains around 0 samples along with Percent No, Percent Yes, technical information and other features such as:
- Total
- Percent No
- and more.
How to use this dataset
&gt; - Analyze Percent Yes in relation to Total
- Study the influence of Percent No on Percent Yes
- More datasets
Acknowledgements
If you use this dataset in your research, please credit After School 
Start A New Notebook!"	24	270	5	yamqwe	teen-education-polle
1477	1477	Adolescent Births	California's adolescent birth rate by county, age group and race/ethnicity	['public health', 'health', 'social science', 'demographics']	"About this dataset
&gt; <p>This dataset contains California’s adolescent birth rate (ABR) by county, age group and race/ethnicity using aggregated years 2014-2016. The ABR is calculated as the number of live births to females aged 15-19 divided by the female population aged 15-19, multiplied by 1,000. Births to females under age 15 are uncommon and thus added to the numerator (total number of births aged 15-19) in calculating the ABR for aged 15-19. The categories by age group are aged 18-19 and aged 15-17; births occurring to females under aged 15 are added to the numerator for aged 15-17 in calculating the ABR for this age group. The race and ethnic groups in this table utilized five mutually exclusive race and ethnicity categories. These categories are Hispanic and the following Non-Hispanic categories of Multi-Race, Black, American Indian (includes Eskimo and Aleut), Asian and Pacific Islander (includes Hawaiian) combined, and White. Note that there are birth records with missing race/ethnicity or categorized as “Other” and not shown in the dataset but included in the ABR calculation overall.</p>
<p>Source: <a href=""https://www.cdph.ca.gov/Programs/CFH/DMCAH/Pages/Data/Adolescent-Health-Data.aspx"" target=""_blank"" rel=""nofollow"">https://www.cdph.ca.gov/Programs/CFH/DMCAH/Pages/Data/Adolescent-Health-Data.aspx</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2020-01-13<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 500 samples along with Strata Name, Births, technical information and other features such as:
- Population
- Upper
- and more.
How to use this dataset
&gt; - Analyze Strata in relation to Lower
- Study the influence of Abr on County
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	17	211	6	yamqwe	adolescent-birthse
1478	1478	Which Social Media Millennials Care About Most?	Data collected from Whatsgoodly about millennial social networks	['business', 'social science', 'online communities', 'social networks']	"About this dataset
&gt; <p>This data was collected by Whatsgoodly, a millennial social polling company.</p>
<p>It was published by Brietbart on 3/17/17.</p>
<p>Link to article here: <a href=""http://www.breitbart.com/tech/2017/03/17/report-snapchat-is-most-important-social-network-among-millennials/"" target=""_blank"" rel=""nofollow"">http://www.breitbart.com/tech/2017/03/17/report-snapchat-is-most-important-social-network-among-millennials/</a></p>
This dataset was created by Adam Halper and contains around 500 samples along with Segment Type, Count, technical information and other features such as:
- Segment Description
- Answer
- and more.
How to use this dataset
&gt; - Analyze Percentage in relation to Question
- Study the influence of Segment Type on Count
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Adam Halper 
Start A New Notebook!"	162	1479	9	yamqwe	which-social-media-millennials-care-about-moste
1479	1479	🍫 What is the UK's favorite chocolate	Dataset with Rank, Brand, Name, Category, Classification	['religion and belief systems']	"About this dataset
&gt; <p>The Easter Bunny is coming this week, so let's have a great debate! What is the UK's favorite chocolate?</p>
<h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/VKLPxYYFRyUeeoCc6Jgj_Screen%20Shot%202018-03-25%20at%2010.32.12%20am.png"" alt=""Image"" style=""""></p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE: <a href=""https://www.cda.eu/"" target=""_blank"" rel=""nofollow"">CDA</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
This dataset was created by Andy Kriebel and contains around 0 samples along with Rank, Brand, technical information and other features such as:
- Age Group
- Rank
- and more.
How to use this dataset
&gt; - Analyze Brand in relation to Age Group
- Study the influence of Rank on Brand
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	54	476	7	yamqwe	2018-w13-what-is-the-uk-s-favorite-chocolate-bare
1480	1480	✨ Disasters on Social Media	Dataset of 10,000 tweets culled with Google Data	['internet', 'online communities', 'social networks']	"About this dataset
&gt; <p>Contributors looked at over 10,000 tweets culled with a variety of searches like ""ablaze"", ""quarantine"", and ""pandemonium"", then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).   Added: September 4, 2015 by CrowdFlower | Data Rows: 10877 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 10000 samples along with Text, Location, technical information and other features such as:
- Trusted Judgments
- Unit State
- and more.
How to use this dataset
&gt; - Analyze Userid in relation to Choose One:confidence
- Study the influence of Choose One on Tweetid
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	145	1632	8	yamqwe	disasters-on-social-mediae
1481	1481	🚊 Consumer Price Index	The Consumer Price Index For All Urban Consumers: All Items (CPIAUCSL)	['business', 'finance', 'economics']	"About this dataset
&gt; <p>9The Consumer Price Index for All Urban Consumers: All Items (CPIAUCSL) is a measure of the <strong>average monthly change in the price for goods and services paid by urban consumers between any two time periods</strong>.(1) It can also represent the buying habits of urban consumers. This particular index includes roughly 88 percent of the total population, accounting for wage earners, clerical workers, technical workers, self-employed, short-term workers, unemployed, retirees, and those not in the labor force.(1)</p>
<p>The CPIs are based on prices for food, clothing, shelter, and fuels; transportation fares; service fees (e.g., water and sewer service); and sales taxes. Prices are collected monthly from about 4,000 housing units and approximately 26,000 retail establishments across 87 urban areas.(1) To calculate the index, price changes are averaged with weights representing their importance in the spending of the particular group. The index measures price changes (as a percent change) from a predetermined reference date.(1) In addition to the original unadjusted index distributed, the Bureau of Labor Statistics also releases a seasonally adjusted index. The unadjusted series reflects all factors that may influence a change in prices. However, it can be very useful to look at the seasonally adjusted CPI, which removes the effects of seasonal changes, such as weather, school year, production cycles, and holidays.(1)</p>
<p>The CPI can be used to recognize periods of inflation and deflation. Significant increases in the CPI within a short time frame might indicate a period of inflation, and significant decreases in CPI within a short time frame might indicate a period of deflation. However, because the CPI includes volatile food and oil prices, it might not be a reliable measure of inflationary and deflationary periods. For a more accurate detection, the core CPI (Consumer Price Index for All Urban Consumers: All Items Less Food &amp; Energy [CPILFESL]) is often used. When using the CPI, please note that it is not applicable to all consumers and should not be used to determine relative living costs.(1) Additionally, the CPI is a statistical measure vulnerable to sampling error since it is based on a sample of prices and not the complete average.(1)</p>
<p>Attribution: <a href=""https://fred.stlouisfed.org/series/CPIAUCSL/downloaddata"" target=""_blank"" rel=""nofollow"">US. Bureau of Labor Statistics from The Federal Reserve Bank of St. Louis</a></p>
<p>For more information on the consumer price indexes, see:</p>
<ul>
<li>(1) <a href=""http://www.bls.gov/cpi/"" target=""_blank"" rel=""nofollow"">Bureau of Economic Analysis. “CPI Detailed Report.” 2013</a></li>
<li>(2) <a href=""http://www.bls.gov/opub/hom/pdf/homch17.pdf"" target=""_blank"" rel=""nofollow"">Handbook of Methods</a></li>
<li>(3) <a href=""http://stats.bls.gov:80/cpi/cpifaq.htm"" target=""_blank"" rel=""nofollow"">Understanding the CPI: Frequently Asked Questions</a></li>
</ul>
This dataset was created by Finance and contains around 900 samples along with Consumer Price Index For All Urban Consumers: All Items, Title:, technical information and other features such as:
- Consumer Price Index For All Urban Consumers: All Items
- Title:
- and more.
How to use this dataset
&gt; - Analyze Consumer Price Index For All Urban Consumers: All Items in relation to Title:
- Study the influence of Consumer Price Index For All Urban Consumers: All Items on Title:
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	73	757	4	yamqwe	consumer-price-indexe
1482	1482	🇭 Hermes Stores Location 	Explore details of Hermes Store Locations in the US	['business', 'retail and shopping']	"About this dataset
&gt; <h3><strong>Hermes Store Location Data</strong></h3>
<p>Hermès, is a French high fashion luxury goods manufacturer and specializes in leather, lifestyle accessories, home furnishings, perfumery, jewellery, watches and ready-to-wear.</p>
<p>This is a complete list of all Hermes store locations, along with their geographic coordinates, Street addresses, City, State, ZIP code etc in the US.</p>
<h3><strong>Get data for free</strong></h3>
<p>Contact Datahut (<a href=""https://datahut.co/"" target=""_blank"" rel=""nofollow"">https://datahut.co/</a>) for more information and a fresh data set. We give this data for free for startups, journalists bloggers, researchers, analysts, etc.</p>
This dataset was created by Tony Paul and contains around 0 samples along with Pinterest, Email 2, technical information and other features such as:
- City
- Zip Code
- and more.
How to use this dataset
&gt; - Analyze Instagram in relation to Latitude
- Study the influence of Country on Fax 2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tony Paul 
Start A New Notebook!"	61	638	8	yamqwe	hermes-store-location-datae
1483	1483	☀️ Let's Check If The Climate Crisis Is Real!	Samples with Land Min Temperature, Land And Ocean Temperatures	['earth and nature', 'atmospheric science', 'weather and climate', 'regression']	"About this dataset
Climate change is the biggest threat of our time. 
However, some say it’s a myth based on dodgy science. 
Here is some data so you can form your own view.
<p>In this dataset, we have include several files:</p>
<p>＊ Global Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv):</p>
<pre><code>  Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures
  LandAverageTemperature: global average land temperature in celsius
  LandAverageTemperatureUncertainty: the 95% confidence interval around the average
  LandMaxTemperature: global average maximum land temperature in celsius
  LandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature
  LandMinTemperature: global average minimum land temperature in celsius
  LandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature
  LandAndOceanAverageTemperature: global average land and ocean temperature in celsius
  LandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature
</code></pre>
<p>＊ Other files include:</p>
<pre><code>  Global Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv)
  Global Average Land Temperature by State (GlobalLandTemperaturesByState.csv)
  Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)
  Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv)
</code></pre>
<p>Source: Kaggle</p>
<p><a href=""https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data"" target=""_blank"">https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data</a></p>
<p>Raw data: Berkeley Earth data page <a href=""http://berkeleyearth.org/data/"" target=""_blank"">http://berkeleyearth.org/data/</a></p>
This dataset was created by Data Society and contains around 3000 samples along with Land Min Temperature, Land And Ocean Average Temperature, technical information and other features such as:
- Land Max Temperature Uncertainty
- Land Max Temperature
- and more.
How to use this dataset
&gt; - Analyze Land Min Temperature Uncertainty in relation to Land Average Temperature Uncertainty
- Study the influence of Land And Ocean Average Temperature Uncertainty on Land Average Temperature
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	68	614	6	yamqwe	global-climate-change-datae
1484	1484	💉 Opioid Overdose Deaths	800 collected samples, technical information	['public health', 'health', 'drugs and medications', 'public safety', 'covid19']	"About this dataset
&gt; <p>Opioid addiction and death rates in the U.S. and abroad have reached ""epidemic"" levels. The CDC's data reflects the incredible spike in overdoses caused by drugs containing opioids.</p>
<blockquote>
<p>The United States is experiencing an epidemic of drug overdose (poisoning) deaths. Since 2000, the rate of deaths from drug overdoses has increased 137%, including a 200% increase in the rate of overdose deaths involving opioids (opioid pain relievers and heroin). <em>Source: <a href=""http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6450a3.htm"" target=""_blank"" rel=""nofollow"">CDC</a></em></p>
</blockquote>
<p><code>In-the-News</code>:</p>
<ul>
<li><em>STAT</em>: <a href=""https://www.statnews.com/2016/08/22/heroin-huntington-west-virginia-overdoses"" target=""_blank"" rel=""nofollow"">26 overdoses in just hours: Inside a community on the front lines of the opioid epidemic</a></li>
<li><em>NPR</em>: <a href=""http://www.npr.org/sections/health-shots/2016/10/14/497799446/organ-donations-spike-in-the-wake-of-the-opioid-epidemic"" target=""_blank"" rel=""nofollow"">Organ Donations Spike In The Wake Of The Opioid Epidemic</a>, <a href=""http://www.npr.org/sections/health-shots/2016/09/25/495052633/deadly-opioid-overwhelms-first-responders-and-crime-labs-in-ohio"" target=""_blank"" rel=""nofollow"">Deadly Opioid Overwhelms First Responders And Crime Labs in Ohio</a></li>
<li><em>Scientific American</em>: <a href=""https://www.scientificamerican.com/article/wave-of-overdoses-with-little-known-drug-raises-alarm-amid-opioid-crisis/"" target=""_blank"" rel=""nofollow"">Wave of Overdoses with Little-Known Drug Raises Alarm Amid Opioid Crisis</a></li>
<li><em>Washington Post</em>: <a href=""https://www.washingtonpost.com/news/to-your-health/wp/2016/10/05/a-7-year-old-told-her-bus-driver-she-couldnt-wake-her-parents-police-found-them-dead-at-home/"" target=""_blank"" rel=""nofollow"">A 7-year-old told her bus driver she couldn’t wake her parents. Police found them dead at home.</a></li>
<li><em>Wall Street Journal</em>: <a href=""http://www.wsj.com/articles/for-small-town-cops-opioid-scourge-hits-close-to-home-1475074699"" target=""_blank"" rel=""nofollow"">For Small-Town Cops, Opioid Scourge Hits Close to Home</a></li>
<li><em>Food &amp; Drug Administration</em>: <a href=""http://www.fda.gov/NewsEvents/Newsroom/PressAnnouncements/ucm520945.htm"" target=""_blank"" rel=""nofollow"">FDA launches competition to spur innovative technologies to help reduce opioid overdose deaths</a></li>
</ul>
<p>This data was compiled using the CDC's WONDER database. Opioid overdose deaths are defined as: deaths in which the underlying cause was drug overdose, and the ICD-10 code used was any of the following: T40.0 (Opium), T40.1 (Heroin), T40.2 (Other opioids), T40.3 (Methadone), T40.4 (Other synthetic narcotics), T40.6 (Other and unspecified narcotics).</p>
<p><strong>Age-adjusted rate of drug overdose deaths and drug overdose deaths involving opioids</strong><br>
<img src=""http://i.imgur.com/ObpzUKq.gif"" alt=""Opioid Death Rate"" style=""""><br>
<em>Source: <a href=""http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6450a3.htm"" target=""_blank"" rel=""nofollow"">CDC</a></em></p>
<p><strong>What are opioids?</strong><br>
Opioids are substances that act on opioid receptors to produce morphine-like effects. Opioids are most often used medically to relieve pain. Opioids include opiates, an older term that refers to such drugs derived from opium, including morphine itself. Other opioids are semi-synthetic and synthetic drugs such as hydrocodone, oxycodone and fentanyl; antagonist drugs such as naloxone and endogenous peptides such as the endorphins.[4] The terms opiate and narcotic are sometimes encountered as synonyms for opioid. <em>Source: <a href=""https://en.wikipedia.org/wiki/Opioid"" target=""_blank"" rel=""nofollow"">Wikipedia</a></em></p>
<p><code>contributors-wanted</code> See <a href=""https://data.world/health/opioid-overdose-deaths/discuss/4008"">comment</a> in Discussion</p>
<h1>Footnotes</h1>
<ul>
<li>The crude rate is per 100,000.</li>
<li>Certain totals are hidden due to suppression constraints. More Information: <a href=""http://wonder.cdc.gov/wonder/help/faq.html#Privacy"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/faq.html#Privacy</a>.</li>
<li>The population figures are briged-race estimates. The exceptions being years 2000 and 2010, in which Census counts are used.</li>
<li>v1.1: Added Opioid Prescriptions Dispensed by US Retailers in that year (millions).</li>
</ul>
<p><em><strong>Citation:</strong> Centers for Disease Control and Prevention, National Center for Health Statistics. Multiple Cause of Death 1999-2014 on CDC WONDER Online Database, released 2015. Data are from the Multiple Cause of Death Files, 1999-2014, as compiled from data provided by the 57 vital statistics jurisdictions through the Vital Statistics Cooperative Program. Accessed at <a href=""http://wonder.cdc.gov/mcd-icd10.html"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/mcd-icd10.html</a> on Oct 19, 2016 2:06:38 PM.</em></p>
<p><em><strong>Citation for Opioid Prescription Data:</strong> IMS Health, Vector One: National, years 1991-1996, Data Extracted 2011. IMS Health, National Prescription Audit, years 1997-2013, Data Extracted 2014. Accessed at <a href=""https://www.drugabuse.gov/about-nida/legislative-activities/testimony-to-congress/2016/americas-addiction-to-opioids-heroin-prescription-drug-abuse"" target=""_blank"" rel=""nofollow"">NIDA article linked (Figure 1)</a> on Oct 23, 2016.</em></p>
<p><em><strong>Data Use Restrictions:</strong></em><br>
<em>The Public Health Service Act (42 U.S.C. 242m(d)) provides that the data collected by the National Center for Health Statistics (NCHS) may be used only for the purpose for which they were obtained; any effort to determine the identity of any reported cases, or to use the information for any purpose other than for health statistical reporting and analysis, is against the law. Therefore users will:</em><br>
<em>Use these data for health statistical reporting and analysis only.<br>
For sub-national geography, do not present or publish death counts of 9 or fewer or death rates based on counts of nine or fewer (in figures, graphs, maps, tables, etc.).<br>
Make no attempt to learn the identity of any person or establishment included in these data.<br>
Make no disclosure or other use of the identity of any person or establishment discovered inadvertently and advise the NCHS Confidentiality Officer of any such discovery.</em><br>
<em>Eve Powell-Griner, Confidentiality Officer<br>
National Center for Health Statistics<br>
3311 Toledo Road, Rm 7116<br>
Hyattsville, MD 20782<br>
Telephone 301-458-4257 Fax 301-458-4021</em></p>
This dataset was created by Health and contains around 800 samples along with Crude Rate, Crude Rate Lower 95% Confidence Interval, technical information and other features such as:
- Year
- Deaths
- and more.
How to use this dataset
&gt; - Analyze Crude Rate Upper 95% Confidence Interval in relation to Prescriptions Dispensed By Us Retailers In That Year (millions)
- Study the influence of State on Crude Rate
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	160	901	7	yamqwe	opioid-overdose-deathse
1485	1485	House_price		[]		0	3	0	saivarshittha	house-price
1486	1486	Dog Names over Time	20000 collected samples, technical information	['people and society', 'social science', 'beginner', 'social issues and advocacy', 'social networks']	"About this dataset
&gt; <p>Dog Names over Time</p>
<p>Source: <a href=""https://data.muni.org/d/da3g-dkms"" target=""_blank"" rel=""nofollow"">https://data.muni.org/d/da3g-dkms</a><br>
Last updated at <a href=""https://data.muni.org/"" target=""_blank"" rel=""nofollow"">https://data.muni.org/</a> : 2019-01-18</p>
This dataset was created by City of Anchorage<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span> and contains around 20000 samples along with Count, Date, technical information and other features such as:
- Set
- Rank
- and more.
How to use this dataset
&gt; - Analyze Dogname in relation to Count
- Study the influence of Date on Set
- More datasets
Acknowledgements
If you use this dataset in your research, please credit City of Anchorage<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span>"	327	3231	26	yamqwe	dog-names-over-timee
1487	1487	Accidental Drug Related Deaths in Connecticut	List of accidental deaths associated with a drug overdose in Connecticut	['health', 'drugs and medications']	"About this dataset
&gt; <p>This dataset contains the list of each accidental death associated with a drug overdose in the state of Connecticut from 2012 to 2018. Deaths are grouped by age, race, ethnicity, and gender and by the types of drugs detected post-death.</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with County, City, technical information and other features such as:
- Is Hydrocodone
- Age
- and more.
How to use this dataset
&gt; - Analyze Race in relation to Is Methadone
- Study the influence of Is Heroin on Is Oxymorphone
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	152	1024	9	yamqwe	accidental-drug-related-deaths-in-connecticute
1488	1488	2016 Federal Firearms Licenses	Dataset of Federal Firearms Licenses	['united states', 'crime', 'law', 'public safety']	"About this dataset
&gt; <h1>Active Federal Firearms Licenses, 2016</h1>
<p>Most recent listing of businesses who currently hold a Federal Firearm License. These business are permitted to sell and manufacture firearms and ammunition. Licenses must be renewed every three years.</p>
<p><strong>DATA SOURCE</strong>: <a href=""https://www.atf.gov/resource-center/data-statistics"" target=""_blank"">U.S. Bureau of Alcohol, Tobacco and Firearms</a></p>
<hr>
<h3>Field names</h3>
<h3>LICENSE SEQUENCE NUMBER</h3>
<p>License Sequence Number</p>
<h3>BUSINESS PREMISE NAME</h3>
<p>Name of the Business Licensed to Sell or Manufacture Firearms</p>
<h3>LICENSE OWNER NAME</h3>
<p>Name of the License Owner</p>
<h3>PREMISE STREET ADDRESS</h3>
<p>Street Address of the Business Premise</p>
<h3>PREMISE CITY</h3>
<p>Business Premise City</p>
<h3>PREMISE STATE</h3>
<p>Business Premise State</p>
<h3>PREMISE ZIP CODE</h3>
<p>Business Premise Zip Code</p>
<h3>PHONE NUMBER</h3>
<p>Phone Number for Business Premise</p>
<h3>LICENSE TYPE</h3>
<p>[01] - Dealer in Firearms Other Than Destructive Devices (Includes Gunsmiths). [02] - Pawnbroker in Firearms Other Than Destructive Devices. [03] - Collector of Curios and Relics. [06] - Manufacturer of Ammunition for Firearms. [07] - Manufacturer of Firearms Other Than Destructive Devices. [08] - Importer of Firearms Other Than Destructive Devices. [09] - Dealer in Destructive Devices. [10] - Manufacturer of Destructive Devices. [11] - Importer of Destructive Devices.</p>
<h3>LICENSE TYPE DESCRIPTION</h3>
<p>Description of License Type</p>
<h3>LICENSE EXPIRE DATE CODE</h3>
<p>License expiration date codes are the last digit of the year and the month as it corresponds the alphabet. (i.e. A = January, B = February, C = March, L = December. etc,.)</p>
<h3>MAIL STREET</h3>
<p>Street of Mailing Address</p>
<h3>MAIL CITY</h3>
<p>City of Mailing Address</p>
<h3>MAIL STATE</h3>
<p>State of Mailing Address</p>
<h3>MAIL ZIP CODE</h3>
<p>Zip Code of Mailing Address</p>
<h3>LICENSE REGION</h3>
<p>License Region</p>
<h3>LICENSE DISTRICT</h3>
<p>License District</p>
<h3>LICENSE COUNTY</h3>
<p>License County</p>
<h3>SERIAL ID</h3>
<p>Serial ID</p>
This dataset was created by Carl V. Lewis and contains around 2600000 samples along with License State, Additional License Type, technical information and other features such as:
- Establishment Address Street1
- Establishment Address State
- and more.
How to use this dataset
&gt; - Analyze Establishment Address Zip in relation to Licensee Address City
- Study the influence of Establishment Address Full on Licensee Address Street2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Carl V. Lewis 
Start A New Notebook!"	21	191	4	yamqwe	2016-federal-firearms-licensese
1489	1489	🏦 Predict the Success of Bank Marketing Campaigns	Bank-marketing_data.world_investing.csv	['business', 'finance', 'banking', 'classification']	"About this dataset
&gt; <p><em>Source</em> on <a href=""https://data.world/uci/bank-marketing"">data.world</a></p>
<p><strong>Source:</strong></p>
<p>[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014</p>
<p><strong>Data Set Information:</strong></p>
<p>The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.</p>
<p>There are four datasets:</p>
<p>bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]<br>
bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.<br>
bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs).<br>
bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).<br>
The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).</p>
<p><strong>Attribute Information:</strong></p>
<p>Input variables:</p>
<h1>bank client data:</h1>
<p>1 - age (numeric)<br>
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br>
5 - default: has credit in default? (categorical: 'no','yes','unknown')<br>
6 - housing: has housing loan? (categorical: 'no','yes','unknown')<br>
7 - loan: has personal loan? (categorical: 'no','yes','unknown')</p>
<h1>related with the last contact of the current campaign:</h1>
<p>8 - contact: contact communication type (categorical: 'cellular','telephone')<br>
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>
11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>
#other attributes: 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>
14 - previous: number of contacts performed before this campaign and for this client (numeric)<br>
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')</p>
<h1>social and economic context attributes</h1>
<p>16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)<br>
17 - cons.price.idx: consumer price index - monthly indicator (numeric)<br>
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)<br>
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)<br>
20 - nr.employed: number of employees - quarterly indicator (numeric)</p>
<p>Output variable (desired target):<br>
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')</p>
<p>For more information about <strong>Relevant Papers and Papers That Cite This Data Set,</strong><br>
please go <a href=""https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"" target=""_blank"" rel=""nofollow"">here</a></p>
<p><strong>Citation Policy:</strong></p>
<p>If you publish material based on databases obtained from this repository, then, in your acknowledgements, please note the assistance you received by using this repository. This will help others to obtain the same data sets and replicate your experiments. We suggest the following pseudo-APA reference format for referring to this repository:</p>
<p>Lichman, M. (2013). UCI Machine Learning Repository [<a href=""http://archive.ics.uci.edu/ml"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p>
<p>Here is a BiBTeX citation as well:</p>
<p><a href=""/misc"" target=""_blank"" rel=""nofollow"">@misc</a>{Lichman:2013 ,<br>
author = ""M. Lichman"",<br>
year = ""2013"",<br>
title = ""{UCI} Machine Learning Repository"",<br>
url = ""<a href=""http://archive.ics.uci.edu/ml"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml</a>"",<br>
institution = ""University of California, Irvine, School of Information and Computer Sciences"" }</p>
<p>A few UCI data sets have additional citation requests. These requests can be found on the bottom of each data set's web page.</p>
This dataset was created by IBM Watson AI XPRIZE - AI and contains around 5000 samples along with Month"""", Job"""", technical information and other features such as:
- Previous""""
- Marital""""
- and more.
How to use this dataset
&gt; - Analyze Education"""" in relation to Poutcome""""
- Study the influence of Housing"""" on Loan""""
- More datasets
Acknowledgements
If you use this dataset in your research, please credit IBM Watson AI XPRIZE - AI 
Start A New Notebook!"	143	1388	5	yamqwe	bank-marketinge
1490	1490	✍ 200,000+ Jeopardy! Questions	Data on over 200000 samples of Jeopardy! questions	['arts and entertainment']	"About this dataset
&gt; <p>This Dataset was found in the subreddit forum <a href=""https://www.reddit.com/r/datasets/top/?sort=top&amp;t=all"" target=""_blank"" rel=""nofollow"">/datasets</a> and it was compiled by user <a href=""https://www.reddit.com/user/trexmatt"" target=""_blank"" rel=""nofollow"">trexmatt</a> in 2014. The data covers questions up to the year 2012.</p>
<p>'category' : the question category, e.g. ""HISTORY""<br>
'value' : $ value of the question as string, e.g. ""$200""<br>
Note: This is ""None"" for Final Jeopardy! and Tiebreaker questions<br>
'question' : text of question<br>
Note: This sometimes contains hyperlinks and other things messy text such as when there's a picture or video question<br>
'answer' : text of answer<br>
'round' : one of ""Jeopardy!"",""Double Jeopardy!"",""Final Jeopardy!"" or ""Tiebreaker""<br>
Note: Tiebreaker questions do happen but they're very rare (like once every 20 years)<br>
'show_number' : string of show number, e.g '4680'<br>
'air_date' : the show air date in format YYYY-MM-DD</p>
<p>Source: <a href=""https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/"" target=""_blank"" rel=""nofollow"">Reddit</a></p>
This dataset was created by Selene Arrazolo and contains around 200000 samples along with Air Date, Round, technical information and other features such as:
- Show Number
- Category
- and more.
How to use this dataset
&gt; - Analyze Answer in relation to Value
- Study the influence of Question on Air Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	27	311	9	yamqwe	200000-jeopardy-questionse
1491	1491	Alcohol Related Deaths in the UK 1994 To 2016	Age-standardized and age-specific alcohol-related death rates in the UK	['alcohol', 'health']	"About this dataset
&gt; <p>This dataset includes information on age-standardized and age-specific alcohol-related death rates in the UK, its constituent countries and regions of England, deaths registered from 1994 to 2016.</p>
<h2>COMMERCIAL LICENSE</h2>
<p>For subscribing to a <strong>commercial license</strong> for John Snow Labs Data Library which includes all datasets curated and maintained by John Snow Labs please visit <a href=""https://www.johnsnowlabs.com/marketplace"" target=""_blank"" rel=""nofollow"">https://www.johnsnowlabs.com/marketplace</a>.</p>
This dataset was created by John and contains around 0 samples along with Deaths, Region Geography Code, technical information and other features such as:
- Year
- Rate Per 100000 Persons
- and more.
How to use this dataset
&gt; - Analyze Gender in relation to Region Of England
- Study the influence of Deaths on Region Geography Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit John 
Start A New Notebook!"	162	996	9	yamqwe	alcohol-related-deaths-in-the-uk-1994-to-2016e
1492	1492	⛏ Agricultural Minerals	Dataset represents agricultural minerals operations in the United States.	['business', 'agriculture']	"About this dataset
&gt; <p>This dataset represents agricultural minerals operations in the United States. The data represent commodities covered by the <strong>Minerals Information Team (MIT) of the U.S. Geological Survey.</strong> The mineral operations are plants and (or) mines surveyed by the MIT and considered currently active in 2003. These data are intended for geographic display and analysis at the national level, and for large regional areas. The data should be displayed and analyzed at scales appropriate for 1:2,000,000-scale data.</p>
<p><a href=""https://hifld-dhs-gii.opendata.arcgis.com/datasets/24dcba08b3bb4af593cfec39be34f55a_0"" target=""_blank"" rel=""nofollow"">SOURCE</a></p>
This dataset was created by Homeland Infrastructure Foundation and contains around 200 samples along with Company Na, Y, technical information and other features such as:
- County
- Site Name
- and more.
How to use this dataset
&gt; - Analyze Commodity in relation to Plant Min
- Study the influence of Latitude on State Loca
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Homeland Infrastructure Foundation 
Start A New Notebook!"	28	251	2	yamqwe	agricultural-mineralse
1493	1493	📰 How Good Are Teens At Spotting Fake News?	Has positive and negative news stories about fake news coverage	['education', 'politics', 'primary and secondary schools', 'news']	"About this dataset
&gt; <p><em>Great write-up by Katie Notopoulos of BuzzFeed News</em>: <a href=""https://www.buzzfeed.com/katienotopoulos/heres-what-teens-think-about-fake-news?utm_term=.ggD729ppn#.dkKgaAeeQ"" target=""_blank"" rel=""nofollow"">Teens Think They're Really Good At Spotting Fake News</a></p>
<p>After School, the largest teen-focused social network, surveyed its users on the issue of fake news.  Over several days, tens of thousands of high school students in all 50 states participated in the poll.</p>
<p>Questions asked include:</p>
<ul>
<li>
<p>Have you heard of “fake news stories?""</p>
</li>
<li>
<p>Are you good at spotting fake news stories?</p>
</li>
<li>
<p>If you see someone someone sharing fake news stories, what do you do?</p>
</li>
<li>
<p>Do you believe most news articles are true?</p>
</li>
<li>
<p>If a  news article wasn't telling the truth, do you think you could tell?</p>
</li>
</ul>
<p>Out of the more than 39,000 students who answered the question ""have you heard of fake news stories?"" 21% of teens had never heard of it. For full results, please look through the data below.</p>
<h1>Data slices</h1>
<p><em>Click links below to see survey results broken out and subset in interesting ways</em></p>
<ul>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/93e6bc36-6737-47d5-99a6-7b403941de9a"">% selecting each response, by state</a></p>
</li>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/a7c6142c-2f63-4f2c-8dc8-fca4476c2e57"">% likely to call out fake news sharers, ranked</a></p>
</li>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/0f7ba89d-95e5-4fd2-87ba-e5f415b162ed"">% selecting each response, by whether electoral college votes went to Clinton or Trump in respondents' state</a></p>
</li>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/30a71ad4-67ef-437c-9cc3-22e7a4543ff1"">Compare any 2 states by changing the values in line 4</a></p>
</li>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/143ed24f-8f1d-441c-9c69-a7f857b14ccc"">10 most media-trusting states</a></p>
</li>
<li>
<p><a href=""https://data.world/afterschool/teen-fake-news-poll-on-after-school/query/929c5a09-0af1-4573-8264-6fd802216984"">10 least media-trusting states</a></p>
</li>
</ul>
<h1>Screenshots</h1>
<p><img src=""https://media.data.world/NjfnCUgmQtjgbZp3BX64_Q1.png"" alt=""Q1.png"" style=""""></p>
<p><img src=""https://media.data.world/wFiE1eF3Q12yJ5dsPGLS_Q2.png"" alt=""Q2.png"" style=""""></p>
<p><img src=""https://media.data.world/aRHALJCwTmnSTvJQqxFB_Q3.png"" alt=""Q3.png"" style=""""></p>
This dataset was created by After School and contains around 30000 samples along with City, Total Events, technical information and other features such as:
- Event Category
- City
- and more.
How to use this dataset
&gt; - Analyze Total Events in relation to Event Category
- Study the influence of City on Total Events
- More datasets
Acknowledgements
If you use this dataset in your research, please credit After School 
Start A New Notebook!"	74	764	14	yamqwe	teen-fake-news-poll-on-after-schoole
1494	1494	🎰 Predicting the lottery with machine learning	Predicting the future of the lottery from lottery data	['games', 'computer science', 'signal processing', 'feature engineering', 'regression']	"About this dataset
&gt; <h2>Context</h2>
<p>Winning the lottery has always been a dream for a lot of people. Because of that, a lot of work have been done in the past to try and tackle the challenge.<br>
We saw the rise of different approaches, from computer softwares that optimize your lottery picks to numerical and statistical analysis, as well as esoteric approaches.</p>
<p>More recently, with the rise of <em>machine learning</em> (<em>hereafter: <strong>ML</strong></em>), people have tried to tackle the problem by trying to have a ML model predict the next combination.<br>
However, most of those who undertook to try this approach had poor understanding of ML-related basics such as data preprocessing and signal processing.</p>
<blockquote>
<p>As an example of bad things that were done, there were projects that built a ML model that took raw draw results and outputted N numbers, hoping that those N numbers would be the correct combination of the next draw.<br>
To give you an analogy with a  real world example, it would be like taking in the raw data (bytes) of an audio file as the (sole) input of a model and hope that it outputs a concept like <em>bpm</em>, <em>music genre</em>, ...<br>
Of course, this resulted in extremely bad results and <strong>ML models that didn't learn a single thing</strong>.<br>
Similarly, it would be the same as trying to predict stock prices using only the price as the sole input variable. It would be bound to fail without creating higher-level features. Models that do not do that would never succeed.</p>
</blockquote>
<p>Because Machine-Learning-based approaches <em>were bound</em> to fail before even beginning <em>unless something was done</em> regarding data and signal processing, I decided to make my contribution by crafting higher-level features (/ abstract concepts) from historic lottery data.</p>
<p>I leave under mouth discussions about the mathematical theory of probability (which I explained more in <a href=""https://github.com/JeffMv/Lofea"" target=""_blank"" rel=""nofollow"">the repository of Lofea</a>, a project I created to generate this dataset) or why mathematicians say it would be theoretically impossible to predict. Those specifics and other questions can be discussed in the comments section.<br>
To let people still dream enough to try and tackle the problem, I'd like to point out that stock market prices (which are also numerical time series data) are said to be unpredictable due to the <a href=""https://en.wikipedia.org/wiki/Efficient-market_hypothesis"" target=""_blank"" rel=""nofollow"">Efficient Market Hypothesis</a>. Regardless, firms and individuals have been trying their best to try and predict the evolution of stock prices. Although theories tell us something is impossible in theory, there might be a practical implementation flaw that might get exploited if studied carefully enough. Who knows unless they try ?</p>
<h2>Content</h2>
<p>Preprocessed historical results.</p>
<p><strong>Tackling a big and complex task</strong> often requires problem solving methodologies, such as <em>divide and conquer</em>. This is why instead of tackling a regular pick 6 among 49 or so lottery, this dataset focuses on simple 1/10 lottery data (i.e. pick 1 among 10). But it also includes a version for the Euromillions (a 5/50 lottery).</p>
<p>You will find in the archive <code>features.04-2021</code> files containing computed features, as well as the whole draw histories used to compute them.<br>
One lottery is the <a href=""https://www.fdj.fr/jeux-de-tirage/euromillions-my-million/resultats"" target=""_blank"" rel=""nofollow"">Euromillions</a>, and the other is  <a href=""https://jeux.loro.ch/games/magic3"" target=""_blank"" rel=""nofollow"">TrioMagic</a>, though similar datasets can be crafted for lotteries that share their respective formats.<br>
Regarding <em>1/10</em> lottery, since most of <em>1/10</em> lotteries have several <em>pools</em> (/columns) from which one has to pick, this kind of dataset with higher-level features can be created for each column individually and compared among different lotteries.</p>
<h3>Preprocessing</h3>
<p>The big idea here is to preprocess historic draws as if it was a <em>signal</em> or a <em>time serie</em> and create higher-level features based on it.<br>
Inspired by the approach of working with numerical time series signals such as stock market prices.</p>
<h3>Labels (referred to as ""<em>Targets</em>"" in the dataset)</h3>
<p>There are several high-level concepts we may want to predict, such as the parity of the next draw. This would be a <em>classification problem</em>. You may also choose to tackle a <em>regression</em> problem, such as trying to predict the repartition of rate of even numbers in the next N draws (for instance N=2, 3 or 5).<br>
There are also other possible targets besides the parity, such as the <em>Universe length</em>, which will be described below.</p>
<p>The target you choose to predict may influence what kind of features you will try to include or craft.</p>
<h3>Features</h3>
<p>Several of the features included in this dataset are based on a concept I came up with I called « <em><strong>Universe Length</strong></em> ».<br>
Basically, <em>Universe Length</em> (referred to as <code>ULen</code> in the dataset) is the number of different numbers in a given time frame.<br>
For instance in a 1/10 lottery, <em>Universe Length</em> over a <em><strong>time frame</strong> of 10 draws</em> with the following draw history <code>[3,4,1,4,9,5,5,9,8,1]</code> would be <code>6</code> since there are 6 different numbers drawn in this running window frame.</p>
<p>Similarly, there are other features that are based on a <strong>running window <code>frame</code></strong>.<br>
For the lotteries 5/50 and 1/10, setting the running window to 10 was a reasonable choice. (For the stars of Euromillions, which is a 2/12, the runnig window <code>frame</code> was set to 6).</p>
<h4><strong>Approach - Features of draws</strong></h4>
<p>There are different kind of approaches. Among them, one approach is to make statistics and features related to each ball. And another approach is to study the characteristics of draws (instead of individual balls).<br>
In this dataset (or at least the early version of it) the chosen approach was the latter: studying draws.</p>
<p>The names of features and targets of this dataset have been chosen for the clarity and unicity (in spite of shortness). Thus they are quite verbose, so feel free to rename them when you get used to them.<br>
Their verbosity allows distinguishing between variations around a same concept, or variations of running window or such...</p>
<p><strong>Explanation of features:</strong></p>
<ul>
<li>
<p><code>universe-length</code> : Number of <em>different</em> numbers in the current running window. (Always see the file's description to know the applied running window <code>frame</code>).</p>
</li>
<li>
<p><code>universe-length-offset-from-center</code> : based on <code>universe-length</code>. It just shows the distance to the center of all possible universe length values. Note that just because a universe length is possible does not mean it will ever occur. For instance, the lowest possible universe length would mean that only the same ball/set of drawn number is drawn over and over again within the running window. Although this could theoretically happen, its low probability makes it impossible.</p>
</li>
<li>
<p><code>parity</code> : Number of even numbers in the <em>current</em> draw</p>
</li>
<li>
<p><code>parity-over-frame</code> : Number of even numbers in the draws of the <em>running window</em></p>
</li>
<li>
<p><code>last-moving-direction-of-universe-length</code> : Las moving direction of the <code>universe-length</code> feature. -1 means decreasing, +1 means increasing.</p>
</li>
<li>
<p><code>move-balance-of-universe-length_latest-minus-mean</code> : formula: The mean of universe lengths over the running window, minus the current value of the <code>universe length</code></p>
</li>
<li>
<p><code>move-balance-of-universe-length_mean-minus-earliest</code> : formula: Current value of the <code>universe length</code> minus the mean of universe lengths over the running window</p>
</li>
<li>
<p><code>move-balance-of-universe-length_latest-minus-mean_runningWindowX2</code>: same as the other one, but the mean is applied on a frame twice bigger.</p>
</li>
<li>
<p><code>move-balance-of-universe-length_mean-minus-earliest_runningWindowX2</code>: same concept as the above feature</p>
</li>
<li>
<p><code>universe-length-drop</code> : How much the universe length can at most drop in the next draw's result</p>
</li>
<li>
<p><code>universe-length-increase</code> : How much the universe length can at most increase in the next draw's result</p>
</li>
<li>
<p><code>universe-length-repetition-same</code> : the number of times the current <code>universe length</code> has been repeated successively</p>
</li>
<li>
<p><code>greater-universe-length-than-repetition</code> : the number successive times we find a higher <code>universe length</code> than the current one over the running window</p>
</li>
<li>
<p><code>universe-length-didfollowincrease</code> : how much did the universe length increase <em>from the previous</em> draw. 0 means no increase or decrease.</p>
</li>
<li>
<p><code>mean-frequency-of-drawn-numbers</code> : the mean frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.</p>
</li>
<li>
<p><code>median-frequency-of-drawn-numbers</code> : the median frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.</p>
</li>
<li>
<p><code>mean-frequency-of-drawn-numbers-over-X-draws</code> : same as <code>mean-frequency-of-drawn-numbers</code> but over a running window of <code>X</code> draws.</p>
</li>
<li>
<p><code>median-frequency-of-drawn-numbers-over-X-draws</code> : same as <code>median-frequency-of-drawn-numbers</code> but over a running window of <code>X</code> draws.</p>
</li>
<li>
<p><code>mean-gap-of-drawn-numbers</code> : the mean appearance gap of each the drawn balls. Takes the individual gaps of each of the balls in the current result draw (i.e. the number of draws between the last time a given ball was drawn), sums them up, and then divides by the number of balls in the draw.</p>
</li>
<li>
<p><code>median-gap-of-drawn-numbers</code> : like <code>mean-gap-of-drawn-numbers</code> but using the median instead of the mean. Note that in 1/N lottery pools (one ball pulled out of N), the two features are always equal, which is logic.</p>
</li>
<li>
<p><code>mean-gap-of-drawn-numbers-bounded-at-X-draws</code> : where <code>X</code> is a number. Same as its counterpart <code>mean-gap-of-drawn-numbers</code> but over a running window of <code>X</code> draws.</p>
</li>
<li>
<p><code>median-gap-of-drawn-numbers-bounded-at-X-draws</code> : similar to the previous one, but using the median.</p>
</li>
<li>
<p><code>mean-of-4-gaps-of-each-drawn-numbers</code> : formula : systematically take the last 4 gaps of each of the balls in the current draw. Sum all these gaps together and take the overall mean. (i.e. average of all those gaps)</p>
</li>
<li>
<p><code>median-of-4-gaps-of-each-drawn-numbers</code> : same formula as its paronym, but takes the median of all. For 1/N lotteries, both are equal.</p>
</li>
<li>
<p><code>median-of-means-of-X-gaps-of-each-drawn-numbers</code> : where <code>X</code> is a number. Formula : systematically take the last X gaps of each of the balls of the current draw. Take the mean for each ball. Then sum up all the means together and take the median of them. In short, it is a median( of average( of [X-latest-gaps-of-symbol-Y] )). A median of averages may not make much sense statistically speaking. This feature is provided as is.</p>
</li>
</ul>
<p><strong>TARGET columns</strong> (i.e. supervised learning)<br>
(i.e. ideas of what you might want to predict)</p>
<ul>
<li><code>target_universe-length-willFollowIncrease</code> : TARGET feature (supervised learning). Same as the <code>universe-length-didfollowincrease</code> feature but for the draw that comes in the future. (DO NOT mistake them). One goal can be to predict this value given only the other features. Was named <code>universe-length-willfollowincrease</code> in an earlier version of the dataset.</li>
<li><code>target_coming-universe-length-change-in-next-draw</code> : same as <code>target_universe-length-willFollowIncrease</code> but indicates decreases as well as increases and stagnations. Might be better suited for a regression, but you do as you see fit.</li>
<li><code>target_coming-mean-universe-length-change-in-next-2-draws</code> : takes the mean change of universe length in the next 2 draws instead of only one. See <code>target_coming-universe-length-change-in-next-draw</code>.</li>
<li><code>target_future-1rst-value-of-universe-length-center-offset-from-center</code> : the next value of the feature <code>universe-length-offset-from-center</code>. You may want to try to predict that.</li>
<li><code>target_coming-universe-length-center-offset-change-in-next-draw</code> : the relative change of the feature <code>universe-length-offset-from-center</code> that will occur in the next draw.</li>
<li><code>target_coming-mean-universe-length-center-offset-change-in-next-2-draws</code> : same as the previous feature, but we take the mean change in the next 2 draws.</li>
</ul>
<p><strong>Special columns:</strong></p>
<ul>
<li><code>date</code> : date of the lottery draw</li>
<li><code>draw</code> / <code>draw-result</code> : most recent draw of the specified date. displayed for convenience</li>
<li><code>running-window-frame-length</code> : convenience a CONSTANT column. it is there to remind you of the base <em>running window</em> size (also called <em>frame length</em>) used to compute most features. When some features use a given multiple of the <em>frame length</em>, this is the value that gets multiplied.</li>
<li><code>draw-id</code> : the draw identifier. It can be the date or in another form (such as reversed date <code>yyyymmdd</code>). Only displayed for convenience</li>
</ul>
<h2>Inspiration / Ideas of approaches</h2>
<ul>
<li>Transforming the problem into another that outputs binary values.<br>
This would allow you/us to tap into the enormity of works and theorems done for <em><strong>binomial problems</strong></em> both in statistics and probability.<br>
It would allow you to compare the distribution of the randomness to much more scholarly examples (such as heads or tails) and deduce the law of probability / probability distribution / random variable behind a particular lottery.</li>
</ul>
<blockquote>
<p>If I remember correctly a maths course, there should be a <strong>theorem / lemme</strong> in probability that say how likely a binomial distribution is to take exaggerated values (big outliers). Regardless, there are a lot of results already, such as <strong>Bernoulli trial</strong> to study and compare theoretical and practical results.</p>
</blockquote>
<ul>
<li>Treating the problem as a numerical time series problem</li>
<li>Treating the problem as a signal processing problem</li>
<li>Asking different high-level questions and formulating the problem differently. For instance
<ul>
<li>Trying to predict the repartition of a feature (like parity) in the next 5 draws (instead of only predicting the parity of the next draw)</li>
</ul>
</li>
<li>Trying to identify critical points in time where several features converge towards predicting the same thing.<br>
For instance, there could be a time where the parity of the last 10 drawn numbers was even. If on top of that several even numbers have had a particularly high appearance rate in a preceding big frame of time, then these two features would lean towards thinking that there is a higher chance of seeing an odd number drawn next. (Though probability independence would say that such convergence are meaningless, here we are to suppose the opposite until we face the hard truth, as in a proof by contradiction).<br>
See the note about <strong>Bernoulli trials</strong>-related <em>theorem / lemme</em> as mentioned above.</li>
</ul>
<h2>Acknowledgements</h2>
<h2>License</h2>
<p>Dataset licensed under <a href=""https://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">CC-BY</a></p>
<p>Personal project for generating the dataset: <a href=""https://github.com/JeffMv/Lofea"" target=""_blank"" rel=""nofollow"">Lofea</a> under <a href=""https://creativecommons.org/licenses/by-nc-sa/4.0/"" target=""_blank"" rel=""nofollow"">CC-BY-NC-SA</a></p>
This dataset was created by Jeffrey Mvutu Mabilama and contains around 600 samples along with Ft U Len Over10 Short Moving Direction Balance, Pred Will Follow Increase Capacity, technical information and other features such as:
- Ft U Len Over10 Lower Than Serie
- Pred2nd Next
- and more.
How to use this dataset
&gt; - Analyze Ft U Len Over10 Last Moving Direction in relation to Ft U Len Over10 Can Decrease Of
- Study the influence of Ft U Len Over10 Same Value Serie on Target Trend
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Jeffrey Mvutu Mabilama 
Start A New Notebook!"	114	1762	17	yamqwe	lottery-features-time-series-machine-learninge
1495	1495	🌯 Restaurants That Sell Burritos and Tacos 	A list of 19,439 Restaurants and similar businesses with menu items.	['business', 'marketing', 'cooking and recipes', 'food', 'restaurants']	"About this dataset
&gt; <h1>About this Data</h1>
<p>This is a list of 19,439 restaurants and similar businesses with menu items containing ""burrito"" or ""taco"" in their names provided by <a href=""https://datafiniti.co/products/business-data"" target=""_blank"" rel=""nofollow"">Datafiniti's Business Database</a>.</p>
<p>The dataset includes the category, cuisine, restaurant information, and more for a menu item. Each row corresponds to a single menu item from the restaurant, and the entirety of each restaurant's menu is not listed. Only burrito or taco items are listed.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with this Data</h1>
<p>You can use this data to discover which parts of the country <a href=""https://datafiniti.co/tacos-vs-burritos/"" target=""_blank"" rel=""nofollow"">offer the most for Mexican food aficionados</a>. E.g.:</p>
<ul>
<li>What is the ratio of burritos and tacos on restaurant menus from each city?</li>
<li>What is the ratio of burritos and tacos on restaurant menus from cities with the most restaurants per capita (10,000 residents)?</li>
<li>What is the ratio of cities with the most authentic Mexican restaurants per capita (10,000 residents)?</li>
<li>Which cities have the most authentic Mexican restaurants?</li>
<li>Which cities have the most Mexican restaurants?</li>
<li>Which Mexican restaurants have the most locations nationally?</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/business-data-schema"" target=""_blank"" rel=""nofollow"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"" rel=""nofollow"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/business-data-pricing/"" target=""_blank"" rel=""nofollow"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"" rel=""nofollow"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 80000 samples along with Address, Name, technical information and other features such as:
- Id
- Websites
- and more.
How to use this dataset
&gt; - Analyze Price Range Max in relation to Menus.description
- Study the influence of Postal Code on Longitude
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	74	580	6	yamqwe	restaurants-that-sell-burritos-and-tacos-in-the-e
1496	1496	🪜 What corporations talk about on social media?	A dataset of corporate messaging 	['business', 'social networks']	"About this dataset
&gt; <p>A data categorization job concerning what corporations actually talk about on social media. Contributors were asked to classify statements as information (objective statements about the company or it's activities), dialog (replies to users, etc.), or action (messages that ask for votes or ask users to click on links, etc.).   Added: February 14, 2015 by CrowdFlower | Data Rows: 3118 Download Now</p>
<p>Source: <a href=""https://www.crowdflower.com/data-for-everyone/"" target=""_blank"" rel=""nofollow"">https://www.crowdflower.com/data-for-everyone/</a></p>
This dataset was created by CrowdFlower and contains around 3000 samples along with Text, Unit State, technical information and other features such as:
- Id
- Golden
- and more.
How to use this dataset
&gt; - Analyze Category Gold in relation to Last Judgment At
- Study the influence of Screenname on Trusted Judgments
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	82	1100	14	yamqwe	corporate-messaginge
1497	1497	Population by Country1980-2010	Total population by country, 1980 to 2010	['social science']	"About this dataset
&gt; <p><a href="""" target=""_blank"" rel=""nofollow"">https://catalog.data.gov/dataset/population-by-country-1980-2010-d0250</a></p>
<p>Total population (in millions) by country, 1980 to 2010.</p>
<p>Compiled by Energy Information Administration (EIA).</p>
This dataset was created by Noah Rippner and contains around 200 samples along with Year1992, Year1987, technical information and other features such as:
- Year1991
- Year1988
- and more.
How to use this dataset
&gt; - Analyze Year2001 in relation to Year1989
- Study the influence of Year1985 on Year1980
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Noah Rippner 
Start A New Notebook!"	112	510	7	yamqwe	population-by-country1980-2010e
1498	1498	📺 Hulu TV Shows and Movies	Explore and analyze data from thousands of Hulu shows	['arts and entertainment', 'movies and tv shows']	"About this dataset
&gt; <h1>Background</h1>
<p>In the market today, there are only a few competitors for Netflix. One of the top leaders in the streaming shows market is Hulu. Recently my family switched from Netflix to Hulu to try out the options and our experience wasn’t as wonderful as we would have expected. However, instead of giving my opinion, lets pull some data for Hulu shows and analyze the results so that you can make your own decision. For those interested in viewing my micro-research study that I did on Netflix, you can read my article Netflix Show Analysis.</p>
<h1>About the Data</h1>
<p>The data from Hulu’s shows were in a well-structured format. An example of the JSON object can be found at HuluShows. The 1,000 shows were sorted by “popular of all time” on Hulu. The raw data can be found at raw data.</p>
<h1>Sources</h1>
<p>Thanks to the micro-research study at <a href=""http://theconceptcenter.com/simple-research-study-hulu-show-analysis/"" target=""_blank"" rel=""nofollow"">The Concept Center</a> for performing this study.</p>
This dataset was created by Chase Willden and contains around 1000 samples along with Show/is Subscriber Only, Show/key Art Url, technical information and other features such as:
- Show/description
- Show/show Rollups/auth On Web/games Count
- and more.
How to use this dataset
&gt; - Analyze Show/annotations/0 in relation to Show/show Rollups/current/feature Films Count
- Study the influence of Show/show Rollups/free On Web/games Count on Show/show Rollups/showtime/html5 Videos Count
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	83	600	6	yamqwe	top-1000-most-popular-hulu-showse
1499	1499	🏫 Graphic Design Courses From Udemy	A collection of published courses from Udemy	['education', 'computer science']	"About this dataset
&gt; <h1>Background</h1>
<p>Udemy is a massive online open course (MOOC) web application. Within Udemy, a student can learn nearly anything. You may wonder, why would anyone take one of these courses? If you use Google’s Trends app, you can enter in different search terms and compare the world-wide volume of searches for that search term.</p>
<p>For example, I put in the terms, who, what, when, where, why and how. In addition, I furthered the comparison and added the terms, how to, what are, who is, why are, when do.</p>
<p>According to the trends on Google, obviously the worlds wants to know how to do things and this is exactly what Udemy does. It teaches people how to do things.</p>
<h1>Methodology</h1>
<p>I scraped the Udemy website and pulled many published courses for the topics of Graphic Design, Business Finance, Web Development and Musical Instruments.</p>
<h1>Source</h1>
<p>For the full study, see <a href=""http://theconceptcenter.com/simple-research-study-udemy-courses/"" target=""_blank"" rel=""nofollow"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 600 samples along with Num Subscribers, Published Time, technical information and other features such as:
- Instructional Level
- Unnamed: 12
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 11 in relation to Num Published Lectures
- Study the influence of Content Info on Is Paid
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	90	633	15	yamqwe	graphic-design-courses-from-udemye
1500	1500	🛟 Contraceptive Method Choice	1000 collected samples, technical information	['public health', 'health', 'social science', 'classification']	"About this dataset
&gt; <p>Dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey.# Source:<br>
Origin:<br>
This dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey<br>
Creator:<br>
Tjen-Sien Lim (limt '@' <a href=""http://stat.wisc.edu"" target=""_blank"" rel=""nofollow"">stat.wisc.edu</a>)<br>
Donor:<br>
Tjen-Sien Lim (limt '@' <a href=""http://stat.wisc.edu"" target=""_blank"" rel=""nofollow"">stat.wisc.edu</a>)</p>
<h1>Data Set Information:</h1>
<p>This dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The samples are married women who were either not pregnant or do not know if they were at the time of interview. The problem is to predict the current contraceptive method choice (no use, long-term methods, or short-term methods) of a woman based on her demographic and socio-economic characteristics.</p>
<h1>Attribute Information:</h1>
<ol>
<li>Wife's age           (numerical)  2. Wife's education        (categorical)   1=low, 2, 3, 4=high  3. Husband's education      (categorical)   1=low, 2, 3, 4=high  4. Number of children ever born  (numerical)  5. Wife's religion        (binary)      0=Non-Islam, 1=Islam  6. Wife's now working?      (binary)      0=Yes, 1=No  7. Husband's occupation      (categorical)   1, 2, 3, 4  8. Standard-of-living index    (categorical)   1=low, 2, 3, 4=high  9. Media exposure         (binary)      0=Good, 1=Not good  10. Contraceptive method used   (class attribute) 1=No-use, 2=Long-term, 3=Short-term</li>
</ol>
<h1>Relevant Papers:</h1>
<p>Lim, T.-S., Loh, W.-Y. &amp; Shih, Y.-S. (1999). A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-three Old and New Classification Algorithms. Machine Learning.  (  or  )</p>
<h1>Papers That Cite This Data Set1:</h1>
<p>Earl Harris Jr. Information Gain Versus Gain Ratio: A Study of Split Method Biases. The MITRE Corporation/Washington C. 2001.</p>
<ul>
<li>Soumya Ray and David Page. Generalized Skewing for Functions with Continuous and Nominal Attributes. Department of Computer Sciences and Department of Biostatistics and Medical Informatics, University of Wis.</li>
<li>Jos'e L. Balc'azar. Rules with Bounded Negations and the Coverage Inference Scheme. Dept. LSI, UPC.</li>
</ul>
<h1>Citation Request:</h1>
<p>Please refer to the Machine Learning <a href=""http://archive.ics.uci.edu/ml/citation_policy.html"" target=""_blank"" rel=""nofollow"">Repository's citation policy.</a><br>
[1] Papers were automatically harvested and associated with this data set, in collaborationwith <a href=""http://rexa.info/"" target=""_blank"" rel=""nofollow"">Rexa.info</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice</a></p>
This dataset was created by UCI and contains around 1000 samples along with 1.1, 2.1, technical information and other features such as:
- 2
- 3.1
- and more.
How to use this dataset
&gt; - Analyze 3 in relation to 1.2
- Study the influence of 24 on 1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	198	1754	12	yamqwe	contraceptive-method-choicee
1501	1501	💊 Drug Induced Deaths	Deaths, Population, Crude Rate, Unintentional Death Rates	['health', 'covid19']	"About this dataset
&gt; <p>This data was compiled using the CDC's <a href=""https://wonder.cdc.gov/"" target=""_blank"" rel=""nofollow"">WONDER database</a> using these parameters:</p>
<ul>
<li>Group By: State, Year</li>
<li>Measures: Deaths, Population, Crude Rate (95% Confidence Interval and Standard Error)</li>
<li>Underlying Cause of Death: UCD - Drug/Alcohol Induced Causes - Drug Induced Causes</li>
<li>Show Totals</li>
<li>Show Zero Values</li>
<li>Show Suppressed Values</li>
</ul>
<h2>Citation</h2>
<p>Centers for Disease Control and Prevention, National Center for Health Statistics. Multiple Cause of Death<br>
1999-2015 on CDC WONDER Online Database, released December, 2016. Data are from the Multiple Cause of Death Files, 1999-2015, as<br>
compiled from data provided by the 57 vital statistics jurisdictions through the Vital Statistics Cooperative Program. Accessed<br>
at <a href=""http://wonder.cdc.gov/mcd-icd10.html"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/mcd-icd10.html</a> on November 3, 2017.</p>
<h2>Caveats</h2>
<ol>
<li>As of April 3, 2017, the underlying cause of death has been revised for 125 deaths in 2014. More information:<br>
<a href=""http://wonder.cdc.gov/wonder/help/mcd.html#2014-Revision"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#2014-Revision</a>.</li>
<li>Circumstances in Georgia for the years 2008 and 2009 have resulted in unusually high death counts for the ICD-10 cause of<br>
death code R99, ""Other ill-defined and unspecified causes of mortality."" Caution should be used in interpreting these data.<br>
More information: <a href=""http://wonder.cdc.gov/wonder/help/mcd.html#Georgia-Reporting-Anomalies"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#Georgia-Reporting-Anomalies</a>.</li>
<li>Circumstances in New Jersey for the year 2009 have resulted in unusually high death counts for the ICD-10 cause of death code<br>
R99, ""Other ill-defined and unspecified causes of mortality"" and therefore unusually low death counts in other ICD-10 codes,<br>
most notably R95, ""Sudden Infant Death Syndrome"" and X40-X49, ""Unintentional poisoning."" Caution should be used in<br>
interpreting these data. More information: <a href=""http://wonder.cdc.gov/wonder/help/mcd.html#New-Jersey-Reporting-Anomalies"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#New-Jersey-Reporting-Anomalies</a>.</li>
<li>Circumstances in California resulted in unusually high death counts for the ICD-10 cause of death code R99, ""Other<br>
ill-defined and unspecified causes of mortality"" for deaths occurring in years 2000 and 2001. Caution should be used in<br>
interpreting these data. More information: <a href=""http://wonder.cdc.gov/wonder/help/mcd.html#California-Reporting-Anomalies"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#California-Reporting-Anomalies</a>.</li>
<li>Death rates are flagged as Unreliable when the rate is calculated with a numerator of 20 or less. More information:<br>
<a href=""http://wonder.cdc.gov/wonder/help/mcd.html#Unreliable"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#Unreliable</a>.</li>
<li>The method used to calculate 95% confidence intervals is documented here: More information:<br>
<a href=""http://wonder.cdc.gov/wonder/help/mcd.html#Confidence-Intervals"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#Confidence-Intervals</a>.</li>
<li>The method used to calculate standard errors is documented here: More information:<br>
<a href=""http://wonder.cdc.gov/wonder/help/mcd.html#Standard-Errors"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#Standard-Errors</a>.</li>
<li>The population figures for year 2015 are bridged-race estimates of the July 1 resident population, from the Vintage 2015<br>
postcensal series released by NCHS on June 28, 2016. The population figures for year 2014 are bridged-race estimates of the July<br>
1 resident population, from the Vintage 2014 postcensal series released by NCHS on June 30, 2015. The population figures for<br>
year 2013 are bridged-race estimates of the July 1 resident population, from the Vintage 2013 postcensal series released by NCHS<br>
on June 26, 2014. The population figures for year 2012 are bridged-race estimates of the July 1 resident population, from the<br>
Vintage 2012 postcensal series released by NCHS on June 13, 2013. Population figures for 2011 are bridged-race estimates of the<br>
July 1 resident population, from the county-level postcensal Vintage 2011 series released by NCHS on July 18, 2012. Population<br>
figures for 2010 are April 1 Census counts. The population figures for years 2001 - 2009, are bridged-race estimates of the July<br>
1 resident population, from the revised intercensal county-level 2000 - 2009 series released by NCHS on October 26, 2012.<br>
Population figures for 2000 are April 1 Census counts. Population figures for 1999 are from the 1990-1999 intercensal series of<br>
July 1 estimates. Population figures for Infant Age Groups are the number of live births. <strong>Note:</strong> Rates and population<br>
figures for years 2001 - 2009 differ slightly from previously published reports, due to use of the population estimates which<br>
were available at the time of release.</li>
<li>The population figures used in the calculation of death rates for the age group 'under 1 year' are the estimates of the<br>
resident population that is under one year of age. More information: <a href=""http://wonder.cdc.gov/wonder/help/mcd.html#Age"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/wonder/help/mcd.html#Age</a> Group.</li>
</ol>
<p><strong><em>Source:</em></strong> <a href=""http://wonder.cdc.gov/mcd-icd10.html"" target=""_blank"" rel=""nofollow"">http://wonder.cdc.gov/mcd-icd10.html</a></p>
This dataset was created by Health and contains around 900 samples along with Crude Rate Lower 95% Confidence Interval, Crude Rate Standard Error, technical information and other features such as:
- Year
- Crude Rate
- and more.
How to use this dataset
&gt; - Analyze Deaths in relation to Crude Rate Upper 95% Confidence Interval
- Study the influence of State on Crude Rate Lower 95% Confidence Interval
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	217	1383	7	yamqwe	drug-induced-deathse
1502	1502	🎦 Greatest Films of All Time - Multiple Sources	Movies dataset from American Film Institute, Sight & Sound Critic's Survey	['arts and entertainment', 'movies and tv shows']	"About this dataset
&gt; <h4>Titles sourced from American Film Institute, Writer's Guild of America, Sight &amp; Sound Critic's Survey, The Guardian, and Barron's 1001 Movies to See Before You Die</h4>
<p>This open dataset I made for movie fans compiles several ‘greatest films’ lists to find the greatest of the great, and the analysis reveals seven films to be the best of the best.</p>
<p><a href=""https://public.tableau.com/profile/owen.temple#!/vizhome/greatest-films/DashboardInterface"" target=""_blank"" rel=""nofollow"">A Movie Recommender visualization and interface for the data hosted on Tableau Public</a>.</p>
<p></p>
This dataset was created by Owen Temple and contains around 1000 samples along with Rank Wga 2005, Included In Guardian List, technical information and other features such as:
- Title
- Country
- and more.
How to use this dataset
&gt; - Analyze Runtime in relation to Released
- Study the influence of Genre on Rated
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Owen Temple 
Start A New Notebook!"	126	1051	9	yamqwe	multi-sourced-greatest-filmse
1503	1503	🚧 Home construction price index	House Price Indexes of New Single-Family houses Under Construction	['housing', 'business', 'real estate', 'economics']	"About this dataset
&gt; <p>Price Indexes of New Single-Family Houses Under Construction<br>
[2005 = 100.0. Index based on kinds of houses sold in 2005]</p>
<p>The houses under construction indexes are a group of price indexes designed for use in deriving a constant dollar series from the current dollar series of single-family value put in place. The indexes are formed with data for houses built for sale, contractor-built houses, owner-built houses, and houses built for rent. All indexes apply to the construction cost of new houses that are under construction as defined for the singlefamily value put in place estimates. Construction cost excludes the value of land and other nonconstruction costs. See Value of Construction Put in Place methodology documentation for more information.</p>
<p>The under construction indexes are only computed at the national level.</p>
<p>The data used for computing these indexes are obtained from the U.S. Census Bureau's Survey of Construction. The survey collects information on the physical characteristics and prices of new single-family houses. This is done through monthly interviews with the builders or owners of a national sample of new houses.</p>
<p>The methodology used to compute the value put in place of new single-family houses influenced the formulation of the indexes for houses under construction. The value put in place of new single-family houses is computed from data collected in the Survey of Construction and evaluation studies performed in the past and is an indirect estimate of construction value.</p>
<p>Laspeyres Price Index (Constant Quality<br>
This index answers the question, ""How much is the sales price today for the same quality house as in the base year?"" The base year we are now using is 2005; its index value is set to 100.0. Quality includes not only the physical size and amenities of the house, but also its geographic location. A hypothetical calculation is made in which the base year kind of house is held constant over time while its selling price is calculated in current dollars.</p>
<p>Uses of This Index</p>
<p>In theory, this index may be used to determine how much of a total price increase is due to an increase in quality - changes in size, amenities, and location - and how much is due to inflation. This index keeps housing quality constant. It has been used to inflate previous years' house prices to determine housing insurance replacement costs, to update local government real estate tax abatement levels, and to update price levels in housing programs for inflation where such updates are required by law or custom. Construction contracts might have a price escalation clause tied to this index. The assumption is that inflation in existing housing or construction can be approximated by inflation in the sales prices of new one-family houses sold.</p>
<p>Fisher Ideal Index (Price Deflator)</p>
<p>This index helps answers the question, ""What is the (unbiased) value of today’s homes being constructed in constant dollars?” In doing this it attempts to eliminate two kinds of problems associated with the previous two indexes:<br>
• the tendency to overstate inflation (Laspeyres); and<br>
• the tendency to understate inflation (Paasche);</p>
<p>The Fisher Ideal index is the geometric average of a Laspeyres and Paasche indexes for the same time period. The geometric average is calculated by multiplying the Laspeyres index by the Paasche index and then taking the square root of the result. The biases associated with each component index are minimized by calculating the geometric average.</p>
<p>Uses of This Index</p>
<p>This index can be used as a price deflator in determining the constant dollar value of today’s output of houses under construction, which is included in the Gross Domestic Product. It does eliminate the problems of either understating or overstating inflation given substitutions made in the marketplace, while at the same time allowing for some change in base characteristics to reflect the variation in size, amenities and geographic location of houses. The geometric average technique has the effect of halving the yearly quality change taking place.</p>
<p>Additional information on methodology: <a href=""http://www.census.gov/construction/cpi/pdf/descpi_uc.pdf"" target=""_blank"" rel=""nofollow"">http://www.census.gov/construction/cpi/pdf/descpi_uc.pdf</a></p>
<p>Source: <a href=""http://www.census.gov/construction/cpi/"" target=""_blank"" rel=""nofollow"">http://www.census.gov/construction/cpi/</a></p>
This dataset was created by Finance and contains around 600 samples along with Laspeyres ( Fixed), Fisher ( Deflator), technical information and other features such as:
- Laspeyres ( Fixed)
- Fisher ( Deflator)
- and more.
How to use this dataset
&gt; - Analyze Laspeyres ( Fixed) in relation to Fisher ( Deflator)
- Study the influence of Laspeyres ( Fixed) on Fisher ( Deflator)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	107	1050	15	yamqwe	home-construction-price-indexe
1504	1504	📜 Hate Speech Identification	A dataset containing over 20000 samples of hate speech.	['nlp']	"About this dataset
&gt; <p>Contributors viewed short text and identified if it a) contained hate speech, b) was offensive but without hate speech, or c) was not offensive at all. Contains nearly 15K rows with three contributor judgments per text string.</p>
<p><em><strong>UPDATE: You can find a larger version of this dataset <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language"" target=""_blank"" rel=""nofollow"">HERE</a>. Please refer to the linked dataset in any published work.</strong></em></p>
<p><strong>CITATION</strong> If you are using this data for academic research then please cite:Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. ""Automated Hate Speech Detection and the Problem of Offensive Language."" Proceedings of the 11th International Conference on Web and Social Media (ICWSM).  The paper is available <a href=""https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15665"" target=""_blank"" rel=""nofollow"">here</a>.</p>
<p><strong>LICENSE</strong> <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/LICENSE"" target=""_blank"" rel=""nofollow"">MIT license</a></p>
This dataset was created by CrowdFlower and contains around 20000 samples along with Neither, Class, technical information and other features such as:
- Hate Speech
- Count
- and more.
How to use this dataset
&gt; - Analyze Offensive Language in relation to Neither
- Study the influence of Class on Hate Speech
- More datasets
Acknowledgements
If you use this dataset in your research, please credit CrowdFlower 
Start A New Notebook!"	176	1941	19	yamqwe	hate-speech-identificatione
1505	1505	💅 Hazardous Chemicals in Cosmetics	Data on hazardous ingredients in cosmetic products sold in California	['healthcare', 'public health', 'environment', 'business', 'health']	"About this dataset
&gt; <p>These data reflect information that has been reported to the California Safe Cosmetics Program (CSCP) in the California Department of Public Health (CDPH). The primary purpose of the CSCP is to collect information on hazardous and potentially hazardous ingredients in cosmetic products sold in California and to make this information available to the public. For all cosmetic products sold in California, the California Safe Cosmetics Act (“the Act”) requires the manufacturer, packer, and/or distributor named on the product label to provide to the CSCP a list of all cosmetic products that contain any ingredients known or suspected to cause cancer, birth defects, or other developmental or reproductive harm. To assist companies with reporting, CDPH has compiled a list of reportable ingredients based on lists and reports available from the authoritative scientific bodies cited in the Act to cause cancer or reproductive harm; it is meant to serve as guidance and is not all-inclusive. Companies with reportable ingredients in their products must submit information to the California Safe Cosmetics Program if the company: * Has annual aggregate sales of cosmetic products of one million dollars or more, and * Has sold cosmetic products in California on or after January 1, 2007. The data table consists of: label names of cosmetic/personal care products, company/manufacturer names, product brand names, product categories, Chemical Abstracts Service registry numbers (CAS#) of the reported chemical ingredients, names of reported chemical ingredients, the number of reported chemicals for each product, and dates of reporting, product discontinuation or reformulation if applicable. All products containing carcinogens or developmental or reproductive toxicants may not be included due to companies failing to report. List of reportable ingredients: <a href=""http://www.cdph.ca.gov/ReportableIngredientsList"" target=""_blank"" rel=""nofollow"">www.cdph.ca.gov/ReportableIngredientsList</a></p>
<p>Source: <a href=""https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/OHB/CSCP/Pages/CSCP.aspx"" target=""_blank"" rel=""nofollow"">https://www.cdph.ca.gov/Programs/CCDPHP/DEODC/OHB/CSCP/Pages/CSCP.aspx</a><br>
Last updated at <a href=""https://data.chhs.ca.gov"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov</a> : 2020-05-16<br>
License: <a href=""https://data.chhs.ca.gov/pages/terms"" target=""_blank"" rel=""nofollow"">https://data.chhs.ca.gov/pages/terms</a></p>
This dataset was created by California Health and Human Services and contains around 100000 samples along with Cas Id, Csf Id, technical information and other features such as:
- Company Name
- Chemical Count
- and more.
How to use this dataset
&gt; - Analyze Chemical Created At in relation to Cas Number
- Study the influence of Company Id on Chemical Date Removed
- More datasets
Acknowledgements
If you use this dataset in your research, please credit California Health and Human Services 
Start A New Notebook!"	63	498	6	yamqwe	chemicals-in-cosmeticse
1506	1506	📒 Credit Approval	This data concerns credit card applications; good mix of attributes	['business', 'computer science', 'classification']	"About this dataset
&gt; <p>This data concerns credit card applications; good mix of attributes# Source:<br>
(confidential source)<br>
Submitted by quinlan '@' <a href=""http://cs.su.oz.au"" target=""_blank"" rel=""nofollow"">cs.su.oz.au</a></p>
<h1>Data Set Information:</h1>
<p>This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data.<br>
This dataset is interesting because there is a good mix of attributes * continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.</p>
<h1>Attribute Information:</h1>
<p>A1:  b, a.A2:    continuous.A3:  continuous.A4:  u, y, l, t.A5:  g, p, gg.A6:    c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.A7: v, h, bb, j, n, z, dd, ff, o.A8:    continuous.A9:  t, f.A10:   t, f.A11:   continuous.A12: t, f.A13:   g, p, s.A14:    continuous.A15: continuous.A16: +,-     (class attribute)</p>
<h1>Relevant Papers:</h1>
<p>Quinlan. ""Simplifying decision trees"", Int J Man-Machine Studies 27, Dec 1987, pp. 221-234.<br>
Quinlan. ""C4.5: Programs for Machine Learning"", Morgan Kaufmann, Oct 1992</p>
<h1>Papers That Cite This Data Set1:</h1>
<p>Xiaoming Huo. FBP: A Frontier-Based Tree-Pruning Algorithm. Seoung Bum Kim. 2002.</p>
<ul>
<li>Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Improved Generalization Through Explicit Optimization of Margins. Machine Learning, 38. 2000.</li>
<li>Kagan Tumer and Joydeep Ghosh. Robust Combining of Disparate Classifiers through Order Statistics. CoRR, csLG/9905013. 1999.</li>
<li>Lorne Mason and Peter L. Bartlett and Jonathan Baxter. Direct Optimization of Margins Improves Generalization in Combined Classifiers. NIPS. 1998.</li>
</ul>
<h1>Citation Request:</h1>
<p>Please refer to the Machine Learning <a href=""http://archive.ics.uci.edu/ml/citation_policy.html"" target=""_blank"" rel=""nofollow"">Repository's citation policy.</a><br>
[1] Papers were automatically harvested and associated with this data set, in collaborationwith <a href=""http://rexa.info/"" target=""_blank"" rel=""nofollow"">Rexa.info</a></p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Credit+Approval"" target=""_blank"" rel=""nofollow"">http://archive.ics.uci.edu/ml/datasets/Credit+Approval</a></p>
This dataset was created by UCI and contains around 700 samples along with 0.1, G.1, technical information and other features such as:
- F
- B
- and more.
How to use this dataset
&gt; - Analyze 00202 in relation to W
- Study the influence of U on G
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	151	1967	8	yamqwe	credit-approvale
1507	1507	🌳 Tree debris removal requests over time 	Open tree debris removal requests made to 311 and all open tree dump requests.	['business']	"About this dataset
&gt; <p>All open tree debris removal requests made to 311 and all requests completed since January 1, 2011. Large piles of branches or bushes may be picked up by the Department of Streets and Sanitation. 311 sometimes creates duplicate requests for tree debris removal. When there is an open tree debris request, a duplicate request is created when the exact same address and the exact same service request type are used. Streets and Sanitation responds to the initial request opened and closes the duplicates. A forestry ""Clam"" is the name of the vehicle the Forestry Bureau deploys to collect tree debris. Data Owner: Streets and Sanitation (<a href=""http://www.cityofchicago.org/content/city/en/depts/streets.html"" target=""_blank"" rel=""nofollow"">http://www.cityofchicago.org/content/city/en/depts/streets.html</a>). Time Period: January 1, 2011 to present. Frequency: Data is updated daily. Related Applications: 311 Service Request Status Inquiry (<a href=""https://servicerequest.cityofchicago.org/web_intake_chic/Controller?op=createsrquery2"" target=""_blank"" rel=""nofollow"">https://servicerequest.cityofchicago.org/web_intake_chic/Controller?op=createsrquery2</a>) and Request Tree Debris Removal (<a href=""https://servicerequest.cityofchicago.org/web_intake_chic/Controller?op=locform&amp;invSRType=SEL&amp;invSRDesc=Tree%20Debris&amp;locreq=Y"" target=""_blank"" rel=""nofollow"">https://servicerequest.cityofchicago.org/web_intake_chic/Controller?op=locform&amp;invSRType=SEL&amp;invSRDesc=Tree Debris&amp;locreq=Y</a>).</p>
<p>Source: <a href=""https://data.cityofchicago.org/d/mab8-y9h3"" target=""_blank"" rel=""nofollow"">https://data.cityofchicago.org/d/mab8-y9h3</a><br>
Last updated at <a href=""https://data.cityofchicago.org/"" target=""_blank"" rel=""nofollow"">https://data.cityofchicago.org/</a> : 2019-03-05</p>
This dataset was created by City of Chicago<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span> and contains around 100000 samples along with If Yes, Where Is The Debris Located?, Latitude, technical information and other features such as:
- Y Coordinate
- Street Address
- and more.
How to use this dataset
&gt; - Analyze Completion Date in relation to Most Recent Action
- Study the influence of Location on Current Activity
- More datasets
Acknowledgements
If you use this dataset in your research, please credit City of Chicago<span class=""ManagedBadge__managedBadge___2TJ9U""><span class=""svg-icon""><svg xmlns=""http://www.w3.org/2000/svg"" viewBox=""0 0 16 16""><circle cx=""8"" cy=""8"" r=""8"" fill=""#8c9caf""></circle><path d=""M7.826 8.689c.061.132.119.27.174.41a9.317 9.317 0 0 1 .366-.811l1.828-3.6a.583.583 0 0 1 .1-.144.355.355 0 0 1 .114-.076.404.404 0 0 1 .144-.024H12v7.112h-1.455V7.463c0-.199.01-.413.03-.645l-1.886 3.658a.615.615 0 0 1-.576.347h-.226a.655.655 0 0 1-.34-.087.622.622 0 0 1-.236-.26L5.416 6.813a5.942 5.942 0 0 1 .04.65v4.093H4V4.444h1.448a.344.344 0 0 1 .259.1.614.614 0 0 1 .1.144L7.64 8.302c.065.124.128.253.187.387z"" fill=""#fff""></path></svg></span></span> 
Start A New Notebook!"	12	148	7	yamqwe	311-service-requests-tree-debrise
1508	1508	🧑 Childhood Obesity in the US	Dataset with Percent Obesity in the United States (1971-2014)	['nutrition', 'public health', 'health']	"About this dataset
&gt; <h1>Childhood Obesity in the United States (1971-2014)</h1>
<p>data source: <a href=""http://www.cdc.gov/nchs/data/hestat/obesity_child_13_14/obesity_child_13_14.htm"" target=""_blank"" rel=""nofollow"">http://www.cdc.gov/nchs/data/hestat/obesity_child_13_14/obesity_child_13_14.htm</a></p>
<h3>Data Files</h3>
<ol>
<li>child_ob_gender.csv</li>
<li>obesity_child_age</li>
</ol>
<h3>Visualizations</h3>
<p><strong>Historical Childhood Obesity Rate by Gender</strong></p>
<p><em>Boys tended to suffer from obesity at a higher rate than girls during 2000 through 2010. More recently however, between 2011 and 2014, boys' and girls' obesity rates converged as a result of an increase for girls and decrease for boys.</em></p>
<p><em>For both genders, obesity rates grew rapidly during the last two decades of the 20th century, but thankfully growth rates have lessened in recent years.</em></p>
<p><img src=""http://i.imgur.com/oyWAjys.png"" alt=""Imgur"" style=""""></p>
<p><strong>Historical Childhood Obesity Rate by Age</strong></p>
<p><em>The data show that older children have been afflicted by the obesity epidemic at a higher rate than very young children.</em></p>
<p><img src=""http://i.imgur.com/7W2Bsz3.png"" alt=""Imgur"" style=""""></p>
This dataset was created by Health and contains around 100 samples along with Se, Percent Obese, technical information and other features such as:
- Gender
- Time
- and more.
How to use this dataset
&gt; - Analyze Age in relation to Se
- Study the influence of Percent Obese on Gender
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	144	1266	5	yamqwe	childhood-obesity-in-the-use
1509	1509	🏰 Disney Characters Contribution Movies Success	Disney characters, box office success & annual gross income	['arts and entertainment', 'movies and tv shows', 'business', 'comics and animation']	"About this dataset
&gt; <p>What are the trends in the Walt Disney Studio’s box office data? How do certain characters contribute to the success or failure of a movie?</p>
<p><a href=""https://data.world/kgarrett/disney-character-success-00-16/file/DisneyReport.pdf"">Read Our Project Report</a></p>
<p><strong><em>Sources:</em></strong></p>
<ul>
<li><a href=""https://www.sugarcane.com/data/walt-disney-animation-studios-films-1"" target=""_blank"" rel=""nofollow"">https://www.sugarcane.com/data/walt-disney-animation-studios-films-1</a></li>
<li><a href=""http://www.the-numbers.com/movies/distributor/Walt-Disney"" target=""_blank"" rel=""nofollow"">http://www.the-numbers.com/movies/distributor/Walt-Disney</a></li>
<li><a href=""https://en.wikipedia.org/wiki/List_of_Disney_animated_universe_characters"" target=""_blank"" rel=""nofollow"">https://en.wikipedia.org/wiki/List_of_Disney_animated_universe_characters</a></li>
</ul>
This dataset was created by Kelly Garrett and contains around 0 samples along with Disney Consumer Products[ni 2], Disney Media Networks, technical information and other features such as:
- Disney Interactive[ni 3][ Rev 1]
- Studio Entertainment[ni 1]
- and more.
How to use this dataset
&gt; - Analyze Disney Consumer Products[ni 2] in relation to Disney Media Networks
- Study the influence of Disney Interactive[ni 3][ Rev 1] on Studio Entertainment[ni 1]
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	198	1706	7	yamqwe	disney-character-successe
1510	1510	Australia Largest Companies	Global 2000 Sales, Profits, Assets and Market Value (Billions)	['business', 'finance', 'economics', 'investing']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p><a target=""_blank"">http://www.economywatch.com/companies/forbes-list/</a></p>
This dataset was created by Finance and contains around 0 samples along with Profits ($billion), Sales ($billion), technical information and other features such as:
- Assets ($billion)
- Market Value ($billion)
- and more.
How to use this dataset
&gt; - Analyze Profits ($billion) in relation to Sales ($billion)
- Study the influence of Assets ($billion) on Market Value ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	191	975	18	yamqwe	australia-largest-companiese
1511	1511	Amazon Product Reviews Dataset	A sample dataset of product reviews from Amazon.com	['business', 'retail and shopping', 'ratings and reviews']	"About this dataset
This dataset contains 30K records of product reviews from <a href=""http://amazon.com"" target=""_blank"">amazon.com</a>. 
<p>This dataset was created by <a href=""https://www.promptcloud.com/"" target=""_blank"">PromptCloud</a>  and <a href=""https://datastock.shop/"" target=""_blank"">DataStock</a>

</p>
<h3>Content</h3>
<p>This dataset contains the following:</p>
<ul>
<li>
<p>Total Records Count: 43729</p>
</li>
<li>
<p>Domain Name: <a href=""http://amazon.com"" target=""_blank"">amazon.com</a></p>
</li>
<li>
<p>Date Range: 01st Jan 2020 - 31st Mar 2020</p>
</li>
<li>
<p>File Extension: CSV</p>
</li>
<li>
<p>Available Fields:<br>
-- Uniq Id,<br>
-- Crawl Timestamp,<br>
-- Billing Uniq Id,<br>
-- Rating,<br>
-- Review Title,<br>
-- Review Rating,<br>
-- Review Date,<br>
-- User Id,<br>
-- Brand,<br>
-- Category,<br>
-- Sub Category,<br>
-- Product Description,<br>
-- Asin,<br>
-- Url,<br>
-- Review Content,<br>
-- Verified Purchase,<br>
-- Helpful Review Count,<br>
-- Manufacturer Response</p>
</li>
</ul>
<h3>Acknowledgements</h3>
<p>We wouldn't be here without the help of our in house teams at PromptCloud and DataStock. Who has put their heart and soul into this project like all other projects?  We want to provide the best quality data and we will continue to do so.</p>
<h3>Inspiration</h3>
<p>The inspiration for these datasets came from research. Reviews are something that is important wit everybody across the globe. So we decided to come up with this dataset that shows us exactly how the user reviews help companies to better their products.</p>
This dataset was created by PromptCloud and contains around 0 samples along with Billing Uniq Id, Verified Purchase, technical information and other features such as:
- Crawl Timestamp
- Manufacturer Response
- and more.
How to use this dataset
&gt; - Analyze Helpful Review Count in relation to Sub Category
- Study the influence of Review Date on Product Description
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	109	799	5	yamqwe	amazon-product-reviews-datasete
1512	1512	Nesting Sea Turtles Size (2018 - 2021)	Turtles at maturity at Atlantic rookery	['earth and nature', 'biology', 'animals']	"About this dataset
&gt; <h2><strong>Article Abstract</strong></h2>
<p>For species reaching maturity at a range of ages or sizes, factors that influence juvenile growth and size at maturity may have lasting impacts on overall fitness. Assessing when animals reach maturity is especially challenging for species which are difficult to follow through time as a result of highly migratory behavior, long life spans, or both. We examined nesting female size in a reproductive assemblage of green turtles (<em>Chelonia mydas</em>) and loggerheads (<em>Caretta caretta</em>) on the east coast of Florida, USA. We used a long-term dataset from 1982-2019 to estimate a minimum size at maturity interval on the basis of two standard deviations below mean female size for each species. The minimum size intervals for green turtles (81.4 - 89.3 cm) and loggerheads (68.1 - 79.1 cm) were lower than most previous estimates in the literature, many of which were simply the smallest individual ever observed. There was a significant decrease in the upper bound of the minimum size interval over the study period for both green turtles (1.6 cm) and loggerheads (4.1 cm). These shifts in size at maturity may be the result of changes in population demographics, habitat quality, and behavioral reactions to these changes. The development and periodic reassessment of robust estimators of maturity are an important part of programs centered around the monitoring and conservation of vulnerable wildlife populations.</p>
<p><strong><a href=""https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.3631"" target=""_blank"">Read Full Article</a></strong> for more details.</p>
<h2><strong>Dataset Citation</strong></h2>
<p>Mansfield, Katherine; Phillips, Katrina; Stahelin, Gustavo; Chabot, Ryan (2021), Long-term trends in marine turtle size at maturity at an important Atlantic rookery, Dryad, Dataset, <a href=""https://doi.org/10.5061/dryad.n8pk0p2v6"" target=""_blank"">https://doi.org/10.5061/dryad.n8pk0p2v6</a></p>
This dataset was created by Amber Thomas and contains around 10000 samples along with Species, Scl, technical information and other features such as:
- Year
- Species
- and more.
How to use this dataset
&gt; - Analyze Scl in relation to Year
- Study the influence of Species on Scl
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	72	598	7	yamqwe	nesting-sea-turtle-sizee
1513	1513	🚓 Fatal Police Shootings	Dataset containing over incidents of updated fatal police shootings 	['crime', 'classification', 'public safety', 'racial equity']	"About this dataset
&gt; <p>The Washington Post has tracked fatal police shootings in the US since 2015, using news and police reports as well as social media and databases like <code>Killed by Police</code> and <code>Fatal Encounters</code>.</p>
<p>The collected data include the race, gender, and age of the deceased, the circumstances of the shooting, and whether the person was armed or experiencing a mental-health crisis.</p>
<p>The Washington Post updates visualizations of the data and provides more information about methodology on the <a href=""https://www.washingtonpost.com/graphics/investigations/police-shootings-database/"" target=""_blank"">Fatal Force</a> page.</p>
<p><strong>Source:</strong> <a href=""https://github.com/washingtonpost/data-police-shootings"" target=""_blank"">https://github.com/washingtonpost/data-police-shootings</a><br>
<strong>Updated:</strong> synced daily<br>
<strong>License:</strong> <a href=""https://creativecommons.org/licenses/by-nc-sa/4.0/"" target=""_blank"">CC BY-NC-SA</a></p>
This dataset was created by Data Society and contains around 7000 samples along with Is Geocoding Exact, Armed, technical information and other features such as:
- Body Camera
- State
- and more.
How to use this dataset
&gt; - Analyze Latitude in relation to Manner Of Death
- Study the influence of Name on Age
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	150	1077	7	yamqwe	fatal-police-shootingse
1514	1514	🎵 All Songs From All Albums of Taylor Swift	Dataset contains 5000 samples along with Track N, Lyric and More	['music']	"About this dataset
&gt; <h3>Context</h3>
<p>This data set was created by <a href=""https://www.promptcloud.com/?utm_source=data-world&utm_medium=data-social&utm_campaign=text-mining"" target=""_blank"">PromptCloud</a> (a Data-as-a-Service provider), using the API exposed by <a href=""http://Genius.com"" target=""_blank"">Genius.com</a>.</p>
<h3>Content</h3>
<p>It has the following data fields:</p>
<ul>
<li>album name</li>
<li>track title</li>
<li>track number</li>
<li>lyric text</li>
<li>line number of the lyric in the track</li>
<li>year of release of the album</li>
</ul>
<h3>Initial analyses</h3>
<p>You can check out <a href=""https://www.promptcloud.com/blog/data-visualization-text-mining-taylor-swift-song-lyrics/?utm_source=data-world&utm_medium=data-social&utm_campaign=text-mining"" target=""_blank"">this article</a> to understand the following initial set of analysis:</p>
<p>– Exploratory analysis</p>
<ul>
<li>word counts based on tracks and albums</li>
<li>time series analysis of word counts</li>
<li>distribution of word counts</li>
</ul>
<p>– Text mining</p>
<ul>
<li>word cloud</li>
<li>bigram network</li>
<li>sentiment analysis (includes chord diagram)</li>
</ul>
This dataset was created by PromptCloud and contains around 5000 samples along with Track N, Lyric, technical information and other features such as:
- Artist
- Album
- and more.
How to use this dataset
&gt; - Analyze Year in relation to Track Title
- Study the influence of Line on Track N
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	102	733	7	yamqwe	taylor-swift-song-data-from-all-the-albumse
1515	1515	asos Products Dataset	Scraped dataset of the fashion and cosmetic retailer asos.	['clothing and accessories', 'business', 'make-up and cosmetics']	"About this dataset
&gt; <p>ASOS plc is a British online fashion and cosmetic retailer. Crawl feeds extracted data from asos. Reach out to us for complete asos products dataset.</p>
<p><a href=""https://crawlfeeds.com/datasets"" target=""_blank"">https://crawlfeeds.com/datasets</a></p>
This dataset was created by Crawl Feeds and contains around 2000 samples along with Name, Brand Name, technical information and other features such as:
- Description
- Crawled At
- and more.
How to use this dataset
&gt; - Analyze Color in relation to Name
- Study the influence of Brand Name on Description
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Crawl Feeds 
Start A New Notebook!"	67	577	10	yamqwe	asos-products-datasete
1516	1516	Missing Children in the US	Dataset from The National Center for Missing and Exploited Children (NCMEC)	['crime']	"About this dataset
&gt; <p>Publicly available information from The National Center for Missing and Exploited Children (NCMEC), released as part of the <a href=""https://childfinder.hackerearth.com/"" target=""_blank"">Cloudera Child Finder Hackathon</a> to develop new methods for finding missing children.</p>
 This dataset was created by James Gray and contains around 3000 samples along with Haircolor, Casetype, technical information and other features such as:
- Missingfromcountry
- Height
- and more.
How to use this dataset
&gt; - Analyze Childlastname in relation to Missingreporteddate
- Study the influence of Birthdate on Missingfromstate
- More datasets
Acknowledgements
If you use this dataset in your research, please credit James Gray 
Start A New Notebook!"	94	491	6	yamqwe	missing-children-in-the-use
1517	1517	🇺🇸 U.S. Household Income Distribution by State	Income Level in Relation to Number Of Households, State and more	['united states', 'business', 'social science', 'economics']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""http://2oqz471sa19h3vbwa53m33yj.wpengine.netdna-cdn.com/wp-content/uploads/2017/11/household-income.png"" alt=""Visual Capitalist""></p>
<h1><strong>About this Dataset</strong></h1>
<p><strong>SOURCE:</strong> <a href=""http://www.visualcapitalist.com/household-income-distribution-u-s-state/"" target=""_blank"">Visual Capitalist</a></p>
<p><strong>DATA SOURCE:</strong> <a href=""https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml"" target=""_blank"">U.S. Census Bureau</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
This dataset was created by Andy Kriebel and contains around 7000 samples along with Number Of Households, State, technical information and other features such as:
- Year
- Percent Of Total
- and more.
How to use this dataset
&gt; - Analyze Income Level in relation to Number Of Households
- Study the influence of State on Year
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	116	752	12	yamqwe	2018-w3-u-s-household-income-distribution-by-stae
1518	1518	Early Detection of Fascism (Hitler's Speeches)	How early can we detect fascism roots from Hitler speeches? 	['history', 'military', 'beginner', 'intermediate', 'advanced']	"&gt; Clarification: This data in this dataset is about a sensitive topic: It is meant for research/education purposes.
The Dataset
The dataset contains all of Adolf Hitler's speeches translated to English as well as the date and time they were given. 
The main goal of this dataset is to be used for the early detection of fascism throughout the time. 
This dataset was created by scraping various sources online following the Wikipedia listings of Hitler speeches. 
How to use this dataset
Study the ""tone shift"" of Hitler's speeches over time. 
More datasets"	23	466	11	yamqwe	early-detection-of-fascism-hitler-speeches
1519	1519	🌆 Big Cities Health Indicators Over Time	34 Health (and six demographics-related) Indicators from 26 most urban cities	['mental health', 'healthcare', 'public health', 'social science', 'psychology', 'health conditions', 'news']	"About this dataset
&gt; <p>This dataset illustrates health status of 26 of the nation’s largest and most urban cities as captured by 34 health (and six demographics-related) indicators. These indicators represent some of the leading causes of morbidity and mortality in the United States and leading priorities of national, state, and local health agencies. Public health data were captured in nine overarching categories: HIV/AIDS, cancer, nutrition/physical activity/obesity, food safety, infectious disease, maternal and child health, tobacco, injury/violence, and behavioral health/substance abuse.</p>
<p>Attribution: U.S. Centers for Disease Control and Prevention</p>
<p>Source: <a href=""https://bchi.bigcitieshealth.org/"" target=""_blank"">Big Cities Health Inventory Data</a></p>
This dataset was created by Health and contains around 10000 samples along with Year, Place, technical information and other features such as:
- Gender
- Bchc Requested Methodology
- and more.
How to use this dataset
&gt; - Analyze Indicator Category in relation to Notes
- Study the influence of Value on Source
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	152	1135	8	yamqwe	big-cities-healthe
1520	1520	🛻 Uber Pickups in NYC	On Over 4.5 million Uber pickups in New York City from April to September 2014	['business', 'transportation', 'regression', 'travel']	"About this dataset
&gt; <p>This data contains on over 4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015. Trip-level data on 10 other for-hire vehicle (FHV) companies, as well as aggregated data for 329 FHV companies, is also included. All the files are as they were received on August 3, Sept. 15 and Sept. 22, 2015.</p>
<p>The dataset contains, roughly, four groups of files:</p>
<ul>
<li>Uber trip data from 2014 (April - September), separated by month, with detailed location information</li>
<li>Uber trip data from 2015 (January - June), with less fine-grained location information</li>
<li>non-Uber FHV (For-Hire Vehicle) trips. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.</li>
<li>aggregate ride and vehicle statistics for all FHV companies (and, occasionally, for taxi companies)</li>
</ul>
<p>Uber trip data from 2014</p>
<p>There are six files of raw data on Uber pickups in New York City from April to September 2014. The files are separated by month and each has the following columns:</p>
<ul>
<li>Date/Time : The date and time of the Uber pickup</li>
<li>Lat : The latitude of the Uber pickup</li>
<li>Lon : The longitude of the Uber pickup</li>
<li>Base : The TLC base company code affiliated with the Uber pickup</li>
</ul>
<p>These files are named:</p>
<ul>
<li>uber-raw-data-apr14.csv</li>
<li>uber-raw-data-aug14.csv</li>
<li>uber-raw-data-jul14.csv</li>
<li>uber-raw-data-jun14.csv</li>
<li>uber-raw-data-may14.csv</li>
<li>uber-raw-data-sep14.csv</li>
</ul>
<p>Uber trip data from 2015</p>
<p>Also included is the file uber-raw-data-janjune-15.csv This file has the following columns:</p>
<ul>
<li>Dispatching_base_num : The TLC base company code of the base that dispatched the Uber</li>
<li>Pickup_date : The date and time of the Uber pickup</li>
<li>Affiliated_base_num : The TLC base company code affiliated with the Uber pickup</li>
<li>locationID : The pickup location ID affiliated with the Uber pickup<br>
The Base codes are for the following Uber bases:</li>
</ul>
<p>B02512 : Unter B02598 : Hinter B02617 : Weiter B02682 : Schmecken B02764 : Danach-NY B02765 : Grun B02835 : Dreist B02836 : Drinnen</p>
<p>For coarse-grained location information from these pickups, the file taxi-zone-lookup.csv shows the taxi Zone (essentially, neighborhood) and Borough for each locationID.</p>
<p>Non-Uber FLV trips</p>
<p>The dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (FHV) companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.</p>
<p>These files are named:</p>
<ul>
<li>American_B01362.csv</li>
<li>Diplo_B01196.csv</li>
<li>Highclass_B01717.csv</li>
<li>Skyline_B00111.csv</li>
<li>Carmel_B00256.csv</li>
<li>Federal_02216.csv</li>
<li>Lyft_B02510.csv</li>
<li>Dial7_B00887.csv</li>
<li>Firstclass_B01536.csv</li>
<li>Prestige_B01338.csv</li>
</ul>
<p>Aggregate Statistics</p>
<p>There is also a file other-FHV-data-jan-aug-2015.csv containing daily pickup data for 329 FHV companies from January 2015 through August 2015.</p>
<p>The file Uber-Jan-Feb-FOIL.csv contains aggregated daily Uber trip statistics in January and February 2015.</p>
This dataset was created by Data Society and contains around 300 samples along with Routing Details, Status, technical information and other features such as:
- Pu Address.1
- Do Address
- and more.
How to use this dataset
&gt; - Analyze Travel Time in relation to Date
- Study the influence of Pu Address on Routing Details
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	252	2295	17	yamqwe	uber-pickups-in-nyce
1521	1521	Every Subway Restaurant Location in the US	Restaurant Locations of all Subway Restaurants in the US	['business', 'restaurants']	"About this dataset
&gt; <h3>Subway Restaurant Location Data</h3>
<p>Subway is an American privately held restaurant franchise that primarily sells submarine sandwiches and salads. It is one of the fastest-growing franchises in the world and, as of October 2019, had 41,512 locations in more than 100 countries.</p>
<p>This is a complete list of all Subway restaurant locations, along with their geographic coordinates, Street addresses, City, State, ZIP code, opening hours etc in the US.</p>
<h3>Get data for free</h3>
<p>Contact Datahut (<a href=""https://datahut.co/"" target=""_blank"">https://datahut.co/</a>) for more information and a fresh data set. We give this data for free for startups, journalists bloggers, researchers, analysts, etc.</p>
<p>​</p>
This dataset was created by Tony Paul and contains around 30000 samples along with Instagram, Longitude, technical information and other features such as:
- Country
- Open Hours
- and more.
How to use this dataset
&gt; - Analyze Name in relation to Phone Number 2
- Study the influence of Latitude on Youtube
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Tony Paul 
Start A New Notebook!"	138	1025	16	yamqwe	subway-restaurant-location-datasete
1522	1522	Global Opera Performances	A dataset of opera performances (2012 - 2018)	['arts and entertainment', 'movies and tv shows', 'music']	"About this dataset
&gt; <p>Six full seasons of global opera performances (2012/13 to 2017/18). Each row represents an individual run of an opera in a single city.</p>
<p>See the <a href=""https://data.world/popculture/global-opera-performances/workspace/data-dictionary"">data dictionary</a> for column information.</p>
<p><em><strong>Source:</strong></em> <a href=""https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8LUFN8"" target=""_blank"">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8LUFN8</a></p>
<p><em><strong>License:</strong></em> <a href=""https://creativecommons.org/publicdomain/zero/1.0/"" target=""_blank"">CC0 Public Domain Dedication</a></p>
<p><em><strong>Citation:</strong></em> Cuntz, Alexander, 2020, ""Replication Data for: Grand rights and opera reuse today"", <a href=""https://doi.org/10.7910/DVN/8LUFN8"" target=""_blank"">https://doi.org/10.7910/DVN/8LUFN8</a>, Harvard Dataverse, V1</p>
<p><em><strong>Updated:</strong></em> not updating</p>
<p>​</p>
This dataset was created by Pop Culture and Entertainment and contains around 30000 samples along with Mf, Nat, technical information and other features such as:
- Work
- Dd
- and more.
How to use this dataset
&gt; - Analyze Composer in relation to Performances
- Study the influence of Type on Iso
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Pop Culture and Entertainment 
Start A New Notebook!"	98	722	17	yamqwe	global-opera-performancese
1523	1523	Flipkart Products Dataset	Product Description Dataset from Flipkart	['business', 'retail and shopping']	"About this dataset
&gt; <h3>Context</h3>
<p>This dataset was created by our team at <a href=""https://www.promptcloud.com/"" target=""_blank"">PromptCloud</a>  and <a href=""https://datastock.shop/"" target=""_blank"">DataStock</a> . We made this dataset for the customers to use for various analytic purposes. This dataset contains a 30K records sample. You can download the full dataset [here](<a href=""https://app.datastock.shop/?site_name=Flipkart"" target=""_blank"">https://app.datastock.shop/?site_name=Flipkart</a> Products 2019) .</p>
<h3>Content</h3>
<p>This dataset contains the following:<br>
available_fields"":<br>
""Uniq Id"",<br>
""Crawl Timestamp"",<br>
""Bb Category"",<br>
""Product Title"",<br>
""Product Description"",<br>
""Brand"",<br>
""Quantity Or Pack Size"",<br>
""Mrp"",<br>
""Price"",<br>
""Site Name"",<br>
""Offers"",<br>
""Combo Offers"",<br>
""Stock Availibility"",<br>
""Image Url"",<br>
""Url""</p>
<h3>Acknowledgements</h3>
<p>We wouldn't be here without the help of our in house web scraping and data mining teams at <a href=""https://www.promptcloud.com/"" target=""_blank"">PromptCloud</a>  and <a href=""https://datastock.shop/"" target=""_blank"">DataStock</a> .</p>
<h3>Inspiration</h3>
<p>This dataset was bundled keeping in mind the analysts and researchers who use data on a daily basis.</p>
This dataset was created by PromptCloud and contains around 10000 samples along with Image Url, Product Title, technical information and other features such as:
- Brand
- Crawl Timestamp
- and more.
How to use this dataset
&gt; - Analyze Combo Offers in relation to Site Name
- Study the influence of Quantity Or Pack Size on Product Description
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	150	870	9	yamqwe	flipkart-products-datasete
1524	1524	NYC Most Popular Baby Names Over the Years	Popular Baby Names in NYC from 2011-2014 with Nm, Rnk	['health', 'classification', 'clustering']	"About this dataset
&gt; <h1>Popular Baby Name Data In NYC from 2011-2014</h1>
<p>Rows: 13962; Columns: 6</p>
<p>The data include items, such as:</p>
<ul>
<li>BRTH_YR: birth year the baby</li>
<li>GNDR: gender</li>
<li>ETHCTY: mother's ethnicity</li>
<li>NM: baby's name</li>
<li>CNT: count of the name</li>
<li>RNK: ranking of the name</li>
</ul>
<p>Source: NYC Open Data</p>
<p><a href=""https://data.cityofnewyork.us/Health/Most-Popular-Baby-Names-by-Sex-and-Mother-s-Ethnic/25th-nujf"" target=""_blank"">https://data.cityofnewyork.us/Health/Most-Popular-Baby-Names-by-Sex-and-Mother-s-Ethnic/25th-nujf</a></p>
This dataset was created by Data Society and contains around 10000 samples along with Nm, Rnk, technical information and other features such as:
- Gndr
- Ethcty
- and more.
How to use this dataset
&gt; - Analyze Brth Yr in relation to Cnt
- Study the influence of Nm on Rnk
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	109	677	9	yamqwe	most-popular-baby-names-in-nyce
1525	1525	🪧 U.S. Presidents and Debt	All U.S. presidents since World War II to present (2016)	['history', 'government', 'military', 'politics']	"About this dataset
&gt; <h1>All U.S. presidents since WWII and their impact on the U.S. debt</h1>
<p>This dataset lists each president's impact on the U.S. debt since World War II to the present (2016), starting with Harry S. Truman to Barack Obama. The data is presented in two different ways:</p>
<ul>
<li>By President - inclusive of all terms served
<ul>
<li>Lists the <em>term debt increase percentage</em></li>
</ul>
</li>
<li>Annual breakdown - from 1949 to 2016
<ul>
<li>Lists the <em>annual debt increase percentage</em></li>
</ul>
</li>
</ul>
<p>The data also includes some meta information such as the term periods and lifespan, age, and party of each president.</p>
<p><a href=""https://www.whitehouse.gov/omb/budget/Historicals"" target=""_blank"">Source</a></p>
This dataset was created by Kevin Nayar and contains around 0 samples along with Party, Date Died, technical information and other features such as:
- Age
- Party
- and more.
How to use this dataset
&gt; - Analyze Date Died in relation to Age
- Study the influence of Party on Date Died
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kevin Nayar 
Start A New Notebook!"	70	401	8	yamqwe	u-s-presidents-and-debte
1526	1526	🏦 2012 US Retail Sales	A comprehensive look at US retailing, by store type and product type	['business', 'economics']	"About this dataset
&gt; <p>The Economic Census is done every 5 years, in years ending in 7 and 2.  The final 2012 numbers came out in 2016.  It is the most comprehensive look at the economy evailable.  Here is detailed information on US retailing, by store type and product type, so you can see what their merchandise mix is.<br>
Reports are custom generated on the <a href=""http://census.gov"" target=""_blank"">census.gov</a> site.</p>
 This dataset was created by Gary Hoover and contains around 10000 samples along with Products And Services Code, Response Coverage Of Product Lines Inquiry (%), technical information and other features such as:
- Product Line Sales As % Of Total Sales Of Estabs Reporting Line (%)
- 2012 Naics Code
- and more.
How to use this dataset
&gt; - Analyze Sales ($1,000) in relation to Meaning Of 2012 Naics Code
- Study the influence of Total Sales Of Estabs Reporting Product Line ($1,000) on Product Line Sales As % Of Total Sales Of All Estabs (%)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	68	474	6	yamqwe	2012-us-retail-salese
1527	1527	Popular Website Traffic Over Time 	Time series of website traffic of different categories	['websites', 'business', 'internet', 'social networks']	"About this dataset
&gt; <h1>Background</h1>
<p>Have you every been in a conversation and the question comes up, who uses Bing? This question comes up occasionally because people wonder if these sites have any views. For this research study, we are going to be exploring popular website traffic for many popular websites.</p>
<h1>Methodology</h1>
<p>The data collected originates from <a href=""http://SimilarWeb.com"" target=""_blank"">SimilarWeb.com</a>.</p>
<h1>Source</h1>
<p>For the analysis and study, go to <a href=""http://theconceptcenter.com/simple-research-study-popular-website-traffic/"" target=""_blank"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 0 samples along with 1/1/2017, Social Media, technical information and other features such as:
- 12/1/2016
- 3/1/2017
- and more.
How to use this dataset
&gt; - Analyze 11/1/2016 in relation to 2/1/2017
- Study the influence of 4/1/2017 on 1/1/2017
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	339	3406	27	yamqwe	popular-website-traffice
1528	1528	👜 Fashion products on Amazon.com	More than 10K fashion products from Amazon	['clothing and accessories', 'business', 'internet', 'retail and shopping', 'e-commerce services']	"&gt; #### This dataset was created by PromptCloud and contains 10000 fashion products extracted from Amazon. See the full dataset with 7M more samples here
<h1>About this Dataset</h1>
<p>This is a pre-crawled dataset, taken as subset of a <strong><a href=""https://www.promptcloud.com/datastock-access-ready-to-use-datasets?utm_source=data-world&utm_medium=referral"" target=""_blank"">bigger dataset (more than 7 million fashion products)</a></strong> that was created by extracting data from Amazon.</p>
<h1>Objectives</h1>
<p>Analyses of the ratings, price and reviews can be performed.</p>
<h1>Background</h1>
<p>This dataset was created by PromptCloud's in-house web-crawling service.</p>
This dataset was created by PromptCloud and contains around 10000 samples along with Customer Questions And Answers, Product Description, technical information and other features such as:
- Amazon Category And Sub Category
- Average Review Rating
- and more.
How to use this dataset
&gt; - Analyze Product Name in relation to Items Customers Buy After Viewing This Item
- Study the influence of Number Of Reviews on Product Information
- More datasets
Acknowledgements
If you use this dataset in your research, please credit PromptCloud 
Start A New Notebook!"	519	3168	25	yamqwe	fashion-products-on-amazon-come
1529	1529	Death Rate & Life-Expectancy Over The Years	Death Rates and Life Expectancy at Birth, 1900–2013	['health', 'social science']	"About this dataset
&gt; <p>This storyboard of U.S. mortality trends over the past 113 years highlights the differences in age-adjusted death rates and life expectancy at birth by race and sex; neonatal mortality and infant mortality rates by race; childhood mortality rates by age; and trends in age-adjusted death rates for five selected major causes of death.</p>
<ul>
<li>Age-adjusted death rates (deaths per 100,000) are based on the 2000 U.S. standard population.</li>
<li>Populations used for computing death rates for 2011–2013 are postcensal estimates based on the 2010 census, estimated as of July 1, 2010.</li>
<li>Rates for census years are based on populations enumerated in the corresponding censuses.</li>
<li>Rates for noncensus years before 2010 are revised using updated intercensal population estimates and may differ from rates previously published.</li>
</ul>
<p><img src=""http://i.imgur.com/9pc2V4v.png"" alt=""Imgur""><br>
<a href=""http://blogs.cdc.gov/nchs-data-visualization/deaths-in-the-us/"" target=""_blank"">National Center for Health Statistics Data Visualization of Deaths in the United States, 1900–2013 (6/01/15)</a></p>
<p>Attribution: <a href=""https://catalog.data.gov/dataset/age-adjusted-death-rates-and-life-expectancy-at-birth-all-races-both-sexes-united-sta-1900"" target=""_blank"">Centers for Disease Control and Prevention</a>.</p>
This dataset was created by Health and contains around 2000 samples along with Sex, Race, technical information and other features such as:
- Year
- Measure Names
- and more.
How to use this dataset
&gt; - Analyze Mortality in relation to Average Life Expectancy
- Study the influence of Sex on Race
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Health 
Start A New Notebook!"	184	1177	9	yamqwe	death-rate-and-life-expectancye
1530	1530	✈ Global Health Nutrition Data	Stats about the health, nutrition and population in the world	['global', 'healthcare', 'nutrition', 'public health', 'health', 'classification', 'regression']	"About this dataset
&gt; <p>HealthStats provides key health, nutrition and population statistics gathered from a variety of international sources. Themes include population dynamics, nutrition, reproductive health, health financing, medical resources and usage, immunization, infectious diseases, HIV/AIDS, DALY, population projections and lending. HealthStats also includes health, nutrition and population statistics by wealth quintiles.</p>
<p>This dataset includes 345 indicators, such as immunization rates, malnutrition prevalence, and vitamin A supplementation rates across 263 countries around the world. Data was collected on a yearly basis from 1960-2016.</p>
<p>Source: Kaggle</p>
<p><a href=""https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics"" target=""_blank"">https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics</a></p>
This dataset was created by Data Society and contains around 90000 samples along with 2000, 1973, technical information and other features such as:
- 1971
- 2011
- and more.
How to use this dataset
&gt; - Analyze Country Name in relation to 2002
- Study the influence of 1975 on 2012
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	302	2285	8	yamqwe	global-health-nutrition-datae
1531	1531	🏥 Predicting Cancer from Socioeconomic Status	Cancer Cases, Mortality, and Socioeconomic Status merged to a single dataset	['healthcare', 'public health', 'income', 'health', 'cancer']	"Credit: This dataset and analysis were made by Noah Rippner to study the relationship between socioeconomics and cancer.
About this dataset
<p>The goal for this analysis is to look for relationships between socioeconomic status and cancer. The main idea is to combine data from disparate open sources to check if poorer regions would have fewer trials, and higher per capita cancer incidence and death rates.</p>

<p>The data is joint data from <a href=""http://clinicaltrials.gov"" target=""_blank"">clinicaltrials.gov</a>, <a href=""http://cancer.gov"" target=""_blank"">cancer.gov</a> & <a href=""http://census.gov"" target=""_blank"">census.gov</a> to examine cancer trials, mortality, incidence and demographics.</p>

<h3>See <a href=""https://data.world/nrippner/cancer-trials/file/data_dict.xlsx"">data dictionary</a></h3>
<h3>See <a href=""https://github.com/nrippner/can_clin_trails_prep_r/blob/master/extract_zips_10.py"" target=""_blank"">data prep in Python</a></h3>

<h2>Input Data:</h2>
<h4>1. clinical trials <a href=""https://data.world/nrippner/cancer-trials/file/study_fields.csv"">('study_fields.csv')</a></h4>
<p>(.csv & .xml for all cancer trails 01/01/2010 through 06/01/2016)</p>
<p>source: <a href=""https://clinicaltrials.gov/ct2/results?term=&recr=&type=&rslt=&age_v=&gndr=&cond=cancer&intr=&titles=&outc=&spons=&lead=&id=&state1=&cntry1=NA%3AUS&state2=&cntry2=&state3=&cntry3=&locn=&rcv_s=01%2F01%2F2010&rcv_e=06%2F01%2F2016&lup_s=01%2F01%2F2010&lup_e=06%2F01%2F2016"" target=""_blank"">(link)</a></p>
<h4>2. Census income/poverty by county <a href=""https://data.world/nrippner/cancer-trials/file/cen_income.csv"">('cen_income.csv')</a>:</h4>
<p>source: <a href=""https://www.census.gov/did/www/saipe/data/statecounty/data/2014.html"" target=""_blank"">(link)</a></p>
<h4>3. FIPS/ZCTA <a href=""https://data.world/nrippner/cancer-trials/file/fips_zip_x.csv"">('fips_zip_x.csv')</a>:</h4>
<p>source: <a href=""https://www.census.gov/geo/maps-data/data/zcta_rel_download.html"" target=""_blank"">(link)</a></p>
<h4>4. Census population by county <a href=""https://data.world/nrippner/cancer-trials/file/census_county_population.csv"">('census_county_population.csv')</a>:</h4>
<p>source: <a href=""https://www.census.gov/popest/data/counties/totals/2015/index.html"" target=""_blank"">(link)</a></p>
<h4>5. Cancer incidence rates by county <a href=""https://data.world/nrippner/cancer-trials/file/incd_r.csv"">('incdf_r.csv')</a>:</h4>
<p>source: <a href=""https://statecancerprofiles.cancer.gov/incidencerates/index.php?stateFIPS=51&cancer=071&race=00&sex=0&age=001&type=incd#results"" target=""_blank"">(link)</a></p>
<h4>6. Cancer death rates by county <a href=""https://data.world/nrippner/cancer-trials/file/death_r.csv"">('death_r.csv')</a>:</h4>
<p>source: <a href=""https://statecancerprofiles.cancer.gov/cgi-bin/deathrates/deathrates.pl?99&001&00&0&001&0&1&1&1#results"" target=""_blank"">(link)</a></p>
<h2>Joined Data:</h2>
<h4>1. Grouped by county level <a href=""https://data.world/nrippner/cancer-trials/file/countyData.csv"">('countyData.csv')</a></h4>
<ul>
<li>studyCount feature aggregated by count, all others by mean</li>
</ul>
<h4>2. Grouped by zip code level <a href=""https://data.world/nrippner/cancer-trials/file/zipCodeData.csv"">('zipCodeData.csv')</a></h4>
<ul>
<li>all features aggregated by mean</li>
</ul>
<h4>3. Ungrouped <a href=""https://data.world/nrippner/cancer-trials/file/fullData.csv"">('fullData.csv')</a></h4>
<h2>Preliminary Findings (county level):</h2>
<h4>1. Number of cancer-related clinical trials per capita (per county) is <strong>not</strong> correlated with income, poverty, population, cancer incidence (per capita), or cancer mortality (per capita).</h4>
<p><img src=""http://i.imgur.com/aCNc0F7.png"" alt=""corr_matrix""></p>
<h4>2. Per capita cancer mortality rate is correlated with poverty</h4>
<p><img src=""http://i.imgur.com/4lJqmbm.png"" alt=""poverty_mortality""></p>
<p><em>Pearson r = 0.43, p = &lt; 0.000001</em></p>
<h4>3. Per capita cancer incidence rates are the same across income levels</h4>
<p><img src=""http://i.imgur.com/CGe5XPk.png"" alt=""income_incidence""></p>
<h4>4. Counties within the poorest decile have a 25% higher cancer mortality rate than the richest decile</h4>
<p><img src=""http://i.imgur.com/lc6xNFQ.png"" alt=""income_mortality""></p>
<h4>5. Richer counties host more clinical trails than poorer counties</h4>
<p><img src=""http://i.imgur.com/4LWCS66.png"" alt=""income_trials""></p>
<h4>6. But, this turns out to be spurious, moderated by population – the relationship between income and number of trials weakens with number of trials <em>per capita</em></h4>
<p><img src=""http://i.imgur.com/Pno0Plu.png"" alt=""income_per_cap_trials""></p>
This dataset was created by Noah Rippner and contains around 3000 samples along with Sumlev, Rbirth2014, technical information and other features such as:
- Stname
- Domesticmig2013
- and more.
How to use this dataset
&gt; - Analyze Rdomesticmig2012 in relation to Domesticmig2015
- Study the influence of Region on Internationalmig2011
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Noah Rippner 
Start A New Notebook!"	193	1462	9	yamqwe	cancer-trialse
1532	1532	🩰 Men's Shoe Prices	A list of 10,000 men's shoes provided by Datafiniti.co	['clothing and accessories', 'business', 'marketing', 'retail and shopping']	"About this dataset
&gt; <h1>About This Data</h1>
<p>This is a list of 10,000 men's shoes provided by <a href=""https://datafiniti.co/products/product-data/"" target=""_blank"">Datafiniti's Product Database</a>.</p>
<p>The dataset includes shoe name, brand, price, and more. Each shoe will have an entry for each price found for it and some shoes may have multiple entries.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with This Data</h1>
<p>You can use this data to <a href=""https://datafiniti.co/cost-of-designer-label/"" target=""_blank"">determine brand markups, pricing strategies, and trends for luxury shoes</a>. E.g.:</p>
<ul>
<li>What is the average price of each distinct brand listed?</li>
<li>Which brands have the highest prices?</li>
<li>Which ones have the widest distribution of prices?</li>
<li>Is there a typical price distribution (e.g., normal) across brands or within specific brands?</li>
</ul>
<p>Further processing data would also let you:</p>
<ul>
<li>Correlate specific product features with changes in price.</li>
<li>You can cross-reference this data with a sample of our <a href=""https://data.world/datafiniti/womens-shoe-prices"">Women's Shoe Prices</a> to see if there are any differences between women's brands and men's brands.</li>
</ul>
<h1>Data Schema</h1>
<p>A full schema for the data is available in our <a href=""https://datafiniti-api.readme.io/docs/product-data-schema"" target=""_blank"">support documentation</a>.</p>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 20000 samples along with Prices.amount Max, Website I Ds, technical information and other features such as:
- Prices.condition
- Date Updated
- and more.
How to use this dataset
&gt; - Analyze Sizes in relation to Prices.amount Min
- Study the influence of Prices.return Policy on Manufacturer Number
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	679	4635	35	yamqwe	men-s-shoe-pricese
1533	1533	🍺 The cost of a beer at MLB stadiums by Year,City	Based on City, Year and Locations	['alcohol', 'baseball', 'sports', 'business']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/f4ft32PQCGRBTGWezGS7_Screen%20Shot%202018-10-17%20at%208.15.36%20pm.png"" alt=""Screen Shot 2018-10-17 at 8.15.36 pm.png""></p>
<h1><strong>About this Dataset</strong></h1>
<ul>
<li>ORIGINAL VISUALIZATION: <a href=""https://www.vizwiz.com/2014/04/makeover-monday-what-beer-will-cost-you.html"" target=""_blank"">VizWiz</a></li>
<li>SOURCE: <a href=""https://www.teammarketing.com/"" target=""_blank"">Team Marketing Report</a></li>
</ul>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
<li>Post your alternative on the discussions page.</li>
</ul>
This dataset was created by Andy Kriebel and contains around 200 samples along with City, Year, technical information and other features such as:
- Size
- Nickname
- and more.
How to use this dataset
&gt; - Analyze Price in relation to Price Per Ounce
- Study the influence of Team on City
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Andy Kriebel 
Start A New Notebook!"	231	1257	12	yamqwe	2018-w43-the-cost-of-a-beer-at-mlb-stadiumse
1534	1534	US Consumer Spending	Annual survey of consumer expenditure patterns over time	['business', 'finance', 'economics']	"About this dataset
&gt; <p>Annual survey of consumer expenditure patterns from<br>
<a href=""http://www.bls.gov/cex/csxmulti.htm"" target=""_blank"">http://www.bls.gov/cex/csxmulti.htm</a></p>
 This dataset was created by Gary Hoover and contains around 200 samples along with 2013, 2014, technical information and other features such as:
- 2015
- Average Annual Expenditures And Characteristics Of All Consumer Units, Consumer Expenditure Survey, 2013 2015
- and more.
How to use this dataset
&gt; - Analyze 2013 in relation to 2014
- Study the influence of 2015 on Average Annual Expenditures And Characteristics Of All Consumer Units, Consumer Expenditure Survey, 2013 2015
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Gary Hoover 
Start A New Notebook!"	247	1836	11	yamqwe	us-consumer-spendinge
1535	1535	😷 NYC Leading Causes of Death	Datasets for Ethnicity Analysis	['public health', 'health']	"About this dataset
&gt; <h1>NYC Leading Causes of Death Data</h1>
<p>Rows: 3840; Columns: 6</p>
<p>The data includes items, such as:</p>
<ul>
<li>Year</li>
<li>Ethnicity</li>
<li>Sex</li>
<li>Cause of Death</li>
<li>Count</li>
<li>Percent</li>
</ul>
<p>Source: NYC Open Data</p>
<p><a href=""https://data.cityofnewyork.us/Health/New-York-City-Leading-Causes-of-Death/jb7j-dtam"" target=""_blank"">https://data.cityofnewyork.us/Health/New-York-City-Leading-Causes-of-Death/jb7j-dtam</a></p>
This dataset was created by Data Society and contains around 4000 samples along with Ethnicity, Sex, technical information and other features such as:
- Percent
- Count
- and more.
How to use this dataset
&gt; - Analyze Cause Of Death in relation to Year
- Study the influence of Ethnicity on Sex
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	216	1490	15	yamqwe	nyc-leading-causes-of-deathe
1536	1536	🍷 Alcohol Consumption by Country (2019)	Liters Of Pure Alcohol Consumed Per Capita, Literally Of Pur Purposes	['alcohol', 'health']	"About this dataset
&gt; <h1><strong>Original Visualization</strong></h1>
<p><img src=""https://media.data.world/JMDbwapcRRihNV0ikOIN_chart%20(1).png"" alt=""chart (1).png""></p>
<h1><strong>About this Dataset</strong></h1>
<p>SOURCE ARTICLE: <a href=""https://www.worldatlas.com/articles/who-drinks-the-most-alcohol-consumption-by-country.html"" target=""_blank"">World Atlas</a><br>
DATA SOURCE: <a href=""https://www.worldatlas.com/articles/who-drinks-the-most-alcohol-consumption-by-country.html"" target=""_blank"">World Atlas</a></p>
<h1><strong>Objectives</strong></h1>
<ul>
<li>What works and what doesn't work with this chart?</li>
<li>How can you make it better?</li>
</ul>
This dataset was created by Eva Murray and contains around 0 samples along with Liters Of Pure Alcohol Consumed Per Capita, Liters Of Pure Alcohol Consumed Per Capita, technical information and other features such as:
- Liters Of Pure Alcohol Consumed Per Capita
- Liters Of Pure Alcohol Consumed Per Capita
- and more.
How to use this dataset
&gt; - Analyze Liters Of Pure Alcohol Consumed Per Capita in relation to Liters Of Pure Alcohol Consumed Per Capita
- Study the influence of Liters Of Pure Alcohol Consumed Per Capita on Liters Of Pure Alcohol Consumed Per Capita
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Eva Murray 
Start A New Notebook!"	416	3566	22	yamqwe	2019-w26-alcohol-consumption-by-countrye
1537	1537	🍉 Food products and their Ingredients	10,000 Food products and their Ingredients.	['nutrition', 'business', 'food']	"About this dataset
&gt; <h1>About this Data</h1>
<p>This is a sample data set of ingredient lists pulled from <a href=""http://datafiniti.co/products/product-data/"" target=""_blank"">Datafiniti's Product Database</a>.  The data set covers 10,000 different food listings and includes the ingredient list for each one.</p>
<p><em>Note that this is a sample of a large dataset. The full dataset is available through Datafiniti.</em></p>
<h1>What You Can Do with this Data</h1>
<p>Use this data to discover insights into ingredients used in various foods.  E.g.:</p>
<ul>
<li>What's the distribution of number of ingredients per listing?</li>
<li>What are the most common ingredients used?</li>
</ul>
<h1>About Datafiniti</h1>
<p>Datafiniti provides instant access to web data.  We compile data from thousands of websites to create standardized databases of business, product, and property information.  <a href=""https://datafiniti.co"" target=""_blank"">Learn more</a>.</p>
<h1>Interested in the Full Dataset?</h1>
<p>Get this data and more by <a href=""https://datafiniti.co/pricing/product-data-pricing/"" target=""_blank"">creating a free Datafiniti account</a> or <a href=""https://datafiniti.co/request-a-demo/"" target=""_blank"">requesting a demo</a>.</p>
This dataset was created by Datafiniti and contains around 10000 samples along with , Manufacturer Number, technical information and other features such as:
- Date Added
- Upc
- and more.
How to use this dataset
&gt; - Analyze Ean in relation to Date Updated
- Study the influence of Features.key on Features.value
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Datafiniti 
Start A New Notebook!"	272	2294	19	yamqwe	food-ingredient-listse
1538	1538	🌁 Air Traffic Passenger Data	San Francisco International Airport Report on Monthly Traffic Statistics	['business', 'transportation', 'classification', 'clustering', 'travel']	"About this dataset
&gt; <p>San Francisco International Airport Report on Monthly Passenger Traffic Statistics by Airline. Airport data is seasonal in nature, therefore any comparative analyses should be done on a period-over-period basis (i.e. January 2010 vs. January 2009) as opposed to period-to-period (i.e. January 2010 vs. February 2010). It is also important to note that fact and attribute field relationships are not always 1-to-1. For example, Passenger Counts belonging to United Airlines will appear in multiple attribute fields and are additive, which provides flexibility for the user to derive categorical Passenger Counts as desired.</p>
<p>Source: San Francisco Open Data</p>
<p><a href=""https://data.sfgov.org/Transportation/Air-Traffic-Passenger-Statistics/rkru-6vcg"" target=""_blank"">https://data.sfgov.org/Transportation/Air-Traffic-Passenger-Statistics/rkru-6vcg</a></p>
This dataset was created by Data Society and contains around 20000 samples along with Terminal, Activity Period, technical information and other features such as:
- Published Airline Iata Code
- Adjusted Activity Type Code
- and more.
How to use this dataset
&gt; - Analyze Year in relation to Adjusted Passenger Count
- Study the influence of Passenger Count on Price Category Code
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Data Society 
Start A New Notebook!"	960	7083	37	yamqwe	air-traffic-passenger-datae
1539	1539	Netflix Shows	1000 collected samples, technical information	['arts and entertainment', 'movies and tv shows']	"About this dataset
&gt; <h1>Background</h1>
<p>Netflix in the past 5-10 years has captured a large populate of viewers. With more viewers, there most likely an increase of show variety. However, do people understand the distribution of ratings on Netflix shows?</p>
<h1>Netflix Suggestion Engine</h1>
<p>Because of the vast amount of time it would take to gather 1,000 shows one by one, the gathering method took advantage of the Netflix’s suggestion engine. The suggestion engine recommends shows similar to the selected show. As part of this data set, I took 4 videos from 4 ratings (totaling 16 unique shows), then pulled 53 suggested shows per video. The ratings include: G, PG, TV-14, TV-MA. I chose not to pull from every rating (e.g. TV-G, TV-Y, etc.).</p>
<h2>Source</h2>
<p>Access to the study can be found at <a href=""http://theconceptcenter.com/simple-research-study-netflix-shows-analysis/"" target=""_blank"">The Concept Center</a></p>
This dataset was created by Chase Willden and contains around 1000 samples along with User Rating Score, Rating Description, technical information and other features such as:
- Release Year
- Title
- and more.
How to use this dataset
&gt; - Analyze User Rating Size in relation to Rating
- Study the influence of Rating Level on User Rating Score
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Chase Willden 
Start A New Notebook!"	1812	10591	66	yamqwe	netflix-showse
1540	1540	COVID-19 Vaccinations: Daily updated by country	Daily Updated Vaccinations (COVID-19) given around the world	['healthcare', 'public health', 'health', 'public safety', 'covid19']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>The Our World in Data team's vaccination dataset uses the most recent official numbers from governments and health ministries worldwide. The population estimates we use to calculate per-capita metrics are all based on the last revision of the United Nations World Population Prospects. A full list of our country-specific sources is available <a href=""https://ourworldindata.org/covid-vaccinations#source-information-country-by-country"" target=""_blank"" rel=""nofollow"">here</a>, and answers to frequently asked questions can be found <a href=""https://ourworldindata.org/covid-vaccinations#frequently-asked-questions"" target=""_blank"" rel=""nofollow"">here</a>.</p>
<p>In our <a href=""https://ourworldindata.org/covid-vaccinations"" target=""_blank"" rel=""nofollow"">Data Explorer</a> you can see all of our data on COVID-19 vaccinations (doses administered, people with at least 1 dose, and people fully vaccinated).</p>
<h2><strong>Caveats</strong></h2>
<p>The <code>vaccinations.csv</code> dataset includes some subnational locations (England, Northern Ireland, Scotland, Wales, Northern Cyprus…) and international aggregates (World, continents, European Union…). They can be identified by their iso_code that starts with OWID_.</p>
<p>The population estimates we use to calculate per-capita metrics are all based on the last revision of the <a href=""https://population.un.org/wpp/"" target=""_blank"" rel=""nofollow"">United Nations World Population Prospects</a>. The exact values can be viewed <a href=""https://github.com/owid/covid-19-data/blob/master/scripts/input/un/population_2020.csv"" target=""_blank"" rel=""nofollow"">here</a>.</p>
<h2><strong>Calculation Example</strong></h2>
<p>4 people take part in a vaccination program, to be given a vaccine that requires 2 doses to be effective against the disease.</p>
<ul>
<li>Dina has received 2 doses;</li>
<li>Joel has received 1 dose;</li>
<li>Tommy has received 1 dose;</li>
<li>Ellie has not received any dose.</li>
</ul>
<p>In our data:</p>
<ul>
<li>The total number of doses administered (total_vaccinations) will be equal to 4 (2 + 1 + 1);</li>
<li>The total number of people vaccinated (people_vaccinated) will be equal to 3 (<code>Dina</code>, <code>Joel</code>, <code>Tommy</code>);</li>
<li>The total number of people fully vaccinated (people_fully_vaccinated) will be equal to 1 (<code>Dina</code>).</li>
</ul>
<h2><strong>Attribution</strong></h2>
<p>All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.</p>
<p>In the case of our vaccination dataset, please give the following citation:</p>
<blockquote>
<p>Mathieu, E., Ritchie, H., Ortiz-Ospina, E. et al. A global database of COVID-19 vaccinations. Nat Hum Behav (2021). <a href=""https://doi.org/10.1038/s41562-021-01122-8"" target=""_blank"" rel=""nofollow"">https://doi.org/10.1038/s41562-021-01122-8</a></p>
</blockquote>
This dataset was created by Amber Thomas and contains around 80000 samples along with Daily Vaccinations Per Million, Total Boosters Per Hundred, technical information and other features such as:
- Location
- Total Vaccinations Per Hundred
- and more.
How to use this dataset
&gt; - Analyze Total Vaccinations in relation to Daily People Vaccinated Per Hundred
- Study the influence of Total Boosters on Date
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	80	440	2	yamqwe	covid-19-vaccinationse
1541	1541	NBL STATS	National Basketball League (Australia) Leaders on Points	['basketball']	"National Basketball League (Australia) Leaders on Points
Glossary
GP Games Played
PPG Points Per Game
FGA Field Goals Attempted
RPG Average Rebounds per game
FGM Field Goals Made
FG% Field Goals Percentage
3PA Three-point Field Goals Attempted
3PM Three-point Field Goals Made
3P% Three-point Field Goals Percentage
FTA Free Throws Attempted
FTM Free Throws Made
FT% Free Throws Percentage
APG Assists per game
BPG Blocks per game
SPG Steals per game
TPG Turnovers per game
F Fouls
Source
https://nbl.com.au/stats/leaderboard"	0	11	5	stpeteishii	nbl-stats
1542	1542	tfbr31		[]		1	31	0	v1olet3	tfbr31
1543	1543	SymbolicGPT		[]		0	1	0	bestofbests9	symbolicgpt
1544	1544	Date Fruit Datasets	Date Fruit Datasets  (BERHI, DEGLET, DOKOL, IRAQI, ROTANA, SAFAVI, SOGAY)	['categorical data', 'computer science', 'classification', 'tabular data', 'multiclass classification']		9	65	15	mkoklu42	date-fruit-datasets
1545	1545	Brazilian Cities	A collection of 79 attributes from Brazilian Cities	['cities and urban areas', 'brazil', 'demographics', 'economics', 'tabular data']	"Context
Brazil is the world's fifth-largest country by area, with 8.5 million square kilometer, and the fifth most populous, with over 208 million people.
The Federative Republic of Brazil is composed of the union of the 26 states, the Federal District, and the 5,570 municipalities.
This dataset is a compilation of several publicly available information about Brazilian Municipalities.
Content
The data was retrieved from several websites, as presented below. I processed and  merged all data by city name, resulting in 79 fields for every city.
I am including the source of the information, in case the user wishes to understand better the information that is being analised.
FIELD - DESCRIPTION - REFERENCE - UNIT -  SOURCE
CITY - Name of the City
STATE - Name of the State
CAPITAL - 1 if Capital of State 
IBGE_RES_POP - Resident Population  - 2010 - source
IBGE_RES_POP_BRAS - Resident Population Brazilian - 2010 - source
IBGE_RES_POP_ESTR - Redident Population Foreigners - 2010 - source
IBGE_DU - Domestic Units Total  - 2010 - source
IBGE_DU_URBAN    - Domestic Units Urban  - 2010 - source
IBGE_DU_RURAL - Domestic Units Rural    2010 - source
IBGE_POP - Resident Population Regular Urban Planning - 2010 - source
IBGE_1 - Resident Population Regular Urban Planning - until 1 y.o - 2010 - source
IBGE_1-4 - Resident Population Regular Urban Planning - from 1 to 4 y.o - 2010 - source
IBGE_5-9 - Resident Population Regular Urban Planning - from 4 to 9 y.o - 2010 - source
IBGE_10-14 - Resident Population Regular Urban Planning - from 10 to 14 y.o - 2010 - source
IBGE_15-59 - Resident Population Regular Urban Planning - from 15 to 59 y.o 2010 - source
IBGE_60+ - Resident Population Regular Urban Planning - above 60 y.o - 2010 - source
IBGE_PLANTED_AREA - Planted Area (hectares)  - 2017 - 1 hectare (1 hectare = 10,000 square meters) - source
IBGE_CROP_PRODUCTION_$ - Crop Production - 2017 - $ 1,000 reais - source
IDHM Ranking - HDI Ranking - 2010 - source
IDHM - HDI Human Development Index - 2010 - source
IDHM_Renda - HDI GNI Index - 2010 - source
IDHM_Longevidade - HDI Life Expectancy index - 2010 - source
IDHM_Educacao   HDI Education index - 2010 - source
LONG - City Longitude - 2010 - source
LAT - City Latitude  - 2010 - source
ALT - City Elevation (meters) - 2010 - 1 meter - source
PAY_TV - PayTV users - 2019-03 - source
FIXED_PHONES - Fixed Fones (not cell phones) users - 2019-03 - source
AREA - City area (square kilometers) - 2018 - 1 square Kilometer (1 kilometer = 1,000,000 square meters) - source
REGIAO_TUR - Turism Category Region - 2017 - source
CATEGORIA_TUR - Turism Category - 2017 - source
ESTIMATED_POP - Estimated Population - 2018-07 - source
RURAL_URBAN - Rural or Urban Tipology - 2016 - source
GVA_AGROPEC - Gross Added Value - Agropecuary - 2016 - $ 1,000 reais - source
GVA_INDUSTRY - Gross Added Value - Industry - 2016 - $ 1,000 reais - source
GVA_SERVICES - Gross Added Value - Services - 2016 - $ 1,000 reais - source
GVA_PUBLIC - Gross Added Value - Public Services - 2016 - $ 1,000 reais - source
GVA_TOTAL   Total Gross Added Value - 2016 - $ 1,000 reais - source
TAXES - Taxes - 2016 - $ 1,000  - reais - source
GDP - Gross Domestic Product - 2016 - $ 1,000 reais - source
POP_GDP - Population - 2016 - source
GDP_CAPITA - Gross Domestic Product per capita   - 2016 - source
GVA_MAIN - Activity with higher GVA contribution - 2016 - source
MUN_EXPENDIT - Municipal expenditures - in reais - 2016 - $ 1 real - source
COMP_TOT - Total number of companies    2016 - source
COMP_A  Number of Companies: Agriculture, livestock, forestry, fishing and aquaculture - 2016 - source
COMP_B  Number of Companies: Extractive industries  2016 - source
COMP_C  Number of Companies: Industries of transformation - 2016 - source
COMP_D  Number of Companies: Electricity and gas - 2016 - source
COMP_E  Number of Companies: Water, sewage, waste management and decontamination activities - 2016 - source
COMP_F  Number of Companies: Construction - 2016 - source
COMP_G - Number of Companies: Trade; repair of motor vehicles and motorcycles - 2016 - source
COMP_H - Number of Companies: Transport, storage and mail - 2016 - source
COMP_I - Number of Companies: Accommodation and food - 2016 - source
COMP_J - Number of Companies: Information and communication - 2016 - source
COMP_K - Number of Companies: Financial, insurance and related services activities - 2016    - source
COMP_L - Number of Companies: Real estate activities - 2016 - source
COMP_M - Number of Companies: Professional, scientific and technical activities - 2016 - source
COMP_N - Number of Companies: Administrative activities and complementary services - 2016 - source
COMP_O - Number of Companies: Public administration, defense and social security - 2016 - source
COMP_P - Number of Companies: Education - 2016 - source
COMP_Q - Number of Companies: Human health and social services - 2016 - source
COMP_R - Number of Companies: Arts, culture, sport and recreation - 2016 - source
COMP_S - Number of Companies: Other service activities - 2016 - source
COMP_T - Number of Companies: Domestic services - 2016 - source
COMP_U - Number of Companies: International and other extraterritorial institutions - 2016 - source
HOTELS - Total number of hotels - 2019-03 - source
BEDS - Total number of hotel beds - 2019-03  - source
Pr_Agencies - Total number of private bank agencies - 2019-02 - source
Pu_Agencies - Total number of public bank agencies - 2019-02 - source
Pr_Bank - Total number of private banks - 2019-02 - source
Pu_Bank - Total number of public banks  2019-02  - source
Pr_Assets - Total amount of private bank assets - 2019-02   $ 1 real    source
Pu_Assets - Total amount of public bank assets - 2019-02    $ 1 real    source
Cars - Total number of cars - 2019-01 - source
Motorcycles - Total number of motorcycles, scooters, moped - 2019-01 - source
Wheeled_tractor - Total number of wheeled tractors - 2019-01 - source
UBER - 1 if UBER    2019-05 - source
MAC - Total number of Mac Donalds stores - 2018-11 - source
WALLMART - Total number of Walmart Stores - 2018-12 - source
POST_OFFICES - Total number of post offices - 2019-05 - source
Inspiration
Hopefully this data would help students that wish to train and improve their analytical skills and also whoever is interested in learning more about my country ! :-)"	6487	37573	188	crisparada	brazilian-cities
1546	1546	Pléiades Panchromatic Stereopairs. Elevation 4 m	Pléiades Panchromatic Stereopairs. Elevation 4 m. Example	['architecture', 'image data']		1	8	0	sergiishchus	pliades-panchromatic-stereopairs-elevation-4-m
1547	1547	AI4-HW2		[]		0	2	0	bichngocbui	ai4hw2
1548	1548	ChestX-Det10-Dataset		['biology']		0	12	0	ztamnaja	chestxdet10dataset
1549	1549	UTD MHAD dataset		['football']		0	0	0	sarthak4u	utd-mhad-dataset
1550	1550	SPOT 6-7. Elevation10 DSM	Barcelona, Spain. Example SPOT elevation date	['europe', 'health', 'architecture', 'image data']		1	6	1	sergiishchus	spot-67-elevation10-dsm
1551	1551	hello1112		[]		0	1	0	iftiben10	hello1112
1552	1552	textstat-0.7.2-py3	Calculate statistical features from text	['statistical analysis', 'text data', 'python']	Python package to calculate statistics from text to determine readability, complexity and grade level of a particular corpus	14	421	7	jarupula	textstat071py3
1553	1553	Imagenes_redes_neuronales		[]		0	9	0	leandropea	imagenes-redes-neuronales
1554	1554	Crop Recommend Data	Thid dataset is used to Predict the crop details	['india', 'business', 'data cleaning', 'tabular data', 'matplotlib', 'pandas']		5	70	4	futurecode	crop-recommend-data
1555	1555	WAR! conflicts and nations who took part in them	A dataset of all wars and nations who fought in them	['games', 'global', 'data analytics', 'clustering', 'tabular data']		62	459	10	guybarash	war-conflicts-and-nations-who-took-part-in-them
1556	1556	Fingerprint Images		[]		0	10	0	saravananms	fingerprint-images
1557	1557	happywhale-tfrecords-v1-128x128		[]		0	8	0	sega1031	happywhale-tfrecords-v1-128x128
1558	1558	Dobble	Object Detection Dataset	['games', 'computer science', 'beginner', 'computer vision', 'image data']		1	26	1	atugaryov	dobble
1559	1559	Google Capstone 1 - Jan 2021 until Dec 2021	Part of Google Data Analytics project	[]		0	9	0	kriukkerupuk	google-capstone-2021-data
1560	1560	Train-test Heart dataset		['health']		0	7	0	maylidadwi	traintest-heart-dataset
1561	1561	simplessd		[]		1	12	0	omoriyuki	simplessd
1562	1562	news_classification		[]		0	3	0	shlshlshk	news-tag
1563	1563	Figures Dataset Organized		[]		0	7	0	naren319	figures-dataset-organized
1564	1564	BEIT-L224		[]		0	9	0	ni7san	beitl224
1565	1565	Dyslexia Handwriting Dataset		[]		2	9	0	drizasazanitaisa	dyslexia-handwriting-dataset
1566	1566	Ubiquant Market Prediction - Feather (float16)		[]		3	22	0	hscosta	ubiquant-market-prediction
1567	1567	BiLSTMtpufold1		[]		0	1	0	mahdibb	bilstmtpufold1
1568	1568	Proyecto 4: Análisis de Cancelaciones Hoteleras	Analizarás cancelaciones de la industria del turismo	['beginner', 'data visualization', 'data analytics', 'text data', 'travel']	"Introducción
En este proyecto realizarás un análisis de reservas en un negocio de hoteles. Para el desarrollo de este proyecto pondrás en práctica lo aprendido hasta el momento y te introducirás en el uso de dos herramientas importantes para toda analista de datos. Una de ellas es BigQuery, un motor de base de datos creado por Google, que permite manipular datos de forma eficiente a través del lenguaje SQL. La segunda es PowerBI, una herramienta de visualización de datos creada por Microsoft.
La situación
Hace unos meses atrás, Estela, tu jefa y mentora en tu antiguo trabajo, se acercó con una noticia que cambió el rumbo de tu carrera. Te comentó que le habían ofrecido ser CEO de una empresa hotelera muy importante del país y quería que te unieras a su nuevo equipo desempeñando el rol de analista de datos. Era un salto gigante para Estela, quien en ese momento se desempeñaba como gerente de producto, pero te hizo total sentido. Estela es una crack de cracks y si alguien podría dar ese salto de gerente a CEO, era ella. Estela es, como dicen, una Jedi Master, o para quienes prefieren referencias de Harry Potter, es el equivalente de Dumbledore. Por eso, le dijiste que sí al instante. Era una gran oportunidad para seguir aprendiendo de ella y, además, representaba un salto importante a nivel profesional para tí también. 
Ya tenían varias semanas en la nueva empresa tratando de entender el negocio y haciendo una inducción detallada, cuando descubrieron algo interesante. Estaban en un Zoom tú, Estela y Daniel, el gerente de finanzas, en el momento que Estela comentó: “Estoy revisando este contrato que tenemos con la agencia de marketing que nos ayuda a traer clientes potenciales al hotel, y estoy viendo que les pagamos un costo fijo mensual pequeño por el servicio pero la mayoría es un costo variable atado a las reservas”. Daniel rápidamente la interrumpió: “¡Sí, eso es bueno! Porque así pagamos contra resultado; es decir, si nos traen un cliente potencial que no hace una reserva, entonces no tenemos que pagarles casi nada. Solo pagamos cuando esos clientes potenciales, esos leads, que nos traen realmente hacen una reserva en el hotel. ¡Yo mismo negocié ese contrato!”
“Sí, eso lo entiendo Daniel”, respondió Estela, haciendo el esfuerzo por ser paciente, “pero ¿qué sucede si el cliente termina cancelando su reserva? En ese caso estamos pagando por algo que tampoco se termina materializando en ventas, porque entiendo que los clientes sólo pagan cuando llegan al hotel, no al momento de hacer su reserva. Yo sé que tengo menos de 2 meses aquí y aún estoy aprendiendo, pero en este tiempo he visto muchas reservas cancelarse. ¿Cuánto es la tasa de cancelación de reservas? No me sorprendería que se cancele 1 de cada 3 reservas. Si es así, estamos perdiendo una buena cantidad de dinero al año”. Daniel se quedó pasmado. Simplemente pudo responder: “Sí, la verdad es que ese contrato lo hicimos hace muchos años atrás, cuando recién iniciamos, y en ese momento las cancelaciones eran muy infrecuentes por lo que lo dejamos por fuera del contrato. La verdad no hemos revisado si esa situación ha cambiado.” 
Estela te miró pero antes de que pudiera hablar, dijiste: “Sí, no te preocupes. Yo me encargo de hacer ese análisis. Sólo necesito la base de datos de las reservas y saber el monto del costo variable que le estamos pagando a la agencia”.  “Gracias” respondió Estela con una sonrisa. “Por lo que estoy leyendo, el contrato establece que debemos pagar 1,5 USD por cada reserva realizada"".
En eso Daniel, quien ya se había despabilado mencionó: “Estaría genial ese análisis. Con esa información podemos renegociar el contrato con la agencia. De hecho, otro factor importante referente a las cancelaciones que yo vengo diciendo desde hace tiempo, es que no todas las cancelaciones son iguales. Cuando un cliente cancela a última hora incurrimos en muchos otros costos adicionales. Según estimados que hemos realizado desde el equipo de finanzas, cuando alguien cancela con menos de 3 días de anticipación nos cuesta alrededor de 120 USD adicionales. Sería bueno agregar esto al análisis. ”
“Buen punto Daniel”, dijo Estela. “También, si ya vamos a entrar a analizar las cancelaciones, ¡entendamos por qué se dan! ¿Cuáles son las reservas con mayor riesgo de cancelación? A mí se me vienen a la cabeza una serie de hipótesis.” 
“¡A mí también!”, respondieron Daniel y tú, casi al mismo tiempo. Entre los tres acordaron que las hipótesis principales que guiarán esta parte del análisis son las siguientes:
Las reservas que se hacen con mayor anticipación tienen mucho riesgo de cancelarse.
Las reservas que incluyen hijos  tienen menor riesgo.
Los usuarios que realizaron algún cambio en su reserva tienen menor riesgo.
Cuando el usuario ha  realizado una solicitud especial el riesgo es menor.
Las reservas que tienen un “adr” bajo el riesgo es menor.
Con esta información te pusiste manos a la obra.
Entregable
Para considerar completado este proyecto deberás entregar, por medio de la plataforma de aprendizaje, lo siguiente:
Un video de máximo 3 minutos simulando un avance que le envías a tu jefa, en donde le expliques tus conclusiones y recomendaciones. Para esto puedes apoyarte en tu dashboard de PowerBI o armar una presentación en Google Slides. Para grabarte te recomendamos la plataforma Loom. En particular, tu video debe responder las siguientes preguntas:
a. ¿Cuánto sería el ahorro anual estimado si renegocian el contrato con la agencia para que el pago sea por reserva materializada (no cancelada)? Explica cómo llegaste a ese número.
b. ¿Qué puedes decir de las hipótesis planteadas sobre las reservas más propensas a ser canceladas? ¿Qué recomendaciones tienes a partir de tus hallazgos?
Un dashboard en Power BI que tenga todas las visualizaciones que utilizaste para argumentar tus conclusiones del video. 
Objetivos de aprendizaje
Al resolver este proyecto, aprenderás a:
- Importar datos desde un archivo en formato .csv
- Comprender el flujo y las etapas del proceso de un análisis.
- Entender y definir métricas de negocio.
- Realizar la limpieza de datos necesaria.
- Visualizar el comportamiento de los datos a través del tiempo.
- Realizar un análisis por cohortes de la información a partir de la data granular.
- Entender y aplicar una metodología RFM para realizar una segmentación a partir del principio de Pareto.
- Elaborar una presentación hacia las partes interesadas.
- Entender la población de análisis
Recursos recomendados
Es muy probable que este proyecto sea la primera vez que te enfrentes a realizar un análisis de datos fuera de un entorno de Spreadsheets y sea tu primer acercamiento a SQL. Está en tí cómo abordar el desafío de este proyecto, pero nuestra recomendación es que antes de enfocarte en el proyecto, te familiarices con SQL y BigQuery de forma general. 
Te recomendamos primero completar el curso SQL en Google BigQuery en la plataforma de SkillsBuild para después continuar con la  guía de desarrollo. Especial énfasis en poder:
Utilizar SELECT para seleccionar los campos de una o varias columnas en una consulta
Filtrar filas de tus consultas con la cláusula WHERE en columnas con números enteros.
Ordenar tus resultados con la sentencia ORDER BY según los datos de una o varias columnas, de forma ascendente y descendente.
Realizar agregaciones utilizando funciones como SUM y COUNT
Agrupar resultados de consultas con la sentencia GROUP BY
Realizar el curso de PowerBI que hemos dejado en el learning path de Skillsbuild."	13	45	3	datacertlaboratoria	proyecto-4-anlisis-de-cancelaciones-hoteleras
1569	1569	University Branding Strategy Attracts Students		[]		0	3	0	sylarlucas	university-branding-strategy
1570	1570	NYC Property Valuation and Assessment Data	From NYC Open Data (Department of Finance)	['united states', 'categorical data', 'real estate', 'exploratory data analysis', 'pandas']	"I found this dataset on NYC's Open Data website. NYC does a very good job of tracking their data. This data shows as of June 30, 2021 all the valuations of all the properties in NYC. There are 9.85 million rows! This is a wonderful dataset to exercise data exploration at a large scale and practice using data management tools like pandas!
Inside you will find specific information of each property such as property type, the borough it is in, and if it is privately or publicly owned. You can see the monetary values set forth by NYC's Department of Finance for the properties. You can answer questions such as: What is the total sum of all the property values in Manhattan? 
All the credit for this dataset goes to NYC's Open Data website. 
Link can be found here: https://data.cityofnewyork.us/City-Government/Property-Valuation-and-Assessment-Data/yjxr-fw8i
How will each borough in NYC grow or shrink in prices? What will the prices be in the future? What are the most expensive properties? What is the standard deviation for prices per borough? What is the correlation between property tax class and price of property? The list of questions can go on and on!"	0	33	1	akuppps	nyc-property-valuation-and-assessment-data
1571	1571	 Raisin Dataset	 Raisin Dataset 7 morphological features and (Kecimen, Besni)	['categorical data', 'artificial intelligence', 'computer science', 'classification', 'tabular data']		4	31	16	mkoklu42	raisin-dataset
1572	1572	pdata set		[]		1	2	0	ahmedallawy	pdata-set
1573	1573	H&M-(1024x1024) dataset	Reduced Image resolution for the H&M competition.	['art', 'clothing and accessories', 'tabular data', 'image data', 'retail and shopping']	"Reduced Image resolution for the H&M competition.
Competition link - https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations
Notebook with which images were resized - https://www.kaggle.com/odins0n/h-m-image-resizer-dataset-256x256"	37	467	20	odins0n	hm1024x1024
1574	1574	Dry Bean Dataset	Dry Bean Dataset SEKER, BARBUNYA, BOMBAY, CALI, HOROZ, SIRA, DERMASON	['categorical data', 'agriculture', 'computer science', 'clustering', 'multiclass classification']	"https://www.kaggle.com/zhixx018/dry-bean-dataset-uci
Source:
Murat KOKLU
Faculty of Technology,
Selcuk University,
TURKEY.
ORCID : 0000-0002-2737-2360
mkoklu '@' selcuk.edu.tr
Ilker Ali OZKAN
Faculty of Technology,
Selcuk University,
TURKEY.
ORCID : 0000-0002-5715-1040
ilkerozkan '@' selcuk.edu.tr
Data Set Information:
Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.
Attribute Information:
1.) Area (A): The area of a bean zone and the number of pixels within its boundaries.
2.) Perimeter (P): Bean circumference is defined as the length of its border.
3.) Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.
4.) Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.
5.) Aspect ratio (K): Defines the relationship between L and l.
6.) Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.
7.) Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.
8.) Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.
9.) Extent (Ex): The ratio of the pixels in the bounding box to the bean area.
10.)Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.
11.)Roundness (R): Calculated with the following formula: (4piA)/(P^2)
12.)Compactness (CO): Measures the roundness of an object: Ed/L
13.)ShapeFactor1 (SF1)
14.)ShapeFactor2 (SF2)
15.)ShapeFactor3 (SF3)
16.)ShapeFactor4 (SF4)
17.)Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)
Relevant Papers:
KOKLU, M. and OZKAN, I.A., (2020), â€œMulticlass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.â€ Computers and Electronics in Agriculture, 174, 105507.
DOI: [Web Link]
Citation Request:
KOKLU, M. and OZKAN, I.A., (2020), â€œMulticlass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.â€ Computers and Electronics in Agriculture, 174, 105507.
DOI: [Web Link]"	4	34	16	mkoklu42	dry-bean-dataset
1575	1575	wbfmaster		[]		0	5	0	v1olet5	wbfmaster
1576	1576	image denoising		['arts and entertainment']		0	3	0	mohamedkhaled201841	image-denoising
1577	1577	Loan Approval Prediction Machine Learning		['lending']		1	25	0	bhupendramishra7	loan-approval-prediction-machine-learning
1578	1578	Spanish Graduates Employability	Employment situation in 2019 of 2013/2014 university graduates in Spain 	['universities and colleges', 'categorical data', 'employment', 'education', 'economics']	"Context
This data set contains information about 2013/2014 university graduates in Spain including their employment situation in 2019 as provided by the Instituto Nacional de Estadística (INE) in the EILU (Encuesta de Inserción Laboral de los titulados Universitarios) survey.
Content
There are two different .csv files. EILU_GRAD_2019.csv contains data of approximately 32.000 BSc and BA graduates (2013/2014), whereas EILU_MAST_2019.csv contains data of approximately 12.000 MSc and MA graduates (2013/2014).
Both datasets provide various personal and demographic features such as gender, age, nationality, major, field, parents' level of education, type of university, etc... 
They also provide information about regulated and supplementary training, mobility, current employment status, and work history.
Most of the data is categorical. The codes for the different categories can be found in the data dictionaries, dr_EILU_GRAD_2019.xlsx, and dr_EILU_MAST_2019.xlsx. Unfortunately, they are only available in Spanish.
Acknowledgements
This data is provided by Instituto Nacional de Estadística (INE)  the official agency in Spain that collects statistics about demography, economy, and Spanish society.
Inspiration
This dataset can provide plenty of questions. 
Some questions of particular interest can be:
- Which factors are the main drivers of employability
- Measuring the mismatch between field of study and occupation
- How long does it take to land their first job to different kinds of graduates
- How does going to a private university impacts employment prospects?"	6	62	0	antoniotello	spanish-graduates-employability
1579	1579	Loan Prediction	Loan Prediction | ML	[]		2	54	8	mohinurabdurahimova	loan-prediction
1580	1580	facial		[]		0	1	0	echoblade	facial
1581	1581	torchvision_git_fcos		[]		0	3	0	venderst	torchvision-git-fcos
1582	1582	xray_binary_covid		['health']	"processed covid-19 Xray images for deep learning models.
reference:
https://data.mendeley.com/datasets/8h65ywd2jr/3 [ Walid El-Shafai,Fathi Abd El-Samie]
https://www.kaggle.com/bachrr/covid-chest-xray [ Bachir ]
https://www.kaggle.com/khushwantparihar/covid-detectionxray-images [Khushwant Parihar]
https://www.kaggle.com/tawsifurrahman/covid19-radiography-database [Tawsifur Rahman, Dr. Muhammad Chowdhury, Amith Khandakar]
W. Ning, S. Lei, J. Yang et al., “Open resource of clinical data from patients with pneumonia for the prediction of covid-19 outcomes via deep learning,” Nature Biomedical Engineering, 2020  [Wanshan Ning, Shijun Lei, Jingjing Yang, Yukun Cao]
https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia [Paul Mooney ]
“Chestx-ray8: Hospital-scale chest xray database and benchmarks on weakly-supervised classification and localization of common thorax diseases "" [Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu ]
Identifying medical diagnoses and treatable diseases by image-based deep learning  [Daniel S. Kermany, Michael Goldbaum,WWenjia Cai]
: https: //www.kaggle.com/mohamedhanyyy/chest-CT scan-images [Mohamed Hany ]"	0	4	0	aravindlade	xray-binary-covid
1583	1583	ct_binary_covid		[]		0	5	0	aravindlade	ct-binary-covid
1584	1584	Quality of the Comments	Categorize comments as Good or BAD based on various parameters	['bayesian statistics', 'beginner', 'tabular data', 'online communities', 'social networks', 'sklearn']	"Context
The CrocDen is an automated comment rating site. It marks each comment in a community forum website as g for “good” or b for “bad” based on the quality of the post.
Content
These are the details of the columns describing each comment,
i. number_words: number of words in the comment
ii. number_characters: number of characters in the comment
iii. number_misspelled: number of misspelled words
iv. binn_end_qmark: if the comment ends with a question mark
v. number_interrogative: number of interrogative words in the comment
vi. binn_start_small: if the comment starts with a lowercase letter. (‘1’ means yes, otherwise no)
vii. number_sentences: number of sentences per comment
viii. number_punctuations: number of punctuation symbols in the comment
ix. label: the label of the comment (‘G’ for good and ‘B’ for bad) as determined.
Acknowledgements
I modified the old dataset from a problem set.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	5	0	chinmaysy	winequality
1585	1585	Indian Civilian Awards	This is the ultimate list of all indian civilian award winners	['arts and entertainment', 'india', 'beginner', 'intermediate', 'news']	"Context
I decided to make a model to, predict who would win the next Padam Shri awards but could find proper datasets so i decided to make my own.
Content
The dataset has three separate files containing the winners of all Indian civilian awards from 1954 onwards. except for the Padma Bhushan winners.
can be used for deploying classification models.
Acknowledgements
Wikipedia : https://en.wikipedia.org/wiki/Padma_Shri
                   https://en.wikipedia.org/wiki/Bharat_Ratna
                   https://en.wikipedia.org/wiki/Padma_Vibhushan
Inspiration
nothing specific read up on some yesterday."	6	52	6	proteanox	indian-civilian-awards
1586	1586	modelh5		[]		0	0	0	datenjisherpa	modelh5
1587	1587	Teacher dataset		['education']		1	10	0	indrar	teacher-dataset
1588	1588	H&M-(512x512) dataset	Reduced Image resolution for the H&M competition.	['art', 'clothing and accessories', 'tabular data', 'image data', 'retail and shopping']	"Reduced Image resolution for the H&M competition.
Competition link - https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations
Notebook with which images were resized - https://www.kaggle.com/odins0n/h-m-image-resizer-dataset-256x256"	81	580	19	odins0n	hm512x512
1589	1589	H&M-(384x384) dataset	Reduced Image resolution for the H&M competition.	['art', 'clothing and accessories', 'tabular data', 'image data', 'retail and shopping']		4	52	5	odins0n	hm384x384
1590	1590	Cifar-10		[]		0	2	0	sophiewrd25	cifar10
1591	1591	finetuning		[]		0	13	0	omoriyuki	finetuning
1592	1592	Pseudo Labelled Data for TPS Feb 2022	TPS Feb 2022 Pseudo labelled data	['computer science']		0	105	0	nikhilkhetan	pseudo-labelled-data-for-tps-2022
1593	1593	Twitchers statistics - high dimensional	statistics coming from different twitcher profiles	['celebrities', 'video games', 'tabular data']	"Dataset about 30 twitchers constructed using public information principally coming from https://twitchtracker.com/ . 
All data were collected by hand. 
High dimensional - feature selection. 
High variability or features having values that are too different can be a problem to predict whatever."	1	20	0	richv0	twitchers-statistics-high-dimensional
1594	1594	House MD Transcripts	Scraped data from complete eight-season scripts of the Fox Medical Drama.	['arts and entertainment', 'movies and tv shows', 'medicine', 'internet', 'nlp']	"Context
House M.D is an American television medical drama that originally ran on the Fox network for eight seasons, from November 16, 2004 to May 21, 2012. The show's main character is Dr. Gregory House, a pain medication-dependent, unconventional, misanthropic medical genius who leads a team of diagnostic fellows at the fictional Princeton–Plainsboro Teaching Hospital (PPTH) in Princeton, New Jersey. 
Content
The dataset contains 72286 rows and 2 columns in a tabular format divided into 8 seasons.
Acknowledgements
Source: Clinic Duty LiveJournal"	22	422	19	kunalbhar	house-md-transcripts
1595	1595	bboxmain		[]		0	0	0	ychikusa	bboxmain
1596	1596	cascade_rcnn_pre-train		[]		0	5	0	dwchen	cascade-rcnn-pretrain
1597	1597	Email_HamSpam_Dataset	Good for Supervised method	['computer science', 'intermediate', 'classification', 'tabular data', 'e-commerce services']		6	24	0	hamedetezadi	email-hamspam-dataset
1598	1598	summarizer weights		['exercise']		0	4	0	creecsh	summarizer-weights
1599	1599	ArzToday	آموزش ارز دیجیتال به زبان ساده و رایگان	[]	ارزتودی، با هدف تولید محتوای جامع در حوزه کریپتوکارنسی فعالیت حرفه ای خود را در سال ۱۴۰۰ شروع کرده است، آموزش ارزهای دیجیتال، نحوه ورود به بازارهای مالی و تحلیل و سرمایه گذاری در حوزه رمزارزها موضوعاتی هستند که به طور جامع در آکادمی به آنها پرداخته شده تا شما عزیزان بتوانید دانش خود را به روز نگه دارید؛ قبل از اینکه وارد مبحث آموزش ارزهای دیجیتال شوید، خوب است بدانید، سال‌هاست بیشتر صنایع موجود در دنیا به سمت دیجیتالی شدن سوق پیدا کرده است و نظام بانکداری و پول هم از این قاعده مستثنی نبوده، به عبارت دیگر ارزهای دیجیتال به همین منظور ظهور پیدا کرده اند. رمز ارزها این امکان را به شما می‌دهند که کالا و خدمات بخرید یا حتی در آنها سرمایه‌ گذاری کنید؛ شاید برای افرادی که تازه با دنیای ارزهای دیجیتال آشنا شده اند، شنیدن اصطلاحات مختلف گنگ و گاها عجیب به نظر برسد، اما با مطالعه از منابع معتبر خواهید دید که به راحتی می‌توان به یک خبره در زمینه ارزهای دیجیتالی تبدیل شد.	0	4	0	zahrashahabizadeh	arztoday
1600	1600	How to Access MDaemon WorldClient to Office 365?	MDaemon to Office 365 Tool to Batch Access MDaemon WorldClient to Office 365	['business', 'software', 'email and messaging']	"Are you looking for the best solution to access MDaemon Worldclient to Office 365 account without any data modification? Or want to export multiple MDaemon mailboxes to O365 with all emails, notes, calendars, contacts and other items? Then, you are at the correct page. Today, in this article, we are going to learn the complete solution to migrate MDaemon server data to Office 365. Just keep reading the post. 
MDaemon to Office 365 Migration Tool- Brilliant & Quick Solution
MDaemon to Office 365 Migration Tool is a powerful and effective solution to migrate MDaemon email, note, calendar, contact etc. to Office 365 account without any data loss. You can also use the batch option to export multiple MDaemon mailboxes at once by providing O365 account login id & password. MDaemon to Office 365 Converter Tool can preserve email elements and folder structure during the migration task. This application comes with a simple and easy to use GUI for all kinds of users. One can download the software on any edition of Windows OS easily. 
How to Use MDaemon to Office 365 Tool?
Download & install MDaemon Converter on your local machine.
Upload MDaemon Files/Folders with dual options like Select Files or Select Folders. 
Choose the desired MDaemon Folders for migration. 
Select the Office 365 option and fill Login Credentials. 
Press on Convert tab to start the MDaemon to O365 migration process.
&gt; Demo Restriction
The demo version will allow users to migrate the first 25 items from each loaded MDaemon folder to an O365 account without any cost. Buy the license key to export multiple MDaemon mailboxes.
Key Features of MDaemon to Office 365 Converter
•   Migrate MDaemon to Office 365 in batch 
•   Export MDaemon to O365 including emails, notes, contacts, calendars, etc. 
•   Migrating emails from MDaemon to O365 with attachments
•   Fast, secure & quick migration application 
•   Provide filters for selective MDaemon to Office 365 migration.
•   Compatible with all Windows OS- 11, 10, 8.1, 8, 7, etc.
Final Words
If you want to access MDaemon Worldclient to Office 365, then we recommend that you use CubexSoft MDaemon Converter Tool. It is a superb program and it will help you to save your time and efforts. The tool allows to migrate multiple MDaemon files in O365 at once and it also facilitates users to export emails from MDaemon to different file formats like Gmail, G Suite, Exchange Server, Outlook.com, Windows Live Mail, Thunderbird, Zimbra, Yahoo, PST, PDF, EML, EMLx, CSV, vCard, TXT, RTF, MBOX, HTML and more. Free trial edition of the software is available for testing purpose."	0	20	0	jennyharbor	how-to-access-mdaemon-worldclient-to-office-365
1601	1601	ktm__bick		[]		0	1	0	vivekbaaganps	ktm-bick
1602	1602	Millroad cleaned - High dimensionality	traffic data of Millroad street	['transportation', 'tabular data']	"Millroad dataset cleaned to be with high dimensionality. 
Every column gives the number of subjects (cars, cyclists, ...) at the different day's hours. 
Rows are different days measured.
Data originally taken from: https://data.cambridgeshireinsight.org.uk/dataset/mill-road-project-traffic-sensor-data
License: UK Open Government Licence (OGL)"	0	2	0	richv0	millroad-cleaned-high-dimensionality
1603	1603	titanic		[]		0	7	0	kadiryaar	titanic
1604	1604	crackImages	crack images with border to adjust distortion	['arts and entertainment']		2	234	0	apisakjutasiriwong	crackimages
1605	1605	keyword-detection-model-84		[]		0	2	0	kcy0206	keyword-detection-model-84
1606	1606	Assignment		[]		2	3	0	allelbhagya	assignment
1607	1607	BTC-USD		['currencies and foreign exchange']		0	2	0	dhanushkeshawa	btcusd
1608	1608	Retail Transaction Data	Slovak Retail Transaction Data	['categorical data', 'tabular data', 'text data', 'retail and shopping']	"Context
This dataset contains transactions and the products they contain, which were obtained by scanning receipts from retail establishments by numerous users. Products were categorized by our proprietary NLP model.
Content
Data was collected over a one-year period and contains product information from purchases made within that period, product category inferred from product name, information about organization, transaction to which products belong to and user that uploaded receipt.
The total user count is 22.
The total retail organization count is 179.
The total transaction count is 805.
The total product count is 7477.
Acknowledgements
@kserno 
Inspiration
Product categorization, User Behaviour Analysis, Product Analysis, Product Price Comparison between Various Retail Stores, Prediction of Next Transaction"	22	232	2	michalfr	retail-transaction-data
1609	1609	New Dataset		[]		0	7	0	kaziimranahmed	new-dataset
1610	1610	whale-tfrecords-512		[]		8	50	0	manojprabhaakr	whale-tfrecords-512
1611	1611	catdog28001200		[]		0	1	0	temmylan	catdog28001200
1612	1612	BddTest		[]		0	3	0	allanpineau	bddtest
1613	1613	train_COVIDx9A		[]		0	3	0	tuanledinh	train-covidx9a
1614	1614	H&M-(128x128) dataset	Reduced Image resolution for the H&M competition.	['art', 'clothing and accessories', 'tabular data', 'image data', 'retail and shopping']	"Reduced Image resolution for the H&M competition.
Competition link - https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations
Notebook with which images were resized - https://www.kaggle.com/odins0n/h-m-image-resizer-dataset-256x256"	37	380	22	odins0n	handm-dataset-128x128
1615	1615	H&M-(256x256) dataset	Reduced Image resolution for the H&M competition.	['art', 'clothing and accessories', 'tabular data', 'image data', 'retail and shopping']	"Reduced Image resolution for the H&M competition.
Competition link - https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations
Notebook with which images were resized - https://www.kaggle.com/odins0n/h-m-image-resizer-dataset-256x256"	22	236	8	odins0n	hm256x256
1616	1616	happywhale-tfrecords-v2		[]		0	2	0	manojprabhaakr	happywhale-tfrecords-v2
1617	1617	Serie A Matches Dataset	Dataset with all Serie A matches since 2018-2019 season	['football', 'sports', 'tabular data']	"Context
This dataset contains results and several stats for all Serie A matches since 2018-2019 season. 
Content
Matches are splitted in file by season. All statistics are collected through free APIs and web scraping. Results are updated after every round.
Tasks
With these feature you can try to perform a lot of task. For example, can you predict match winner or if the teams score more or less than 2.5 goals (classic under/over for bookmakers)?"	40	449	2	giovannicarlozzi	serie-a-matches-dataset
1618	1618	Creditcard_Data		[]		0	23	11	syedhaideralizaidi	creditcard-data
1619	1619	airbox841		[]		0	3	0	woodiedudy	airbox841
1620	1620	BiLSTMtpu20*20		[]		0	2	0	mahdibb	bilstmtpu2020
1621	1621	Hyderabad Salaried Employees Dataset [Clustering]	Perform EDA and apply machine learning models for clustering	['india', 'computer science', 'beginner', 'data visualization', 'clustering', 'tabular data']	"Clustering Task
You will be provided a user dataset (filename hyderabad-salaried-employees.csv) on which clustering needs to be performed
Perform EDA (exploratory data analysis) on this dataset
Apply machine learning models and perform clustering
Plot Clusters Graph
No pre-determined number of clusters. The proper number of clusters need to be found.
Feel free to include/exclude the columns in the dataset based on their perceived importance"	53	323	10	shubamsumbria	hyderabad-salaried-employees-dataset-clustering
1622	1622	play-tennis		[]		0	0	0	devil1602	playtennis
1623	1623	Telco_Customer_Churn		[]		0	8	1	serapgr	telco-customer-churn
1624	1624	keyword-detection-model-60		[]		0	2	0	kcy0206	keyword-detection-model-60
1625	1625	myfeedbackdata		[]		0	23	0	longhuqin	myfeedbackdata
1626	1626	Cervical Cancer vs Demographic, Habits, MedHistory	Predicting Cancer from Demographic data, Habits, and historic medical records	['healthcare', 'public health', 'health', 'classification', 'cancer']	"<h1>Source:</h1>
Kelwin Fernandes (kafc <em>at</em> inesctec <em>dot</em> pt) - INESC TEC & FEUP, Porto, Portugal. Jaime S. Cardoso - INESC TEC & FEUP, Porto, Portugal. Jessica Fernandes - Universidad Central de Venezuela, Caracas, Venezuela.<p></p>
<h1>Data Set Information:</h1>
<p>The dataset was collected at 'Hospital Universitario de Caracas' in Caracas, Venezuela. The dataset comprises demographic information, habits, and historic medical records of 858 patients. Several patients decided not to answer some of the questions because of privacy concerns (missing values).</p>
<h1>Attribute Information:</h1>
<p>(int) Age(int) Number of sexual partners(int) First sexual intercourse (age)(int) Num of pregnancies(bool) Smokes(bool) Smokes (years)(bool) Smokes (packs/year)(bool) Hormonal Contraceptives(int) Hormonal Contraceptives (years)(bool) IUD(int) IUD (years)(bool) STDs(int) STDs (number)(bool) STDs:condylomatosis(bool) STDs:cervical condylomatosis(bool) STDs:vaginal condylomatosis(bool) STDs:vulvo-perineal condylomatosis(bool) STDs:syphilis(bool) STDs:pelvic inflammatory disease(bool) STDs:genital herpes(bool) STDs:molluscum contagiosum(bool) STDs:AIDS(bool) STDs:HIV(bool) STDs:Hepatitis B(bool) STDs:HPV(int) STDs: Number of diagnosis(int) STDs: Time since first diagnosis(int) STDs: Time since last diagnosis(bool) Dx:Cancer(bool) Dx:CIN(bool) Dx:HPV(bool) Dx(bool) Hinselmann: target variable(bool) Schiller: target variable(bool) Cytology: target variable(bool) Biopsy: target variable</p>
<h1>Relevant Papers:</h1>
<p>Kelwin Fernandes, Jaime S. Cardoso, and Jessica Fernandes. 'Transfer Learning with Partial Observability Applied to Cervical Cancer Screening.' Iberian Conference on Pattern Recognition and Image Analysis. Springer International Publishing, 2017.</p>
<h1>Citation Request:</h1>
<p>Kelwin Fernandes, Jaime S. Cardoso, and Jessica Fernandes. 'Transfer Learning with Partial Observability Applied to Cervical Cancer Screening.' Iberian Conference on Pattern Recognition and Image Analysis. Springer International Publishing, 2017.</p>
<p><strong><em>Source:</em></strong> <a href=""http://archive.ics.uci.edu/ml/datasets/Cervical+cancer+%28Risk+Factors%29"" target=""_blank"">http://archive.ics.uci.edu/ml/datasets/Cervical+cancer+(Risk+Factors)</a></p>
This dataset was created by UCI and contains around 900 samples along with St Ds:vulvo Perineal Condylomatosis, St Ds:pelvic Inflammatory Disease, technical information and other features such as:
- St Ds (number)
- Smokes
- and more.
How to use this dataset
&gt; - Analyze Citology in relation to Biopsy
- Study the influence of St Ds: Time Since First Diagnosis on Age
- More datasets
Acknowledgements
If you use this dataset in your research, please credit UCI 
Start A New Notebook!"	243	1645	19	yamqwe	cervical-cancer-risk-factorse
1627	1627	poisson		[]		1	51	0	saankhya1997	poisson
1628	1628	More COTS Annotations	Annotations added to earlier frames	[]		0	16	0	bartmaciszewski	more-cots-annotations
1629	1629	rohit ravi		[]		9	7	0	traderbilly	rohit-ravi
1630	1630	ump-datasets-lag3d		[]		0	3	0	juetaochen	umpdatasetslag3d
1631	1631	student perfomance histogram 1		[]		9	13	1	traderbilly	student-perfomance-histogram-1
1632	1632	Malaria Dataset	Cell Images for Detecting Malaria	['healthcare', 'diseases', 'health', 'health conditions']	"Context
Malaria is a life-threatening disease caused by parasites that are transmitted to people through the bites of infected female Anopheles mosquitoes.
It is preventable and curable.
In 2018, there were an estimated 228 million cases of malaria worldwide.
The estimated number of malaria deaths stood at 405 000 in 2018.
Children aged under 5 years are the most vulnerable group affected by malaria;
in 2018, they accounted for 67% (272 000) of all malaria deaths worldwide.
The WHO African Region carries a disproportionately high share of the global malaria burden.
In 2018, the region was home to 93% of malaria cases and 94% of malaria deaths.
Data
The dataset contains 2 folders
Test 
Train 
In Those two folder we have 2 folders
- Infected
- Uninfected
there are total 550 images 
Inspiration
Save humans by detecting and deploying Image Cells that contain Malaria or not!
preview of data"	5	41	1	meetnagadia	malaria-dataset
1633	1633	DB Pix2Pix Dataset		[]		3	7	0	shankarmahadevan	db-pix2pix
1634	1634	Top 48 automakers daily stock prices 2010-2022	Explore top automakers stock price performance	['business', 'finance', 'automobiles and vehicles', 'investing']	"Context
Historical daily stock prices of top 48 automakers from 2010-2022.
Acknowledgements
data source:
- https://finance.yahoo.com/quote/TSLA?p=TSLA&.tsrc=fin-srch
- https://companiesmarketcap.com/automakers/largest-automakers-by-market-cap/"	19	159	7	prasertk	top-48-automakers-daily-stock-prices-20102022
1635	1635	restimages		[]		1	3	0	gyanendradas	restimages
1636	1636	Binance Full History	1 minute candlesticks for all 1928 cryptocurrency pairs	['business', 'finance', 'investing', 'currencies and foreign exchange']	"Introduction
This is a collection of 1 minute candlesticks of the top 1000 cryptocurrency pairs on Binance.com. In total there are 1928, but Kaggle limits file count to 1000. Both retrieval and uploading the data is fully automated—see this GitHub repo.
Content
For every trading pair, the following fields from Binance's official API endpoint for historical candlestick data are saved into a Parquet file:
Column                        Dtype
---  ------                        -----       
 0   open_time                     datetime64[ns]
 1   open                          float32     
 2   high                          float32     
 3   low                           float32     
 4   close                         float32     
 5   volume                        float32     
 6   quote_asset_volume            float32     
 7   number_of_trades              uint16      
 8   taker_buy_base_asset_volume   float32     
 9   taker_buy_quote_asset_volume  float32     
dtypes: datetime64ns, float32(8), uint16(1)
The dataframe is indexed by open_time and sorted from oldest to newest. The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest running pairs.
Here are two simple plots based on a single file; one of the opening price with an added indicator (MA50) and one of the volume and number of trades:
Inspiration
One obvious use-case for this data could be technical analysis by adding indicators such as moving averages, MACD, RSI, etc. Other approaches could include backtesting trading algorithms or computing arbitrage potential with other exchanges.
License
This data is being collected automatically from crypto exchange Binance."	9093	49021	272	jorijnsmit	binance-full-history
1637	1637	Gold Rate New Delhi	Monthly rate of gold from January 2017 to January 2022 in New Delhi	[]		0	4	0	harshm27	gold-rate-new-delhi
1638	1638	UbiBot Online Store		['retail and shopping']		0	2	0	ubibotstore	ubibot-online-store
1639	1639	hello1111		[]		0	3	0	iftiben10	hello1111
1640	1640	Plant Disease data		[]		9	22	0	arjunmuralidharan123	plant-disease-data
1641	1641	Glaucoma_cupcropped		[]		1	11	0	klmsathishkumar	glaucoma-cupcropped
1642	1642	Tamil Miso Data		['arts and entertainment']		1	3	0	shantanupatankar	tamil-miso-data
1643	1643	scan-coco-vocab		[]		0	2	0	larry2000	scancocovocab
1644	1644	QIQC 50-50 random training samples		[]		0	0	0	p1nklava	qiqc-5050-random-training-samples
1645	1645	cloth pattern		[]		15	43	0	rahilmehtaucoe2784	cloth-pattern
1646	1646	remdatawhale		[]		1	7	0	gyanendradas	remdatawhale
1647	1647	BART Ridership	Bay Area Rapid Transit hourly ridership broken up by year (starting 2011)	['computer science', 'transportation', 'rail transport', 'tabular data', 'travel']	"Context
Bay Area Rapid Transit or BART is a public rail system that connects much of California's San Francisco Bay Area. The transport system ""connects the San Francisco Peninsula with Berkeley, Oakland, Fremont, Walnut Creek, Dublin/Pleasanton and other cities in the East Bay"".
Content
This dataset is the most detailed information of trip information for BART and was provided by BART directly. Specifically, this data was pulled from the provided source http://64.111.127.166/origin-destination/. The data are automatically updated on the site and BART says they ""are usually available by the 5th of the next month"". 
Acknowledgements
This obviously wouldn't be available without BART collecting and providing the data. It's great that the data is publicly available to this essential transportation to those living in the Bay Area!
Inspiration
This data was originally pulled in July 2020 during the COVID-19 pandemic. As counties in the Bay Area begin relaxing quarantine/lockdown restrictions yet an increase of COVID-19 cases continues, it could be important to see how public transportation has changed. It's possible to see the travel habits of different areas in the Bay.
A quick note
This is the maintained iteration from the first Kaggle dataset (which is no longer mainted): https://www.kaggle.com/mrgeislinger/bart-ridership"	174	1814	9	mrgeislinger	bartridership
1648	1648	simple_data		[]		1	11	0	heaeae	simple-data
1649	1649	6,000 Largest Companies Ranked by Market Cap	List of largest companies in the world (as of 8-Feb-2022)	['business', 'finance', 'exploratory data analysis']	"Context
Largest Companies by Market Cap.
Acknowledgements
Data source: https://companiesmarketcap.com/
Cover image credit: https://www.pexels.com/photo/high-rise-buildings-443383/"	94	547	11	prasertk	6000-largest-companies-ranked-by-market-cap
1650	1650	XGB-Toxic-100		[]		0	6	0	cdeotte	xgbtoxic100
1651	1651	Quaterly Petrol Prices in Delhi (in Rupees)	January 2012 to October 2021	[]		0	3	1	harshm27	quaterly-petrol-prices-in-delhi-in-rupees
1652	1652	Population of each country in the world 2022		['social science']	"Context
This dataset was scraped from Countries in the world by population (2022) and has not been cleaned yet.
The information is current (2022); however, the column's name was stated as 2020. This list included both countries and dependent territories. Data based on the latest United Nations Population Division estimates. 
I couldn't find the metadata of this dataset, so I tried searching the description for some columns. I apologize if there are any errors and would be happy to hear your suggestions. The list contains the following columns:
Country (or dependency)
Population (2020)
Yearly Change = percent changes of population compare to the previous year.
Net Change = the net changes of population compare to the previous year
Density (P/Km²) = population density (people per sq. km of land area)
Land Area (Km²)
Migrants (net) = the net number of migrants
Fert. Rate = fertility rate = the average number of children that would be born to a woman over her lifetime
Med. Age = median age
Urban Pop % = urban population = the total population living in areas termed as urban by that country
World Share = world population share by country
Acknowledgements
We wouldn't be here without the help of others. Special thanks to Worldometer for the information provided."	1	9	0	chutikarnwongsung	population-of-each-country-in-the-world-2022
1653	1653	Toxic-Comp-Scripts		[]		0	3	0	cdeotte	toxiccompscripts
1654	1654	one001		[]		0	1	0	mohamedk0emad	one001
1655	1655	Bay Area GeoJson		[]		0	0	0	cargcastro	bay-area-geojson
1656	1656	pytorch image models (timm)		['computer science', 'programming']	"How to use:
import sys
sys.path.append('../input/pytorchimagemodels')
import timm
or
!pip install ../input/pytorchimagemodels/
**New version : Feb 2, 2022
prepping to merge the norm_norm_norm branch back to master (ver 0.6.x)
Add ConvNeXT models /w weights 
...
Deit:
import torch
import torch.nn as nn
from functools import partial
from timm.models.vision_transformer import VisionTransformer, _cfg
from timm.models.registry import register_model
@register_model
def deit_base_patch16_224(pretrained=False, kwargs):
    model = VisionTransformer(
        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6), kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.hub.load_state_dict_from_url(
            url=""https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth"",
            map_location=""cpu"", check_hash=True
        )
        model.load_state_dict(checkpoint[""model""])
    return model"	531	1495	4	truonghoang	pytorchimagemodels
1657	1657	heartrate		['health']		0	0	0	kdhone	heartrate
1658	1658	fitbit		['exercise']		0	3	0	kdhone	fitbit
1659	1659	detoxify anotation wikipedia		['health']		0	0	0	tomooinubushi	detoxify-anotation-wikipedia
1660	1660	detoxify machine anotated wikipedia		['health']		0	0	1	tomooinubushi	detoxify-machine-anotated-wikipedia
1661	1661	detoxify anotated wikipedia		['health']		0	1	0	tomooinubushi	detoxify-anotated-wikipedia
1662	1662	machine anotated wikipedia		[]		0	1	0	tomooinubushi	machine-anotated-wikipedia
1663	1663	1Million_Rows_of_Motel_Data	File for 450 class item	['software']		1	12	0	forseeswriting	1million-rows-of-motel-data
1664	1664	San Francisco Neighborhoods GeoJson with Zipcodes		[]		0	1	0	cargcastro	san-francisco-neighborhoods-geojson-with-zipcodes
1665	1665	tf_rec_3		[]		0	0	0	keithsz	tf-rec-3
1666	1666	tf_rec_2		[]		0	1	0	keithsz	tf-rec-2
1667	1667	tf_rec_1		[]		0	3	0	keithsz	tf-rec-1
1668	1668	reef-yolox-m-aug2-v1		[]		0	2	0	sangayb	reef-yolox-m-aug2-v1
1669	1669	stock sentiment prices prediction		['business']		1	13	0	abidilmunfisabil	stock-sentiment-prices-prediction
1670	1670	SAS_dataset		[]		22	67	0	phngnamdng	sas-dataset
1671	1671	Saque y remate Voleibol 	Caracterización de gesto de saque y remate de jugadores de voleibol	['sports', 'bayesian statistics', 'data analytics', 'clustering', 'volleyball']		4	37	0	talledoshugo	saque-y-remate-voleibol
1672	1672	NBA Team Stats	Various team stats sourced from nba.com for 2000-2001 through 2020-2021 seasons	['basketball', 'sports']	"Content
Various NBA team statistics (Wins, Losses, Points, etc...) scraped from stats.nba.com for the 2000-2001 to 2020-2021 regular and playoff seasons. 
Acknowledgements
NBA.com"	705	4064	20	mharvnek	nba-team-stats-00-to-18
1673	1673	jigsaw2021_private		[]		1	26	0	tomooinubushi	jigsaw2021-private
1674	1674	d2chars2		[]		0	13	0	daehoyang	d2chars2
1675	1675	H&M2022_low_memory_fast_loading		[]		0	40	3	sytuannguyen	hm2022-low-memory-fast-loading
1676	1676	y1axis		[]		1	6	0	yanlokko234	y1axis
1677	1677	y2axis		[]		1	6	0	yanlokko234	y2axis
1678	1678	x_axis		[]		1	6	0	yanlokko234	x-axis
1679	1679	tiroides	Thyroid classification using EU-TIRADS.	[]		3	111	1	yharyarias	tiroides
1680	1680	Song Shuffle Simulation		['music']		0	7	0	derekxuzhao	song-shuffle-simulation
1681	1681	Blackcoffer Insights URLs	Collection of webpages to perform web scraping	['websites', 'people and society', 'business', 'finance', 'science and technology', 'news']	"Collection of webpages to perform web scraping.
Content
BlackCoffer Insights correspond to publications of articles about several topics related to IT. In the dataset Sheet1 of the unique dataset you can see that I have collected the URL links of 78 publications since August 18th 2020 to January 16th 2022, these are formatted as an excel file.
Acknowledgements
BlackCoffer Insights
https://insights.blackcoffer.com/"	2	70	6	georgesaavedra	blackcoffer-insights-urls
1682	1682	daft.ie house price data	house price data extracted from daft.ie for house market analysis in Ireland	['housing', 'real estate', 'beginner']	"Context
With 22 explanatory variables describing an important aspect of residential homes in Ireland, this competition challenges you to predict the final price of each home.
Practice Skills
Feature selection
Data imputation
Regression  techniques
Plot a map data using geocode
Content
id: Property listing id used in the website.
title: property address.
featuredLevel: ['featured' 'premium' 'standard']
publishDate: published date for the listing.
price: house price
numBedrooms: No. of bedrooms
numBathrooms: No. of bathrooms
propertyType: ['End of Terrace' 'Semi-D' 'Terrace' 'Detached' 'Apartment' 'Bungalow'
'Townhouse' 'Duplex' 'Site' 'Studio' 'House']
propertySize: Size of the property
category: ['Buy' 'New Homes'] (Buy means the houses listed under Buy menu in the websites)
AMVprice: Advised minimum value (some properties have AMV price. The price column was mixed with regular price and AMV price, created a new column for AMV price to flag if it is AMV or not.) 
sellerId: Seller's Id 
sellername: Seller's name
sellerbranch: Seller's branch if it is an agency. 
sellerType: ['BRANDEDAGENT' 'UNBRANDEDAGENT' 'PRIVATEUSER']
m_totalImages: Total images posted on the website. 
m_hasVideo: Whether the property posted a video or not.
m_hasVirtualTour: Whether the property has a virtual tour or not. 
m_hasBrochure: Whether the property has a brochure or not.
longitude: Coordinate
latitude: Coordinate
ber_rating: ['C2' 'C1' 'A3' 'G' 'D2' 'B3' 'D1' 'C3' 'E1' 'SI666' 'F' 'E2' 'B2' 'XXX', 'A2,''B1' 'A1' 'A1A2']
 (XXX means the rating was missing)
Reference link about Building Energy Rating
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Daftlistings library enables programmatic interaction with daft.ie. daft.ie has nationwide coverage and contains about 80% of the total available properties in Ireland.
The data was extracted by using the daftlistings library below.
https://github.com/AnthonyBloomer/daftlistings
Banner Photo from pixabay.com."	12	100	2	eavannan	daftie-house-price-data
1683	1683	US energy consumption - EIA	Detailed data from California	['energy']		1	26	3	konradb	us-energy-consumption-eia
1684	1684	Smart Grid Stability	Augmented version of the original hosted in the UCI Machine Learning Repository	['arts and entertainment', 'earth and nature', 'renewable energy', 'artificial intelligence', 'computer science', 'deep learning', 'neural networks']	"This dataset corresponds to an augmented version of the ""Electrical Grid Stability Simulated Dataset"", created by Vadim Arzamasov (Karlsruher Institut für Technologie, Karlsruhe, Germany) and donated to the University of California (UCI) Machine Learning Repository (link here), where it is currently hosted.
Two primary references support this development and demand special mention:
""Taming instabilities in power grid networks by decentralized control"" (B. Schäfer, et al, The European Physical Journal, Special Topics, 2016, 225.3: 569-582), in which Dr. Schäfer (Network Dynamics, Max Planck Institute for Dynamics and Self-Organization - MPIDS, Göttingen, Germany) and his co-authors describe in detail the DSGC (Decentral Smart Grid Control) differential equation-based model to assess stability of smart grids;
""Towards Concise Models of Grid Stability"" (V. Arzamasov, K. Böhm and P. Jochem, 2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Aalborg, 2018, pp. 1-6), in which Dr. Arzamasov and his co-authors explore how data-mining techniques can address DSGC model simplifications.
The author is particularly thankful for Dr. Arzamasov's personal guidance and comments on the overall dataset structure.
Renewable Energy Sources and Smart Grids
The ascent of renewable energy sources provides the global community with a much demanded alternative to traditional, finite and climate-unfriendly fossil fuels. However, their adoption poses a set of new paradigms, out of which two interrelated aspects deserve particular attention:
Prior to the rise of renewable energy sources, the traditional operating ecosystem involved few production entities (sources) supplying energy to consumers over unidirectional flows. With the advent of renewable options, end users (households and enterprises) now not only consume energy but have the ability to produce and supply it - hence a new term to designate them, 'prosumers'. As a result, energy flow within distribution grids - 'smart grids' - has become bidirectional;
Despite the increased flexibility brought in by the introduction of renewable sources and the aforementioned emergence of 'prosumers', the management of supply and demand in a more complex generation / distribution / consumption environment and the related economic implications (particularly the decision to buy energy at a given price or not) have become even more challenging.
Relevant contributions on how to tackle the requirements of such new scenario have been offered by academy and industry over the past years. Special attention has been devoted to the study of smart grid stability."	1	22	1	smartgrids	smart-grid-stability
1685	1685	mmdetection		[]		4	1312	0	xujingzhao	mmdetection
1686	1686	City-level Data (GIS Friendly Format),		['logistic regression', 'linear regression', 'matplotlib', 'numpy', 'pandas']		0	3	3	qusaybtoush	citylevel-data-gis-friendly-format
1687	1687	Prevalence and Mean of Dental Caries 	 (Cavities) among Maryland School Children, 2005-2006	['dentistry', 'health', 'logistic regression', 'linear regression', 'matplotlib', 'pandas']		3	37	6	qusaybtoush	prevalence-and-mean-of-dental-caries
1688	1688	Hugging Face models		['clothing and accessories']		0	3	0	vkonstantakos	hugging-face-models
1689	1689	atmosphere status 20y to 21y in Seoul, South Korea	time series data ex) temp, windDirection, windVelocity ... etc	['atmospheric science', 'time series analysis', 'datetime']		0	15	1	zagabi	south-korea-seoul-atmosphere-status-2020-to-2021
1690	1690	Marine Mammal Food Habits , 1995-2018	The Marine Mammal Laboratory (MML) Food Habits Reference Collection	['earth science', 'logistic regression', 'linear regression', 'matplotlib', 'pandas', 'seaborn']		0	19	5	qusaybtoush	marine-mammal-food-habits-19952018
1691	1691	.jhfgilhgj		[]		1	3	0	dazmony	jhfgilhgj
1692	1692	swifter		[]		0	13	0	dzisandy	swifter
1693	1693	Asteroid Dataset	NASA JPL Asteroid Dataset	['earth and nature', 'astronomy']	"Story Behind This Dataset
I am an Astronomy and Astrophysics Researcher. As a Mathematics background I am a data science, machine learning, and deep learning enthusiast. Nowadays Machine Learning is solving so many problems in Astronomy and Astrophysics fields. Asteroid is nice topic for Machine Learning projects like classification and regression problems.
Data Source And Method of Collection
I have collected this Dataset from which is officially maintained by Jet Propulsion Laboratory of California Institute of Technology which is an organization under NASA. In this Dataset all kinds of Data related to Asteroid is included. This Dataset is publicly available in their website. The Basic Definitions of the Columns have been given below.
Website Link-  JPL Small-Body Database Search Engine
Basic Column Definition
SPK-ID: Object primary SPK-ID
Object ID: Object internal database ID
Object fullname: Object full name/designation
pdes: Object primary designation
name: Object IAU name
NEO: Near-Earth Object (NEO) flag
PHA: Potentially Hazardous Asteroid (PHA) flag
H: Absolute magnitude parameter
Diameter: object diameter (from equivalent sphere) km Unit
Albedo: Geometric albedo
Diameter_sigma: 1-sigma uncertainty in object diameter km Unit
Orbit_id: Orbit solution ID
Epoch: Epoch of osculation in modified Julian day form
Equinox: Equinox of reference frame
e: Eccentricity
a: Semi-major axis au Unit
q: perihelion distance au Unit
i: inclination; angle with respect to x-y ecliptic plane
tp: Time of perihelion passage TDB Unit
moid_ld: Earth Minimum Orbit Intersection Distance au Unit
Acknowledgements
I wouldn't be here without the help of NASA. I heartily thank NASA and JPL for maintaining such an wonderful database which is user friendly. JPL-authored documents are sponsored by NASA under Contract NAS7-030010. All documents available from this server may be protected under the U.S. and Foreign Copyright Laws. Permission to reproduce may be required.
Inspiration
I have been Inspired by Astronomers and Astro Community who are working hard to reveal the Unsolved Questions."	1207	18777	54	sakhawat18	asteroid-dataset
1694	1694	Sweden Covid-19 Dataset	Latest statistics on new cases and deaths in Sweden by region	['europe', 'health', 'geospatial analysis', 'covid19']	"Context
Covid-19 is a global pandemic which requires a global effort to enable innovative solutions. We hope that this dataset will encourage such thinking and bring us closer to mapping an uncertain future for Sweden and the world. 
Content
This data represents both confirmed cases and confirmed deaths from Covid-19 in Sweden by region per day. It is updated regularly and get transferred here as soon as an update is made. The data is collected from the National Health Agency of Sweden (Folkshälsomyndigheten) as well as regional health agencies for more up-to-date information.
Acknowledgements
All the credit for this dataset goes to Elin Lutz. All the data is updated from her Github repository https://github.com/elinlutz/gatsby-map. 
Inspiration
The author also provides a live map of Sweden viewable at https://www.coronakartan.se/."	769	18529	42	jannesggg	sweden-covid19-dataset
1695	1695	Cyclistic_Ride_Share		[]		6	31	1	aliciadelgado	cyclistic-ride-share
1696	1696	JS MPNet		[]		0	13	0	aishikai	js-mpnet
1697	1697	Energy Data		['energy']		1	8	0	anasanand	energy-data
1698	1698	Ezhi Victims	подробная информация о всех жертвах ежи владимирского с 2012 по 2022 год	['tabular data']	"Описание
Уникальный датасет, собравший в себя все известные преступления легендарного и неуловимого Ежи Владимирского. Эксперты криминалистики верят, что лишь подробный анализ собранных нами данных поможет погрузиться в психологию этого страшного преступника и, наконец, восстановить правосудие.
Содержание датасета
| Название колонки | Пояснение |
| --- | --- |
| EzhisVictimID | Уникальный ID жертвы |
| Survived | Выжила ли жертва? 1 - да, 0 - нет |
| IQ | Результаты прохождения жертвой теста IQ |
| Sex | Пол жертвы |
| Age | Возраст жертвы |
| Siblings | Количество братьев и сестер жертвы на месте преступления |
| Parents | Количество родителей жертвы на месте преступления |
| PassID | Код пропуска жертвы в школу |
| Weight | Вес жертвы |
| MetroStation | Станция метро, на которой произошло преступление |
| SceneOfTheCrime | Место преступление |
| Name | Имя жертвы |
Благодарности
Сердечно благодарим фонд помощи попавшим в анекдотное рабство! Сделать свое пожертвование можно по ссылке"	5	226	21	gu1ldenstern	ezhi-victims
1699	1699	KDD-Datasets		[]		2	5	0	rasoulrahimii	kdddatasets
1700	1700	BigEarthNetS1	BigEarthNet with Sentinel-1 Image Patches	['earth science', 'intermediate', 'advanced', 'image data', 'news']	"About BigEarthNet
The BigEarthNet archive was constructed by the Remote Sensing Image Analysis (RSiM) Group and the Database Systems and Information Management (DIMA) Group at the Technische Universität Berlin (TU Berlin). This work is supported by the European Research Council under the ERC Starting Grant BigEarth and by the Berlin Institute for the Foundations of Learning and Data (BIFOLD). Before BIFOLD, the Berlin Big Data Center (BBDC) supported the work.
BigEarthNet is a benchmark archive, consisting of 590,326 pairs of Sentinel-1 and Sentinel-2 image patches. The first version (v1.0-beta) of BigEarthNet includes only Sentinel 2 images. Recently, it has been enriched by Sentinel-1 images to create a multi-modal BigEarthNet benchmark archive (called also as BigEarthNet-MM).
BigEarthNet with Sentinel-1 Image Patches
To construct BigEarthNet with Sentinel-1 image patches (called as BigEarthNet-S1), 321 Sentinel-1 scenes acquired between June 2017 and May 2018 that jointly cover the area of all original 125 Sentinel-2 tiles with close temporal proximity were selected and processed. BigEarthNet-S1 consists of 590,326 preprocessed Sentinel-1 image patches - one for each Sentinel-2 patch. A more detailed explanation on the processing is given in its dataset description document."	1	19	2	javidtheimmortal	bigearthnetsentinel1
1701	1701	Financial Sentiment Analysis	Financial sentences with sentiment labels	['business', 'finance', 'nlp', 'tabular data']	"Data
The following data is intended for advancing financial sentiment analysis research. It's two datasets (FiQA, Financial PhraseBank) combined into one easy-to-use CSV file. It provides financial sentences with sentiment labels.
Citations
Malo, Pekka, et al. ""Good debt or bad debt: Detecting semantic orientations in economic texts."" Journal of the Association for Information Science and Technology 65.4 (2014): 782-796."	66	681	14	sbhatti	financial-sentiment-analysis
1702	1702	lgbm_tuned		[]		0	25	0	oleksandrrechynskyi	lgbm-tuned
1703	1703	COVID-19 data - Estonia	Detailed COVID-19 tests data in Estonia	['healthcare', 'diseases', 'health', 'demographics', 'covid19']	"Content
Detailed Estonia COVID-19 test data made in Estonia
opendata_covid19_test_results.csv contains individiual anonymized COVID-19 test results.
Each row corresponds to one test and contains age group, county and test time data.
Acknowledgements
Source of this data is Republic of Estonia Health Board open data (in Estonian).
Inspiration
Dataset can be used to predict infection rates for different age groups and also for different counties in Estonia. This might also be used to create early warings for certain age groups or counties. Fun example, it can be used to predict how long it would take to retrieve your test result (AnalysisInsertTime - ResultTime)."	14	396	0	taanieluleksin	covid19-test-data-estonia
1704	1704	COVID-19 vaccination data - Estonia	Detailed COVID-19 vaccination data in Estonia	['healthcare', 'diseases', 'public health', 'health', 'demographics', 'public safety', 'covid19']	"Content
Detailed Estonia COVID-19 test data made in Estonia
opendata_covid19_vaccination_location_county_agegroup_gender.csv contains daily COVID-19 vaccinations.
Each row corresponds to total vaccinations for age group, county, gender and vaccination type (how many doses administered).
Acknowledgements
Source of this data is Republic of Estonia Health Board open data (in Estonian).
Inspiration
Dataset can be used to predict vaccinations for different age groups and also for different counties in Estonia. This might also be used to help to decide on where to put vaccination efforts."	82	1143	7	taanieluleksin	covid19-vaccination-data-estonia
1705	1705	feedback_prize		[]	https://github.com/abhishekkrthakur/long-text-token-classification	10	34	0	awaptk	feedback-prize
1706	1706	bogdan_dataset		[]		0	7	0	bohdanvey	bogdan-dataset
1707	1707	JS Albert		[]		5	7	0	aishikai	js-albert
1708	1708	covid-19 Daily data Updated 	Daily Data about Covid-19 updated weekly	['global', 'diseases', 'intermediate', 'data analytics', 'classification', 'news']	I wanted to do some analysis on Covid-19 to improve my skills. As a Kaggler, I just searched on Kaggle for the relevant datasets. But to my surprise, there was not even a single dataset available on Kaggle which was updated and had all the data I need. Later I found a good dataset, So, I shared it on Kaggle. I will keep updating this dataset weekly.	249	1740	15	affanazhar	covid19-daily-data-updated
1709	1709	JS deBERTa		[]		10	12	0	aishikai	js-deberta
1710	1710	ua-inferna-1		['retail and shopping']		0	5	0	jvppvj	uainferna1
1711	1711	magnetic-tile-dataset-2018		['earth and nature']		0	0	0	sonramsirirat	magnetictiledataset2018
1712	1712	Sample-Dataset-Text		['business']		4	4	0	karloison	sampledatasettext
1713	1713	Salary Data		[]		0	9	2	imnoob	salary-data
1714	1714	pbvs-limited-tfrecs		[]		0	3	0	adityakane	pbvslimitedtfrecs
1715	1715	titanic1me		[]		0	5	0	nagarajsg	titanic1me
1716	1716	Jigsaw Bert LM		['puzzles']		1	26	0	yaremamishchenko	jigsaw-bert-lm
1717	1717	Olympics Dataset		[]		0	7	0	olexiyp	olympics
1718	1718	Restaurant_Reviews		[]		1	2	0	mdwasimakhtar03	restaurant-reviews
1719	1719	cars on wheels	Cars Image Classification Dataset	['popular culture', 'automobiles and vehicles', 'computer vision', 'image data']	"Cars Image Classification Dataset
This dataset contains images of cars or parts of cars. 
These images are collected mostly from unsplash.com with a great thanks!
This dataset is meant for academic and research purposes ony!
cars_annotated.csv is a supporting file with meta-data and annotated features as follows:
1. file_name represents the jpg image file name.
2. wheels_visible represents whether wheels are visible in the image or not.
3. humna_visible represents whether any person or persons are visible in the image, or not.
Annotations are made manually.
cars_labels.jpg describes the class distribution of annotated features."	141	2074	18	rajkumarl	cars-on-wheels
1720	1720	Covid-19 in Singapore : Latest Detailed Data	Latest and regularly updated covid 19 in Singapore dataset	['health', 'law', 'exploratory data analysis', 'data visualization', 'tabular data', 'covid19']	"Latest Covid 19 Data of Singapore
Data with detailed columns and metrics such as breakdown of cases - local,  imported, linked, unlinked, ICU, Oxygen Requirements etc. In Singapore, from 3 January 2020 to14 October 2021, there have been 135,395 confirmed cases of COVID-19 with 192 deaths, reported to WHO. As of 8 October 2021, a total of 9,401,855 vaccine doses have been administered.
Inspiration
Some insights could be
Changes in number of affected cases over time
Change in cases over time at country level
Latest number of affected cases
Acknowledgements
Originally Collected and Maintained by : 
- Ministry of Health (MOH) Singapore
- Hui Xiang Chu (https://twitter.com/hxchuaruns)"	455	3865	45	shivamb	covid19-in-singapore-latest-detailed-data
1721	1721	Google Ads: R/UR data		[]		8	59	1	maunish	google-ads-rur-data
1722	1722	Jigsaw - models PL 512 roberta base		[]		0	4	0	prateekagnihotri	jigsaw-models-pl-512-roberta-base
1723	1723	AvioeF!		[]		0	2	0	arvoeidaw	avioe
1724	1724	WLASL (World Level American Sign Language) Videos	21k processed videos of Word-Level American Sign Language	['linguistics', 'intermediate', 'nlp', 'video data', 'pytorch']		8	33	0	gazquez	wlasl-processed
1725	1725	ODI match dataset		['cricket']		0	4	0	rashedsabbir	odi-match-dataset
1726	1726	Latest Covid-19 Confirmed Cases Kerala	Time series data, from January 31, 2020 to February 06, 2022	['india', 'healthcare', 'public health', 'time series analysis', 'covid19']	"Content
This dataset contains the confirmed Covid-19 cases in Kerala, India from January 31, 2020 to February 06, 2022. 
It contains dates and confirmed cases, which can be used for time series analysis.
Attribute Information
Date - Date from January 31, 2020
Confirmed - Covid-19 Daily Confirmed Cases
Source
Link : https://dashboard.kerala.gov.in/covid/daily.php
Other updated Covid datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	497	3793	67	anandhuh	covid19-confirmed-cases-kerala
1727	1727	mimic-weights		[]		0	14	1	captaintrakas	mimicweights
1728	1728	Latest Covid19 Data - Kerala, India	Covid-19 Data from January 31, 2020 to February 07, 2022	['india', 'exploratory data analysis', 'data visualization', 'data analytics', 'covid19']	"Content
The data contains the confirmed, recovered and deceased cases of Covid-19 cases in Kerala, India from January 31, 2020 to February 07, 2022 . 
This dataset can be used for EDA and time series analysis
Attribute Information
Date - Date from January 31, 2020 
Confirmed  - Daily Covid-19 confirmed cases
Recovered - Daily Covid-19 recovered cases
Deceased - Daily Covid-19 deceased cases
Source
Link : https://dashboard.kerala.gov.in/index.php
Other updated Covid datasets
Link : https://www.kaggle.com/anandhuh/datasets
Please appreciate the effort with an upvote 👍 
Thank You"	657	3296	79	anandhuh	covid19-latest-data-kerala
1729	1729	Jigsaw-Private-data		[]		4	138	0	nkitgupta	jigsawprivatedata
1730	1730	category_file		[]		1	7	0	hrishikeshkini	category-file
1731	1731	_test2		[]		0	0	0	yanngrandgirard	-test2
1732	1732	_test1		[]		0	1	0	yanngrandgirard	-test1
1733	1733	yolovs6_2560_r676_map769		[]		0	2	0	dragonzhang	yolovs6-2560-r676-map769
1734	1734	Options Dataset		[]		1	5	0	gmshroff	options-dataset
1735	1735	MovieData		[]		0	6	0	yonasew422	moviedata
1736	1736	Name Census top 100 name list	Name list with gender and popularity of 100 most popular names per country.	['education', 'social science', 'demographics', 'computer science', 'programming', 'text data']	"Context
In 2018 we were programming our name parsing service and we needed to know if a name was male or female. After Googling for hours for we discovered that there was not a workable, complete list with names and gender for all countries. To get all the names we reached out to governments, statistical agencies and gathered open data from different resources. We used millions of social media profiles that where publicly available to cross-reference and count each name per country. This way we were sure that the names in our name list are actually used and we could create our popularity metric. We now offer the complete name lists and the name parsing service as separate services. This Name Census top 100 file is a free list per country with the top 100 names.
Content
The Name Census top 100 is a name list that is created by using baby name lists obtained from governments and cross-referencing it with millions of names from publicly available social media profiles."	71	1000	5	namecensus	name-census-top-100-name-list
1737	1737	Nobel Prize Winners	Nobel Prize Winners from 1901 to current day	['research', 'earth and nature', 'education', 'computer science']	"About this dataset
&gt; <p>This dataset shows all Nobel Prize Laureates from the award's inception in 1901 through current day (October 2016). Laureates are announced at the beginning of October and a ceremony is held on December 10 each year. The Nobel Prize was established in 1895 by Swedish inventor Alfred Nobel and first awarded in 1901. It recognizes people in the fields of Physics, Chemistry, Physiology or Medicine, Literature, Peace, and Economics.</p>
<p><strong><a href=""http://www.nobelprize.org/nobel_prizes/about/prize_announcements/index.html"" target=""_blank"">2016 Prize Announcement Dates.</a></strong></p>
<p><code>In-the-News</code>:</p>
<ul>
<li><em>Live Science</em>: <a href=""http://www.livescience.com/56351-nobel-prize-winners.html"" target=""_blank"">Nobel Prize 2016: Here Are the Winners (and What They Achieved)</a></li>
<li><em>FiveThirtyEight</em>: <a href=""http://fivethirtyeight.com/features/the-typical-chemistry-nobel-winner-changed-after-1980/"" target=""_blank"">The Typical Chemistry Nobel Winner Changed After 1980</a>, <a href=""http://fivethirtyeight.com/features/harvard-wins-the-most-nobel-prizes-in-physiology-and-medicine-for-dudes/"" target=""_blank"">Harvard Wins The Most Nobel Prizes In Physiology And Medicine (For Dudes)</a>, <a href=""http://fivethirtyeight.com/features/and-the-typical-nobel-prize-winner-in-physics-is/"" target=""_blank"">And The Typical Nobel Prize Winner In Physics Is …</a></li>
<li><em>Washington Post</em>: <a href=""https://www.washingtonpost.com/news/speaking-of-science/wp/2016/10/04/2016-nobel-prize-in-physics-awarded-to-three-men-for-revealing-the-secrets-of-exotic-matter/?hpid=hp_hp-more-top-stories_nobel-6a%3Ahomepage%2Fstory"" target=""_blank"">2016 Nobel Prize in physics awarded for revealing ‘the secrets of exotic matter’</a></li>
<li><em>Chicago Tribune</em>: <a href=""http://www.chicagotribune.com/news/nationworld/ct-nobel-prize-chemistry-20161005-story.html"" target=""_blank"">Northwestern University professor among 3 Nobel Prize winners in chemistry</a></li>
<li><em>Firstpost</em>: <a href=""http://www.firstpost.com/bollywood/throwback-thursday-films-about-nobel-prize-winners-starring-daniel-craig-daniel-radcliffe-3036814.html"" target=""_blank"">Throwback Thursday: Films about Nobel Prize winners starring Daniel Craig, Daniel Radcliffe</a><br>
<strong>and my favorite in the new article :)</strong></li>
<li><em>The Hollywood Reporter</em>: <a href=""http://www.hollywoodreporter.com/live-feed/simpsons-predicted-yesterdays-nobel-prize-937303"" target=""_blank"">'The Simpsons' Predicted Yesterday's Nobel Prize in 2010</a></li>
</ul>
<p><img src=""http://i.imgur.com/qw5o54v.png?1"" alt=""alt text""><br>
<em>Chart from the Nobel website</em></p>
<p>As the <a href=""http://fivethirtyeight.com/features/the-typical-chemistry-nobel-winner-changed-after-1980/"" target=""_blank"">FiveThirtyEight</a> articles have pointed out, the US really dominates the Nobel Prize awards.</p>
<p><img src=""http://i.imgur.com/dgMUKV3.png"" alt=""alt text""></p>
<p>I was curious to see what parts of the US those scientists and thought leaders hailed from.... not surprised to find NYC at the top, the ""center of the universe"" and the home of amazing work like the <a href=""https://www.nobelprize.org/educational/peace/nuclear_weapons/readmore.html"" target=""_blank"">Manhattan Project</a>. Omitting cities with only 1 winner, I created this little map graphic using <a href=""https://blogs.bing.com/maps/2014/07/15/4-easy-ways-to-visualize-excel-data-on-bing-maps/"" target=""_blank"">Bing</a>. Overall, the eastern half of the US dominates, with Chicago and NYC being the birthplace to over 20% of Nobel recipients.</p>
<p><img src=""http://i.imgur.com/Uq80EQW.png"" alt=""alt text""></p>
<p>Data gathered using the Nobel Prize <a href=""https://nobelprize.readme.io/docs/prize"" target=""_blank"">API</a> and their <a href=""http://data.nobelprize.org/"" target=""_blank"">SPARQL Endpoint</a>.</p>
<p>Source:<a href=""http://data.nobelprize.org/"" target=""_blank"">Nobel Prize Database</a></p>
<p><a href=""/kidpixo"" target=""_blank"">@kidpixo</a> I just added a python notebook that explore the dataset usng seborn, the original code is here <a href=""https://gist.github.com/kidpixo/a9366ac703e3fd6d47a4e631fc33fc28"" target=""_blank"">nobel dataset exploration</a>: fork and use it!</p>
This dataset was created by Selene Arrazolo and contains around 900 samples along with Born Country, Died Country Code, technical information and other features such as:
- Country
- Died Country
- and more.
How to use this dataset
&gt; - Analyze Died in relation to Died City
- Study the influence of Id on Surname
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Selene Arrazolo 
Start A New Notebook!"	283	1507	14	yamqwe	nobel-prize-winnerse
1738	1738	Food demand forecasting data		['business']		2	11	1	darshanbhavsar	food-demand-forecasting-data
1739	1739	HBR Raw Data		['earth and nature']		1	2	0	nishaaluselvakumar	hbr-raw-data
1740	1740	2021 Divvy Tripdata		[]		0	2	0	danielsotoubilla	divvy-tripdata
1741	1741	Pokemon Moves List CSV Gens 1 - 8		[]		2	9	0	someone1234567	pokemon-moves-list-csv-gens-1-8
1742	1742	How People Get Hurt	Products involved in personal injury and trips to the  Emergency Room	['healthcare', 'tabular data', 'text data', 'public safety', 'hospitals and treatment centers']	"For more than 45 years, the CPSC has operated a statistically valid injury surveillance and follow-back system known as the National Electronic Injury Surveillance System (NEISS). The primary purpose of NEISS is to collect data on consumer product-related injuries occurring in the United States. CPSC uses these data to produce nationwide estimates of product-related injuries.
NEISS is based on a nationally representative probability sample of hospitals in the U.S. and its territories. Each participating NEISS hospital reports patient information for every emergency department visit associated with a consumer product or a poisoning to a child younger than five years of age. The total number of product-related hospital emergency department visits nationwide can be estimated from the sample of cases reported in the NEISS.
Cost of Injury Data is provided by the US Center for Disease Control (CDC). The costs include spending on health care, lost work productivity, as well as estimates of cost for lost quality of life and lives lost."	251	2750	24	jpmiller	how-people-get-hurt
1743	1743	classfication2		[]		0	0	0	wanghuijun	classfication2
1744	1744	fracture		['health conditions']		1	22	3	pengmarvin	fracture
1745	1745	pbvs_all_tfrecs		[]		0	23	0	adityakane	pbvs-all-tfrecs
1746	1746	Subreddit Categorization	7.5k unique subreddits categorized into groups, scraped from findareddit.com	['internet', 'online communities', 'social networks']	"Context
Subreddit information including membership, description, and user-submitted categorization scraped from FindAReddit on February 4th, 2022. 
There are three category columns, and they are hierarchical, i.e. Category3 is a subgroup of Category2 is a subgroup of Category1. Many subreddits, especially those which are popular, have several categorizations. Also, bear in mind this website is based on user input, and I don't know how much scrutiny input is subjected to, so some classifications may seem odd.
Acknowledgements
Thanks so much to the creators of FindAReddit and everyone who has submitted their favorite subreddits to this website!"	1	7	0	morganoneka	subreddit-categorization
1747	1747	Covid-19 in India		[]		0	15	2	avinesh2003	covid19-in-india
1748	1748	msk_dost_super_lite		[]		0	1	0	polinanazarova	msk-dost-super-lite
1749	1749	Monthly Brazilian oil production		['oil and gas']		0	25	1	erivanoliveirajr	oilproduction
1750	1750	Behavioral Risk Factor Surveillance System-2013	Preventive Health Practices & Risk Behaviors on Adults	['demographics', 'exploratory data analysis', 'data visualization', 'diabetes']	"About BRFSS dataset
The Behavioral Risk Factor Surveillance System (BRFSS) is a collaborative project between all of the states in the United States (US) and participating US territories and the Centers for Disease Control and Prevention (CDC). The BRFSS is administered and supported by CDC’s Population Health Surveillance Branch, under the Division of Population Health at the National Center for Chronic Disease Prevention and Health Promotion.
BRFSS is an ongoing surveillance system designed to measure behavioral risk factors for the non-institutionalized adult population (18 years of age and older) residing in the US. The BRFSS was initiated in 1984, with 15 states collecting surveillance data on risk behaviors through monthly telephone interviews. Over time, the number of states participating in the survey increased; by 2001, 50 states, the District of Columbia, Puerto Rico, Guam, and the US Virgin Islands were participating in the BRFSS. Today, all 50 states, the District of Columbia, Puerto Rico, and Guam collect data annually and American Samoa, Federated States of Micronesia, and Palau collect survey data over a limited point- in-time (usually one to three months). In this document, the term “state” is used to refer to all areas participating in BRFSS, including the District of Columbia, Guam, and the Commonwealth of Puerto Rico.
Table of Contents
Main Survey - Section 0 - Record Identification
state: State Fips Code
fmonth: File Month
idate: Interview Date
imonth: Interview Month
iday: Interview Day
iyear: Interview Year
dispcode: Final Disposition
seqno: Annual Sequence Number
_psu: Primary Sampling Unit
ctelenum: Correct Telephone Number?
pvtresd1: Private Residence?
colghous: Do You Live In College Housing?
stateres: Resident Of State
cellfon3: Cellular Telephone
ladult: Are You 18 Years Of Age Or Older?
numadult: Number Of Adults In Household
nummen: Number Of Adult Men In Household
numwomen: Number Of Adult Women In Household
Main Survey - Section 1 - Health Status
genhlth: General Health
Main Survey - Section 2 - Healthy Days - Health-Related Quality of Life
physhlth: Number Of Days Physical Health Not Good
menthlth: Number Of Days Mental Health Not Good
poorhlth: Poor Physical Or Mental Health
Main Survey - Section 3 - Health Care Access
hlthpln1: Have Any Health Care Coverage
persdoc2: Multiple Health Care Professionals
medcost: Could Not See Dr. Because Of Cost
checkup1: Length Of Time Since Last Routine Checkup
Main Survey - Section 4 - Inadequate Sleep
sleptim1: How Much Time Do You Sleep
Main Survey - Section 5 - Hypertension Awareness
bphigh4: Ever Told Blood Pressure High
bpmeds: Currently Taking Blood Pressure Medication
Main Survey - Section 6 - Cholesterol Awareness
bloodcho: Ever Had Blood Cholesterol Checked
cholchk: How Long Since Cholesterol Checked
toldhi2: Ever Told Blood Cholesterol High
Main Survey - Section 7 - Chronic Health Conditions
cvdinfr4: Ever Diagnosed With Heart Attack
cvdcrhd4: Ever Diagnosed With Angina Or Coronary Heart Disease
cvdstrk3: Ever Diagnosed With A Stroke
asthma3: Ever Told Had Asthma
asthnow: Still Have Asthma
chcscncr: (Ever Told) You Had Skin Cancer?
chcocncr: (Ever Told) You Had Any Other Types Of Cancer?
chccopd1: (Ever Told) You Have (Copd) Chronic Obstructive Pulmonary Disease, Emphysema Or
havarth3: Told Have Arthritis
addepev2: Ever Told You Had A Depressive Disorder
chckidny: (Ever Told) You Have Kidney Disease?
diabete3: (Ever Told) You Have Diabetes
Main Survey - Section 8 - Demographics
veteran3: Are You A Veteran
marital: Marital Status
children: Number Of Children In Household
educa: Education Level
employ1: Employment Status
income2: Income Level
weight2: Reported Weight In Pounds
height3: Reported Height In Feet And Inches
numhhol2: Household Telephones
numphon2: Residential Phones
cpdemo1: Do You Have A Cell Phone For Personal Use?
cpdemo4: What Percent Of All Calls Are Received On Your Cell Phone?
internet: Internet Use In The Past 30 Days?
renthom1: Own Or Rent Home
sex: Respondents Sex
pregnant: Pregnancy Status
qlactlm2: Activity Limitation Due To Health Problems
useequip: Health Problems Requiring Special Equipment
blind: Blind Or Difficulty Seeing
decide: Difficulty Concentrating Or Remembering
diffwalk: Difficulty Walking Or Climbing Stairs
diffdres: Difficulty Dressing Or Bathing
diffalon: Difficulty Doing Errands Alone
Main Survey - Section 9 - Tobacco Use
smoke100: Smoked At Least 100 Cigarettes
smokday2: Frequency Of Days Now Smoking
stopsmk2: Stopped Smoking In Past 12 Months
lastsmk2: Interval Since Last Smoked
usenow3: Use Of Smokeless Tobacco Products
Main Survey - Section 10 - Alcohol Consumption
alcday5: Days In Past 30 Had Alcoholic Beverage
avedrnk2: Avg Alcoholic Drinks Per Day In Past 30
drnk3ge5: Binge Drinking
maxdrnks: Most Drinks On Single Occasion Past 30 Days
Main Survey - Section 11 - Fruits and Vegetables
fruitju1: How Many Times Did You Drink 100 Percent Pure Fruit Juices?
fruit1: How Many Times Did You Eat Fruit?
fvbeans: How Many Times Did You Eat Beans Or Lentils?
fvgreen: How Many Times Did You Eat Dark Green Vegetables?
fvorang: How Many Times Did You Eat Orange-Colored Vegetables?
vegetab1: How Many Times Did You Eat Other Vegetables?
Main Survey - Section 12 - Exercise (Physical Activity)
exerany2: Exercise In Past 30 Days
exract11: Type Of Physical Activity
exeroft1: How Many Times Walking, Running, Jogging, Or Swimming
exerhmm1: Minutes Or Hours Walking, Running, Jogging, Or Swimming
exract21: Other Type Of Physical Activity Giving Most Exercise During Past Month
exeroft2: How Many Times Walking, Running, Jogging, Or Swimming
exerhmm2: Minutes Or Hours Walking, Running, Jogging, Or Swimming
strength: How Many Times Did You Do Physical Activities Or Exercises To Strengthen Your Mu
Main Survey - Section 13 - Arthritis Burden
lmtjoin3: Limited Because Of Joint Symptoms
arthdis2: Does Arthritis Affect Whether You Work
arthsocl: Social Activities Limited Because Of Joint Symptoms
joinpain: How Bad Was Joint Pain
Main Survey - Section 14 - Seatbelt Use
seatbelt: How Often Use Seatbelts In Car?
Main Survey - Section 15 - Immunization
flushot6: Adult Flu Shot/Spray Past 12 Mos
flshtmy2: When Received Most Recent Seasonal Flu Shot/Spray
tetanus: Received Tetanus Shot Since 2005?
pneuvac3: Pneumonia Shot Ever
Main Survey - Section 16 - HIV/AIDS
hivtst6: Ever Tested HIV
hivtstd3: Month And Year Of Last HIV Test
whrtst10: Location Of Last HIV Test
Optional Module 1 - Pre-Diabetes
pdiabtst: Had A Test For High Blood Sugar In Past Three Years
prediab1: Ever Been Told You Have Pre-Diabetes Or Borderline Diabetes
Optional Module 2 - Diabetes
diabage2: Age When Told Diabetic
insulin: Now Taking Insulin
bldsugar: How Often Check Blood For Glucose
feetchk2: How Often Check Feet For Sores Or Irritations
doctdiab: Times Seen Health Professional For Diabetes
chkhemo3: Times Checked For Glycosylated Hemoglobin
feetchk: Times Feet Check For Sores/Irritations
eyeexam: Last Eye Exam Where Pupils Were Dilated
diabeye: Ever Told Diabetes Has Affected Eyes
diabedu: Ever Taken Class In Managing Diabetes
Optional Module 3 - Healthy Days (Symptoms)
painact2: How Many Days Hard To Do Usual Activities In Past 30 Days
qlmentl2: How Many Days Depressed In Past 30 Days
qlstres2: How Many Days Felt Anxious In Past 30 Days
qlhlth2: How Many Days Full Of Energy In Past 30 Days
Optional Module 4 - Health Care Access
medicare: Do You Have Medicare?
hlthcvrg: Health Insurance Coverage
delaymed: Delayed Getting Medical Care
dlyother: Delayed Getting Medical Care Other Response
nocov121: Without Health Care Coverage Past 12 Months
lstcovrg: Time Since Last Had Health Care Coverage
drvisits: Doctor Visits Past 12 Months
medscost: Could Not Get Medicine Due To Cost
carercvd: Satisfied With Care Received
medbills: Currently Have Medical Bills
Optional Module 5 - Sugar Drinks
ssbsugar: How Often Do You Drink Regular Soda Or Pop?
ssbfrut2: How Often Did You Drink Sugar-Sweetened Drinks?
Optional Module 6 - Sodium or Salt-Related Behavior
wtchsalt: Watching Sodium Or Salt Intake
longwtch: How Long Watching Salt/Sodium Intake
dradvise: Doctor Advised Reduced Sodium/Salt Intake
Optional Module 7 - Adult Asthma History
asthmage: Age At Asthma Diagnosis
asattack: Asthma During Past 12 Months
aservist: Emergency Asthma Care During Past 12 Months
asdrvist: Urgent Asthma Treatment During Past 12 Months
asrchkup: Routine Asthma Care During Past 12 Months
asactlim: Activities Limited Due To Asthma During Past 12 Months
asymptom: Asthma Symptoms During Past 30 Days
asnoslep: Sleep Difficulty Due To Asthma During Past 30 Days
asthmed3: Days Used Prescribed Preventative Asthma Med In Past 30 Days
asinhalr: Times Used Asthma Inhaler During An Attack In Past 30 Days
Optional Module 8 - Cardiovascular Health
harehab1: Outpatient Rehab After Heart Attack Hosp Stay
strehab1: Outpatient Rehab After Hosp Stay For Stroke
cvdasprn: Take Aspirin Daily Or Every Other Day
aspunsaf: Health Makes Taking Aspirin Unsafe
rlivpain: Take Aspirin To Relieve Pain
rduchart: Take Aspirin To Reduce Chance Of Heart Attack
rducstrk: Take Aspirin To Reduce Chance Of Stroke
Optional Module 9 - Arthritis Management
arttoday: What Can You Do Because Of Arthritis Or Joint Symptoms
arthwgt: Dr. Suggest Lose Weight For Arthritis Or Joint Symptoms
arthexer: Dr. Suggest Use Of Physical Activity Or Exercise For Arthritis Or Joint Symptoms
arthedu: Ever Taken Class In Managing Arthritis Or Joint Symptoms
Optional Module 10 - Influenza
imfvplac: Where Did You Get Your Last Flu Shot/Vaccine?
Optional Module 11 - Adult Human Papilloma Virus (HPV)
hpvadvc2: Ever Had The HPV Vaccination?
hpvadsht: Number Of HPV Shots Received
Optional Module 12 - Breast and Cervical Cancer Screening
hadmam: Have You Ever Had A Mammogram
howlong: How Long Since Last Mammogram
profexam: Ever Had Breast Physical Exam By Doctor
lengexam: How Long Since Last Breast Physical Exam
hadpap2: Ever Had A Pap Test
lastpap2: How Long Since Last Pap Test
hadhyst2: Had Hysterectomy
Optional Module 13 - Colorectal Cancer Screening
bldstool: Ever Had Blood Stool Test Using Home Kit
lstblds3: Time Since Last Blood Stool Test
hadsigm3: Ever Had Sigmoidoscopy/Colonoscopy
hadsgco1: Was Last Test A Sigmoidoscopy Or Colonoscopy
lastsig3: Time Since Last Sigmoidoscopy/Colonoscopy
Optional Module 14 - Prostate Cancer Screening
pcpsaad2: Has A Health Professional Ever Talked With You About The Advantages Of The PSA Test
pcpsadi1: Has A Health Professional Ever Talked With You About The Disadvantages Of The PSA Test
pcpsare1: Has A Doctor Ever Recommended That You Have A PSA Test?
psatest1: Ever Had PSA Test
psatime: Time Since Last PSA Test
pcpsars1: What Was The Main Reason You Had This PSA Test?
Optional Module 15 - Prostate Cancer Screening Decision Making Module
pcpsade1: Why Was PSA Test Done?
pcdmdecn: Who Made The Decision With You To Have PSA Test?
Optional Module 16 - Reactions to Race
rrclass2: How Do Other People Usually Classify You In This Country?
rrcognt2: How Often Do You Think About Your Race?
rratwrk2: How Do You Feel You Were Treated At Work Compared To People Of Other Races In Pa
rrhcare3: When Seeking Health Care Past 12 Months, Was Experience Worse, Same, Better Than
rrphysm2: Times Past 30 Days Felt Physical Symptoms Because Of Treatment Due To Your Race
rremtsm2: Times Past 30 Days Felt Emotionally Upset Because Of Treatment Due To Your Race
Optional Module 17 - Mental Illness and Stigma
misnervs: How Often Feel Nervous Past 30 Days
mishopls: How Often Feel Hopeless Past 30 Days
misrstls: How Often Feel Restless Past 30 Days
misdeprd: How Often Feel Depressed Past 30 Days
miseffrt: How Often Feel Everything Was An Effort Past 30 Days
miswtles: How Often Feel Worthless Past 30 Days
misnowrk: Emotional Problem Kept You From Doing Work Past 30 Days
mistmnt: Receiving Medicine Or Treatment From Health Pro For Emotional Problem
mistrhlp: Mental Health Treatment Can Help People Lead Normal Life
misphlpf: People Are Generally Caring Toward People With Mental Illness
Optional Module 19 - Social Context
scntmony: Times Past 12 Months Worried/Stressed About Having Enough Money To Pay Your Rent
scntmeal: Times Past 12 Months Worried/Stressed About Having Enough Money To Buy Nutritiou
scntpaid: How Are You Generally Paid For The Work You Do
scntwrk1: How Many Hours Per Week Do You Work
scntlpad: How Were You Generally Paid For The Work You Did
scntlwk1: How Many Hours Per Week Did You Work
scntvot1: Did You Vote In The Last Presidential Election?
Optional Module 20 - Random Child Selection
rcsgendr: Gender Of Child
rcsrltn2: Relationship To Child
Optional Module 21 - Childhood Asthma Prevalence
casthdx2: Hlth Pro Ever Said Child Has Asthma
casthno2: Child Still Have Asthma?
Optional Module 22 - Emotional Support and Life Satisfaction
emtsuprt: How Often Get Emotional Support Needed
lsatisfy: Satisfaction With Life
Cell Phone Introduction
ctelnum1: Correct Phone Number?
cellfon2: Is This A Cellular Telephone?
cadult: Are You 18 Years Of Age Or Older?
pvtresd2: Do You Live In A Private Residence?
cclghous: Do You Live In College Housing?
cstate: Are You A Resident Of [State]?
landline: Do You Also Have A Landline Telephone?
pctcell: What Percent Of Your Calls Are Received On Your Cell Phone?
Questionnaire Version
qstver: Questionnaire Version Identifier
Questionnaire Language
qstlang: Language Identifier
Weighting Variables
mscode: Metropolitan Status Code
_ststr: Sample Design Stratification Variable
_strwt: Stratum Weight
_rawrake: Raw Weighting Factor Used In Raking
_wt2rake: Design Weight Used In Raking
_imprace: Imputed Race/Ethnicity Value
_impnph: Imputed Number Of Phones
_impeduc: Imputed Education Level
_impmrtl: Imputed Marital Status
_imphome: Imputed Rent Or Own Home Status
Child Demographic Variables
_chispnc: Child Hispanic, Latino/A, Or Spanish Origin Calculated Variable
_crace1: Child Non-Hispanic Race Including Multiracial
_impcage: Imputed Child Age
_impcrac: Imputed Child Race/Ethnicity
_impcsex: Imputed Child Gender
_cllcpwt: Final Child Weight: Land-Line And Cell-Phone Data
Weighting Variables (2)
_dualuse: Dual Phone Use Categories
_dualcor: Dual Phone Use Correction Factor
_llcpwt2: Truncated Design Weight Used In Adult Combined Land Line And Cell Phone Raking
_llcpwt: Final Weight: Land-Line And Cell-Phone Data
Calculated Variables
rfhlth: Adults With Good Or Better Health
_hcvu651: Respondents Aged 18-64 With Health Care Coverage
_rfhype5: High Blood Pressure Calculated Variable
_cholchk: Cholesterol Checked Calculated Variable
_rfchol: High Cholesterol Calculated Variable
_ltasth1: Lifetime Asthma Calculated Variable
_casthm1: Current Asthma Calculated Variable
_asthms1: Computed Asthma Status
_drdxar1: Respondents Diagnosed With Arthritis
_prace1: Computed Preferred Race
_mrace1: Calculated Non-Hispanic Race Including Multiracial
_hispanc: Hispanic, Latino/A, Or Spanish Origin Calculated Variable
_race: Computed Race-Ethnicity Grouping
_raceg21: Computed Non-Hispanic Whites/All Others Race Categories Race/Ethnic Group Codes
_racegr3: Computed Five Level Race/Ethnicity Category.
_race_g1: Computed Race Groups Used For Internet Prevalence Tables
_ageg5yr: Reported Age In Five-Year Age Categories Calculated Variable
_age65yr: Reported Age In Two Age Groups Calculated Variable
_age_g: Imputed Age In Six Groups
htin4: Computed Height In Inches
htm4: Computed Height In Meters
wtkg3: Computed Weight In Kilograms
_bmi5: Computed Body Mass Index
_bmi5cat: Computed Body Mass Index Categories
_rfbmi5: Overweight Or Obese Calculated Variable
_chldcnt: Computed Number Of Children In Household
_educag: Computed Level Of Education Completed Categories
_incomg: Computed Income Categories
_smoker3: Computed Smoking Status
_rfsmok3: Current Smoking Calculated Variable
drnkany5: Drink Any Alcoholic Beverages In Past 30 Days
drocdy3: Computed Drink-Occasions-Per-Day
rfbing5: Binge Drinking Calculated Variable
_drnkdy4: Computed Number Of Drinks Of Alcohol Beverages Per Day
_drnkmo4: Computed Total Number Drinks A Month
_rfdrhv4: Heavy Alcohol Consumption Calculated Variable
_rfdrmn4: Adult Men Heavy Alcohol Consumption Calculated Variable
_rfdrwm4: Adult Women Heavy Alcohol Consumption Calculated Variable
ftjuda1: Computed Fruit Juice Intake In Times Per Day
frutda1: Computed Fruit Intake In Times Per Day
beanday_: Computed Bean Intake In Times Per Day
grenday_: Computed Dark Green Vegetable Intake In Times Per Day
orngday_: Computed Orange-Colored Vegetable Intake In Times Per Day
vegeda1_: Computed Vegetable Intake In Times Per Day
misfrtn: The Number Of Missing Fruit Responses
misvegn: The Number Of Missing Vegetable Responses
_frtresp: Missing Any Fruit Responses
_vegresp: Missing Any Vegetable Responses
_frutsum: Total Fruits Consumed Per Day
_vegesum: Total Vegetables Consumed Per Day
_frtlt1: Consume Fruit 1 Or More Times Per Day
_veglt1: Consume Vegetables 1 Or More Times Per Day
_frt16: Reported Consuming Fruit &gt;16/Day
_veg23: Reported Consuming Vegetables &gt;23/Day
_fruitex: Fruit Exclusion From Analyses
_vegetex: Vegetable Exclusion From Analyses
_totinda: Leisure Time Physical Activity Calculated Variable
metvl11: Activity Met Value For First Activity
metvl21: Activity Met Value For Second Activity
maxvo2_: Estimated Age-Gender Specific Maximum Oxygen Consumption
fc60_: Estimated Functional Capacity
actin11_: Estimated Activity Intensity For First Activity
actin21_: Estimated Activity Intensity For Second Activity
padur1_: Minutes Of First Activity
padur2_: Minutes Of Second Activity
pafreq1_: Physical Activity Frequency Per Week For First Activity
pafreq2_: Physical Activity Frequency Per Week For Second Activity
minac11: Minutes Of Physical Activity Per Week For First Activity
minac21: Minutes Of Physical Activity Per Week For Second Activity
strfreq: Strength Activity Frequency Per Week
pamiss1: Missing Physical Activity Data
pamin11_: Minutes Of Physical Activity Per Week For First Activity
pamin21_: Minutes Of Physical Activity Per Week For Second Activity
pa1min_: Minutes Of Total Physical Activity Per Week
pavig11_: Minutes Of Vigorous Physical Activity Per Week For First Activity
pavig21_: Minutes Of Vigorousphysical Activity Per Week For Second Activity
pa1vigm_: Minutes Of Total Vigorous Physical Activity Per Week
_pacat1: Physical Activity Categories
_paindx1: Physical Activity Index
_pa150r2: 150 Minute Physical Activity Calculated Variable
_pa300r2: 300 Minute Physical Activity Calculated Variable
_pa30021: 300 Minute Physical Activity 2-Level Calculated Variable
_pastrng: Muscle Strengthening Recommendation
_parec1: Aerobic And Strengthening Guideline
_pastae1: Aerobic And Strengthening (2-Level)
_lmtact1: Limited Usual Activities
_lmtwrk1: Limited Work Activities
_lmtscl1: Limited Social Activities
_rfseat2: Always Or Nearly Always Wear Seat Belts
_rfseat3: Always Wear Seat Belts
_flshot6: Flu Shot Calculated Variable
_pneumo2: Pneumonia Vaccination Calculated Variable
_aidtst3: Ever Been Tested For HIV Calculated Variable
_age80: Imputed Age Value Collapsed Above 80
For further information, please visit the codebook:
CODEBOOK
References
BRFSS web site: http://www.cdc.gov/brfss/
BRFSS Questionnaire (Mandatory and Optional Modules): http://www.cdc.gov/brfss/questionnaires/pdf-ques/2013%20BRFSS_English.pdf
BRFSS Codebook: http://www.cdc.gov/brfss/annual_data/2013/pdf/CODEBOOK13_LLCP.pdf
BRFSS Guide to Calculated Variables: http://www.cdc.gov/brfss/annual_data/2013/pdf/2013_Calculated_Variables_Version15.pdf
BRFSS Guide to Optional Modules Used, by State: http://apps.nccd.cdc.gov/BRFSSModules/ModByState.asp?Yr=2013"	28	325	9	nguyenngocphung	behavioral-risk-factor-surveillance-system2013
1751	1751	cyclistsx	 2021 Cyclistic's Bike Trip Data - Summary, Pivot Tables & Charts, and Combined.. 	[]	"The spreadsheet contains the different summary worksheets for the January 2021 to December 2021 Cyclistic's Bike Trip Data including  Pivot Tables & Charts, and the Combined dataset using Power Query used for Data Analyst Capstone project.
Grateful for Google and the tutors that guided me through the program."	0	9	0	akaymsa	cyclistsx
1752	1752	BIMjson_to_CSV.ipynb		[]		1	3	0	ferlive	bimjson-to-csvipynb
1753	1753	GBR Starfish TFRecords Mini 1X 1 1		[]		0	6	0	mmelahi	gbr-starfish-tfrecords-mini-1x-1-1
1754	1754	GBR Starfish TFRecords Mini 1X 1 0		['sports']		0	1	2	mmelahi	gbr-starfish-tfrecords-mini-1x-1-0
1755	1755	TPS_Feb_2022_Kfold	TPS_Feb_2022_Kfold ss	[]		0	15	0	cristianminas	tps-feb-2022-kfold
1756	1756	GBR Starfish TFRecords Mini 1X 2 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-1x-2-1
1757	1757	my_data_sets		[]		2	63	0	ajaybu	my-data-sets
1758	1758	GBR Starfish TFRecords Mini 1X 2 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-1x-2-0
1759	1759	Superbowl Commercial Challenge Analysis		[]		1	3	0	paragzode	superbowl-commercial-challenge-analysis
1760	1760	GBR Starfish TFRecords Mini 1X 0 1		['sports']		0	5	0	mmelahi	gbr-starfish-tfrecords-mini-1x-0-1
1761	1761	GBR Starfish TFRecords Mini 1X 0 0		['sports']		0	2	1	mmelahi	gbr-starfish-tfrecords-mini-1x-0-0
1762	1762	Insurance Portfolio Analysis		['insurance']		6	31	0	paragzode	insurance-portfolio-analysis
1763	1763	Harry Potter Analysis		['movies and tv shows']		0	7	0	paragzode	harry-potter-analysis
1764	1764	NYP Arrests		[]		0	1	0	paragzode	nyp-arrests
1765	1765	Pneumonia due to Covid-19 : Analysis & Prediction3	Covid Pneumonia dataset- Cleaned for Prediction	['healthcare', 'public health', 'health', 'medicine', 'covid19']		2	26	0	homelysmile	datasetclean2
1766	1766	Challenge 16		['arts and entertainment']		0	1	0	paragzode	challenge-16
1767	1767	fer2013		[]		26	3	0	xerno99	fer2013
1768	1768	reef-yolox-m-aug-v9		[]		0	1	0	sangayb	reef-yolox-m-aug-v9
1769	1769	Top 100 Rotten Tomatoes movies by genres	Explore top movies by genres	['movies and tv shows', 'beginner', 'exploratory data analysis', 'tabular data']	"Context
Explore top movies by genres rated by Rotten Tomatoes. 
Data source
https://www.rottentomatoes.com/top/bestofrt/top_100_action__adventure_movies/"	176	871	16	prasertk	top-100-rotten-tomatoes-movies-by-genres
1770	1770	Rice Image Dataset	Rice image dataset (Arborio, Basmati, Ipsala, Jasmine, Karacadag)	['agriculture', 'computer science', 'deep learning', 'image data', 'multiclass classification']		12	102	14	mkoklu42	rice-image-dataset
1771	1771	dataconvralec	data image of radiografia torax	[]		0	127	0	j4m3sc	dataconvralec
1772	1772	Text To ARPABET	This is used to make convert your text file into ARPABET	['computer science', 'programming', 'data cleaning', 'python']	"This python script will turn your transcription text file into Arpabet.
This can be used for your datasets when creating AI TTS (Text-To-Speech) models E.G. Tacotron 2, HiFi-GAN, Etc.
ALL CREDITS ALL GO TO https://github.com/johnpaulbin for creating this script."	4	99	4	coldfir4	arpabet
1773	1773	pppppp		[]		0	14	0	tracerboy	pppppp
1774	1774	mattingonefolder		[]		0	4	0	medetovyerma	mattingonefolder
1775	1775	University Data	A Dataset for The Algerian Universities, Institutes, and National Schools.	['universities and colleges']	"Univ-Data
Description
A Dataset for The Algerian Universities, Institutes, and National Schools.
The Dataset contains the name in Arabic, French, and English, with the Website and the region (west, center, east).
The dataset exists in both CSV and JSON format."	9	160	2	ta7lilteam	univ-data
1776	1776	fb2021-2		[]		0	1	0	uncletian2021	fb20212
1777	1777	reef_bests		[]		0	0	0	tylorkwon	reef-bests
1778	1778	fb2021		[]		0	4	0	uncletian2021	fb2021
1779	1779	xlnet_base		[]		0	2	0	quincyqiang	xlnet-base
1780	1780	TPU-model5		['internet']		0	0	0	vigneshirtt	tpumodel5
1781	1781	TPU-model4		['internet']		0	0	0	vigneshirtt	tpumodel4
1782	1782	TPU-model3		['internet']		0	4	0	vigneshirtt	tpumodel3
1783	1783	License Plate Characters - Detection OCR	2026 character bounding boxes for 209 license plate images	['automobiles and vehicles', 'computer vision', 'image data']	"Content
The images come from this kaggle dataset. I cropped 209 license plates using the original bounding boxes and using LabelImg i labelled all the single characters, creating a total of 2026 character bounding boxes. Every image comes with a .xml annotation file with the same name, the format used is PascalVOC.
Inside the count.txt you can find the total occurences of each character."	7	189	6	francescopettini	license-plate-characters-detection-ocr
1784	1784	TPU-model1		['internet']		0	5	0	vigneshirtt	tpumodel1
1785	1785	mask detection(normalized)	(train,test,validation)	[]		1	6	0	muhriddinmalik	mask-detectionnormalized
1786	1786	halfmatting		[]		0	3	0	medetovyerma	halfmatting
1787	1787	NBA Game Logs 1990-2022	Statistics of each game from 1990 to 2022	['games', 'basketball', 'sports']	"INFO
Data only contains regular seasons. It is the row version of the data, GAME_ID is not unique, every game has two row: away team and home team."	3	82	0	karakasatarik	nba-game-logs-19902022
1788	1788	cyclegan-proekt		[]		0	4	0	bojansofronievski	cycleganproekt
1789	1789	Dictionary_JSON		[]		0	0	0	vivekbaaganps	dictionary-json
1790	1790	droo_mobile_edge		[]		0	4	0	dwchen	droo-mobile-edge
1791	1791	sanshiroh_natsume_souseki		[]		1	3	0	wawarero	sanshiroh-natsume-souseki
1792	1792	Maxpulse__Calories		[]		0	1	0	vivekbaaganps	maxpulse-calories
1793	1793	jrpretrained2		[]		1	9	0	wowfattie	jrpretrained2
1794	1794	Calories		['nutrition']		0	4	0	vivekbaaganps	calories
1795	1795	bilstm300		[]		0	2	0	hadisahmadian	bilstm300
1796	1796	paica2		[]		2	8	0	sebastianlinjiang	paica2
1797	1797	NovelUpdates Dataset	Contains translated English novels' info from 8 different original languages.	['literature']		1	17	0	debanganthakuria	novelupdates-dataset
1798	1798	summarizer		[]		0	4	0	creecsh	summarizer
1799	1799	tianchi		[]		0	5	0	axiongya	tianchi
1800	1800	NLP_sample		[]		0	11	0	heaeae	sample
1801	1801	Israel RealEstate		['finance']		3	10	0	amirbialer	israel-realestate
1802	1802	Sales-Data-1990-2021	A Synthetic Dataset for practicing data managment and visualization. 	['business', 'exploratory data analysis', 'data cleaning', 'data visualization', 'tabular data']		19	110	1	ananta	sales-data-1990-2021
1803	1803	gan_dataset2		[]		0	2	0	antonioxv	gan-dataset2
1804	1804	IMDB Movie Dataset Latest	Movie Dataset of IMDB	['movies and tv shows', 'beginner', 'intermediate']	"Context
This dataset is being extracted from the website imdb.com using we scrapping in python( Beautiful Soup Library).It contains 1000 rows and 10 columns.
Content
This dataset contains rating of movie based on viewers review and arranged in descending order of rating using web scrap .
Inspiration
Viewer seeing this data will have an opportunity to perform various analytics technique on data and analyze the data."	42	269	3	ayushjain001	imdb-movie-dataset-latest
1805	1805	MSME-India-Unitnos		[]		0	3	0	gandhalijoshi	msme-registration-india
1806	1806	testData		['business']		0	0	0	radwanow	testdata
1807	1807	nlptest		[]		0	6	0	noobming	nlptest
1808	1808	India's first 1000 ODIs	Innings totals and results for India's 1st 1000 One Day Internationals	['cricket', 'india', 'sports']		15	80	6	jbomitchell	indias-first-1000-odis
1809	1809	Person		[]		0	2	0	heritipsi	person
1810	1810	ppgabp		[]		0	2	0	ckc666	ppgabp
1811	1811	Languages spoken across various nations	Dataset needed to parse countries which are bi or even trilingual in culture	['languages', 'linguistics', 'beginner', 'exploratory data analysis', 'text data']	"Context
I was fascinated by this type of data as this gives a slight peek on cultural diversity of a nation and what kind of literary work to be expected from that nation
Content
This dataset is a collection of all the languages that are spoken by the different nations around the world. Nowadays, Most nations are bi or even trilingual in nature this can be due to different cultures and different groups of people are living in the same nation in harmony. This type of data can be very useful for linguistic research, market research, advertising purposes, and the list goes on.
Acknowledgements
This dataset was published on the site Infoplease which is a general information website.
Inspiration
I think this dataset can be useful to understand which type of literature publication can be done for maximum penetration of the market base"	60	243	6	shubhamptrivedi	languages-spoken-across-various-nations
1812	1812	validation5fold		[]		0	3	0	vzty233	validation5fold
1813	1813	Handwritten signature verification	Image dataset for handwritten signature verification	['business', 'image data']	"Context
v8
Dataset is containing over 5000 handwritten signatures with correspondent images and crops for real and forged signatures
Each image contains about 10 handwritten signatures from the same user id. The image is then cropped with help of a segmentation neural network. Every crop contains one handwritten signature
You can define id from the image filename
Unique IDs total original:  275
Signatures total original:  2913
Unique IDs total forged:  247
Signatures total forged:  2713
Total signatures 5626
Content
real folder contains real images + corresponding crops of an individual signature
fake folder contains forged images + corresponding crops of an individual signature
tsv file contains info about real - forged image correspondense 
Acknowledgements
Created with COMPTECH2022 support  by Toloka.ai
Inspiration
Thanks to COMPTECH2022 ""WhoSigned?"" team and Toloka.ai"	18	283	9	tienen	handwritten-signature-verification
1814	1814	Small Dataset for Auto Retouching task		['art', 'computer vision', 'image data']		3	20	3	anikishaev	photo-retouch-sm-ds
1815	1815	nuScene Mini		[]		0	10	1	didiruh	nuscene-mini
1816	1816	 Journeys less than 20km completed in 60 mins	Journeys on public transport less than 20km to be completed within 60 minutes	[]	"Context
Data consolidated from LTA yearly report and Land Transport Master Plans"	1	28	1	xxre34	journeys-on-public-transport-less-than-20km
1817	1817	animals		[]		0	1	0	chiragkaushik14	animals
1818	1818	Global Superstore Dataset		['business']		4	9	0	kajalkuchhadiya	global-superstore-dataset
1819	1819	Sample Super Store	Sample Super Store Dataset	['categorical data', 'marketing', 'beginner', 'data visualization', 'data analytics']	"Context
super Store in USA , the data contain about 10000 rows 
Data Dictionary
| Attributes | Definition | example |
| --- | --- | ---|
| Ship Mode |  | Second Class |
| Segment | Segment Category | Consumer |
| Country |  | United State|
| City |  | Los Angeles |
| State |  | California |
| Postal Code |  | 90032 |
| Region |  | West |
| Category | Categories of product | Technology |
| Sub-Category |  | Phones |
| Sales | number of sales | 114.9 |
| Quantity |  | 3 |
| Discount |  | 0.45 |
| Profit |  | 14.1694 |
Acknowledgements
All thanks to  The Sparks Foundation For making this data set
Inspiration
Get the data and try to take insights. Good luck ❤️"	12	49	0	ibrahimelsayed182	sample-super-store
1820	1820	Human Resource Dataset		[]		4	20	0	kajalkuchhadiya	human-resource-dataset
1821	1821	Crypto Currency Dataset		['currencies and foreign exchange']		10	43	0	kajalkuchhadiya	crypto-currency-dataset
1822	1822	VG games sales dataset		['sports']		7	20	0	kajalkuchhadiya	vg-games-sales-dataset
1823	1823	ASL-alphabets	Alphabet hand sign images and hand landmark tensors	['health']		0	7	0	pradiptomondal	aslalphabets
1824	1824	Airport Transfers The Most Efficient Way to Travel		[]	"Airport transfers make it easy to travel to and fro any destination. You can either book your transfer online or use the service to transport you. While they may cost more than other options, they are much more affordable than public transportation. Continue reading to learn more about airport transfers. You might be surprised at just how affordable they can be. This is the easiest way to travel to and from any destination! Once you've booked your transfer, all you have to do is enjoy the convenience of having a cab driver pick you up at the airport.
There are many companies that offer airport transfers. But how do you choose the right one for your needs? A white-label service is a great option if you are looking for an easy way of getting from the airport to your hotel. These services are simple to set up and offer a great alternative to regular taxis or Uber. Here are some tips on how to choose the right airport transfer service for you. It may be the best way to ensure that you have the best possible trip.
Consider the cost of airport transfer services before you make your decision. Many companies offer a fixed-price service, which means you won't have to worry about paying for unexpected expenses. This method allows you to book a car as soon as you arrive rather than waiting until your flight is available. Once you've decided on the price and other details, you can book a transfer for yourself or your guests.
Airport transfers aren't charged by meters, unlike Uber and regular taxis. This means you won't be ripped off. The price you pay for your airport transfer will be the same as what you were quoted in your booking confirmation email. This means you won't have to worry about the price going up. You can also rest assured that your travel budget is more secure. To make a reservation, you can use a website.
Once you've chosen a city or airport, you can book a transfer service for a specific time. These transfers can be booked at any time, and you can even use a shared service for multiple people. Since these transfers are more expensive than regular taxis, they're a better choice for travelers. You can also share most airport transfers, which will save you money on a taxi ride.
Although a shuttle service is convenient, it is important to book a transfer via a website. Most hotels near airports have free shuttle services that will take you to your hotel. While they're more convenient than a taxi, these services often require reservations. Before booking, make sure to check the schedule and confirm the availability of a transfer. You can also use a shuttle service to and from the airport. Once you've arrived, the driver will greet you and get you to your hotel in no time."	0	9	0	sashagreg	airport-transfers-the-most-efficient-way-to-travel
1825	1825	statistic_oecd_betterlife	find the gap of life satisfaction in OECD	[]	"Context
Why is life satisfaction in some countries high and in some other countries low ? Please analize the factors which have influence to life satisfaction.
Content
I got the data from OECD website and preprocessed them for analysis.
Inspiration
Why is life satisfaction in some countries high and in some other countries low ? There are some reason for it. By EDA of a specific countries, we may be able to find a specific reason for it.
Data reference : https://www.oecdbetterlifeindex.org/#/11111111111"	0	20	1	sasakitetsuya	statistic-oecd-betterlife
1826	1826	Bilstmtpuv2		[]		0	14	6	mbonyani	bilstmtpuv2
1827	1827	Market Data		['business']		2	5	0	gmshroff	market-data
1828	1828	food-11		[]		1	5	0	w934091831	food11
1829	1829	mm_detection_lib		[]		1	9	0	dwchen	mm-detection-lib
1830	1830	Human trafficking	Human Trafficking/Commercial Sex Acts	['united states', 'crime', 'advanced', 'tabular data', 'social issues and advocacy']	"Context
The act requires the FBI to collect human trafficking offense data and to make distinctions between prostitution, assisting or promoting prostitution, and purchasing prostitution.
To comply with the Wilberforce Act, the national UCR Program created two additional offenses in the Summary Reporting System (SRS) and the National Incident-Based Reporting System (NIBRS) through which the UCR Program collects both offense and arrest data. 
Content
The FBI began accepting data on human trafficking from states in January 2013. Human trafficking includes offenses related to commercial sex acts and involuntary servitude. Human Trafficking data available through the Crime Data Explorer include offenses and arrests recorded by state and local agencies that currently have the ability to report this crime to the national UCR Program."	113	829	9	andrej0marinchenko	human-trafficking
1831	1831	UIEB TFRecords		[]		0	1	0	mmelahi	uieb-tfrecords
1832	1832	Bitcoin VS Altcoins and its possible correlation		['currencies and foreign exchange']	"Context
This project was created as a Case Study for the Google Data Analytics Capstone Project.
Content
The case study main goal is to investigate if there is a correlation between Bitcoin Prices and the Altcoins Prices."	0	16	0	fernandooliveira80	bitcoin-vs-altcoins-and-its-possible-correlation
1833	1833	corpus		[]		0	0	0	wilmerdurazno	corpus
1834	1834	SCAN_f30k_vocab		[]		0	17	0	larry2000	scan-f30k-vocab
1835	1835	TrainLabels		[]		0	0	0	gilbertmuchori	trainlabels
1836	1836	CRYPTO COINS PRICES FROM FEB.2021 TO FEB.2022		[]		2	10	1	fernandooliveira80	crypto-coins-prices-from-feb2021-to-feb2022
1837	1837	Top 1000 TikTok Influencers Ranking	Explore top 1,000 TikTok Influencers	['marketing', 'internet', 'exploratory data analysis', 'online communities', 'social networks']	"Context
Find the top TikTok accounts.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
Data source: https://hypeauditor.com/top-tiktok/"	174	1281	11	prasertk	top-1000-tiktok-influencers-ranking
1838	1838	Covid19arData COVID-19 Argentina data 	Data COVID-19 Argentina updated and in open formats	['tabular data', 'covid19']	"Spreadsheet exportado
para acceder al spreadsheet dinamico: https://docs.google.com/spreadsheets/d/16-bnsDdmmgtSxdWbVMboIHo5FRuz76DBxsz_BbsEVWA/edit?usp=sharing
Contexto
Repositorio creado por Sistemas Mapache con el objetivo de poder contar con datos abiertos de la información oficial proveniente de los partes diarios sobre la situación de COVID-19 en Argentina.
También se suman datos con mayor segmentación territorial de fuentes provinciales.
Los datos historicos provienen de fuentes oficiales y no se mezclan con fuentes no oficiales.
Data Dictionary
| Column Name         | Description                                                        |
|----------------------|-------------------------------------------------------------------|
| fecha                | fecha a la que corresponde los datos                              |
| dia_inicio           | cant dias desde el inicio del caso 1                              |
| dia_cuarentena_dnu260 | cant dias desde la cuarentena por DNU 260               |
| osm_admin_level_2         | nombre administrativo en OpenStreetMap escala país                                            |
| osm_admin_level_4     | nombre administrativo en OpenStreetMap escala provincia                                     |
| osm_admin_level_8     | nombre administrativo en OpenStreetMap escala ciudad                                      |
| tot_casosconf          | total de casos de infectados confirmados. Columna que sumariza fila a fila el total de casos confirmados                                    |
| nue_casosconf_diff           | nuevos casos infectados del dia                                           |
| tot_fallecidos             | total de fallecidos. Columna que sumariza fila a fila el total de fallecidos                                             |
| nue_fallecidos_diff                | nuevos casos fallecidos del dia                                                      |
| tot_recuperados                  | total acumulado de casos recuperados.                                                       |
| tot_test_negativos                  | total acumulado de tests negativos                                                    |
| tot_test             | total acumulado de tests |
| transmision_tipo | tipo de transmision al dia de la fecha                                            |
| informe_link | URL de acceso al informe de donde sale el dato                                            |
| transmision_tipo | Region shapefile in WKT                                            |
| observacion | observaciones relacionadas al dato o diferencias entre reportes                                           |
| covid19argentina_admin_level_4 | formato provincia requerido por  necesario covid19argentina.com                                            |"	170	3875	16	vladimirobellini	covid19ardata
1839	1839	Computer Vision for the Humanities (PH)	Programming Historian lessons on computer vision 	['education', 'computer vision']	The Dataset contains images derived from Newspaper Navigator, a dataset of images drawn from the Library of Congress Chronicling America collection. The data is shared primiarly to support a Programming Historian lesson on computer vision.	7	197	2	davanstrien	computer-vision-for-the-humanities-ph
1840	1840	Birds Aren't Real on Twitter Either	"Tweets about ""Birds Aren't Real"" (a Generation Z conspiracy theory)"	['biology', 'nlp', 'text data', 'social networks']	"Context
A significant number of members of Generation Z actively propagate (as a joke or seriously) the myth that birds doesn't exist anymore, because were gradually replaced by Government with drones.  These drones perform, in this conspiracy theory, surveillance of the citizens.
The movement took a certain momentum recently, here is a selection of articles documenting this strange phenomena:
* Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory
* ‘Birds Aren’t Real’: How A Parody Conspiracy Movement Fought ‘misinformation With Lunacy’
Collection
Tweets from @birdsarenotreal account on Twitter and using #birdsarenotreal hashtag
Collected using tweepy.
The data is not filtered.
Inspiration
Use the texts in this dataset to:   
improve your skills on doing data analysis using this data; 
perform sentiment analysis; 
perform topic modelling on the text corpus.
Note: you can combine this data (from Twitter) with data collected from Reddit, here:  Birds Aren't Real."	175	2426	28	gpreda	birds-arent-real-on-twitter-either
1841	1841	Phising		[]		0	5	0	axithchoudhary	phising
1842	1842	BestWorkOfSCAN		[]		0	9	0	larry2000	bestworkofscan
1843	1843	Credit Card Fraud 	Credit Card Fraud Dataset	[]		12	67	8	mohinurabdurahimova	credit-card-fraud-detection-using-machine-learning
1844	1844	Michelin Guide Restaurants	Restaurants awarded with Michelin stars in 2021	['food', 'restaurants']	"Context
Since 1900, Michelin is publishing today's well-known Michelin Guide which includes all kinds of helpful information for travelers such as restaurants' reviews.! Michelin has what's called restaurant inspectors who visit restaurants and award them with stars.
Content
The dataset contains information for all the restaurants awarded with a Michelin star/s in 2021. For instance, what cuisine do the awarded restaurants sever, their location, dining cost, etc? The data were scrapped through the Michelin Guide official website using python.
Source
Michelin Guide official website: https://guide.michelin.com/ (dataset created in December 2021)
Inspiration
• How many restaurants were awarded one, two, or three Michelin stars in 2021?
• Where are these restaurants located across the globe? Which country has the most?
• What type of cuisine is the most popular within awarded restaurants?
• What is the cost of visiting such restaurants? Do more stars translate to a more expensive dinner?
These are examples of interesting questions that could be answered by analyzing this dataset.
If you are interested, please have a look at the Tableau dashboard that I have created to help answer the above questions.
Tableau dashboard: https://public.tableau.com/views/MichelinStarRestaurants_16382046438400/MichelinGuide?:language=en-US&:display_count=n&:origin=viz_share_link"	11	61	0	dimitrisangelide	michelin-star-restaurants-2021
1845	1845	kambition-severity-dataset		[]		0	51	0	taishioikawa	kambitionseveritydataset
1846	1846	World Happiness Score Prediction		['religion and belief systems']		0	12	0	swrrocky17	worldhappinessscore
1847	1847	Dataset_CPS	Hello Model save Cross Pseudo Segmentation	['online communities']		0	11	0	sdfasfas	dataset-cps
1848	1848	jigsaw-train-roberta-base-ub		['games']		0	6	2	prateekagnihotri	jigsaw-train-roberta-base-ub
1849	1849	jigsaw-train-roberta-large-ub		[]		0	0	0	prateekagnihotri	jigsaw-train-roberta-large-ub
1850	1850	train_data_version2_xgb.json		[]		0	1	0	shaoqh	train-data-version2-xgbjson
1851	1851	datacb		[]		0	0	0	rfairoza	datacb
1852	1852	jigsaw-old-train-csv		[]		0	2	0	suleyman71	jigsawoldtraincsv
1853	1853	Spike Wheat		['nutrition']		0	7	3	hungkhoi	spike-wheat
1854	1854	ECGchallenge2020		[]		6	12	0	mohammad87	ecgchallenge2020
1855	1855	wheat 2017		[]		0	4	0	hungkhoi	wheat-2017
1856	1856	Meteor falls in india	Meteor falls in different parts of India 	['india', 'astronomy', 'data cleaning', 'data visualization', 'tabular data']		3	11	0	ankitanshu	meteor-falls-in-india
1857	1857	jigsaw-train-roberta-base-wiki		['video games']		1	15	2	prateekagnihotri	jigsaw-train-roberta-base-wiki
1858	1858	jigsaw-train-roberta-large-wiki		['video games']		0	1	0	prateekagnihotri	jigsaw-train-roberta-large-wiki
1859	1859	Arknights Operator DMG dataset	All damage per minute data from all recruit-able operators in Arknights	['games', 'tabular data', 'matplotlib', 'pandas']	"Context
From a player to the players. 
Operators are important. Indeed, no bad operators. But sometimes we must think objectively about them. 
Content
The dataset is a processed version of this dataset that I took on early February, 2022. It's an ongoing dataset, so expect I will also update this dataset when the newest operators are released.
Acknowledgements
Thanks for José Costa who webscraping the dataset so we can use them. May all the doctors benefited by our effort. Long live Rhodes Island!
Inspiration
It started from my own question: Which operators best to invest? So I search their data, I process them, and I answer my own question. Well, not truly answered. But at least my finding help me to at least understand my own operators. Hopefully it also help other Arknights players. Your sanity is deserved some support, doctor."	4	208	3	yafethtb	arknights-operator-dmg-dataset
1860	1860	Dyni Odontocete Click		[]		0	7	0	grgoryadam	dyni-odontocete-click
1861	1861	IoT Healthcare requirements dataset		[]		5	26	0	iqrakhurshid	iot-healthcare-requirements-dataset
1862	1862	tfbr32		[]		1	23	0	v1olet3	tfbr32
1863	1863	NSW COVID-19 cases by location	Confirmed COVID-19 cases in NSW, Australia by location	['australia', 'law', 'medicine', 'beginner', 'intermediate', 'covid19']	"Context
NSW has been hit by the Omicron variant, with skyrocketing cases. This dataset, updated regularly, details the location of positive cases. A prediction of where the most cases could occur can be derived from this dataset and a potential prediction of how many cases there is likely to be. 
Content
notification_date: Text, dates to when the positive case was notified of a positive test result. 
postcode: Text, lists the postcode of the positive case.
lhd_2010_code: Text, the code of the local health district of the positive case.
lhd_2010_name: Text, the name of the local health district of the positive case.
lga_code19: Text, the code of the local government area of the positive case.
lga_name19: Text, the name of the local government area of the positive case.
Acknowledgements
Thanks to NSW Health for providing and updating the dataset.
Inspiration
The location of cases is highly important in NSW. In mid-2021, Western Sydney had the highest proportion of COVID-19 cases with many deaths ensuing. Western Sydney is one of Sydney's most diverse areas, with many vulnerable peoples. The virus spread to western NSW, imposing a risk to the Indigenous communities. With location data, a prediction service can be made to forecast the areas at risk of transmission."	16	220	2	livheaton	nsw-covid19-cases-by-location
1864	1864	jigsaw_luke_base		[]		0	5	0	trongminhle	jigsaw-luke-base
1865	1865	input_data		[]		1	2	0	adeshtrivedi	input-data
1866	1866	yago_3.0.x		[]		0	18	0	vincentholmes	yago-30x
1867	1867	Musical Instrument Chord Classification (Audio)	Classify if a tune is major or minor for piano and guitar	['music', 'beginner', 'classification', 'deep learning', 'audio data']	"Task
Using modern algorithms classify if a tune is of a major chord or minor chord.
Content
The dataset contains audio files from two instruments, guitar and piano. The data is scraped form various sources. Music is all about patterns. Once you know those “rules” and patterns, you can figure out pretty much anything on your own. Most of the time, when all else is held constant, music in a major key is judged as happy while minor key music is heard as sad.
Similar Works
@mehanat96 has a similar dataset poster earlier with rock guitar you can check out his dataset here. 
My dataset is a combination of Acoustic Guitar and Piano. Users can try combining our datasets and experiment. 
Also refer to  @mpwolke's notebook here for basic EDA understanding for audio data."	18	427	15	deepcontractor	musical-instrument-chord-classification
1868	1868	Brazilian-fire-data		['business']	"Forest fires are a serious problem for the preservation of the Tropical Forests. Understanding the frequency of forest fires in a time series can help to take action to prevent them.
Brazil has the largest rainforest on the planet that is the Amazon rainforest.
Content
This dataset report of the number of forest fires in Brazil divided by states. The series comprises the period of approximately 10 years (1998 to 2017). The data were obtained from the official website of the Brazilian government.
Acknowledgements
We thank the brazilian system of forest information
Inspiration
With this data, it is possible to assess the evolution of fires over the years as well as the regions where they were concentrated.
The legal Amazon comprises the states of Acre, Amapá, Pará, Amazonas, Rondonia, Roraima, and part of Mato Grosso, Tocantins, and Maranhão."	0	7	0	nithyalakshmis	fire-data
1869	1869	goodreads books		['literature']		0	13	0	nithyalakshmis	goodreads-authors
1870	1870	trained-RoBERTa		[]		0	21	0	kakenatio	tcbr-model
1871	1871	HSBR_model		[]		0	10	0	kdshjp0501	hsbr-model
1872	1872	TitanicNathanFile 		[]		0	1	0	nathanberhe	titanicnathanfile
1873	1873	RSNA Ischemia Detected		[]		0	2	0	fereshtej	rsna-ischemia-detected
1874	1874	Breast cancer		['cancer']		4	42	0	fairizatafida	breast-cancer
1875	1875	[NumeraiSignals] Ticker Universe and Mapper	Also Historical Targets	['finance', 'tabular data', 'investing']	"Content
Numerai Signals' historical targets, ticker universe, and sample submission file fetched from the official website."	12	283	3	code1110	numeraisignals-ticker-universe-and-mapper
1876	1876	ML_from_scratch	Implementation of Machine Learning algorithms from scratch	['computer science', 'nlp', 'classification', 'clustering', 'regression']	This dataset contains implementations of various Machine Learning algorithms from scratch. These implementations have helped me in having a deeper understanding of algorithms thus having better clarity and comfortability while explaining in interviews. I keep adding more implementations with time.	92	1047	23	rajat95gupta	ml-from-scratch
1877	1877	TinyHero - Retro pixel characters dataset 	Around 4000 images 64x64x3 of tiny game characters	[]		66	1737	6	calmness	retro-pixel-characters-generator
1878	1878	weatheraqi		[]		0	1	0	harshitjaisiyan003	weatheraqi
1879	1879	Student Performance		['universities and colleges']		3	16	0	keshavbhai	student-performance
1880	1880	Wind Speed of Bangladesh from 1980 to 2019		[]		0	6	0	rabeyaakter	wind-speed-of-bangladesh-from-1980-to-2019
1881	1881	datakanker		[]		0	0	0	putrianggia	datakanker
1882	1882	feature_engineering_wids2022		[]		0	1	0	schopenhacker75	feature-engineering-wids2022
1883	1883	kankerdata		[]		0	0	0	rfairoza	kankerdata
1884	1884	cancer-data		['cancer']		1	9	0	zidanaharisma	cancerdata
1885	1885	Jigsaw-PL		['puzzles']		0	5	1	prateekagnihotri	jigsawpl
1886	1886	data kanker		[]		0	0	0	putrianggia	data-kanker
1887	1887	ATP/WTA Year-End Top 10	ATP 1973-2021, WTA 1975-2021	['games']	"ATP/WTA Year-End Top 10
★ indicates player's highest year-end ranking
ATP data during 1973-2021
https://en.wikipedia.org/wiki/ATP_Rankings
WTA data during 1975-2021
https://en.wikipedia.org/wiki/WTA_Rankings"	1	16	7	stpeteishii	atp-wta-top10
1888	1888	Walmart Sales Data	Sales data for 45 stores of Walmart	['beginner', 'exploratory data analysis', 'data analytics', 'tabular data', 'retail and shopping']		20	99	0	godwinabah	walmart-sales-data
1889	1889	Model Comvis No.1		['clothing and accessories']		0	5	0	edgardjonathan	model-comvis-no1
1890	1890	UMP npy dataset		['earth and nature']		38	215	6	takamichitoda	ump-npy-dataset
1891	1891	Philippi		[]		0	2	0	thabangmalapane	philippi
1892	1892	jrpretrained3		[]		3	4	0	wowfattie	jrpretrained3
1893	1893	20-Bn Jester TFRecord (16 Classes)	20bn Jester Dataset with 16 classes TFRecord Format..	['education', 'computer vision', 'multiclass classification', 'tensorflow', 'transformers']	"Context
Loading Jester Dataset for personal project in limited hardware was problematic. Facing producer-consumer problem I decided to create TFRecord for a limited set of classes.
Content
20BN Jester Dataset was filtered and a small set of gestures were extracted and converted to TFRecordFormat.
Image size 100x100 with 36 frames.
Gestures =  [ 
    ""Swiping Up"", ""Swiping Down"", ""Swiping Left"", ""Swiping Right"",
    ""Zooming In With Two Fingers"",""Zooming Out With Two Fingers"",
    ""Zooming In With Full Hand"",""Zooming Out With Full Hand"",""Turning Hand Clockwise"",
    ""Turning Hand Counterclockwise"",""Thumb Up"",""Thumb Down"",""Shaking Hand"",
    ""Stop Sign"",""Doing other things"",""No gesture""
]
Train Samples: 71724
Validation Samples: 9036"	0	17	1	sahasprajapati	20bn-jester-tfrecord-16-classes
1894	1894	yolov555		[]		0	0	0	srachejack	yolov555
1895	1895	jrpretrained4		[]		1	4	0	wowfattie	jrpretrained4
1896	1896	aisegmentcom-matting-human-datasets		[]		0	7	0	medetovyerma	matting
1897	1897	yolodd		[]		0	1	0	srachejack	yolodd
1898	1898	MONEY HEIST IMDB DATA		['movies and tv shows']		71	567	3	ankita9sharma	money-heist-imdb-data
1899	1899	tts_download_pretrained_models		[]		0	2	0	oldbirdaz	tts-download-pretrained-models
1900	1900	jrconfigs		[]		1	15	0	wowfattie	jrconfigs
1901	1901	UnEven		[]		0	2	0	mengyangyang0001	uneven
1902	1902	India: Budget 2022-23 (Higher Education)	Budget for Higher Education in India 2022-23	['education', 'finance']	"Context
Total Union Budget allocation for the Department of Higher Education under the Ministry of Education.
Content
It contains budgetary allocations by the Union Government for interventions in University and Higher education through University Grants Commission (UGC), Assistance to State Governments for Degree Colleges, various schemes and programmes like Rashtriya Uchcha Shiksha Abhiyan (RUSA), National Mission on Teachers and Teaching, National Initiative on Sports and Wellness, Support for Skill based Higher Education including Community Colleges, Andhra Pradesh and Telangana Tribal Universities, and Establishment of Tribunals, Accreditation Authority, NCHER and National Finance Corporation.
Acknowledgements
https://openbudgetsindia.org/
Inspiration
What kind of resources are being allotted to each section of Higher Education?
What states are on top of the priority list?
What resources are being allotted to advancement of R&D in Higher Education?"	12	90	3	samkhare	educationbudget
1903	1903	HW1_Team4_Q3		[]		0	6	0	hiraabdulaziz	hw1-team4-q3
1904	1904	sclab class speech command	재직자 교육을 위해 올린 데이터 입니다	['education']	"Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition
SCLAB 제직자 교육을 위해 가져온 교육용 데이터입니다.
출처 : https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/audio/speech_commands.py"	0	11	0	kcy0206	sclab-class-speech-command
1905	1905	jrpretrained1		[]		1	8	1	wowfattie	jrpretrained1
1906	1906	Our World in Data - COVID-19	COVID-19 Dataset by Our World in Data	['public health', 'health', 'intermediate', 'advanced', 'public safety', 'covid19']	"Our World in Data - COVID-19
▶ About Our World in Data 🏢
Our World in Data website
Our World in Data GitHub
▶ Similar Datasets 📄
COVID-19 World Vaccination Progress
The Our World in Data COVID Vaccination Data
Data on COVID-19 (coronavirus)
COVID-19 dataset by Our World in Data
▶ Context 📝
The complete COVID-19 dataset is a collection of the COVID-19 data maintained and provided by Our World in Data. Our World in Data team will update it daily throughout the duration of the COVID-19 pandemic.
▶ Content 📃
These are the following information that includes in the dataset:
| Metrics | Source | Updated | Countries |
| --- | --- |
| Vaccinations | Official data collated by the Our World in Data team | Daily | 218 |
| Tests & positivity | Official data collated by the Our World in Data team | Weekly | 139 |
| Hospital & ICU | Official data collated by the Our World in Data team | Weekly | 39 |
| Confirmed cases | JHU CSSE COVID-19 Data | Daily | 196 |
| Confirmed deaths | JHU CSSE COVID-19 Data | Daily | 196 |
| Reproduction rate | Arroyo-Marioli F, Bullano F, Kucinskas S, Rondón-Moreno C | Daily | 185 |
| Policy responses | Oxford COVID-19 Government Response Tracker | Daily | 186 |
| Other variables of interest | International organizations (UN, World Bank, OECD, IHME…) | Fixed |
Data dictionary is available below ⤵
▶ Acknowledgements 🙏
I'd like to clarify that I'm only making data about vaccines collected by Our World in Data available to Kaggle community. 
This dataset is gathered, integrated, and posted the new version on a daily basis, as maintained by Our World in Data on their GitHub repository.
▶ Inspiration 💭
Forecasting daily new confirmed cases of COVID-19 in specific country.
Perform data analysis/data visualization of COVID-19 cases/death/etc.
📷 Images by Fusion Medical Animation."	69	711	16	caesarmario	our-world-in-data-covid19-dataset
1907	1907	Unregulated SE Arizona Groundwater Pumping	Well depths & water levels in at-risk Willcox and Douglas groundwater basins.	['north america', 'environment', 'business', 'agriculture', 'energy']	"Context
There are currently no regulations for groundwater pumping in the Willcox and Douglas basins of drought-stricken Southeast Arizona. Over the past few decades, the number of industrial farms and dairy lots have expanded, along with the depth of their wells. Residents near these half-mile deep wells have been reporting their wells running dry, and in some areas, the ground has cracked open due to excessive groundwater pumping. 
This dataset was collected using publicly available well registration data from the AZ Department of Water. Included are all registered wells, split into 4 data sets, divided by area and basin. 
For a head start on processing, and to see the data split into categories of ownership (Farms, Oil, Energy, Residential, Federal, State,...) see my notebook ""Processing and Categorizing.""
Data Source:
AZ Well Registry Data:
Cochise County data last collected on January 2nd, 2022
Individual Basin data collected on January 22nd, 2022
For useful information on the Indexing of the Registry numbers, see this guide. 
News Coverage on the Arizona Water Crisis in Cochise County:
""In southeastern Arizona, farms drill a half-mile deep while families pay the price,"" 
Published Dec 5th, 2019 by Arizona Central.
“Draining Arizona: Residents say corporate mega-farms are drying up their wells” 
Published Sept 17th, 2019 by NBC News.
Willcox basin groundwater flow model simulation results and analysis. 
Arizona Department of Water Resources Groundwater Flow Model of the Willcox Basin
Published July 16th, 2018 by the Arizona Department of Water Resources."	3	23	0	torinhodge	unregulated-se-arizona-groundwater-pumping
1908	1908	EPIC RPG Captcha		[]		0	7	0	evriskon	epic-rpg-captcha
1909	1909	Dataset Saham Bursa Efek	harga saham di bursa efek indonesia	['economics']	Dataset ini diambil dan diolah dari website PT Bursa Efek Indonesia (https://idx.co.id/). Semua data yang ada dalam dataset adalah milik PT Bursa Efek Indonesia. Silakan mengacu pada Syarat Penggunaan (https://idx.co.id/footer-menu/tautan-langsung/syarat-penggunaan/) yang dimiliki oleh PT Bursa Efek Indonesia.	46	387	4	agungpambudi	dataset-saham-bursa-efek
1910	1910	cots_albs_2560_10		[]		0	4	0	dragonzhang	cots-albs-2560-10
1911	1911	Forged Character Detection Dataset on Visa Cards		[]		0	13	0	turabbajeer	forged-character-detection-dataset-on-visa-cards
1912	1912	roberta		['arts and entertainment']		0	3	0	elleking	roberta
1913	1913	Jigsaw-toxic-severity-rating-public-LB-2021-12-27	Leaderboard Distribution Dataset for Jigsaw Toxic Comments	['computer science', 'beginner', 'data visualization', 'outlier analysis', 'tabular data', 'online communities']	"Context
This is a snapshot of the Jigsaw Toxic Comment Severity Rating competition downloaded on 2021-12-27 from the Public Leaderboard.
Content
Kaggle provides the Team Id number, Team name, Submission Date, and Score. I wish they provided number of submissions just to see if there is any correlation.
Inspiration
I did this to just look at the distribution of the Leaderboard Scores. Curious as to whether popular shared notebooks copied would show up..."	3	105	2	krist0phersmith	jigsawtoxicseverityratingpubliclb20211227
1914	1914	data_cats_and_dogs		[]		0	4	0	temmylan	catsanddogs
1915	1915	reef-yolox-m-aug-v8		[]		0	0	0	sangayb	reef-yolox-m-aug-v8
1916	1916	XLNET_NO_CLEAN_256_TRY2_10folds		[]		0	6	0	hangy132	xlnet-no-clean-256-try2-10folds
1917	1917	roberta-large-config		[]		0	3	0	cpmpml	roberta-large-config
1918	1918	BERT-based Models for Special Applications	A collection of models for structured documents and medical NLP	['computer science', 'deep learning', 'text data']	"This dataset includes two types of models: LayoutLM models for structured documents and BioBERT based models for medical applications.
LayoutLM
LayoutLM is a simple but effective multi-modal pre-training method of text, layout and image for visually-rich document understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets.
https://github.com/microsoft/unilm/tree/master/layoutlm
Update: LayoutLM has been integrated into the transformers library! You can download models directly in your code with an internet connection. The files here can be used in any Kaggle notebook with or without an internet connection, which is useful for code competitions.
Update2: LayoutLMv2 is released! You will want to use these models in many cases, especially if images are available.
<br>
BioBert
BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement)
You can get details on each derivative from the file descriptions."	363	9950	30	jpmiller	layoutlm
1919	1919	trabajo		[]		0	7	0	lloclli	emnist
1920	1920	Crown-of-thorns starfish	About 429 images of COTs,  2021 labels	['image data']		46	401	14	antonsibilev	crownofthorns-starfish
1921	1921	pytorch-w-b-jigsaw-starter2		[]		0	10	0	g720052	pytorch-w-b-jigsaw-starter2
1922	1922	soft_faster_pretrained_on_livecell_img_scale_plus		[]		0	2	0	itaynivtau	soft-faster-pretrained-on-livecell-img-scale-plus
1923	1923	Weighted-Boxes-Fusion	Git of weighted boxes fusion library	[]		0	11	0	hey24sheep	weightedboxesfusion
1924	1924	JS RoBERTa		['music']		6	32	0	aishikai	js-roberta
1925	1925	Train Dataset 	Data Science Tutorials	['education', 'computer science']		0	6	0	mucahidozcelik	traintsv
1926	1926	landuse2		[]		0	14	0	sergiovitale	landuse2
1927	1927	JS ML Models		[]		0	2	0	aishikai	js-ml-models
1928	1928	Ubiquant Dataset Compressed		['earth and nature']		0	3	0	eduardogutierrez	ubiquant-dataset-compressed
1929	1929	Los números @123	Lenguaje binario (lenguaje dé computación).	[]		2	7	2	stevensoncr	los-nmeros-123
1930	1930	FIFA FOOTBALL PLAYERS		[]		2	24	1	marianaponce	fifa-football-players
1931	1931	eda_project_analyze_US_Citizens		[]		0	4	1	menesakpinar	eda-project-analyze-us-citizens
1932	1932	Arabic sign language unaugmented dataset	Unaugmented dataset of hand gestures for arabic sign language 	['arts and entertainment', 'artificial intelligence', 'deep learning', 'image data', 'regression', 'pytorch']	"Context
This is a dataset of hand gestures that represent arabic sign language. the creation of this dataset was one of the tasks for developement of an web app for the detection of arabic sign language.
Content
The arabic sign language is composed of 290 images for testing set, 4651 images for training set, 891 images for validation set for a total of 5832 images with each image having the size of 416 × 416 pixels.
These images where taken in different environments using a cell phone camera with different backgrounds plus different hand angles.
This is the unaugmented version of Arabic sign language dataset : https://www.kaggle.com/sabribelmadoui/arabic-sign-language-augmented-dataset"	1	30	4	sabribelmadoui	arabic-sign-language-unaugmented-dataset
1933	1933	MOMA Artworks on View	Data on Artworks Currently on View at MoMA	['arts and entertainment', 'art', 'text data']	"About
This dataset contains data for all the works currently on view at the Museum of Modern Art as of February 1st 2022. The data includes information such as the medium of the art piece, the artist, a description of the art piece, dimensions of the art, etc. 
Data was scraped from MoMA's website here: https://www.moma.org/collection/works/"	43	373	9	jackogozaly	moma-artworks-on-view
1934	1934	Player Counts on Steam	For top ~1000 games, goes back monthly to 2012 or release of game	['video games', 'tabular data']	"Content
steam_charts.csv contains monthly average player counts by game for ~1000 games, going back to 2012 or when the game was released. The current version was scraped on October 3rd, 2021. I plan on updating this dataset around the beginning of each month, with hopefully more games in the future. 
Acknowledgements
Huge thanks to steamcharts, where this data was scraped from with this code! 
Image"	200	1407	10	josephvm	player-counts-on-steam
1935	1935	soft_pretrainlive_imgscaleplus1333,800 1690,960		['software']		0	4	0	itaynivtau	soft-pretrainlive-imgscaleplus1333800-1690960
1936	1936	Arabic sign language augmented dataset	Augmented dataset of hand gestures for arabic sign language 	['earth and nature', 'artificial intelligence', 'deep learning', 'image data', 'regression', 'pytorch']	"Context
This is a dataset of hand gestures that represent arabic sign language. the creation of this dataset was one of the tasks for developement of an web app for the detection of arabic sign language.
Content
The arabic sign language is composed of 290 images for testing set, 13926 images for training set, 870 images for validation set for a total of 15086 images with each image having the size of 416 × 416 pixels.
These images where taken in different environments using a cell phone camera with different backgrounds plus different hand angles.
A set of augmentation technique were applied to this dataset which are:
- Salt and pepper noise : Up to 5% of the pixels
- Gaussian blur : 1px
- Grayscale transformation : 25% of images
- Rotation : Between -20° and + 20°"	8	34	3	sabribelmadoui	arabic-sign-language-augmented-dataset
1937	1937	motocicletaszipado		[]		0	6	0	leonardommarques	motocicletaszipado
1938	1938	norfair_last_commit		[]		0	4	0	svyatoslavsokolov	norfair-last-commit
1939	1939	CPS2015.RData		[]		0	5	0	michaelvanniekerk	cps2015rdata
1940	1940	Divvy bike share data 2021 	Merged 2021 data set 	['north america', 'business', 'beginner']	"Divvy bike share data
The dataset contains the Divvy bike share data unedited but merged over 2021.
For usage with the coursera Google Data Analytics capstone project Cyclistic. 
Origin of the data
The data can be downloaded here: https://divvy-tripdata.s3.amazonaws.com/index.html
The data has been made available by Motivate International Inc. under the following license https://ride.divvybikes.com/data-license-agreement"	0	14	0	bontius	divvy-bike-share-data-2021
1941	1941	Rolling Stone's Top 500 Songs Of All Time	Audio features from Spotify for Rolling Stone's top 500 songs	['popular culture', 'music']	"Find out what makes a song great!
I scraped Rolling Stone's website to obtain the list of the 500 greatest songs of all time. Subsequently, I downloaded data from Spotify's API to retrieve audio features (such as danceabilty, time signature key, tempo) for eachof the songs on the list.
What you will find.
The csv. file contains the final dataset with all the song's authors, titles, and additional data from the RS website (producer, writer, and release year), as well as all the audio features from Spotify.
The documentation for the audio features is available here.
I am writing a series on Medium describing the process if you are interested.
Acknowledgements
The original list can be found following this link.
Kudos to the creator and the contributors of the spotipy library for such an awesome tool.### Find out what makes a song great!
I scraped Rolling Stone's website to obtain the list of the 500 greatest songs of all time. Subsequently, I downloaded data from Spotify's API to obtain audio features for each of the songs on the list.
What you'll find.
The csv. file contains the  final dataset with all the song's authors, titles, and additional data from the RS website (producer, writer and release year),
 as well as all the audio features from Spotify.
The documentation for the audio features is available here.
I am writing a series on Medium describing the process if you are interested.
Acknowledgements
The original list can be found following this link.
Kudos to the creator and the contributors of the spotipy library for such an awesome tool."	169	1375	2	bernardinosassoli	rolling-stones-top-500-songs-of-all-time
1942	1942	Word-cloud Masks	Images to use in creating masked word-clouds for reviews' data.	['arts and entertainment', 'ratings and reviews']	"Content
5 png images of the numbers 1 to 5, useful as masks in word-clouds for reviews' data (See https://www.kaggle.com/timothyabwao/amazon-food-reviews-eda/)
Acknowledgements & Inspiration
https://amueller.github.io/word_cloud/auto_examples/masked.html"	0	28	3	timothyabwao	wordcloud-masks
1943	1943	Synthesised Time Series Data	Times-series data synthesised with Python.	['earth and nature', 'intermediate', 'time series analysis', 'tabular data', 'pandas']	"Context
I used Python to synthesise some periodic time series data. This can be found on the Neuron AI datasets repo.
Content
The ID is the number, the others are the values.
Usage
Please use for learning time series analysis."	1	56	4	passwordclassified	synthesised-time-series-data
1944	1944	motocicletas		[]		4	12	0	leonardommarques	motocicletas
1945	1945	jigsaw_weights		[]		0	6	0	prathyushaakundi	jigsaw-weights
1946	1946	Concert1_2		[]		2	16	0	melslater	concert1-2
1947	1947	handpose-weights-updated		['exercise']		29	16	0	rustyelectron	handposeweightsupdated
1948	1948	Titanic		[]		0	4	0	aashimaaa	titanic
1949	1949	wiki_az		[]		0	0	0	fidanmusazade	wiki-az
1950	1950	concert2		[]		0	11	0	melslater	concert2
1951	1951	iris dataset		[]		1	2	0	saraeldeeb	iris-dataset
1952	1952	Training data		['education']		0	7	0	swastikjha	train
1953	1953	Spotify Daily Top 200 Tracks in the Philippines	Data of Philippines' top 200 streamed songs in Spotify, 2017-present	['music']	"Data of Philippines' top 200 streamed songs in Spotify, 2017-present
Terms of Use
Use of this dataset is bound by the Spotify Developers's Terms and Conditions https://developer.spotify.com/terms/"	498	3615	20	jcacperalta	spotify-daily-top-200-ph
1954	1954	Retail Store Sales Data	Database on product sales and their respective managers	['brazil', 'data cleaning', 'data visualization', 'data analytics']	"Inspiration
What are the countries where the basic subscription is cheaper? And more expensive?
What are the countries where the standard subscription is cheaper? And more expensive? 
What are the countries where the premium subscription is cheaper? And more expensive?
Which manager earned the most? and with which product?
Which manager earned the least?"	213	1421	14	henriqueliberato	retail-store-sales-data
1955	1955	Happywhale test species	Predicted species for test dataset based on train data	['earth and nature', 'animals']	"What is it?
This is predicted species classes for test dataset from Happywhale - Whale and Dolphin Identification competition
How was it train?
The kernel with train setup (Swin + fastai): https://www.kaggle.com/kwentar/species-classification
Some analysis: https://www.kaggle.com/kwentar/test-data-species"	1	56	1	kwentar	happywhale-test-species
1956	1956	battiti		[]		0	10	0	alicescande	battiti
1957	1957	Rainforest_csv_only		[]		0	0	0	napsugrgti	rainforest-csv-only
1958	1958	Hutton Rock Dataset	rock images dataset for image classification 	['geology', 'artificial intelligence', 'classification', 'image data', 'tensorflow']	"Context
I have always been curios of nature. One of the most insightful information that humans can get to perceive nature and Earth comes from rocks. A rock indicates the content of its surroundings, so that scientists can map the environment and understand the story behind it. Therefore, this dataset has been created to make a way for the human perception of nature.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered"	19	482	7	quelich	rock-photos
1959	1959	SYSU-CD		['music']		0	8	0	kacperk77	sysucd
1960	1960	Music_data		[]		0	4	0	prakharkhanduri	music-data
1961	1961	VCTK Dataset		['earth and nature']	"Info
Speakers: 109
Num Audio: ~44 k
Sample Rate: 48 kHz
Storage: 16 bit
Description
This CSTR VCTK Corpus (Centre for Speech Technology Voice Cloning Toolkit) includes speech data uttered by 109 native speakers of English with various accents. 96kHz versions of the recordings are available at https://doi.org/10.7488/ds/2101. Each speaker reads out about 400 sentences, most of which were selected from a newspaper plus the Rainbow Passage and an elicitation paragraph intended to identify the speaker's accent. The newspaper texts were taken from The Herald (Glasgow), with permission from Herald & Times Group. Each speaker reads a different set of the newspaper sentences, where each set was selected using a greedy algorithm designed to maximize the contextual and phonetic coverage. The Rainbow Passage and elicitation paragraph are the same for all speakers. The Rainbow Passage can be found in the International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf . 
All speech data were recorded using an identical recording setup: an omni-directional head-mounted microphone (DPA 4035), a 96kHz sampling frequency at 224 bits, and in a hemi-anechoic chamber of the University of Edinburgh. All recordings were converted into 16 bits, downsampled to 48 kHz based on STPK, and manually end-pointed. This corpus was recorded for the purpose of building HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis using average voice models trained on multiple speakers and speaker adaptation technologies. The file was previously available on the CSTR website, and was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 108 of the 109 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error
&gt; This item has been replaced by the one which can be found at https://doi.org/10.7488/ds/2645 ##'"	0	8	0	showmik50	vctk-dataset
1962	1962	Augmented validation dataset COCO VQA v2.0	Dataset contains augmented question texts from COCO VQA v2.0 validation set	['earth and nature', 'computer science']		0	10	0	manwithaflower	augmented-validation-dataset-coco-vqa-v20
1963	1963	ConfigYolov4		[]		0	2	0	ahmedstohy	configyolov4
1964	1964	Pass your Data Science Interviews	Case Study: Applicants for a Gold Digger position	['business', 'computer science', 'exploratory data analysis', 'data cleaning', 'data visualization']	"Context
This dataframe describes applications for a Gold Digger position. According to each applicants's characteristics, can you create the best model to classify whether a candidate is hired or not ?
It is a good playground to harden your data science skills and try new models. Ideal to prepare interviews.
Content
This dataframe contains 20000 observations and 11 columns:
date: date of the application
age: age of the candidate
diplome: highest qualification diploma (bac, licence, master, doctorat)
specialite: minor of the diploma (geologie, forage, detective, archeologie,...)
salaire: salary expectation
dispo: oui : directly available, non : not directly available
sexe: female (F) or male (M)
exp: years of relevant experience
cheveux: hair color (chatain, brun, blond, roux)
note: grade (out of 100) for gold digging exam
embauche: Has the candidate been hired ? (0 : no, 1 : yes)"	211	3767	23	bryanb	applicants-for-a-gold-digger-position
1965	1965	Hamada		['music']		0	3	0	radwanow	hamada
1966	1966	with Xml final		['computer science', 'programming']		0	3	0	kazimahadihasan	with-xml-final
1967	1967	cgrh4kfolds		[]		0	2	0	rasterbunny	cgrh4kfolds
1968	1968	saless		[]		0	0	1	muhammadammarjamshed	saless
1969	1969	pretrained-models		[]		0	12	0	longhuqin	pretrained-models
1970	1970	Great-Barrier-Reef-to-tfrecords		[]		0	5	0	eduardogutierrez	greatbarrierreeftotfrecords
1971	1971	NFT Collections 	This includes data about over 250 NFT Collections 	['music', 'art', 'beginner', 'tabular data', 'matplotlib', 'pandas']	"NFT DATASET
This dataset consists of 250 collections and their all time statistics such as sales, transactions, ownership and buyers
A sample EDA performed by me could be found in the Code section! Do check it out!
will update the dataset in every 15 days.
The updated dataset will be named as nft_sales _v[DD][MM].csv
CONTENTS
The dataset is a .csv file with the following columns:
- Collections
- Sales
- Buyers
- Transactions
- Owners
Scraping code at GitHub repo: https://github.com/hemil26/NFT-Dataset
CREDITS
This dataset has been scraped from https://cryptoslam.io/
INSPIRATION
Which collection has highest all time buyers
Is there any correlation between buyers and sales?
Simple EDA and Visualization
Amount of ETH earned by a collection?"	599	6157	52	hemil26	nft-collections-dataset
1972	1972	jigsaw_cleaned_data.csv'		[]		0	0	0	tsashuabowetumawe	jigsaw-cleaned-datacsv
1973	1973	JS Cleaned Validation Data		['programming']		12	5	0	aishikai	js-cleaned-validation-data
1974	1974	Shark-Tank-India	A File containing all the Pitches that happened at Shark Tank India	['arts and entertainment', 'india', 'business', 'beginner', 'tabular data']	"Context
Shark Tank recently came to India and became a massively popular show replacing the stereotypical family drama.
So I wanted to make a Dataset that keeps to date with all the Pitches that happen on Shark Tank India
Content
There are 14 columns with Episode Number, Pitch Id, Name of the Brand, Brief Idea of the Brand, The amount invested by the sharks, The amount of debt given (if any), the amount of equity diluted by the brand, and the Next seven columns correspond to the Sharks and the Y states if they have invested in that Brand or not. The last column is for the Season Number
Expected Update Frequency
With every new season of Shark Tank India, I wish to keep this data up to date with all the Pitches so the dataset will be updated with every new season.
Acknowledgements
https://en.wikipedia.org/wiki/Shark_Tank_India"	184	1133	25	anshulmehtakaggl	sharktankindia
1975	1975	Pharma Air Handling Units Data	Temperature, RH and Other Room Conditions	[]		2	10	0	vitthalmadane	pharma-air-handling-units-data
1976	1976	Ontario Covid Cases by Health Unit		['health']		2	28	0	luckypen	ontario-covid-cases-by-health-unit
1977	1977	datasetnew		[]		0	4	0	adityasanju	datasetnew
1978	1978	dataset		[]		0	1	0	adityasanju	dataset
1979	1979	IMDB_Dataset		[]		0	3	0	mostafaahmed15156	imdb-dataset
1980	1980	last_target_dataset_ubiquant		[]		0	3	0	sashakuzmin	last-target-dataset-ubiquant
1981	1981	LibriSpeech-ASR: WAV Dataset		[]	"About:
LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from reading au audiobooks from the LibriVox project, and has been carefully segmented and aligned.
Acoustic models, trained on this data set, are available at kaldi-asr.org and language models, suitable for evaluation can be found at http://www.openslr.org/11/.
For more information, see the paper ""LibriSpeech: an ASR corpus based on public domain audiobooks"", Vassil Panayotov, Guoguo Chen, Daniel Povey and Sanjeev Khudanpur, ICASSP 2015 (submitted) (pdf)"	1	8	0	showmik50	librispeech-asr-wav-dataset
1982	1982	hair-masks		[]		1	4	0	mirfan899	hairmasks
1983	1983	spopdf		[]		1	4	0	trulith	spopdf
1984	1984	women_electors_india_2009_2019	Women electors in three General Elections from 2009 to 2019 (2009, 2014, 2019)	['india', 'politics', 'beginner', 'intermediate', 'tabular data']	"Source
This data has been downloaded from Election Commission of India's website and it's in public domain. The data can be freely used.
Format
The data for the year 2019 and 2014 is available in .xlxs format. However, 2009's data is available in pdf format and that has been converted by me using Tabula Technology tool in .xlsx format and then uploaded here."	3	33	3	aroravish	women-electors-india-20092019
1985	1985	images		[]		0	6	0	neerajkaroshi	images
1986	1986	FlightDelays.cvs		[]		0	3	0	kshitijsingh23	flightdelayscvs
1987	1987	Happy Mammals with 128x128 Image Size	Happy Whales and Dolphins with Image Size of 128x128	['business', 'image data']	"Context
Additional data (resized images) for the competition Happywhale - Whale and Dolphin Identification
Content
Train and test images were resized (using cv2) to 128x128.
Notebook for resizing images: https://www.kaggle.com/gpreda/resize-images-of-happy-whales-and-dolphins
Inspiration
Use this data to accelerate your model pipeline for the competition."	5	79	3	gpreda	happy-mammals-with-128x128-image-size
1988	1988	Github_topics_dataset	Top 30 Topics of github - https://github.com/topics	['software']		0	5	0	jokervb	github-topics-dataset
1989	1989	augmented_dataset		[]		0	7	0	vitomanuguerra	augmented-dataset
1990	1990	umpfeature		[]		0	8	0	mbonyani	umpfeature
1991	1991	Pizza Price Data	Pizza Price Data v.1.0	['intermediate', 'exploratory data analysis', 'statistical analysis', 'tabular data', 'restaurants']	"Context
I created 4 pizza prices list data from various companies, such as Pizza Hut, Domino's Pizza, etc. And my purpose for making this data is only for learning purposes, and I don't use it for my personal interests, here's the description for every company:
Desc
Domino’s Pizza Menu Prices
If Domino’s doesn’t ring a bell, then you must have been living in Mongolia, as this is one of the only places this chain doesn’t have a restaurant. With over 10,000 stores in 73 countries, Domino’s have conquered the global pizza market. Not to mention being the second-largest pizza chain in the USA.
From their humble beginnings as a traditional pizzeria, they have continuously expanded their menu and at current locations, you can find a huge variety of items. Worldwide you’ll find first and foremost a vast range of pizzas; but also chicken wings, pasta, garlic bread, cheesy bread, potato wedges, a range of soft drinks and a divine selection of delicious desserts. All baked in-house and to your personal preferences.
Pizza Hut Menu Prices
Pizza Hut is a company founded in Wichita Kansas. The franchise was started by two brothers in the 1950’s and grew from there. There are several options for customers to choose. The. There are salads, pizzas, pasta, and various other food items. The food’s taste is generally great and has good quality.
The company employs over 160,000 people worldwide. Pizza Hut is a wholly-owned subsidiary owned by Yum! Brands. Currently t the restaurant is headquartered in Plano Texas. The franchise’s slogan is Flavor of Now. This slogan shows of the brand want to improve itself int into the future. The franchise has found consistent success and growth throughout the years.
Godfather’s Pizza Menu Prices
This fast-casual Italian chain has been a long time favourite across America, at one point ranking the third-largest pizza chain in the country. While they no longer hold the title of one of the largest, they remain one of the best. Their slogan is simple and straight to the point “We serve good pizza”.
Godfather’s Pizza has always been committed to providing a better tasting, all-around superior product and this is exactly what they provide. They make their signature dish the way it’s supposed to be made, with four different types of delicious crust, 100% real cheese and incredible toppings.
IMO’s Pizza Menu Prices
This chain has been around for many years, filling the homes of their customers with consistently improving delights.  Their thin crust is something their particularly famous for, cooked to perfection and cut into delightful bitesize squares, their slogan “the square beyond compare” definitely holds up to its high expectations.
Imo’s Pizza is also famous for the incredible St. Louis style pizza that is made with amazing Provel cheese. The company are dedicated to ensuring their customers are fully satisfied with every aspect of the business. So, whether you stop in and collect or get it delivered straight to your home, try one of their legendary dishes today.
Content
Independent Variables
Company: The company that made the pizza.
Pizza Name: The name of the pizza.
Type: The type of pizza.
Size: The size of the pizza.
Dependant Variable
Price: The price of the pizza.
Task
Q1: What is the most expensive pizza in each company?
Q2: Which company has more pizzas on the menu?
Q3: What is the average ($\mu$) price of pizza in each company?
Q4: Suppose the Pizza Hut company owns $P = {X | X \ unique \ price \ value}$ and Domnino's Pizza owns $D = {X | X \ unique \ price \ value}$ what is $P \cap D$?
Q5: For example, the average success probability (success makes customers happy) in the service of each existing company is $87\%$
What is the probability for any company to succeed in making customers happy?
What is the probability for each company to fail to make customers happy?
What is the probability of at least one of the four companies succeeding in making customers happy?
Q6: In a city, the city has $4$ pizza selling companies, namely Domino's Pizza, Pizza Hut, Godfather's Pizza, and IMO's Pizza, Employees want to buy pizza to accompany their lunch break, unfortunately these employees can only choose to buy from $3$ companies only, How many choices can the five employees choose?
Disclaimer
All attempts are made to provide up to date pricing information. However, prices and menu offerings can vary by location and time of the day. Hence please consider prices to be estimated prices.  RealMenuPrices.com is an independent site and is not associated with or are affiliate of any restaurants food chains or entity listed on the site.
Data Source
https://realmenuprices.com/dominos-pizza-menu-prices/"	184	1153	16	knightbearr	pizza-price-prediction-real-data
1992	1992	umpwattnet		[]		0	1	0	hadisahmadian	umpwattnet
1993	1993	School Absence Pivot		['education']		0	14	0	luckypen	school-absence-pivot
1994	1994	jigsaw-deberta-v3-base-train-model-3		['games']		0	6	0	handudu	jigsawdebertav3basetrainmodel3
1995	1995	subm2_roberta_large_ruddit		[]		1	6	0	alexander1980	subm2-roberta-large-ruddit
1996	1996	subm2_model_toxic_comments_unitary_rob_att		[]		1	5	0	alexander1980	subm2-model-toxic-comments-unitary-rob-att
1997	1997	subm2_model_toxic_comments_unary_toxic_rober		[]		2	6	0	alexander1980	subm2-model-toxic-comments-unary-toxic-rober
1998	1998	output_BERT-jigsaw-severetoxic		[]		13	53	0	li874039270	output-bertjigsawseveretoxic
1999	1999	subm2_model_ruddit_unary_toxic		[]		1	5	0	alexander1980	subm2-model-ruddit-unary-toxic
2000	2000	GitHub Repositories	Top 30 GitHub Repositories from each topic	['education', 'science and technology', 'computer science', 'text data', 'online communities']	"📌 Context
This Dataset contains the data of top GitHub repositories. this contains all of the domains that GitHub have covered on their website and extracted 30 top repositories from each of the topic listed. This is my first dataset that I have scraped and have collected some general information for the repositories i.e. Username, Repository name, Stars and URL of the Repository.
⚡ Acknowledgements
The Data was collected from the well known GitHub website, and as my first web scraping project. ""Jovian"" YouTube videos on the similar topic helped me a lot."	1	10	1	siddhesh08	github-repositories
2001	2001	ENEM Categories Colums		[]		0	3	0	caneiro	enem-categories-colums
2002	2002	Distance calibration measurement in Toy Train		['electronics', 'beginner', 'intermediate', 'tabular data']	"Context
The data is taken from the ultrasonic sensor using raspberry pi. 
Content
Toy Train model is set and at one corner ultrasonic sensor is placed. Whenever the train comes in range with the ultrasonic sensor it collects the data i.e the distance at which the contact is made and a LED glows. When the contact is made a value of 1 is passed. If the contact is not made, the LED does not glows and the value is 0.
The data is taken from firebase directly 
Acknowledgements
The experiment is done for about 15 minutes. 
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	8	1	rohithmahadevan	distance-calibration-measurement-in-toy-train
2003	2003	test_img		[]		0	4	0	timxinxinpeng	test-img
2004	2004	reef-yolox-m-aug-v6		[]		0	0	0	sangayb	reef-yolox-m-aug-v6
2005	2005	bioinfokit	Bioinformatics data analysis and visualization toolkit	['business', 'computer science', 'data visualization', 'statistical analysis', 'pca']	"bioinfokit
The 'bioinfokit' package for Kaggle. For more details, see the author's GitHub (bioinfokit package).
Thanks to reneshbedre for this great package!"	10	400	3	maksymshkliarevskyi	bioinfokit
2006	2006	YouTubers saying things	Dataset containing popular Youtuber's video subtitles from different categories.	['popular culture', 'arts and entertainment', 'internet', 'nlp', 'text data']	"Intro
Founded and maintained since 2005, YouTube is one of the internet's biggest platforms. With their number of videos watched per day exceeding 1 Billion, it's easy for any user to differentiate genres just by glancing at the thumbnail and the Title. My inspiration to make this dataset was to try and answer the question of whether it is equally easy for a computer to do.
The Transcript column in the dataset contains the subtitles for the respective videos. However, the reliability of the subtitles may vary. Even though the auto-generated subtitles work great (most of the time). Sometimes under heavy pressure of thick accents, it lets go of the ball. Please consult the CC attribute to check whether the subtitle is auto-generated or not. 1381 of these video subtitles are auto-generated, the rest of the 1134 are manual ones.
Since the values of Subscribers and Views are based on the time when the dataset was generated. That's to be taken into account. The most recent version of this dataset was generated on 05-Feb-2022.
Description
This dataset contains subtitles from over 91 different YouTubers, ranging from all different kinds of categories. The data were collected and cleaned (as much as necessary) by me. Currently, the dataset contains 2515 unique videos and their subtitles. There are 11 columns in the dataset. You can find their purpose in the column descriptors.
Improvements
I am open to suggestions please feel free to let me know of any major Categories or Channels that I've missed or you'll like to be included. I'll try my best to include them in the dataset. Find the dataset page on my Github."	30	345	8	praneshmukhopadhyay	youtubers-saying-things
2007	2007	Covid-19 Dataset till 2022		[]		2	16	0	prayasabhinav	covid19-dataset-till-2022
2008	2008	Comvis Dataset 2		[]		0	3	0	edgardjonathan	comvis-dataset-2
2009	2009	Cyclistic-Bike-Rides-Data		[]		0	3	0	lasetawak	cyclisticbikeridesdata
2010	2010	context_toxicity-master		[]	Including data are downloaded from https://github.com/ipavlopoulos/context_toxicity	0	6	0	naoism	context-toxicitymaster
2011	2011	comments_to_score_LB		[]		0	1	0	hasana1	comments-to-score-lb
2012	2012	QS World University Rankings 2017 - 2022	Discover and explore the world's leading academic institutions	['universities and colleges', 'categorical data', 'beginner', 'exploratory data analysis', 'data visualization']	"Context
QS World University Rankings is an annual publication of global university rankings by Quacquarelli Symonds. The QS ranking receives approval from the International Ranking Expert Group (IREG), and is viewed as one of the three most-widely read university rankings in the world. QS publishes its university rankings in partnership with Elsevier. 
Content
This dataset contains university data from the year 2017 to 2022. It has a total of 15 features.
- university - name of the university
- year - year of ranking
- rank_display - rank given to the university
- score - score of the university based on the six key metrics mentioned above
- link - link to the university profile page on QS website
- country - country in which the university is located
- city - city in which the university is located
- region - continent in which the university is located
- logo - link to the logo of the university
- type - type of university (public or private)
- research_output - quality of research at the university
- student_faculty_ratio - number of students assigned to per faculty
- international_students - number of international students enrolled at the university
- size - size of the university in terms of area
- faculty_count - number of faculty or academic staff at the university
Acknowledgements
This dataset was acquired by scraping the QS World University Rankings website with Python and Selenium.
Cover Image: Source 
Inspiration
Some of the questions that can be answered with this dataset,
1. What makes a best ranked university?
2. Does the location of a university play a role in its ranking?
3. What do the best universities have in common?
4. How important is academic research for a university?
5. Which country is preferred by international students?"	20	129	2	padhmam	qs-world-university-rankings-2017-2022
2013	2013	Police Killings US		['united states', 'crime', 'law', 'data analytics', 'tabular data', 'public safety']	"""In 2015, The Washington Post began to log every fatal shooting by an on-duty police officer in the United States. In that time there have been more than 5,000 such shootings recorded by The Post. After Michael Brown, an unarmed Black man, was killed in 2014 by police in Ferguson, Mo., a Post investigation found that the FBI undercounted fatal police shootings by more than half. This is because reporting by police departments is voluntary and many departments fail to do so. The Washington Post’s data relies primarily on news accounts, social media postings, and police reports. Analysis of more than five years of data reveals that the number and circumstances of fatal shootings and the overall demographics of the victims have remained relatively constant..."" SOURCE ==&gt; Washington Post Article
For more information about this story
This dataset has been prepared by The Washington Post (they keep updating it on runtime) with every fatal shooting in the United States by a police officer in the line of duty since Jan. 1, 2015.
2016 PoliceKillingUS DATASET<br>
2017 PoliceKillingUS DATASET<br>
2018 PoliceKillingUS DATASET<br>
2019 PoliceKillingUS DATASET<br>
2020 PoliceKillingUS DATASET<br>
Features at the Dataset:
The file fatal-police-shootings-data.csv contains data about each fatal shooting in CSV format. The file can be downloaded at this URL. Each row has the following variables:
id: a unique identifier for each victim
name: the name of the victim
date: the date of the fatal shooting in YYYY-MM-DD format
manner_of_death: shot, shot and Tasered
armed: indicates that the victim was armed with some sort of implement that a police officer believed could inflict harm
undetermined: it is not known whether or not the victim had a weapon
unknown: the victim was armed, but it is not known what the object was
unarmed: the victim was not armed
age: the age of the victim
gender: the gender of the victim. The Post identifies victims by the gender they identify with if reports indicate that it differs from their biological sex.
M: Male
F: Female
None: unknown
race:
W: White, non-Hispanic
B: Black, non-Hispanic
A: Asian
N: Native American
H: Hispanic
O: Other
None: unknown
city: the municipality where the fatal shooting took place. Note that in some cases this field may contain a county name if a more specific municipality is unavailable or unknown.
state: two-letter postal code abbreviation
signs of mental illness: News reports have indicated the victim had a history of mental health issues, expressed suicidal intentions or was experiencing mental distress at the time of the shooting.
threat_level: The threat_level column was used to flag incidents for the story by Amy Brittain in October 2015. http://www.washingtonpost.com/sf/investigative/2015/10/24/on-duty-under-fire/ As described in the story, the general criteria for the attack label was that there was the most direct and immediate threat to life. That would include incidents where officers or others were shot at, threatened with a gun, attacked with other weapons or physical force, etc. The attack category is meant to flag the highest level of threat. The other and undetermined categories represent all remaining cases. Other includes many incidents where officers or others faced significant threats.
flee: News reports have indicated the victim was moving away from officers
Foot
Car
Not fleeing
The threat column and the fleeing column are not necessarily related. For example, there is an incident in which the suspect is fleeing and at the same time turns to fire at gun at the officer. Also, attacks represent a status immediately before fatal shots by police while fleeing could begin slightly earlier and involve a chase.
- body_camera: News reports have indicated an officer was wearing a body camera and it may have recorded some portion of the incident.
SOURCE"	12	100	15	azizozmen	police-killings-us
2014	2014	EC Europa Grants	Scraped grants from the EU's eletronic system of funding opportunities	['europe', 'government', 'intermediate', 'nlp', 'text data']	"Context
This dataset was created in part to automate the task of selecting viable funding opportunities at my employer's company MSDK-Research
Content
The dataset has two files:
1. ec_europa_data.csv
&gt; This dataset contains around 5000 grants scraped from the EU's eletronic system of funding opportunities. Data was scraped using a selenium headless chrome.   
ec_grouped_data.csv 
&gt; This dataset contains all the grants' descriptions grouped by their respective calls. Furthermore, TF-IDF was used to then extract relevant keywords from each call.
Inspiration
It would be interesting to see informaiton extraction techniques applied to each individual grant, in order to determine how many funding opportunities are there, in that way we can tangibly calculate how much the European Comission  cares about the development of certain fields such as human rights, renewable energies, etc."	22	302	14	caiofleury	ec-europa-grants
2015	2015	MyMusicPre		[]		0	1	0	huangzyi	mymusicpre
2016	2016	pretrained-models		[]		0	20	0	simonmeoni	pretrainedmodels
2017	2017	jigsaw22models_infer		[]		4	9	0	alturutin	jigsaw22models-infer
2018	2018	AmbitionBox Companies Details		['business']		0	1	0	prayasabhinav	ambitionbox-companies-details
2019	2019	Digital Terrain and Surface Models of São Paulo	50cm resolution models from LiDAR 3D survey in 2017	['art', 'cities and urban areas', 'brazil', 'geography', 'geology', 'geospatial analysis']		2	75	6	andasampa	dtm-dsm-sao-paulo
2020	2020	Top 100 TV Shows	Information of top 100 tv shows	['arts and entertainment', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	This data contains the information of the top 100 TV Shows of all time.	36	356	11	sanjeetsinghnaik	top-100-tv-shows
2021	2021	HINDUNILVR.NS-2022-02-06		[]		0	1	0	l0new0lf	hindunilvrns20220206
2022	2022	TMDB Top Rated Movies Of All Time		['movies and tv shows']		0	6	0	prayasabhinav	tmdb-top-rated-movies-of-all-time
2023	2023	ToyDVD		[]		1	4	0	darthvader4067	toydvd
2024	2024	Titanic Passanger Survival Analysis		['movies and tv shows']		1	5	0	bhupendramishra7	titanic-passanger-survival-analysis
2025	2025	penzexSet	penzex dfsdsdffsdfsdfsdfsd	['text data']	pen- hakai	2	14	0	hakai1	penzex
2026	2026	ga11111		[]		2	16	1	gadafadagnachew	ga11111
2027	2027	Customer Information Dataset		['business']		5	43	12	syedhaideralizaidi	customer-information-dataset
2028	2028	Features		[]		0	6	0	alicescande	features
2029	2029	ECO764		[]		0	3	0	deepakadarsh	eco764
2030	2030	Univariate Time Series	Practice Univariate Time Series	['time series analysis']	"One can use this dataset to learn and implement Univariate Time Series
Aim:
1. To understand how to read, plot and work with time series data
2. Basic Manipulation of time series data
3. Many more...
Data Dictionary:
dates =&gt; date when observation was taken
count =&gt; number on given date
Aim:
Use this data and forecast count for next say-3 or 6 or 7 months"	4	53	2	mukeshmanral	univariate-time-series
2031	2031	FFHQ 256x256		[]		5	19	1	rahulbhalley	ffhq-256x256
2032	2032	outputadata		[]		0	2	0	adeshtrivedi	outputadata
2033	2033	condenser		[]		0	4	0	truonghoang	condenser
2034	2034	sentence t5 11b		[]		0	1	0	decoflight	sentence-t5-11b
2035	2035	Steel fault	Classification of Type of Fault present in Steel Plates	['earth and nature', 'education', 'manufacturing', 'logistic regression', 'naive bayes', 'tabular data', 'multilabel classification']	"Context
Applying machine learning for Steel Plates facture Detection
Content
The dataset contains 27 variables and 7 corresponding fault types generated for a Steel Plate
Acknowledgements
We wouldn't be here without the help of others.
Inspiration
Applying machine learning for traditional tasks is good? Isn't it ?"	2	31	0	sourabhsahoo10	steel-fault
2036	2036	Computer Vision 1		[]		0	3	0	edgardjonathan	computer-vision-1
2037	2037	Software Development Methods		['computer science', 'programming']		2	21	0	mostafizmim	software-development-methods
2038	2038	mymusicalprefrences		[]		0	0	0	ws287eduspbsturu	mymusicalprefrences
2039	2039	MusicPreferences		[]		0	6	0	ws287eduspbsturu	musicpreferences
2040	2040	DLAI-3-Phase3		[]		0	2	0	nehaadawadkar	dlai3phase31
2041	2041	ubiquant models		['clothing and accessories']		1	10	0	ryuhwankam	ubiquant-models
2042	2042	happywhale-tfrecords-5743-v1		[]		0	8	0	tomato0813	happywhale-tfrecords-5743-v1
2043	2043	soda bottle		[]		0	16	0	sebastianlinjiang	soda-bottle
2044	2044	AutoXTrial233		[]		1	8	0	chuxincheng	autoxtrial233
2045	2045	jigsaw-202201		['puzzles']		0	47	0	thajime	jigsaw-202201
2046	2046	Tabla taala dataset 	A 10 class dataset for Tabla Claassification!	['music']	"Context
The work done by people around the world from the Music Information Retrieval(MIR) community revolves around western music, their genres and their instruments, but no significant work has been carried around Indian Classical Music and specifically Tabla.
Content
Audio files are separated per class into respective folders. The classes are the frequent taalas(patterns) that are played in Indian Classical music.
Inspiration
There were hardly any datasets for Tabla taala classification."	41	737	4	pranav6670	tabla-taala-dataset
2047	2047	LibriSpeech 500 hours	Large-scale corpus of read English speech	['audio data']		0	15	1	tuannguyenvananh	librispeech-500-hours
2048	2048	happy-whale-tfrecords		[]		0	4	0	uplus26e7	happy-whale-tfrecords
2049	2049	Chest xray		[]		1	2	0	roshanichavan	chest-xray
2050	2050	Hungary 2021 F1 Prediction Test		['auto racing']		1	9	0	joshuaemslie	hungary-2021-f1-prediction-test
2051	2051	The-massive-Indian-Food-Dataset	A dataset containing Multi Categorical Images of Indian delicacies	['arts and entertainment', 'categorical data', 'classification', 'gan', 'image data', 'food']	"Content
Multi Categorical Images can good starter for CV Projects as this eventually will turn out to be a Massive Dataset containing Folders of a lot of Indian delicacies. All the images are resized to (300,300) to maintain size uniformity.
Inspiration
You don't need any inspiration for some good food Images 😋.
Frequency of Update
I will add new Dishes every Week."	124	2156	31	anshulmehtakaggl	themassiveindianfooddataset
2052	2052	TPS022022 Reference FBC Spectra		['genetics', 'biology', 'medicine', 'internet', 'beginner', 'tabular data']	"Content
Fractional base content (FBC) spectrum for the following reference genomes, organized into 2 dataframes:
train_ref_fbc_spec.csv:
* Klebsiella pneumoniae (NC_012731.1)
* Enterococcus hirae (LR134297.1)
* Campylobacter jejuni (NC_002163.1)
* Streptococcus pneumonia (NC_003028.3)
* Salmonella enterica (NC_003197.2)
* Staphylococcus aureus (NC_007622.1)
* Escherichia coli (U00096.3)
* Streptococcus pyogenes (NC_002737.1)
* Escherichia fergusonii (NC_011740.1)
* Bacteroides fragilis (NC_006347.1 )
test_ref_fbc_spec.csv:
* Streptococcus pyogenes MGAS315 (AE014074.1)
* Salmonella enterica Newport (CP006631.1)
* Staphylococcus aureus MRSA252 (BX571856.1)
* Enterococcus hirae ATCC9790 (NC_018081.1)
* Campylobacter jejuni RM1221 (NC_003912.7)
* Bacteroides fragilis strainBOB25 (CP011073.1)
* Escherichia fergusonii EFCF056 (NZ_CP040805.1)
* Escherichia coli O157 (AE005174.2)
* Streptococcus pneumoniae JJA (CP000919.1)
* Klebsiella pneumoniae Kp52145 (FO834906.1)
* Enterobacter aerogenes (NC_015663.1)
* Mycobacterium tuberculosis (AP018035.1)"	0	20	2	siukeitin	tps022022-reference-fbc-spectra
2053	2053	project		[]		3	11	0	yoboys	project
2054	2054	tensorflow text 2.6.0		['computer science']		0	5	0	decoflight	tensorflow-text-260
2055	2055	covidmine		[]		0	4	0	neerajcheryala	covidmine
2056	2056	number of arrests for drug abuse violations 	monthly number of arrests for drug abuse violations 	['united states', 'crime', 'law', 'advanced', 'tabular data']	"Context
This dataset contains monthly number of arrests for drug abuse violations reported by participating law enforcement agencies. 
Content
The arrests are by offense and broken down by age and sex or age and race. Not all agencies report race and/or ethnicity for arrests but they must report age and sex. Note that only agencies that have reported arrests for 12 months of the year are represented in the annual counts that are included in the database.  Totals of reported arrests for drug abuse violation in the nation from 1995–2016."	41	238	9	andrej0marinchenko	number-of-arrests-for-drug-abuse-violations
2057	2057	YT Trend WorldWide		[]		0	9	0	jeevaananth	yt-trend-worldwide
2058	2058	Exp-030-toxic-xlm-roberta-Pseudo-Jigsaw1		[]		0	7	0	mst8823	exp-030-toxic-xlm-roberta-pseudo-jigsaw1
2059	2059	tf od centernet		[]		1	18	0	outwrest	tf-od-centernet
2060	2060	Plant Detection		['business']		31	546	0	shreya4763	plant-detection
2061	2061	uts_traffic		[]		0	9	0	salmanikrima	uts-traffic
2062	2062	bertendtoend		[]		0	6	0	toongzhhang	bertendtoend
2063	2063	Table Tennis Games Dataset TTNet	Dataset for Real-time temporal and spatial video analysis of table tennis	['sports', 'computer vision', 'deep learning', 'image data', 'cv2']	"Computer Vision tasks in games for event detection, Iou Segmentation, Ball detection are becoming more and more popular.
Content
There are 5  training games and a couple of test data images. Apart from that,###
Computer Vision tasks in games for event detection, Iou Segmentation, Ball detection are becoming more and more popular.
Content
There are 5  training games and a couple of test data images. Apart from that, there are two markup JSON Files namely event_markup and ball_markup that represent the event happening and the x,y co-ordinate of the ball respectively.
Acknowledgements
Credit for this Awesome Dataset: https://lab.osai.ai/
Source Video: Video
Inspiration
Article : https://arxiv.org/pdf/2004.09927.pdf
Roman Voeikov, Nikolay Falaleev, and Ruslan Baikulov TTNet: Real-time temporal and spatial video analysis of table tennis. The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020, pp. 884-885 [Publication][Bibtex]
Some TTNet Implementations
Implementation"	0	66	8	anshulmehtakaggl	table-tennis-games-dataset-ttnet
2064	2064	datakanker		[]		0	0	0	annisaagmelia	datakanker
2065	2065	d2chars		[]		0	17	0	daehoyang	d2chars
2066	2066	tryyyyyyy		[]		0	1	0	cpx1971	tryyyyyyy
2067	2067	brain-models		['biology']		1	138	0	manoj312002	brainmodels
2068	2068	Airline Data.csv		[]		1	17	0	wenxiwu1109	airline-datacsv
2069	2069	Marketing Campaign for clustering		[]		4	32	0	mukeshkumar95	marketing-campaign-for-clustering
2070	2070	Le Morte D'Arthur Tokenized		[]		1	3	1	gabrielferrer	le-morte-darthur-tokenized
2071	2071	twitter		['email and messaging']		0	7	1	muhammadabdulhafizh	twitter
2072	2072	Rosemary/Acetone Extract fractions COSI Data 		['earth and nature']		1	4	1	patrickmcintyre	rosemaryacetone-extract-fractions-cosi-data
2073	2073	testingdata		[]		0	2	0	abhishekjha1994	testingdata
2074	2074	model_original_best_score		[]		0	14	0	yisanying	model-original-best-score
2075	2075	Ubiquant_model		[]		0	15	0	tensorchoko	ubiquant-model
2076	2076	Platinum Resistant Ovarian Cancer Clinical Trials	Analyze Clinical Trials data for PROC	['cancer']	"Context
This dataset is from the clinicaltrails.gov website, specifically searched for platinum-resistant ovarian cancer in January 2022. 
dataset source: 
https://clinicaltrials.gov/ct2/results?cond=Platinum-resistant+Ovarian+Cancer&term=&cntry=&state=&city=&dist=
Ovarian cancer Wikipedia:
https://en.wikipedia.org/wiki/Ovarian_cancer
Ovarian cancer (OC) is the seventh most common cancer among women with an approximately 70 percent recurrence rate. Despite the high recurrence probability, most diagnoses happen in an advanced stage because of lack of symptoms and preventative measures. 
The initial and most effective treatment against OC, followed by the surgery, remains to be platinum-based chemotherapy typically given in 6 rounds. However, in Platinum-Resistant Ovarian Cancer (PROC) where relapse happens within 6 months after the last round of chemotherapy treatment options become limited to non-platinum-based agents with a low treatment response rate. Active research is therefore in a critical need for PROC and OC 
patients as the most relapsed patients, which are then considered incurable, eventually also develop resistance against platinum-based treatments.
PROC patients are strongly recommended to participate in a clinical trial. This dataset contains past and ongoing clinical trials for PROC. The goal is to identify intervention trends, as well as trial characteristics. 
Clinical Trials require to undergo 3 phases. Trials meeting the superior outcomes will get FDA approval for the market.
Please refer to the following link for clinical trial phase designs details:
https://en.wikipedia.org/wiki/Phases_of_clinical_research#Phase_0
Content
Rank - row numbers
NCT Number - Clinical Trial identification number. To see the trail details simply google this id.
Title - Trial title
Acronym - Trial acronym. Some trials are missing in this column despite having one. They are usually written along with the title instead.
Status - Trial status.
Study Results - indicates whether the trial posted the result. Some trials might not have posted the trial but links the related papers.
Conditions - Medical conditions for a given trial. Might be more than one.
Interventions - Type of interventions used. Typically drugs but could also contain Devices, Radiation, etc.
Outcome Measures -  Trial outcome goals. 
Sponsor/Collaborators - Trial Sponsors and/or Collaborators
Gender - Subject gender requirement
Age - Subject age requirement
Phases - Current Trial Phase.
Enrollment - Number of participating patients
Funded Bys - Funding agencies
Study Type - Type of experiment
Study Designs - Experiment design details
Other IDs - Other relative IDs
Start Date - Trial Start Date
Primary Completion Date -  the date of the last study visit where data was collected for the primary study outcome(s)
Completion Date -  the date of the last study visit where data was collected for any of the study outcomes.
First Posted -  First posted date
Results First Posted - Results posted date
Last Update Posted - Last site update date
Locations - Participating Locations
Study Documents - Relative documents
URL -  Trial URL
Acknowledgements
The world is in forever debt for the PROC patients and family f for contributing to the research."	0	41	0	hansoullee	platinum-resistant-ovarian-cancer-clinical-trials
2077	2077	ensemble_boxes		[]		0	3	0	tattaka	ensemble-boxes
2078	2078	Happywhale train/val/test/ to TFRecords 128x128		['animals', 'transportation', 'image data', 'tensorflow']	"Still Work in Progress!
Context
Images from Happywhale - Whale and Dolphin Identification in TFRecords format.
The images were resized to 128x128.
Already splitted in train/val/test datasets.  
The filenames has the following pattern:
{dataset}-file_{shard_number}-{number_of_images}.tfrec
Acknowledgements
https://keras.io/examples/keras_recipes/creating_tfrecords/#define-tfrecords-helper-functions"	2	32	3	igorkf	happywhale-tfrecords-128-128
2079	2079	UUTR_NO_CLEAN_MUL_TRY2		[]		0	3	0	hangy132	uutr-no-clean-mul-try2
2080	2080	cyclistic-tripdata		[]		0	7	0	agostinagranja	cyclistictripdata
2081	2081	happywhale-tfrecords-5743-v0		[]		0	5	0	tomato0813	happywhale-tfrecords-5743-v0
2082	2082	sartorius_configs		[]		0	17	0	itaynivtau	sartorius-configs
2083	2083	soft_teacher_95_5_pretrained_on_livecell_img_scale		[]		0	1	0	itaynivtau	soft-teacher-95-5-pretrained-on-livecell-img-scale
2084	2084	precomp_dataset		[]		0	1	0	larry2000	precomp-dataset
2085	2085	Fin. multiples of Russian companies (MOEX)	Financial multiples and its stocks prices of Russian companies (MOEX)	['russia', 'tabular data', 'investing']	This dataset is a segment of the financial market, which is represented by multiples of Russian companies traded on the Moscow Stock Exchange. Multiples are collected and calculated based on the financial statements of companies for the period from 2014 to 2021 (it is also possible to split into quartiles in the last 2 years). Stocks are obtained by averaging its price for the period under review. This may be useful for fundamental analysis of the Russian stock market. One of the goals of this study is to build a model to predict the change in the price of a stock based on changes in the company's multiples. I would be interested to see your ideas for solving this problem.	5	39	2	redeugene	rus-stocks-funds
2086	2086	Gossypium		[]		0	1	0	harshiniaiyyer	gossypium
2087	2087	School Closures and Absences - Ontario Canada 2022	To Be Updated Bi-Monthly	['education', 'primary and secondary schools', 'covid19']	"Context
As we enter into the tail end of the Coronavirus pandemic, school closures are becoming an ever evolving issue. In this dataset, I thought to shed some light on the extend of school closures in Ontario, Canada.
Content The dataset consists of two csv files: schoolclosures.csv and schoolabsences2022
schoolclosures2022.csv: report of school closures
Column Description
schoolclosures2022.csv:
report_date: Date of school closure report
schools_closed: Number of schools closed
total_schools: Number of schools in Ontario
schoolabsences2022.csv:
date: date of report
school_board: Name of the School Board
school_id: School's ID Number
school: School Name
city_or_town: Municipality of School
absence_percentage_staff_students: Percentage of Staff and Student Absence
Acknowledgements Data provided for public use by Ontario Data Catalog.
Inspiration This dataset is meant to gain insight into the severity of the Covid pandemic through the angle of school functionalities."	2	28	1	luckypen	school-closure-ontario-canada-2022
2088	2088	AdAnalyse		[]		1	4	0	saicharansirangi	adanalyse
2089	2089	normal		['health']		2	32	0	esraaashry	normalandabnormal
2090	2090	jigsaw rud		[]		1	21	0	gonnbe	jigsaw-rud
2091	2091	Whales and Dolphins are Happy with Image Sizes	Add image sizes to the images in train and test	['arts and entertainment', 'tabular data']	"Context
Add image sizes to train and test data for the competition Happywhale - Whale and Dolphin Identification data.
Content
Image sizes added to train and test set created with images."	3	136	8	gpreda	whales-and-dolphins-are-happy-with-image-sizes
2092	2092	Climate change dataset: Drought Over Time 	300K samples measuring drought in the US along with other details	['atmospheric science']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>This dataset originated from the <a href=""https://droughtmonitor.unl.edu/DmData/DataDownload/ComprehensiveStatistics.aspx"" target=""_blank"">US Drought Level Monitor</a>.</p>
<h2><strong>Attribution</strong></h2>
<p>According to the US Drought Monitor, any usage of these data should be accompanied with the following wording:</p>
<blockquote>
<p>The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture, and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC.</p>
</blockquote>
<p>You'll find additional Terms of Service <a href=""https://drought.unl.edu/WebPolicy.aspx"" target=""_blank"">here</a>.</p>
<h2><strong>Data Wrangling</strong></h2>
<p>Data in the file <code>drought.csv</code> was cleaned and wrangled by R's TidyTuesday team. Find more details, including the code used to combine the data <a href=""https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-20"" target=""_blank"">on their GitHub</a>.</p>
<h2><strong>Limitations & Caveats</strong></h2>
<p>Additional limitations to this dataset and caveats regarding its use, can be found <a href=""https://droughtmonitor.unl.edu/About/AbouttheData/PopulationStatistics.aspx"" target=""_blank"">here</a>.</p>
This dataset was created by Amber Thomas and contains around 300000 samples along with Area Total, Map Date, technical information and other features such as:
- Stat Fmt
- State Abb
- and more.
How to use this dataset
&gt; - Analyze Pop Total in relation to Valid Start
- Study the influence of Area Pct on Pop Pct
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Amber Thomas 
Start A New Notebook!"	127	1125	15	yamqwe	us-drought-monitore
2093	2093	Jigsaw_cleaned_data_V6		[]		0	5	0	moniquebadjemaa	jigsaw-cleaned-data-v6
2094	2094	tfgbr-stage2-v5-yolov5l-128		[]		0	6	0	ks2019	tfgbr-stage2-v5-yolov5l-128
2095	2095	SCANPY Python package for scRNA-seq analysis	Tutorials and use cases for scanpy	['genetics', 'earth and nature', 'biology', 'biotechnology']	"https://scanpy.readthedocs.io/en/stable/
Scanpy – Single-Cell Analysis in Python
Scanpy is a scalable toolkit for analyzing single-cell gene expression data built jointly with anndata. It includes preprocessing, visualization, clustering, trajectory inference and differential expression testing. The Python-based implementation efficiently deals with datasets of more than one million cells.
Single cell RNA sequencing data - count matrices: rows - correspond to cells, columns to genes,
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
SCANPY is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with SCANPY, we present ANNDATA, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).
Paper: 
Wolf, F., Angerer, P. & Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Genome Biol 19, 15 (2018). https://doi.org/10.1186/s13059-017-1382-0
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-017-1382-0
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	0	16	4	alexandervc	scanpy-python-package-for-scrnaseq-analysis
2096	2096	airlinedatav2		[]		0	3	0	rikdifos	airlinedatav2
2097	2097	NYC_TLC_yellow_taxi_2018_clean_up		[]		0	14	0	yothinpukongnin	nyc-tlc-yellow-taxi-2018-clean-up
2098	2098	World Arrests (Pretend Data)	Make believe data taken from a sample of the USA arrests between 1995 and 2016	['categorical data', 'crime', 'government', 'beginner']	"Context
This data was manipulated and is not at all true to real world examples. The arrests_world table shows the crime rates for particular crimes in one country over the same 1 year span. This dataset was created for practice purposes only and should not be used to draw real life conclusions
Content
Rows display the city's given number; the table is sorted in descending of population and each column will display the sum and percentage of six different types of crimes.
Acknowledgements
Shout to ANDREJ MARINCHENKO for providing the raw data that was manipulated for this example.
Inspiration
This dataset is for my personal growth and practice. It should not be used to draw any real like conclusion.
***In this make believe world, the dataset displayed the populations of 22 different earthly countries in the year 2315. The goal in my query is to find out how organized crime percentages differ from other crime. In turn allowing us to see where we should focus our effort and resources to stop crime altogether."	1	11	0	samanthachades	arrests-world
2099	2099	Animals 151	Dataset of 151 Animal species for image classification	['biology', 'animals', 'intermediate', 'computer vision', 'classification', 'image data']	"Animals 151 Dataset
This was a toy dataset I collected for me to test image classification models quickly. This dataset does not have much images per class. Each class has a maximum of 60 images, with some classes having only 30 images. I resized every image to 224x224 to save on space.
The challenge is to train a high accuracy model with this limited data. 
Check out some of the sample notebooks for inspiration. I have included a translation.json that translates scientific names into Common names, to make it easier for the you to understand the animals you are identifying.
I update this dataset approximately every month, but expect there to be long intervals of no updates, since this is just a hobby and not a full time project of mines.
Data Collection
&gt; I collected this data from Google Images with just minimal checking, so there will be some weird images that might confuse the model. ALL IMAGES BELONG TO THE ORIGINAL AUTHORS."	135	1501	12	sharansmenon	animals141
2100	2100	Jigsaw Regression Based Data	Regression Based Data for Jigsaw Rate Severity and FastText Word Embeddings	['computer science', 'nlp', 'deep learning', 'text data', 'regression']	"Data Files
This dataset contains 2 Files and 2 Folders
File 1 : train_data_version1.csv
File2 : train_data_version2.csv
File3 : train_data_version3.csv
Folder 1 : FastText-Jigsaw-100D
Folder 2 : FastText-Jigsaw-256D
Content
File 1 : This File is in CSV format contains two columns
Column 1 contains text data. This text data is preprocessed and balanced, balanced in the sense this data contains an equal number of non-toxic (with toxicity = 0) and toxic (with toxicity &gt;= 0) comments.
Column 2 contains float data. This column stores information about the toxicity of text data.
File 2 : This File is in CSV format contains two columns
Column 1 contains text data. In this version of the file, we did implement some more pre-processing techniques like spelling corrections. Also, this dataset is balanced means this data contains an equal number of non-toxic (with toxicity = 0) and toxic (with toxicity &gt;= 0) comments.
Column 2 contains float data. This column stores information about the toxicity of text data.
File 3 : This File is in CSV format contains two columns
Column 1 contains text data. In this version of the file, we did implement some more pre-processing techniques like spelling corrections. Also, this dataset is balanced means this data contains an equal number of non-toxic (with toxicity = 0) and toxic (with toxicity &gt;= 0) comments.
Column 2 contains float data. This column stores information about the toxicity of text data.
Folder 1 : This folder contains 2 files of 100D fasttext word embeddings.
Jigsaw-Fasttext-Word-Embeddings.bin: This file is a binary file that will be used to load the fasttext embeddings for use. 
Jigsaw-Fasttext-Word-Embeddings.bin.wv.vectors_ngrams.npy: This file contains word vectors.
Folder 2 : This folder contains 4 files of 256D fasttext word embeddings.
Jigsaw-Fasttext-Word-Embeddings-256D.bin: This file is a binary file that will be used to load the fasttext embeddings for use. 
Jigsaw-Fasttext-Word-Embeddings-256D.bin.syn1neg.npy: This file contains word vectors.
Jigsaw-Fasttext-Word-Embeddings-256D.bin.wv.vectors_ngrams.npy: This file contains word vectors.
Jigsaw-Fasttext-Word-Embeddings-256D.bin.wv.vectors_vocab.npy: This file contains word vectors.
All the FastText Word embeddings in this dataset were learned using python's gensim library with window size = 4 and sg = 0 implies Continuous bag of words (CBOW) approach to learn word embeddings
Continuous bag of words (CBOW)
In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word—jumps.
If you are interested then you can learn more about FastText from below attached resources:
Text-Representations
Word2Vec and FastText Word Embedding with Gensim
Text Classification & Word Representations using FastText (An NLP library by Facebook)"	897	6567	89	nkitgupta	jigsaw-regression-based-data
2101	2101	Annotated Anime Faces Dataset	Collection of Anime Images with annotated faces	['arts and entertainment', 'computer vision', 'classification', 'image data', 'anime and manga']	"Context
This dataset contains 6640 anime images crawled from pixiv top 100 daily rankings. The author of the dataset used nagadomi's face detector for anime/manga using OpenCV.
The original dataset was converted into YOLOv5 PyTorch TXT Annotation Format and split into 80/10/10 on train, test and valid using roboflow. 
Acknowledgements
Original dataset by qhgz2013
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	17	462	8	andy8744	annotated-anime-faces-dataset
2102	2102	iris flower data		[]		0	0	0	aashimaaa	iris-flower-data
2103	2103	iris flower-dataset		['biology']		2	9	0	aashimaaa	iris-flowerdataset
2104	2104	GB-YF-SPY	YF-SPY...............................	[]		0	2	0	ritikabhardwaj06	gb-yf-spy
2105	2105	Irisss dataset		['earth and nature']		0	1	0	aashimaaa	irisss-dataset
2106	2106	 Employee Attrition Dataset		[]		0	8	1	syedhaideralizaidi	employee-attrition-dataset
2107	2107	Iris-dataset 		[]		0	1	0	aashimaaa	irisdataset
2108	2108	COCO/WikiArt NST Dataset		['computer science']	"Dataset for Neural Style Transfer consisting of COCO2017 images         and Kaggle competition ""Painter by Numbers"" dataset.
The number of COCO images and style images is the same.
        Intended for use with NST using Adaptive Instance Normalization.
        All respective licenses for used datasets apply to corresponding parts of this dataset."	5	16	4	shaorrran	coco-wikiart-nst-dataset-512-100000
2109	2109	penzexSet	penzex dfsdsdffsdfsdfsdfsd	['text data']	pen- hakai	2	9	3	iamzed	iamzedset
2110	2110	Black Friday Dataset		['holidays and cultural events']		6	148	17	syedhaideralizaidi	black-friday-dataset
2111	2111	mmcv_TGBR		[]		0	4	0	vincentwang25	mmcv-tgbr
2112	2112	mmdetection		[]		0	17	0	vincentwang25	mmdetection
2113	2113	stacking_folds		[]		0	5	1	varunnagpalspyz	stacking-folds
2114	2114	segonly_sartorius_pretrain_on_livecell_ep14		[]		0	8	0	sagibenitzhak	segonly-sartorius-pretrain-on-livecell-ep14
2115	2115	Motorcycle Dataset - Indian Market	Motorcycle/ Scooter data with features, specs and ex-showroom prices	['automobiles and vehicles']	"Context
The Indian motorcycle market is the largest in the world. There are about 98 motorcycle manufactures in India with more than 250 models in the market.
Content
The data is in JSON format. Due to some import, the JSON structure is broken. 
Acknowledgements
BikeDekho"	13	91	6	anmolmittalll	motorcycle-dataset-indian-market
2116	2116	Bike Sharing Demand Dataset		['online communities']		1	15	2	syedhaideralizaidi	bike-sharing-demand-dataset
2117	2117	yolov5 models		['clothing and accessories']		0	26	3	prateekagnihotri	yolov5-models
2118	2118	Cuisine Dataset		['cooking and recipes']		0	8	0	mfarazf	cuisine-dataset
2119	2119	Bilstmmodel		[]		0	4	0	mbonyani	bilstmmodel
2120	2120	filmstop100		[]		1	2	0	cristinaluque	filmstop100
2121	2121	Covid19 Data		[]		2	5	0	mfarazf	covid19-data
2122	2122	effiecient_net_b2		[]		1	5	0	enkrish188	effiecient-net-b2
2123	2123	Ai-Academy-competition		[]		1	14	0	mikeyasnov	aiacademycompetition
2124	2124	Fake_News		[]		0	5	0	tithee	fake-news
2125	2125	Test Data		[]		0	0	0	erikaswang	test-data
2126	2126	top100moviescristi		[]		1	2	1	cristinaluque	top100moviescristi
2127	2127	Exp-029-toxic-xlm-roberta-Pseudo-Ruddit		[]		0	10	0	mst8823	exp-029-toxic-xlm-roberta-pseudo-ruddit
2128	2128	dontpatronizeme_pcl		[]		1	25	0	mohammadmakahleh	dontpatronizeme-pcl
2129	2129	Pedestrian and Car Dataset		['science and technology']		0	22	0	adityasinghz	pedestrian-and-car-dataset
2130	2130	lessmoretextnpz		[]		0	6	1	banbeipi	lessmoretextnpz
2131	2131	grow your channel	https://www.kaggle.com/ziedbenhamed/notebook7a2b0f62fe	[]	"Context
https://www.kaggle.com/ziedbenhamed/notebook7a2b0f62fe
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	9	0	ziedbenhamed	grow-your-channel
2132	2132	Pedestrian Detection Dataset	This is the pedestrian dataset for machine learning.	['science and technology']		1	10	1	adityasinghz	pedestrian-detection-dataset
2133	2133	COVID-19: Predicting 3rd wave in India	Daily & cumulative COVID cases & deaths in India from 03/01/2020 to 10/01/2022	['india', 'law', 'intermediate', 'time series analysis', 'lstm', 'covid19']	"Content
The WHO coronavirus (COVID-19) dashboard presents official daily counts of COVID-19 cases, deaths and vaccine utilization reported by countries, territories and areas. Through this dashboard, we aim to provide a frequently updated data visualization, data dissemination and data exploration resource, while linking users to other useful and informative resources.
Caution must be taken when interpreting all data presented, and differences between information products published by WHO, national public health authorities, and other sources using different inclusion criteria and different data cut-off times are to be expected. While steps are taken to ensure accuracy and reliability, all data are subject to continuous verification and change. All counts are subject to variations in case detection, definitions, laboratory testing, vaccination strategy, and reporting strategies. 
Acknowledgements
© World Health Organization 2020, All rights reserved.
WHO supports open access to the published output of its activities as a fundamental part of its mission and a public benefit to be encouraged wherever possible. Permission from WHO is not required for the use of the WHO coronavirus disease (COVID-19) dashboard material or data available for download. It is important to note that:
WHO publications cannot be used to promote or endorse products, services or any specific organization.
WHO logo cannot be used without written authorization from WHO.
WHO provides no warranty of any kind, either expressed or implied. In no event shall WHO be liable for damages arising from the use of WHO publications.
For further information, please visit WHO Copyright, Licencing and Permissions.
Citation: WHO COVID-19 Dashboard. Geneva: World Health Organization, 2020. Available online: https://covid19.who.int/
Inspiration
Daily cases start increasing suddenly just before the new year and there's a fear for the upcoming wave. Everybody starts to predict the peak cases in the 3rd wave and the date the peak will be reached. Assume you are in the 1st week of January 2022 and there's panic in the country, for the Omicron variant is said to be highly transmittable. Using your machine learning and deep learning skills, you have to create a model that predicts accurately the peak for the 3rd wave."	53	314	10	aayush7kumar	covid19-predicting-3rd-wave-in-india
2134	2134	lessmorepkl		[]		0	6	1	banbeipi	lessmorepkl
2135	2135	titanic		[]		1	1	0	nathanberhe	titanic
2136	2136	monthly number of reported arrests	number of reported arrests for various agencies	['united states', 'crime', 'advanced', 'tabular data']	"Context
This dataset contains the monthly number of reported arrests for various offenses reported by participating law enforcement agencies. The arrests are by offense and broken down by age and sex or age and race. Not all agencies report race and/or ethnicity for arrests but they must report age and sex. 
Content
Note that only agencies that have reported arrests for 12 months of the year are represented in the annual counts that are included in the database. Download this dataset to see totals of reported arrests for the nation from 1995–2016."	21	177	9	andrej0marinchenko	monthly-number-of-reported-arrests
2137	2137	npznpznpz		[]		0	9	1	banbeipi	npznpznpz
2138	2138	ljsloan		[]		0	7	0	philipxyc	ljsloan
2139	2139	ipl_matches		[]		0	0	0	alfiyarakshik	ipl-matches
2140	2140	phosphorus		[]		0	9	0	ezrakeshet	phosphorus
2141	2141	comments_clean		[]		0	3	0	banbeipi	comments-clean
2142	2142	7folds-deberta-4epoch-v1		[]		0	25	0	tonymarkchris	7folds-deberta4-epoch-v1
2143	2143	Pokemon Data	Pokemon Data with Pokedexnr, Types, Base Stats and Abilities	['video games', 'anime and manga']	"Pokemon Dataset
last updated on 28.01.22 19:40
I will update in the upcoming days for new pokemon in Pokemon Legends: Arceus.
I also will try to extend the information on the pokemon with stats and other data, but i wanted to upload this dataset, so you can make fresh analysis on type distribution."	109	998	9	anatolinischakow	pokemon-data
2144	2144	OpenSea Daily Polygon Transactions	This data represents the raw on-chain activity of the 1 tracked smart contract.	['environment', 'finance', 'economics', 'investing', 'currencies and foreign exchange']	"This all-time data represents the raw on-chain activity of the tracked smart contracts.
I am thankful that we could collect the data from the dapprader platform: https://dappradar.com/polygon/marketplaces/opensea
These are for 1 Polygon Smart Contract as mentioned in the above site."	25	229	6	ankanhore545	opensea-daily-polygon-transactions
2145	2145	huggingface-family		[]		1	21	0	shimishige	huggingface-family
2146	2146	data analysis		[]		0	4	0	naglaakamel	movie
2147	2147	7folds-xlnet-base-4epoch		[]		0	19	1	tonymarkchris	7folds-xlnet-base-4epoch
2148	2148	TPSFeb2022_ds_to_pickle_with_folds		[]		3	30	1	sytuannguyen	tpsfeb2022-ds-to-pickle-with-folds
2149	2149	tmdb_toprated_movies		[]		0	4	0	geekjks	tmdb-toprated-movies
2150	2150	whale2-cropped-dataset		[]		92	387	33	phalanx	whale2-cropped-dataset
2151	2151	U.S. Air Quality, PM2.5 Concentrations, 2016-2020	EPA collocated FRM and FEM PM2.5 Concentrations, including MET conditions	['united states', 'atmospheric science', 'environment', 'tabular data']	"Context
In the U.S. Environmental Protection Agency (EPA) National PM2.5 monitoring program, the use of Federal Equivalent Method (FEM) continuous monitors continues to increase in comparison to the Federal Reference Method (FRM) gravimetric monitors. At many monitoring locations, both FRM and FEM monitors are operated simultaneously. There exists a known bias in 24-hour ambient PM2.5 concentrations between these two methods. Monitoring agencies need to understand these biases, their causes, and determine if PM2.5 continuous monitoring is appropriate for their network.
Content
This dataset includes EPA daily PM2.5 concentration data from monitoring sites in which both an FRM and FEM are operational. The dataset spans all states between 2016-2020. Site-specific information and local meteorological conditions are also included. 
Monitoring and concentration data were extracted from the EPA’s Air Quality System (AQS) through an API (https://aqs.epa.gov/aqsweb/documents/data_api.html). The two critical datasets downloaded were 'Daily Summary' and 'Monitor' data. To determine a monitor's designated as either FRM or FEM, the EPA method code description dataset was downloaded from EPA’s website at: https://aqs.epa.gov/aqsweb/documents/codetables/methods_all.html.
Lastly, given a monitoring site’s latitude, longitude, elevation, and the sampling date, the local weather conditions (temperature, pressure, wind speed/direction, rain, etc.) were imported through the Meteostat Python library (https://dev.meteostat.net/python/).
Acknowledgements
U.S. EPA.
Inspiration
Two potential uses of this dataset include:
1. Analyzing ambient PM2.5 concentration trends, and
2. Investigate biases between FRM and FEM monitors, determine the contributing factors, and predict concentrations based on collocated monitor results and other factors."	13	107	2	brannonseay	us-air-quality-pm25-concentrations-20162020
2152	2152	2048_epoch10_batch1_best_weights		[]		0	0	0	aaquibsyed	2048-epoch10-batch1-best-weights
2153	2153	Urban Floods In India  	Raw data extracted using twitterscraper, needs preprocesssing	[]		2	23	1	gayatrisharma6	urban-floods-in-india
2154	2154	vowpalwabbit		[]		1	9	1	thomasmeiner	vowpalwabbit
2155	2155	TFRecords_202202_01		[]		0	0	0	miguelespinozac	tfrecords-202202-01
2156	2156	PANDAS_BASICS		[]		1	5	0	revanthkundina	pandas-basics
2157	2157	tgbr-train-rl-ub		[]		0	0	0	prateekagnihotri	tgbr-train-rl-ub
2158	2158	Shark Tank India Companies	This dataset contains pitches and investments by sharks in Shark Tank India	['arts and entertainment', 'movies and tv shows', 'business', 'investing']	"Context
Recently, I saw a dataset based on Shark Tank USA. This dataset inspired me to create one for India as well and since season 1 recently ended, I thought this was the perfect time to look at some insights based on the deals.
Content
This dataset contains the following information - 
1. episode - episode number
2. pitch_no - pitch number (unique)
3. company - company name
4. idea - company description
5. deal - final deal that was taken
6. ashneer - Did Ashneer invest?
7. namita - Did Ashneer invest?
8. anupam - Did Anupam invest?
9. vineeta - Did Vineeta invest?
10. aman - Did Aman invest?
11. peyush - Did - Did Peyush invest?
12. ghazal - Did Ghazal invest?
Acknowledgements
This data was scraped from Wikipedia."	142	930	16	devanshu125	shark-tank-india-companies
2159	2159	tx_dbrtv_base		[]		0	1	0	datafan07	tx-dbrtv-base
2160	2160	Maxar satellite data	Example data Maxar sensor. Resolution 15 cm	['image data']		2	22	0	sergiishchus	maxar-satellite-data
2161	2161	pooling_7_model		[]		0	1	0	kookheejin	pooling-7-model
2162	2162	UIEB Dataset		[]		0	8	1	mmelahi	uieb-dataset
2163	2163	submission		['business']		0	1	0	zedbjddxd	submission
2164	2164	Prediction of car sales prices	Dataset to practice the ML validation process	[]		6	43	0	computingschool	car-sales
2165	2165	rock paper scissor		[]		0	3	0	mainakml	rock-paper-scissor
2166	2166	Exp-027-toxic-xlm-roberta-Ruddit		['arts and entertainment']		0	3	0	mst8823	exp-027-toxic-xlm-roberta-ruddit
2167	2167	map1000		[]		0	3	0	woodiedudy	map1000
2168	2168	melbourn housing		['real estate']		1	5	0	anggafahri	melbourn-housing
2169	2169	greenhouse_dataset		[]		6	13	0	konradb	greenhouse-dataset
2170	2170	Top rated movie		['movies and tv shows']		2	7	0	chandrabhanshirshwar	top-rated-movie
2171	2171	MRI Image based Brain Tumor Classification 		[]		6	33	0	iashiqul	mri-image-based-brain-tumor-classification
2172	2172	Indian Coin Dataset		['currencies and foreign exchange']		0	6	0	mainakml	indian-coin-dataset
2173	2173	tfgbr-stage2-v6-yolov5l-128		[]		0	13	0	ks2019	tfgbr-stage2-v6-yolov5l-128
2174	2174	Fake-Vs-Real-Faces (Hard)	Small Dataset of Real And Fake Human Faces for Model Testing	['computer vision', 'classification', 'image data']	"Context
The motivation behind the creation of this dataset is to have a challenging Test set for the task of classifying fake and real human faces. Most of the available datasets on Kaggle are ""Uniform"" and doesn't present a good variance of face features, particularly for the ""Fake"" class. Moreover, the fake faces collected in this dataset are generated using the StyleGAN2, which present a harder challenge to classify them correctly even for the human eye. The real human faces in this dataset are gathered so that we have a fair representation of different features(Age, Sex, Makeup, Ethnicity, etc...) that may be encountered in a production setup.
Content
The images available in this dataset are in a JPEG format and of uniform size of 300x300.
There ""Fake"" faces are collected from the website thispersondoesnotexist.com.
There ""Real"" faces images are collected through the API of the website Unsplash and then the faces are cropped out of using OpenCV library.
Total number of images: 1288
Number of ""Fake"" faces: 700
Number of ""Real"" faces: 589
The data.csv contains the images Id and the corresponding label.
Inspiration
Can you achieve a high accuracy on this dataset?"	20	326	3	hamzaboulahia	hardfakevsrealfaces
2175	2175	tmdb_dataset	This dataset contain 10000 movies info	['arts and entertainment']		1	10	0	bhushanraipurkar	tmdb-dataset
2176	2176	/kaggle/input		['computer science']		0	1	0	churikovsergey	kaggleinput
2177	2177	Brain Tumor MRI Image Classification 	Brain image classification with MRI Image dataset	['cancer']		5	68	3	iashiqul	brain-tumor-mri-image-classification
2178	2178	Pléiades. 3D Textured Model of Dubai, UAE	Pléiades. 3D Textured Model of Dubai, UAE. For Windows	[]		0	3	0	sergiishchus	pliades-3d-textured-model-of-dubai-uae
2179	2179	Pictures from most popular animes	701 pictures from one hundred most popular animes by myanimest.net	['websites', 'image data', 'comics and animation', 'anime and manga']	"Context
myanimelist.net is the most popular site where fans of Japanese animated films and series share their opinions on various productions. The portal works in a similar way to IMDb and allows users to rate various positions and then create various types of rankings based on them. In this database you will find a distribution of votes on a scale from 1 to 10 for the 100 most popular (most often cast) anime votes from a specific point in time.
Content
The data is grouped into folders where one folder corresponds to one series. We have a total of 100, as we took the 100 most popular anime based on the current ranking on the site. In each of the folders we can find pictures from the ""pics"" tab for a specific anime title. The number of pictures in each series differ from each other and for some there is only one picture, and for some even a dozen. Data was obtained using webscraping. Python was used for this process with the ""BeautifulSoup"", ""requests"", ""re"", ""urllib"", and ""os"" packages. For each movie or series, we managed to link to the pictures, and by using the HTML page preview, we managed to find direct links to the pictures. Then, in the loop, we managed to download each picture in turn and save to the appropriate folder with the appropriate name based on the link and the title of the series.
Inspiration
The inspiration to create a notebook based on this data may be a comparative analysis of the titles among themselves based on the pictures or the creation of a model which, based on the pictures, can predict which anime they come from. Unfortunately, there is little data in each category, which means that working with a small amount of data can be quite a challenge.
<br>
Photo by <a href=""https://unsplash.com/@dexezekiel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Dex Ezekiel</a> on <a href=""https://unsplash.com/s/photos/anime?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	3	210	13	michau96	pictures-from-most-popular-animes
2180	2180	Avengers Dataset	Dataset consisting of images and labels (YOLO format) of The Avengers	['arts and entertainment']		6	20	0	danielmalky	avengers-dataset
2181	2181	Capstone Project on Retail and Marketing Analysis		[]		7	31	0	rajpatelds	capstone-project-on-retail-and-marketing-analysis
2182	2182	Stocks Training Dataset		[]		0	10	0	subhadeepzilong	stocks-training-dataset
2183	2183	tfgbr-stage2-v6-yolov5l-75		[]		0	6	0	ks2019	tfgbr-stage2-v6-yolov5l-75
2184	2184	pomfile		[]		0	0	0	manoranjanpradhan	pomfile
2185	2185	my musician preferences---zedbjddxd		['music']		0	2	0	zedbjddxd	my-musician-preferenceszedbjddxd
2186	2186	tgbr-train-dbu-ub		['business']		0	1	2	prateekagnihotri	tgbr-train-dbu-ub
2187	2187	ucmerced_testing_eai		[]		0	6	0	sergiovitale	ucmerced-testing-eai
2188	2188	ggggggggggggggggggggggggggg		[]		0	2	0	anaselmasry	ggggggggggggggggggggggggggg
2189	2189	Nifty IT Index daily data 2011 to 2022	India's Stock exchange Nifty IT sector latest data	['business', 'finance', 'computer science', 'programming', 'beginner', 'time series analysis', 'investing', 'datetime']	"Context
IT is one of the biggest revenue making industry in India.
Around 8% of India's GDP is contributed by IT sector.
This dataset shows how Top 10 IT companies of India are performing on daily basis.
Content
Dataset contains Daily performance of Top 10 IT companies of India listed on Nifty stock Exchange.
Below are the brief In-sites about each column.
1. Date - Date of trading
2. Open - At what price trading started
3. High - Day's high traded price
4. Low - Day's low traded price
5. Close - At what price trading ended
6. Shares Traded - Volume of shares traded
7. Turnover - Turnover in Rupees (Cr.)
Acknowledgements
This dataset is taken from NSE site
Inspiration
Lets analyze Nifty IT sector and forcast how it will perform in future"	48	317	9	abhijeetbhilare	nifty-it-index-daily-data-2011-to-2022
2190	2190	tgbr-train-dbu-w		[]		0	0	0	prateekagnihotri	tgbr-train-dbu-w
2191	2191	Happywhale: BoundingBox [YOLOv5] ds		[]		0	4	0	awsaf49	happywhale-boundingbox-yolov5-ds
2192	2192	tgbr-train-rb-ub		[]		0	2	0	prateekagnihotri	tgbr-train-rb-ub
2193	2193	Give Life: Predict Blood Donations		[]		0	13	0	bhupendramishra7	give-life-predict-blood-donations
2194	2194	Pneumonia due to Covid-19: Analysis & Prediction2	Covid Pneumonia dataset- Cleaned for analysis	['healthcare', 'public health', 'health', 'health conditions', 'covid19']		5	20	0	homelysmile	datasetclean
2195	2195	Spot 7 - Optical Imagery	Amman, Jordan. Example satellite imagery	['image data']		1	10	0	sergiishchus	spot-7-optical-imagery
2196	2196	Covid-19 worldwide data	This is the data repository for the 2019 Novel Coronavirus Visual Dashboard.	['exploratory data analysis', 'data cleaning', 'data analytics', 'covid19']	"Context
Cleaned data ready for visualisation of COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University
Content
This is the data repository for the 2019 Novel Coronavirus Visual Dashboard operated by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). Also, Supported by ESRI Living Atlas Team and the Johns Hopkins University Applied Physics Lab (JHU APL).
Acknowledgements
Johns Hopkins University Center
Inspiration
I am learning data cleaning."	286	1767	24	kunwarakash	covid19-cleaned-data-worldwide
2197	2197	Pléiades Neo - radar	Spain, Barcelona - ORI, SE	[]		1	4	0	sergiishchus	pliades-neo-radar-spain-barcelona-ori-se
2198	2198	WLASL Complete		[]		1	41	4	utsavk02	wlasl-complete
2199	2199	cicid_gan1	cicid 2017 pre process dataset	['earth and nature']		0	14	0	rasikagawande	cicid-gan1
2200	2200	Starfish_Object_Detection		[]		0	6	0	balasubramaniamv	starfish-object-detection
2201	2201	three-classes-covid-xray-balanced		['health']		0	5	0	tuanledinh	threeclassescovidxraybalanced
2202	2202	RSNA CQ500 New Hemorrhage Data Frames		[]		0	5	0	fereshtej	rsna-cq500-new-hemorrhage-data-frames
2203	2203	XLNET_512_NO_CLEAN_TRY2		[]		0	5	0	hangy132	xlnet-512-no-clean-try2
2204	2204	mirror		[]		0	5	0	lweize	mirror
2205	2205	coronadataset		[]		0	2	0	yesika	coronadataset
2206	2206	COCO/WikiArt NST Dataset (small)	Neural style transfer with AdaInstNorm, COCO + Painter-by-Numbers (5k images)	['arts and entertainment', 'data cleaning', 'image data']		12	10	0	shaorrran	cocowikiart-nst-dataset-small
2207	2207	masterdictionary		[]		0	5	0	adeshtrivedi	masterdictionary
2208	2208	mimic-medvill-split	split of mimic from medvill	[]		0	9	1	christoforostrakas	mimicmedvillsplit
2209	2209	tgbr-train-rb-w		['transportation']		0	2	0	prateekagnihotri	tgbr-train-rb-w
2210	2210	output_roberta-base	daniel-milan111111111111	[]		17	46	0	li874039270	output-robertabase
2211	2211	Quick, Draw! Shuffled!	Google's Quick Draw Dataset - shuffled	['arts and entertainment', 'games']	"Context
""Quick, Draw!"" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.
The data is originally acquired from an old kaggle competition."	0	40	3	thevishnupradeep	quick-draw-shuffled
2212	2212	SIAE-Steganography-DB-Speech-G723.1		[]		0	16	1	hamzakheddar	siae-db-speech
2213	2213	desafio1-model-classifier		['computer science']		1	7	0	williambruno	desafio1modelclassifier
2214	2214	School Data		[]		0	7	1	raheeltalha	school-data
2215	2215	Pre-Post Product purchase Questions	Product Purchase Questions while shopping on E-Commerce sites	['nlp', 'retail and shopping', 'e-commerce services']	"Context
When we shop on any e-commerce site we usually see some questions asked by people before or after purchasing any product.
These questions are very helpful for shoppers to understand the in-sites of a product.
With this dataset we can understand what type of questions usually asked by people before or after purchasing any product.
Content
This dataset has following information
1. Product ID of a item
2. Question asked by people
3. Name of product
4. When particular question is asked (before or after purchasing the product)
Acknowledgements
I would like to thank aws open dataset for providing this amazing dataset.
Special cites and thanks to
""Did you buy it already?"", Detecting Users Purchase-State From Their Product-Related Questions. Lital Kuchy, David Carmel, Elad Kravi & Thomas Huet. SIGIR 2021. 
Inspiration
Lets use out machine leaning knowledge to find in-sites of this data."	38	619	13	abhijeetbhilare	prepost-product-purchase-questions
2216	2216	4cemht		[]		0	0	0	wallacefqq	4cemht
2217	2217	tfgbr-stage2-v5-yolov5l-75		[]		0	10	0	ks2019	tfgbr-stage2-v5-yolov5l-75
2218	2218	distilbert_linear_v1		[]		0	5	0	alexander1980	distilbert-linear-v1
2219	2219	roberta_base_linear_v9		[]		0	6	0	alexander1980	roberta-base-linear-v9
2220	2220	happywhale-tfrecords-v1		[]		36	414	15	ks2019	happywhale-tfrecords-v1
2221	2221	Ubiquant models	RNN and CatBoost Models for Ubiquant Market Prediction	['clothing and accessories', 'time series analysis', 'gradient boosting', 'rnn']	Dataset contains Recurrent Neural Networks(LSTM) & CatBoost Regressor models trained on Ubiquant Data.	0	99	3	devkhant24	models
2222	2222	cq500_selected_series		[]		9	4	0	fereshtej	cq500-selected-series
2223	2223	TFR_V2		[]		0	0	0	miguelespinozac	tfr-v2
2224	2224	Bacteria images TPS Feb 22	Images created from the tabular dataset	['biology']		10	38	1	austinpowers	bacteria-images-tps-feb-22
2225	2225	NMR/JPY Daily 2021	NMR JPY Daily OHLCV in 2021	['tabular data', 'investing']	"Content
NMR/JPY Daily OHLCV, obtained from Yahoo! Finance.
For calculating tax..."	3	27	4	code1110	nmrjpy-daily-2021
2226	2226	flex_yolo_pack		[]		0	3	0	timxinxinpeng	flex-yolo-pack
2227	2227	YOLOv5 lib ds		[]		741	1991	41	awsaf49	yolov5-lib-ds
2228	2228	gbr traincsv		['sports']		0	8	0	v1olet	gbr-traincsv
2229	2229	iris_data		[]		0	1	0	yanjiaxi	irisd
2230	2230	happywhale-tfrecords-v2		[]		0	4	0	ks2019	happywhale-tfrecords-v2
2231	2231	ASVP-ESD(Speech & Non-Speech Emotional utterances)		['social science', 'artificial intelligence', 'signal processing', 'multiclass classification', 'audio data']	"The main idea to collect this dataset(ASVP-ESD) actually came from my lab supervisor after a long period of discussion, on coming out with something more efficient that can improve the robustness of the recognition model when applied in real life. We decided to build a more realistic emotion dataset with natural environment noise taking speech and non-speech utterances into consideration. Emotions are collected from online platforms, movies, real communication recordings, etc.
The Audio, Speech, and Vision Processing Lab Emotional&nbsp; Sound database&nbsp;(ASVP-ESD)
Dejoli Tientcheu Touko Landry; Qianhua He; Wei Xie
Citing the ASVP-ESD:
The ASVP-ESD emotional sound database is released by Audio, Speech, and Vision Processing Lab(http://www.speech-led.com/main.htm, from the South China University of Technology), so please cite the ASVP-ESD if it is used in your work in any form. &nbsp; Personal works, such as machine learning projects or posts, should provide a URL to this page, through a reference.
Contact Information:
If you would like further information about the ASVP-ESD, when; facing any issues downloading files, please contact us at&nbsp;&nbsp;201722800077@mail.scut.edu.cn,&nbsp;1197581424@qq.com
Data labeling process:
The first version&nbsp;dataset labeling&nbsp; (containing 99+2 folders) was done by 5 different annotators through a tagging application specially designed for audio tagging the latest added folders were done by 3 other annotators.&nbsp; After listening to each audio the judge should choose the corresponding label according to personal feeling; Then after the tagging part, a simple voting algorithm was built for voting and upgrading the corresponding audio to the class having the most number of votes.
Construction and Validation:
The ASVP-ESD contains 8397 audio files(with additional 1204 files for babies' voices ); It is an emotional-based database, containing speech and non-speech emotional sound; The audio was recorded and collected from movies, tv shows, youtube channels, and other e emotional sound websites... Compared to other public emotional databases, ASVP-ESD is more realistic and non-scripted with no language restriction.
.Description:
The Audio, Speech, and Vision Processing Lab Emotional Sound database(ASVP-ESD) contains audio files regrouped in 97 folders; The data are organized as follows:  odd folder number for females, even for males (total size: 1.87 GB). As it's a realistic dataset some folders contain dialog or several people interacting in the audio; Speech and non-speech Emotional sounds include boredom, neutral, happiness (laugh, gaggle), sadness(cry, sniff), angry, fear (scream, deep breath, panic), surprise(amazed), disgust, excite(agitation), pleasure, pain, disappointment; expressions total of 12 different emotions. 2 levels of intensity were used for the database (normal and high). Audio is available in 16k, 1 channel, .wav format, the average length of the file is between 0.5 to 20 seconds, for a total of about 11 hours. Note, there are two additional folders (acteur_150,acteur_50) that contains only babies audio(laugh, cry); Audio-only files are regrouped as:
Actor_00 and Actor_50 are composed of mixed audio samples from movies and website sounds.
From actor_01 to actor_19 and actor_31 to acteur_38 &nbsp;are different actors from 3 different movie sounds.
From actor_21 to actor_29 &nbsp;and &nbsp;actor_39 to actor_88; are only for sounds randomly collected on the online platform.
Actor_100(actor_100 are crowd or many people voices) ; to actor_102 are from the same website ,same for actor_103 to actor_106;
File naming convention
Each of the audio files has a unique filename. The filename consists of numerical identifiers (e.g., 02-01-06-01-02-105-02-01-02.wav, for speech 02-01-06-01-02-105-02-01-02-01-03.wav, for non-speech) these identifiers define the stimulus characteristic.
Filename identifiers:
Modality ( 03 = audio-only).
Vocal channel (01 = speech, 02 = non speech).
Emotion ( 01 = boredom, 02 = neutral, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised, 09 = excited, 10 = pleasure, 11 = pain, 12 = disappointment, 13 = others).
Emotional intensity (01 = normal, 02 = high).
Statement (as it’s non scripted this refer to the number of sample select per actor folder ).
Actor ( even&nbsp;numbered acteurs are male, odd numbered actors are female).
Age(01 = above 65, 02 = between 20~64, 03 = under 20,04=new born).
Source of downloading (01 =website , 02 = youtube channel, 03= movies).
Language(01=Chinese , 02=English ,04 = french , others).
Filename example: 03-01-06-01-02-12-02-01-01-16-04.wav:
1.audio-only (03)
2.Speech (01)
3.Fearful (06)
4.Normal intensity (01)
5.Statement (02)
6.12th Actorr (12)&nbsp;folder 12 male as its even
7.Age(02)
8.Source(01)
9.language(01)
10.Screaming “only for non speech”(16)
11.the 4th;sample from the same dialog(04)
All audio file with 77 at the end means files with a high noise environment.
for non-speech data:
Happyness is a collection of (laugh=13,gaggle=23,others=33)
sadness is a collection of (cry=14, sigh=24,sniffle=34,suffering=44)
fear is a collection of (scream 16, breath=26 ,panic=36)
angry (rage=15,frustration=25 ,other=35)
surprise (surprised=18, amazed=28 ,astonishment=38,others=48)
disgust(disgust=17, rejection=27)
For any suggestions please don't hesitate just send us an email.
We wouldn't be here without the help of other lab mates such and annotators. The goal is to reach 10000 samples, and update versions with a link to download the materials used such as the annotation Exe App, and the voting code to allow each person interested to make his personal annotation or tagging. For audio emotion analysis, and classification the audio file is the corresponding file to be used.
voting : https://github.com/landryroni/emotions-voting
tagging : https://github.com/landryroni/emotion_tagging"	35	146	0	dejolilandry	asvpesdspeech-nonspeech-emotional-utterances
2232	2232	demofile		[]		0	2	0	adeshtrivedi	demofile
2233	2233	Space X Launches Falcon9 and Falcon Heavy	Launch Site, Customer, Success and Failure 	['science and technology']	"Please Upvote If u Like the dataset it motivates me :)
Context
Space Exploration Technologies Corp. (SpaceX) is an American aerospace manufacturer, space transportation services.
Acknowledgements
The data is collected from <a href=""https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches""> Wikipedia - SpaceX Launches</a>."	61	909	23	heyrobin	space-x-launches-falcon9-and-falcon-heavy
2234	2234	Stopword		[]		0	2	0	adeshtrivedi	stopword
2235	2235	bioseq whl	Bioinformatics package for working with biological sequences	['computer science', 'programming']	"Bioseq
Simple package to work with biological sequence
Notebooks that use package (to date):
Biological Sequence Operations
Biological Sequence Alignment
Prepare <code>whl</code> file using:
python setup.py bdist_wheel --universal
Install in notebook via:
!pip install /path/"	7	235	23	shtrausslearning	bioseq
2236	2236	Whale_effnetv1_b7		[]		0	6	0	mathurinache	whale-effnetv1-b7
2237	2237	car price and features dataset		[]		14	54	0	shahz001	cardekho-dataset
2238	2238	Tehran Air Pollution Analysis		['earth and nature']		6	150	1	ashkanranjbar	tehran-air-pollution-analysis
2239	2239	BlackCoffer2		[]		0	3	0	adeshtrivedi	blackcoffer2
2240	2240	cassava disease version one	cassava plant disease consist of 5 classes	['food']		5	21	0	alexlianardo	cassava-disease-v1
2241	2241	Jigsaw_unintended_bias_dataset	Jigsaw_unintended_bias_dataset	['text data']	"Competition data
Jigsaw Unintended Bias in Toxicity Classification"	0	9	0	koheishima	jigsaw-unintended-bias-dataset
2242	2242	Jigsaw_toxic_comment_dataset	Jigsaw_toxic_comment_dataset	['text data']	"Competition data
Toxic Comment Classification Challenge"	9	16	0	koheishima	jigsaw-toxic-comment-dataset
2243	2243	768 image data whaless ssssssssss		[]		0	10	0	mithilsalunkhe	768-image-data-whaless-ssssssssss
2244	2244	umptransformer		[]		0	3	0	hadisahmadian	umptransformer
2245	2245	768 image data whaless		[]		0	5	0	mithilsalunkhe	768-image-data-whaless
2246	2246	Hemorrhage_NonHemorrhage_3DModel		[]		0	4	0	fereshtej	hemorrhage-nonhemorrhage-3dmodel
2247	2247	768 images whale Dataset		[]		0	2	0	mithilsalunkhe	768-images-whale-dataset
2248	2248	512*512 Whale Dataset		['earth science']		36	26	1	mithilsalunkhe	512512-whale-dataset
2249	2249	IDAO_Features_X_train		[]		0	9	0	anirudhvadakedath	idao-features-x-train
2250	2250	Netflix Stock Price Prediction	Netflix Stock PricePrediction using Time Series	['business', 'intermediate', 'time series analysis', 'lstm', 'investing', 'python']	"The Dataset contains data for 5 years ie. from 5th Feb 2018 to 5th Feb 2022
The art of forecasting stock prices has been a difficult task for many of the researchers and analysts. In fact, investors are highly interested in the research area of stock price prediction. For a good and successful investment, many investors are keen on knowing the future situation of the stock market. Good and effective prediction systems for the stock market help traders, investors, and analyst by providing supportive information like the future direction of the stock market."	127	961	11	jainilcoder	netflix-stock-price-prediction
2251	2251	cassava disease with 1000 each class		['food']		1	6	0	alexlianardo	cassava-disease-with-1000-each-class
2252	2252	Ubiquant Market Prediction np memmap		['investing']		5	31	0	assign	ubiquant-market-prediction-np-memmap
2253	2253	IAU designated constellations		['earth and nature']	"IAU designated constellations
Column
Constellation: Constellation name
IAU: Abbreviations in IAU
NASA: Abbreviations in NASA
Genitive:
Origin: 
Meaning: 
Brightest star: 
Source
https://en.wikipedia.org/wiki/IAU_designated_constellations"	0	6	2	stpeteishii	iau-designated-constellations
2254	2254	Data_Negara_HELP.csv		[]		0	9	0	joshxp	data-negara-helpcsv
2255	2255	python2		['computer science', 'programming']		0	10	0	wahyuprasetyoaji	python2
2256	2256	3DModel_Hemorrhage_NonHemorrhage		[]		0	12	0	hamedghavami	3dmodel-hemorrhage-nonhemorrhage
2257	2257	cancer data 1		['cancer']		4	21	0	khoirunnisalarasati	cancer-data-1
2258	2258	cancer data		['cancer']		1	15	0	khoirunnisalarasati	cancer-data
2259	2259	Synboost Cityscapes		['art']		0	12	0	mlrc2021anonymous	synboost-cityscapes
2260	2260	Haiti damage assessment	Damage assessment from social media image using Haiti earthquake data	[]		1	6	0	mayerantoine	haiti-damage-assessment
2261	2261	UFO Subreddit Posts		['religion and belief systems']		1	18	0	jacksonkarel	ufo-subreddit-posts
2262	2262	marketing_data		[]		2	5	0	lancengck	marketing-data
2263	2263	iphone_purchase		[]		0	9	0	anggafahri	iphone-purchase
2264	2264	Whale Datset		[]		0	1	0	mithilsalunkhe	whale-datset
2265	2265	512 Whale Dataset		['earth science']		0	10	1	mithilsalunkhe	512-whale-dataset
2266	2266	tugas python2		['computer science']		0	3	0	wahyuprasetyoaji	tugas-python2
2267	2267	yolo_flex_model		[]		2	7	0	timxinxinpeng	yolo-flex-model
2268	2268	bankmarketing		[]		0	5	0	anggafahri	bankmarketing
2269	2269	flexible_yolov5		[]		0	14	0	timxinxinpeng	flexible-yolov5
2270	2270	BERT_weights	Used in My BERT-BaseLine notebook	[]		3	20	0	sasukess1	bert-weights
2271	2271	test_v2		[]		0	5	1	thomasdubail	test-v2
2272	2272	Country GeoLocations	Some fun with gapminder data mapping using country locations as centroids	[]		0	14	0	jorgegarciainiguez	country-geolocations
2273	2273	048-exp		[]		0	2	0	shimishige	048-exp
2274	2274	weightedboxesfusion		[]		53	359	0	markpeng	weightedboxesfusion
2275	2275	new_nlp 		[]		0	14	0	banbeipi	new-nlp
2276	2276	Mental Health Patients 2021-2022 	District Wise Number of Mental Health Patients in India (Karnataka)	['mental health', 'healthcare', 'health conditions', 'hospitals and treatment centers']	"Title
District Wise Number of Mental Health Patients in year 2021-2020 in Country India State Karnataka
Description
District Wise number of mental health patients such as severe mental illness, common mental disorder, alcohol, and substance abuse, cases referred to higher centers, suicide attempt cases
Contributor
Karnataka, Health and Family Welfare Department, Karnataka
Sectors
Health and Family welfare › Health
Source
Karnataka data government Click Here to visit the website
Group Name
Department of Health and Family Welfare"	61	622	10	meetnagadia	district-wise-mental-health-patients-20212022
2277	2277	pklpklpklpkl		[]		0	14	0	banbeipi	pklpklpklpkl
2278	2278	News Summarization	News articles with corresponding summaries	['nlp', 'tabular data', 'news']	"Data
The following data is intended for advancing news summarization research. It's three datasets (XSum, CNN/Daily Mail, Multi-News) combined into one easy-to-use CSV file. It provides the original ID of each record, along with the article and its corresponding summary.
Citations
Narayan, Shashi, Shay B. Cohen, and Mirella Lapata. ""Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization."" arXiv preprint arXiv:1808.08745 (2018).
See, Abigail, Peter J. Liu, and Christopher D. Manning. ""Get to the point: Summarization with pointer-generator networks."" arXiv preprint arXiv:1704.04368 (2017).
Fabbri, Alexander R., et al. ""Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model."" arXiv preprint arXiv:1906.01749 (2019)."	39	356	9	sbhatti	news-summarization
2279	2279	COTS v NotCOTS Cropped Crown of Thorns Dataset	11,898 Cropped Tensorflow Kaggle Crown of Thorns Starfish from Bounding Boxes	['beginner', 'exploratory data analysis', 'computer vision', 'image data', 'video data']	"Context
This data comes entirely from the TensorFlow - Help Protect the Great Barrier Reef competition and should not be used outside of the competition! I do not own these images and to the extent possible want to ensure this complies with terms of the competition - I believe it does. All users/viewers of this dataset should adhere to the terms&conditions of the competition.
I wanted an easily accessible repository of the cots images and not cots images to help with data augmentation and possibly improving the models in other ways. In the spirit of the competition I thought it made the most sense to make this available to the other competitors.
Content
This notebook was used to pre-process / create this dataset: Cropped Crown of Thorns Dataset Builder. It walks through the steps in a readable way. 
About the dataset:
* This dataset contains an equal number (11,898) of images of COTS and Not Cots .jpg images. 
* These images come from cropping out the bounding box regions from each video frame in the competition.
* Use this for data augmentation
* Alternatively, if you're just getting started, try building binary classifiers for COTS vs. Not COTS if you want to build up the skill to create more complicated object detection models.
Acknowledgements
This comes directly from the TensorFlow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef) competition. Alternative citations include: 
Liu, J., Kusy, B., Marchant, R., Do, B., Merz, T., Crosswell, J., ... & Malpani, M. (2021). The CSIRO Crown-of-Thorn Starfish Detection Dataset. arXiv preprint arXiv:2111.14311.
Inspiration
See Notebook used to build this dataset here: Cropped Crown of Thorns Dataset Builder"	46	618	17	alexteboul	binary-cropped-crown-of-thorns-dataset
2280	2280	AirbnbToronto		[]		0	22	0	pandapugs44	airbnbtoronto
2281	2281	BPL Dataset (2012-2022)	The latest and complete Bangladesh premier league matches data.	['cricket', 'sports']		17	181	4	abdunnoor11	bpl-data
2282	2282	COD Vanguard Stats	the data contains top 13k players statistics  	['sports']		3	87	1	fahadalqahtani	cod-vanguard-stats
2283	2283	College Scorecard 19-20		['universities and colleges']		1	10	0	jamesvernon	college-scorecard-1920
2284	2284	Class Playlist		['education']		0	3	0	rosemaryli	class-playlist
2285	2285	5 playlist		[]		0	2	0	dayanagomezs	5-playlist
2286	2286	nbme_roberta_large_models		[]		0	0	0	mathurinache	nbme-roberta-large-models
2287	2287	Class playlist per. 5		['education']		3	18	0	fernandolopez2238	class-playlist-per-5
2288	2288	wildfire_prediction		[]		3	15	0	dimitriosmavrofridis	wildfire-prediction
2289	2289	News.csv	Data used for a Fake News Project	['news']		1	24	1	antonioskokiantonis	newscsv
2290	2290	Colorado Avalanche Power Play Data 2021-2022	Data on Colorado Avalanche Power Play Opportunities	['sports']		0	20	0	maxbet10	colorado-avalanche-special-teams-data-20212022
2291	2291	hotel bookings		['hotels and accommodations']		0	11	0	eidmohamed8	hotel-bookings
2292	2292	xm-tx-base		[]		0	6	0	datafan07	xmtxbase
2293	2293	my_data1		[]		0	5	0	maneya87	my-data1
2294	2294	H&M Sales Data		['clothing and accessories', 'beginner', 'intermediate', 'data visualization', 'tabular data']	"Context
The historical data contains tons of records for the H&M products sale. H&M – Hennes & Mauritz AB is a Swedish multinational clothing-retail company known for its fast-fashion clothing for men, women, teenagers, and children.
It is an excellent database for people who want to try learning techniques of data visualization, data analytics, and many different forms of data processing."	17	152	1	tulasiram574	hm-sales-data
2295	2295	Professional Lacrosse Statistics (PLL & MLL)	Per year statistics for professional outdoor lacrosse players across two leagues	['games', 'sports']	"Context
The Premiere Lacrosse League (PLL) is entering its third season and has seen continued growth since inauguration. Data-driven insights, now common place in popular sports, could be a large benefit for coaches of the league and those involved in sports betting or fantasy lacrosse leagues. I will use this data to draw insights and hopefully win my fantasy lacrosse league. I hope others can find some use out of this data as well.
Content
These data sets contain per year statistics for every player in the 2019 MLL, 2020 MLL, 2019 PLL, and 2020 PLL  seasons. 
General legend:
player_name - Player's name 
teamId - Team player was on 
gamesPlayed - number of games player
points - sum of goals, two point goals, and assists 
twoPointGoals - goals scored from two-point arc that count as two goals, worth two points 
powerPlayGoals - goals scored on power play 
shortHandedGoals - goals scored while other team is on power play    power play
pim - penalty minutes
SOFF - shots off goal, either regular or two-point shots
statLeague - League player was in at the time PLL or MLL
seasonSegment - either regular season or post season  
Acknowledgements
I'd like to acknowledge and thank Jake Watts of the PLL for providing these statistics."	51	305	0	tjmulderig	professional-lacrosse-statistics-pll-mll
2296	2296	Film Adaptations of Video Games	Live-action films adapted from video games	['arts and entertainment', 'movies and tv shows', 'video games']	"Summary
Data about live-action films adapted from video games. The data file includes information about 42 films adapted from video games, starting with 1993's Super Mario Bros. up until today.
Column Descriptors
Title of the film
Release date
Worldwide box office (USD)
Rotten Tomatoes score (%)
Metacritic score (out of 100)
Film Distributor
Original game publisher"	36	272	8	bcruise	film-adaptations-of-video-games
2297	2297	Julia-hello-world-and-install-packages	Julia Code Example of Lorenz Attractor	[]		0	18	0	tpmeli	juliahelloworldandinstallpackages
2298	2298	Maryland Cable Complaints with Geocoded Addresses		[]		0	11	0	kevingrij	maryland-cable-complaints
2299	2299	tai  tan		[]		1	6	1	baifanbufan	tai-tan
2300	2300	COVID-19 Deaths by Region in USA	Provisional count of deaths involving coronavirus  COVID-19 in the USA by week	['health', 'advanced', 'data analytics', 'tabular data', 'covid19']		1	42	5	andrej0marinchenko	covid19-deaths-by-region-in-usa
2301	2301	Crawling Databrick 	Crawling Databrick - TWEET CRYPTOCURRENCY	['currencies and foreign exchange']	"Mencoba mengambil informasi/data dari twitter api key dan mengimplementasikannya kedalam kodingan dengan Bahasa Pemrograman  python di databrick. 
Tujuan :
1. Menerapkan data crawling.
2. Memproses datasets dengan databricks"	1	21	3	dzikrifaizziyan	crawling-databrick-tweet-cryptocurrency
2302	2302	yolov5s6		[]		386	941	12	freshair1996	yolov5s6
2303	2303	Dataset Jigsaw Toxic Comments	Custom Dataset for Jigsaw Rate Toxic Comments Competition 2021	['tabular data', 'text data']	"Context
This dataset contains custom data for Jigsaw Rate Toxic Comments Competition 2021.
Disclaimer: The data contains text that may be considered profane, vulgar, or offensive.
Source
Link : https://www.kaggle.com/c/jigsaw-toxic-severity-rating
Link : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
Link : https://github.com/unitaryai/detoxify"	1	100	0	renokan	dataset-jigsaw-comments
2304	2304	Synboost W/O Data Generator		['business']		0	7	0	mlrc2021anonymous	synboost-without-data-generator
2305	2305	BilstmModel		[]		0	6	0	mahdibb	bilstmmodel
2306	2306	Synboost Light Data		[]		0	14	0	mlrc2021anonymous	synboost-light-data
2307	2307	STRIP AI	Stroke Thromboembolism Registry of Imaging and Pathology (STRIP)	['image data', 'heart conditions']	"Worldwide, stroke remains the second-leading cause of death and the third-leading cause of combined disability and mortality. In the United States, stroke is usually the fourth leading cause of mortality, where each year, approximately ≈795000 individuals experience a stroke, of which 88% are ischemic. With mechanical thrombectomy becoming the standard of care treatment for acute ischemic stroke (AIS) from large vessel occlusion in the last decade, retrieved clots became amenable to analysis.
The risk of stroke recurrence remains high, with 23% of total events being recurrent, however, a second stroke can be mitigated through appropriate prevention measures. Secondary stroke prevention strategies are highly dependent on the determination of stroke etiology which influences the therapeutic management following stroke events. The two major AIS etiology subtypes are Cardioembolic (CE) and Large Artery Atherosclerosis (LAA), in addition to a significant proportion of cases (45%) being cryptogenic where the clot origin remains undetermined. Considering the impact of identifying stroke etiology on secondary preventive care, the ability to differentiate between cardiac and large artery atherosclerosis origins of retrieved clots is of critical importance. 
New artificial intelligence approaches could help render AIS etiology prediction feasible based on histological insights of retrieved stroke emboli. It is conceivable that clots of different origins and formation mechanisms could have variations in their structural organization as well as histological signatures potentially distinguishable using pathomics and deep learning techniques.
Stroke Thromboembolism Registry of Imaging and Pathology (STRIP) is a uniquely large multicenter project led by Mayo Clinic Neurovascular Lab with the aim of histopathologic characterization of thromboemboli of various etiologies and examining clot composition and its relation to mechanical thrombectomy revascularization. STRIP AI is an extension that builds on STRIP findings to provide complete digitization and online access to the labeled whole slide images (WSI) of collected clots, as well as all available anonymized related clinical data. Successful application of machine learning and deep learning tools could be useful for accurate prediction of stroke origin, such algorithms could be validated and routinely deployed to histological specimens retrieved during thrombectomy, and thus addressing a major public health issue and helping alleviate stroke burden worldwide."	2	141	3	barbaroserdal	strip-ai
2308	2308	ROSBAG DATASET TURTLEBOT3		['earth and nature']		0	3	0	youssefad	rosbag-dataset-turtlebot3
2309	2309	Mercedes-Benz Greener Manufacturing Dataset	Mercedes-Benz Greener Manufacturing Predicting 	['automobiles and vehicles', 'beginner', 'tabular data', 'regression']	"Context
The dataset contains the production time for Mercedes manufacturing testbech. 
Can you determine which platforms are most important using Machine Learning to prioritize manufacturing.
Acknowledgements
The dataset is referred from Kaggle."	19	157	8	yasserh	mercedesbenz-greener-manufacturing-dataset
2310	2310	Afinn.csv		[]		0	10	0	harshiis	afinncsv
2311	2311	Trout - data		[]		3	7	0	maciekroyski	trout-data
2312	2312	Covid_19_Analysis		[]		1	12	0	omarmuhammadatta	covid-19-analysis
2313	2313	Internet Speed Dataset	Dataset of Internet speed tests performed on Ookla	['internet', 'geospatial analysis']	"Introduction
This dataset provides global fixed broadband and mobile (cellular) network performance metrics in zoom level 16 web mercator tiles (approximately 610.8 meters by 610.8 meters at the equator). Data is provided in both Shapefile format as well as Apache Parquet with geometries represented in Well Known Text (WKT) projected in EPSG:4326. Download speed, upload speed, and latency are collected via the Speedtest by Ookla applications for Android and iOS and averaged for each tile. Measurements are filtered to results containing GPS-quality location accuracy.
Content
| Field Name   | Type        | Description                                                                                        |
|--------------|-------------|----------------------------------------------------------------------------------------------------|
| avg_d_kbps | Integer     | The average download speed of all tests performed in the tile, represented in kilobits per second. |
| avg_u_kbps | Integer     | The average upload speed of all tests performed in the tile, represented in kilobits per second.   |
| avg_lat_ms | Integer     | The average latency of all tests performed in the tile, represented in milliseconds.                |
| tests      | Integer     | The number of tests taken in the tile.                                                             |
| devices    | Integer     | The number of unique devices contributing tests in the tile.                                       |
| quadkey    | Text        | The quadkey representing the tile.                                                                 |
| tile | Text | Well Known Text (WKT) representation of the tile geometry. |
Quadkeys can act as a unique identifier for the tile. This can be useful for joining data spatially from multiple periods (quarters), creating coarser spatial aggregations without using geospatial functions, spatial indexing, partitioning, and an alternative for storing and deriving the tile geometry.
Two layers are distributed as separate sets of files:
mobile - Tiles containing tests taken from mobile devices with GPS-quality location and a cellular connection type (e.g. 4G LTE, 5G NR).
fixed - Tiles containing tests taken from mobile devices with GPS-quality location and a non-cellular connection type (e.g. WiFi, Ethernet).
Quarter 1 refers to data from January to March. Quarter 2 refers to data from April to June. Quarter 3 refers to data from July to September. Quarter 4 refers to data from October to December. All the data is from the year 2020.
Citation
Speedtest® by Ookla® Global Fixed and Mobile Network Performance Maps. Based on analysis by Ookla of Speedtest Intelligence® data for 2020. Provided by Ookla and accessed February 15, 2021. Ookla trademarks used under license and reprinted with permission.
Image Credits: Unsplash - umby"	4340	7433	33	dhruvildave	ookla-internet-speed-dataset
2314	2314	Animal Dataset	398 different species and sub-species of animals 	['earth and nature']		1	103	2	goelyash	animal-dataset
2315	2315	Delhi Bus Data		[]		0	7	0	joyee19	delhi-bus-data
2316	2316	exp0-lflarge-baseline-f0		[]		0	4	0	atharvaingle	exp0-f0
2317	2317	ubi_data		[]		3	36	0	piershinds	ubi-data
2318	2318	Top 10 NFT stats	Data extracted using OPENSEA API	['tabular data']	"Context
All-time top 10 NFT's
Content
Collection stats
Acknowledgements
https://docs.opensea.io/reference/api-overview"	11	129	5	outoftheloop	top-10-nft-stats
2319	2319	Movies data	In This Data You will Know The Rating Of Movies 	['movies and tv shows']		0	14	0	mohammadjavedkhan	movies-data
2320	2320	Population_Data	Population Data of all countries	['popular culture', 'people', 'people and society', 'social science']	"This file contains all the count of the population throughout the years for each and every country in the world. Total population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship.
The values shown are midyear estimates.
This dataset was downloaded from WorldBanks' website of link:
https://databank.worldbank.org/Population-Data/id/afc02899"	13	104	3	sergegeukjian	population-data
2321	2321	with_dist_data		[]		0	2	0	vladimirmynka	with-dist-data
2322	2322	nlsy97		[]		0	3	0	tomharrismit	nlsy97
2323	2323	roberta-8		['arts and entertainment']		0	10	2	cdeotte	roberta-8
2324	2324	Price of Various Commodities from Various Markets	Indian State-wise Markets	[]		0	16	3	raj401	price-of-various-commodities-from-various-markets
2325	2325	transformers		['movies and tv shows']		0	9	0	rvnrvn1	transformers
2326	2326	Daily Refresh-Commodities Price from India Markets		[]		0	15	0	satyampd	daily-refreshcommodities-price-from-india-markets
2327	2327	dataset new		[]		0	4	0	saarunpnairiiitk	dataset-new
2328	2328	ump ds tfrec 0205		[]		0	13	0	wuliaokaola	ump-ds-tfrec-0205
2329	2329	data.csv		[]		0	11	0	agemigna	datacsv
2330	2330	Covid19-Disorientation-Survey		['business']		1	13	0	gurchani	covid19disorientationsurvey
2331	2331	latihandatakanker		[]		0	5	0	hanifaramadhani	latihandatakanker
2332	2332	datakanker		[]		0	6	0	hanifaramadhani	datakanker
2333	2333	audio_set_seven_split		[]		12	10	0	xiaojin712	audio-set-seven-split
2334	2334	DVD_Video		[]		0	6	0	darthvader4067	dvd-video
2335	2335	casicien	transparent jpg pvc bottles pictures from its bottom.	[]		0	16	1	tektronix475	casicien
2336	2336	dist_data		[]		0	1	0	vladimirmynka	dist-data
2337	2337	kubeedge_sedna		[]		4	204	1	jaypume	kubeedge-sedna
2338	2338	fake_real_dataset		[]		0	7	0	rohinisalvi	fake-real-dataset
2339	2339	047-exp		[]		0	2	0	shimishige	047-exp
2340	2340	embedding transfomer		['computer science']	"From this orginal Kaggle dataset but it didn't worked (some Kaggle version error) hence this new dataset:
https://www.kaggle.com/iezepov/gensim-embeddings-dataset 
Sources and what's inside:
Glove: https://nlp.stanford.edu/projects/glove/
Fasttext: https://fasttext.cc/docs/en/english-vectors.html
Paragram: https://cogcomp.org/page/resource_view/106
word2vec on Google news: https://code.google.com/archive/p/word2vec/"	0	9	1	kirderf	embeddingtransfomer
2341	2341	ASAYAR: A Dataset for Arabic-Latin Text Detection 	A dataset for French and Arabic Text & Sign Detection in Highway panels	['software', 'computer vision', 'image data', 'comics and animation']	"ASAYAR
This is a description for the paper: <br>
ASAYAR: A Dataset for Arabic-Latin Scene Text Localization in Highway Traffic Panels<br>
Mohammed Akallouch; Kaoutar Sefrioui Boujemaa; Afaf Bouhoute; Khalid Fardousse; Ismail Berrada
Overview
ASAYAR is the first public dataset dedicated to Latin (French) and Arabic Scene Text Detection in Highway panels. It comprises more than 1800 well-annotated images. The dataset was collected from Moroccan Highway,## ASAYAR
Annotation format
In the dataset, each instance's location is annotated by a rectangle bounding box. The bounding box can be denoted as : <br> {XMIN, YMIN, XMAX, YMAX}. An object has a class name denoted as CLASS. The global image information is defined as follows: FOLDER, PATH, NAME, and SIZE.
Dataset structure
Train or Test/
├── ASAYAR_SIGN/
│   ├── Annotations/
│   │   ├── image_1.xml
│   │   └── ...
│   └── Images
│       ├── image_1.png
│       └── ...
│     
├── ASAYAR_TXT/
│    ├── Annotations/
│    │      ├── Line-Level/
│    │      │     ├── image_1.xml
│    │      │     └── ...
│    │      └── Word-Level/
│    │              ├── image_1.xml
│    │              └── ...
│    └── Images/
│          ├── image_1.png
│          └── ...
└── ASAYAR_SYM/
    ├── Annotations/
    │   ├── image_1.xml
    │   └── ...
    └── Images/
        ├── image_1.png
        └── ...
Import data
We provide a Jupyter Notebook with an example to import images and their annotations. 
Convert to text format
To convert annotations from Voc pascal to txt format (xmin,ymin,xmax,ymax,class) use  convert2txt.py.
Examples of Annotated Images
<img src=""https://vcar.github.io/ASAYAR/images/image_895.png"">
Website
The data website: ASAYAR
Citation
Our paper introducing the dataset and the evaluations methods is published at the IEEE Transactions on Intelligent Transportation Systems 2020 and is available here. If you make use of the ASAYAR dataset, please cite our following paper:
@ARTICLE{9233923,
      author={M. {Akallouch} and K. S. {Boujemaa} and A. {Bouhoute} and K. {Fardousse} and I. {Berrada}},
      journal={IEEE Transactions on Intelligent Transportation Systems}, 
      title={ASAYAR: A Dataset for Arabic-Latin Scene Text Localization in Highway Traffic Panels}, 
      year={2020},
      pages={1-11},
      doi={10.1109/TITS.2020.3029451}}"	5	195	7	akallouch	asayar
2342	2342	test123		[]		0	13	0	minggatsby	test123
2343	2343	COVID19_data	Covid_19 mask mandating and case count data from CDC web site data.cdc.gov/	['health']		0	14	0	suradechk	covid19-data
2344	2344	21czcdf		[]		0	4	0	xianchaoliu	21czcdf
2345	2345	Happy, Neutral and Sad Audio and Image data		['arts and entertainment']		0	9	0	chadgarratt	happy-neutral-and-sad-audio-and-image-data
2346	2346	datakanker		[]		0	12	0	nuruldewi	datakanker
2347	2347	music.csv		['music']		10	24	0	shantanuchaubey	musiccsv
2348	2348	latihandatakanker		[]		0	6	0	nuruldewi	latihandatakanker
2349	2349	Arknights Guard Class Analysis	Analyzing damage per minute of guard class in Arknights.	['games', 'video games', 'exploratory data analysis', 'tabular data', 'anime and manga']	"Context
This dataset is heavily inspired by José Costa's <a href=""https://www.kaggle.com/ze1598/arknights-operator-stats"">Arknights Operator Stats</a>. This simple dataset contains only guard class operators from that dataset.
Content
The dataset comes from José Costa's dataset that I've analyzed so it only contains guard operators only. I create damage per minute analysis and compose this dataset only on their maximum damage per minute number.
Acknowledgements
Thanks to <a href=""https://www.kaggle.com/ze1598""> José Costa </a> to create Arknights data I use for this dataset.
Inspiration
Arknights, a tower defense type game, have caught my attention since its first global launch. It have many operators we can choose according to the stage condition and our gameplay style. But too many choice can lead to overchoice. I made this dataset to help myself and other players who feel overwhelmed by the many operators to be able at least choose which operators they want to raise to maximum."	1	23	0	yafethtb	arknights-guard-class-analysis
2350	2350	leaf-dataset		['biology']		0	10	0	aniketverma19233	leafdataset
2351	2351	Loan Repayment Data		[]		2	12	0	mpkanalytics	loan-repayment-data
2352	2352	data kanker		[]		0	9	0	nuruldewi	data-kanker
2353	2353	Reddit r/conspiracy Moderator Logs		['online communities']		0	30	0	openmodlogs	reddit-rconspiracy-moderator-logs
2354	2354	SST-2 dataset		[]		2	11	0	jkhanbk1	sst2-dataset
2355	2355	horse_data		[]		1	4	0	aniketverma19233	horse-data
2356	2356	lego sets		[]		0	25	0	manojvarmalakkamraju	lego-setss
2357	2357	Position_Salaries_dataset		[]		0	9	0	mdwasimakhtar03	position-salaries-dataset
2358	2358	shahar_walk_data		[]		1	6	0	aniketverma19233	shahar-walk-data
2359	2359	ArcFace + GeM dataset		['earth and nature']		2	18	2	awsaf49	arcface-gem-dataset
2360	2360	Cricket Data		['cricket']		23	17	0	zsinghrahulk	cricket-data
2361	2361	roberta-7		['arts and entertainment']		0	5	0	cdeotte	roberta-7
2362	2362	jigsaw-checkpoints		[]		16	82	0	simonmeoni	jigsawcheckpoints
2363	2363	VOC2007		[]	The original dataset of PascalVoc 2007 for Yolo	0	16	0	rainumdo	voc2007
2364	2364	isic_2019		[]		0	9	0	vitomanuguerra	isic-2019
2365	2365	Los Angeles Crime Data	Reported crime in Los Angeles from 2020 to present	['crime', 'government', 'law', 'tabular data', 'public safety']	"Content
This data is collected on the basis of LAPD radio comms and Investigation done by them after the crime is reported. It contains crime data from 2020 to present(Jan 2022). In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified.
The Dataset includes 20 columns which includes information on areas such as - 
- Record Number
- Date and time at which crime was reported/occurred
- Victim Age,Sex, Ethnicity
- Location where the crime took place
- Weapon of crime
- Status of the case
Acknowledgements
The data was provided by https://data.lacity.org/, y'all can find the original dataset being updated here
banner photo by Jake Blucker
Inspiration
EDA and Visulisation of the cases
Crime rate prediction and analysis
Where and when is mostly the crime commited?
What is the most common timing/date for most crimes to occur
Most common type of crime 
In which Los Angeles Patrol Division(Area) does most crime reported?"	378	2818	29	hemil26	crime-in-los-angeles
2366	2366	IDAO_PowderPattern_Features		[]		0	10	0	anirudhvadakedath	idao-powderpattern-features
2367	2367	Car Buyers	Find out who prefer supercars the most. Men or Women?	['automobiles and vehicles']	"Context
Cars are probably the most favourite means of transport for almost everyone. A nice compact compartment that is private to you and matches your comfort and lets you travel to wherever you want, providing you have the license and the fuel.  So let us find out who finds cars more addicting? Men or Women?
About
Describe the car model, the company, transmission, engine power and related detail. Along with this it also includes stats of the Cars are probably the most favourite means of transport for almost everyone. A nice compact compartment that is private to you and matches your comfort and lets you travel to wherever you want providing you have the license and the fuel.  So let us find out who finds cars more addicting? Men or Women?
Content
Describe the car model, the company, transmission, engine power and related detail. Along with this it also includes a stats of number of male and female customers for the cars.
Acknowledgements
This data has been fetched from https://www.car-data.com/ by means of allowed scrapping of data and I do not hold any proprietory data rights for the same.
Inspiration
The sole inspiration is my passion for CARS"	463	2485	19	brijlaldhankour	car-buyers
2368	2368	Images-for-presentation		[]		0	4	0	williambruno	imagesforpresentation
2369	2369	synth scene tfrecords recog		['music']		0	21	0	riadhossainapsis	synth-scene-tfrecords-recog
2370	2370	Feedback Prize Preprocessing Oversampling		[]		1	9	0	markwijkhuizen	feedback-prize-preprocessing-oversampling
2371	2371	talib-lib		[]		0	6	0	notabene	talib-lib
2372	2372	Big_TreeK		[]		0	5	0	aniketverma19233	big-treek
2373	2373	⛽ Fuel stations: Locations, Metrics and more 	Location of fueling stations, and various metrics associated with these stations 	['business', 'automobiles and vehicles']	"About this dataset
&gt; <p><em>Original Title</em>: Fuel Station Metrics - Columbus 2018 -- Location Analytics v2</p>
<p>This Geotab dataset represents the location of fueling stations, and various metrics associated with these stations including average amount of time spent at the station, popular hours, fuel type, and vehicle type. The dataset is based on the previous 6 months of commercial vehicle data and is updated on a monthly basis. Contact <a href=""mailto:smartcity@geotab.com"" target=""_blank"" rel=""nofollow"">smartcity@geotab.com</a> for more details. This dataset is contributed by Geotab to enable Heartbeat of the City use case for May 2018 hackathon event. Any further queries about this dataset should be routed to Geotab using the Contact email listed below.</p>
<p>Source: <a href=""https://www.geotab.com/"" target=""_blank"" rel=""nofollow"">https://www.geotab.com/</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2018-05-18</p>
This dataset was created by Kelly Garrett and contains around 1000 samples along with Percenthdt, Hourlydistribution, technical information and other features such as:
- Hasgas
- Version
- and more.
How to use this dataset
&gt; - Analyze Updatedate in relation to Country
- Study the influence of Percentmdt on Hasdiesel
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	42	408	5	yamqwe	fuel-station-metrics-columbus-2018-location-anale
2374	2374	talib-lib		[]		0	6	0	notabene	talib-lib
2375	2375	Big_TreeK		[]		0	5	0	aniketverma19233	big-treek
2376	2376	⛽ Fuel stations: Locations, Metrics and more 	Location of fueling stations, and various metrics associated with these stations 	['business', 'automobiles and vehicles']	"About this dataset
&gt; <p><em>Original Title</em>: Fuel Station Metrics - Columbus 2018 -- Location Analytics v2</p>
<p>This Geotab dataset represents the location of fueling stations, and various metrics associated with these stations including average amount of time spent at the station, popular hours, fuel type, and vehicle type. The dataset is based on the previous 6 months of commercial vehicle data and is updated on a monthly basis. Contact <a href=""mailto:smartcity@geotab.com"" target=""_blank"" rel=""nofollow"">smartcity@geotab.com</a> for more details. This dataset is contributed by Geotab to enable Heartbeat of the City use case for May 2018 hackathon event. Any further queries about this dataset should be routed to Geotab using the Contact email listed below.</p>
<p>Source: <a href=""https://www.geotab.com/"" target=""_blank"" rel=""nofollow"">https://www.geotab.com/</a><br>
Last updated at <a href=""https://discovery.smartcolumbusos.com"" target=""_blank"" rel=""nofollow"">https://discovery.smartcolumbusos.com</a> : 2018-05-18</p>
This dataset was created by Kelly Garrett and contains around 1000 samples along with Percenthdt, Hourlydistribution, technical information and other features such as:
- Hasgas
- Version
- and more.
How to use this dataset
&gt; - Analyze Updatedate in relation to Country
- Study the influence of Percentmdt on Hasdiesel
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Kelly Garrett 
Start A New Notebook!"	42	408	5	yamqwe	fuel-station-metrics-columbus-2018-location-anale
2377	2377	Desafio 2 mlp pretreinada		[]		0	3	0	rywgar	desafio-2-mlp-pretreinada
2378	2378	audio_set_seven		[]		2	11	0	xiaojin712	audio-set-seven
2379	2379	my_iris		[]		0	12	0	yanjiaxi	input
2380	2380	Uzbek Languages voices for NLP Enthusiasts 		[]		1	37	11	mohinurabdurahimova	uzbek-languages-voices-for-nlp-enthusiasts
2381	2381	"scRNA-seq ""Tabul Muris"" selected parts"		['genetics', 'biology', 'biotechnology']	"These dataset contains small part cutted from the big data: 
https://www.kaggle.com/alexandervc/scrnaseq-tabula-muris-mouse-85-000-cells
Data and Context
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes (or vice versa).
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Particular data:
""Tabula Muris"" project https://tabula-muris.ds.czbiohub.org/
Tabula Muris is a compendium of single cell transcriptome data from the model organism Mus musculus, containing nearly 100,000 cells from 20 organs and tissues. The data allow for direct and controlled comparison of gene expression in cell types shared between tissues, such as immune cells from distinct anatomical locations. They also allow for a comparison of two distinct technical approaches:
microfluidic droplet-based 3’-end counting: provides a survey of thousands of cells per organ at relatively low coverage
FACS-based full length transcript analysis: provides higher sensitivity and coverage.
We hope this rich collection of annotated cells will be a useful resource for:
Defining gene expression in previously poorly-characterized cell populations.
Validating findings in future targeted single-cell studies.
Developing of methods for integrating datasets (eg between the FACS and droplet experiments), characterizing batch effects, and quantifying the variation of gene expression in a many cell types between organs and animals.
The peer reviewed article describing the analysis and findings is available on Nature.
https://www.nature.com/articles/s41586-018-0590-4
Nature volume 562, pages367–372 (2018)Cite this article
GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE109774
See also tutorials:
Course at Sanger's institute
https://scrnaseq-course.cog.sanger.ac.uk/website/tabula-muris.html
Course at CZ-hub:
https://chanzuckerberg.github.io/scRNA-python-workshop/intro/about
On kaggle - copies of the notebooks and data from the course above
https://www.kaggle.com/aayush9753/singlecell-rnaseq-data-from-mouse-brain
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	0	26	3	alexandervc	scrnaseq-tabul-muris-selected-parts
2382	2382	whale-test-set		['earth and nature']		0	2	0	mehmetzahitangi	whaletestset
2383	2383	happywhale-tfrecords-v1		[]		0	9	1	priyankdl	happywhale-tfrecords-v1
2384	2384	datadictionaries		[]		0	5	0	annaalenaarlt	datadictionaries
2385	2385	Indian Startup Funding Data (2021-2018)	Data of all startup funding's since 2018. Clean dataset included. 	['india', 'business', 'beginner', 'tabular data', 'investing']	"Context
As startups have become the new attraction in India, I decided to analyze the fundings of different startups across industries and locations in India. This dataset is for the same purpose.
Content
There are various columns that you can go through for your analysis. Both, clean as well as the original scrapped data is provided in case you want to clean it yourself.
Acknowledgements
This dataset of obtained from trak.in so thanks to them!"	93	398	10	manomayjamble	startup-funding-clean
2386	2386	deeplabV3		[]		0	5	0	lzm0224	deeplabv3
2387	2387	Breast Cancer Dataset	Detecting Breast Cancer using csv file	['health and fitness', 'healthcare', 'medicine', 'beginner', 'cancer']	"Content
The Dataset contains 32 Columns and 570 rows consisting all the parameters used for detecting a Breast Cancer
Inspiration
The task for you will be predicting that wheter the cancer is Benign or Malignant.
You can also perform Exploratory Data Analysis and Visualize it for practicing"	37	243	3	jainilcoder	breast-cancer-dataset
2388	2388	Financial Dataset Sample		[]		0	3	0	rahimzulfiqarali	financial-dataset-sample
2389	2389	Data Penjualan barang Toko lidya		[]		9	41	1	kadekgunamulyaa	data-penjualan-barang-toko-lidya
2390	2390	Seven_eight_nine		[]		0	18	0	janakklal	seven-eight-nine
2391	2391	addfile		[]		0	18	0	lenacab	addfile
2392	2392	dataDictionary		[]		0	10	0	j8teco	datadictionary
2393	2393	roberta-large		[]		1	18	0	goldenlock	roberta-large
2394	2394	Gesture v1.0	Gesture dataset, from digit 1 to digit 5, it is suitable for object detection.	['earth and nature', 'computer science', 'deep learning', 'neural networks', 'image data']	"<p>
<img src=""https://pic.imgdb.cn/item/61fdd1912ab3f51d912185c7.png"">
</p>

🚀 Gesture v1.0
Founder: Zeng Yifu
🔥 Introduction
The Gesture dataset is an open source computer vision dataset that can be applied to deep learning fields such as object detection.
The dataset was collected by opencv-webcam-script v0.5, capture every 5 frames, 5 categories, from number 1 to number 5. a total of 2500 pictures, and only took 13.55 minutes.
Original dataset file name: gesture_v1_raw
⚡ Annotation
The dataset is labeled with label-studio.
The label format is Pascal VOC XML.
Annotated dataset name: gesture_v1_annotation
💎 Train
Meanwhile, the dataset is trained in YOLOv5.
Dataset name in yolov5 format: gesture_v1_yolov5
🎉 Detection Effect
🎨 Training Loss
🍷 Thanks
YOLOv5Founder: Glenn Jocher
label-studio labeling software"	0	82	0	zenggis	gesture-v1
2395	2395	dataDictionary		[]		1	9	0	lenacab	datadictionary
2396	2396	Breast		['health']		0	4	0	amalianurkahasanah	breast
2397	2397	cancer_data		[]		0	1	0	arinifadhilah	cancer-data
2398	2398	MS in United States Dataset		['earth and nature']		0	9	0	rithikvardhan	ms-in-united-states-dataset
2399	2399	baophong_test_dim1280_yolov5m		[]		2	21	0	mxtson	baophong-test-dim1280-yolov5m
2400	2400	IDS2018		[]		0	3	0	ak8186	ids2018
2401	2401	Covid-19 vaccination worldwide	Covid-19 vaccination worldwide coverage	['public health', 'public safety']		0	7	0	utkarshsaraswat083	covid19-vaccination-worldwide
2402	2402	Cyclistic Bike Share (Case Study)	google data analytics capstone project	['cycling', 'beginner', 'advanced', 'data analytics']	"Introduction
Welcome to the Cyclistic bike-share analysis case study! In this case study, you will perform many real-world tasks of a junior
data analyst. You will work for a fictional company, Cyclistic, and meet different characters and team members.
Scenario
You are a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director
of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore,
your team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights,
your team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives
must approve your recommendations, so they must be backed up with compelling data insights and professional data
visualizations.
Characters and teams
● Cyclistic: A bike-share program that features more than 5,800 bicycles and 600 docking stations. Cyclistic sets itself
apart by also offering reclining bikes, hand tricycles, and cargo bikes, making bike-share more inclusive to people with
disabilities and riders who can’t use a standard two-wheeled bike. The majority of riders opt for traditional bikes; about
8% of riders use the assistive options. Cyclistic users are more likely to ride for leisure, but about 30% use them to
commute to work each day.
● Lily Moreno: The director of marketing and your manager. Moreno is responsible for the development of campaigns
and initiatives to promote the bike-share program. These may include email, social media, and other channels.
● Cyclistic marketing analytics team: A team of data analysts who are responsible for collecting, analyzing, and
reporting data that helps guide Cyclistic marketing strategy. You joined this team six months ago and have been busy
learning about Cyclistic’s mission and business goals — as well as how you, as a junior data analyst, can help Cyclistic
achieve them.
● Cyclistic executive team: The notoriously detail-oriented executive team will decide whether to approve the
recommended marketing program.
Data Source
The data has been made available by Motivate International Inc. under this license.
Dataset download link Click Here"	0	14	0	sayantanbagchi	divvytripdata
2403	2403	Alphafold2_src		[]		0	7	0	victorfernandezalbor	alphafold2-src
2404	2404	final_model_1		[]		0	7	0	kookheejin	final-model-1
2405	2405	ISTANBUL STOCK EXCHANGE	Know what you own, and know why you own it – By Peter Lynch	['universities and colleges', 'business', 'beginner', 'intermediate', 'classification', 'tabular data', 'regression']	"&gt; If you don’t know what you’re looking for, look hard enough and you’ll find it.    ~Yogi Berra
Data Set & Attribute Information:
Data is collected from imkb.gov.tr and finance.yahoo.com. Data is organised with regard to working days in Istanbul Stock Exchange.
Stock exchange returns. Istanbul stock exchange national 100 index, Standard & poorâ€™s 500 return index, Stock market return index of Germany, Stock market return index of UK, Stock market return index of Japan, Stock market return index of Brazil, MSCI European index, MSCI emerging markets index
Data sets includes returns of Istanbul Stock Exchange with seven other international index; SP, DAX, FTSE, NIKKEI, BOVESPA, MSCE_EU, MSCI_EM from Jun 5, 2009 to Feb 22, 2011.
Source:
https://archive.ics.uci.edu/ml/datasets/ISTANBUL+STOCK+EXCHANGE
Dr.Oguz Akbilgic,
oguzakbilgic '@' gmail.com
University of Tennessee, Knoxville"	44	290	9	harikrishnareddyb	istanbul-stock-exchange
2406	2406	Data Professionals Salary - 2022	Salaries of Data Scientists, ML Engineers, Data Analysts, Data Engineers in 2022	['business', 'education', 'computer science', 'beginner', 'data analytics', 'tabular data', 'currencies and foreign exchange']	"Context
Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns towards effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.
Organizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include predictive analytics, prescriptive analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big Data Analytics, retail analytics, supply chain analytics, store assortment and stock-keeping unit optimization, marketing optimization and marketing mix modeling, web analytics, call analytics, speech analytics, sales force sizing and optimization, price and promotion modeling, predictive science, graph analytics, credit risk analysis, and fraud analytics. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics.
Content
This Dataset consists of salaries for Data Scientists, Machine Learning Engineers, Data Analysts, Data Engineers in various cities across India (2022).
Acknowledgements
For more, please visit: https://www.glassdoor.co.in/"	3326	19576	106	iamsouravbanerjee	analytics-industry-salaries-2022-india
2407	2407	datasetsss		[]		0	12	0	kharchoufi	datasetsss
2408	2408	klasifikasi 		[]		0	6	0	muhrifqi	klasifikasi
2409	2409	MICUSP	The Michigan Corpus of Upper-level Student Papers	['research', 'education', 'tabular data']	"About
The Michigan Corpus of Upper-level Student Papers (MICUSP) is a collection of around 830 A-grade papers (roughly 2.6 million words) from a range of disciplines across four academic divisions (Humanities and Arts, Social Sciences, Biological and Health Sciences, Physical Sciences) of the University of Michigan, Ann Arbor. MICUSP was created by a team of researchers and students at the U-M English Language Institute.
For more information on the categories used, click here."	5	82	4	ironicninja	micusp
2410	2410	qiqi voice lines		[]		2	15	0	nhinguyen12345678	qiqi-voice-lines
2411	2411	epoch777		[]		1	18	0	rishengyang	epoch777
2412	2412	TPS Feb pseudolabelling data	pseudolabels are generated from training two ensembled models for 5 folds 	[]		0	14	0	chaudharypriyanshu	febpseudolabelling
2413	2413	155_train01		[]		1	23	0	sakura1986	155-train01
2414	2414	bertweet-hate-speech		[]		0	8	0	dzisandy	bertweethatespeech
2415	2415	Financial Credit Prediction	Financial Credit Prediction	['lending']		3	34	3	syxuming	fin-train
2416	2416	reddit_split1		[]		0	5	0	rajkumargovarthanan	reddit-split1
2417	2417	Business		['business']		1	35	2	aryantiwari123	business
2418	2418	klasifikasi data penyakit jantung		[]		1	12	0	muhrifqi	klasifikasi-data-penyakit-jantung
2419	2419	umbrellas	kaggle umbrellas data downloaded and uploaded for google data analytics exercise	['computer science']		0	7	0	johnsonkongor	umbrellas
2420	2420	DB Style Transfer		[]		0	3	0	shankarmahadevan	dbstyle
2421	2421	dataaaaa		[]		2	22	0	shrideviangadi	dataaaaa
2422	2422	includeprocessed		[]		3	8	0	durveshmalpure	includeprocessed
2423	2423	cassava disease sudah setara 1000 image per gambar		[]		2	15	0	alexlianardo	cassava-disease-sudah-setara-1000-image-per-gambar
2424	2424	finalbasic		[]		0	8	1	shubh07101	finalbasic
2425	2425	librispeech 4-gram language model		[]		0	7	0	tuannguyenvananh	librispeech-4gram-language-model
2426	2426	workshop_2022_day3		[]		0	9	0	bilal1907	workshop-2022-day3
2427	2427	pokemon_example		[]		1	12	1	shubh07101	pokemon-example
2428	2428	My_SmapleDataset		[]		0	6	0	vivekbaaganps	my-smapledataset
2429	2429	covin-19		[]		0	6	0	pf1234	covin19
2430	2430	xxyyzzmm		[]		0	23	1	banbeipi	xxyyzzmm
2431	2431	deepspeech model		['clothing and accessories']		0	19	1	tuannguyenvananh	deepspeech-model
2432	2432	retrained_lstm_attention_baseline		[]		0	5	0	narminj	retrained-lstm-attention-baseline
2433	2433	data global		['business']		0	2	0	widhiwinata	data-global
2434	2434	PreTrained SOTA Models		['retail and shopping']		0	4	0	shantanupatankar	pretrained-sota-models
2435	2435	Google 2019 Cluster sample	Google Cluster Workload Traces 2019	['internet']	"From https://research.google/tools/datasets/google-cluster-workload-traces-2019/ 
his is a trace of the workloads running on eight Google Borg compute clusters for the month of May 2019. The trace describes every job submission, scheduling decision, and resource usage data for the jobs that ran in those clusters.
It builds on the May 2011 trace of one cluster, which has enabled a wide range of research on advancing the state-of-the-art for cluster schedulers and cloud computing, and has been used to generate hundreds of analyses and studies.
Since 2011, machines and software have evolved, workloads have changed, and the importance of workload variance has become even clearer. The new trace allows researchers to explore these changes. The new dataset includes additional data, including:
CPU usage information histograms for each 5 minute period, not just a point sample;
information about alloc sets (shared resource reservations used by jobs); and
job-parent information for master/worker relationships such as MapReduce jobs.
Just like the last trace, these new ones focus on resource requests and usage, and contain no information about end users, their data, or access patterns to storage systems and other services.
The trace data is being made available via Google BigQuery so that sophisticated analyses can be performed without requiring local resources. This site provides access instructions and a detailed description of what the traces contain.
https://drive.google.com/file/d/10r6cnJ5cJ89fPWCgj7j4LtLBqYN9RiI9/view"	6	86	7	derrickmwiti	google-2019-cluster-sample
2436	2436	linear_pkl		[]		0	12	0	banbeipi	linear-pkl
2437	2437	aug_cot		[]		1	18	1	zhuhaoqi	aug-cot
2438	2438	5 Letter Words		[]		1	6	0	ryanleland	5-letter-words
2439	2439	wbf-20220204		['card games']		0	26	1	haqishen	wbf20220204
2440	2440	covin-19		[]		0	16	0	pf1234	covin-19
2441	2441	yolov5-20220204		[]		0	19	0	haqishen	yolov520220204
2442	2442	songs12		[]		0	5	0	debashishghosh12	songs12
2443	2443	model2		[]		3	30	0	freeyjq	model22
2444	2444	tfrecords-1001		[]		0	1	0	bamps53	tfrecords-1001
2445	2445	MNIST PICKLE Dataset		['computer science']		2	2	3	towhidultonmoy	mnist-pickle-dataset
2446	2446	Classification with knn		[]		0	14	0	mpkanalytics	classification-with-knn
2447	2447	Classified Data		[]		0	6	0	mpkanalytics	classified-data
2448	2448	happywhale-tfrecords-v1-ifti		[]		0	10	0	iftiben10	happywhale-tfrecords-v1-ifti
2449	2449	en_dictionary		[]		0	11	0	ajenningsfrankston	en-dictionary
2450	2450	NASA Project; Marine Debris Machine Learning	NASA Project; Plastic Marine Debris Classification-Machine Learning Software	['earth and nature', 'earth science', 'science and technology', 'artificial intelligence', 'computer science', 'programming']	"NASA Project; Plastic Marine Debris Classification-Machine Learning Software
Github: https://github.com/emirhanai/NASA-Project-Plastic-Marine-Debris-Classification-Machine-Learning-Software"	51	827	3	emirhanai	nasa-project-marine-debris-machine-learning
2451	2451	1,000 most-followed Instagram accounts in USA	top Instagram influencers by topics of influence	['marketing', 'beginner', 'exploratory data analysis', 'social networks']	"Context
Discover 1000 Top Ranked Influencers by Type and Category of Influence in United States.
Acknowledgements
Data source: https://starngage.com/app/global/influencer/ranking/united-states"	21	143	6	prasertk	1000-mostfollowed-instagram-accounts-in-usa
2452	2452	Death Cause by Country	Death Reasons by Country	['healthcare', 'public health', 'health', 'health conditions', 'covid19']	"Context
Across low- and middle-income countries, mortality from infectious disease, malnutrition, nutritional deficiencies, neonatal and maternal deaths are common – and in some cases, dominant. In Kenya, for example, diarrheal infections are still the primary cause of death. HIV/AIDS is the major cause of death in South Africa and Botswana. However, in high-income countries, the proportion of deaths due by these causes is quite low.
Content
The dataset contains thirty two columns and contains the death causes by All Genders (Male, Female) and by all age group.
Acknowledgements
Users are allowed to use, copy, distribute and cite the dataset as follows: “Majyhain, Death Causes by Country, Kaggle Dataset, February 04, 2022.”
Inspiration
The ideas for this data is to:
•   The amount of people dying by various diseases.
•   What is the death cause reasons by country.
•   Number of People dying by various diseases.
•   Which disease is causing more deaths by country.
•   Which disease is causing more deaths by world.
References:
The Data is collected from the following sites:
https://www.who.int/"	656	4489	32	majyhain	death-cause-by-country
2453	2453	Yolo_input		[]		0	8	0	laliali	yolo-input
2454	2454	roberta-base-422-drive		['arts and entertainment']		0	8	0	suecx9	robertabase422drive
2455	2455	detonly_sartorius_cascade_pretrainLivecTestAsTrain		[]		0	7	0	itaynivtau	detonly-sartorius-cascade-pretrainlivectestastrain
2456	2456	jigsaw-batch-tfr-roberta-base		['software']		0	17	1	ks2019	jigsaw-batch-tfr-roberta-base
2457	2457	Warthunder Manual Kill-Death Record	Manual listing of kills and deaths in War Thunder.	['games', 'categorical data', 'tabular data', 'text data']	"Context
This data is made for personal use. WarThunder's developer, Gaijin Games, does not have specific statistics about players. Unlike Dota 2 or League of Legends, there are very few granular comparative statistics. For example, Dota 2 and LoL both show how much gold you earn and your relative performance at certain ""skills"" compared to others. You may sort based on what ""heroes"" or ""champions"" you use. WarThunder does not have these features. You may get a list of your total ""ground vehicle"" kills but cannot see exactly which ""vehicles"" you have killed or been killed by. Imagine Dota and LoL not specifying which character you use. Each character has different abilities just like the vehicles of WarThunder. 
Content
Details about kills and deaths are manually typed into a text file following arbitrary conventions. A Python script generates a CSV file based on the text files. It is important to note that the ""orientation"" column is very subjective. It describes the apparent position of the vehicle that got shot. If it is a ""kb"" event, this column refers to the ""player vehicle"". If it is a ""k"" event, the orientation refers to the ""enemy vehicle."" The possible values are Diag or Diagonal, Perp (Perpendicular), Para (parallel), Near-perp (nearly perpendicular), and Near-para (nearly parallel). The near-perp and near-para values could be troublesome. The orientation is noted as near-parallel if it appears to be parallel when seen through the sight but the kill camera shows that the vehicle hit is actually slightly turned to the left or right. Similarly, a near-perp listing means the vehicle hit looked like it was ninety degrees to the ""shooter"". The kill camera then shows a slight deviation. This can cause dangerous amounts of human error in the data.
Purpose
The purpose for this data is simple: for each vehicle in WarThunder, at what distances and speeds do they function best? Hopefully, in the future, a relative comparison will become possible. Comparison to other players will make the data more meaningful. At the moment, all this data only applies to one player. There isn't much to be learned from a single sample."	18	689	0	joachimrives	warthunder-manual-killdeath-record
2458	2458	Blood_Sugar	Blood Sugar Dataset 768	[]		2	25	0	ziadfellahidrissi	blood-sugar
2459	2459	Ômicronvirus in UK region	In this project are experimented some comands before regressionAnalisys. 	[]		0	21	0	orlandosantos	micronvirus-in-uk-region
2460	2460	tx-crdf-off-rbrt		[]		0	1	0	datafan07	txcrdfoffrbrt
2461	2461	jigsaw_cleaned_dataV5		[]		0	4	0	moniquebadjemaa	jigsaw-cleaned-datav5
2462	2462	Acórdãos TCU	"Dados do livro ""Jurimetria Aplicada aos Tribunais de Contas"""	[]		0	24	0	marcosfs2006	acordaos-tcu
2463	2463	cs_machine_learning		[]		0	11	0	marwanbattach	cs-machine-learning
2464	2464	Mask-Detector	Dataset that consists ~150 images of people with and without mask	['health', 'public safety']		20	18	0	danielmalky	mask-detector
2465	2465	ECG_dataset		[]		0	3	0	aamiradam	ecg-dataset
2466	2466	EEG_dataset		[]		0	4	0	aamiradam	eeg-dataset
2467	2467	ensemble_boxes_git		[]		0	7	0	itaynivtau	ensemble-boxes-git
2468	2468	With_buildings_data		[]		0	4	0	vladimirmynka	with-buildings-data
2469	2469	With_Buildings		[]		0	5	0	vladimirmynka	with-buildings
2470	2470	Titanic dataset for beginner		['history']		2	10	0	jatinakad	titanic-dataset-for-beginner
2471	2471	news_trail		[]		0	6	0	axword	news-trail
2472	2472	"scRNA-seq ""Tabula muris"" - mouse, 85 000+ cells"	85 000+ cells from 20 distinct organs and tissues collected from 7 mices	['genetics', 'biology', 'biotechnology']	"Data and Context
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes (or vice versa).
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Particular data:
""Tabula Muris"" project https://tabula-muris.ds.czbiohub.org/
Tabula Muris is a compendium of single cell transcriptome data from the model organism Mus musculus, containing nearly 100,000 cells from 20 organs and tissues. The data allow for direct and controlled comparison of gene expression in cell types shared between tissues, such as immune cells from distinct anatomical locations. They also allow for a comparison of two distinct technical approaches:
microfluidic droplet-based 3’-end counting: provides a survey of thousands of cells per organ at relatively low coverage
FACS-based full length transcript analysis: provides higher sensitivity and coverage.
We hope this rich collection of annotated cells will be a useful resource for:
Defining gene expression in previously poorly-characterized cell populations.
Validating findings in future targeted single-cell studies.
Developing of methods for integrating datasets (eg between the FACS and droplet experiments), characterizing batch effects, and quantifying the variation of gene expression in a many cell types between organs and animals.
The peer reviewed article describing the analysis and findings is available on Nature.
https://www.nature.com/articles/s41586-018-0590-4
Nature volume 562, pages367–372 (2018)Cite this article
GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE109774
See also tutorials:
Course at Sanger's institute
https://scrnaseq-course.cog.sanger.ac.uk/website/tabula-muris.html
Course at CZ-hub:
https://chanzuckerberg.github.io/scRNA-python-workshop/intro/about
On kaggle - copies of the notebooks and data from the course above
https://www.kaggle.com/aayush9753/singlecell-rnaseq-data-from-mouse-brain
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	4	240	1	alexandervc	scrnaseq-tabula-muris-mouse-85-000-cells
2473	2473	Line detection using CV		['jobs and career']		2	9	0	yashsethi24	line-detection-using-cv
2474	2474	coins 500x500		[]		30	48	0	miguelangelmig	coins-500x500
2475	2475	Denver Police Pedestrian Stops and Vehicle Stops	Denver Police Stops (updated monthly)	['crime', 'public safety']	"Context
The city of Denver, CO has an Open Data Catalog: https://www.denvergov.org/opendata/
Content
Police Pedestrian Stops and Vehicle Stops
https://www.denvergov.org/opendata/dataset/city-and-county-of-denver-police-pedestrian-stops-and-vehicle-stops
This dataset includes person and vehicle stops by the Denver Police Department from the Computer Aided Dispatch system for the previous four calendar years and the current year to date. Data without a valid address is excluded from the dataset. This data is updated Monday through Friday.Disclaimer----------The information provided here regarding public safety in Denver are offered as a courtesy by the City and County of Denver. By downloading this data, you acknowledge that you have read and understand the Disclaimer below and agree to be bound by it. All materials contained on this site are distributed and transmitted ""as is,"" without any representation as to completeness or accuracy and without warranty or guarantee of any kind. The City and County of Denver is not responsible for any error or omission on this site or for the use or interpretation of the results of any research conducted here.
Acknowledgements
Author: Denver Police Department | Data Analysis Unit
Maintainer: City and County of Denver, Technology Services / Enterprise Data Management
Maintainer Email: denvergis@denvergov.org
Disclaimer
ACCESS CONSTRAINTS:
None. The information provided here regarding public safety in Denver are offered as a courtesy by the City and County of Denver. By downloading this data, you acknowledge that you have read and understand the Disclaimer below and agree to be bound by it. All materials contained on this site are distributed and transmitted ""as is,"" without any representation as to completeness or accuracy and without warranty or guarantee of any kind. The City and County of Denver is not responsible for any error or omission on this site or for the use or interpretation of the results of any research conducted here.
USE CONSTRAINTS: The City and County of Denver is not responsible and shall not be liable to the user for damages of any kind arising out of the use of data or information provided by the City and County of Denver, including the installation of the data or information, its use, or the results obtained from its use.
ANY DATA OR INFORMATION PROVIDED BY THE City and County of Denver IS PROVIDED ""AS IS"" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. Data or information provided by the City and County of Denver shall be used and relied upon only at the user's sole risk, and the user agrees to indemnify and hold harmless the City and County of Denver, its officials, officers and employees from any liability arising out of the use of the data/information provided.
NOT FOR ENGINEERING PURPOSES
You are free to copy, distribute, transmit and adapt the data in this catalog under an open license.
For details, please review the terms of use: https://www.denvergov.org/opendata/termsofuse
Photo by Matt Popovich on Unsplash
https://www.denvergov.org/opendata/dataset/city-and-county-of-denver-police-pedestrian-stops-and-vehicle-stops"	1064	18530	45	paultimothymooney	police-pedestrian-stops-and-vehicle-stops
2476	2476	Minneapolis Police Stops and Police Violence	Updated weekly (with automatic updates)	['crime', 'social issues and advocacy', 'racial equity']	"Context
Minneapolis was the location of a famous incident of police violence: https://en.wikipedia.org/wiki/Killing_of_George_Floyd.
Content
Minneapolis Police Stop Data
callDisposition 
citationIssued 
gender 
lastUpdateDate 
lat 
long 
masterIncidentNumber 
neighborhood 
OBJECTID 
personSearch 
policePrecinct 
preRace 
problem 
race 
reason 
responseDate 
vehicleSearch 
x 
y
Minneapolis Police Use of Force Data
CaseNumber 
CenterGBSID 
CenterLatitude 
CenterLongitude 
CenterX 
CenterY 
DateAdded 
EventAge 
ForceReportNumber 
ForceType 
ForceTypeAction 
Is911Call 
Neighborhood 
OBJECTID 
PoliceUseOfForceID 
Precinct 
PrimaryOffense 
Problem 
Race 
ResponseDate 
Sex 
SubjectInjury 
SubjectRole 
SubjectRoleNumber 
TotalCityCallsForYear 
TotalNeighborhoodCallsForYear 
TotalPrecinctCallsForYear 
TypeOfResistance
Acknowledgements
Data from http://opendata.minneapolismn.gov/datasets/police-stop-data and http://opendata.minneapolismn.gov/datasets/police-use-of-force.
Minneapolis Data by City of Minneapolis is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License
Banner Photo by Steijn Leijzer on Unsplash"	500	9394	34	paultimothymooney	minneapolis-police-stops-and-police-violence
2477	2477	Google_Certificate_Capstone	Cyclistic project for earning Google data analytic certificate	['business', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	"Context
This capstone was the last step of earning ""Google data Analytics Certificate"" by Coursera.
Content
Skills I used to do this project include: Data cleaning, problem solving, critical thinking, data ethics, and data visualization.
Acknowledgements
Thank you Google and Coursera for providing me with different skills regarding data analysis.
Inspiration
Managing big data is a kind of world !"	2	44	1	pooryakoochebaghy	google-certificate-capstone
2478	2478	model123		[]		0	9	0	mohamedardif	model123
2479	2479	Inverted_index		[]		0	6	0	mkimiti	inverted-index
2480	2480	Wine Caracteristics	These data are the result of a chemical analysis of wines grown	['alcohol', 'north america', 'intermediate', 'classification', 'IPython']	"These data are the result of a chemical analysis of wines grown in the same region of Italy, but derived from three different cultivars. The analysis determined the amounts of 13 constituents found in each of the three types of wines. 
I think the initial dataset had around 30 variables, but for some reason I only have the 6 dimensional version. I think they are the most important for an academic study
target
alcohol level
malic acid
Alkalinity
color intensity
Proline
Hue"	6	76	1	michaelmarshal1	datasetprioricaracteristics
2481	2481	alchemist_trained 		[]		0	2	0	ishanharshvardhan	alchemist-trained
2482	2482	5 fold rob base nbme		['standardized testing']		10	30	2	nbroad	5f-rob-b-nbme
2483	2483	wine-quality		['alcohol']		1	18	0	priyanshusahu23	winequality
2484	2484	employee_practice		[]		0	7	0	justinspears	employee-practice
2485	2485	San Francisco COVID-19 Data	Race, Ethnicity, Age, and Gender (cases and hospitalizations)	['public health', 'online communities', 'covid19']	"Context
COVID-19 data for San Francisco from data.sfgov.org
Content
COVID-19 Hospitalizations
COVID-19 Cases Summarized by Date, Transmission and Case Disposition
COVID-19 Cases Summarized by Race and Ethnicity
COVID-19 Cases Summarized by Age Group and Gender
COVID-19 Tests
Acknowledgements
Data from https://data.sfgov.org/analytics and
https://data.sfgov.org/COVID-19/COVID-19-Cases-Summarized-by-Age-Group-and-Gender/sunc-2t3k and
https://data.sfgov.org/COVID-19/COVID-19-Tests/nfpa-mg4g and
https://data.sfgov.org/COVID-19/COVID-19-Hospitalizations/nxjg-bhem and
https://data.sfgov.org/COVID-19/COVID-19-Cases-Summarized-by-Date-Transmission-and/tvq9-ec9w and
https://data.sfgov.org/COVID-19/COVID-19-Cases-Summarized-by-Race-and-Ethnicity/vqqm-nsqg
Dataset license: https://datasf.org/opendata/terms-of-use/
Banner Photo by Maarten van den Heuvel on Unsplash"	158	8934	17	paultimothymooney	san-francisco-covid19-data
2486	2486	Penguins (for use instead of iris dataset)	Penguins dataset that can be used instead of the iris dataset	['earth and nature']		2	18	1	orcunizmirli	penguins-for-use-instead-of-iris-dataset
2487	2487	Fashion-image-classification-dataset	image-classification	['clothing and accessories']		3	46	2	shubhamsindal0098	fashionimageclassificationdataset
2488	2488	Forest Fire - C4	This Dataset contains 4 Classes of Forest Fire	['education', 'deep learning', 'image data']	"Check out the Live Working Sample 👇
Click Here 👈🏻"	4	19	0	obulisainaren	forest-fire-c4
2489	2489	no_show_data		[]		0	9	0	yasmineshaker	no-show-data
2490	2490	data_q1_q2		[]		0	5	0	aniketverma19233	data-q1-q2
2491	2491	einops		[]		0	5	0	yushkevich	einops
2492	2492	sbermarket_competition		[]		0	21	0	olgayaremchuk	sbermarket-competition
2493	2493	without-buildings		['arts and entertainment']		0	10	1	vladimirmynka	withoutbuildings
2494	2494	Without_buildings		[]		0	5	0	vladimirmynka	without-buildings
2495	2495	US Natural Disaster Declarations	County-level data from the Federal Emergency Management Agency: 1953 - today	['natural disasters', 'government', 'public safety', 'covid19']	"Context
The United States experience a large variety of natural disasters each year: devastating hurricanes, seasonal tornadoes, and scorching wild fires are among the events that endanger many lifes and cause billions of dollars in damages. The Federal Emergency Management Agency (FEMA) is in charge of ""helping people before, during, and after disasters"" by coordinating disaster response and providing relief funds (see also wikipedia). I will update this dataset regularly.
Note, that the data also includes biological disasters, in particular declarations made in response to the ongoing Covid-19 pandemic.
Content
This summary dataset is a high-level summary of all federally declared disasters since 1953. I downloaded it from the FEMA website and applied a few simple data cleaning and formatting measures. The features of the main dataset will be described in detail right below. In addition, I provide a sub-dataset that is tailored to the parameters of the ongoing M5 Forecasting competition; which is outlined below the main one. Some example for using the data in the M5 competition are given in my EDA Kernel.
us_disaster_declarations.csv: the full dataset with all rows and columns. The geographical resolution is the county level, with FIPS codes being used to encode the counties. In addition to the fips and timing features, the data provides the type of disaster and also binary flags that indicate whether specific aid programs were triggered in response.
us_disasters_m5.csv: the M5-specific subset. Constrained to the 3 states CA, TX, and WI; as well as to the time range of Jan 2011 - June 2016. I also removed a few columns that I consider unnecessary for this more tailored analysis.
Column Description
Most of those descriptions have been taken verbatim from the FEMA website. I added small clarifications to some of them:
Full dataset us_disaster_declarations.csv:
fema_declaration_string: Agency standard method for uniquely identifying Stafford Act declarations. Concatenation of declaration_type, disaster_number and state. 
disaster_number: Sequentially assigned number used to designate an event or incident declared as a disaster.
state: US state, district, or territory.
declaration_type: One of ""DR"" (= major disaster), ""EM"" (= emergency management), or ""FM"" (= ""fire management"")
declaration_date: Date the disaster was declared.
fy_declared: Fiscal year in which the disaster was declared.
incident_type: Type of incident such as ""Fire"", ""Flood"", or ""Hurricane"". The incident type will affect the types of assistance available. 
declaration_title:  Title for the disaster. This can be a useful identifier such as ""Hurricane Katrina"" or ""Covid-19 Pandemic"".
ih_program_declared: Binary flag indicating whether the ""Individuals and Households program"" was declared for this disaster.
ia_program_declared: Binary flag indicating whether the ""Individual Assistance program"" was declared for this disaster.
pa_program_declared: Binary flag indicating whether the ""Public Assistance program"" was declared for this disaster.
hm_program_declared: Binary flag indicating whether the ""Hazard Mitigation program"" was declared for this disaster.
incident_begin_date: Date the incident itself began.
incident_end_date: Date the incident itself ended. This feature has about 14% NA entries.
disaster_closeout_date: Date all financial transactions for all programs are completed. This column has 98% NA entries.
fips: 5-digit FIPS county code; used to identify counties and county equivalents in the United States, the District of Columbia, US territories, outlying areas of the US and freely associated states. Concatenated from the 2 source columns ""fipsStateCode"" and ""fipsCountyCode"".
place_code: A unique code system FEMA uses internally to recognize locations that takes the numbers '99' + the 3-digit county FIPS code. There are some declared locations that don't have recognized FIPS county codes in which case a unique identifier was assigned.
designated_area: The name or phrase describing the U.S. county that was included in the declaration. Can take the value ""Statewide"".
declaration_request_number: Unique ID assigned to request for a disaster declaration.
hash: MD5 Hash of the fields and values of the record.
last_refresh: Date the record was last updated by FEMA.
id: Unique ID assigned to the record. Those last 4 columns are primarily for bookkeeping.
M5-specific dataset us_disasters_m5.csv:
disaster_number: Sequentially assigned number used to designate an event or incident declared as a disaster.
state: One of the three states ""CA"", ""TX"", or ""WI"".
declaration_type: One of ""DR"" (= major disaster), ""EM"" (= emergency management), or ""FM"" (= ""fire management"")
declaration_date: Date the disaster was declared.
incident_type: Type of incident such as ""Fire"", ""Flood"", or ""Biological"". The incident type will affect the types of assistance available. 
declaration_title:  Title for the disaster.
ih_program_declared: Binary flag indicating whether the ""Individuals and Households program"" was declared for this disaster.
ia_program_declared: Binary flag indicating whether the ""Individual Assistance program"" was declared for this disaster.
pa_program_declared: Binary flag indicating whether the ""Public Assistance program"" was declared for this disaster.
hm_program_declared: Binary flag indicating whether the ""Hazard Mitigation program"" was declared for this disaster.
incident_begin_date: Date the incident itself began.
incident_end_date: Date the incident itself ended. The M5 constraint is that the disaster must have begun before the start of the evaluation time period on 2016-05-23 and ended on or after the begin of the training data range on 2011-01-29. This feature has about 14% NA entries, which were considered under the assumption that start data = end date.
fips: 5-digit FIPS county code; used to identify counties and county equivalents in the United States, the District of Columbia, US territories, outlying areas of the US and freely associated states. Concatenated from the 2 source columns ""fipsStateCode"" and ""fipsCountyCode"".
designated_area: The name or phrase describing the U.S. county that was included in the declaration. 
Acknowledgements
All the credit goes to FEMA for building and maintaining this dataset.
Banner and vignette photo by Kelly Sikkema on Unsplash.
Licence
Data and content created by government employees within the scope of their employment are not subject to domestic copyright protection under 17 U.S.C. § 105. Government works are by default in the U.S. Public Domain."	1619	15217	83	headsortails	us-natural-disaster-declarations
2496	2496	fashion-outfit-items	5 categories and 13 sub-categories of clothing items and accessories	['clothing and accessories']	"The dataset contains over 48k images of 5 categories further divided into a total of 13 sub categories for fashion items.
Categories:
* Upperwear 
* Bottomwear
* Footwear
* One-piece
* Accessories 
Sub-categories:
* Shirt
* T-shirt
* Jacket
* Pants
* Skirt
* Shorts
* Shoes
* Sneakers
* Heels
* Flats
* Dress
* Hats
* Bags"	13	42	1	kritanjalijain	fashionoutfititems
2497	2497	Fasttext Word Embeddings		['computer science']		15	7	0	aishikai	fasttext-word-embeddings
2498	2498	reef-yolox-m-v12		[]		1	4	0	sangayb	reef-yolox-m-v12
2499	2499	cocomosaic		[]		0	5	0	mhdslhi	cocomosaic
2500	2500	cocomain		[]		0	5	0	mhdslhi	cocomain
2501	2501	setttt		['art', 'africa', 'tabular data']		2	16	0	lyzahamadache	setttt
2502	2502	Ecomm-SBERT		[]		10	14	0	maunish	ecommsbert
2503	2503	ocrlds		[]		70	464	0	tombutton	ocrlds
2504	2504	RSVP Movies case study	Analyzing IMDB dataset for RSVP movies case study	['movies and tv shows', 'data analytics', 'sql']	"Problem Statement:
RSVP Movies is an Indian film production company which has produced many super-hit movies. They have usually released movies for the Indian audience but for their next project, they are planning to release a movie for the global audience in 2022.
The production company wants to plan their every move analytically based on data and have approached you for help with this new project. You have been provided with the data of the movies that have been released in the past three years. You have to analyse the data set and draw meaningful insights that can help them start their new project. 
Steps to follow(Database creation and analysis):
•   Download the IMDb dataset from above.
•   The first tab contains the ERD and the table details. Study that carefully and understand the relationships between the table.
•   Inspect each table given in the subsequent tabs and understand the features associated with each of them.
•   Open your MySQL Workbench and start writing the DDL and DML commands to create the database.
Files:
Script with DDL commands to import the dataset.
Excel workbook with ERD and data of all the tables.
Solution script.
Executive summary."	1	23	0	blitzapurv	rsvp-case-study
2505	2505	astronauts-mission	Analysing Astronaut and Space mission Dataset	['arts and entertainment']		16	186	2	aashishchaubey	astronautsmission
2506	2506	nytimes covid-19 data	cumulative counts of US COVID-19 cases over time at the state and county level	['politics', 'covid19']	"Context
NYTIMES: Coronavirus (Covid-19) Data in the United States
https://github.com/nytimes/covid-19-data
Content
The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time.  Because of the widespread shortage of testing, however, the data is necessarily limited in the picture it presents of the outbreak.  The data begins with the first reported coronavirus case in Washington State on Jan. 21, 2020. We will publish regular updates to the data in this repository.
Acknowledgements
Banner Photo by Ashkan Forouzani on Unsplash
License and Attribution
Data source: https://github.com/nytimes/covid-19-data.
Data license: https://github.com/nytimes/covid-19-data/blob/master/LICENSE
If you use this data, you must attribute it to “The New York Times” in any publication. If you would like a more expanded description of the data, you could say “Data from The New York Times, based on reports from state and local health agencies.”  https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html."	515	11698	18	paultimothymooney	nytimes-covid19-data
2507	2507	GBR Starfish TFRecords Mini3 1X 1 1		['sports']		0	6	5	mmelahi	gbr-starfish-tfrecords-mini3-1x-1-1
2508	2508	GBR Starfish TFRecords Mini3 1X 1 0		['sports']		0	1	1	mmelahi	gbr-starfish-tfrecords-mini3-1x-1-0
2509	2509	041_exp		[]		0	4	0	shimishige	041-exp
2510	2510	GBR Starfish TFRecords Mini3 1X 0 0		['sports']		0	4	0	mmelahi	gbr-starfish-tfrecords-mini3-1x-0-0
2511	2511	GBR Starfish TFRecords Mini3 1X 0 1		['sports']		0	0	0	mmelahi	gbr-starfish-tfrecords-mini3-1x-0-1
2512	2512	GBR Starfish TFRecords Mini3 1X 2 0		['sports']		0	1	0	mmelahi	gbr-starfish-tfrecords-mini3-1x-2-0
2513	2513	GBR Starfish TFRecords Mini3 1X 2 1		['sports']		0	1	0	mmelahi	gbr-starfish-tfrecords-mini3-1x-2-1
2514	2514	GBR Starfish TFRecords Mini3 1X 2 2		['sports']		0	0	0	mmelahi	gbr-starfish-tfrecords-mini3-1x-2-2
2515	2515	GBR Starfish TFRecords Mini3 1X 1 2		['sports']		0	1	2	mmelahi	gbr-starfish-tfrecords-mini3-1x-1-2
2516	2516	GBR Starfish TFRecords Mini3 1X 0 2		['sports']		0	1	2	mmelahi	gbr-starfish-tfrecords-mini3-1x-0-2
2517	2517	jigsaw-deberta-v3-1st		['puzzles']		2	16	0	trongminhle	jigsawdebertav31st
2518	2518	myprojecttfrecords		[]		0	23	0	raviteja124536	myprojecttfrecords
2519	2519	Extensive products data		[]		4	26	0	mdhamani	extensive-products-data
2520	2520	2021DriversDB		[]		0	11	1	piotrkazala	2021driversdb
2521	2521	Wordle 5 Letter Words	Analyze word probability by character and position	['games', 'linguistics', 'beginner', 'data visualization', 'text data']	"Context
2499 five letter words. Each row is a separate word, with each letter assigned to a column.
Source
https://eslforums.com/5-letter-words/"	31	504	7	cprosser3	wordle-5-letter-words
2522	2522	deepfake and real images		[]		5	38	0	manjilkarki	deepfake-and-real-images
2523	2523	XLNET_MUL_NO_CLEAN_TRY_2		[]		0	5	0	hangy132	xlnet-mul-no-clean-try-2
2524	2524	HappyWhale Supervised Models		[]		0	25	0	ayuraj	happywhale-supervised
2525	2525	Scraped Auto Data		[]		1	13	0	atharvaj9	scraped-auto-data
2526	2526	trainprice		[]		0	1	0	sleepysanjinlee	trainprice
2527	2527	LitCovid	A curated literature hub containing information about COVID-19	['literature', 'nlp', 'text mining', 'covid19']	"Context
This dataset contains full-text research COVID-19 research articles.
Content
LitCovid is a curated literature hub for tracking up-to-date scientific information about the 2019 novel Coronavirus. It is the most comprehensive resource on the subject, providing a central access to 18076 (and growing) relevant articles in PubMed. The articles are updated daily and are further categorized by different research topics and geographic locations for improved access.  For more info see https://www.nature.com/articles/d41586-020-00694-1, https://www.ncbi.nlm.nih.gov/research/coronavirus/, and https://www.ncbi.nlm.nih.gov/research/coronavirus/faq.
The LitCovid project is very similar to the CORD-19 project, but only contains COVID-19 articles published in 2020 (whereas CORD-19 also contains articles about additional coronaviruses and articles that were published prior to 2020). Here are a few key differences between LitCovid and CORD-19: LitCovid uses the following search terms on Pubmed only (""coronavirus""[All Fields] OR ""ncov""[All Fields] OR ""cov""[All Fields] OR ""2019-nCoV""[All Fields] OR ""COVID-19""[All Fields] OR ""SARS-CoV-2""[All Fields]) and then they manually go through and discard irrelevant articles. Whereas CORD-19 using the following search terms on Pubmed+PMC+WHO+bioRxiv+medRxiv (""COVID-19"" OR Coronavirus OR ""Corona virus"" OR ""2019-nCoV"" OR ""SARS-CoV"" OR ""MERS-CoV"" OR “Severe Acute Respiratory Syndrome” OR “Middle East Respiratory Syndrome”) but then there is no manual effort taken to discard irrelevant articles.
Acknowledgements
Data from https://www.nature.com/articles/d41586-020-00694-1, https://www.ncbi.nlm.nih.gov/research/coronavirus/, and https://www.ncbi.nlm.nih.gov/research/coronavirus/faq.
Photo by Glen Carrie on Unsplash
Chen Q, Allot A, Lu Z. Keep up with the latest coronavirus research. Nature. 2020;579(7798):193."	83	7264	16	paultimothymooney	litcovid
2528	2528	FiveEight Comic Charachter		[]		0	5	0	antonmw	fiveeight-comic-charachter
2529	2529	Happywhale 2022 image dims	Original image dimensions	['tabular data']		2	49	0	greendolphin	happywhale-2022-image-dims
2530	2530	PRImA Research		['earth and nature']	datasets were collected from  : https://www.primaresearch.org/datasets	0	17	0	haque1407028	prima-research
2531	2531	Test Match Declarations	Target-setting declarations in the third innings of the match	['cricket', 'sports']		16	204	10	jbomitchell	test-match-declarations
2532	2532	ext_tx_txcrbrta		[]		0	2	0	datafan07	ext-tx-txcrbrta
2533	2533	mmdet-v2-20		[]		0	21	0	clwwlc	mmdet-v2-20
2534	2534	image-test-for-labels		['arts and entertainment']		0	9	0	williambruno	imagetestforlabels
2535	2535	Titanic		[]		1	4	6	mohinurabdurahimova	titanic
2536	2536	Coronavirus in Italy (COVID-19)	Coronavirus (COVID-19) cases in Italy by region and by province	['covid19']	"Context
To inform citizens and make the collected data available, the Department of Civil Protection has developed an interactive geographic dashboard accessible at the addresses http://arcg.is/C1unv (desktop version) and http://arcg.is/081a51 (mobile version) and makes available, with CC-BY-4.0 license, the following information updated daily at 18:30 (after the Head of Department press conference).  For more detail, see https://github.com/pcm-dpc/COVID-19.
Content
COVID-19 data Italy
National trend
Json data
Provinces data
Regions data
Summary cards
Areas
Repository structure
COVID-19 /
│
├── national-trend /
│ ├── dpc-covid19-eng-national-trend-yyyymmdd.csv
├── areas /
│ ├── geojson
│ │ ├── dpc-covid19-ita-aree.geojson
│ ├── shp
│ │ ├── dpc-covid19-eng-areas.shp
├── data-provinces /
│ ├── dpc-covid19-ita-province-yyyymmdd.csv
├── data-json /
│ ├── dpc-covid19-eng - *. Json
├── data-regions /
│ ├── dpc-covid19-eng-regions-yyyymmdd.csv
├── summary-sheets /
│ ├── provinces
│ │ ├── dpc-covid19-ita-scheda-province-yyyymmdd.pdf
│ ├── regions
│ │ ├── dpc-covid19-eng-card-regions-yyyymmdd.pdf
Data by Region
Directory: data-regions
Daily file structure: dpc-covid19-ita-regions-yyyymmdd.csv (dpc-covid19-ita-regions-20200224.csv)
Overall file: dpc-covid19-eng-regions.csv
An overall JSON file of all dates is made available in the ""data-json"" folder: dpc-covid19-eng-regions.json
Data by Province
Directory: data-provinces
Daily file structure: dpc-covid19-ita-province-yyyymmdd.csv (dpc-covid19-ita-province-20200224.csv)
Overall file: dpc-covid19-ita-province.csv
Acknowledgements
Banner photo by CDC on Unsplash
Data from https://github.com/pcm-dpc/COVID-19 released under a CC 4.0 license.  See https://github.com/pcm-dpc/COVID-19 for more detail."	1172	17854	33	paultimothymooney	coronavirus-in-italy
2537	2537	cocopm7		[]		0	4	0	mhdslhi	cocopm7
2538	2538	cocopm6		[]		0	4	0	mhdslhi	cocopm6
2539	2539	cocop3		[]		0	5	0	mhdslhi	cocop3
2540	2540	cocop2		[]		0	10	0	mhdslhi	cocop2
2541	2541	cocopm1		[]		0	2	0	mhdslhi	cocopm1
2542	2542	Automobile Data for Private Licensed Cars in UK	Automobile Data  Private Licensed Cars in UK	['categorical data', 'automobiles and vehicles', 'beginner', 'intermediate', 'classification']		5	23	1	atharvaj9	automobile-data-for-private-licensed-cars-in-uk
2543	2543	Shift CFT new test dataset		[]		2	13	0	nikolaimakarov	shift-cft-new-test-dataset
2544	2544	temp_data_yolox		[]		1	12	0	raufmomin	temp-data-yolox
2545	2545	phishing		[]		1	22	0	rezaoffice	phishing
2546	2546	Africa Lotto Zimbabwe Ball Statistics	Africa Lotto Ball Statistics	['africa', 'categorical data', 'intermediate', 'data analytics', 'gambling']	"Context
I was eager to understand if ball winning frequency can assist in finding the right pick,
Content
This is a collection of ball statistics of lottery results from https://www.africalotto.co.zw
Inspiration
Can ML predict winning ball?
Does ball winning frequency matter?"	38	336	4	bevennyamande	africa-lotto-ball-statistics
2547	2547	Bikeshare_case_study	Case study worked on for Fictional company	['business']		0	17	2	knndchk	bikeshare-case-study
2548	2548	Hindi-Stop-Wordz		[]		0	9	0	yeeandres	hindistopwordz
2549	2549	Ufo in Kościan, Poland		['religion and belief systems']		0	8	0	aresroces	ufo-in-kocian-poland
2550	2550	Hindi Dataset Info Ret		[]		0	6	0	divyanshuk34	hindi-dataset-info-ret
2551	2551	distilbert-base-multilingual-cased		[]		1	10	0	laineyzheng	distilbert-base-multilingual-cased
2552	2552	NLP_model		[]		1	89	0	jinkaido	nlp-model
2553	2553	English Dataset Info Ret		[]		0	8	0	divyanshuk34	english-dataset-info-retrieval
2554	2554	testski[		[]		0	0	0	poudelsagar	testski
2555	2555	Tafsir Ibn Kathir in English		['religion and belief systems']	"Ibn Kathir wrote a famous commentary on the Qur'an named Tafsir al-Qur'an al-'Adhim which linked certain Hadith, or sayings of Muhammad, and sayings of the sahaba to verses of the Qur'an, in explanation. Tafsir ibn Kathir is famous all over the Muslim world, and among Muslims in the Western world is one of the most widely used explanations of the Qu'ran today.
Ibn Kathir was renowned for his great memory regarding the sayings of Muhammad and the entire Qur'an. Ibn Kathir is known as a qadi, a master scholar of history, also a muhaddith and a mufassir (Qur'an commentator). Ibn Kathir saw himself as a Shafi'i scholar. This is indicated by two of his books, one of which was Tabaqaat ah-Shafa'iah, or The Categories of the Followers of Imam Shafi.
pdf's were downloaded from : http://www.quran4u.com/Tafsir%20Ibn%20Kathir/Index.htm"	0	21	7	mobassir	tafsir-ibn-kathir-in-english
2556	2556	Chemical Production India [2013 to 2020] 	chemical production in India from 2013 to 2020 in metric tonnes 	['india', 'chemistry', 'beginner', 'exploratory data analysis', 'data visualization']	"Context
Get product wise, group wise and year wise production of major chemicals products in India. Production figures have been aggregated based on Monthly Production returns received directly by the Department of Chemicals and Petrochemicals from the manufacturers under large and medium scale
Acknowledgements
Ministry of Chemicals and Fertilizers, India 
Department of Chemicals and Petrochemicals, India 
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?
Surprise with cool visualisation!"	29	193	7	timontunes	chemicalproduction2013to2020
2557	2557	private_handwriting		[]		0	18	1	huangjingstark	private-handwriting
2558	2558	PhD Stipends, Salaries, and LW Ratios	Self-reported data on the size of PhD stipends around the world	['universities and colleges', 'income', 'education', 'jobs and career']	"Context
PhD candidates typically get paid to study.
Content
Self-reported PhD salaries from phdstipends.com.
['University', 'Department', 'Overall Pay', 'Living Wage Ratio', 'Academic Year', 'Program Year', etc.]
Acknowledgements
Data from http://www.phdstipends.com/csv
License: Unknown + https://twitter.com/pfforphds/status/1222921605493313537?s=12
Banner Photo by Photo by Good Free Photos on Unsplash"	436	12020	41	paultimothymooney	phd-stipends
2559	2559	Urdu Characters	📄 Complete collection of Urdu language characters.	[]	"📄 Complete ( 🇵🇰 ) Urdu Language Characters
Complete collection of Urdu language characters.
Table of contents
About Urdu Language
What is Encoding
What is Unicode
Quick start
Python2 vs Python3
Urdu vs Arabic Characters Challenge
Comparison of Unicode values of Urdu and Arabic characters
Contributing
Bugs and feature requests
Contributors
Copyright and license
About Urdu Language
Urdu is widely known as the national language of Pakistan, but it is also one of India’s 22 official languages. Modern Standard Urdu, once commonly known as a variant of Hindustani, a colloquial language combining the modified Sanskrit words found in Hindi with wordsbrought to India via Persian, Arabic, Portuguese, Turkish and other languages, is a language with one of the most fascinating and complex histories in the world.
The Urdu alphabet is the right-to-left alphabet used for the Urdu language. It is a modification of the Persian alphabet known as Perso-Arabic, which is itself a derivative of the Arabic alphabet. 
The Urdu alphabet has up to 58 letters with 39 basic letters and no distinct letter cases, the Urdu alphabet is typically written in the calligraphic Nastaʿlīq script.
http://www.panl10n.net/english/outputs/Pakistan/Urdu-Encoding-Collation.pdf
http://www.bhurgri.com/bhurgri/downloads/PakLang.pdf
http://jrgraphix.net/r/Unicode/0600-06FF
https://www.unicode.org/charts/PDF/U0600.pdf
https://www.unicode.org/cldr/charts/34/collation/ur.html
http://www.unics.uni-hannover.de/nhtcapri/urdu-alphabet.html
https://r12a.github.io/scripts/arabic/urdu
https://blogs.transparent.com/urdu/a-short-history-of-urdu/
What is Encoding
Character encoding may be defined as assigning a unique number to each language character to be processed by the computer. Whenever a character is input from keyboard or other input devices, this particular code is generated internally in the computer. Arbitrary encoding may be defined for any application (e.g. 80 for letter ‘a’, 81 for letter ‘b’). However, if different vendors are defining arbitrary encodings, their encodings may not agree with one another. 
With the advent of the Internet, it has now become increasingly essential to standardize the encoding scheme because users are accessing data created by a variety of sources through web browsers (a single application). Realizing the significance of standardizing encoding, work was done early for English and American Standard Code for Information Interchange (ASCII) was defined in 1968. This standard had 128 slots defined using 7 bits by American National Standards Institute (ANSI).
What is Unicode
Initially most documentation was done in a single language, therefore 8-bit single language code pages served the need. However, in 1990s, with increasing needs for multi-lingual documents (where one could require Japanese and Arabic in the same document), it was realized that defining 8-bit code pages were not a scalable solution. Adding code pages for various languages and scripts and using them together in one application created a lot of difficulty and complexity in processing because users had to keep toggling between them.
To address this issue, major vendors got together and created Unicode consortium (www.unicode.org). This consortium started working on developing a singular, unified and universal code chart which would contain all characters of all languages. As 8-bit (256 slots) code pages were insufficient for this requirement, Unicode character encoding standard was developed using 16 bits (65536 slots). This space has been divided to cater to various scripts and thus bypassed the need for toggling for different languages.
Urdu Unicode Range(0600-06ff)
Py file Content
Quick start
urdu_characters.py contains complete urdu characters with unicode Range (0600 - 06ff). This file contains three type of characters list. 
URDU_DIGITS set contains 10 URDU DIGITS.
URDU_ALPHABET set contains 44  characters.
URDU_PUNCTUATION set contains 6 PUNCTUATION characters.
DIACRITICS set contains 4 basic characters.
Urdu vs Arabic Characters Challenge
Unicode provides support for Urdu language but there is a problem we have to cater in order to utilise that support. The Urdu is incorporated in Arabic language's block in the Unicode table as Urdu is derived from Arabic script. This makes things a little bit complicated for computer scientists trying to develop applications for Urdu language.
For example consider a word ""خاموشی"", now if we see the codes at the back-end for this word we can find two different sets of codes form Unicode table.
Set of codes #1
<img src=""https://raw.githubusercontent.com/urduhack/urdu-characters/master/img/soc_arabic.png"">
Set of codes #2
<img src=""https://raw.githubusercontent.com/urduhack/urdu-characters/master/img/soc_urdu.png"">
Now the problem is how do we know on which codes we have to train our model on? If we train our model on a specific range (Urdu 0600-06ff) and our dataset has some words formed using the Arabic set of codes then our application will fail to recognize those words resulting in low accuracy. This redundancy in codes of words hinders us to achieve a high accuracy.
So how do we handle this issue? You can go up and look at the Urdu Unicode Range table. Unicode has standardized this range (0600-06ff) for Urdu only. So all we need to do is to do some data pre-processing before running any alogrithm on data. For each word in data having redundant codes, we can replace that word with the same standardized Urdu word belonging to the range 0600 to 06ff. That's it!
Urdu Characters Shapes
Urdu characters take on different forms based on the position they are used inside a word. Like an urdu character used at the start of a word will have a different shape
and the same character used in the middle or at the end of a word will have a completely different shape. This is only concerned with the font shape for that character. For illustration purpose, let's take an example of urdu character ""ﻑ"". Now notice the
difference in ""ﻑ"" shape. 
Used at the Start: ""آفاق""
Used in the middle: ""مفاحمت""
Used at the End: ""کیف""
Isolated Use: ""موصوف""
As you would have noticed ""ﻑ"" takes on a different shape based on its position of usage.
<img src=""https://raw.githubusercontent.com/urduhack/urdu-characters/master/img/urdu_chr_shapes.png"">
Urdu/Arabic Character Presentation Fonts
Now to get a bit more understanding of the above part, let's look at the unicode range for combined characters. These combined characters are given a unicode range separately. 
This range was defined for the intuition purpose only. How two characters appear when they are combined. 
It has more to do with the usage of characters in different positions rather than the context the character is used in.  In arabic ""Qaida"", for teaching purpose,
it is taught how two characters like ""ل"" and ""ح"" when combined will appear like ""لح"". This ""لح"" is given a new unicode FC40. There wil hardly be any keyboard or a system which will use
these combined characters so this just to show different presentation forms of a character. 
For more illustrative purpose, look at the below links. 
Unicode Charts
Arabic Presentations Forms-A
Arabic Presentation Forms-B
Comparison of Unicode values of Urdu and Arabic characters
Thanks to (https://github.com/urdutext/UrduArabicCompare)
CSV file (https://github.com/urduhack/urdu-characters/blob/master/img/Urdu_Arabic_Unicode_comparison.csv)
Contributing
All contributions are more than welcomed. Contributions may close an issue, fix a bug (reported or not reported), improve the existing code and so on.
Bugs and feature requests
Have a bug or a feature request? If you wish to remove or update some thing, please file an issue first before sending a PR on the repo. [please open a new issue]
Contributors
Special thanks to everyone who contributed to getting the UrduHack to the current state.
Backers 
Thank you to all our backers! 🙏 [Become a backer]
<a href=""https://opencollective.com/urduhack#backers"" target=""_blank""><img src=""https://opencollective.com/urduhack/backers.svg?width=890""></a>
Sponsors 
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]
<a href=""https://opencollective.com/urduhack/sponsor/0/website"" target=""_blank""><img src=""https://opencollective.com/urduhack/sponsor/0/avatar.svg""></a>
<a href=""https://opencollective.com/urduhack/sponsor/1/website"" target=""_blank""><img src=""https://opencollective.com/urduhack/sponsor/1/avatar.svg""></a>
Copyright and license
Code released under the MIT License."	69	2390	4	akkefa	urdu-characters
2560	2560	features200		[]		0	5	0	edwardkhusnutdinov	features200
2561	2561	tafsir ibn kathir by islamic foundation		['religion and belief systems']	"তাফসীর ইবনে কাসীর ইসলামিক ফাউন্ডেশন pdf
বিসমিল্লাহির রহমানির রহিম – ইমাম আবুল ফিদা ইসমাঈল ইবনে কাসীর রহঃ কর্তৃক রচিত তাফসীর গ্রন্থ তাফসীর ইবনে কাসীর এর সব খন্ডের pdf ফাইল by islamic foundation.
pdf's were collected from this source : https://www.tauhiderdak.com/2021/05/tafsir-ibn-kathir-pdf.html"	4	35	4	mobassir	tafsir-ibn-kathir-by-islamic-foundation
2562	2562	tafsir ibn kathir bangla		['religion and belief systems']	"বই: তাফসীর ইবনে কাসীর (১ম-১৮শ খণ্ড, সম্পূর্ণ)
ডাউনলোড করুন তাফসীর ইবনে কাসীর -এর  পিডিএফ (PDF) সংস্করণ।
তাফসীর ইবনে কাসীর হচ্ছে কালজয়ী মুহাদ্দিস মুফাসসির যুগশ্রেষ্ঠ মনীষী আল্লামা হাফিয ইবন কাসীরের একনিষ্ঠ নিরলস সাধনা ও অক্লান্ত পরিশ্রমের অমৃত ফল। তাফসীর জগতে এ যে বহুল পঠিত সর্ববাদী সম্মত নির্ভরযোগ্য এক অনন্য সংযোজন ও অবিস্মরণীয় কীর্তি এতে সন্দেহ সংশয়ের কোন অবকাশ মাত্র নেই।
হাফিজ ইমাদুদ্দীন ইবন কাসীর এই প্রামাণ্য তথ্যবহুল, সর্বজন গৃহীত ও বিস্তারিত তাফসীরের মাধ্যমে আরবী ভাষাভাষীদের জন্য পবিত্র কালামের সত্যিকারের রূপরেখা অতি স্বচ্ছ সাবলীল ভাষায় তুলে ধরেছেন তাঁর ক্ষুরধার বলিষ্ঠ লেখনীর মাধ্যমে। এসব কারণেই এর অনবদ্যতা ও শ্রেষ্ঠত্বকে সকল যুগের বিদগ্ধ মনীষীরা সমভাবে অকপটে এবং একবাক্যে স্বীকার করে নিয়েছেন। তাই এই সসাগরা পৃথিবীর প্রায় প্রতিটি মুসলিম অধ্যুষিত দেশে, সকল ধর্মীয় প্রতিষ্ঠানের, এমনকি ধর্মনিরপেক্ষ শিক্ষায়তনের গ্রন্থাগারেও সর্বত্রই এটি বহুল পঠিত, সুপরিচিত, সমাদৃত এবং হাদীস-সুন্নাহর আলোকে এক স্বতন্ত্র মর্যাদার অধিকারী।
প্রায় দেড় যুগ পরিশ্রমের পর ১৯৮৪ সালে ড. মুহাম্মাদ মুজীবুর রাহমান তাফসীরটির বাংলা অনুবাদ সম্পন্ন করেন।
তাফসীর খন্ডগুলিতে যে ইসরাঈলী রিওয়ায়াত এবং দুর্বল কিংবা যঈফ হাদীস রয়েছে তা বাছাই করে বাদ দেওয়া হয়েছে। প্রতিটি তাফসীর খন্ডে, বিষয়বস্ত্তর উপর লক্ষ্য রেখে, তাফসীরের বিভিন্ন শিরোনাম সংযোজন করা হয়েছে, যাতে পাঠকবর্গের নির্দিষ্ট কোন বিষয়ের আলোচনা খুঁজে পেতে সুবিধা হয়। এ ছাড়া বর্ণিত হাদীসের সূত্র নম্বরগুলিও সংযোজন করা হয়েছে। কুরআনের কোন কোন শব্দ বাংলায় লেখা কিংবা উচ্চারণ সঠিক হয়না বিধায় তার আরাবী শব্দটিও পাশে লিখে দেয়া হয়েছে।
pdf's are collected from here : https://i-onlinemedia.net/3185"	0	22	5	mobassir	tafsir-ibn-kathir-bangla
2563	2563	Licit_AutoEncoder_model_trained_on_elliptic_data	notebook name - licit_autoencoder, version7	[]		1	16	0	k2awanish	licit-autoencoder-model-trained-on-elliptic-data
2564	2564	All Tsunamis between 1950-2020	Relevant Information regarding World Tsunamis for 70 years.	['earth and nature', 'geology', 'environment', 'weather and climate', 'geospatial analysis']	"Tsunamis are considered to be one of the most destructive natural calamity on our planet. Therefore, its necessary to analyze this hazard.
Please find all the relevant data on Tsunamis for the past seventy years on the planet.
The dataset would surely help all of us to analyse the cause, geographies and patterns behind the repeated hazards that is an imminent danger to our planet.
The data was collected from the site: https://www.ngdc.noaa.gov/
We wouldn't be here without the help of others. Please cite DOI:10.7289/V5PN93H7
Your data will be in front of the world's largest data science community.
For any further queries: haz.info@noaa.gov"	56	456	5	ankanhore545	tsunami-19502000
2565	2565	Epedemic of covid 19		[]		0	5	0	alexis01	epedemic-of-covid-19
2566	2566	jigsaw-batched-tfr-roberta-base		[]		0	9	0	ks2019	jigsaw-batched-tfr-roberta-base
2567	2567	Audio-Anomaly-Dataset	Audio Anomaly Dataset	['music']		0	36	0	ahmedabbasi	audioanomalydataset
2568	2568	Simple_Linear_Reg_Salary_Data		[]		0	7	0	mdwasimakhtar03	simple-linear-reg-salary-data
2569	2569	insurance		['insurance']		0	9	0	saikumarreddyp	insurance
2570	2570	Bostan house price prediction		[]		6	30	1	rahulmishra5	bostan-house-price-prediction
2571	2571	XLNET_MUL_clean_256_slr		[]		0	8	0	hangy132	xlnet-mul-clean-256-slr
2572	2572	Covid19 by state in Malaysia		[]		0	8	0	alexis01	covid19-by-state-in-malaysia
2573	2573	Hospital covid19 Malaysia		['hospitals and treatment centers']		0	8	0	alexis01	hospital-covid19-malaysia
2574	2574	land registration system		[]		0	5	0	nithyadnithya	land-registration-system
2575	2575	dataset		[]		0	1	0	alexis01	dataset
2576	2576	TGRNet Model and Checkpoint		[]		3	15	0	piyushlife	tgrnet-model-and-checkpoint
2577	2577	trained_models		[]		0	13	0	sakshamdwivedi10	trained-models
2578	2578	tf_model1		[]		0	4	0	vigneshirtt	tf-model1
2579	2579	IP02-Dataset	A Large-Scale Benchmark Dataset for Insect Pest Recognition	['agriculture', 'computer vision', 'deep learning', 'image data', 'multiclass classification']	"Context
Insect pest are one of the main factors affecting agricultural product. Accurate recognition and classification of insect pests can prevent huge economic losses. This dataset will play a great role in this regard.
Content
IP02 dataset has 75,222 images and average size of 737 samples per class. The dataset has a split of 6:1:3.
Acknowledgements
This dataset was proposed and accepted by the authors={Xiaoping Wu and Chi Zhan and Yukun Lai and Ming-Ming Cheng and Jufeng Yang} in CVPR 2019
Inspiration
What information can we gain by using deep learning from image to solve pest recognition and classification problem task"	32	532	9	rtlmhjbn	ip02-dataset
2580	2580	winequality-red		[]		1	15	0	malikumairayub	winequalityred
2581	2581	Covid19deaths in Malaysia		[]		0	11	0	alexis01	covid19deaths-in-malaysia
2582	2582	Covid19ClusterMalaysia	Case study of covid19 clusters in Malaysia	['universities and colleges']		3	49	2	alexis01	covid19clustermalaysia
2583	2583	hate_model		[]		0	6	0	kookheejin	hate-model
2584	2584	showmaker	ubiquant-XGB-model-first_trial	[]		0	24	0	derrick01	showmaker
2585	2585	New_kflods_round2		[]		0	9	3	varunnagpalspyz	new-kflods-round2
2586	2586	Sample_data		[]		0	9	0	mdwasimakhtar03	sample-data
2587	2587	nbme-models		['standardized testing']		0	10	1	mathurinache	nbmemodels
2588	2588	Covid19casesMalaysia		[]		0	6	0	alexis01	covid19casesmalaysia
2589	2589	speech_recognition		[]		0	3	0	alaminsheikh	speech-recognition
2590	2590	ANI2K_N0		[]		0	17	0	alexandersen	ani2k-n0
2591	2591	7folds-roberta-base-4epoch-v2		['arts and entertainment']		2	14	1	tonymarkchris	7folds-roberta-base-4epoch-v2
2592	2592	/Linear_Regression-Project		[]		0	6	1	sangitamule	linear-regressionproject
2593	2593	WiDSDatathon22		[]		0	9	0	jy2040	widsdatathon22
2594	2594	Kmeans		[]		0	3	0	sangitamule	kmeans
2595	2595	Car_dataset		[]		0	3	0	sangitamule	car-dataset
2596	2596	Pressure-dataset		[]		0	8	0	abhisingh007224	pressuredataset
2597	2597	sales data 		[]		4	28	0	mohammednisar623	sales
2598	2598	ddbo1315		[]		0	4	0	movie112	ddbo1315
2599	2599	bertweet-base		[]		0	3	0	oheast	bertweet-base
2600	2600	coco_sport_classification		[]		1	3	0	a24998667	coco-sport-classification
2601	2601	Jenis Kanker		[]		0	9	0	msulthonmubarok	jenis-kanker
2602	2602	Flight_vs_Train	Compare the prices between flights and trains in India	['india', 'beginner', 'intermediate', 'classification', 'tabular data', 'travel']	"FLIGHT VS TRAIN
Flight vs Train fare comparison in India.
Content
Collection of train and plane fairs between some pairs of Indian cities.
Acknowledgements
The data is obtained by scrapping sites and I do not hold any proprietary rights to these
Inspiration
Inspired by travel time"	89	838	12	brijlaldhankour	flight-vs-train
2603	2603	Miniproject#1		[]		1	35	0	chuxincheng	miniproject1
2604	2604	Playlist 		['arts and entertainment']		0	9	1	anarjinotgonbayar	www-kaggle-com-anarjinotgonbayar
2605	2605	assignment01		[]		0	4	0	chance3924	assignment01
2606	2606	datayi.csv	Analsys of  sntiments	['text data']		1	7	1	johnchiqui	datayicsv
2607	2607	jigsawdata		[]		0	9	0	yowtlm	jigsawdata
2608	2608	ffcv-main		[]		0	6	0	bibhabasumohapatra	ffcvmain
2609	2609	Amazon Food	List of Amazon Food with UserId & ProductId	['artificial intelligence', 'text data', 'food']	"Context
This dataset consists of product details from amazon. The details include product and user information (added productName), ratings, and a plain text review. It also includes reviews from all other Amazon categories.
Content
Comprises of a very small and simple subset of the wide number of food products in Amazon, but it will suffice for working on some simple projects/projects that needs the product name to be present"	319	2147	12	aistct	amazonfood
2610	2610	customer data		[]		1	32	1	concyclics	customer-data
2611	2611	ml_dasar		[]		0	6	0	asadhumam	ml-dasar
2612	2612	belajar_ML_dasar		[]		0	5	0	asadhumam	belajar-ml-dasar
2613	2613	Validation.csv		['software']		1	13	0	bwtest	validationcsv
2614	2614	model1		[]		0	4	0	panupongsaejeong	model1
2615	2615	applied_ai_connect_4		[]		1	18	0	bwtest	applied-ai-connect-4
2616	2616	YOLOX-L-POKEMAN		[]		0	8	0	derrick01	yoloxlpokeman
2617	2617	040_exp		[]		0	4	0	shimishige	040-exp
2618	2618	Video Game Projectt		['video games']		2	47	0	zennnnn	video-game-projectt
2619	2619	pima-indians-diabetes		[]		0	6	0	destaensermu	pimaindiansdiabetes
2620	2620	20220203		[]		0	19	0	guodongjin	20220203
2621	2621	Language detection dataset	Language detection is one of the most famous NLP projects enjoy learning!	['education', 'computer science']	"Context
In order to apply what I learned in web-scrapping using selinium python library, I applied webscrapping of twitters in 3 different languages; English, French and moroccan dialect Darija
Content
This dataset consists of 3 labels and around 420 rows for each label total of 13153 rows and 2 columns. I scrapped it from twitter it contains tweets posted between 2021 and 2022 of specific hashtags such us ['Jhon_Bayden', 'Chrismas_Eve', 'it_is_what_it_is', 'Ce_soir', 'Smi9li', 'Khoya'...]"	2	55	0	lailaboullous	language-detection-dataset
2622	2622	Extra HappyWhale Metadata		['business']		9	71	2	dschettler8845	extra-happywhale-metadata
2623	2623	This should be the last playlist	This data set is for my class	[]		1	10	0	ryanrosado4	this-should-be-the-last-playlist
2624	2624	Bitcoin Historical Data 	Updated on January 2022	['currencies and foreign exchange']		12	101	2	kaviga	bitcoin-2019-2022-price
2625	2625	h3fcgr		[]		6	21	0	rasterbunny	h3fcgr
2626	2626	AnimeScenery64	64x64 anime scenery images taken from screenshots	['anime and manga']		0	245	1	manuelamalasaa	animescenery64-pretrained
2627	2627	Titanic - Machine Learning from Disaster		['business']		0	10	0	mpkanalytics	titanic-machine-learning-from-disaster
2628	2628	All Pokemon with stats	All pokemon types and stats until legends arceus	['video games', 'anime and manga']	"All pokemon types and stats until legends arceus
This CSV file contains pokemon types, Forms/Variations, stats such as HP, Attack, Defense, Sp.Atk, Sp.Def and Total sum"	231	1576	16	thiagoazen	all-pokemon-with-stats
2629	2629	 Predict the Fish's weight		[]		0	11	1	houssemaminetouihri	predict-the-fishs-weight
2630	2630	Playlist		['arts and entertainment']		0	4	0	josephinebowden	playlist
2631	2631	Fish's weight		[]		0	6	1	houssemaminetouihri	fishs-weight
2632	2632	pycocotools		[]		0	8	0	vincentwang25	pycocotools
2633	2633	SPP Base models collection		['business']		0	6	0	vkonstantakos	spp-base-models-collection
2634	2634	Cyclistic Dataset	Google Data Analytics Capstone	['business']		0	18	0	ethantylerrundquist	cyclistic-dataset
2635	2635	COVID-19 Coronavirus Dataset Worldwide (Real-Time)	COVID-19 Cases Worldwide, by European Centre for Disease Prevention and Control	['health', 'covid19']	"This is the real-time JSON version of the COVID-19 Coronavirus Dataset Worldwide dataset, furthermore, the collection methodology can be read through: https://www.ecdc.europa.eu/en/covid-19/data-collection
Context
The worldwide situation about the COVID-19 (by 2019-03-23), data provided by the European Centre for Disease Prevention and Control and published on the EU Open Data Portal.
Content
The dataset contains the latest available public data on COVID-19 including a daily situation update, the epidemiological curve and the global geographical distribution (EU/EEA and the UK, worldwide). On 12 February 2020, the novel coronavirus was named severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) while the disease associated with it is now referred to as COVID-19. ECDC is closely monitoring this outbreak and providing risk assessments to guide EU Member States and the EU Commission in their response activities. 
Acknowledgements
Official link: https://data.europa.eu/euodp/en/data/dataset/covid-19-coronavirus-data
Inspiration
What applications can we develop to understand COVID-19 current and prospective behavior better?"	429	7551	26	hgultekin	covid19-stream-data
2636	2636	Coursera Cyclistic Case Study	Casual Rider to Annual Riders Conversion Analysis	['education']		2	64	1	malikcraven	coursera-case-study-1
2637	2637	Olympique de Marseille	This dataset aims to gather all the metrics and stats of Olympique de Marseille.	['football', 'europe', 'data analytics']		1	68	2	clementtassart	olympique-de-marseille
2638	2638	The RadarScenes data set	A Real-World Radar Point Cloud Data Set for Automotive Applications	['artificial intelligence', 'automobiles and vehicles', 'classification', 'cnn', 'image data']	"Context
The RadarScenes data set contains recordings from four automotive radar sensors, which were mounted on one measurement vehicle. Images from one front-facing documentary camera are added. It was recorded between 2016 and 2018 in Ulm, Germany.
The data set has a length of over 4h and in addition to the point cloud data from the radar sensors, semantic annotations on a point-wise level from 12 different classes are provided. 
In addition to point-wise class labels, a track-id is attached to each individual detection of a dynamic object, so that individual objects can be tracked over time.
Content
Four radar sensors;
One documentary camera;
&gt; 100 km driving;
&gt; 4 hours recording time;
158 different sequences;
&gt; 7500 unique road users;
11 different object classes;
About 4 million annotated detections.
Acknowledgements
Thanks to Ole Schumann, Markus Hahn, Nicolas Scheiner, Fabio Weishaupt, Julius Tilly, Jürgen Dickmann and Christian Wöhler for posting the dataset
Inspiration
The success of the community depends on the success of each of us, the more information the better"	1	132	3	aleksandrdubrovin	the-radarscenes-data-set
2639	2639	MaeFinetune data MR = 0.75		[]		0	8	0	tanishmittal0658	maefine75
2640	2640	Black History Month	Racism visualization by M. Chalabi	['image data']	"Context
""Statistics are subjective. This much is clear to British data journalist Mona Chalabi, who has built a career highlighting neglected information through her data illustrations. She recognizes that everyone—even data storytellers—brings their own inherent biases to numbers.""
""When looking at a given dataset from her perspective (Chalabi is of Iraqi descent), she finds herself naturally honing in on issues she is particularly interested in. If numbers convey a glaring gender pay gap, or an issue that can be tied to systemic racism, these usually end up shaping her interpretation of the data.""
“The statistics were really important, but they were only being shared with a handful of experts,” Chalabi says. “And she just wanted to reach the broader public.”
https://news.artnet.com/art-world/meet-mona-chalabi-1893221
Content
Illustration by Mona Chalabi.
Acknowledgements
Illustrator and data journalist Mona Chalabi.
Photo by Jason Leung on Unsplash
Inspiration
Black History Month that is an annual celebration of achievements by African Americans and a time for recognizing their central role in U.S. history.
https://www.history.com/topics/black-history/black-history-month"	2	48	5	mpwolke	cusersmarildownloadsdollarjpg
2641	2641	Death from COVID-19 (vaccinated people) / Poland	Data on deaths due to COVID-19 in Poland, including the vaccinated - 01.21-01.22	['public health', 'people', 'medicine', 'tabular data']	"&gt;### COVID-19 infection and death statistics including COVID-19 vaccination 
&gt; Public government data - Poland. E-Health Center (Centrum E-Zdrowia)
Data source and short description of the repository
The dane.gov.pl website fulfills the purpose of the Central Repository of Public Information, indicated in Polish legislation as one of the modes of access and re-use of public information.
https://dane.gov.pl/pl/dataset/2582
The e-Health Center is a state budget unit established by the Minister of Health. It started its activity on August 1, 2000. The main activity of the E-Health Center is the implementation of tasks in the field of building the information society, including the organization and protection of health, as well as supporting the management decisions of the minister competent for health on the basis of the analyzes carried out.
License
CC0 1.0"	30	156	3	krystianadammolenda	death-from-covid19-vaccinated-people-poland
2642	2642	reef-yolox-m-v11		[]		0	4	0	sangayb	reef-yolox-m-v11
2643	2643	emilydickinson	Collected works of poet Emily Dickinson	['literature', 'united states', 'text data']	"Context
These data are the public domain works of the poet Emily Dickinson (1830 - 1866). 
Content
The files here are originally from the text files made available by The Gutenberg Project. The original files are cleaned to remove Gutenberg project information, the book's original introductions, and the roman numerals associated with the poem, leaving only the poetry and titles (where applicable, most of her work was untitled).
Acknowledgements
Thanks to The Gutenberg Project for their effort to make these texts available and accessible.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	18	3	rrbaker	emily-dickinson-three-series-complete-cleaned
2644	2644	EDA_and_Specie_Classification		[]		3	4	0	matheusfilipemartins	eda-and-specie-classification
2645	2645	Formula 1 Race Events	Tables of safety car deployments and red flags in Formula 1 Grand Prix races	['auto racing', 'sports', 'history', 'tabular data']	"This dataset extends the existing ergast.com F1 dataset with events that happened in Grand Prix, currently:
- red flags
- safety car deployments
- virtual safety car deployments
Data is collected from these pages:
https://en.wikipedia.org/wiki/Safety_car#List_of_safety_car_deployments_in_Formula_One_races
https://en.wikipedia.org/wiki/List_of_red-flagged_Formula_One_races
Example usage in F1 Race Traces notebooks:
1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021."	133	1362	21	jtrotman	formula-1-race-events
2646	2646	bitfinex orderbooks	bitfinex orderbooks of various cryptocurrencies	['currencies and foreign exchange']		4	80	2	maxisoft	bitfinex-orderbooks
2647	2647	telecom churn013651632		[]		0	6	0	ritrex	telecom-churn013651632
2648	2648	COTS Cascade-RCNN		[]		0	27	0	aishikai	cots-cascadercnn
2649	2649	COTS Cascade-RCNN		[]		0	27	0	aishikai	cots-cascadercnn
2650	2650	Worldwide Residential Mobility in COVID-19	Increase in Residential Stay across Globe since outbreak of COVID-19 Pandemic	['research', 'public health', 'beginner', 'covid19']	"Staying at home is the new going out.
With the surge of COVID-19 and government restrictions being put down, we see more and more people preferring to stay home. Companies, Schools and institutions across the globe switched to WFH (Work From Home) mode.
The dataset mentioned below captures the % increase in people preferring to stay home in COVID-19. This % increase is relative to the no. of people staying home before the pandemic hit the globe."	115	1150	31	aestheteaman01	people-staying-in-home-during-covid19
2651	2651	ps1-data-ay2122-sem2	Data given by professor for HW 1	[]		0	32	0	darrenmok	emh-data
2652	2652	heartdata		[]		2	11	1	avplustvru	heartdata
2653	2653	Neuro CNN MRI - 41k Imgs - 42 Cat - 256x256p	MRI Brain images for disease classification with convolutional neural networks	['computer vision', 'classification', 'neural networks', 'cnn']	41000 images in 42 categories: abscess, cerebellar agenesis, pyloric astrocytoma, cerebellar atrophy, stroke, basal cell carcinoma, cyst, Creutzfeldt-Jakob disease, Erdheim Chester disease, Neuro Behcet disease, encephalitis, encephalomalacia, hypoxic encephalopathy ischemia, multiple sclerosis, schizencephaly, ganglioglioma, germinoma , glioblastoma, glioma, hemangioblastoma, subdural hematoma, hemorrhage, hydrocephalus, intracranial hypotension, lymphoma, medulloblastoma, meningioma, metastases, mucocele, neurocysticercosis, neurocytoma, oligodendroglioma, oeteopetrosis, papilloma, schannoma, cavernous malformation syndrome, arterial tortuosity syndrome, subependymoma , tuberculoma, miliary tuberculosis, generic tumor + abscess, leptomeningeal glioneural tumor.	11	258	4	fernando2rad	neuro-cnn-mri-41k-imgs-42-cat-256x256p
2654	2654	IBM Data Science Chicago assignment	A project with data from Chicago's education, census and crime 	['computer science']	"Goals:
Understand three Chicago datasets
Load the three datasets into three tables in a IBM Db2 database
Execute SQL queries to answer assignment questions"	10	131	3	antonioskokiantonis	ibm-data-science-chicago-assignment
2655	2655	altınveriseti		[]		0	14	0	yunusemrealan	altnveriseti
2656	2656	Yfinance World Indices Price Data	Daily Updates of Major World Indices (Close, High, Low, Open, Volume)	['business', 'finance', 'tabular data', 'investing']	Daily OHLCV data for major world indices (S&P 500, Dow 30, Nasdaq, NYSE COMPOSITE (DJ), NYSE AMEX COMPOSITE INDEX, Cboe UK 100, Russell 2000, CBOE Volatility Index, FTSE 100, DAX PERFORMANCE-INDEX, CAC 40, ESTX 50 PR.EUR, EURONEXT 100, BEL 20, MOEX Russia Index, Nikkei 225, HANG SENG INDEX, SSE Composite Index, Shenzhen Component, STI Index, S&P/ASX 200, ALL ORDINARIES, S&P BSE SENSEX, Jakarta Composite Index, FTSE Bursa Malaysia KLCI, S&P/NZX 50 INDEX GROSS, KOSPI Composite Index, TSEC weighted index, S&P/TSX Composite index, IBOVESPA, IPC MEXICO, S&P/CLX IPSA, MERVAL, TA-125, EGX 30 Price Return Index, Top 40 USD Net TRI Index): fetched via the YFinance API, updated daily.	79	2440	13	code1110	yfinance-world-indices-price-data
2657	2657	jigsaw-models-20220202	Models used in jigsaw-toxic-severity-rating competition	[]		0	3	0	bachan	jigsaw-models-20220202
2658	2658	detoxify-master		['health']		1	3	0	bachan	detoxify-master
2659	2659	Gender Pay Gap Dataset	The Gender Wage Gap: Extent, Trends, and Explanations for differences in Salary	['employment', 'income', 'business', 'finance', 'jobs and career']	"Similar Datasets
Company Bankruptcy Prediction: LINK
The Boston House-Price Data: LINK
California Housing Prices Data (5 new features!): LINK
Context
The gender pay gap or gender wage gap is the average difference between the remuneration for men and women who are working. Women are generally considered to be paid less than men. There are two distinct numbers regarding the pay gap: non-adjusted versus adjusted pay gap. The latter typically takes into account differences in hours worked, occupations were chosen, education, and job experience. In the United States, for example, the non-adjusted average female's annual salary is 79% of the average male salary, compared to 95% for the adjusted average salary.
The reasons link to legal, social, and economic factors, and extend beyond ""equal pay for equal work"".
The gender pay gap can be a problem from a public policy perspective because it reduces economic output and means that women are more likely to be dependent upon welfare payments, especially in old age.
This dataset aims to replicate the data used in the famous paper ""The Gender Wage Gap: Extent, Trends, and Explanations"", which provides new empirical evidence on the extent of and trends in the gender wage gap, which declined considerably during the 1980–2010 period.
Citation
&gt; fedesoriano. (January 2022). Gender Pay Gap Dataset. Retrieved [Date Retrieved] from https://www.kaggle.com/fedesoriano/gender-pay-gap-dataset.
Content
There are 2 files in this dataset: a) the Panel Study of Income Dynamics (PSID) microdata over the 1980-2010 period, and b) the Current Population Survey (CPS) to provide some additional US national data on the gender pay gap.
PSID variables:
&gt; NOTES: THE VARIABLES WITH fz ADDED TO THEIR NAME REFER TO EXPERIENCE WHERE WE HAVE FILLED IN SOME ZEROS IN THE MISSING PSID YEARS WITH DATA FROM THE RESPONDENTS’ ANSWERS TO QUESTIONS ABOUT JOBS WORKED ON DURING THESE MISSING YEARS. THE fz variables WERE USED IN THE REGRESSION ANALYSES
THE VARIABLES WITH A predict PREFIX REFER TO THE COMPUTATION OF ACTUAL EXPERIENCE ACCUMULATED DURING THE YEARS IN WHICH THE PSID DID NOT SURVEY THE RESPONDENTS. THERE ARE MORE PREDICTED EXPERIENCE LEVELS THAT ARE NEEDED TO IMPUTE EXPERIENCE IN THE MISSING YEARS IN SOME CASES.
NOTE THAT THE VARIABLES yrsexpf, yrsexpfsz, etc., INCLUDE THESE COMPUTATIONS, SO THAT IF YOU WANT TO USE FULL TIME OR PART TIME EXPERIENCE, YOU DON’T NEED TO ADD THESE PREDICT VARIABLES IN. THEY ARE INCLUDED IN THE DATA SET TO ILLUSTRATE THE RESULTS OF THE COMPUTATION PROCESS.
THE VARIABLES WITH AN orig PREFIX ARE THE ORIGINAL PSID VARIABLES. THESE HAVE BEEN PROCESSED AND IN SOME CASES RENAMED FOR CONVENIENCE. THE hd SUFFIX MEANS THAT THE VARIABLE REFERS TO THE HEAD OF THE FAMILY, AND THE wf SUFFIX MEANS THAT IT REFERS TO THE WIFE OR FEMALE COHABITOR IF THERE IS ONE. AS SHOWN IN THE ACCOMPANYING REGRESSION PROGRAM, THESE orig VARIABLES AREN’T USED DIRECTLY IN THE REGRESSIONS. THERE ARE MORE OF THE ORIGINAL PSID VARIABLES, WHICH WERE USED TO CONSTRUCT THE VARIABLES USED IN THE REGRESSIONS. HD MEANS HEAD AND WF MEANS WIFE OR FEMALE COHABITOR.
intnum68: 1968 INTERVIEW NUMBER
pernum68: PERSON NUMBER 68
wave: Current Wave of the PSID
sex: gender SEX OF INDIVIDUAL (1=male, 2=female)
intnum: Wave-specific Interview Number
farminc: Farm Income
region: regLab Region of Current Interview
famwgt: this is the PSID’s family weight, which is used in all analyses
relhead: ER34103L this is the relation to the head of household (10=head; 20=legally married wife; 22=cohabiting partner)
age: Age
employed: ER34116L Whether or not employed or on temp leave (everyone gets a 1 for this variable, since our wage analyses use only the currently employed)
sch: schLbl Highest Year of Schooling
annhrs: Annual Hours Worked
annlabinc: Annual Labor Income
occ: 3 Digit Occupation 2000 codes
ind: 3 Digit Industry 2000 codes
white: White, nonhispanic dummy variable
black: Black, nonhispanic dummy variable
hisp: Hispanic dummy variable
othrace: Other Race dummy variable
degree: degreeLbl Agent's Degree Status (0=no college degree; 1=bachelor’s without advanced degree; 2=advanced degree)
degupd: degreeLbl Agent's Degree Status (Updated with 2009 values)
schupd: schLbl Schooling (updated years of schooling)
annwks: Annual Weeks Worked
unjob: unJobLbl Union Coverage dummy variable
usualhrwk: Usual Hrs Worked Per Week
labincbus: Labor Income from Business
yrsexp: Experience
yrsftexp: FT Experience
yrsptexp: PT Experience
yrsptexpsq: PT Experience^2
yrsftexpsq: FT Experience^2
yrsExpSq: Experience^2
yrsexpfz: Experience (filling in zeros)
yrsftexpfz: FT Experience (filling in zeros)
yrsptexpfz: Years of Part-Time Experience (Filling in zeros)
yrsexpfzsq: Experience^2 (filling in zeros)
yrsftexpfzsq: FT Experience^2 (filling in zeros)
wtrgov: Works in Government (dummy variable)
selfemp: selfEmpLbl =1 If Self Employed for ANY Job in the Current Wave. Everyone gets a zero for this variable because our wage analyses only include wage and salary workers.
predict98: Total Experience must be predicted for 1998
predictft98: FT Experience must be predicted for 1998
predict00: Total Experience must be predicted for 2000
predictft00: Experience must be predicted for 2000
predict02: Total Experience must be predicted for 2002
predictft02: FT Experience must be predicted for 2002
predict04: Total Experience must be predicted for 2004
predictft04: FT Experience must be predicted for 2004
predict06: Total Experience must be predicted for 2006
predictft06: FT Experience must be predicted for 2006
predict08: Total Experience must be predicted for 2008
predictft08: FT Experience must be predicted for 2008
predict1: Total Experience must be predicted for 2010
predictft10: FT Experience must be predicted for 2010
origage:
origannHrsHD:
origannHrsWF:
origannLabIncHD:
origannLabIncWF:
origannWeeksHD:
origannWeeksWF:
origcurrHrWkHD:
origcurrHrWkWF:
origdegreeHD:
origdegreeWF:
origemp: ER34116L
origeverwrkHD07: ER36351L BC62 WTR EVER WORKED
origeverwrkHD09: ER42376L BC62 WTR EVER WORKED
origeverwrkHD11: ER47689L BC62 WTR EVER WORKED
origeverwrkHD99: ER13476L C4 EVER WORKED? (HD-U)
origeverwrkWF07: ER36609L DE62 WTR EVER WORKED
origeverwrkWF09: ER42628L DE62 WTR EVER WORKED
origeverwrkWF11: ER47946L DE62 WTR EVER WORKED
origeverwrkWF99: ER13988L E4 EVER WORKED? (WF-U)
origfamWgt:
origfarmInc:
origindHD:
origindWF:
origmarSt: ER47323L
orignumChld: ER47320L
origoccHD:
origoccWF:
origraceHD: ER51904L
origraceWF: ER51810L
origregion: ER52398L
origrelHead: ER34103L
origsch: ER34119L
origschfamHD07: ER41037L COMPLETED ED-HD
origschfamHD09: ER46981L COMPLETED ED-HD
origschfamHD11: ER52405L COMPLETED ED-HD
origschfamHD81: V8039L M28 EDUCATION-HD
origschfamHD99: ER16516L COMPLETED ED-HD
origschfamWF07: ER41038L COMPLETED ED-WF
origschfamWF09: ER46982L COMPLETED ED-WF
origschfamWF11: ER52406L COMPLETED ED-WF
origschfamWF81: V7998L L2 EDUCATION-WF
origschfamWF99: ER16517L COMPLETED ED-WF
origsexHead: ER47318L
origspanHD: ER51903L Spanish Descent Head
origspanWF: ER51809L Spanish Descent Wife
origstopw~DE299: ER13307L B53 STOP WRK OTR EMP H-E
origstopw~DE399: ER13388L B92 STOP WRK XJOB1 (H-E)
origstopw~DE499: ER13413L B104 STOP WORK XJOB2 H-E
origstopw~DE599: ER13437L B116 STOP WRK XTRA JOB3
origstopw~DE699: ER13461L B128 STOP WRK XTRA JOB4
origstopw~DU299: ER13560L C45 STOP WRK OTR EMP H-U
origstopw~DU399: ER13641L C84 STOP WORK XJOB1 H-U
origstopw~DU499: ER13665L C96 STOP WORK XJOB2 H-U
origstopw~DU599: ER13689L C108 STOP WRK XTRA JOB3
origstopw~DU699: ER13713L C120 STOP WORK XTRA JOB4
origstopw~FE299: ER13819L D53 STOP WRK OTR EMP W-E
origstopw~FE399: ER13900L D92 STOP WRK XJOB1 (W-E)
origstopw~FE499: ER13925L D104 STOP WRK XJOB2 W-E
origstopw~FE599: ER13949L D116 STOP WRK XTRA JOB3
origstopw~FE699: ER13973L D128 STOP WRK XTRA JOB4
origstopw~FU299: ER14072L E45 STOP WRK OTR EMP W-U
origstopw~FU399: ER14153L E84 STOP WORK XJOB1 W-U
origstopw~FU499: ER14177L E96 STOP WORK XJOB2 W-U
origstopw~FU599: ER14201L E108 STOP WRK XTRA JOB3
origstopw~FU699: ER14225L E120 STOP WRK XTRA JOB4
origtotYrsFTHD: ER51956L
origtotYrsFTWF: ER51862L
origtotYrsHD: ER51955L
origtotYrsWF: ER51861L
origunJobHD: ER47491L
origunJobWF: ER47748L
origwrkPriorJ~D: ER47453L
origwrkPriorJ~F: ER47710L
origwtrNewHD: ER51865L
origwtrNewWF: ER51771L
origyrNewHD: this is the year the family acquired a new head
origyrNewWF: this is the year the family acquired a new wife
predict97: Total Experience must be predicted for 1997
predictft97: FT Experience must be predicted for 1997
predictfz97: fz Total Experience must be predicted for 1997
predictftfz97: fz FT Experience must be predicted for 1997
predictfz98: fz Total Experience must be predicted for 1998
predictftfz98: fz FT Experience must be predicted for 1998
predict1999: Total Experience must be predicted for 1999
predictft1999: FT Experience must be predicted for 1999
predictfz99: fz Total Experience must be predicted for 1999
predictftfz99: fz FT Experience must be predicted for 1999
predictfz00: fz Total Experience must be predicted for 2000
predictftfz00: fz FT Experience must be predicted for 2000
predict01: Total Experience must be predicted for 2001
predictft01: FT Experience must be predicted for 2001
predictfz01: fz Total Experience must be predicted for 2001
predictftfz01: fz FT Experience must be predicted for 2001
predictfz02: fz Total Experience must be predicted for 2002
predictftfz02: fz FT Experience must be predicted for 2002
predict03: Total Experience must be predicted for 2003
predictft03: FT Experience must be predicted for 2003
predictfz03: fz Total Experience must be predicted for 2003
predictftfz03: fz FT Experience must be predicted for 2003
predictfz04: fz Total Experience must be predicted for 2004
predictftfz04: fz FT Experience must be predicted for 2004
predictfz06: fz Total Experience must be predicted for 2006
predictftfz06: fz FT Experience must be predicted for 2006
predict2007: Total Experience must be predicted for 2007
predictft2007: FT Experience must be predicted for 2007
predictfz07: fz Total Experience must be predicted for 2007
predictftfz07: fz FT Experience must be predicted for 2007
predictfz08: fz Total Experience must be predicted for 2008
predictftfz08: fz FT Experience must be predicted for 2008
predict2009: Total Experience must be predicted for 2009
predictft2009: FT Experience must be predicted for 2009
predictfz09: fz Total Experience must be predicted for 2009
predictftfz09: fz FT Experience must be predicted for 2009
predictfz10: fz Total Experience must be predicted for 2010
predictftfz10: fz FT Experience must be predicted for 2010
predict2011: Total Experience must be predicted for 2011
predictft2011: FT Experience must be predicted for 2011
predictfz11: fz Total Experience must be predicted for 2011
predictftfz11: fz FT Experience must be predicted for 2011
origAdvHD: Adv is advanced degree
origAdvWF:
origBAHD: BA is bachelor’s degree
origBAWF:
origannWeeksHDE: annWeeks is annual weeks worked E means currently employed
origannWeeksHDR: R means currently retired
origannWeeksHDU: U means currently not employed
origannWeeksWFE:
origannWeeksWFR:
origannWeeksWFU:
origindHDE: ind is industry
origindWFE:
origindHDU:
origindWFU:
origindHDR:
origindWFR:
origoccHDE: occ is occupation
origoccHDR:
origoccHDU:
origoccWFE:
origoccWFR:
origoccWFU:
origrace: race is race
origschHD: sch is years of schooling
origschWF:
origyrHghstDe~D: yrHghstDegHD is year of highest degree for head
origyrHghstDe~F:
origwtrCollDe~D: whether college degree
origwtrCollDe~F:
origwtrCollHD: whether attended college
origwtrCollWF:
predict: ==1 if Logit Prediction Needed for ANY gap year
predictft: ==1 if FT Logit Prediction Needed for ANY gap year
smsa: SMSA dummy variable variable
perconexp: T-1 Personal Consumption
Expenditure:
hrwage: hourly wage
annhrs2: alternate measure of annual hours worked
expendbase10: level of National Income and Products Account Personal Consumption Expenditures (PCE) price deflator for 2010
inflate: inflation factor to multiply earnings by in order to convert to 2010 dollars
realhrwage: Real Hourly Wage in 2010 PCE corrected dollars
immigrantsamp: Immigrant Sub-Sample (zero for everyone since we don’t use the immigrant subsample)
northeast: Region: North-East
northcentral: Region: North-Central
south: Region: South
west: Region: West, Alaska and Hawaii
lnrealwg: Log(Real Hourly Wage)
ft: full time work dummy variable
potexp: potential experience (age-years of schooling-6) truncated to be between 0 and age-18
potexp2: potential experience squared
ba: bachelor's Degree
adv: advanced Degree
military: zero for everyone since we study civilians
basesamp: this is base sample, which is 1 for everyone in this data set
wagesamp: this is wage sample
female:
ind2: 2-digit Industry
occ2: 2-digit Occupation
occ2name:
Agriculture:
miningconstru~n: Ind: Mining and Construction
durables: Ind: Durables Manufacturing
nondurables: Ind: Non-durables Manufacturing
Transport: Ind: Transport
Utilities: Ind: Utilities
Communications: Ind: Communications
retailtrade: Ind: Retail Trade
wholesaletrade: Ind: Wholesale Trade
finance: Ind: Finance
SocArtOther: Ind: Social Work, Arts and Recreation, Other Services
hotelsrestaur~s: Ind: Hotels and Restaurants
Medical: Ind: Medical
Education: Ind: Education
professional: Ind: Professional Services
publicadmin: Ind: Public Administration
sumind: this is the sum of industry dummy variables, which is 1 for everyone
manager: Manager
business: Business Operations Specialists
financialop: Financial Operations Specialists
computer: Computer and Math Technicians
architect: Architects an Engineers
scientist: Life, Physical and Social Sciences
socialworker: Community and Social Workers
postseceduc: Post-secondary educators
legaleduc: Other Education, Training, Library and Legal Occupations
artist: Arts, Design, Entertainment, Sports and Media
lawyerphysician: Physicians and Dentists
healthcare: Nurses and HealthCare Practitioners and Technicians
healthsupport: Healthcare Support Occupations
protective: Protective Service Occupations
foodcare: Food Preparation and Serving and Personal Care Services
building: Building and Grounds Cleaning and Maintenance
sales: Sales and Related
officeadmin: Office and Administrative Support
farmer:
constructextr~l: Construction, Extraction, Installation
production: Production
transport: Transportation and Materials Moving
sumocc: this is sum of the occupation dummy variables which is 1 for everyone
LEHS: High School or Less
CPS variables:
&gt; NOTES: VARIABLES WITH A q AT THE BEGINNING ARE DATA QUALITY FLAGS. ANY VALUE GREATER THAN ZERO INDICATES SOME ISSUE WITH DATA QUALITY. THE EARNINGS DATA WITH ZEROS WAS ONLY USED DURING THE CREATION OF THIS VARIABLE. DUE TO LACK OF DATA AVAILABILITY for 1981, ALL OF THE HOURS AND WEEKS DATA WERE FORCED TO USE REGARDLESS OF THE DATA QUALITY FLAG.
THE ORIGINAL CPS VARIABLES HAVE BEEN KEPT.
OCCUPATION AND INDUSTRY WERE NOT USED IN THE CPS ANALYSIS.
THE VARIABLES WITH tc AT THE BEGINNING INDICATE TOPCODED VALUES.
year: Survey year
serial: Household serial number
numprec: Number of person records following
hwtsupp: hwtsupp_lbl Household weight, Supplement
gq: gq_lbl Group Quarters status
region: region_lbl Region and division
statefip: statefip_lbl State (FIPS code)
metro: metro_lbl Metropolitan central city status
metarea: metarea_lbl Metropolitan area
county: FIPS county code
farm: farm_lbl Farm (1=this is a farm, 2= it’s not a farm)
month: month_lbl Month
pernum: Person number in sample unit
wtsupp: Supplement Weight
relate: relate_lbl Relationship to household head (Head/hous=101, Spouse=201, Child=301, Stepchild=303, Parent=501, Sibling=701, Grandchil=901, Other rel=1001, Partner/r=1113, Unmarried=1114, Housemate=1115, Roomer/bo=1241, Foster ch=1242, Other non=1260)
age: age_lbl Age
sex: sex_lbl Sex (1=male, 2=female)
race: raceLbl Race (White nonhisp=1, Black nonhisp=2, Hispanic=3, Other nonhisp=4)
marst: marst_lbl Marital status (Married, spouse present=1, Married, spouse absent=2, Separated=3, Divorced=4, Widowed=5, Never mar=6)
popstat; popstat_lbl Adult civilian, armed forces, or Child (1 for everyone—we include only civilian adults)
bpl: bpl_lbl Birthplace
yrimmig: yrimmig_lbl Year of immigration
citizen: citizen_lbl Citizenship status
mbpl: mbpl_lbl Mother's birthplace
fbpl: fbpl_lbl Father's birthplace
nativity: nativity_lbl Foreign birthplace or parentage
hispan: hispan_lbl Hispanic origin
sch: educLbl Educational attainment recode (None=0, 1=1, Grades 1=2, 2.5=2.5, 3=3, 4=4, Grades 5=5, 5.5=5.5, 6=6, Grades 7=7, 7.5=7.5, 8=8, Grade 9=9, Grade 10=10, Grade 11=11, Grade 12=12, Some Coll=13, Assoc.=14, BA=16, Adv. Degr=18)
educ99: educ99_lbl Educational attainment, 1990, available for 1999 and later (No school=1, 1st-4th g=4, 5th-8th g=5, 9th grade=6, 10th grad=7, 11th grad=8, 12th grad=9, High scho=10, Some coll=11, Associate=13, Associate=14, Bachelors=15, Masters d=16, Professio=17, Doctorate=18)
schlcoll: schlcoll_lbl School or college attendance; available only in 2013 (High school full time=1, High school part time=2, College or univ full time=3, College or univ part time=4, Does not attend school=5)
empstat: empstat_lbl Employment status (At work=10, Has job, not at work now=12)
labforce: labforce_lbl Labor force status everyone gets a 2—in the labor force
occ: occ_lbl Occupation
occ1990: occ1990_lbl Occupation, 1990 basis
ind1990: ind1990_lbl Industry, 1990 basis
occ1950: occ1950_lbl Occupation, 1950 basis
ind: ind_lbl Industry
ind1950: ind1950_lbl Industry, 1950 basis
classwkr: classwkr_lbl Class of worker (Self-empl=10, Wage/salary, private sector=21, Wage/salary, government=24, Federal govt employee=25, State govt employee=27, Local govt employee=28, Unpaid family worker=29)
occly: occly_lblOccupation last year
occ50ly: occ50ly_lbl Occupation last year, 1950 basis
indly: indly_lbl Industry last year
ind50ly: ind50ly_lbl Industry last year, 1950 basis
classwly: classwly_lbl Class of worker last year (Self-employed=14, Wage/salary private=22, Federal govt=25, State gov=27, Local gov=28, Unpaid family worker=29)
wkswork1: wkswork1_lbl Weeks worked last year
wkswork2: wkswork2_lbl Weeks worked last year, intervalled
hrswork: hrswork_lbl Hours worked last week
uhrswork: uhrswork_lbl Usual hours worked per week (last yr)
union: union_lbl Union membership (only available for outgoing rotation group) (NIU=0, No union coverage=1, Member of labor union=2, Covered by union but not a member=3)
incwage: incwage_lbl Wage and salary income
incbus: incbus_lbl Non-farm business income
incfarm: incfarm_lbl Farm income
inclongj: inclongj_lbl Earnings from longest job
oincwage: oincwage_lbl Earnings from other work included wage and salary earnings
srcearn: srcearn_lbl Source of earnings from longest (1=wage and salary; 4=without pay) job
ftype: ftype_lbl Family Type (Primary family=1, Nonfamily householder=2, Related subfamily=3, Unrelated subfamily=4, Secondary individual=5)
quhrswor: quhrswor_lbl Data quality flag for UHRSWORK
qwkswork: qwkswork_lbl Data quality flag for WKSWORK1 and WKSWORK2
qincbus: qincbus_lbl Data quality flag for INCBUS
qincfarm: qincfarm_lbl Data quality flag for INCFARM
qinclong: qinclong_lbl Data quality flag for INCLONGJ
qincwage: qincwage_lbl Data quality flag for INCWAGE
qsrcearn: qsrcearn_lbl Data quality flag for SRCEARN
o_numprec: Original Number of person records following'
o_hwtsupp: Original Household weight, Supplement'
o_gq: Original Group Quarters status'
o_region: Original Region and division'
o_statefip: Original State (FIPS code)'
o_metro: Original Metropolitan central city status'
o_metarea: Original Metropolitan area'
o_county: Original FIPS county code'
o_farm: Original Farm'
o_month: Original Month'
o_pernum: Original Person number in sample unit'
o_wtsupp: Original Supplement Weight'
o_relate: Original Relationship to household head'
o_age: Original Age'
o_sex: Original Sex'
o_race: Original Race'
o_marst: Original Marital status'
o_popstat: Original Adult civilian, armed forces, or child'
o_bpl: Original Birthplace'
o_yrimmig: Original Year of immigration'
o_citizen: Original Citizenship status'
o_mbpl: Original Mother's birthplace'
o_fbpl: Original Father's birthplace'
o_nativity: Original Foreign birthplace or parentage'
o_hispan: Original Hispanic origin'
o_educ: Original Educational attainment recode'
o_educ99: Original Educational attainment, 1990'
o_schlcoll: Original School or college attendance'
o_empstat: Original Employment status'
o_labforce: Original Labor force status'
o_occ: Original Occupation'
o_occ1990: Original Occupation, 1990 basis'
o_ind1990: Original Industry, 1990 basis'
o_occ1950: Original Occupation, 1950 basis'
o_ind: Original Industry'
o_ind1950: Original Industry, 1950 basis'
o_classwkr: Original Class of worker'
o_occly: Original Occupation last year'
o_occ50ly: Original Occupation last year, 1950 basis'
o_indly: Original Industry last year'
o_ind50ly: Original Industry last year, 1950 basis'
o_classwly: Original Class of worker last year'
o_wkswork1: Original Weeks worked last year'
o_wkswork2: Original Weeks worked last year, intervalled'
o_hrswork: Original Hours worked last week'
o_uhrswork: Original Usual hours worked per week (last yr)'
o_union: Original Union membership'
o_incwage: Original Wage and salary income'
o_incbus: Original Non-farm business income'
o_incfarm: Original Farm income'
o_inclongj: Original Earnings from longest job'
o_oincwage: Original Earnings from other work included wage and salary earnings'
o_srcearn: Original Source of earnings from longest job'
o_ftype: Original Family Type'
o_quhrswor: Original Data quality flag for UHRSWORK'
o_qwkswork: Original Data quality flag for WKSWORK1 and WKSWORK2'
o_qincbus: Original Data quality flag for INCBUS'
o_qincfarm: Original Data quality flag for INCFARM'
o_qincwage: Original Data quality flag for INCWAGE'
origrace: race_lbl Race
white: white nonhispanic dummy variable
black: black nonhispanic dummy variable
hisp: Hispanic dummy variable
othrace: other race dummy variable
educorig: original education codes (see CPS documentation site mentioned above)
ba: bachelor’s degree dummy variable
adv: advanced degree dummy variable
groupquar: group quarters dummy variable. Zero for everyone.
potexp: age-yrs of schooling-6
potexp2: potexp squared
selfemp: self employment dummy variable. Zero for everone.
military: =1 if military (Based on popstat variable). Variable is zero for everyone.
employed: equals 1 for everyone
annhrs: annual work hours
ft: full time work dummy variable
niincwage: Not Imputed incwage
incwageman: Manually Created INCWAGE
tcoincwage
tcinclongj
tcincwage: True Topcoded INCWAGE (Includes Imputed Values)
hrwage: hourly wage
perconexp: T-1 Personal Consumption Expenditure
expendbase10: 2010 PCE value
inflate: inflation factor for expressing wages in 2010 dollars
realhrwage: Real Hourly Wage, inflated to 2010 dollars
uncenrealhrwage: Real Hourly Wage (same as realhrwage)
lnrwg: log of real hourly wage
hdwfcoh: Head/Wife/Cohabitator Indicator
notalloc: not allocated wage. Equals 1 for everyone.
basesamp: base sample; 1 for everyone
wagesamp: wage sample dummy variable
occ_orig:
adj_occ: in some years, occupation has four digits and in others it has 3. This expresses occupation in 3 digits
occ_2010_orig
ind_orig
adj_ind
ind_2002_orig
ind_2007_orig
occ_81
ind_81
occ_2000female: 2000 Occupation Code
unmatched_fe~81
occ_2000male: 2000 Occupation Code
unmatched_ma~81
ind_2000
occ2000_81: 1981 occupation codes converted to 2000 codes
ind2000_81: 1981 industry codes converted to 2000 codes
occ_1990
ind_1990
occ_1999
ind_1999
unmatched_oc~90
occ2000_90: 1990 occupation codes converted to 2000 codes
unmatched_in~90
ind2000_90: 1990 industry codes converted to 2000 codes
indname2000_90
unmatched_oc~99
occ2000_99: 1990 occupation codes converted to 2000 codes
unmatched_in~99 
ind2000_99: 1990 industry codes converted to 2000 codes
indname2000_99
un_lnrealwg: Log of Real Hourly Wage
northeast: Region: North-East
northcentral: Region: North-Central
south: Region: South
west: Region: West, Alaska and Hawaii
female
adj_ind2
adj_occ2
adj_occ2name
Agriculture
miningconstru~n: adj_ind: Mining and Construction
durables: adj_ind: Durables Manufacturing
nondurables: adj_ind: Non-durables
Manufacturing
Transport: adj_ind: Transport
Utilities: adj_ind: Utilities
Communications: adj_ind: Communications
retailtrade: adj_ind: Retail Trade
wholesaletrade: adj_ind: Wholesale Trade
finance: adj_ind: Finance
SocArtOther: adj_ind: Social Work, Arts and Recreation, Other Services
hotelsrestaur~s: adj_ind: Hotels and Restaurants
Medical: adj_ind: Medical
Education: adj_ind: Education
professional: adj_ind: Professional Services
publicadmin: adj_ind: Public Administration
sumadj_ind: sum of industry dummy variables
manager: Manager
business: Business Operations Specialists
financialop: Financial Operations Specialists
computer: Computer and Math Technicians
architect: Architects an Engineers
scientist: Life, Physical and Social Sciences
socialworker: Community and Social Workers
postseceduc: Post-secondary educators
legaleduc: Other Education, Training, Library and Legal adj_occupations
artist: Arts, Design, Entertainment, Sports and Media
lawyerphysician: Physicians and Dentists
healthcare: Nurses and HealthCare Practitioners and Technicians
healthsupport: Healthcare Support adj_occupations
protective: Protective Service adj_occupations
foodcare: Food Preparation and Serving and Personal Care Services
building: Building and Grounds Cleaning and Maintenance
sales: Sales and Related
officeadmin: Office and Administrative Support
farmer
constructextr~l: Construction, Extraction,
Installation
production: Production
transport: Transportation and Materials
Moving
sumadj_occ: sum of occupation dummy variables
LEHS: dummy for less than or equal to high school
Acknowledgements
Journal Article:
Blau, Francine D. Kahn, Lawrence M. The Gender Wage Gap: Extent, Trends, and Explanations Journal of Economic Literature 55 3 789-865 2017"	272	2412	21	fedesoriano	gender-pay-gap-dataset
2660	2660	UbiquantTrainn		[]		0	11	0	mbonyani	ubiquanttrainn
2661	2661	bike_data	12 months of Cyclistic trip data.	['united states', 'business', 'cycling', 'tabular data', 'travel']	"Context
Cyclistic: A bike-share program that features more than 5,800 bicycles and 600 docking stations
Content
Cyclistic’s historical trip data.Note: The datasets have a different name because Cyclistic is a fictional company. For the purposes of this case study, the datasets are appropriate and will enable you to answer the business questions. The data has been made available by Motivate International Inc. under license.)"	0	17	0	zhantorenurbay	bike-data
2662	2662	reef-yolox-m-v10		[]		0	6	0	sangayb	reef-yolox-m-v10
2663	2663	mymusicalpref		[]		0	1	0	markovrga	mymusicalpref
2664	2664	KNN_model		[]		0	5	0	lionelbottan	knn-model
2665	2665	bank customers churn	Focused customer retention programs	['business', 'e-commerce services']		3	71	3	lashkingl	bank-customers-churn
2666	2666	dataDictionary WiDS 2022		[]		0	10	0	nicolasperez123	datadictionary-wids-2022
2667	2667	Soccer Dataset - Barclays Premier League	The players, managers, clubs, results and stats from 1992/93 through 2020/21.	['football']	"This soccer dataset includes the results of all the English Premier League matches from 1992/93 through 2020/21 seasons, with the teams' respective starting squads and their bench for each match. It has the detailed results of each match, with the events including goal scores, assists, yellow/red cards, and substitution information with the IDs of players being swapped, all of them along with the exact minute at which the event took place. 
Starting from 2006/07 season, it also has more details (stats) for each result, including the assists, goals scored by penalty or own goal. Furthermore, the rows of these more recent matches contain their respective stats, including the possession percentage, number of corners, shots, shots on target, passes, tackles, etc. for each team.
The dataset was scraped from https://www.premierleague.com/.
As a clarification, the type_of_stat column in player_performance.csv can represent different information depending on its values:
 goal = 1
 goal (by penalty) = 2
 assist = 3
 own goal = 4
 red card = 5
 Second Yellow Card (and/or Red Card) = 6
Hope you find this useful."	101	488	2	narekzamanyan	barclays-premier-league
2668	2668	detoxify-models		['health']		0	2	0	bachan	detoxify-models
2669	2669	IsolationForest		[]		0	6	0	lionelbottan	isolationforest
2670	2670	segmentation_masks		[]		0	4	0	kagglegrandmaster1	segmentation-masks
2671	2671	xlnet-base-cased		[]		0	2	0	bachan	xlnet-base-cased
2672	2672	roberta-base		['arts and entertainment']		0	2	0	bachan	roberta-base
2673	2673	bert-base-uncased		[]		0	2	0	bachan	bert-base-uncased
2674	2674	images		[]		1	7	0	yuggarg	images
2675	2675	happy-face		[]		1	7	0	prathamgoyal	happyface
2676	2676	scRNA-seq HeLaS3 wild-type and AGO2KO (knockout)	GSE142277 The transcriptome dynamics of single cells during the cell cycle. 	['genetics', 'biology', 'biotechnology', 'cancer']	"Content
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes.
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Source: Nikolaus Rajewsky Lab 2020:
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE142277
The present data for cell ""HeLa"" - the first immortal human cell line ever established:
https://en.wikipedia.org/wiki/HeLa
Two variants of data (see last letters in filenames)
WT - wild type https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4224315
KO - knockout https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4224316
(Monoclonal cell line with both AGO2 alleles knock-out (AGO2dKO))
Acknowledgements
Source: Nikolaus Rajewsky Lab 2020:
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE142277
Paper:
https://pubmed.ncbi.nlm.nih.gov/33205894/
Mol Syst Biol. 2020 Nov;16(11):e9946. doi: 10.15252/msb.20209946.
The transcriptome dynamics of single cells during the cell cycle
Daniel Schwabe 1, Sara Formichetti 2 3 4, Jan Philipp Junker 5, Martin Falcke 1 6, Nikolaus Rajewsky 2
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6"	0	176	2	alexandervc	scrnaseq-helas3-wildtype-and-ago2ko-knockout
2677	2677	Basic Math Operation	Classify numbers and basic mathematics operation images	['education']		2	30	1	joshuawzy	basic-math-operation
2678	2678	artounk		[]		0	17	0	jiccrlla	artounk
2679	2679	traintest		[]		0	12	0	markovrga	traintest
2680	2680	Ortophotos of São Paulo city in 2017	Spatial resolution of 0.12m RGB covering all of 1.524Km2	['cities and urban areas', 'brazil', 'geospatial analysis']	"Ortofotos 2017
Repositório espelho do site GeoSampa contendo as imagens das ortofotos de 2017 da Cidade de São Paulo
Objetivo
Esse repositório tem por objetivo espelhar os downloads realizados diretamente do site do GeoSampa das ortofotos do voo de 2017 sobre a cidade de São Paulo
Motivação
O site do GeoSampa é um bem público e possui limite de banda para download afim de, entre outros motivos, democratizar o acesso a informaçao. Muitas vezes necessitamos de imagens de toda a cidade para algum processamento ou levantamento, como diversos aqui nesta pesquisa. No entanto, algumas vezes o processo de download pode ser lento e exigir estratégias de abordagem. Dessa forma, aproveitando os limites disponibilizados pelo GitHub, sua capacidade de banda, e os esforços de download já realizados, aproveito para compartilhar com quem necessite os arquivos por aqui, de forma íntegra do download realizado no site do GeoSampa em 31/01/2022.
Método
AS URLs foram geradas a partir do arquivo de articulalção de imagens de 2017, classificados pelo SCM (https://github.com/geoinfo-smdu/SCM). De posse das URLs os arquivos foram baixados de forma automatizada utilizando Python com a biblioteca Selenio que controla uma instância do Google Chrome aberta. Esse  processo teve que ser realizado pois os links acionam JavaScript no Browser para permitir o download. Os arquivos levaram cerca de 16 horas para serem baixados e então foram validados com o arquivo de articulação de imagens. Dessa forma, os poucos downloads que falharam puderam ser baixados manualmente para completar o conjunto de arquivos. 
Os arquivos então foram descomprimidos, validados novamente pelo arquivo de articulação de imagens na pasta RGB-2017. Como não foi possível commitar todos os arquivos aqui no repositório, os arquivos foram zipados em 20 volumes e disponibilizados em um DataSet do Kaggle disponível em https://www.kaggle.com/andasampa/ortofotos-2017-RGB
O método está disponibilizado no Jupyter Notebook aqui no repositório intitulado Download de arquivos de imagens.ipynb e o script de download no arquivo download-imagens.py
Resultados
Os resultados, ou seja, os arquivos de imagens RGB de 2017 baixados do GeoSampa podem então ser acessados pelo DataSet do Kaggle (https://www.kaggle.com/andasampa/ortofotos-2017-RGB). Eles estão organizados pelo SCM (https://github.com/geoinfo-smdu/SCM) com os 3 primeiros dígitos, conforme legenda abaixo. Dessa forma é possível baixar apenas porções específicas da cidade conforme a necessidade."	7	65	5	andasampa	ortofotos-2017-rgb
2681	2681	dasaaq		[]		0	2	0	ouzylmz12	dasaaq
2682	2682	Forecast		[]		1	16	1	ayusha232	forecast
2683	2683	miniProj		[]		0	0	0	fyyrikka9	miniproj
2684	2684	BERT-SBIC-offensive		[]		0	9	0	dzisandy	bertsbicoffensive
2685	2685	miniProject		[]		0	5	0	fyyrikka9	miniproject
2686	2686	rsna_bt_tfrec_flair_512		[]		0	5	0	atamazian	rsna-bt-tfrec-flair-512
2687	2687	DocBank		[]	collected from : https://doc-analysis.github.io/docbank-page/	0	24	0	haque1407028	docbank
2688	2688	beep-KcELECTRA-base-hate		[]		0	7	0	dzisandy	beepkcelectrabasehate
2689	2689	Cuisine	Ingredients of Cuisine of Pakistan China and Arab	['cooking and recipes']		0	11	0	duaazehraalvi	final-data
2690	2690	pakistan_education_Tweets_Dataset		[]		0	29	1	furqanamjad	pakistan-education-tweets-dataset
2691	2691	Spanish_Tweets_Dataset		[]		0	15	1	furqanamjad	spanish-tweets-dataset
2692	2692	apple_orange		[]		0	6	0	theblackmamba31	apple-orange
2693	2693	feedback-ma		[]		0	11	1	mathurinache	feedbackma
2694	2694	Train Stations in Europe	Names, Coordinates, and Properties of European Railway Stations	['europe', 'transportation', 'geospatial analysis']	"Context
Many European countries possess an extensive net of public transport railroads which connect large and small cities. This dataset contains the names, coordinates, and basic properties of more than 36000 train stations in (and adjacent to) Europe. It was derived from data provided by the Trainline EU ticketing website that has been published on github. I will update this dataset regularly. 
Note, that the data contains a few train stations in the European parts of Russia and Turkey, as well as a small number of stations in the African country of Morocco.
Content
The dataset train_stations_europe.csv is based on the Trainline EU github repo. It contains 36k+ stations at the time of creation of this Kaggle Dataset. The github dataset contains many more columns, most of which are covering operator-specific properties (e.g. Renfe or Trenitalia) or translations into different languages (most of which are missing, though). I decided to extract this subset to provide a more focussed and complete data source.
Column Description
Most of those descriptions have been taken verbatim from the github repo. I have added some extra context info or explanations where I felt they were necessary. Note, that some columns contain a significant percentage of NA values.
id: Numeric internal unique identifier. Primary key.
name: Name of the station as it is locally known. These names include accents and other special characters.
name_norm: Normalised version of name; transformed into [A-Za-z] character space (aka 'Latin-ASCII') to replace special characters with their standard-Latin counterparts (e.g. è become e, ü becomes u).
uic: The UIC code of the station. UIC is the International Union of Railways, ""an international rail transport industry body"". About 1/3 of all stations have no UIC code in this dataset.
longitude & latitude: Station coordinates. About 5% of all stations have no coordinates in this dataset.
parent_station_id: A station can belong to a meta station whose id is this value, i.e. Paris Gare d’Austerlitz (id = 4921) belongs to the meta-station Paris (id = 4916). About 92% of rows have NA entries.
country: Country codes in ISO 3166-1 alpha-2 format (2 digits). 
time_zone: Continent/Country ISO codes. Those appear to be equivalent to Olson names (e.g. ""Europe/Berlin"").
is_city: Marked as ""unreliable"" in the source dataset. Might be worth investigating what exactly that means.
is_main_station: Marked as ""unreliable"" in the source dataset. Might be worth investigating what exactly that means.
Acknowledgements
All credit for creating this dataset and providing the public version goes to the Trainline EU team. 
Banner and vignette photo by Michał Parzuchowski on Unsplash.
Licence
Data is distributed under the Open Database License (ODbL) licence, see here. In short, any modification to this data source must be published."	461	4965	29	headsortails	train-stations-in-europe
2695	2695	Cuisine2	Pakistan's Arab's and China's Cuisine	['cooking and recipes']		0	2	0	duaazehraalvi	final-labexam-mid-3
2696	2696	Olist - Clients Segmentation		[]		2	24	0	mickaelnarboni	olist-clients-segmentation
2697	2697	FipeZap-SP_01-2008_09-2021	Índice de preços de imóveis na cidade de São Paulo, de jan/2008 até dez/2021	['brazil', 'housing']	"&gt; O Índice FipeZAP+ é o primeiro índice de preço com abrangência nacional que acompanha os preços de imóveis residenciais e comerciais. O índice é calculado pela Fipe com base em informações anúncios de imóveis (apartamentos prontos, salas e conjuntos comerciais de até 200 m²) para venda e locação veiculados nos portais ZAP+.
Retirado do site da Fundação Instituto de Pesquisas Econômicas, à qual todos os créditos pelos dados e captação são devidos."	3	294	0	ricardoalvesgomes	fipezapsp-012008-022021
2698	2698	KPIs for the Mexican touristic GDP	Key performance indicators for prediction of the mexican touristic GDP	['news']		0	26	2	hdcortes	kpis-for-the-mexican-touristic-gdp
2699	2699	IDAO_Dataset		[]		0	15	0	anirudhvadakedath	idao-dataset
2700	2700	Public_Books_Dataset	Books_Dataset_For _Recommender_Systems	['literature', 'intermediate', 'feature engineering', 'recommender systems', 'tabular data', 'text data']	"Context
This Dataset is a collection Public Books 7000+ Public Books with their Titles,Authors,Edition,Reviews,Ratings, Synopsis,Genre and Book Categories. It Contains full information and no Missing Values. It is not annotated i.e Labels are not provided therefor it cant be used for classification but is ideal for Book Recommenders, EDA and Unsupervised Learning..
Content
Data Was used as a Hackathon for predicting Book prices with Labels provided. It was updated and Cleaned and provided for public use to be used for other purposes as deemed necessary.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
This Dataset may be used to create a book recommender systems and for other purposes  ?"	3	27	1	joshuaokechukwu	public-books-dataset
2701	2701	WiDSDatathon2022		[]		0	16	0	muddywaters23	widsdatathon2022
2702	2702	XLNET_MUL_CLEAN_try2_6epochs		[]		0	9	0	hangy132	xlnet-mul-clean-try2-6epochs
2703	2703	Quandl WIKI Prices US Equites	End of day stock prices, dividends and splits for 3,000 US companies	['finance', 'tabular data', 'investing']	"It is very difficult to find institutional quality equity pricing data with sufficient cross-sectional coverage for free. This dataset is the rare exception. The data was made freely available in the public domain by Quandl, a data aggregator and provider acquired by Nasdaq and now operating under the brand ""Nasdaq Data Link."" Unfortunately this data stopped being maintained by Quandl on April 11, 2018. Nonetheless, it remains an important and useful dataset for research."	9	149	8	marketneutral	quandl-wiki-prices-us-equites
2704	2704	ramses18		['arts and entertainment']		0	16	0	asiooo	ramses18
2705	2705	train_data		[]		4	29	0	tonymarkchris	train-data
2706	2706	data-door		[]		2	11	0	deepaksuthr	datadoor
2707	2707	model_checkpoints		[]		0	21	0	flowjan	model-checkpoints
2708	2708	Bonos Dolar MEP	Cotizaciones de bonos AL29, AL30, AL35, GD29, GD30, GD35 al cierre en $ y u$s	['arts and entertainment', 'business', 'finance', 'investing']	"Contexto
La necesidad de contar con un dataset para poder calcular y comparar distintas cotizaciones de dolar MEP 🇦🇷
Info sobre peso Argentino y dolar MEP
Descripción
El archivo contiene las cotizaciones de cierre de los bonos: AL29, AL30, AL35, GD29, GD30, GD35,  y GD38, en pesos y series D. 
Actualización
Mientras la fuente del scrap este disponible, la idea es actualizarlo por semana"	23	1084	2	juanfe	bonos-dolar-mep
2709	2709	Semantic Segmentation		['earth and nature']		0	11	0	anshumankaneki	semantic-segmentation
2710	2710	The GOAT of Tennis	Who is The GOAT of Tennis ?	['sports', 'beginner', 'intermediate', 'data visualization', 'text data']	"Context
This Data Shows' The all BIg Event of Tennis so its Now Your turn to visualize and decide who is the goat of tennis. 
Content
All big event title winner and runner up details.
Acknowledgements
this dataset is only for educational purpose and for fun.
Inspiration
inspiration go tableau."	177	1518	26	sandipdevre	the-goat-of-tennis
2711	2711	Ubiquant RNN Models		[]		1	4	0	ayuraj	ubiquant-rnn
2712	2712	Medium 2020/21 articles with numerical stats	Features of more than 150 000 data science articles	['websites', 'computer science', 'text mining', 'tabular data', 'text data']	"Dataset consists of merged data science Web articles data for 2020 and 2021. Original datasets were obtained by Vinicius Lambert (https://www.kaggle.com/viniciuslambert) scraping Medium and other popular data science article platforms. Title, subtitle and author name are cleaned from stopwords, lemmatized and transformed to lowercase; other textual features are left unchanged.
25 numerical features were extracted from the text and added to this dataset as a contribution:
- sum of claps, responses and reading time received by the author before posting a new article
- sum of claps, responses and reading time received by the author for previous articles with the same tag
- length of preprocessed title, subtitle and author in words
- number of numericals in preprocessed title, subtitle and author
- number of jargon and technical terms (words that are not present in NLTK English dictionary) in title and subtitle text
Original dataset for 2020: https://www.kaggle.com/viniciuslambert/medium-data-science-articles-dataset
Original dataset for 2021: https://www.kaggle.com/viniciuslambert/medium-2021-data-science-articles-dataset
Several minor data fixes and additional features generation are planned."	2	57	1	evgenyparenchenkov	medium-202021-mlds-articles-numerical-stats
2713	2713	HJDataset		[]	collected from : https://dell-research-harvard.github.io/HJDataset/	0	9	0	haque1407028	hjdataset
2714	2714	Crown_of_thorns_starfish		[]		0	4	0	hugonaya	crown-of-thorns-starfish
2715	2715	test daata stage2		[]		0	10	0	torliko	test-daata-stage2
2716	2716	Salary Dataset		[]		2	16	0	priy998	salary-dataset
2717	2717	NBME-Bert-V1		['standardized testing']		4	32	2	tchaye59	nbmebertv1
2718	2718	distilbert_base_multilingual_cased		[]		0	6	0	ksherho	distilbert-base-multilingual-cased
2719	2719	XLNET_MUL_CLEAN_try2_skf		[]		0	5	0	hangy132	xlnet-mul-clean-try2-skf
2720	2720	fashion		['clothing and accessories']		7	15	0	rahilmehtaucoe2784	fashion
2721	2721	recognition natrual datasets tfrecords		['earth and nature']		0	10	0	riadhossainapsis	recognition-natrual-datasets-tfrecords
2722	2722	🐋🐬Happywhale: Augmented	Augmented competition data	['earth and nature', 'fish and aquaria']	"This dataset contains augmented images for Happywhale - Whale and Dolphin Identification.
RandomSunFlare
RandomFog
RandomBrightness
Rotate
RGBShift
RandomSnow
HorizontalFlip
VerticalFlip
RandomContrast
HSV"	0	220	8	ruchi798	happywhale-augmented
2723	2723	RSNA Abnormal Series Labels		['health']		0	9	0	fereshtej	rsna-abnormal-series-labels
2724	2724	lstm_3_expert_models		[]		0	6	0	narminj	lstm-3-expert-models
2725	2725	cascade_22		[]		1	36	0	megha1424	cascade-22
2726	2726	brain_tumor_preprocessed_dataset	Preprocessed version of tumor dataset with pretrained model.	['cancer']		1	10	0	suhasp58	brain-tumor-preprocessed-dataset
2727	2727	038-exp		[]		0	2	0	shimishige	038-exp
2728	2728	UShouseprices		[]		0	1	0	chiranjeevbit	ushouseprices
2729	2729	jigsaw4_exp016_mlm		[]		16	9	0	yutoshibata	jigsaw4-exp016-mlm
2730	2730	Brain_tumour_dataset_split		[]		0	3	0	suhasp58	brain-tumour-dataset-split
2731	2731	VOC_Bounding_Boxes		[]		1	19	0	kaushaljadhav	voc-bounding-boxes
2732	2732	Sex Offender Registry	DC Data On Sex Offenders (public domain)	['crime', 'tabular data', 'datetime']	"Context
Sex Offender work and home locations, created as part of the DC Geographic Information System (DC GIS) for the D.C. Office of the Chief Technology Officer (OCTO) and participating D.C. government agencies. If users want to obtain more information about sex offenders, they should go to the Sex Offender Mapping Application (here) and download the “More Details” PDF. A database provided by the Court Services and Offender Supervision Agency identified sex offender registry providing location at the block level. https://www.csosa.gov/.
File information
the data set is CSV file
Content
Column Features
This data contain 
1.ADDDATE
2.ALIASES
3.BIRTHDATE
4.Date or Time 
5.BLOCK_X
6.BLOCK_Y
7.BLOCKNAME
8.DCS_LAST_MOD_DTM
9.DISTRICT
10.EYECOLOR
11.FIRSTNAME
12.HAIRCOLOR
13.HEIGHTNUM
14.LASTNAME
15.MARKINGS
16.MAXCLASSIFICATION
17.OBJECTID
18.Unique ID
19.PSA
20.QUADRANT
21.REGISTRATIONDATE
22.SEXOFFENDERCODE
23.Shape
24.TYPE
25.WEIGHTNUM
26.ZIP
additional info
The Sex Offender Registry database provides information on Class A and Class B sex offenders only. If you would like to see a complete list of all Class A, B, and C Offenders, you must go, in person, to one of the MPDC Registry Book locations. For a complete listing of locations, click here. Descriptions of the different Offender Classes are listed below.
Class A
Class A Sex Offenders have been convicted of, or found not guilty by reason of insanity of the following sex offenses:
First degree sexual abuse
Second degree sexual abuse
Forcible rape
First degree child sexual abuse of a child under 12 years of age
Carnal knowledge of a child under 12 years of age
Murder or manslaughter committed before, during or after engaging or attempting to engage in a sexual act, sexual contact or rape
Forcible sodomy
Sodomy committed against a child under 12
Assault with the intent to commit any of the aforementioned act
Attempting to commit any of the aforementioned Acts
Conspiring to commit any of the aforementioned Acts
Two separate convictions for registration offenses
A conviction for committing registration offenses against two or more victims
Class B
Class B Sex Offenders have been convicted of, or found not guilty by reason of insanity of the following sex offenses against a minor, that is, a person under the age of 18:
Third degree sex abuse
Fourth degree sex abuse
Misdemeanor sex abuse
First degree child sexual abuse
Second degree child sex abuse
Carnal knowledge
Sodomy against a minor
Indecent acts on a child
Enticing a child
Lewd, indecent or obscene acts
Sexual performance using a minor
Incest
Obscenity
Prostitution
Pandering
Assault (unwanted sexual touching)
Threatening to commit a sexual offense
Kidnapping
First or second degree burglary with intent to commit a sexual offense
Assault with intent to commit any of the above offenses
Attempting to commit any of the above offenses
Conspiracy to commit any of the above offenses
Any offense against a minor for which the offender agreed in a plea agreement to be subject to Sex Offender Registration
Conviction in another jurisdiction of a similar criminal offense
First or second degree sexual abuse against a ward or resident of a hospital, treatment facility or other institution
First or second degree sexual abuse of a patient or client
Class C
C Classification will be given to persons convicted of the following offenses against a person 18 years of age or older:
Third or fourth degree sexual abuse
First or second degree burglary with the intent to commit a sexual offense
Kidnapping with the intent to commit a sexual offense
Threatening to commit a sex offense or assault with the intent to commit any of the above criminal offenses
Attempting to commit any of the above criminal offense
Conspiracy to commit any of the above criminal offenses
Any offense for which the offender agreed in a plea agreement to be subject to Sex Offender Registration
Conviction in another jurisdiction of a similar offense
In accordance with enactment of the Sex Offender Registration Act of 1999, this information is being provided to the community. Unlawful use of this information to threaten, intimidate, harass, or injure a registered sex offender is prohibited and will be prosecuted to the full extent of the law.
Acknowledgements
This data is in the public domain at opendata .dc.gov."	177	2447	43	karnikakapoor	sex-offender-registry
2733	2733	lecNan		[]		0	33	1	nancyalaswad90	review
2734	2734	SMSSpamTable		[]		0	5	0	chiranjeevbit	smsspamtable
2735	2735	typish		[]		0	4	0	imokuri	typish
2736	2736	nptyping		[]		0	6	0	imokuri	nptyping
2737	2737	fire smoke dataset		[]		12	109	3	hhhhhhdoge	fire-smoke-dataset
2738	2738	Covid19 vaccine status in Malaysia		[]		1	12	0	alexis01	covid19-vaccine-status-in-malaysia
2739	2739	ddbo1314		[]		0	5	0	movie112	ddbo1314
2740	2740	train_aug		[]		0	7	0	vitomanuguerra	train-aug
2741	2741	Boosting of covid19 vaccine in malaysia		[]		1	17	0	alexis01	boosting-of-covid19-vaccine-in-malaysia
2742	2742	data analyst		['business']		1	17	0	alexis01	data-analyst
2743	2743	homeprices		[]		0	6	0	diptendunandi	homeprices
2744	2744	Secondary Mushroom (Date Donated 2021)	Dataset simulated mushrooms for binary classification into edible and poisonous	['categorical data', 'biology', 'computer science', 'programming', 'classification', 'cnn']	"Context
The given information is about the Secondary Mushroom Dataset, the Primary Mushroom Dataset used for the simulation and the respective metadata can be found in the zip.
Content
This dataset includes 61069 hypothetical mushrooms with caps based on 173 species (353 mushrooms
per species). Each mushroom is identified as definitely edible, definitely poisonous, or of
unknown edibility and not recommended (the latter class was combined with the poisonous class).
Acknowledgements
Donor: D. Wagner, dwagner93 '@' gmx.de
Product of bachelor thesis at Philipps-UniversitÃ¤t Marburg, Bioinformatics Division, supervised by Dr. G. Hattab.
Repository containing the related Python scripts and all the data sets: https://mushroom.mathematik.uni-marburg.de/files/
Inspired by the Mushroom Data Set of J. Schlimmer: url:https://archive.ics.uci.edu/ml/datasets/Mushroom.
Inspiration
The related Python project contains a Python module secondary_data_generation.py
used to generate this data based on primary_data_edited.csv also found in the repository.
Both nominal and metrical variables are a result of randomization.
The simulated and ordered by species version is found in secondary_data_generated.csv.
The randomly shuffled version is found in secondary_data_shuffled.csv.
Donor: D. Wagner, dwagner93 '@' gmx.de
Product of bachelor thesis at Philipps-UniversitÃ¤t Marburg, Bioinformatics Division, supervised by Dr. G. Hattab.
Repository containing the related Python scripts and all the data sets: https://mushroom.mathematik.uni-marburg.de/files/
Inspired by the Mushroom Data Set of J. Schlimmer: url:https://archive.ics.uci.edu/ml/datasets/Mushroom."	2	50	2	yasserhessein	secondary-mushroom-dataset
2745	2745	spamset		[]		0	5	0	bilaliqbalai	spamset
2746	2746	NLP_Project		[]		0	22	0	prashantrao5321	nlp-project
2747	2747	sample		[]		0	3	0	diptendunandi	sample
2748	2748	salary		[]		0	9	0	diptendunandi	salary
2749	2749	HappyWhale Train Images by Class		['transportation']		0	57	3	ayuraj	happywhale-train-by-class
2750	2750	Titanic Survival Prediction Model	Titanic Train Dataset	['transportation']		9	168	0	vishantmalik	titanic-survival-prediction-model
2751	2751	Boston dataset		[]		0	5	0	roshananto	boston-dataset
2752	2752	UbiquantTrain2		[]		0	14	0	hadisahmadian	ubiquanttrain2
2753	2753	HPI Edge AI Course pretrained Models	Pretrained image classification models on cifar100 using pytorch	['arts and entertainment']		4	57	0	yanghaojin	pretrained-models
2754	2754	irisdataset	This data contains the Iris species	['biology']		6	206	1	malikumairayub	irisdataset
2755	2755	UbiquantTrain1		[]		0	18	0	hadisahmadian	ubiquanttrain1
2756	2756	IPL_2022_PlayerAuction_List_Sets		[]		0	6	0	prasanthkumarsuraty	ipl-2022-playerauction-list-sets
2757	2757	cyclistsbp	Combined dataset  used in R for Data Analyst Capstone project.  	['business']	"The dataset is the  Combined dataset using Power Query and used in R for Data Analyst Capstone project.
Grateful for Google and the tutors that guided me through the program."	0	17	0	akaymsa	cyclistsbp
2758	2758	UMP features history		[]		0	29	0	takamichitoda	ump-features-history
2759	2759	CENSUS2011	Indian Census data  which was collected in 2011	['education', 'government', 'social science', 'data cleaning', 'data visualization', 'data analytics']		2	33	2	yashmestry	census2011
2760	2760	autocorrect		['email and messaging']		0	12	0	ajenningsfrankston	autocorrect
2761	2761	eel-weight		[]		0	8	0	shinchangyu	eelweight
2762	2762	Competitions Analysis	Competitions Analysis	[]		0	36	5	vad13irt	tabular-series-analysis
2763	2763	HappyWhale Splits		[]		87	166	1	ks2019	happywhale-splits
2764	2764	Speech Data		[]		0	15	0	theresalusiana	speech-data
2765	2765	depth_map_leres		[]		0	9	0	derekcai	depth-map-leres
2766	2766	Indian Festival Classification		[]		0	15	1	arun2002	indian-festival-classification
2767	2767	Bitcoin Correlation to Stocks		['currencies and foreign exchange']		0	29	0	jamesharrisiii	bitcoin-correlation-to-stocks
2768	2768	League of Legends Stats	League of Legends Champion Stats for Every Patch	['classification']	"Context
This dataset contains champion stats of ranked League of Legends games.
The dataset will be updated for every patch.
Content
+100 rows and 11 columns.
Columns' description are listed below.
Name : Name of the champion
Class : Fighter, Assassin, Mage, Marksman, Support or Tank
Role : Top, Mid, ADC, Support or Jungle
Tier : God, S, A, B, C or D
Score : Overall score of the champion
Trend : Trend of the score
Win % : Win rate of the champion
Role % : Role rate played with the champion
Pick % : Pick rate of the champion
Ban % : Ban rate of the champion
KDA : (Kill+Death)/Assist ratio of the champion
Acknowledgements
Data from METAsrc.
Image from League of Legends Wiki.
If you're reading this, please upvote."	234	2266	26	vivovinco	league-of-legends-champion-stats
2769	2769	IPL Auction 2022 - Player Fair Price	Models For Demand Based On Player Attribute, Nationality, Budget Remaining 	['cricket', 'india', 'sports', 'business', 'tabular data']	"Context
In Cricket Crazy Nation like India IPL Auction is a huge subject of interest. Considering the upcoming IPL Mega Auction 2022, here is a simple excel model for deciding a fair price for a player.
Content
I downloaded the players' list from www.iplt20.com. Created models based on the demand of players in the current scenario. Referring to the sheets of excel file you will get to know the assumptions of the models, you can refine the demand models on your judgment.
Acknowledgements
Data Reference: https://bcciplayerimages.s3.ap-south-1.amazonaws.com/documents/IPL/document/2022/02/IPL2022PlayerAuction_List_Sets.pdf
Inspiration
Inspiration is to calculate the fair price, expected bid prices of players, auction strategy development."	60	238	4	omkarswami	ipl-auction-fair-price-suggestion
2770	2770	dataexam1		[]		0	3	0	fatimazehraalvi	dataexam1
2771	2771	mlm pretrain roberta model		[]		1	6	0	yuzhoudiyishuai	mlm-pretrain-roberta-model
2772	2772	JPEG Happywhale 384x384	Reduce Resolution Images for the Happy Whale Competition	['beginner', 'intermediate', 'advanced', 'computer vision', 'image data']	"Context
Hello all!
I have taken the training & testing images (JPEG) and created reduced image size datasets. These images were resized to 384 x 384. This resolution is a step up from another dataset that I have provided for the same competition where the image resolution is 384 x 384. From what we have seen from other competitions (most of the time) is that the higher we go with our image resolution the more performance we can get from our models. 384 x 384 is the next step up to those higher resolutions. We can continue to experiment and gain confidence in our models through experimenting with image resolution.
Happy Kaggling! 🎉
Content
The contents of the data set are found under the train_images-384-384 & test_images-384-384. Each folder has all of the original images for the Happywhale competition just resized to 384 x 384. 
Inspiration
The inspiration for this dataset is to provide the Happywhale competition data set are lower resolutions to help beginners and experts alike get started 🔥"	41	239	13	rdizzl3	jpeg-happywhale-384x384
2773	2773	Blahblahblah		[]		0	8	0	morganvalverde	blahblagg
2774	2774	JPEG Happywhale 256x256	Reduced Resolution Images for the Happy Whale Competition	['earth and nature', 'beginner', 'intermediate', 'advanced', 'image data']	"Context
Hello all!
I have taken the training & testing images (JPEG) and created reduced image size datasets. These images were resized to 256 x 256. This resolution is a step up from another dataset that I have provided for the same competition where the image resolution is 256 x 256. From what we have seen from other competitions (most of the time) is that the higher we go with our image resolution the more performance we can get from our models. 256 x 256 is the next step up to those higher resolutions. We can continue to experiment and gain confidence in our models through experimenting with image resolution.
Happy Kaggling! 🎉
Content
The contents of the data set are found under the train_images-256-256 & test_images-256-256. Each folder has all of the original images for the Happywhale competition just resized to 256 x 256. 
Inspiration
The inspiration for this dataset is to provide the Happywhale competition data set are lower resolutions to help beginners and experts alike get started 🔥"	49	201	11	rdizzl3	jpeg-happywhale-256x256
2775	2775	Detect-COTS-model-YOLOv5		[]		1	69	0	hngbiquc	detectcotsmodelyolov5
2776	2776	JPEG Happywhale 128x128	Reduced Resolution Images for the Happy Whale Competition 🐋	['beginner', 'intermediate', 'advanced', 'computer vision', 'image data']	"Context
Hello all!
I have taken the training & testing images (JPEG) and created reduced image size datasets. These images were resized to 128 x 128. We know that when it comes to lower resolution (in most cases) that our performance metrics might not be as good as higher resolutions. The benefit of the 128 x 128 image sizes for this competition are being able to iterate quickly and check your pipeline for any bugs 😫. Once you are confident in your pipeline and modeling technique you can scale to high images sizes. The process of resizing also takes some time so hopefully this will save Kagglers from having to do this themselves and focus on what matters most.
Happy Kaggling! 🎉
Content
The contents of the data set are found under the train_images-128-128 & test_images-128-128. Each folder has all of the original images for the Happywhale competition just resized to 128 x 128. 
Inspiration
The inspiration for this dataset is to provide the Happywhale competition data set are lower resolutions to help beginners and experts alike get started 🔥"	129	867	23	rdizzl3	jpeg-happywhale-128x128
2777	2777	Image Editor		[]		1	7	0	theblackmamba31	image-editor
2778	2778	Legal Clauses	Clauses from fin contracts	['law', 'intermediate', 'nlp', 'data analytics', 'text data']	"Context
Contracts and NLP in-depth:
By allowing computer interactions, we can dig deeper to how we want to understand legal documents. We can use natural language processing (NLP) which is a subfield of linguistics and computer that is specializes in artificial intelligence to analyze textual big data. The process of analyzing texts can be segmented into supervised and unsupervised learning. Mostly, NLP aims to solve unsupervised learning that has far more challenges and complex structures. In this paper, I will explore some of the major and most recent implications of how to use NLP.
Content
The data source is from was scraped from contracts website where I have collected over 21k legal clauses from 16 type of clauses that are related to ‘finance’. I used python using different type of libraries for scraping since the website keep blocking automated querying from the website, so I used selenium library to extract clause text and clause type.
Inspiration
In order to deep dive into legal terms, the most fundamental questions are to understand NLP common practices when analyzing texts using NLP methodologies such as ‘term frequency’, ‘term frequency–inverse document frequency (Tf-idf)’, ‘word embedding’, and ‘sentiment analysis’
Think of answering questions like:
Question 1: What are the most frequent terms in Legal Clause
Question 2: Most Frequent Words By Clause Type
Question 3: How Important a term to document? Using Tf-idf
Question 4 :Can we find Relationship Between Legal Terms
Question 5: How each Clause is explained based by topics.
Question 6: Does Sentiment Analysis work best in Legal terms? can we proof that contracts are ambiguously constructed so that we do not find positive/negative words that persuade readers? (intuitively not) but how to proof it?"	0	32	0	mohammedalrashidan	contracts-clauses-datasets
2779	2779	labeled_faces_in_the_wild	contains more than 67,00,82,675 face data for computer vision projects	['biology', 'computer vision', 'classification', 'deep learning', 'image data', 'gpu']	"Labeled Faces In The Wild Collection.
Site: http://vis-www.cs.umass.edu/lfw/ contains more than 67,00,82,675 labeled face data for computer vision projects like Face Recognition
The dataset contains ppm image format and labels in FacesInTheWild MS Access"	24	1108	7	rupakroy	labeled-faces-in-the-wild
2780	2780	Vietcombank Stock Price Dataset	Vietcombank stock data from 2012 to 2021	['business', 'banking', 'beginner', 'time series analysis']		2	42	1	hphat99	vietcombank-stock-price-dataset
2781	2781	OOD Genomics Data	DNA Sequences from 10 Bacteria Classes	['biology', 'biotechnology', 'tabular data']		1	35	0	washingtongold	cleaned-genomics-data
2782	2782	sentiment_unclear		[]		0	10	0	skyzeng	sentiment-unclear
2783	2783	output_bertweet-base-sentiment-analysis		[]		1	18	0	li874039270	output-bertweetbasesentimentanalysis
2784	2784	MA4829_Report1	MA4829_2022 report notebook	[]		2	13	0	yomama69	ma4829-report1
2785	2785	SPA-Data		[]		0	12	0	leftthomas	spadata
2786	2786	fasterrcnn_resnet50_fpn_coco-258fb6c6		[]		0	14	0	treadstone27	fasterrcnn-resnet50-fpn-coco258fb6c6
2787	2787	XLNET_MUL_CLEAN_try2_256		[]		0	4	0	hangy132	xlnet-mul-clean-try2-256
2788	2788	EDAwithpython		[]		0	1	0	rakaumridreftanta	edawithpython
2789	2789	digit-recognizer-data-files		[]		0	8	0	abdullahzr	digitrecognizerdatafiles
2790	2790	Height of Male and Female by Country 2022	Average Height of Male and Female by Country	['global', 'people', 'people and society', 'education', 'social science']	"Context
The metric system is used in most nations to measure height.Despite the fact that the metric system is the most widely used measurement method, we will offer average heights in both metric and imperial units for each country.To be clear, the imperial system utilises feet and inches to measure height, whereas the metric system uses metres and centimetres.Although switching between these measurement units is not difficult, countries tend to choose one over the other in order to maintain uniformity.
Content
The dataset contains six columns:
    •   Rank
    •   Country Name
    •   Male height in Cm
    •   Female height in Cm
    •   Male height in Ft
    •   Female height in Ft
Acknowledgements
Users are allowed to use, copy, distribute and cite the dataset as follows: “Majyhain, Average Height of Male and Female by Country 2022, Kaggle Dataset, February 02, 2022.”
Inspiration
The ideas for this data is to:
Average Height of Men in Countries?
Average Height of Women in Countries?
Height of Male and Female in Feet? 
Which country have the tallest male and female?
Which countries have the smallest male and female average height?
References:
The Data is collected from the following sites:
https://ourworldindata.org/"	1077	5169	51	majyhain	height-of-male-and-female-by-country-2022
2791	2791	depth_map_midasv2	Using BMD-midasv2 to generate dog vid depth map	[]		1	20	0	derekcai	dog-vid-frame-depth-map
2792	2792	NBA Stats (1947-present)	Regular Season Data from Basketball-Reference over 73+ Years	['basketball', 'sports']	"Context
When I set out to gather data for my first NBA-related project, I used the IMPORTHTML function from Google Sheets repeatedly on Basketball-Reference's Play Index (now Stathead), which was...not ideal to say the least. So I decided to learn how to web-scrape and compile this historical data for everyone to use (in the mold of Lahman's Baseball Database).
Content
There are 3 leagues represented: the National Basketball Association (1950-present), the NBA's predecessor in the Basketball Association of America (1947-1949) and the NBA's past competitor in the American Basketball Association (1968-1976)
There are two informational files: Player Season Info & Team Abbrevs. Each player was given a unique player ID to facilitate aggregation into career stats.
On the team side, there are 7 files:
- totals & opponent totals
- per game & opponent per game statistics 
- per 100 possessions & opponent per 100 possessions statistics (starting from 1974)
- team summaries
On the player side, there are 9 files:
- player totals
- player per game stats
- player per 36 minute stats
- player per 100 possessions stats (starting from 1974)
- player advanced stats
- player play by play stats (starting from 1999) - percentage of time spent at different positions, fouls drawn & committed, etc
- player shooting stats (starting from 1999) - success rate and attempt rate from different shot distances
- end of season teams (All-Defense, All-Rookie, All-League)
- awards voting results (Rookie of the Year, Sixth Man of the Year, Most Valuable Player, Defensive Player of the Year, Most Improved Player)
For explanations about a certain stat, I'd suggest checking out Basketball-Reference's glossary.
Acknowledgements
Basketball-Reference for being the greatest comprehensive basketball stats site (in my humble opinion)
Robert Frey and David Schoch for determining how Basketball-Reference formats its pages
Photo by <a href=""https://unsplash.com/@tjdragotta?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">TJ Dragotta</a> on <a href=""https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	315	2368	17	sumitrodatta	nba-aba-baa-stats
2793	2793	Football transfers 2021	Scraped from transfermarkt	['football', 'sports']	"Context
This data was scraped by me from transfermarkt. 
Please be aware that some data cleaning is required in the transfer fee column since it combines text and numerical data. 
Some columns like destination team, destination country and destination league may contain missed values for players who retired or have contracts that finished (free agents).
I think this dataset is good to practice data cleaning and EDA, there are many interesting visualizations that can be done"	66	450	6	matiastaron	football-transfers-2020
2794	2794	centraleSupelec ML2022		[]		0	8	0	nocturnejay2	centralesupelec-ml2022
2795	2795	ext_tx_rbt_l		[]		0	1	1	datafan07	ext-tx-rbt-l
2796	2796	Case study in pdf (4)		['education']		0	10	0	andreps	case-study-in-pdf-4
2797	2797	Podcasts Episodes (2007-2016)	30,000 Podcast Episodes	['arts and entertainment', 'movies and tv shows', 'music']	"Content
In this dataset, you will find over 30,000 episodes of several different podcasts shows.
Acknowledgements
This dataset comes from https://data.world/brandon-telle/podcasts-dataset."	39	449	14	rishidamarla	podcasts-episodes-20072016
2798	2798	Case study in pdf (3)		['education']		1	10	1	andreps	case-study-in-pdf-3
2799	2799	Bone Classification and Detection Dataset	3 Bone type classification bbox labeled dataset (hand,wrist,elbow)	['healthcare', 'public health', 'biology', 'health', 'image data']		4	66	0	ardacanuckan	bone-classification-and-detection-dataset
2800	2800	Case study in pdf (2)		['education']		3	19	0	andreps	case-study-in-pdf-2
2801	2801	3 Bone Type Classification and Detection 		['biology', 'health']		0	11	0	ardacanuckan	3-bone-type-classification-and-detection
2802	2802	ROSBAGS		[]		26	24	2	hazem9806	rosbags
2803	2803	LogRef_ru1		[]		1	4	0	juliencoquard	logref-ru1
2804	2804	SoftAsia Tech Banner	SoftAsia Tech Banner New	[]		0	9	1	muhammedtausif	softasia-tech-banner
2805	2805	art2flowersh5		[]		0	8	0	jiccrlla	art2flowersh5
2806	2806	India_GDP_1960_2020		[]		0	10	0	rudrakshideshpande	india-gdp-1960-2020
2807	2807	Songs1		[]		0	4	0	saurabh2323	songs1
2808	2808	tfgbr-ds-3		[]		0	14	0	ks2019	tfgbr-ds-3
2809	2809	tfgbr-ds-2		[]		0	7	0	ks2019	tfgbr-ds-2
2810	2810	tfgbr-ds		[]		1	7	0	ks2019	tfgbr-ds
2811	2811	NBA 2020-21 Regular Season Player Stats Per Game	Traditional Stats(Points, 3P%, 2P%, win , lost, minute, game play etc..)	['basketball', 'united states', 'sports']		126	686	6	zgrbalbay	202021-reg-tra-player
2812	2812	Iran Stock Exchange	Iran Stock Data from 2005 to 2020	['business', 'investing']	This data set includes information on Iran Stock Exchange and information on 710 different shares from 2005 to 2020. Collected fromTSETMC	20	178	7	lashkingl	iran-stock-exchange
2813	2813	Manzoor-Umair: Sea Image Dataset (MU-SID )		['asia', 'computer science', 'computer vision', 'image data']	"The Manzoor-Umair: Sea Image Dataset (MU-SID) consist of 2673 high-definition (1920 × 1080 pixels) RGB images. The dataset offers 36 different features. A combination of these features such as different day, weather, and environmental conditions, sea states, linear features, partial occlusion, foreground and background objects, and artifacts present a challenge for sea horizon line detection algorithms.
Citation
M. A. Hashmani and M. Umair, “A Novel Visual-Range Sea Image Dataset for Sea Horizon Line Detection in Changing Maritime Scenes,” Journal of Marine Science and Engineering, vol. 10, no. 2, p. 193, Jan. 2022, doi: 10.3390/jmse10020193.
Acknowledgements
This research was funded by Yayasan Universiti Teknologi PETRONAS—Fundamental Research Grant (YUTP-FRG-2019, grant number 015LC0-158)."	2	124	0	umairatwork	manzoorumair-sea-image-dataset-musid
2814	2814	New Girl IMBd Data	Scraped IMBd data for the comedy show New Girl	['arts and entertainment', 'categorical data', 'beginner', 'intermediate', 'text data']	"This is a simple dataset i scraped from IMBd on the television show New Girl. It contains:
Title
Air Date
Rating
Total Votes
Description of Episode
Season-Episode
This dataset was scraped from my inspriation to make vizzes with it. The viz of inspiration is this one:
https://public.tableau.com/app/profile/rohit.ghadge/viz/TheBigBangTheory_16260356651700/Dashboard"	5	90	4	senadc	new-girl-imbd-data
2815	2815	sign-data	Sign data for alphabets C and S	[]		0	7	0	kiranraghavendra	signdata
2816	2816	regression lineaire1		[]		0	8	0	ganetfranckkouassi	regression-lineaire1
2817	2817	mymodel6extra		[]		0	7	0	keyhanashoori	mymodel6extra
2818	2818	SteamReviewsNLP		[]		6	37	0	zeeenb	steamreviewsnlp
2819	2819	Extended Flowers Recognition	Try out flower classification with this data set of numerous flower species!	['plants', 'computer vision', 'image data', 'multiclass classification']	"Context
This data set is an extension of the Flowers Recognition data set. The original Flowers Recognition data set inspired me to try out some flower classification with ML and to scrape images of more species of flowers online. So far I have added 5 species of flowers to the original data set and intend to add more. There are currently ~8,100 labeled images of 10 different flower species.
Content
The image data are all .jpg files that were scraped from the internet and are stored in folders with the corresponding class name. There are currently 10 flower classes with somewhere around ~600-900 images each: aster, daffodil, dahlia, daisy, dandelion, iris, orchid, rose, sunflower, tulip. All images have different file sizes and resolutions.
Acknowledgements
Credit goes to Alexander Mamaev (alxmamaev) for uploading the original Flowers Recognition data set to Kaggle. I used Flickr, Yandex, and Google images to scrape together more images and add more classes of flowers.
Inspiration
Can we build a model that can classify flowers?"	5	122	4	jonathanflorez	extended-flowers-recognition
2820	2820	NYC Traffic Accidents	NYC Traffic Accidents2020	[]		3	20	0	melodyyiphoiching	nyc-traffic-accidents
2821	2821	Google Cyclistics dataset (Jan to Dec 2021 )	Google Capstone project - Divvy ride share data set	['business', 'internet', 'transportation', 'cycling', 'data analytics', 'ggplot2', 'r']	"Context
This data set is a part of Google data analytics course capstone project, aiming to study the difference in behavior between casual and member riders of a bike ride share company (Cyclistic) in order to help the marketing team achieve their goal of converting casual riders to members.
Content
The data set contains data about every ride between January 2021 till December 2021 ."	1	85	2	shadymehany	google-cyclistics-dataset-jan-to-dec-2021
2822	2822	COVID-19 Post Vaccination Statewide Stats	California Vaccination Statistics	['diseases', 'public health', 'health', 'medicine', 'data analytics', 'public safety', 'covid19']	"Context
This is a dataset that compares vaccinated versus unvaccinated cases, death, and hospitalizations for the State of California
Acknowledgements
This information (and updated datasets) are publicly available from the State of California here ... https://data.chhs.ca.gov/dataset/covid-19-post-vaccination-infection-data
Inspiration
This dataset can used to answer the question of the efficiency of being fully vaccinated
License
The use of this dataset is governed by the original license
https://data.chhs.ca.gov/pages/terms"	108	667	10	tdison	covid19-post-vaccination-statewide-stats
2823	2823	bella beats		[]		0	5	0	jeremymatthews	bella-beats
2824	2824	Bellabeatscasestudy	Bella Beats conclusion after analyzing and organizing in R	['languages', 'arts and entertainment', 'health and fitness', 'health', 'cv2', 'dplyr']		0	13	1	jeremymatthews	bellabeatscasestudy
2825	2825	Bellabeatscasestudy	Bella Beats conclusion after analyzing and organizing in R	['languages', 'arts and entertainment', 'health and fitness', 'health', 'cv2', 'dplyr']		0	13	1	jeremymatthews	bellabeatscasestudy
2826	2826	Manzoor-Umair: Cloud Cover Dataset (MU-CCD)		['asia', 'weather and climate', 'computer science', 'classification', 'image data']	"Manzoor-Umair: Cloud Cover Dataset (MU-CCD) is a visual-range cloud cover image dataset for deep learning classification models. The dataset consists of 9,600 RGB images (1920x1080 pixel) divided into a ratio of 80-20 for training and testing purpose.
Paper Citation
Muhammad Umair and Manzoor Ahmed Hashmani, “A Visual-Range Cloud Cover Image Dataset for Deep Learning Models” International Journal of Advanced Computer Science and Applications(IJACSA), 13(1), 2022. http://dx.doi.org/10.14569/IJACSA.2022.0130166
Acknowledgements
This research work is a part of a research project funded by Yayasan Universiti Teknologi PETRONAS – Fundamental Research Grant (YUTP-FRG) - 2019."	6	227	0	umairatwork	manzoorumair-cloud-cover-dataset-muccd
2827	2827	mymodel7extra		[]		0	12	0	mhashoorii	mymodel7extra
2828	2828	Enem 2019(filtred)	Data from the main Brazilian exam for admission to higher education 	['brazil', 'education', 'social science', 'tabular data', 'standardized testing']	"Context and Content
Data referring to ENEM 2019. A filter was applied where missing/eliminated students from the exam and some columns understood as unnecessary were removed.
You can consult the original data here: https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/enem
Inspiration
The dataset provides a number of features that can be used to provide insights related to the social situation in Brazil, such as the quality of education in the country, how democratic the exam is, and other insights.
Regarding predictive models, some tool can be developed for the accuracy of the average grade. Note that the average score is not given merely by correct answers, as the IRT (Item Response Theory) is applied.
Data Dictionary
you can see the dictionary of data on this file: https://docs.google.com/spreadsheets/d/1LkXEJbip8Ny4XbiSazuPqXLoTPNaFbwW/edit?usp=drivesdk&ouid=103442764714892509081&rtpof=true&sd=true"	10	207	5	joseedmario	enem-2019filtred
2829	2829	mymodel5	date: November 17th new FE model	['automobiles and vehicles']		0	185	0	stavangar	premodel
2830	2830	Warhol		['art']		1	21	0	hales27	warhol
2831	2831	Tire Track Images		['categorical data', 'computer vision', 'classification', 'cnn', 'image data']		2	49	1	greysky	tire-track-images
2832	2832	lg resized image		[]		0	4	0	sky4268	lg-resized-image
2833	2833	Melbourne Housing		[]		0	14	0	nirmaldash	melbourne-housing
2834	2834	TableBank		[]	collected from : https://doc-analysis.github.io/tablebank-page/index.html	0	18	0	haque1407028	tablebank
2835	2835	SPM SOTA Models		[]		0	9	0	shantanupatankar	spm-sota-models
2836	2836	scRNA-seq early developing mouse retina cells	GSM3466902 Retinal E15.5 cells Progenitors, neuroblasts and nascent neurons	['genetics', 'biology', 'biotechnology']	"Content
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes.
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Acknowledgements
Paper:
""Single-cell transcriptional logic of cell-fate specification and axon guidance in early-born retinal neurons""
https://journals.biologists.com/dev/article/146/17/dev178103/222981/Single-cell-transcriptional-logic-of-cell-fate
Quentin Lo Giudice 1, Marion Leleu 2 3, Gioele La Manno 2 4, Pierre J Fabre 5
The paper contains very nice movie visualization: https://movie.biologists.com/video/10.1242/dev.178103/video-1
Data:
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM3466902
Also see review : 
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6"	2	132	1	alexandervc	scrnaseq-early-developing-mouse-retina-cells
2837	2837	AlonTryPublic		[]		0	8	0	alonlevy1	alontrypublic
2838	2838	short_with_r2l		[]		0	9	0	danielnekrassov	short-with-r2l
2839	2839	NeRF Dataset	Official scenes from the NeRF paper	[]		0	8	0	sauravmaheshkar	nerf-dataset
2840	2840	reef-yolox-m-v8		[]		0	12	0	sangayb	reef-yolox-m-v8
2841	2841	Loan Prediction		[]		1	15	0	vishubhardwaj007	loan-prediction
2842	2842	One Piece Characters and Chapters	All canon characters and manga's chapters	['popular culture', 'movies and tv shows', 'japan', 'anime and manga']	"Context
One Piece is one of the most popular anime nad manga on the world. Japanese manga series written and illustrated by Eiichiro Oda  (serialized in Shueisha's shōnen manga magazine Weekly Shōnen Jump since July 1997) is about the adventures of Monkey D. Luffy,  a boy whose body gained the properties of rubber after unintentionally eating a Devil Fruit. With his crew of pirates, named  the Straw Hat Pirates, Luffy explores the Grand Line in search of the world's ultimate treasure known as ""One Piece"" in order  to become the next King of the Pirates. Toei Animation produces an anime television series based on the One Piece manga.  The series, which premiered in Japan on Fuji Television on October 20, 1999, has aired more than 950 episodes, and has been  exported to various countries around the world.
Content
The database contains 2 tables: canon characters and manga's chapters. ach line from the database corresponds to the character/chapter accordingly. The data was taken from One Piece Wiki which is synonymous with wikipedia for this manga/anime. The tables were  obtained using the ""rvest"" package in R 4.0 and the ""SelectorGadet"" add-on, which made the work with the site easier.
Inspiration
Data might be intresting for One Piece fans. We can see how manga is developing with chapters, which volume has most interesting names, in which episodes are most new characters and much more. We can connect this dataset to anothet - for example this one.
Photo by Net Sama on Flickr"	215	2594	31	michau96	one-piece-characters-and-chapters
2843	2843	LibriSpeech-ASR: Clean Dataset		[]		1	7	0	showmik50	librispeech-asr-clean-dataset
2844	2844	California Housing		['real estate', 'social issues and advocacy']		2	36	1	pseudoprogrammer	california-housing
2845	2845	Mobiles sold at Flipkart	Details about all smartphones and basic phones that are sold on flipkart	['mobile and wireless', 'beginner', 'exploratory data analysis', 'data cleaning', 'pandas']		2	25	0	mohitdhapodkar	mobiles-sold-at-flipkart
2846	2846	Airline _Passenger 	Airline passenger DataSets	['categorical data', 'beginner', 'data analytics', 'classification', 'sklearn']	"Context
This Is a Dataset of San Francisco International Airport Report on Monthly Passenger Traffic Statistics by Airline.
About:
Airport data is seasonal in nature, therefore any comparative analyses should be done on a period-over-period basis (i.e. January 2010 vs. January 2009) as opposed to period-to-period (i.e. January 2010 vs. February 2010). It is also important to note that fact and attribute field relationships are not always 1-to-1. For example, Passenger Counts belonging to United Airlines will appear in multiple attribute fields and are additive, which provides flexibility for the user to derive categorical Passenger Counts as desired.
Questions:
Where we can do Some Visualization using Data.
Do Clustering and Classification.
Classification on price
Analysis of Data
Price Prediction on Flight."	4	105	0	vanamayaswanth	airline-passenger
2847	2847	Utr_mul_try2		[]		0	5	0	hangy132	utr-mul-try2
2848	2848	myemoji900		[]		0	5	0	mldfshr	myemoji900
2849	2849	dlhlp-hw2-voiceconversion-data		[]		0	12	0	a24998667	dlhlp-hw2-voiceconversion-data
2850	2850	topone124results		[]		0	14	0	mollelmichael	topone124results
2851	2851	MNIST in CV	A fun collection of MNISTs for different computer vision tasks. 	['computer science']	"MNIST in CV
A fun collection of MNISTs for different computer vision tasks. Most of the datasets are created with code from different sources  Below are some respected mentions. 
Classification
source: https://www.kaggle.com/c/digit-recognizer
Detection
source: https://github.com/hukkelas/MNIST-ObjectDetection
Video-Sequences
source: http://www.cs.toronto.edu/~nitish/unsupervised_video/
Segmentation 
source: https://github.com/LukeTonin/simple-deep-learning"	0	29	3	ipythonx	mnist-in-cv
2852	2852	reef-yolox-m-v7		[]		0	14	2	sangayb	reef-yolox-m-v7
2853	2853	Real Estate California	Real estate listings collected in the first 6 months in 2021	['housing', 'real estate', 'random forest', 'xgboost', 'regression']	"Context
For various project occasions, a comprehensive US listings dataset was created with a colleague. This dataset is a subset for the State of Georgia.
Content
This dataset shows real estate listing for California (US) for the first 6 months of 2021. Prices are listed in USD.
Acknowledgements and Citation
This dataset is a co-production, thanks to my fellow mate Jordan for building a great comprehensive US dataset from which I could filter this.
Using this dataset requires citing the contributor.
Credits for the Cover Image
Thank you very much <a href=""https://unsplash.com/@sterlingdavisphotola?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Sterling Davis</a> on <a href=""https://unsplash.com/s/photos/california?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	132	902	6	yellowj4acket	real-estate-california
2854	2854	Real Estate Georgia	Real Worlds Dataset for Georgia (US) for the first 6 months of 2021	['real estate', 'random forest', 'linear regression', 'tabular data', 'regression']	"Context
For various project occasions, a comprehensive US listings dataset was created with a colleague. This dataset is a subset for the State of Georgia.
Content
This dataset shows real estate listing for Georgia (US) for the first 6 months of 2021. Prices are listed in USD.
Acknowledgements and Citation
This dataset is a co-production, thanks to my fellow mate Jordan for building a great comprehensive US dataset from which I could filter this.
Using this dataset requires citing the contributor.
Credits for the Cover Image
Thank you very much <a href=""https://unsplash.com/@kriztheman?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Christopher Alvarenga</a> who provided this through <a href=""https://unsplash.com/s/photos/atlanta?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	34	298	7	yellowj4acket	real-estate-georgia
2855	2855	Beijing Olympic(2022) athlete Profile	2,874 Athletes , 15 Sports	['sports']	"The Beijing Winter Olympics will be held from February 4 to February 20, 2022.
We have collected profile information of athletes from the official website of the Beijing Winter Olympics.
It contains information such as each player's competition event, age, country, hobbies, and motto.  
source : link"	28	118	6	kukuroo3	beijing-olympic2022-athlete-profile
2856	2856	bert-toxic-comment-classification-challenge		['arts and entertainment']		0	29	0	abxmaster	berttoxiccommentclassificationchallenge
2857	2857	XLNET_clean_mul		[]		0	7	0	hangy132	xlnet-clean-mul
2858	2858	The CirCor DigiScope Phonocardiogram Dataset	A heart sound dataset comprising of 5272  recordings from 1568 subjects	['music', 'health', 'time series analysis', 'heart conditions']	"Abstract
A total number of 5272 heart sound recordings were collected from the main four auscultation locations of 1568 subjects, aged between 0 and 21 years (mean ± STD = 6.1 ± 4.1 years), with a duration between 4.8 to 80.4 seconds (mean ± STD = 22.9 ± 7.4 s), totaling more than 33.5 hours of recording. Each cardiac murmur in the dataset has been annotated in detail by a human annotator, in terms of timing, shape, pitch, grading, quality and location. Moreover, segmentation annotations regarding the location of fundamental heart sounds (S1 and S2) in the recordings have been obtained using a semi-supervised scheme. The segmentation annotations were performed by voting between three state-of-the-art machine-based algorithms. An expert annotator later studied the consensus and mismatches between the algorithms beat-by-beat and performed a manual annotation whenever the algorithms had disagreed or were not acceptable for the expert. To date, the dataset is the largest publicly available pediatric heart sound dataset, supporting deeper research on the topic of auscultation-based health recommendation systems. The dataset is being used in the George B. Moody PhysioNet Challenge 2022 on Heart Murmur Detection from Phonocardiogram Recordings.
Content
There are four data file types in the dataset (per subject):
A wave recording file (binary .wav format) per auscultation location for each subject, which contains the heart sound data
A header file (text .hea format) describing the .wav file using the standard WFDB format 8
A segmentation data file (text .tsv format) per auscultation location for all subjects, which contains segmentation information regarding the start and end points of the fundamental heart sounds S1 and S2
A subject description text file (text .txt format) per subject, where the name of the file corresponds to the subject ID. Demographic data such as weight, height, sex, age group and pregnancy status as well as a detailed description of murmur events are provided in this file.
In addition to the subject-wise description files, the file training_data.csv contains the overall information for all the records of the training set in the PhysioNet Challenge 2022; this file will not be provided with the Challenge data
The filenames for the audio data, the header file, the segmentation annotation, and the subject description file names are formatted as ABCDE_XY.wav, ABCDE_XY.hea, ABCDE_XY.tsv and ABCDE.txt, respectively. Here ABCDE is a numeric subject identifier and XY is one of the following codes corresponding to the auscultation location where the PCG was collected on the body surface:
PV corresponds to the pulmonary valve point;
TV corresponds to the tricuspid valve point;
AV corresponds to the aortic valve point;
MV corresponds to the mitral valve point; 
Phc for any other auscultation Location.
If more than one recording exists per auscultation location, an integer index succeeds the auscultation location code in the file name, i.e. ABCDE_XY_n.wav, ABCDE_XY_n.hea and ABCDE_XY_n.tsv, where n is an integer (1, 2, …). Accordingly, each audio file has its own header and annotation segmentation file, but the subject description file ABCDE.txt is shared between all auscultation recordings of the same subject ID. Multi-location records or repeated records with the same subject ID base name (e.g., ABCDE as described above), have been recorded in the same session, sequentially. Therefore, the corresponding .wav files have different lengths and there is no time synchrony between them.
All heart sound records were screened for the presence of murmurs at each auscultation location. Each murmur was classified according to its timing (early-, mid-, and late- systolic/diastolic) 9, shape (crescendo, decrescendo, diamond, plateau), pitch (high, medium, low), quality (blowing, harsh, musical) 9, and grade (according to the Levine scale 10). This information is provided in the subject description files (with .txt extension), in the following format:
The first line indicates the subject’s identifier, the number of recordings, and the sampling frequency (in Hz) separated by space delimiters.
The second line contains information about the heart sound data files corresponding to the current subject ID, also separated by empty spaces. Here the location of the recording (AV, PV, TV, MV, or Phc), the name of the header file, the name of the .wav file, and the name of the file that includes the information about the segmentation (.tsv) are included.
The rest of the lines start with a hash symbol (#) and indicate the information described in the following table.
Acknowledgements and Inspiration
This dataset is copied from https://physionet.org/content/circor-heart-sound/1.0.1/ to make it more convenient for you as a Kaggler, so remember to cite these sources if you use this data in your work or in a publication:
When using this resource, please cite: (show more options)
Oliveira, J., Renna, F., Costa, P., Nogueira, M., Oliveira, A. C., Elola, A., Ferreira, C., Jorge, A., Bahrami Rad, A., Reyna, M., Sameni, R., Clifford, G., & Coimbra, M. (2022). The CirCor DigiScope Phonocardiogram Dataset (version 1.0.1). PhysioNet. https://doi.org/10.13026/7bkn-d780.
Additionally, please cite the original publication:
J. H. Oliveira, F. Renna, P. Costa, D. Nogueira, C. Oliveira, C. Ferreira, A. Jorge, S. Mattos, T. Hatem, T. Tavares, A. Elola, A. Rad, R. Sameni, G. D. Clifford, & M. T. Coimbra (2021). The CirCor DigiScope Dataset: From Murmur Detection to Murmur Classification. IEEE Journal of Biomedical and Health Informatics, https://doi.org/10.1109/JBHI.2021.3137048
Please include the standard citation for PhysioNet: (show more options)
Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220."	0	37	0	bjoernjostein	the-circor-digiscope-phonocardiogram-dataset
2859	2859	mileage		[]		0	7	1	balupeddireddy	mileage
2860	2860	reddit-showerthoughts-corpus		[]		0	10	3	a24998667	reddit-showerthoughts-corpus
2861	2861	DC comics	Dataset of DC universe characters	['beginner', 'intermediate', 'advanced', 'comics and animation']	"The DC Universe (DCU) is the fictional shared universe where most stories in American comic book titles published by DC Comics take place. DC superheroes such as Superman, Batman, Wonder Woman, Martian Manhunter, The Flash, Green Lantern, and Aquaman are from this universe, as well as teams such as the Justice League and the Teen Titans. It also contains well-known supervillains such as Lex Luthor, the Joker, Sinestro, Harley Quinn, Reverse-Flash, Darkseid, General Zod, Penguin, the Riddler, Catwoman, Ra’s al Ghul, Bane, and Two-Face. In context, the term ""DC Universe"" usually refers to the main DC continuity. The main DC Universe, as well as the alternate realities related to it, were quickly adapted to other media such as film serials or radio dramas. In subsequent decades, the continuity between all of these media became increasingly complex with certain storylines and events designed to simplify or streamline the more confusing aspects of characters' histories. The basic concept of the DC Universe is that it is just like the real world, but with superheroes and supervillains existing in it. However, there are other corollary differences resulting from the justifications implied by that main concept. Many fictional countries, such as Qurac, Vlatava, and Zandia, exist in it. Though stories are often set in the United States of America, they are as often as not set in fictional cities, such as Gotham City or Metropolis. These cities are effectively archetypes of cities, with Gotham City embodying more of the negative aspects of life in a large city, and Metropolis reflecting more of the positive aspects. Sentient alien species (such as Kryptonians and Thanagarians) and even functioning interstellar societies are generally known to exist, and the arrival of alien spacecraft is not uncommon. Technologies which are only theoretical in the real world, such as artificial intelligence, or are outright impossible according to modern science, such as faster-than-light travel, are functional and reproducible, though they are often portrayed as highly experimental and difficult to achieve. Demonstrable magic exists and can be learned. The general history of the fictional world is similar to the real one (for instance, there was a Roman Empire, and World War II and 9/11 both occurred), but many fantastic additions exist, such as the known existence of Atlantis. In recent years, stories have increasingly described events which bring the DC Universe farther away from reality, such as World War III occurring, Lex Luthor being elected as President of the United States in 2000, and entire cities and countries being destroyed. There are other minor variations, such as the Earth being slightly larger than ours (to accommodate the extra countries), and the planet Saturn having 18 moons rather than 19 because Superman destroyed one.
This data set consists of ten columns. The columns are :
Page_id - An ID assigned to each record 
Name- Name of the DC character 
URL - It consists of the Wikipedia link of the information regarding each character
ID- The district from which the patient belongs
Align - Information whether the character is a good or a bad character in the DC universe
Eye - Eye color of each character
Hair- Hair color of each character
Sex- Gender of the character
Alive - Information whether the character is alive or deceased
Appearances - The total number of appearances of the character in the DC universe
Would love to see your analysis and insights from this data set using various algorithms. Kindly let me know about your work from this data set via email. Feel free to connect on LinkedIn.
email : arunashivaprakash575@gmail.com
LinkedIn: https://www.linkedin.com/in/aruna-s-/"	1	49	5	arunasivapragasam	dc-comics
2862	2862	Exp-006-deberta-v3-large-Jigsaw1-Multi		[]		0	10	0	mst8823	exp-006-deberta-v3-large-jigsaw1-multi
2863	2863	adl-arxiv-hw4-GANckpts		['education']		0	9	1	a24998667	adl-arxiv-hw4-ganckpts
2864	2864	ruddit jigsaw dataset	Ruddit Comment Offensiveness Score	['earth and nature', 'text mining', 'text data']	"ruddit-comments
Ruddit Comments extracted from Reddit using Python Web Scraper PRAW. This data is a supportive data to Jigsaw Comment Severity Rating Dataset.
Original Work
Ruddit: Norms of Offensiveness for English Reddit Comments is a dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). Data sampling procedure, annotation, and analysis have been discussed in detail in the accompanying paper. Authors have provided the comment IDs, post IDs and not the bodies, in accordance to the GDPR regulations. They have suggested that the comments and post bodies can be extracted from any Reddit API using the IDs provided.
The original paper can be found here: Ruddit: Norms of Offensiveness for English Reddit Comments
The source github repo can be found here: https://github.com/hadarishav/Ruddit
Acknowledgement to Original work:
@inproceedings{hada-etal-2021-ruddit,
    title = ""Ruddit: {N}orms of Offensiveness for {E}nglish {R}eddit Comments"",
    author = ""Hada, Rishav  and
      Sudhir, Sohi  and
      Mishra, Pushkar  and
      Yannakoudakis, Helen  and
      Mohammad, Saif M.  and
      Shutova, Ekaterina"",
    booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.acl-long.210"",
    doi = ""10.18653/v1/2021.acl-long.210"",
    pages = ""2700--2717"",
    abstract = ""On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best{--}Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset."",
}
Comment Extraction
I have used PRAW, a python library to communicate with Reddit API to extract the comment texts using given comment ids and post ids.
Find here the notebook that performs comment extraction
Disclaimer:
This dataset contains offensive comments. Reddit Comments and their URL are available in this dataset. This not meant to breach the privacy of Reddit users, but, rather only to enhance research in Jigsaw Comments Severity Rating Competition. Use of this dataset other than the intended use is not allowed. Owner of this dataset is not responsible for any concerns raised out of any unintended usage."	1084	3433	74	rajkumarl	ruddit-jigsaw-dataset
2865	2865	MCO_clean_tiny		[]		0	12	0	hangy132	mco-clean-tiny
2866	2866	DLHLP-HW2-VoiceConversion-ckpts-class		[]		0	4	1	a24998667	dlhlp-hw2-voiceconversion-ckpts-class
2867	2867	DLHLP-HW2-VoiceConversion-ckpts1		[]		0	8	1	a24998667	dlhlp-hw2-voiceconversion-ckpts1
2868	2868	IPL 2022 Auction	Data Analysis of Listed Players in IPL 2022 Auction	['cricket', 'sports', 'exploratory data analysis', 'statistical analysis', 'data analytics']		32	176	1	shivdeepmishra	ipl-2022-auction
2869	2869	test_mentley		[]		0	5	1	lvishalraju	test-mentley
2870	2870	jigsaw-lstm-5-folds		['games']		0	24	1	chiranthans23	jigsaw-lstm-5-folds
2871	2871	depression_detection		[]		0	13	1	nikithadatascientist	depression-detection
2872	2872	Updated Resume Dataset	Resume Dataset with every Tech Fields.	['beginner', 'intermediate', 'advanced', 'matplotlib', 'pandas']	"Context ### Content ### Acknowledgements  ### Inspiration
Companies often receive thousands of resumes for each job posting and employ dedicated screening officers to screen qualified candidates.
Hiring the right talent is a challenge for all businesses. This challenge is magnified by the high volume of applicants if the business is labor-intensive, growing, and facing high attrition rates.
IT departments are short of growing markets. In a typical service organization, professionals with a variety of technical skills and business domain expertise are hired and assigned to projects to resolve customer issues. This task of selecting the best talent among many others is known as Resume Screening.
Typically, large companies do not have enough time to open each CV, so they use machine learning algorithms for the Resume Screening task."	45	376	12	jillanisofttech	updated-resume-dataset
2873	2873	cots_effdet4_600		[]		0	6	0	kfk42kfk	cots-effdet4-600
2874	2874	kambition_submission_test		[]		1	14	2	taishioikawa	kambition-submission-test
2875	2875	World Tennis Records and Rankings 2022	World Tennis Records and Rankings till 31st January 2022	['tennis', 'sports', 'statistical analysis']	"Context
Tennis is one of the famous sports in the world.
Here we have data which comprises Tennis Records and Rankings dataset is updated till 31st Jan 2022.
Content
Dataset contains 2 files Records and Rankings of all tennis players till date.
Acknowledgements
I would like to thank https://www.ultimatetennisstatistics.com/ for this ultimate dataset
Inspiration
Lets take out some amazing analysis on this data."	18	191	5	abhijeetbhilare	world-tennis-records-and-rankings-2022
2876	2876	Premium Bonds - High Value Winners	December 2021 - present	['tabular data', 'investing']	"Context
This dataset from https://www.nsandi.com/prize-checker/winners contains all the high value prize winners from this months premium bond draw. Premium bonds are a way to save money in the UK, where you invest an amount up to £50,000 and every month you get put into draw where you can win tax free prizes. Every pound invested counts as 1 raffle so the more money you have invested the more likely you are to win. Prizes can range from £25 to £1,000,000. 
Content
This dataset contains all the prizes worth £1,000 or more that were awarded each month between Dec 2021 to present. It includes the prize values, the bond numbers, total value holdings, locations and dates of purchases for each winner. (Note: data format for Dec 2021 is slightly different from the rest, format for 2022 onwards will be uniform.)
Acknowledgements
Data was collected from https://www.nsandi.com/prize-checker/winners by downloading an excel file and converting it to a csv."	10	209	11	samuelcortinhas	premium-bond-winners-december-2021
2877	2877	seqclassifiers6		[]		11	21	1	chasembowers	seqclassifiers6
2878	2878	brain_ptm_2021_data_and_conv4d_impl		[]		0	6	0	albertozorzetto	brain-ptm-2021-data-and-conv4d-impl
2879	2879	first model		[]		0	116	0	samiraashoori	first-model
2880	2880	AnomalyDetection_Dataset		[]		0	9	1	dinoatkaggle	anomalydetection-dataset
2881	2881	CRIME IN CHICAGO		['crime']		1	11	0	nikhleshrao	crime-in-chicago
2882	2882	E-commerce data		['e-commerce services']		4	47	1	sadafaleem	ecommerce-data
2883	2883	DLHLP-HW1-ASR		[]		0	8	1	a24998667	dlhlp-hw1-asr
2884	2884	IPL 2022 Mega Auction Player List	All 590 Players Data	['cricket', 'india', 'sports']		26	120	4	plavak10	ipl-2022-mega-auction-player-list
2885	2885	Squid Game IMDB User Reviews	IMDB User Reviews for the Netflix Tv-show Squid Game	['movies and tv shows', 'beginner', 'tabular data', 'text data', 'ratings and reviews']	"Source: IMDB
Collection Method: Custom Scraper using selenium.
Image Credits: Sky News"	515	3800	37	deepcontractor	squid-game-imdb-user-reviews
2886	2886	shopee-image-classification-models		['arts and entertainment']		0	13	1	a24998667	shopee-image-classification-models
2887	2887	dog_heat_map		[]		0	5	0	aditikajala	dog-heat-map
2888	2888	Comic Books  - current values and other data	issue cover images, prices, characters, creators, cover variants and much more	['arts and entertainment', 'categorical data', 'classification', 'clustering', 'comics and animation']	"What makes a comic book valuable?
Comics books from the Golden, like the first appearance of Superman, and Silver age of comics are getting sold for millions in auctions. 
Modern-age comic books can also get really valuable within a couple of years (e.g. the first issue of The Walking Dead, also a hit TV series, is valued around 1800$). 
Unfortunately no one can predict whether a comic is going to rise in value beforehand! Could ML methods help with that?
The Data
This dataset was scraped from comicbookrealm.com, a free price-guide website for american comic-books,  at the beginning of January 2022. 
It includes almost all data relevant to a comic issue like issue number, date, cover price, current value, synopsis, creators, characters, publisher, volume type,cover image link, cover variant, print number etc.
I plan to repeat the scraping from time to time in order to update the dataset with latest issues.
Content
The dataset can be downloaded in .csv and .parquet format for a more convenient size:
Note: it all columns giving a link the address of the website https://comicbookrealm.com/ should be added at the beginning of each string.
Take a look at a typical series link to understand better all the relevant info
comic_data.csv:
Includes all information about each single issue that could be scraped. In detail:
title: the series title
title_link: the link to the title in the website
issue_link: the link to each issue of the title
cover_link: the link to the cover image of the issue
issue: the number of the issue
cover_date: the date (month and year) the issue was released
cover_price: the price in which the issue was sold
current_value: the highest price the issues was ever sold
hist_price_link: the link for the historical prices as recorder on the website
searched: the times the issue was searched in the site's database
owned: the times a user logged the issue as owned
pages: number of pages in issue
rating: the average rating by the site's users
rating count: the number of rating votes
ISBN-UPC: the unique issue code
est_print_run: the estimated number of copies printed
variant_of: if the issue is a variant cover of 
preview: if the issue is a preview
synopsis: brief description of the plot and/or issue edition  e.g. if it is a limited edition or exclusive etc.
contributors_names: lists of the names of the contributors
contributors_roles: lists that correspond to the role of each contributor (e.g. script, inks etc.)
characters: List of characters appearing in the issue as well as extra info (e.g. secret identity, link to character photo, special event like first appearance, death etc.) check on the the website for an example on the link above
pub_id: the unique publisher's id as given by the website
'volume': volume number of the title or whether it is a one-shot, mini-series, limited-series etc.
years: The year span the title was published
issues_total: Total number of issues in the series
pub_name: The name of the publisher
pub_titles_total: total number of titles released by the publisher
pub_issues_total: total number of issues released by the publisher
Inspiration
What makes a comic valuable?
What common characteristics do valuable comics share?
What other data would be needed to put odds on a comic book rising in value?
Let me know of your ideas and I can enrich the current dataset with more relevant data like sales data, historical prices etc.
All comments and suggestions are welcome!"	54	467	9	iasonaskatechis	comic-books-current-values-and-other-data
2889	2889	TestData-Lung Sound	Lung Sound Dataset online	['biology']		3	27	1	gowthambg	testdata
2890	2890	TrainData-LungSounds	Lung Sounds Dataset obtained online	['biology']		1	24	1	gowthambg	traindata
2891	2891	adl-arxiv-hw2		['education']		0	13	0	a24998667	adl-arxiv-hw2
2892	2892	qmkaggle		[]		1	7	0	a24998667	qmkaggle
2893	2893	cls-sharing-model		['clothing and accessories']		0	1	0	kookheejin	clssharingmodel
2894	2894	spam_ham_dataset		[]		4	22	0	bilaliqbalai	spam-ham-dataset
2895	2895	GBR Starfish TFRecords Mini 3X 1 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-mini-3x-1-1
2896	2896	GBR Starfish TFRecords Mini 3X 1 0		[]		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-3x-1-0
2897	2897	IPL 2022 Auction Player List		['cricket']		30	331	1	sammithsb	ipl-2022-auction-player-list
2898	2898	shahar_walk		[]		0	4	0	aniketverma19233	shahar-walk
2899	2899	Uybor | Toshkent shahridagi uylar 	uybor.uz e'lonlari asosidagi dataset	[]		2	10	1	hikmatilloahatov	uybortahlil
2900	2900	Cyclistic bike-share 12 Months Analysis	Cyclistic Bike Share company data for members from Dec 2021 to Dec 2020	['united states', 'cycling', 'intermediate', 'data cleaning', 'data visualization', 'data analytics', 'retail and shopping']		1	26	0	ekenetobi	cyclistic-bikeshare-12-months-analysis
2901	2901	Hearthstone Decks 2022	Tournament Deck Data From Battlefy	['games', 'card games', 'data cleaning', 'data analytics', 'classification', 'clustering']	"Context
Hearthstone is a popular collectible card game released by Blizzard Entertainment in 2014. Constructed mode represents the most typical mode of Hearthstone play, which allows players build decks of cards from their own collection and use them to play against others. Players may refer to sites like HEARTHSTONE TOP DECKS, HSReplay etc. to rapidly build high win-rate decks based on metagame(trends).  Under the metagame, many popular decks are composed of different cards while having some similarity, i.e. different decks may belong to the same archetype under the same class. For example, players may encounter many Aggro Hunters(AKA Face Hunters) with only minor differences. 
In the case of Hearthstone data mining and analysis, a large amount of deck data is required to uncover the features and trends of the deck. We can obtain player deck data from Hearthstone Tournament. The official Hearthstone tournament consists of a three-tiered tournament system, collectively called Hearthstone Masters, including Hearthstone Masters Qualifiers/Hearthstone Masters Tour/Hearthstone Grandmasters. Usually, conquest is the main tournament format used in Hearthstone tournaments, which require players bring a specific number of decks from different classes.
This dataset was inspired by Tim Inzitari¹ and constructed by part of the code in his work. This dataset collects player deck data from Battlefy's tournaments since 2022 and can be further exploited by python-hearthstone² and api³ after processing.
Content
This dataset consists of two folders:
- PlayerDetails - Players' 3 decks with detailed information. Named as ""playerDecks-WeekOfYYYY-MM-DD.csv"".
   - TournyID: Tournament's ID
   - Player: Player's name
   - Deck1: Detailed information of Deck1
   - Deck2: Detailed information of Deck2
   - Deck3: Detailed information of Deck3
- DecodedDecks - Deck code extracted from previous data and converted by python-hearthstone. Named as ""decodedDecks-WeekOfYYYY-MM-DD.csv"".
   - DeckInfo: Detailed information of deck
   - Code: Deck code extracted from DeckInfo
   - DecodedDeck: DBF IDs of deck
Acknowledgements & References
Inzitari, Tim, ""Clustering Data to Classify Hearthstone Decks"" (2021). Williams Honors College, Honors Research Projects. 1261. https://ideaexchange.uakron.edu/honors_research_projects/1261 
HearthSim, python-hearthstone, https://github.com/hearthsim/python-hearthstone
HearthSim, HearthstoneJSON, https://api.hearthstonejson.com/v1/"	1	91	3	undefinenull	hearthstone-decks-2022
2902	2902	Berlin_Airbnb		[]		1	33	0	temporaryaccabhi	berlin-airbnb
2903	2903	Few-shot version of NAR	few-shot-version-of-nar	['video games']		3	23	0	gmshroff	few-shot-nar
2904	2904	GDP SECTOR WISE		[]		2	11	0	raghavpunglia	gdp-sector-wise
2905	2905	Airbnb Listings & Reviews	Airbnb Listings & Reviews	['geospatial analysis', 'time series analysis', 'travel', 'hotels and accommodations']	Airbnb data for 250,000+ listings across 10 major cities, along with ~5 million guest reviews.	10	71	1	vaibhav2025	airbnb-listings-reviews
2906	2906	VesselNet_Code		[]		9	289	2	srinjoybhuiya	vesselnet-code
2907	2907	EMPLOYEE ATTRITION DATASET	It's a real world data which include more than 20 companies and 650 entries.	[]		6	75	0	purvibhoyar	employee-attrition-dataset
2908	2908	120 years of Olympic History	Olympics details of 120 years	['sports', 'geospatial analysis', 'time series analysis']	Historical data on the modern Olympic Games, from Athens 1896 to Rio 2016.	19	91	1	vaibhav2025	120-years-of-olympic-history
2909	2909	titanic		[]		0	13	1	deepakkarkisf	titanic
2910	2910	Bigg Boss India - Hindi Telugu Tamil Kannada	Bigg Boss Telugu/Tamil/Kannada/Malayalam/Hindi/Marathi/Bengali Data sets	['arts and entertainment', 'india', 'exploratory data analysis', 'data visualization', 'classification']	"Bigg Boss India series - Telugu/Tamil/Kannada/Hindi/Malayalam/Marathi/Bengali Data sets.
Do you like to predict Bigg Boss winner using any Machine/Deep Learning algorithm ? or visualize Bigg Boss India seasons data
Bigg Boss India - data set of all Indian (Big Brother) versions and seasons
Bigg Boss Hindi Season 1/2/3/4/5/6/7/8/9/10/11/12/13/14/15 and Halla bol, BiggBoss OTT
Bigg Boss Kannada Season 1/2/3/4/5/6/7/8
Bigg Boss Telugu Season 1/2/3/4/5
Bigg Boss Tamil Season 1/2/3/4/5 and Ultimate
Bigg Boss Malayalam Season 1/2/3
Bigg Boss Marathi Season 1/2/3
Bigg Boss Bangla Season 1/2
Here is the data dictionary for Big Boss (India) season's dataset.
Language - Language in which the show was telecasted/designed
Season Number - Season number
Name - Name of participant. Original Entrants are sorted in
 alphabetical order
Profession - Profession of house mate
Gender - Gender of the BiggBoss contestant, such as Male, Female, LGBT
Entry Date - Date of entry into house
Elimination Date - Date of elimination/Eviction
Elimination Week Number - (Final) Eviction week number
Wild Card - Entered through wild card or not
Season Length - Number of days of season
Number of Housemates - Total number of house mates, in entire season, including wildcard entries
Season Start Date - Season start date
Season End Date - Season last day or Finale day
Host Name - Name of the host (who hosted most of the weekend episodes)
Guest Host Name - Name of the guest/temporary host
Prize Money (INR) - Total prize money in INR
Broadcasted By - Channel name who has broadcasted the season (in India)
Average TRP - Average TV TRP of the season
Number of Evictions Faced - Number of Evictions Faced by the Bigg Boss housemate
Number of re-entries - Number of reentries into the house, in the same season (not fake evictions & secret room)
Number of times elected as Captain - Number of times/weeks elected as Captain
Social Media Popularity - Popularity in Social media (1 - lowest, 10 - highest) during the show
Finalist - Whether entered into final or not (0 - No, 1 - Yes)
Winner - Winner or not (1 - winner, 0 - otherwise)"	446	7466	25	thirumani	bigg-boss-india-hindi-telugu-tamil-kannada
2911	2911	Boston		[]		0	7	0	mayur1998	boston
2912	2912	Ubiquant LightGBM Models		[]		0	6	0	ayuraj	ubiquant-lgbm
2913	2913	Oakland Pallet Co., Inc.		['transportation']	Oakland Pallet is a Manufacturer of Pallets in California. We offer Custom Recycled Hybrid Pallets, Hardwood, Deck Wing Pallets in Bay Area, California. Oakland Pallet is your premier pallet supplier providing customer-driven solutions. We offer the largest variety of&nbsp;Upcycled&nbsp;48x40 GMA grades and custom&nbsp;Recycled&nbsp;Hybrid pallets&nbsp;in California.	0	10	0	oaklandpallet	oakland-pallet-co-inc
2914	2914	retail-data-MB-analysis		[]		0	18	0	ujoshi076	retaildatambanalysis
2915	2915	Testing github actions for uploading datasets		[]		0	11	0	ayuraj	test-dataset
2916	2916	simpledatasettoxiccompetition		[]		0	4	1	crischir	simpledatasettoxiccompetition
2917	2917	COVID-QU 		['universities and colleges']	"COVID-QU-Ex Dataset
The researchers of Qatar University have compiled the COVID-QU-Ex dataset, which consists of 33,920 chest X-ray (CXR) images including:
-   11,956 COVID-19
-   11,263 Non-COVID infections (Viral or Bacterial Pneumonia)
-   10,701 Normal
Ground-truth lung segmentation masks are provided for the entire dataset. This is the largest ever created lung mask dataset. 
If you use  COVID-QU-Ex Dataset in your research, please consider to cite the publications/dataset below:
1 A. M. Tahir, M. E. H. Chowdhury, A. Khandakar, Y. Qiblawey, U. Khurshid, S. Kiranyaz, N. Ibtehaz, M. S. Rahman, S. Al-Madeed, S. Mahmud, M. Ezeddin, K. Hameed, and T. Hamid, “COVID-19 Infection Localization and Severity Grading from Chest X-ray Images”, Computers in Biology and Medicine, vol. 139, p. 105002, 2021, https://doi.org/10.1016/j.compbiomed.2021.105002.
2 Anas M. Tahir, Muhammad E. H. Chowdhury, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Serkan Kiranyaz, Uzair Khurshid, Nabil Ibtehaz, Sakib Mahmud, and Maymouna Ezeddin, “COVID-QU-Ex .” Kaggle, 2021, https://doi.org/10.34740/kaggle/dsv/3122958.
3 T. Rahman, A. Khandakar, Y. Qiblawey A. Tahir S. Kiranyaz, S. Abul Kashem, M. Islam, S. Al Maadeed, S. Zughaier, M. Khan, M. Chowdhury, ""Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-rays Images,"" Computers in Biology and Medicine, p. 104319, 2021, https://doi.org/10.1016/j.compbiomed.2021.104319.
4 A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. H. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj, ""Covid-19 infection map generation and detection from chest X-ray images,"" Health Inf Sci Syst 9, 15 (2021), https://doi.org/10.1007/s13755-021-00146-8.
5 M. E. H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir, Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. A. Emadi, M. B. I. Reaz, M. T. Islam, ""Can AI Help in Screening Viral and COVID-19 Pneumonia?,"" IEEE Access, vol. 8, pp. 132665-132676, 2020, https://doi.org/10.1109/ACCESS.2020.3010287.
To the best of our knowledge, this is the first study that utilizes both lung and infection segmentation to detect, localize and quantify COVID-19 infection from X-ray images. Therefore, it can assist the medical doctors to better diagnose the severity of COVID-19 pneumonia and follow up the progression of the disease easily.
The experiments were conducted on two CXR sets, where each set is divided into train, validation and test sets: 
1)  Lung Segmentation Data
        Entire COVID-QU-Ex dataset (33,920 CXR images with corresponding ground-truth lung masks)
2)  COVID-19 Infection Segmentation Data
         A subset of COVID-QU-Ex dataset (1,456 Normal and 1,457 Non-COVID-19 CXRs with corresponding lung mask, plus 2,913 COVID-19 CXRs with 
         corresponding lung mask from COVID-QU-Ex dataset and corresponding infections masks from QaTaCov19 dataset).
References
In COVID-QU-Ex, the X-ray images are collected from the following repositories and studies:
•   COVID-19 Samples: [1- 7].
•   Non-COVID Samples: [8- 10].
•   Normal Samples: [8- 10].
1 QaTa-COV19 Database. https://www.kaggle.com/aysendegerli/qatacov19-dataset. Accessed 14 March 2021.
2 Covid-19-image-repository. Available: https://github.com/ml-workgroup/covid-19-image-repository/tree/master/png. Accessed 14 March 2021.
3 Eurorad. Available: https://www.eurorad.org/. Accessed 14 March 2021.
4 Covid-chestxray-dataset. Available: https://github.com/ieee8023/covid-chestxray-dataset. Accessed 14 March 2021.
5 COVID-19 DATABASE. Available: https://www.sirm.org/category/senza-categoria/covid-19/. Accessed 14 March 2021.
6 Kaggle. (2020). COVID-19 Radiography Database. Available: https://www.kaggle.com/tawsifurrahman/covid19-radiography-database. Accessed 14 March 2021.
7 GitHub. (2020). COVID-CXNet. Available: https://github.com/armiro/COVID-CXNet. Accessed 14 March 2021.
8 RSNA Pneumonia Detection Challenge. Available: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data. Accessed 14 March 2021.
9 Chest X-Ray Images (Pneumonia). Available: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia. Accessed 14 March 2021.
10 Medical Imaging Databank of the Valencia Region. PadChest: A large chest x-ray image dataset with multi-label annotated reports. Available: https://bimcv.cipf.es/bimcv-projects/padchest/. Accessed 14 March 2021."	76	1018	6	anasmohammedtahir	covidqu
2918	2918	ruddit_with_text_arguments		[]		0	16	0	takeshiiijima	ruddit-with-text-arguments
2919	2919	India Food Price (2000 - 2020)	Predict 2019-2020 Onions Price	['exploratory data analysis', 'time series analysis', 'logistic regression', 'regression', 'datetime']	"Datasets
This dataset is data showing the price of 7 food ingredients in India as a time series.  
It represents the monthly price (INR unit) from 2000 to 2020.
Problem
The problem to be solved is to predict the onion price from 2019.  
I have added the correct answer to ""result.csv"" separately.
source : link and post-processing of Data"	226	1093	12	kukuroo3	india-food-price-2000-2020
2920	2920	output_hateBert		[]		1	17	0	li874039270	output-hatebert
2921	2921	TF2_MAS		[]		2	12	0	anjararakotoarisoa12	tf2-mas
2922	2922	GBR Starfish TFRecords Mini 3X 0 1		[]		0	9	0	mmelahi	gbr-starfish-tfrecords-mini-3x-0-1
2923	2923	pbvs_limited_600		[]		0	7	0	adityakane	pbvs-limited-600
2924	2924	Google Play Store Category wise Top 500 Apps	Top 500 category wise app data on the google play store (ranked asper Jan 2022)	['search engines', 'exploratory data analysis', 'data visualization', 'data analytics', 'tabular data']	"Context
Google Play stores top 500 app data based on their rankings on January 2022 for all the available categories.
Link to scraping code: https://github.com/Shakthi-Dhar/AppPin
Link to backup datafiles: github data files
Content
The dataset contains the top 500 android apps available on the google play store for the following categories:
All Categories, Art & Design, Auto & Vehicles, Beauty, Books & Reference, Business, Comics, Communication, Education, Entertainment, Events, Finance, Food & Drink, Health & Fitness, House & Home, Libraries & Demo, Lifestyle, Maps & Navigation, Medical, Music & Audio, News & Magazines, Parenting, Personalization, Photography, Productivity, Shopping, Social, Sports, Tools, Travel & Local, and Video Players & Editors.
The app rankings are based on google play store app rankings for January 2022.
Abbreviations
In Review and Downloads, the alphabet T, L, Cr represents Thousands, Lakhs, Crores as per the google play store naming convention. They are similar to M, B which represent millions, billions.
1L (1 Lakh) = 100T (100 Thousand)
10L (10 Lakhs) = 1M (1 Million)
1Cr( 1 Crore) = 10M (10 Million)
Acknowledgements
This data is not provided directly by Google, so I used Appium an automation tool with python to scrape the data from the google play store app.
Inspiration
Inspired by Fortune500. Fortune500 provides data on top companies in the world, so why not have a data source for top apps in the world."	485	2154	23	shakthidhar	google-play-store-category-wise-top-500-apps
2925	2925	FB dataset_1024x256		['social networks']		0	23	0	tchaye59	fb-dataset-1024x256
2926	2926	cycleGAN_74690		[]		0	1	0	huangkailong	cyclegan-74690
2927	2927	ProfileDataset		[]		2	7	1	rameshbadrinath	profiledataset
2928	2928	NBA_2013		[]		1	11	1	areebameerkhan	nba-2013
2929	2929	cassava disease augmented		[]		10	12	0	alexlianardo	cassava-disease-augmented
2930	2930	MyImages		[]		0	0	0	kssvp1407	myimages
2931	2931	riiid_networktttt		[]		0	4	0	zfffff	riiid-networktttt
2932	2932	output_toxic-bert		[]		1	20	0	li874039270	output-toxicbert
2933	2933	depressive_tweets_processed		[]		0	13	0	dongphilyoo	depressive-tweets-processed
2934	2934	output_unbiased-toxic-roberta		['music']		2	24	0	li874039270	output-unbiasedtoxicroberta
2935	2935	Spotify_last_decade_songs		['music']	"Context
This dataset is extracted from Spotify API, containing audio features of the songs.
Features
artist_name (object): Name of the artist.
track_id (object): Spotify unique track id.
track_name (object): Name of the track/song.
acousticness (float): A confidence measure from 0.0 to 1.0 of whether the 
track is acoustic. 1.0 represents high confidence 
the track is acoustic.
danceability (float): Danceability describes how suitable a track is for 
dancing based on a combination of musical elements 
including tempo, rhythm stability, beat strength, 
and overall regularity. A value of 0.0 is least 
danceable and 1.0 is most danceable.
duration_ms (int): The duration of the track in milliseconds.
energy (float): Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, 
energetic tracks feel fast, loud, and noisy. For example, 
death metal has high energy, while a Bach prelude scores low 
on the scale. Perceptual features contributing to this 
attribute include dynamic range, perceived loudness, timbre, 
onset rate, and general entropy.
instrumentalness (float): Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this 
context. Rap or spoken word tracks are clearly “vocal”. 
The closer the instrumentalness value is to 1.0, the 
 greater likelihood the track contains no vocal content. 
Values above 0.5 are intended to represent instrumental 
tracks, but confidence is higher as the value approaches 1.0.
key (int): The estimated overall key of the track. Integers map to pitches using 
standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. 
If no key was detected, the value is -1.
liveness (float): Detects the presence of an audience in the recording. 
Higher liveness values represent an increased probability that the 
track was performed live. A value above 0.8 provides strong likelihood 
that the track is live.
loudness (float): The overall loudness of a track in decibels (dB). 
Loudness values are averaged across the entire track and are useful for comparing 
relative loudness of tracks. Loudness is the quality of a sound that is the primary 
psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.
mode( int): Mode indicates the modality (major or minor) of a track, the type of scale 
from which its melodic content is derived. Major is represented by 1 and minor is 0.
speechiness (float): Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry),the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably 
made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain 
both music and speech, either in sections or layered, including such cases as rap music. 
Values below 0.33 most likely represent music and other non-speech-like tracks. 
tempo - beats per minute
time_signature (int): An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).
valence (float): A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks 
with low valence sound more negative (e.g. sad, depressed, angry).
popularity - overall popularity score (based on # of clicks) The popularity of the track. The value will be between 0 and 100, with 100 being the most popular.
The popularity of a track is a value between 0 and 100, with 100 being the most popular. 
The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.
Generally speaking, songs that are being played a lot now will have a higher popularity than songs that 
were played a lot in the past.
USE
This dataset can be used in many ways try out
- clustering
- classification
- visualization
- Exploratory Data Analysis
- Add new features of your own"	0	8	0	tanishayadav10000	spotify-last-decade-songs
2936	2936	Student Performance Data Set (competition form)	regression training dataset	['social science', 'linear regression', 'regression', 'social networks']	"Context
This dataset was taken from link and preprocessed and separated into competition format.
The label for the test data is provided in the form of a function."	745	7009	43	kukuroo3	student-performance-data-set-competition-form
2937	2937	Mosquito Indicator in Seoul, Korea	2016~ 2019, daily data with Temperature, precipitation	['atmospheric science', 'exploratory data analysis', 'linear regression', 'regression']	"Context
This dataset deals with Mosquito Indicator and Temperature, precipitation  in Seoul, South Korea. 
Mosquito Indicator is a number of Mosquito per Specific area.
Content
This data provides six cols (date, mosquito_Indica, rain(mm), mean_T(℃), min_T(℃), max_T(℃))
Data were measured every day between  2016-05 and 2019-12.
Data were measured for Specific area in Seoul.
Acknowledgements
Data is provided from here.
https://news.seoul.go.kr/welfare/mosquito"	281	2228	29	kukuroo3	mosquito-indicator-in-seoul-korea
2938	2938	cassava plant disease augmented		[]		1	13	0	alexlianardo	cassava-plant-disease-augmented
2939	2939	GBR Starfish TFRecords Mini 3X 2 1		[]		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-3x-2-1
2940	2940	GBR Starfish TFRecords Mini 3X 2 0		[]		0	5	0	mmelahi	gbr-starfish-tfrecords-mini-3x-2-0
2941	2941	GBR Starfish TFRecords Mini 3X 0 0		[]		0	5	0	mmelahi	gbr-starfish-tfrecords-mini-3x-0-0
2942	2942	cv-data		[]		0	3	0	darkdatascientist007	cvdata
2943	2943	Cyclistic project	Divvy Monthly Trip Data 2021 	['travel']		0	27	0	samuelmaduka	cyclistic-project
2944	2944	GBR Starfish TFRecords Mini 3X 0 2		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-3x-0-2
2945	2945	GBR Starfish TFRecords Mini 3X 2 2		[]		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-3x-2-2
2946	2946	DNS-source		[]		0	8	0	jerrymark611	dnssource
2947	2947	202107-divvy-tripdata.csv 		[]		0	10	0	andreps	202107divvytripdatacsv
2948	2948	dataset		[]		0	5	0	asadhumam	dataset
2949	2949	Tugas visual		[]		1	12	0	wahyuprasetyoaji	tugas-visual
2950	2950	Tugas visualisasi		[]		0	6	0	wahyuprasetyoaji	tugas-visualisasi
2951	2951	Temperature of this year around Hirosaki City	This year's temperature around Hirosaki and forecast for the next two weeks	['computer science']	"Context
This year's temperature data to predict the flowering of cherry blossoms at Hirosaki Park.
Content
This year's temperature and 2-week forecast released by the Japan Meteorological Agency.
Acknowledgements
Thanks for the data provided by the Japan Meteorological Agency.
Inspiration
How to predict future temperatures?"	63	2241	4	akioonodera	hirosaki-this-year
2952	2952	Fitness Tracker	Weight, fat and muscle 	['asia', 'exercise', 'data analytics', 'text data']	"I start to collect weight, fat and muscle data after registered private heavy training course in 2020.
As you can see, it's a raw data and with some simple calculations. There are several sheets to describe details and yearly weight.
After realizing my weight of muscle is under standard, I start to record and think about how can I improve this.  Thanks to myself that i spend time on this program. 
Keep health is one of important values in my life; therefore, I also encourage others to eat health food and exercise regularly."	15	142	3	rubyhsu	fitness-tracker
2953	2953	ext_tx_dstbrt		[]		0	5	1	datafan07	ext-tx-dstbrt
2954	2954	Bike Sharing System	Build a model for the prediction of demand for shared bikes	['automobiles and vehicles', 'cycling', 'linear regression']	"Problem Statement
A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a ""dock"" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.
A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. 
In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.
They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:
Which variables are significant in predicting the demand for shared bikes.
How well those variables describe the bike demands
Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. 
Business Goal
You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market."	1	14	0	venkatasubramanian	bike-sharing-system
2955	2955	SEC Filings 1994-2020	SEC Filings 1994-2020	['business', 'finance', 'investing']	"Context
With the sole mission to democratize financial data, Finnhub is excited to release the new Filings metadata dataset for bulk download. The data is cleaned and sourced directly from SEC filings from 1994-2020. 
If you don't need bulk download, you can query this data for free on our website: https://finnhub.io/docs/api#filings. We also provide various type of financial data such as global fundamentals, deep historical tick data, estimates and alternative data.
Finnhub Stock API"	713	7562	15	finnhub	sec-filings
2956	2956	sentiment_nuclear_power1.csv		[]		0	11	0	greglevay	sentiment-nuclear-power1csv
2957	2957	Financials as Reported 2010-2020 - SEC Filings	Financial data parsed from 10-Q, 10-Q/A, 10-K, 10-K/A SEC filings from 2010.	['finance', 'investing']	"Context
With the sole mission to democratize financial data, Finnhub is excited to release the new Financials as Reported dataset for bulk download. The data is cleaned and sourced directly from SEC filings from 2010-2020. 
If you don't need bulk download, you can query this data for free on our website: https://finnhub.io/docs/api#financials-reported. We also provide various type of financial data such as global fundamentals, deep historical tick data, estimates and alternative data.
Finnhub Stock API
https://finnhub.io/terms-of-service"	1464	22240	72	finnhub	reported-financials
2958	2958	Identifying Entities in HealthCare data	Build a custom NER to get the list of diseases and their treatment	['health', 'nlp']	"Now, let’s consider a hypothetical example of a health tech company called ‘BeHealthy’. Suppose ‘BeHealthy’ aims to connect the medical communities with millions of patients across the country. 
‘BeHealthy’ has a web platform that allows doctors to list their services and manage patient interactions and provides services for patients such as booking interactions with doctors and ordering medicines online. Here, doctors can easily organise appointments, track past medical records and provide e-prescriptions.
So, companies like ‘BeHealthy’ are providing medical services, prescriptions and online consultations and generating huge data day by day.
Let’s take a look at the following snippet of medical data that may be generated when a doctor is writing notes to his/her patient or as a review of a therapy that he or she has done.
“The patient was a 62-year-old man with squamous cell lung cancer, which was first successfully treated by a combination of radiation therapy and chemotherapy.”
As you can see in this text, a person with a non-medical background cannot understand the various medical terms. We have taken a simple sentence from a medical data set to understand the problem and where you can understand the terms ‘cancer’ and ‘chemotherapy’. 
Suppose you have been given such a data set in which a lot of text is written related to the medical domain. As you can see in the dataset, there are a lot of diseases that can be mentioned in the entire dataset and their related treatments are also mentioned implicitly in the text, which you saw in the aforementioned example that the disease mentioned is cancer and its treatment can be identified as chemotherapy using the sentence.
But, note that it is not explicitly mentioned in the dataset about the diseases and their treatment, but somehow, you can build an algorithm to map the diseases and their respective treatment.
Suppose you have been asked to determine the disease name and its probable treatment from the dataset and list it out in the form of a table or a dictionary.
After discussing the problem given above, you need to build a custom NER to get the list of diseases and their treatment from the dataset."	2	51	0	venkatasubramanian	entity-recognition-medical
2959	2959	1920_yolov5m6_6_COTS		[]		0	3	0	vexxingbanana	1920-yolov5m6-6-cots
2960	2960	rdr-detectoion-train-output		[]		0	8	0	hanifamalrobbani	rdrdetectoiontrainoutput
2961	2961	Images		[]		0	7	0	andreps	images
2962	2962	Telecom Churn Case Study	Identify customers at high risk of churn and the main indicators of churn	['business', 'logistic regression']	"Problem Statement
Business problem overview
In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.
For many incumbent operators, retaining high profitable customers is the number one business goal.
To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.
In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.
Understanding and defining churn
There are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).
In the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.
However, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).
Thus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and Southeast Asia, while postpaid is more common in Europe in North America.
This project is based on the Indian and Southeast Asian market.
Definitions of churn
There are various ways to define churn, such as:
Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.
The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.
Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.
A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.
In this project, you will use the usage-based definition to define churn.
High-value churn
In the Indian and the Southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.
In this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.
Understanding the business objective and the data
The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. 
The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.
Understanding customer behaviour during churn
Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :
The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.
The ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)
The ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.
In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase."	3	44	0	venkatasubramanian	telecom-churn-case-study
2963	2963	dados.csv		['internet']		0	11	0	marianaaraki	dadoscsv
2964	2964	House Price Prediction	Predict the actual value of the prospective properties	['real estate', 'linear regression']	"Problem Statement
A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.
The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.
The company wants to know the following things about the prospective properties:
Which variables are significant in predicting the price of a house, and
How well those variables describe the price of a house.
Also, determine the optimal value of lambda for ridge and lasso regression.
Business Goal
You are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market."	3	47	1	venkatasubramanian	house-price-prediction
2965	2965	Steam Reviews		['ratings and reviews']		9	98	1	nourelkamel	steam-reviews
2966	2966	MolDyn_OPL_CK1		[]		0	11	1	marioangelopagano	moldyn-opl-ck1
2967	2967	tiger_grand_challenge_data_DLV3plus_pre		[]		0	1	0	albertozorzetto	tiger-grand-challenge-data-dlv3plus-pre
2968	2968	Dataset		[]		0	6	0	andreps	dataset
2969	2969	cylistic_trip_data	Bikeshare company's historical trip data	['business', 'cycling', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics', 'news']	"Context
Welcome to the Cyclistic bike-share analysis case study! In this case study, you will perform many real-world tasks of a junior data analyst. You will work for a fictional company, Cyclistic, and meet different characters and team members. In order to answer the key business questions, you will follow the steps of the data analysis process: ask, prepare, process, analyze, share, and act. Along the way, the Case Study Roadmap tables — including guiding questions and key tasks — will help you stay on the right path. By the end of this lesson, you will have a portfolio-ready case study. 
You are a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore, your team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights, your team will design a new marketing strategy to convert casual riders into annual members. But first, Cyclistic executives must approve your recommendations, so they must be backed up with compelling data insights and professional data visualizations.
In 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime.
Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments. One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers who purchase annual memberships are Cyclistic members.
Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs.
Moreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.
Content
The datasets contain the previous 12 months of Cyclistic trip data. The datasets have a different name because Cyclistic is a fictional company. For the purposes of this case study, the datasets are appropriate and will enable you to answer business questions. 
Acknowledgements
This data has been made available by Motivate International Inc. under this license. This is public data that you can use to explore how different customer types are using Cyclistic bikes. But note that data-privacy issues prohibit you from using riders’ personally identifiable information. This means that you won’t be able to connect pass purchases to credit card numbers to determine if casual riders live in the Cyclistic service area or if they have purchased multiple single passes.
Inspiration
Research question: How do annual members and casual riders use Cylistic bikes differently."	12	91	2	trnguyen1510	cylistic-trip-data
2970	2970	whaler		[]		0	12	0	neverstoppredicting	whaler
2971	2971	yhallc		[]		0	10	0	neverstoppredicting	yhallc
2972	2972	2020 Kansas HMDA Data	Mortgage origination and application data	['united states', 'finance', 'lending', 'banking']	"Context
The Home Mortgage Disclosure Act (HMDA) requires many financial institutions to collect and report data on originated loans and loan applications for mortgages. The Consumer Financial Protection Bureau (CFPB) currently has rulemaking authority on Regulation C which implements HMDA. Data for years 2007 - 2017 available at CFPB.gov while data for 2017 and onward is available at FFIEC.cfpb.gov.
Content
This HMDA data set is all applicable data for the state of Kansas during 2020.
For detailed information on the fields and how they should be reported: here is the CFPB's HMDA site"	1	32	0	janaedoes	2020-kansas-hmda-data
2973	2973	scRNA-seq neuroepithelial-derived cells and glioma	GSE117004 Neural G0: a quiescent-like state found in glioma and neuroep.-derived	['genetics', 'biology', 'biotechnology']	"Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes (csv file is vice versa).
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Particular data from: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117004
There are several subdatasets corresponding to cells with/without some knockouts and etc. 
Paper:  Neural G0: a quiescent-like state found in neuroepithelial-derived cells and glioma
https://www.biorxiv.org/content/10.1101/446344v3.full
https://www.embopress.org/doi/full/10.15252/msb.20209522
Abstract: Here, we used scRNA-seq to examine the cell cycle states of expanding human neural stem cells (hNSCs). From this data, we created a cell cycle classifier, which, in addition to traditional cell cycle phases, also identifies a putative quiescent-like state in neuroepithelial-derived cell types during mammalian neurogenesis and in gliomas. This state, Neural G0, is enriched for expression of quiescent NSC genes and other neurodevelopmental markers found in non-dividing neural progenitors. For gliomas, Neural G0 cell populations and gene expression is significantly associated with less aggressive tumors and extended patient survival. Genetic screens to identify modulators of Neural G0 revealed that knockout of genes associated with the Hippo/Yap and p53 pathways diminished Neural G0 in vitro, resulting in faster G1 transit, down regulation of quiescence-associated markers, and loss of Neural G0 gene expression. Thus, Neural G0 represents a dynamic quiescent-like state found in neuro-epithelial derived cells and gliomas.
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	6	248	0	alexandervc	scrnaseq-neuroepithelialderived-cells-and-glioma
2974	2974	forward_datasets		[]		2	15	0	abdelghanibelgaid	forward-datasets
2975	2975	Spike waveforms	Spike waveforms from individual neurons	['earth and nature', 'biology', 'neuroscience', 'science and technology', 'medicine', 'electronics']	"Introduction
Neurons in the brain use electrical signals communication. Different spike waveforms correspond to different cell types or different neuron morphologies (Henze et al., J. Neurophysiol. 2000). In learning about different spike waveforms, we may identify different neuronal types in our sample.
Content
The data contains 150 numerical categories (0 to 149, in columns) corresponding to the sampling of a 5-millisecond extracellular voltage at 30 kHz. Since the extracellular voltage amplitude related to the squared of the distance to the cell, we normalize the whole trace to the minimal voltage (through) that corresponds to the peak of the spike. To load it in Python.
import pandas as pd
waveforms = pd.read_csv('waveforms.csv', index_col = 'uid')
waveforms.info()
waveforms.iloc[0, :-1,].plot() # plot the first waveform (last column is organoid)
To know more
For more information visiy my GitHub repository"	126	2359	20	joseguzman	waveforms
2976	2976	AR Tags dataset	A collection consisting of ALVAR AR Tags photos in the open air	['image data']	"Context
The dataset contains photos of AR Tags, which are located on landmarks representing those originally appearing at the European Rover Challenge - annual international Martian robots competition attended by teams from around the world.
This dataset can be used to test fiducial markers detection methods, in particular ALVAR AR Tags detection, which are used, inter alia, in the aforementioned robotic competition.
Content
The dataset contains 112 photos of AR Tags taken outdoors (located in the 'images' folder).
The 'images_roi.json' file contains bounding box coordinates (for each image), inside which there are markers (one bounding box per marker).
The 'display_ROI_on_images.m' script contains a function to visualize the bounding boxes for a selected photo using MATLAB software.
Acknowledgements
Inspiration
Creation of this dataset was inspired by the author's participation in the European Rover Challenge - to develop and test more and more accurate marker detection algorithms for the competition needs."	1	47	0	tirukei	ar-tags-dataset
2977	2977	competition		[]		0	6	0	ismailtchich	competition
2978	2978	test_data		[]		0	3	0	ulyssebaruchel	test-data
2979	2979	train_data		[]		0	4	0	ulyssebaruchel	train-data
2980	2980	Lead Scoring Case Study	We will focus on improving the Lead Conversion Rate of an Education company	['education']	"Problem Statement
An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. 
The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. 
Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:
As you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.
X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.
Data
You have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page. Another thing that you also need to check out for are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value"	3	20	0	venkatasubramanian	lead-scoring-case-study
2981	2981	EudraVigilance		[]		0	8	0	zrafiws	eudravigilance
2982	2982	Wordle Word List		[]		3	25	0	talhaazim	wordle-word-list
2983	2983	Julia Tarball		[]		0	16	0	tpmeli	julia-tarball
2984	2984	Talk like F.R.I.E.N.D.S 🤗	Dataset Comprising of all the lines said by famous F.R.I.E.N.D.S cast.	['arts and entertainment', 'movies and tv shows', 'data analytics', 'text data', 'spaCy']	"About Data
Hey lobsters after all the hard work, the sweat, the frustration, the strain, and omg what not should I say. I have taken to complete this project was so surreal, I mean from data collection(WEB SCRAPING USING SELENIUM), cleaning the data, and to extract as many answers as I wanted and then loading them onto excel was so exciting and a bit why?, because although there were many data sources there was never like this one before, so in order to put an end to this gap, I have taken up this challenge for myself and created this dataset and made it public so that everyone can enjoy from it,(Note: this type of dataset is not available on the internet, believe me, I tried a lot/ maybe I didn’t try hard enough, but I got this. So, after all the hard work, now is the time for tasting the dish that we have prepared, yes it is time to perform some Analysis and answers the question and prepare a cool dashboard too,, you know,,, to make yourself happy that you have used your unique skills and learned something from this project and being proud of yourself, that you have not only created this dataset but also inspired people who are as passionate as you are to work on this, that is the only thing which separates from the ordinary people and if you guys have anything that you guys would like to know about the sitcom, the Data is yours, feel free to play with the data and know your answers.
But, before that, please give me credits for creating this data and for my friend(fangj) who provided the scripts 
script_links
and if you guys need all the step-by-step cleaning datasets which involve from web scraping to loading them onto excel, please feel free to contact me at 
gmail,
believe me, I'll reply within 48 hrs., ciao.
Although the Scraping code which I have built may be simple and not so precise, but damn it took a lot of hours to build that exact piece of code to extract the information I needed.
Again please give me some tips on how to improve myself on this, watch out for other popular web series datasets coming in the future.
Oh, yeah the scraped code for this data is in my GitHub so you can check that out from the below link.
Github
Watch me out on Linkedin
Linkedin
Feel Free to involve me in any kind of discussion's to improve my knowledge of data analysis
Content
So this data consists of around 10 columns and approximately 40k+ rows which comprises all the lines said by one of the famous and greatest sitcoms ever made F.R.I.E.N.D.S.
All the way from, Monica saying There's nothing to tell! He's just some guy I work with! till Chandlers iconic Sure, Where each and every line said by famous casts involved in the show or if you want all the lines spoken by all the people involved in the show, feel free to check out my GitHub account, you can find all the datasets over there.
Acknowledgements
We wouldn't be here without the help of an article but I really did forget that website where I saw the article which inspired me to do this project, but  credits to the owner of the scripts which he had collected  over time and made it to the public so that they can work on them, a big thanks to fangj.github.io
Inspiration
What is the one thing we can do, which can help make this world a better place to live using Data and Humans?"	0	20	0	cyygnusx1	talk-like-friends
2985	2985	Customer Personality		[]		1	23	0	leidythaispulidov	customer-personality
2986	2986	2019 LOK Sabha Election Data	2019 election data candidates winners votes	['politics']		4	18	1	shubhamkumarvaish	2019-lok-sabha-election-data
2987	2987	Pennsylvania SNAP Monthly County & Statewide		[]		0	8	0	bluewall	pennsylvania-snap-monthly-county-statewide
2988	2988	Cardiac_differentiation_RNA-seq		[]		0	2	0	sofiaagostinho	cardiac-differentiation-rnaseq
2989	2989	Waffles and divorce rates	Is there a relationship between waffle houses and divorce rates?	[]	"Context
This dataset comes from the book Statistical Rethinking: A Bayesian Course with Examples in R and Stan
Content
Location : State name
Loc : State abbreviation
Population : 2010 population in millions
MedianAgeMarriage: 2005-2010 median age at marriage
Marriage : 2009 marriage rate per 1000 adults
Marriage.SE : Standard error of rate
Divorce : 2009 divorce rate per 1000 adults
Divorce.SE : Standard error of rate
WaffleHouses : Number of diners
South : 1 indicates Southern State
Slaves1860 : Number of slaves in 1860 census
Population1860 : Population from 1860 census
PropSlaves1860 : Proportion of total population that were slaves in 1860
Acknowledgements
All credit should go to Richard McElreath: https://xcelab.net/rm/statistical-rethinking/"	2	14	0	tylerbonnell	waffles
2990	2990	ważneeee		[]		0	10	0	ignacyruszpel	wazne
2991	2991	IHME’s COVID-19 projections	IHME’s COVID-19 projections show demand for hospital services in each state.	['hospitals and treatment centers', 'covid19']	"Context
Referenced in a White House press briefing as the ""Chris Murray Model,"" IHME’s COVID-19 projections show demand for hospital services in each state.
Content
IHME’s COVID-19 projections show demand for hospital services in each state.
For more info see:
- http://www.healthdata.org/covid
http://www.healthdata.org/covid/faqs
http://www.healthdata.org/covid/updates
Acknowledgements
Data from http://www.healthdata.org/covid/data-downloads
Data license: http://www.healthdata.org/about/terms-and-conditions
Photo by engin akyurt on Unsplash"	132	8541	7	paultimothymooney	ihmes-covid19-projections
2992	2992	Unet_0_255_2D		[]		1	10	0	keval2232	unet-0-255-2d
2993	2993	IPL Salary		['cricket', 'india', 'sports']	"Context
The Story of Salary and IPL
Content
Consider the IPL dataset available in csv. The data set provides the details of the salary of 103 batsmen and all-rounders in 2019 edition of IPL. It also provides several performance indicators for each of these players. The following table reports the description of all the variables.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	1	7	0	cocktailsguy	ipl-salary
2994	2994	Jigsaw HateBert Regression		['games']		1	9	0	leolu1998	jigsaw-hatebert-regression
2995	2995	ubiquant_train_target		[]		3	37	0	pronichevav	ubiquant-train-target
2996	2996	CloudTypes	Files from Cloudsat and GOES16, Southamerica daytime.	['south america', 'categorical data', 'computer science', 'geospatial analysis', 'image data', 'multiclass classification']	"Context
This dataset is meant to be used for Multiclass classification.
Content
The dataset consist of products from GOES-16 and products from CloudSat.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	70	779	0	polavr	cloudtypes
2997	2997	features		[]		0	0	0	abrahamanderson	features
2998	2998	PACKAGES_AS_DATASET		[]		0	45	0	rumata	packages
2999	2999	avacado data		[]		0	3	0	abrahamanderson	avacado-data
3000	3000	input_dict_brown		[]		0	1	0	v3dsingh	input-dict-brown
3001	3001	Music Dataset : 1950 to 2019	Lyrics and Metadata from 1950 to 2019	['beginner', 'intermediate', 'advanced', 'tabular data']	"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; França, Mardônio (2020), “Music Dataset: Lyrics and Metadata from 1950 to 2019”, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3"	700	6189	31	saurabhshahane	music-dataset-1950-to-2019
3002	3002	playlist		['arts and entertainment']		0	2	0	celeste2345	playlist
3003	3003	plotsimage		[]		0	0	0	johnbrooksmith	plotsimage
3004	3004	CarsData		[]		1	8	0	chaitanyakumar001	carsdata
3005	3005	Activity2#		[]		0	8	0	chaitanyakumar001	activity2
3006	3006	final model  colorized 30012022_1		['arts and entertainment']		0	1	0	dmitriyisaev	final-model-colorized-30012022-1
3007	3007	Top games on Twitch 2016 - 2021	Monthly top 200 games on the platform	['video games', 'internet', 'online communities']	"<img src=""https://m.media-amazon.com/images/S/abs-image-upload-na/d/AmazonStores/ATVPDKIKX0DER/a5ac5d9e72132475ffba05e008ace74e.w3000.h600._CR0%2C0%2C3000%2C600_SX1500_.png"">
<br><br>
Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
In this dataset you can find the top 200 games on twitch on each month from 2016 to present day.
The data is divided into two datasets:
- A bigger file - Twitch_game_data in which we find 200 obeservations per month representing the top games or categories on twitch for that month. <br>
 - A smaller file -   Twitch_global_data in which there is one obeservation per month that contains the general statistics about viewership on twitch.
Acknowledgements
All information was taken from  sullygnome.com  - a twitch analytics and statistics site.
Inspiration
Where does your favorite game stands in relation to the top game category on twich?
Is there one game who is the true king of twitch?
Just chatting, is this category just a fad or is it here to stay?
Suggestions and/or comments are welcomed :)"	2151	13321	53	rankirsh	evolution-of-top-games-on-twitch
3008	3008	Fitness Trackers Products Ecommerce	Different products from various Brands with specifications, ratings & reviews	['exercise', 'health', 'internet', 'tabular data', 'python']	"This is a fitness tracker product dataset consisting of different products from various brands with their specifications, ratings and reviews for the Indian market. The data has been collected from an e-commerce website (Flipkart) using webscraping technique.
Content
This dataset contains 565 samples with 11 attributes. There are some missing values in this dataset. Here are the columns in this dataset-
Brand Name: This indicates the manufacturer of the product (fitness tracker)
Device Type: This has two categories- FitnessBand and Smartwatch
Model Name: This indicates the variant/Product Name
Color: This includes the color of the Strap/Body of the fitness tracker
Selling Price: This column has the Selling Price or the Discounted Price of the fitness tracker
Original Price: This includes the Original Price of the product from the manufacturer.
Display: This categorical variable shows the type of display for the fitness tracker. eg: AMOLED, LCD,OLED, etc.
Rating (Out of 5): Average customer ratings on a scale of 5.
Strap Material: Details of the material used for the strap of the fitness tracker.
Average Battery Life (in days): Quoted average battery life from the manufacturer based on the individual product pages. (It is not the scraped data)
Reviews: count of product reviews received.
Inspiration
This dataset could be used to find answers to some interesting questions like -
    1. Is there a significant demand for fitness trackers in the Indian market?
    2. Information on the top 5 brands for fitness bands and smart watches
    3. Is there a correlation between the prices and product specifications, ratings, etc.
    4. Different types of fitness trackers, their price segments for different users"	570	4321	32	devsubhash	fitness-trackers-products-ecommerce
3009	3009	Weather Data (Edexcel Large Data Set)		['news']		5	35	0	tombutton	weather-data-edexcel-large-data-set
3010	3010	Huggingface Accelerate	Run your raw PyTorch training scripts on any kind of device.	['business', 'nlp', 'computer vision', 'tpu', 'gpu']	"Accelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs/TPU/fp16.
Accelerate abstracts exactly and only the boilerplate code related to multi-GPUs/TPU/fp16 and leaves the rest of your code unchanged."	4	781	14	debarshichanda	huggingface-accelerate
3011	3011	predictions		['religion and belief systems']		0	35	0	joaopfonseca	predictions
3012	3012	models		['clothing and accessories']		0	36	0	joaopfonseca	models
3013	3013	Esports Earnings 1998 - 2021	A monthly look at money distributed in various Esports tournaments	['arts and entertainment', 'video games', 'sports', 'online communities']	"Context
Information about cash prizes distributed in various Esport events from 1998 to 2021.
Acknowledgements
Data comes from EsportEarnings.com
Inspiration came from Rushikesh's original dataset, thank you.
If you want to reproduce the database feel free to check my github.
Version Log
Version 3 - Changed the database to present information on a monthly basis
Version 4 - Split the data into two files:
                      - HistoricalEsportData which contains the earnings per month for every game.
                      - GeneralEsportData which contains generic data on each game.
                      * Also added another variable - OnlineEarnings
Feel free to upvote and propose changes :)"	2611	58050	53	rankirsh	esports-earnings
3014	3014	New York Taxi Trips 	New York Taxi trips from 2017-2020	['transportation', 'geospatial analysis', 'time series analysis']		0	71	3	vaibhav2025	new-york-taxi-trips
3015	3015	Churn.csv		[]		1	6	0	priyanshusrivastava8	churncsv
3016	3016	ext_tx_bart		[]		0	2	1	datafan07	ext-tx-bart
3017	3017	kenyasign600512384		[]		0	25	0	mikemollel	kenyasign600512384
3018	3018	corpus		[]		0	7	0	poudelsagar	corpus
3019	3019	placement_dataset		[]		1	6	0	anjalisharma123	placement-dataset
3020	3020	kenyasign600512384tfrecords		[]		1	46	0	mikemollel	kenyasign600512384tfrecords
3021	3021	MARS Rover Environmental Monitoring Station 	Weather and environment data for the planet mars 	['earth and nature', 'astronomy', 'science and technology', 'beginner', 'exploratory data analysis']	"Context
This data is received directly through the Curiosity Rover that is present on the planet Mars. 
Content
Rover Environmental Monitoring Station (REMS) is a weather station on Mars for Curiosity rover contributed by Spain and Finland. REMS measures humidity, pressure, temperature, wind speeds, and ultraviolet radiation on Mars. This Spanish project is led by the Spanish Astrobiology Center and includes the Finnish Meteorological Institute as a partner, contributing pressure and humidity sensors.
Acknowledgements
Image Credits"	139	1604	36	deepcontractor	mars-rover-environmental-monitoring-station
3022	3022	vocab json		[]		0	2	0	gamingnation	vocab-json
3023	3023	casiadataset		[]		7	11	0	sivasathguru	casiadataset
3024	3024	yolov5_f2		[]		1	7	0	selfdriving	yolov5-f2
3025	3025	Satellite Next Day Wildfire Spread	Combines imagery from MODIS, atmospheric, elevation and land-use with fire masks	[]		0	181	0	satellitevu	satellite-next-day-wildfire-spread
3026	3026	seg_cell_modules		[]		0	25	0	itaynivtau	seg-cell-modules
3027	3027	art2forms		[]		0	9	0	jiccrlla	art2forms
3028	3028	GBR Starfish TFRecords Mini 4X 2 2		[]		0	5	0	mmelahi	gbr-starfish-tfrecords-mini-4x-2-2
3029	3029	GBR Starfish TFRecords Mini 4X 1 2		['sports']		0	5	1	mmelahi	gbr-starfish-tfrecords-mini-4x-1-2
3030	3030	GBR Starfish TFRecords Mini 4X 0 2		['sports']		0	4	1	mmelahi	gbr-starfish-tfrecords-mini-4x-0-2
3031	3031	2 Branch Sales Data Set	Sales Data for Forecasting and Visualization Practice	['business']	"Context
This data is random generated in Excel to practice forecasting and visualizations.
Content
The two branches utilize data of thousands of generated product data with nearly 200 different employees.  Product ID numbers are randomly generated for each file
Acknowledgements
This project was for my practice
Inspiration"	5	26	0	michaelbrueckmann	2-branch-sales-data-set
3032	3032	Rossman		[]		0	2	0	moussaadamouidrissa	rossman
3033	3033	model-3		['clothing and accessories']		0	10	0	vigneshirtt	model3
3034	3034	Klasifikasikan jenis kanker		[]		0	6	0	melyas	klasifikasikan-jenis-kanker
3035	3035	Object Detection		['computer science']		10	30	2	songulerdem	object-detection
3036	3036	Denver Crime Data	Denver Crime Data (Updated Monthly)	['crime']	"Context
The city of Denver, CO has an Open Data Catalog: https://www.denvergov.org/opendata/
Content
https://www.denvergov.org/opendata/dataset/city-and-county-of-denver-crime
This dataset includes criminal offenses in the City and County of Denver for the previous five calendar years plus the current year to date. The data is based on the National Incident Based Reporting System (NIBRS) which includes all victims of person crimes and all crimes within an incident. The data is dynamic, which allows for additions, deletions and/or modifications at any time, resulting in more accurate information in the database. Due to continuous data entry, the number of records in subsequent extractions are subject to change. Crime data is updated Monday through Friday.
Acknowledgements
Disclaimer
The information provided here regarding public safety in Denver are offered as a courtesy by the City and County of Denver. By downloading this data, you acknowledge that you have read and understand the Disclaimer below and agree to be bound by it. Certain information is omitted, in accordance with legal requirements and as described more fully in this Disclaimer.
All materials contained on this site are distributed and transmitted ""as is,"" without any representation as to completeness or accuracy and without warranty or guarantee of any kind. The City and County of Denver is not responsible for any error or omission on this site or for the use or interpretation of the results of any research conducted here.
About Crime Data
The Denver Police Department strives to make crime data as accurate as possible, but there is no avoiding the introduction of errors into this process, which relies on data furnished by many people and that cannot always be verified. Data on this site are updated Monday through Friday, adding new incidents and updating existing data with information gathered through the investigative process.
Not surprisingly, crime data become more accurate over time, as new incidents are reported and more information comes to light during investigations.
Crimes that occurred at least 30 days ago tend to be the most accurate, although records are returned for incidents that happened yesterday. This dynamic nature of crime data means that content provided here today will probably differ from content provided a week from now. Likewise, content provided on this site will probably differ somewhat from crime statistics published elsewhere by the City and County of Denver, even though they draw from the same database.
Withheld Data
In accordance with legal restrictions against identifying sexual assault and child abuse victims and juvenile perpetrators, victims, and witnesses of certain crimes, this site includes the following precautionary measures: (a) Addresses of sexual assaults are not included. (b) Child abuse cases, and other crimes which by their nature involve juveniles, or which the reports indicate involve juveniles as victims, suspects, or witnesses, are not reported at all.
Crimes that are initially reported, but that are later determined not to have occurred, are called ""unfounded"" offenses. These incidents are excluded once they have been designated as unfounded.
About Crime Locations
Crime locations reflect the approximate locations of crimes but are not mapped to actual property parcels. Certain crimes may not appear on maps if there is insufficient detail to establish a specific, mappable location.
Author: City and County of Denver, Denver Police Department / Data Analysis Unit
Maintainer: City and County of Denver, Technology Services / DenverGIS Data
Maintainer Email:   denvergis@denvergov.org
Photo by Fezbot2000 on Unsplash
https://www.denvergov.org/opendata/dataset/city-and-county-of-denver-crime"	8033	62272	190	paultimothymooney	denver-crime-data
3037	3037	Dataset with 210,000+ Indian finance news headline		['news']		1	29	4	iamdhruval	dataset-with-210000-indian-finance-news-headline
3038	3038	Supplimentary_FE_Online		[]		0	6	0	hangy132	supplimentary-fe-online
3039	3039	2. MODULES, COMMENTS & PIP (CLASS NOTES)		['software']		0	0	0	profvishnudas	2-modules-comments-pip-class-notes
3040	3040	BellaBeatLogo		[]		0	1	0	johnbrooksmith	bellabeatlogo
3041	3041	Datathon		[]		0	7	0	punithasakthivel	datathon
3042	3042	WiDs Datathon 2022		['standardized testing']		5	81	0	punithasakthivel	wids-datathon-2022
3043	3043	GBR Starfish TFRecords Mini 4X 1 1		['sports']		0	2	0	mmelahi	gbr-starfish-tfrecords-mini-4x-1-1
3044	3044	GBR Starfish TFRecords Mini 4X 1 0		['sports']		0	6	0	mmelahi	gbr-starfish-tfrecords-mini-4x-1-0
3045	3045	EUR_NOK_SEK_Exchange_Rates_2015-2019		['europe', 'economics', 'beginner', 'exploratory data analysis', 'tabular data', 'currencies and foreign exchange']	"Content
Representative Rates for selected currencies between 2015 and 2019, downloaded from IMF: Euro (EUR), Norwegian krone (NOK), Swedish krona (SEK)
These rates, normally({}^{(1)}) quoted as currency units per U.S. dollar, are reported daily to the IMF by the issuing central bank. There is no reporting on holidays.
({}^{(1)}) With some exceptions, include Euro, which are in terms of U.S. dollars per currency unit.  For consistency, the columns for Euro in this dataset are converted from the original IMF data by taking the reciprocal.
EUR_NOK_SEK_Exchange_Rates_2015-2019_NA.csv: Exchange rate data as is, with missing entries on holidays
EUR_NOK_SEK_Exchange_Rates_2015-2019_noNA.csv: Exchange rate data with missing entries imputed using the PCHIP algorithm."	2	43	1	siukeitin	eur-nok-sek-exchange-rates-20152019
3046	3046	GBR Starfish TFRecords Mini 4X 2 1		['sports']		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-4x-2-1
3047	3047	GBR Starfish TFRecords Mini 4X 2 0		[]		0	5	0	mmelahi	gbr-starfish-tfrecords-mini-4x-2-0
3048	3048	GBR Starfish TFRecords Mini 4X 0 1		['sports']		0	4	0	mmelahi	gbr-starfish-tfrecords-mini-4x-0-1
3049	3049	GBR Starfish TFRecords Mini 4X 0 0		['sports']		0	3	0	mmelahi	gbr-starfish-tfrecords-mini-4x-0-0
3050	3050	model1		[]		0	9	0	vigneshirtt	model1
3051	3051	intents		[]		0	8	0	arjitupadhyay	intents
3052	3052	List of films, series and cartoons with gidonline	Information about 14567 films cartoons and TV series from the gidonline website	['arts and entertainment', 'movies and tv shows', 'business', 'exploratory data analysis', 'comics and animation']	"Context
It was interesting to analyze the data on movies TV series and cartoons
Content
Data obtained from the website https://gidonline.io using a python script. 
- name (The name of the movie, cartoon or TV series)
- year (Year of release)
- score  (Rating set by users (rating from 0 to 10))
- cusers (The number of those who voted)
- rdv (Rating divided by the number of those who voted) [score/rdv]"	4	71	4	pavelbiz	list-of-films-series-and-cartoons-with-gidonline
3053	3053	inputdictionary		[]		0	12	0	v3dsingh	inputdictionary
3054	3054	labelled-sample_data-2		['arts and entertainment']		0	6	0	kiruthigaa	labelledsample-data2
3055	3055	cycleGAN_improve_pth		[]		0	6	0	huangkailong	cyclegan-improve-pth
3056	3056	Roberta_clean_mul		[]		0	14	0	hangy132	roberta-clean-mul
3057	3057	Distilbert_Huggingface_TF_PT_models	Huggingface Distilbert models for Pytorch and Tensorflow	['text data', 'transformers']		0	10	1	bajajra	distilbert-huggingface-tf-pt
3058	3058	India Stock Market (daily updated)	Daily Updated Data on ALL Stocks Listed in the NSE	['business', 'finance', 'time series analysis', 'investing', 'currencies and foreign exchange']	"About this dataset
&gt; India's National Stock Exchange (NSE) has a total market capitalization of more than US$3.4 trillion, making it the world's 10th-largest stock exchange as of August 2021, with a trading volume of ₹8,998,811 crore (US$1.2 trillion) and more 2000 total listings.
&gt; NSE's flagship index, the NIFTY 50, is a 50 stock index is used extensively by investors in India and around the world as a barometer of the Indian capital market.
&gt; This dataset contains data of all company stocks listed in the NSE, allowing anyone to analyze and make educated choices about their investments, while also contributing to their countries economy.
How to use this dataset
&gt; - Create a time series regression model to predict NIFTY-50 value and/or stock prices.
- Explore the most the returns, components and volatility of the stocks.
- Identify high and low performance stocks among the list.
Highlighted Notebooks
&gt; - Your kernel can be featured here!
- Related Dataset: S&P 500 Stocks - daily updated
- More datasets
Acknowledgements
&gt; ### License
CC0: Public Domain
&gt; ### Splash banner
Stonks by unknown memer."	71	479	10	andrewmvd	india-stock-market
3059	3059	gta_2205_data		[]		1	5	0	cotoga	gta-2205-data
3060	3060	nfnets_tf	NFNet implementations in tensorflow 2.x	['computer science', 'computer vision', 'image data', 'gpu', 'keras', 'tensorflow']	"NFNet in tensorflow 2.x
https://github.com/hoangthang1607/nfnets-Tensorflow-2
pretrain models
models"	3	340	3	kozistr	nfnets-tf
3061	3061	kitti_web_dataset_v2		[]		5	15	0	lackadacka	kitti-web-dataset-v2
3062	3062	Surrenal image dataset		[]		0	10	0	turkertuncer	surrenal-image-dataset
3063	3063	labelled_sample_data		[]		0	6	0	kiruthigaa	labelled-sample-data
3064	3064	unet_mark_3		[]		2	24	0	kevalpandya2232	unet-mark-3
3065	3065	NER_raw_data		[]		0	14	0	kiruthigaa	ner-raw-data
3066	3066	Deforestation Cover India- 2000- 2020	Forest Cover Loss Analysis for twenty years.	['india', 'economics', 'forestry', 'data visualization', 'data analytics']	"This data set includes tree cover extent, aboveground live biomass stocks and densities, annual tree cover loss, annual forest GHG emissions, and average annual forest CO2 removals (sequestration) and annual net GHG flux at the country and first (state, province) sub-national levels. Tree cover loss and emissions are available as annual data for 2001-2020. Emissions, removals and net flux are available as annual averages for 2001-2020. Tree cover is available for 2000 and 2010. Aboveground biomass stocks and densities are available for 2000. The tree cover data was produced by the University of Maryland's GLAD laboratory in partnership with Google. Carbon densities, emissions, removals, and net flux (megagrams CO2e/yr) are from Harris et al. 2021. The emissions data quantifies the amount of carbon dioxide emissions to the atmosphere where forest disturbances have occurred, and includes CO2, CH4, and N2O and multiple carbon pools. (This replaces the emissions data previously on GFW.) Removals includes the average annual carbon captured by aboveground and belowground woody biomass in forests. Net flux is the difference between average annual emissions and average annual removals; negative values are net sinks and positive values are net sources. All values besides emissions, removals, and net flux are presented for percent canopy cover levels &gt;=10%, 15%, 20%, 25%, 30%, 50% and 75%, while emissions, removals, and net flux are presented only for canopy &gt;=30%, 50%, and 75% and areas with tree cover gain. We recommend that you select your desired percent canopy cover level and use it consistently throughout any analysis. The Global Forest Watch website uses a &gt;=30% canopy cover threshold as a default for all statistics.
Citations 
Hansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, S. V. Stehman, S. J. Goetz, T. R. Loveland, A. Kommareddy, A. Egorov, L. Chini, C. O. Justice, and J. R. G. Townshend. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (15 November): 850–53. Data available on-line from: http://earthenginepartners.appspot.com/science-2013-global-forest.   
Harris, N.L., Gibbs, D.A., Baccini, A.&nbsp;et al.&nbsp;Global maps of twenty-first century forest carbon fluxes.&nbsp;Nat. Clim. Chang.&nbsp;(2021). https://doi.org/10.1038/s41558-020-00976-6
Global Administrative Areas Database, version 3.6. Available at http://gadm.org/  
For further questions regarding this data set, please contact Mikaela Weisse at the World Resources Institute (mikaela.weisse@wri.org)."	25	232	7	ankanhore545	deforestation-cover-india-2000-2020
3067	3067	bigbird		[]		0	11	1	tensorchoko	bigbird
3068	3068	Students marksheet dataset		['earth and nature']		5	20	1	rohithmahadevan	students-marksheet-dataset
3069	3069	scaler10		[]		0	4	0	valentinzastrozhny	scaler10
3070	3070	model10		[]		0	8	0	valentinzastrozhny	model10
3071	3071	background		[]		0	7	0	kevinsunpark	background
3072	3072	COVID-19 in the Netherlands	Dataset on Covid cases in NL - based on RIVM information	['public health', 'health']	"This file contains the following characteristics per case tested positive in the Netherlands: Date for statistics, Age group, Sex, Hospital admission, Death, Week of death, Province, Notifying GGD
The file is structured as follows: A record for every laboratory confirmed COVID-19 patient in the Netherlands, since the first COVID-19 report in the Netherlands on 27/02/2020 (Date for statistics may be earlier). The file is refreshed daily at 4 pm, based on the data as registered at 10 am that day in the national system for notifiable infectious diseases (Osiris AIZ).
Description of the variables
Date_file: Date and time when the data was published by RIVM
Date_statistics: Date for statistics; first day of illness, if unknown, date lab positive, if unknown, report date to GGD (format: yyyy-mm-dd)
Date_statistics_type: Type of date that was available for date for the variable ""Date for statistics"", where: DOO = Date of disease onset : First day of illness as reported by GGD. Please note: it is not always known whether this first day of illness was really Covid-19. DPL = Date of first Positive Labresult : Date of the (first) positive lab result. DON = Date of Notification : Date on which the notification was received by the GGD.
Agegroup: Age group at life; 0-9, 10-19, ..., 90+; at death &lt;50, 50-59, 60-69, 70-79, 80-89, 90+, Unknown = Unknown
Sex: Sex; Unknown = Unknown, Male = Male, Female = Female
Province: Name of the province (based on the patient's whereabouts)
Hospital_admission: Hospital admission reported by the GGD. Unknown = Unknown, Yes = Yes, No = No From May 1, 2020, the indication of hospitalization will be related to Covid-19. If not, the value of this column is ""No"". Until June 1, only seriously ill people were tested, a large part of these people had already been or were admitted shortly afterwards. As a result, the hospital admissions registered by the GGD were more complete during the first wave. As of June 1, everyone can be tested and more people will be tested at an early stage. As a result, the GGD is not always informed, or with a delay, of a hospital admission. That is why RIVM has been actively naming the registered hospital admissions of the NICE Foundation (https://data.rivm.nl/geonetwork/srv/dut/catalog.search#/metadata/4f4ad069-8f24-4fe8-b2a7-533ef27a899f) since 6 October. RIVM uses these figures as a guideline because they provide a more complete picture than the hospital admissions reported by the GGD. Click here (https://www.rivm.nl/nieuws/nummer-nieuw-melde-covid-19-verzekeraars-stable) for more information about this.
Deceased: Death. Unknown = Unknown, Yes = Yes, No = No
Week of Death: Week of death. YYYYMM according to ISO week notation (start from Monday to Sunday)
Municipal_health_service: GGD that made the report."	151	1683	21	konradb	covid19-in-the-netherlands
3073	3073	breast cancer		['cancer']		1	16	0	bimbimsimonstock	breast-cancer
3074	3074	Kaggle Competitions Top 100	Monthly Updated Kaggle Competitions Top 100	['computer science']	"Context
This dataset contains top 100 of Kaggle competitions ranking.
The dataset will be updated every month.
Content
100 rows and 13 columns.
Columns' description are listed below.
User : Name of the user
Tier : Grandmaster, Master or Expert
Company/School : Company/School info of the user if mentioned
Country : Country info of the user if mentioned
Competitions_Num : Number of competitions joined
Competitions_Gold : Number of competitions gold medals won
Competitions_Silver : Number of competitions silver medals won
Competitions_Bronze : Number of competitions bronze medals won
Datasets_Num : Number of public datasets
Notebooks_Num : Number of public notebooks
Discussions_Num : Number of topics/comments posted
Points : Total points
Profile : Link of Kaggle profile
Acknowledgements
Data from Kaggle.
Image from Smartcat.
If you're reading this, please upvote."	19	433	7	vivovinco	kaggle-competitions-top-100
3075	3075	skin-cancer-malignant-vs-benign		['cancer']		3	47	1	ehabibrahim758	skincancermalignantvsbenign
3076	3076	Processed_LJSpeech		[]		0	10	0	skoushik	processed-ljspeech
3077	3077	Turnover Team Data	Example data set used for HR analytics	['business', 'data analytics', 'tabular data', 'regression']	"Content
Example data set for making some analyses with HR data
Data set is licensed by Kogan Page Ltd.
Only for educational purposes, as it's an example data set
Data is cleaned.
Inspiration
The data set has been uploaded mainly to analyse team-level turnover by country and predicting team turnover
The data variables look like this:
1 Team identifier (unique team number).
2 Team size (number of people in the team).
3 Team turnover 2014 (separation rate between 0 and 1).
4 Country (1 = UK; 2 = United States; 3 = CANADA; 4 = SPAIN).
5 SURVEY: ENGAGEMENT items COMBO (composite engagement percentage across the team).
6 SURVEY: TeamLeader Rating (composite team leader percentage across the team).
7 SURVEY: CSR rating (composite corporate social responsibility percentage across the team).
8 SURVEY: Drive for Performance (composite percentage team score on perceived ‘Drive for performance’ percentage).
9 SURVEY: Performance, Development and Reward (composite percentage team score on perceived fairness of performance, development and reward across the team).
10 SURVEY: Work–Life Balance (composite percentage team score on perceived work–life balance across the team).
11 UK dummy variable (0 = not UK; 1 = UK).
12 USA dummy variable (0 = not United States; 1 = United States).
13 Canada dummy variable (0 = not Canada; 1 = Canada).
14 Spain dummy variable (0 = not Spain; 1 = Spain)."	20	138	9	helddata	turnover-team-data
3078	3078	image2		[]		0	2	0	pradwd	image2
3079	3079	GDPdata3		[]		0	3	0	usaver	gdpdata3
3080	3080	Hibbs Election Data		['politics']		1	7	0	carloscinelli	hibbs-election-data
3081	3081	ridge_dataset		[]		1	8	0	alexander1980	ridge-dataset
3082	3082	Disaster		[]		0	10	0	aniketgupta1001	disaster
3083	3083	Top 100 Cryptocurrency 2022	Ranking the Top Cryptocurrency	['finance', 'economics', 'computer science', 'internet', 'currencies and foreign exchange']	"Context
The crypto currency market continues to pique the curiosity of individuals all over the world month after month. The number of crypto currencies in the globe increased from 2,817 to 7,557 between November 2019 and November 2021.More than a 250 percent increase. There are 16,304 crypto currencies in circulation today. The 541 exchanges, Furthermore, the total trading volume of Crypto currencies in the last 24 hours was $81,976,605,427.But, in 2022, what are the best Crypto currencies to invest in?
Content
The dataset contains ten columns:
•   Ranking
•   Crypto Name
•   Price
•   Changes 24H
•   Changes 7D
•   Changes 30D
•   Changes 1Y
•   Market Cap
•   Volume 24H
•   Available Supply
Acknowledgements
Users are allowed to use, copy, distribute and cite the dataset as follows: “Majyhain, Top 100 Crypto Currency 2022 Kaggle Dataset, January 31, 2022.”
Inspiration
The ideas for this data is to:
Know which crypto to invest in?
To calculate the increase or decrease in the crypto currencies price by daily, monthly, weekly, monthly and yearly stats?
What is the market cap and volume traded daily and the available supply? 
References:
The Data is collected from the following sites:
https://coinmarketcap.com/"	575	3459	36	majyhain	top-100-cryptocurrency-2022
3084	3084	starfish		['earth science']		3	7	0	mirmahathirmohammad	starfish
3085	3085	fair_deit	DeiT: Data-efficient Image Transformers	['arts and entertainment', 'computer vision', 'classification', 'deep learning', 'image data']	"DeiT: Data-efficient Image Transformers
This repository contains PyTorch evaluation code, training code, and pre-trained models for DeiT (Data-Efficient Image Transformers).
For details see Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles and Hervé Jégou.
If you use this code for a paper please cite:
@article{touvron2020deit,
  title={Training data-efficient image transformers &amp; distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\'e J\'egou},
  journal={arXiv preprint arXiv:2012.12877},
  year={2020}
}"	7	763	5	kozistr	fair-deit
3086	3086	Flight Price		[]		3	16	4	chiragseth	flight-price
3087	3087	kratom		[]		0	11	0	nwpwstp	kratom
3088	3088	pricetag_for_yolov4		[]		0	9	0	octoded	pricetag-for-yolov4
3089	3089	cybertweets		[]		0	9	0	prakash99	cybertweets
3090	3090	jackofall		[]		0	16	0	chenghonghu	jackofall
3091	3091	bigbird_roberta_large_model		[]		0	0	0	sskkaz	bigbird-roberta-large-model
3092	3092	longformer_large_4096		[]		0	2	0	sskkaz	longformer-large-4096
3093	3093	Ubiquant DNN Models		[]		0	8	0	ayuraj	ubiquant-dnn
3094	3094	ddbo1312		[]		0	5	0	movie112	ddbo1312
3095	3095	Crypto Forecasting Challenge Models	Simple models used in a submission to the 2022 challenge	[]		0	13	0	douglaskgaraujo	2020-crypto-forecasting-models
3096	3096	DatasetProj2 labels		[]		2	9	0	tanushrikumar	datasetproj2-labels
3097	3097	PNW mountains NAIP 1m resolution		['image data']	"Content
This dataset contains 30k images that are 1024 by 1024 rgb. They are 1m satellite images from two patches of Pacific Northwest mountain ranges
Acknowledgements
Data collected by USDA and I got it distributed through google earth engine."	1	19	0	noahbadoa	pnw-mountains-naip-1m-resolution
3098	3098	human_activity_recognition 	Human Activity Recognition Template 	['artificial intelligence', 'computer science', 'computer vision', 'deep learning', 'image data']	"The dataset includes sample images and the default model trained with more than 100+ activities available in the 'action_recognition_kinetics.txt'  and further can be used for improvement over the base model for ML use cases.
USAGE
python human_activity_reco_deque.py --model resnet-34_kinetics.onnx --classes action_recognition_kinetics.txt --input example_activities.mp4
python human_activity_reco_deque.py --model resnet-34_kinetics.onnx --classes action_recognition_kinetics.txt"	16	548	5	rupakroy	human-activity-recognition
3099	3099	librispeech alphabet		[]		0	6	0	tuannguyenvananh	librispeech-alphabet
3100	3100	Great-Barrier-Reef-Preprocessed		[]		0	11	0	mycmyc	greatbarrierreefpreprocessed
3101	3101	xlnet_clean_1_epoch		[]		0	4	0	hangy132	xlnet-clean-1-epoch
3102	3102	motion_heatmap_opencv	Motion Detection Heatmap template using opencv	['artificial intelligence', 'programming', 'computer vision', 'data visualization', 'image data']	"Motion Detection Heat Map template can be further upgraded for use cases. 
To run. simply type from terminal:  python motion_heatmap.py"	6	133	6	rupakroy	motion-heatmap-opencv
3103	3103	WIDS-AN		[]		0	4	0	neetas	wids-an
3104	3104	train_agu	wxqzqqqqqqqqqqqqqqqqqqqqqqqqqqqqqw	[]		2	12	0	yangjieleo	train-agu
3105	3105	Datasetproj2		[]		0	8	0	tanushrikumar	datasetproj2
3106	3106	unet_model_weights_2019		[]		0	3	0	shreyaspj	unet-model-weights-2019
3107	3107	data_cleaned_df1df2		[]		3	31	4	banbeipi	data-cleaned-df1df2
3108	3108	aila-data		[]		1	36	0	moore0403	ailadata
3109	3109	FilledDataset		[]		2	11	0	axzhang	filleddataset
3110	3110	paddy Leaf Disease		[]		2	28	0	iashiqul	paddy-leaf-disease
3111	3111	Predict Categories of items using NLP		['social science']		0	15	0	shivam1298	predict-categories-of-items-using-nlp
3112	3112	CryptoInfo		[]		0	4	0	harshiniaiyyer	cryptoinfo
3113	3113	COTS Masks		['retail and shopping']		27	155	5	alexandrecc	cots-masks
3114	3114	Spotify Skip Action Datasets		['music']		2	20	0	frasonfrancis	spotify-skip-action-datasets
3115	3115	Cyclist_Station		[]		0	1	0	syahwizadean	cyclist-station
3116	3116	RoastMe images	Roast me comments and images (NSFW)	[]		1	47	2	mithilsalunkhe	roastme-images
3117	3117	hotdog vs nothotdog classify		['arts and entertainment', 'image data', 'cooking and recipes', 'food', 'restaurants']	I got a bunch of images of hotdogs and not hotdogs in order to make an image classifier inspired by the show silicon valley.	0	10	0	madeelbadar	hotdog-vs-nothotdog-classify
3118	3118	OSM: Central region of Russia		[]		25	147	1	rus91adds	osm-central-region-of-russia
3119	3119	nobel prize data (data cleaning)	practice data cleaning with uncleaned web scraped data	['business', 'tabular data']	"very early in my data science journey, I scraped this data from the noble prize website and spent days cleaning it. This dataset helped me get familiarized with pandas, string manipulation, and many other parts of the data cleaning process and I hope it does the same for you! This data currently requires a lot of cleaning but that's specifically why I uploaded it.
Data was scraped from: https://www.nobelprize.org/prizes/lists/nobel-laureates-and-research-affiliations/"	1	18	0	madeelbadar	scraped-nobel-prize-data-data-cleaning-excersize
3120	3120	republic		[]		0	7	0	preethamtn	republic
3121	3121	Google Data Analtyics Capstone	Bellabeat_case-study	['education']		5	15	0	fredoreyes	google-data-analtyics-capstone
3122	3122	IC_DATA	CT DICOM scraping for &gt; 4000 datasets	[]		1	9	2	djjolly	ic-data
3123	3123	Waste Bin Detection Dataset (RISS2021)	RISS 2021 - Transit Bus Waste Bin Object Detection	[]		4	148	0	elirotondo	waste-bin-detection-dataset-riss-2021
3124	3124	Bacterial Morphology on Agar Plates		['biology']		5	13	1	david1opez	bacterial-morphology-on-agar-plates
3125	3125	republic.txt		[]		1	3	0	nischithajayaraj	republictxt
3126	3126	wine_data		[]		0	1	0	jul2018	wine-data
3127	3127	Covid-19 and Hospitals US County Time Series 	County level cases, deaths and hospital info with state level testing data	['united states', 'education', 'time series analysis', 'hospitals and treatment centers', 'covid19', 'datetime']	"Context
This data was collected and created for a project in a data science course I took in college in the Spring of 2020.
I have updated the data to include more dates into the summer and decided to share it and the code so others can explore it.
Content
Data
Hospitals.csv
Available here: https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals
Information on hospitals in the United States.
us-counties.csv
Available here: https://github.com/nytimes/covid-19-data
Daily covid cases and death data for us counties.
co-est2019-alldata.csv
Available here: https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/
Data sheet available here: https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2010-2019/co-est2019-alldata.pdf
2019 county level census estimates.
daily.csv
Available here: https://covidtracking.com/api/v1/states/daily.csv
Daily state level covid testing data.
Uploaded with Git LFS
CountyHospitalCombined.csv, CovCountyHospitalTimeSeries.csv, and StateTestingTimeSeries.csv
Intereim data views created by me to hold cleaned data and used to create the final datset.
MasterTimeSeries.csv
Final combined dataset, a days X 3142(num of us counties+dc) long time series with variables stored as a proportion of population.
Uploaded with Git LFS
Code
The python scripts have comments to explain which datasets they're responsible for generating.
Feel free to use and edit them to tailor the datasets generated to your liking.
There is also a helper function library in the main directory.
Scripts can be ran by calling \&gt;python"	84	1400	6	jmzahn	covid19-and-hospitals-us-county-time-series
3128	3128	Uruguayan Media Historical Tweets	Spanish tweets from some Uruguayan media since each media account creation.	['news']	"Context
In the context of my thesis, we had to scrape Twitter's account of Uruguayan media to create a medias analysis platform and we dicided to publicate this dataset to help every person that may need it.
Content
The file contains tweets scrapped from Twitter from six different Uruguayan media (El País, Brecha, Búsqueda, El Observador, La República and La Diaria) since the creation of each account until approximately october 2021."	7	137	5	leadominguez	uruguayan-media-historical-tweets
3129	3129	Waste Bin Multi-Class Detection Dataset (RISS2021)	RISS 2021 - Transit Bus Waste Bin Multi-Class Object Detection	['transportation']		6	146	0	elirotondo	waste-bin-multiclass-detection-dataset-riss2021
3130	3130	Darth Vader image	Darth Vader helmet image for Word Cloud notebook	['arts and entertainment']		0	11	1	asimislam	darth-vader-image
3131	3131	Dados do Exame Nacional do Ensino Médio (Enem)	Arquivos em parquet para facilitar o carregamento	[]		11	41	4	caneiro	ml-olympiad-quality-education
3132	3132	No_clean_multi_data_xlnet		[]		0	10	0	hangy132	no-clean-multi-data-xlnet
3133	3133	Constitution of the United States	Preamble, Articles, Amendments	['government', 'law']		5	96	4	asimislam	constitution-of-the-united-states
3134	3134	SteamReviewOther		[]		4	24	0	zeeenb	steamreviewother
3135	3135	reef-yolox-s-fold0-v3		[]		0	1	0	sangayb	reef-yolox-s-fold0-v3
3136	3136	IEGM 2020		[]		0	23	0	marcosfs2006	iegm-2020
3137	3137	ext_rbt_crdf		[]		0	1	1	datafan07	ext-rbt-crdf
3138	3138	Letherboxd Scraping (Doing)	Scraping movies from Letherboxd 300.000 movies	['movies and tv shows', 'intermediate', 'tabular data', 'pandas']	"Letherboxd Movies Scraping
Well this dataset is about Scraping movies from Letherboxd. The principal goal is scraping data to community. 
Steps:
-First of all, i've scraped movies links by category. The path ""Filmes por Categoria"", have movies links by category;
-After i've make a csv sheet with difference between Letherboxd website amount of movies and mine scraping amount, to estipulate data loss scraping. The archive with estipulate data loss scraping by genres is ""letherboxd percas"";
-So i've make a data treatment with python and csv movies links categories scraped, because most movies have two or more genres. So some movies are duplicates. The archive with no movies duplicates and with join of all movies genres is ""Log_Filmes_Tratados""
-Ok, now we have a new archive with join of all movies genres without duplicates, so i've started scraping movie by movie.
Observations:
-The data loss scraping its high bacause letherboxd uses javascript to show informations, so a simple requests not workings, instead of a simple request we need a instance (I use Selenium) to run javascript rendering, although is extremely slow to scrape. So the scraping takes long hours to finish, and in this time many errors occurs, and some links not was scraped.
-Data Scraped and data loss scraping estipulate need be report in same moments, because the data information is update every time. In this dataset, i calculated the data loss scraping many days after scraping. For me a data loss scraping estipulate acceptable is between -1% and -7%;
-The principal archive ""Arquivo filmes"" with scraping information movie by movie are being scraping, so is not the last version. We have 300 thousands movies links, i not scraped more than 3% of link movies, this process is slow.
Principal usages with this dataset, and my projects with this data:
-Machine learning to predict a good movie or aproximate rate (a idea and a notebook that a will upload);
-Machine learning to predict if a new movie from a franchise gonna be cool (a idea);
-Machine learning to predict movies that you probably will like in website or desktop exe (a project that i will make);
*Example: imagine a sunday and you would like watch a new movie, but you dont make idea watch movie you will would watch, so you get this tool to recommend a movie that you probably will like, based in your letherboxd list or in pre select movies in tool.
-Tinder (movinder) to classify movies, recommendations, and social media, like a letherboxd. (a idea, in this dataset we have movie link png, this probably would help);
Well guys any doubts just call me!!
Letherboxd website: https://letterboxd.com
(11) 949370306 |Whatsapp|
marcus.rodrigues4003@gmail.com |email|"	1	67	2	markfinn1	letherboxd-scrapingdoing
3139	3139	reef-submission-model		[]		0	18	0	sentimentron	reef-submission-model
3140	3140	Job Interview Assignments test	This dataset contains collection of dataset asked in different job interviews	['employment', 'business', 'intermediate', 'tabular data', 'e-commerce services']	"Task 1
Business roles at AgroStar require a baseline of analytical skills, and it is also critical that we are able to explain complex concepts in a simple way to a variety of audiences. This test is structured so that someone with the baseline skills needed to succeed in the role should be able to complete this in under 4 hours without assistance.
Use the data in the included sheet to address the following scenario...
Since its inception, AgroStar has been leveraging an assisted marketplace model. Given that the market potential is huge and that the target customer appreciates a physical store nearby, we have taken a call to explore the offline retail model to drive growth. The primary objective is to get a larger wallet share for AgroStar among existing customers.
Assume you are back in time, in August 2018 and you have been asked to determine the location (taluka) of the first AgroStar offline retail store.
1. What are the key factors you would use to determine the location? Why?
2. What taluka (across three states) would you look open in? Why?
Guidelines:
-- (1) Please mention any assumptions you have made and the underlying thought process
-- (2) Please treat the assignment as standalone (it should be self-explanatory to someone who reads it), but we will have a follow-up discussion with you in which we will walk through your approach to this assignment.
-- (3) Mention any data that may be missing that would make this study more meaningful
-- (4) Kindly conduct your analysis within the spreadsheet, we would like to see the working sheet. If you face any issues due to the file size, kindly download this file and share an excel sheet with us
-- (5) If you would like to append a word document/presentation to summarize, please go ahead.
-- (6) In case you use any external data source/article, kindly share the source.
Task 4 Cohort
The file CDNOW_master.txt contains the entire purchase history up to the end of June 1998 of the cohort of 23,570 individuals who made their first-ever purchase at CDNOW in the first quarter of 1997. This CDNOW dataset was first used by Fader and Hardie (2001).
Each record in this file, 69,659 in total, comprises four fields: the customer's ID, the date of the transaction, the number of CDs purchased, and the dollar value of the transaction.
CustID = CDNOW_master(:,1); % customer id
Date = CDNOW_master(:,2); % transaction date
Quant = CDNOW_master(:,3); % number of CDs purchased
Spend = CDNOW_master(:,4); % dollar value (excl. S&H)
See ""Notes on the CDNOW Master Data Set"" (http://brucehardie.com/notes/026/) for details of how the 1/10th systematic sample (http://brucehardie.com/datasets/CDNOW_sample.zip) used in many papers was created. 
Reference:
Fader, Peter S. and Bruce G.,S. Hardie, (2001), ""Forecasting Repeat Sales at CDNOW: A Case Study,"" Interfaces, 31 (May-June), Part 2 of 2, S94-S107."	19	209	5	yekahaaagayeham	job-interview-assignments-test
3141	3141	Bad Bunny Genius Lyrics	All of Bad Bunny lyrics from the Genius API as of January 30, 2022	['music', 'text data']		0	25	1	lvlvlv210	bad-bunny-lyrics-013022
3142	3142	gtp3_embeddings_ada	"GPT3 embeddings for ""Toxic Severity Rating"" challgne by Jigsaw"	['computer science', 'nlp', 'text data']	"GPT-3 embeddings (Ada) for ""Jigsaw Toxic Severity Rating"" challenge
GPT-3 text embeddings can be generated by a set of GPT-3 models: https://beta.openai.com/docs/guides/embeddings/what-are-embeddings
Note that different engines produce embeddings of different sizes:
Ada (1024 dimensions),
Babbage (2048 dimensions),
Curie (4096 dimensions),
Davinci (12288 dimensions).
This notebook generates GPT-3 embeddings for all comments from https://www.kaggle.com/c/jigsaw-toxic-severity-rating challenge.
See the usage example below and don't forget to like this notebook if you find it interesting!"	10	277	10	vslaykovsky	gtp3-embeddings-ada
3143	3143	yhcn21-realc		[]		0	9	0	neverstoppredicting	yhcn21realc
3144	3144	yhrn21		[]		0	6	0	neverstoppredicting	yhrn21
3145	3145	full-ensemble		[]		6	19	0	chasembowers	fullensemble
3146	3146	SHIT SHIT SHIT		[]		0	4	0	malk1510	shit-shit-shit
3147	3147	DNN Model		[]		0	4	0	ayuraj	dnn-model
3148	3148	LSTM_Attention_Model_Inputs		[]		0	4	1	narminj	lstm-attention-model-inputs
3149	3149	mymodel2		[]		0	74	0	mohammadhosseinash	mymodel2
3150	3150	efficientdet_d2_coco17_tpu-32		[]	Source: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md	0	5	0	atamazian	efficientdet-d2-coco17-tpu32
3151	3151	ComprehensiveData		[]		1	16	0	suprarai	comprehensivedata
3152	3152	myemoji		[]		2	9	0	mldfshr	myemoji
3153	3153	comprehensiveDataAnalysis		[]		2	8	0	suprarai	comprehensivedataanalysis
3154	3154	output70		[]		0	8	0	zhihao95	output70
3155	3155	sales analysis	EDA on sales analysis	[]		2	17	0	neerajgwalani	sales-analysis
3156	3156	ImageNet		[]		0	18	0	rahultandial	imagenet
3157	3157	Feedback Prize Pytorch roberta-large ITPT		['games']		1	18	3	julian3833	feedback-prize-pytorch-robertalarge-itpt
3158	3158	Feedback Prize Roberta Weights	ShortFormer RoBERTa weights	['exercise', 'nlp', 'pytorch']	"Training notebook: 📖 PyTorch- ""ShortFormer"" w/Chunks - Train [0.604]
Infer notebook: 📖 PyTorch- ""ShortFormer"" w/Chunks - Infer [0.604]"	18	395	7	julian3833	feedback-prize-roberta-weights
3159	3159	res18-150epoch-30012022		[]		0	8	0	dmitriyisaev	res18150epoch30012022
3160	3160	model_with_yearid		[]		0	6	0	dianhuanlin	model-with-yearid
3161	3161	Cholera Outbreak, Stockholm, 1853	Death records from S:t Catherine parish.	['europe', 'diseases', 'history', 'data visualization', 'text data']	"Context
This is part of research work of the history of Stockholm in the 19th century. 
Since Sweden has well preserved church archives this permits getting detailed information about e.g. pandemic events. 
In this data collection I will start gathering information about the large cholera outbreak in late August 1853. 
About 3% of the population died within a couple of weeks. Poor parts of the city, such as S:t Catherine parish were hit very hard. 
This is a page from the church records in the worst period in September: https://sok.riksarkivet.se/bildvisning/C0055812_00226
For more information see: https://en.wikipedia.org/wiki/1853_Stockholm_cholera_outbreak 
Content
The data is collected from hand-written church records. The church was responsible for all population registration until around 1870. Only death records have been analyzed. Since Cholera has around 40% CFR we can assume a larger number of affected but surviving individuals. 
Data Sources
All data can be found at the National Archive of Sweden. See https://sok.riksarkivet.se/digitala-forskarsalen (press ""Other languages"" for an English version).
Another important source for further analysis is ""Sundhets-collegii underdåniga berättele om Kolerafarsoten i Sverge, 1853"" which is a broad overview of the epidemic situation in all of Sweden in 1853. This contemporary report can be found at Statistics Sweden (SCB) : See this link. (Page 137 describes the situation in S:t Catherine parish).
Inspiration
I hope the data (and upcoming code/notebooks) can illustrate the speed of a cholera pandemic in the 19th century in an urban environment with very rudimentary sanitation, and related traumatic effects on society and individuals. 
Currently the dataset contains a  ""profession"" column which is just raw transcriptions from the church records. Possibly we could make an attempt to interpret this information into societal status. There is also the possibility to continue with spatial analys of the spread of the disease based on parish locations (and in some cases more detailed geographical information)."	204	1785	25	augustlinnman	cholera-outbreak-stockholm-1853
3162	3162	Latest Popular Movies Dataset	Compare Latest Movies Rating	['arts and entertainment', 'movies and tv shows', 'computer science']	"Context
There's no story behind this.
Content
This was my first time creating a dataset using API.
Acknowledgements
Yeah, I learned something.
Inspiration
To sleep."	31	238	6	hamzansariii	latest-popular-movies-dataset
3163	3163	output70		[]		0	12	0	kaiyanmaster	output70
3164	3164	Miami Housing Dataset	Predict the prices of Miami Houses.	['real estate', 'beginner', 'tabular data', 'regression']	"Content
The dataset contains information on 13,932 single-family homes sold in Miami .
Content
The dataset contains the following columns:
PARCELNO: unique identifier for each property. About 1% appear multiple times.
SALE_PRC: sale price ($)
LND_SQFOOT: land area (square feet)
TOT_LVG_AREA: floor area (square feet)
SPEC_FEAT_VAL: value of special features (e.g., swimming pools) ($)
RAIL_DIST: distance to the nearest rail line (an indicator of noise) (feet)
OCEAN_DIST: distance to the ocean (feet)
WATER_DIST: distance to the nearest body of water (feet)
CNTR_DIST: distance to the Miami central business district (feet)
SUBCNTR_DI: distance to the nearest subcenter (feet)
HWY_DIST: distance to the nearest highway (an indicator of noise) (feet)
age: age of the structure
avno60plus: dummy variable for airplane noise exceeding an acceptable level
structure_quality: quality of the structure
month_sold: sale month in 2016 (1 = jan)
LATITUDE
LONGITUDE"	498	4011	36	deepcontractor	miami-housing-dataset
3165	3165	Squid Game Netflix Twitter Data	This data set contains twitter dump for the hashtag #squidgame. 	['arts and entertainment', 'beginner', 'exploratory data analysis', 'text data', 'online communities']	"The dataset contains the recent tweets about the record-breaking Netflix show ""Squid Game""
The data is collected using tweepy Python package to access Twitter API."	1289	15153	61	deepcontractor	squid-game-netflix-twitter-data
3166	3166	Marvel Comic Books Dataset	A complete dataset of all existing Marvel Comics.	['arts and entertainment', 'beginner', 'recommender systems', 'tabular data', 'comics and animation']	"Context
The dataset contains info on all the comic books ever released in the Marvel Universe.
Content
Following are the columns:
Comic name: Name of the comic series.
Active years: Years between which the comic series was active.
Issue title: Tv shows have episodes, comic books have issues. For eg, Spiderman comic series has xyz number of issues.
Publish date: Date of publish for the comic issue.
Issue_description: Description of the issue.
Penciler: Penciper of the comic issue.
Writer: Original writer.
Cover artist: Person responsible for cover art of the comic.
Imprint: An imprint of a publisher is a trade name under which it publishes a work
Format: Comic format.
Rating: Age Rating.
Price: Price of the comic.
Data Source
A-Z Marvel Comic Series"	474	5091	51	deepcontractor	marvel-comic-books
3167	3167	Australian Fatal Road Accident 1989-2021	A Countrywide Traffic Accident Dataset (1989-2021)	['law', 'transportation', 'beginner', 'geospatial analysis', 'tabular data']	"Context
The Australian Road Deaths Database provides basic details of road transport crash fatalities in Australia as reported by the police each month to the State and Territory road safety authorities. Road deaths from recent months are preliminary and the series is subject to revision.
Content
The data is collected from the year 1989 to 2021, Click here to lear  more about the dataset.
Acknowledgements
Bureau of Infrastructure and Transport Research Economics: The Bureau of Infrastructure and Transport Research Economics (BITRE) provides economic analysis, research and statistics on infrastructure, transport and cities issues to inform both Australian Government policy development and wider community understanding. click here
Data Removal
Please contact me if any part of the data is to be removed."	689	4983	36	deepcontractor	australian-fatal-car-accident-data-19892021
3168	3168	Unir-lab-2		[]		0	17	0	leonardoparra	unirlab2
3169	3169	Top Video Games 1995-2021 Metacritic	Details about the top rated video games 1995-2021	['video games', 'history', 'software', 'tabular data', 'ratings and reviews']	"Content
This dataset contains a list of video games dating from 1995 to 2021, it also provides things such as release dates, user review rating, and critic review rating.
Acknowledgements
The data is scraped from here.
Picture Credit
Unsplash."	2089	15467	88	deepcontractor	top-video-games-19952021-metacritic
3170	3170	goal_of_baoat	bildofgoalbildofgoal	[]		2	12	0	scalpah	goal-of-baoat
3171	3171	sentiment_nuclear_power_1_.csv		['internet']		0	13	0	evyoung	sentiment-nuclear-power-1-csv
3172	3172	Olympic Games, 1986-2021	Results, Medals, Athletes and Hosts from Athens to Tokyo	['sports', 'data visualization', 'tabular data']	"Context
More than 19,000 medals, 150,000 results, 74,000 athletes, 20,000 biographies, and 52 hosts of Olympic Games you can find here.
This is an Olympic Games dataset that describes results, medals, athletes, and hosts from the first games till 2018. The data was created from Olympics.
I strongly recommend looking into a notebook Starter Notebook before working with data. Some tricks and visualizations are presented here.
You can support the dataset via the upvote button!
Data: 
1. olympic_medals.csv - dataset includes a row for every Olympic Medal (Athlete or Team) that has won a medal.
2. olympic_results.csv - dataset includes a row for every Olympic Result (Athlete or Team) that has placed in the final.
3. olympic_hosts.csv - dataset includes a row for every Olympic Game Host.
4. olympic_athletes.csv - dataset includes a row for every Olympic Athlete that took part in the final.
5. olympic_results.pkl - the same as olympic_hosts.csv.
Disadvantages:
- There are no results for qualification rounds. For instance, event 100-m men contains only final results without semi-finals and other hits.
- There is no information about athletes for team competitions that consist of more than 2 participants. Only team record.
Related Datasets
Beijing 2022 Olympics
Tokyo 2020 Olympics
Tokyo 2020 Paralympics
Tokyo 2020 Horses
Olympic Games Hosts
Data Visualization
Olympics Exploratory Dashboard by Geoffrey Nel
Starter Notebook
Tokyo 2020 Paralympics Dataset Visualization
Dataset History
2021-10-22 - Tokyo-2020 data added.
2021-08-04 - all dataset files are uploaded.
2021-07-18 - dataset is created.
2022-20-01 - olympic_results.pkl is added.
Q&A
If you have any questions please start a discussion."	834	4706	14	piterfm	olympic-games-medals-19862018
3173	3173	TITANIC Easy data analysis(beginner)		['business']		1	10	0	vikas70111	titanic-easy-data-analysisbeginner
3174	3174	Banana Classification Dataset	Convolutional Neural Networks	['south america', 'agriculture', 'computer science', 'cnn', 'image data', 'food']	"Context
Una de cada tres bananas que se exportan en el mundo son de origen ecuatoriano.
Esto representa para el Ecuador un ingreso anual de aproximadamente $3.66 mil millones, y
una producción que oscila entre las 7 millones de toneladas de banana, las cuales son
clasificadas de forma manual entre aquellas calificadas para exportación y aquellas que no
(denominados “rechazo”). Ayude a los a clasificar bananos de exportación previo
a su empaque.
Content
Exportación: bananos aptos para la exportación
Rechazo: bananos NO aptos para la exportación
Inspiration
Es posible diferenciar entre un tipo de banano y otro? Se pueden implementar modelos de IA en el campo de la agricultura?"	6	56	0	andreruizcalle	banana-classification-dataset
3175	3175	Pothole 		['law']		1	10	0	mohdalelaiwi	pothole
3176	3176	nsk_image_search3_man7d		[]		0	1	0	motono0223	nsk-image-search3-man7d
3177	3177	torch-1.5.1_torchvision-0.6.1_cu92		[]		0	3	0	hngbiquc	torch151-torchvision061-cu92
3178	3178	Insurance Claims Data		['finance', 'statistical analysis', 'data analytics', 'insurance']	"Autobi(Automobile Bodily Injury Claims) -
The data contains information on demographic information about the claimant, attorney involvement and the economic loss (LOSS, in thousands), among other variables.The full data contains over 70,000 closed claims based on data from thirty-two insurers. 
A data frame with 1340 observations on the following 8 variables.
CASENUM- Case number to identify the claim, a numeric vector
ATTORNEY- Whether the claimant is represented by an attorney (=1 if yes and =2 if no), a numeric vector
CLMSEX - Claimant's gender (=1 if male and =2 if female), a numeric vector
MARITAL- claimant's marital status (=1 if married, =2 if single, =3 if widowed, and =4 if divorced/separated), a numeric vector
CLMINSUR- Whether or not the driver of the claimant's vehicle was uninsured (=1 if yes, =2 if no, and =3 if not applicable), a numeric vector
SEATBELT- Whether or not the claimant was wearing a seatbelt/child restraint (=1 if yes, =2 if no, and =3 if not applicable), a numeric vector
CLMAGE- Claimant's age, a numeric vector
LOSS- The claimant's total economic loss (in thousands), a numeric vector
AutoClaims(Automobile Insurance Claims) -
A data frame with 6773 observations on the following 5 variables.
STATE
CLASS - Rating class of operator, based on age, gender, marital status, use of vehicle
GENDER
AGE - Age of operator
PAID - Amount paid to settle and close a claim
AutoCollision(Automobile UK Collision Claims)
8,942 collision losses from private passenger United Kingdom (UK) automobile insurance policies. The average severity is in pounds sterling adjusted for inflation.
A data frame with 32 observations on the following 4 variables.
Age - Age of driver
Vehicle_Use - Purpose of the vehicle use
Severity - Average amount of claims
Claim_Count - Number of claims
Additional information can be found in the document: https://cran.r-project.org/web/packages/insuranceData/index.html"	11	26	2	saisatish09	insuranceclaimsdata
3179	3179	linux terminal code		[]		0	3	0	htesadeghi	linux-terminal-code
3180	3180	ddbo1311		[]		0	1	0	movie112	ddbo1311
3181	3181	ext_tx_rbrta_base		[]		0	3	1	datafan07	ext-tx-rbrta-base
3182	3182	World_Suicide_Rates_2000-2019	# of deaths per 100 000 population are influenced by the age distribution.	['mental health', 'social science', 'demographics', 'mortality', 'data visualization']	Suicides are preventable.  Suicide prevention efforts require coordination and collaboration among multiple sectors of society, including the health sector and other sectors such as education, labour, agriculture, business, justice, law, defence, politics, and the media. These efforts must be comprehensive and integrated as no single approach alone can make an impact on an issue as complex as suicide. Let us help each other in solving this worldwide problem and address the problem at the broadest stage prossible.	550	3041	25	ankanhore545	world-suicide-rates-20002019
3183	3183	Comments_score		[]		2	3	0	esgintn	comments-score
3184	3184	validation		[]		2	7	0	esgintn	validation
3185	3185	train_test		[]		2	2	0	esgintn	train-test
3186	3186	Klasifikasi jenis kanker		[]		0	4	0	rasaputra	klasifikasi-jenis-kanker
3187	3187	klasifikasi kanker payudara		[]		0	7	0	zulkarnainnurawwal	klasifikasi-kanker-payudara
3188	3188	Google Data Analytics Capstone: Case Study		['business', 'beginner', 'data analytics', 'r']		0	39	0	gmkthalapathy007	google-data-analytics-capstone-case-study
3189	3189	dimensionality_reduction		[]		3	19	0	bucheron	dimensionality-reduction
3190	3190	DCIC-CAPTCHA		[]		0	12	0	bigeyeahh	dciccaptcha
3191	3191	practcoco		[]		0	7	0	nishant10k	practcoco
3192	3192	klasifikasi kanker payudara		[]		4	21	0	muhammadridwan23	klasifikasi-kanker-payudara
3193	3193	Parkinson		[]		0	16	0	nabilahafrin	parkinson
3194	3194	klasifikasi-kanker-payudara		[]		0	9	0	muhammadridwan23	klasifikasikankerpayudara
3195	3195	toxicdata		[]		0	11	0	arpansharma11	toxicdata
3196	3196	datasmalltest1		[]		0	2	0	samaphonphromsamlee	datasmalltest1
3197	3197	SKIN_CANCER_DETECTION_USING_DEEP__LEARNING_MODELS		[]		0	4	0	anjalisharma123	skin-cancer-detection-using-deep-learning-models
3198	3198	MultiWOZ 2.2	A fully-labeled human written dialogues spanning over multi-domains and topics.	['business', 'nlp', 'text data']	"Source
From the original repo: An official GitHub for MultiWOZ datasets
Definition
Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.
Data structure
There are 3,406 single-domain dialogues that include booking if the domain allows for that and 7,032 multi-domain dialogues consisting of at least 2 up to 5 domains. To enforce reproducibility of results, the corpus was randomly split into a train, test and development set. The test and development sets contain 1k examples each. Even though all dialogues are coherent, some of them were not finished in terms of task description. Therefore, the validation and test sets only contain fully successful dialogues thus enabling a fair comparison of models. There are no dialogues from hospital and police domains in validation and testing sets.
Each dialogue consists of a goal, multiple user and system utterances as well as a belief state. Additionally, the task description in natural language presented to turkers working from the visitor’s side is added. Dialogues with MUL in the name refers to multi-domain dialogues. Dialogues with SNG refers to single-domain dialogues (but a booking sub-domain is possible). The booking might not have been possible to complete if fail_book option is not empty in goal specifications – turkers did not know about that.
The belief state have three sections: semi, book and booked. Semi refers to slots from a particular domain. Book refers to booking slots for a particular domain and booked is a sub-list of book dictionary with information about the booked entity (once the booking has been made). The goal sometimes was wrongly followed by the turkers which may results in the wrong belief state. The joint accuracy metrics includes ALL slots.
Benchmarks
Dialogue state tracking
Response generation"	1	25	0	taejinwoo	multiwoz-22
3199	3199	bert_singel_sentence_cls		[]		0	7	0	longhuqin	bert-singel-sentence-cls
3200	3200	SELF_TRIANED_MULTI_CLEAN		[]		0	3	0	hangy132	self-trianed-multi-clean
3201	3201	gun_detection_opencv2	Gun detection using opencv template which is can used for further improvement	['artificial intelligence', 'programming', 'computer vision', 'deep learning', 'image data']		6	398	5	rupakroy	gun-detection-opencv2
3202	3202	GDP666		[]		0	10	0	slg520net	gdp666
3203	3203	COTS_Ganv1		[]		12	165	6	marcinstasko	cots-ganv1
3204	3204	pip-no-internet		[]		0	4	0	yk4r22	pipnointernet
3205	3205	Car Driving Distance Range Dataset	Driving Range of over 500 cars segregated in petrol and diesel types.	['australia', 'automobiles and vehicles', 'data analytics', 'tabular data', 'regression']	"What’s in a tank of fuel?
The cars with the best (and worst) driving range in Australia is included in the dataset. 
Content
The data is divided into two files:
- PETROL.csv
- DIESEL.csv
Both the datasets contain the same type of columns and one can combine the two by just adding the is_petrol_diesel column.
Dataset Description is as follows:
- MAKE: car company
- MODEL: car model
- TYPE: car type
- CYL: number of cylinders
- ENGINE L: engine capacity in Litres
- FUEL TANK L: fuel tank capacity
- CONS. L/100km: fuel consumption per 100 km
 RANGE km: the distance range of the car
Acknowledgements
The data is been collected from drive.com.au. A detailed and nice article has been published on site which can help while analyzing the data."	4	27	0	adityamahimkar	car-driving-distance-range-dataset
3206	3206	Dreamhome		[]		0	1	0	sitihawarosle	dreamhome
3207	3207	carla-dataset	Annotated dataset collected in CARLA	['transportation', 'automobiles and vehicles', 'computer vision', 'image data', 'simulations']	"Context
10 classes
Good for traffic light detection by color
Good for traffic sign detection by speed
Cars,trucks,... have been simplified to ""vechicles""
Bikes, motobikes and persons
Acknowledgement
This dataseet was annotated and generated with the use of Roboflow."	1	19	0	alechantson	carladataset
3208	3208	faces train 6		[]		5	3	0	mdhamani	faces-train-6
3209	3209	faces train 4		[]		6	4	0	mdhamani	faces-train-4
3210	3210	faces valid		[]		6	3	0	mdhamani	faces-valid
3211	3211	Survey for Impact of AI on Account and finance 		[]		10	63	0	vishnupragash	survey-for-impact-of-ai-on-account-and-finance
3212	3212	Retail Beef Cuts Dataset 	7 classes of retail beef cuts	['business', 'food']		1	38	3	abdallahabuzaid	retail-beef-cuts-dataset
3213	3213	Taipei Housing Dataset UCI		['real estate', 'regression']	"Context
Use this dataset to polish up Regression techniques.
Content
The market historical data set of real estate valuation are collected from Sindian Dist., New Taipei City, Taiwan. 
The real estate valuation is a regression problem. 
The data set was randomly split into the training data set (2/3 samples) and the testing data set (1/3 samples).
Attribute Information:
The inputs are as follows
X1=the transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)
X2=the house age (unit: year)
X3=the distance to the nearest MRT station (unit: meter)
X4=the number of convenience stores in the living circle on foot (integer)
X5=the geographic coordinate, latitude. (unit: degree)
X6=the geographic coordinate, longitude. (unit: degree)
The output is as follows
Y= house price of unit area (10000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)
Acknowledgements
I got this dataset from UCI Machine Learning Repository.
Inspiration
What is the ideal metric to measure model performance on Regression problems like this. Is there an error value we aim for?"	0	11	0	hastingssibanda	taipei-housing-dataset-uci
3214	3214	Daily Weather in Airports, 1971-2020	Weather indicators recorded daily in the airports of Berlin, New York and Sydney	['united states', 'europe', 'australia', 'weather and climate', 'tabular data']	"Context
This dataset was collected as I was working on my final task for ""Applied Plotting, Charting & Data Representation in Python"" course by University of Michigan. In my assignment, I had to find data from Germany describing some weather phenomena, so I started with Berlin - but I got an idea to go a bit beyond, and took some more data just to have values from different continents. This is how New York and Sydney came into game.
Why airports? Because weather data is collected there extremely precisely, and can be trusted.
Why 50 years? Just because I wanted to get a wide range of values 😉 
Content
Data Source
The data was collected at NOAA .
On the search page, select: 
- ""Daily Summaries"" under ""Select Weather Observation Type/Dataset""; 
- ""01 January, 1971"" as start date and 31 December, 2020 under ""Select Date Range""; 
- ""Stations"" under ""Search For""; 
- the station code (GME00127930 for Berlin Schonefeld, USW00094789 for NY JFK, ASN00066037 for Sydney Kingsford Smith Airport) under ""Enter a Search Term"";
Proceed to the cart.
On the ""Cart: Daily Summaries"" page, select:
- ""Custom GHCN-Daily CSV"" under ""Select the Output Format"".
On the ""Custom Options: Daily Summaries"" page, select:
- ""Station Name"", ""Geographic Location"", ""Units: Standard"" under ""Station Detail & Data Flag Options"";
- ""Precipitation"" and ""Air Temperature"" under ""Select data types for custom output"".
Data Structure
The dataset contains 3 csv files, one for each city. The structure of the files is precisely the same (index + 10 columns), and they all are of the same length. 
Original column names are preserved. 
The only cleaning done was removing a few columns from the dataset, leaving only the most interesting ones: minimum and maximum temperature, precipitation and snow depth (note: presumably for climatic reasons, snow depth for Sydney is mostly missing).
While I have provided description for every column, I will keep here also the link to official documentation.
Acknowledgements
Dataset Source: NOAA Climate Data Online. As stated in FAQ:
&gt; Climate Data Online is a collection of climatic data that offers public access and consumption via discovery and ordering services.  The data available through CDO is available at no charge and can be viewed online or ordered and delivered to your email inbox. 
Big thanks for the cover photo by <a href=""https://unsplash.com/@kmages?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Ken Mages</a> on <a href=""https://unsplash.com/s/photos/city-weather?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>
Inspiration
Everything related to the weather and climate changes and trends."	7	133	3	olgaluzhetska	daily-weather-in-berlin-ny-and-sydney-19712020
3215	3215	train_infer		[]		0	8	0	jellyz9	train-infer
3216	3216	Hugging face | gpt2	Uploaded on 30/01/2022	[]		0	13	0	tdjogi010	hugging-face-gpt2
3217	3217	MUlTIPLE_SOURCE_CLEAN_RoBERTa		[]		0	5	0	hangy132	multiple-source-clean-roberta
3218	3218	Trendyol Smartphones	 Trendyol Smartphone Price Prediction 	[]	"Trendyol Smartphones Data
This dataset has 19 columns and 939 rows, these data were taken from Trendyol.com with Python-BeautifulSoup.
Click on the link to see the Python script we used to create the dataset and our project with the dataset. Link
The people who created this dataset: Ahmet Furkan DEMIR, Batuhan TURK
Detailed information about the dataset
One of the 19 columns in the dataset is the product titles, one is the product links, and the other is the prices of the products, so the regression column that we will use for the estimation, remaining 16 columns are the features of products, namely phones, by using these columns you can develop machine learning algorithms or perform various operations on the data.
Columns:
* Urun Başlığı : Title of products, it is used for verification only. |-&gt;No missing data&lt;-|
* Link : Ürünlerin Link, it is used for verification only (Unique for each product). |-&gt;No missing data&lt;-|
* Marka : Phone Manufacturer (Samsung, Apple etc.) |-&gt;No missing data&lt;-|
* İşletim Sistemi : Phones operating system (Android, IOS) |-&gt;No missing data&lt;-|
* Dahili Hafıza : Total ROM memory of the phone (X GB) |-&gt;No missing data&lt;-|
* RAM Kapasitesi : Total RAM capacity of the phone (Y GB) |-&gt;No missing data&lt;-|
* Görüntü Teknolojisi : Display Technology (LCD, AMOLED etc.) |-&gt;Yes missing data&lt;-|
* Ekran Çözünürlüğü : Screen resolution (HD+, FHD+ etc.) |-&gt;Yes missing data&lt;-|
* Kamera Çözünürlüğü : Camera Resolution (Z-X MP etc.) |-&gt;Yes missing data&lt;-|
* Batarya Kapasitesi Aralığı : Battery Capacity Range (4500-5000 mAh etc.) |-&gt;No missing data&lt;-|
* Mobil Bağlantı Hızı : Mobile Connection Speed (4G, 4.5G, 5G)  |-&gt;No missing data&lt;-|
* Ekran Boyutu : Screen size (6 inç&nbsp;ve üstü) |-&gt;Yes missing data&lt;-|
* Parmak İzi Okuyucu : Fingerprint reader (yes, no) |-&gt;Yes missing data&lt;-|
* Yüz Tanıma : Face recognition (yes, no) |-&gt;Yes missing data&lt;-|
* Suya/Toza Dayanıklılık : Water/Dust Resistance (yes, no) |-&gt;No missing data&lt;-|
* Çift Hat : Double Line (yes, no) |-&gt;No missing data&lt;-|
* Arttırılabilir Hafıza (Hafıza Kartı Desteği) : Expandable Memory (Memory Card Support) (yes, no) |-&gt;Yes missing data&lt;-|
* Ekran Yenileme Hızı : Screen Refresh Rate (120 Hz etc.) |-&gt;Yes missing data&lt;-|
* Fiyat : Price of phones (Z TL) |-&gt;No missing data&lt;-|
What you can do with the dataset
With dataset, you can perform a price estimation application using the attributes of the Phones.
You can visualize data with dataset.
You can clear or fill in the missing data in the dataset."	2	94	1	ahmetfurkandemr	trendyol-smartphones
3219	3219	cocosformovingtest		[]		0	2	0	bobma1215	cocosformovingtest
3220	3220	stockprice_yahoo		[]		0	0	0	zulfiqarshah	stockprice-yahoo
3221	3221	ofdmperfect		[]		0	4	0	vanirudhrajkaushik	ofdmperfect
3222	3222	TL xml		[]		0	6	0	blindwang	tl-xml
3223	3223	TL jpg		[]		0	10	0	blindwang	tl-jpg
3224	3224	classification dataset(Cancer prediction)		['cancer']		3	37	0	prashanthpacchi	classification-datasetcancer-prediction
3225	3225	data_drone		['computer vision', 'deep learning', 'image data', 'multiclass classification', 'optimization']		0	29	2	kyriakitychola	data-drone
3226	3226	.CSV Fiiling missing values - Song popularity		[]		1	21	0	dienhoa	csv-fiiling-missing-values-song-popularity
3227	3227	seoul_air		[]		0	6	0	raeeyu	seoul
3228	3228	Daddy Ben Cebuano		['arts and entertainment']		36	39	0	benzajtil	daddy-ben-cebuano
3229	3229	68PersonsBmp		[]		0	3	0	hinowirachapong	68personsbmp
3230	3230	feedbacks_models		[]		0	15	4	mathurinache	feedbacks-models
3231	3231	Seoul Bike Trip Duration Prediction	Predict the Bike Trip Duration while considering atmospheric factors	['beginner', 'intermediate', 'advanced', 'regression']	"Context
Trip duration is the most fundamental measure in all modes of transportation. Hence, it is crucial to predict the trip-time precisely for the advancement of Intelligent Transport Systems (ITS) and traveller information systems. In order to predict the trip duration, data mining techniques are employed in this paper to predict the trip duration of rental bikes in Seoul Bike sharing system. The prediction is carried out with the combination of Seoul Bike data and weather data. 
Content
The Data used include trip duration, trip distance, pickup-dropoff latitude and longitude, temperature, precipitation, wind speed, humidity, solar radiation, snowfall, ground temperature and 1-hour average dust concentration. 
Acknowledgements
V E, Sathishkumar (2020), “Seoul Bike Trip duration prediction”, Mendeley Data, V1, doi: 10.17632/gtfh9z865f.1
Inspiration
Predict the trip duration"	422	4250	27	saurabhshahane	seoul-bike-trip-duration-prediction
3232	3232	mlm roberta folds		[]		0	7	0	yuzhoudiyishuai	mlm-roberta-folds
3233	3233	autonlp toxic folds		[]		0	8	0	yuzhoudiyishuai	autonlp-toxic-folds
3234	3234	Crypto		[]		0	3	0	chinnarajc	crypto
3235	3235	Telecom churn data		['business']		15	29	0	chiranjeevbit	telecom-churn-data
3236	3236	paperdataset_test		[]		0	7	0	mengyangyang0001	paperdataset-test
3237	3237	BreakHis-Resampled		['business']		2	25	0	asmiyahasan	breakhisresampled
3238	3238	Dataset Rasio Bed to Population Faskes II	Dataset rasio untuk rumah sakit umum kelas C dan D se-Indonesia per tahun 2020 	['healthcare', 'categorical data', 'social science', 'tabular data']	"Selayang Pandang
Pelayanan kesehatan Indonesia mengenal sistem rujukan berjangka pasien yang didasarkan pada kebutuhan perawatan pasien dan disesuaikan dengan tingkat kemampuan melayani dari fasilitas kesehatan yang ada. Fasilitas kesehatan dibagi menjadi tiga:
- Faskes tingkat I, yaitu klinik atau Puskesmas. Umumnya menangani permasalahan klinis biasa yang cukup diatasi dengan berobat jalan. Dokter yang melayani umumnya adalah dokter umum.
- Faskes tingkat II, yaitu rumah sakit kelas D, C, dan B. Faskes ini menangani pasien yang membutuhkan pelayanan lebih spesifik yang fasilitasnya tidak tersedia di klinik atau Puskesmas. Dokter yang melayani umumnya sudah memiliki spesialisasi. Apabila pasien tidak bisa ditangani oleh rumah sakit kelas C dan D, pasien akan dirujuk ke rumah sakit kelas B.
- Faskes tingkat III, yaitu rumah sakit kelas A. Faskes ini melayani pasien yang membutuhkan pelayanan yang sangat spesifik dengan fasilitas yang lebih lengkap. Dokter yang melayani umumnya telah memiliki spesialisasi yang lebih spesifik menangani suatu bagian tubuh atau penyakit.
Sistem pelayanan bertingkat ini mengoptimalkan penggunaan sumber daya faskes. Pasien dengan penyakit yang tidak begitu serius, seperti misalnya diare, tidak perlu ke rumah sakit kelas A yang seharusnya menangani pasien dengan keluhan lebih serius, seperti misalnya kanker usus. Demikian pula pasien yang membutuhkan rawat inap, apabila penanganan penyakitnya bisa dilakukan di rumah sakit rujukan pertama, pasien tidak perlu dirujuk ke faskes tingkat III. Dengan begitu tempat tidur yang bisa digunakan untuk pasien yang membutuhkan penanganan serius bisa dialokasikan dengan semestinya. Dan berbicara soal tempat tidur, kita tidak bisa lepas dari istilah bed to population ratio
Bed to population ratio adalah ukuran yang dikeluarkan oleh WHO untuk menghitung ketersediaan tempat tidur per 1000 jiwa jumlah penduduk. Selama ini jika membicarakan soal rasio bed to population Indonesia, perhatian kita selalu ke keseluruhan tempat tidur di rumah sakit se-Indonesia. Sangat jarang, bahkan mungkin tidak pernah disinggung mengenai rasio bed to population * dari fasilitas rujukan pertama. Padahal merekalah garda terdepan penanganan rujukan pasien. Apabila faskes tingkat II kewalahan menangani pasien, mereka akan terpaksa merujuk pasien ke faskes yang lebih tinggi. Dan itu pada akhirnya akan mengurangi  sumber daya yang seharusnya dialokasikan ke pasien dengan kasus yang lebih serius.
Dataset ini kami susun untuk melihat seberapa besar rasio  bed to population * garda pertama penanganan rujukan pasien di Indonesia. Kami beranggapan jika rasio mereka tinggi, maka seharusnya mereka bisa lebih banyak menangani kasus yang membutuhkan rawat inap. 
Isi Dataset
Dataset ini terdiri dari dua sumber data:
- Data ketersediaan tempat tidur rumah sakit per tahun 2020 yang diambil dari situs Yankes Kemenkes.
- Data populasi dari situs BPS yang sudah kami modifikasi  sehingga hanya menampilkan data untuk tahun 2020.
Data hasil disediakan dalam bentuk file CSV ""Rasio Bed To Population Rumah Sakit Kelas C dan D tiap Provinsi Di Indonesia""
Acknowledgements
Terima kasih untuk pemerintah Indonesia yang sudah membuka data-data ini secara publik.
Inspirasi
Jika ada satu pertanyaan yang saya harap bisa terbantu terjawab melalui dataset ini, itu adalah ""Bagaimana mengingkatkan rasio bed to population Indonesia?"" Ini mungkin harapan yang muluk, tetapi saya berharap data yang saya tampilkan bisa memberikan gambaran provinsi-provinsi mana saja yang rasionya masih rendah. Karena dengan terlebih dulu mengetahui kelemahan kita baru bisa memperbaiki kelemahan-kelemahan itu dengan optimal."	4	16	0	yafethtb	dataset-rasio-bed-to-population-faskes-ii
3239	3239	ddbo139		[]		0	1	0	movie112	ddbo139
3240	3240	Hand Pose Detection 	OpenCV2 hand pose detection model template for further opencv2 integration 	['artificial intelligence', 'computer science', 'programming', 'advanced', 'computer vision', 'deep learning']	"The Repo contains 
Prototxt file and the caffemodel along with sample image data
Also includes the py files for further implementation. 
Python
For using it on single image :
python handPoseImage.py
For using on video :
python handPoseVideo.py"	33	757	9	rupakroy	handpose-detection
3241	3241	Proton Exchange Membrane (PEM) Fuel Cell Dataset	Nafion 112 membrane standard tests + MEA activation tests 	['earth and nature', 'chemistry', 'physics', 'energy', 'computer science']	"This dataset are about Nafion 112 membrane standard tests and MEA activation tests of PEM fuel cell in various operation condition. Dataset include two general electrochemical analysis method, Polarization and Impedance curves. In this dataset, effect of different pressure of H2/O2 gas, different voltages and various humidity conditions in several steps are considered. Behavior of PEM fuel cell during distinct operation condition tests, activation procedure and different operation condition before and after activation analysis can be concluded from data. In Polarization curves, voltage and power density change as a function of flows of H2/O2 and relative humidity. Resistance of the used equivalent circuit of fuel cell can be calculated from Impedance data. Thus, experimental response of the cell is obvious in the presented data, which is useful in depth analysis, simulation and material performance investigation in PEM fuel cell researches.
Github
Data Paper"	164	4784	13	sepandhaghighi	proton-exchange-membrane-pem-fuel-cell-dataset
3242	3242	churnmoddellig		[]		0	5	0	atillaatilla	churnmoddellig
3243	3243	ddbo1310		[]		0	3	0	movie112	ddbo1310
3244	3244	Softbank Investments Stock Prices		['business', 'investing']		0	8	1	peretzcohen	softbank-investments-stock-prices
3245	3245	The way people taking feces	זה דטאסט של איך המוצא שלך משפיע על הדרך בה אתה מנגב את התחת שלך לאחר הוצאת צואה 	['computer science', 'geospatial analysis', 'dnn', 'text data', 'datetime']		1	15	0	urigoldstein	the-way-people-taking-feces
3246	3246	68personbmp		[]		0	8	0	pongpopsukanunta	68personbmp
3247	3247	bike_share_data		[]		0	2	0	chiranjeevbit	bike-share-data
3248	3248	mlm pretrain roberta		[]		0	4	0	yuzhoudiyishuai	mlm-pretrain-roberta
3249	3249	ash_semeval_spans		[]		0	0	0	izuna385	ash-semeval-spans
3250	3250	GSE9476-Abnormal-Expression-Change-in-AML	AML vs Normal hematopoietic cells	['cancer']	"Context
Acute myeloid leukemia (AML) is one of the most common and deadly forms of hematopoietic malignancies. It was hypothesized that microarray studies could identify previously unrecognized expression changes that only occur only in AML blasts. We were particularly interested in those genes with increased expression in AML, believing that these genes may be potential therapeutic targets.
Content
The data is provided in two .txt files. We encourage you to find answers for the following questions using R programming language. 
Difficulty of working with this project lies in getting the required data into a data frame format from series_matrix file. It is reuired to access both the file that are provided to answer the questions
Inspiration
The aim of this project is data exploration and data analysis.
Here we call each probeset as a gene.
The questions to be answered are....
1.  Check any significant difference between sample ages of AMLs and Controls? Assume the age follows a normal distribution.
2.  Create a table to include mean_AML, sd_AML, min_AML, max_AML, mean_Control, sd_Control, min_Control, max_Control, and Fold_change (i.e, mean_AML – mean_Control) for each gene. It is fine to use built-in functions.
Hints: split the dataset into AML data and normal data sets, and then for each gene/probeset, calculate its mean, standard deviation, min and max expression values across samples separately (using a built-in function), and further merge these statistical values for each gene into one table. Apply data.frame() and give the created table the same row names as the gene expression data table.
3.  With the table created above (2), make a scatterplot of the mean expression of AML vs. Control sample. Are mean_AML and mean_Control significantly different?
4.  For each gene/probeset, conduct t-test between AML samples and Control samples, and then create a table including t scores and p-values from t-test, and merge to the table created in 2.
5.  How many genes/probesets are left after the following filters? a.) mean_AML &gt; 3 and mean_Control &gt; 3
                                                                                                              b.) max_AML &lt; 14 or max_Control   &lt; 14
Apply a) and b) together
6.  Following 5, if we further filter gene/probeset by applying p-value &lt; 0.05 and the absolute value of log2 Fold_change bigger than 1, then how many genes will be left which we call them significantly expressed genes (DEGs)? Among them, how many are up-regulated (Fold_change &gt; 0) and how many are down-regulated (Fold_change
&lt; 0)? List top 10 most upregulated DEGs and 10 most down-regulated DEGs including their p-value, log2FC, and mean values in each sample group.
7.  If you apply the limma package rather than t-test, then how many DEGs will be discovered by limma? (p-value &lt; 0.05 & abs(log2FC) &gt; 1). How many overlapped DEGs by these two different approaches? please draw a Venn diagram for demonstration."	0	12	0	sivalathakalluri	gse9476aml
3251	3251	Housedata		[]		2	22	0	chiranjeevbit	housedata
3252	3252	 used car price dataset (competition format)	regression training (R2score)	['automobiles and vehicles', 'exploratory data analysis', 'linear regression', 'regression']	"Context
This dataset was taken from link and preprocessed and separated into competition format.
The label for the test data is provided in the form of a function."	1721	9958	54	kukuroo3	used-car-price-dataset-competition-format
3253	3253	winequality_data		[]		0	1	0	chiranjeevbit	winequality-data
3254	3254	bank_loan_data		[]		0	4	0	chiranjeevbit	bank-loan-data
3255	3255	BdSL-D1500	Bangla Sign Language Dataset	['computer vision', 'deep learning', 'cnn', 'image data', 'multiclass classification']	"BdSL-D1500
Please email : kanchon.k.podder@bmpt.du.ac.bd for full access.
You can find the full-dataset here: https://drive.google.com/file/d/1FFRrQ8GbcI_Tm9GXcJPqRSxs20MtJCjC/view?usp=sharing
Bangla Sign Language Dataset or  BdSL-D1500 contains both  one-hand and two-hand representations of Bangla Sign Alphabets, numbers, and gestures. In total, there are 87 classes. 38 classes for one-hand representation of Bangla Sign Language, 36 classes for two-hand representation, 12 numbers, and one gesture. In each class, there are approximately 1517 images. All the images are in RGB. The corresponding label indicates Bangla Alphabets are given below:
০০: '00',
 ঘ: 'S13_gho_CC',
 স/শ/ষ: 'S34_so_CC',
 ত: 'Sign 19_to',
 গ: 'S12_go_CC',
 ড: 'Sign 17_DO',
 ও: 'S8_O_CC',
 ট: 'Sign 15_To',
 থ: 'Sign 20_tho',
 ঃ : 'Sign 36_bishorgo',
 ধ: 'S28_dho_CC',
 ব: 'S31_bo_CC',
 ঐ: 'S7_Oi_CC',
 ৪: '4',
 ০০০: '000',
 ফ: 'S30_fo_CC',
 ৬: '6',
 ল: 'S19_Io_CC',
 ন: 'S24_no_CC',
 ই: 'S3_i_CC',
 গ: 'Sign 9_go',
 জ: 'S17_jo_CC',
 ৯: '9',
 ঘ: 'Sign 10_ Gho',
 স: 'Sign 31_so',
 ম: 'Sign 27_mo',
 ট: 'S20_To_CC',
 উ: 'S4_u_CC',
 ব: 'Sign 25_bo',
 ল: 'S33_lo_CC',
 ছ: 'Sign 12_cho',
 দ: 'Sign 21_do',
 অ 'Sign 1_o',
 ঝ: 'Sign 14_Jho',
 ঁ: 'S38_Chandrabindu_CC',
 খ: 'S11_Kho_CC',
 র/ড়/ঢ়/ঋ: 'Sign 34_RHo',
 ঈ: 'Sign 6_I',
 ঢ: 'S23_Dho_CC',
 ৫: '5',
 চ: 'Sign 11_co',
 ঢ: 'Sign 18_DHO',
 ং: 'Sign 35_onosshar',
 ছ: 'S16_Cho_CC',
 ক: 'S10_ko_CC',
 চ: 'S15_co_CC',
 আ: 'Sign 2_a',
 এ: 'Sign 5_e',
 ং: 'S36_onnosar_CC',
 ড়: 'Sign 33_RO',
 হ: 'S35_ho_CC',
 ঠ: 'Sign 16_THo',
 থ: 'S26_tho_CC',
 গণনা: 'Counting',
 প: 'Sign 23_po',
 ক: 'Sign 7_ko',
 ম: 'S32_mo_CC',
 ঊ: 'Sign 4_U',
 হ: 'Sign 32_ho',
 ঔ: 'S9_OU_CC',
 ত: 'S25_to_CC',
 ০: '0',
 ও: 'Sign 3_O',
 ড: 'S22_Do_CC',
 ফ: 'Sign 24_fo',
 ধ: 'Sign 22_dho',
 ঙ: 'S14_Umo_CC',
 এ: 'S6_e_CC',
 ১: '1',
 ৭: '7',
 ভ: 'Sign 26_bho',
 ঝ: 'S18_jho_CC',
 ২: '2',
 ঃ: 'S37_bissorgo_CC',
 অ: 'S1_o_CC',
 ৮: '8',
 র: 'Sign 29_ro',
 র: 'S5_ro_CC',
 জ: 'Sign 13_jo',
 ল: 'Sign 30_lo',
 খ: 'Sign 8_kho',
 ৩: '3',
 দ: 'S27_do_CC',
 ঠ: 'S21_THo_CC',
 আ: 'S2_a_CC',
 ঞ: 'Sign 28_yo',
 প: 'S29_po_CC'"	6	361	3	kanchonkantipodder	bdsld1500
3256	3256	stylumia_nb1		[]		4	16	1	enkrish259	stylumia-nb1
3257	3257	phishing-domain-detection	phishing-domain-detection	['search engines', 'india', 'beginner', 'classification', 'xgboost']	"a b s t r a c t:
Phishing stands for a fraudulent process, where an attacker
tries to obtain sensitive information from the victim. Usually,
these kinds of attacks are done via emails, text messages, or
websites. Phishing websites, which are nowadays in a considerable rise, have the same look as legitimate sites. However, their backend is designed to collect 
sensitive information that is inputted by the victim. Discovering and detecting phishing websites has recently also gained the machine
learning community’s attention, which has built the models and performed classifications of phishing websites. This
paper presents two dataset variations that consist of 58,645
and 88,647 websites labeled as legitimate or phishing and allow the researchers to train their classification models, build
phishing detection systems, and mining association rules.
Specifications Table:
Subject- Computer Science
Specific subject area- Artificial Intelligence
Type of data- csv file
How data were acquired- Data were acquired through the publicly available lists of phishing and legitimate websites, from which the features presented in the datasets were extracted.
Data format Raw: csv file
Parameters for data collection- For the phishing websites, only the ones from the PhishTank registry were included, which are verified from multiple users. For the legitimate websites,
   we included the websites from publicly available, community labeled and organized lists 1, and from the Alexa top ranking websites.
Description of data collection- The data is comprised of the features extracted from the collections of websites addresses. The data in total consists of 111 features, 96 of which are
   extracted from the website address itself, while the remaining 15 features were extracted using custom Python code. Data source location Worldwide
Value of the Data
• These data consist of a collection of legitimate, as well as phishing website instances. Each
website is represented by the set of features that denote whether the website is legitimate
or not. Data can serve as input for the machine learning process.
• Machine learning and data mining researchers can benefit from these datasets, while also
computer security researchers and practitioners. Computer security enthusiasts can find these
datasets interesting for building firewalls, intelligent ad blockers, and malware detection
systems.
• This dataset can help researchers and practitioners easily build classification models in systems 
preventing phishing attacks since the presented datasets feature the attributes which
can be easily extracted.
• Finally, the provided datasets could also be used as a performance benchmark for developing
state-of-the-art machine learning methods for the task of phishing websites classification.
Data Description
The presented dataset was collected and prepared for the purpose of building and evaluating
various classification methods for the task of detecting phishing websites based on the uniform
resource locator (URL) properties, URL resolving metrics, and external services. The attributes of
the prepared dataset can be divided into six groups:
• attributes based on the whole URL properties presented in Table 1,
• attributes based on the domain properties presented in Table 2,
• attributes based on the URL directory properties presented in Table 3,
• attributes based on the URL file properties presented in Table 4,
• attributes based on the URL parameter properties presented in Table 5, and
• attributes based on the URL resolving data and external metrics presented in Table 6"	17	101	3	ravirajkukade	phishingdomaindetection
3258	3258	toxic_SEMEVAL_data		[]		0	1	0	izuna385	toxic-semeval-data
3259	3259	playstore		[]		0	7	0	chiranjeevbit	playstore
3260	3260	UK Energy Consumption 2015 (A to L)	Electricity/Power consumption by different areas of the UK	['geography', 'exploratory data analysis', 'geospatial analysis', 'data visualization', 'electricity', 'plotly']	"Content
This datasets contains the 2015 estimates of domestic electricity consumption for postcodes in Great Britain and a GeoPackage for Visualizing.
Acknowledgements
Special thanks to :
- Government of UK
- UK Code-point Open"	6	85	0	infinator	uk-energy-consumption-2015-a-to-l
3261	3261	twitter roberta base offensive		['email and messaging']		0	26	0	yuzhoudiyishuai	twitter-roberta-base-offensive
3262	3262	mini project		[]		0	1	0	vinodbhat222	mini-project
3263	3263	reef-yolox-v57		[]		0	8	0	sangayb	reef-yolox-v57
3264	3264	ChineseNovelDataset	ChineseNovelDataset 中文小说数据集	['literature', 'china ', 'nlp', 'text data']	"6742部中文小说数据集
40G 精校小说文本，无标注，适用于文本生成任务。"	2	30	2	orange1423	chinesenoveldataset
3265	3265	resize_ekh-dacon		[]		0	18	0	lxinha	resize-ekhdacon
3266	3266	UMP Agg Average Value Features		['sports']		0	38	4	takamichitoda	ump-agg-average-value-features
3267	3267	finaloutput		[]		0	4	1	captainabhijeeth	finaloutput
3268	3268	People of Skyrim	A dataset that contains information about Skyrim named NPCs	['games', 'video games']	"Context
The Elder Scrolls V: Skyrim is one of the most amazing RPG that I have ever played. One of the most interesting things about Skyrim for me is the diverse cast of NPCs that you can interact with. Each NPC has their own background and personality which makes it really fun to interact with them. This is why I think it would be interesting if we can make analysis from NPC in Skyrim. 
The script to scrape the data is available at: https://github.com/muhajipra/scrapeskyrimwiki
Content
This dataset is acquired by using Beautiful Soup 4 to scrape the data from infobox of each character listed in https://en.uesp.net/wiki/Skyrim:People
After cleaning the data a little bit, the total number of named characters are 1009, I only include named characters from the link above so characters such as hostile NPC are not included in this dataset.
Acknowledgements
This dataset uses information from Unofficial Elder Scrolls Page which contains a lot of amazing information, I would like to thank the fans for contributing to the page.
The creation of this dataset is also inspired from Skyrim Census made by Ant Pulley using Tableau which you can view by clicking this link. It inspires me to gather my own dataset.
Inspiration
I hope this dataset can help you to gain more insight about the world of Skyrim.
License
This article is licensed under the Creative Commons by-sa license. It uses material from the UESP page Skyrim: People"	0	23	0	muhajipra	people-of-skyrim
3269	3269	Auto XGB	Auto XGB for offline sessions	['xgboost', 'automl']	Auto XGB	1	25	2	devkhant24	auto-xgb
3270	3270	kagglerza_v1_f_K001_r2_0		[]		0	7	0	resistance0108	kagglerza-v1-f-k001-r2-0
3271	3271	Unsplash dataset images downloaded	Images in the unsplash image dataset downloaded in 250*250 resolution	['research', 'artificial intelligence', 'computer vision', 'image data']	"In August 7 2020, Unsplash released the Unsplash dataset, which provides useful metadata for over 25k images that could be used to train machine learning models. 
The metadata is available for download at github, but downloading all the images for training machine learning models is quite a hassle. Therefore I downloaded all the downloadable images in the database into the zip file for everyone to use😊"	4	152	4	jettchentt	unsplash-dataset-images-downloaded-250x250
3272	3272	Feedback Prize Train NER csv		[]		3	10	1	julian3833	feedback-prize-train-ner-csv
3273	3273	ensamble-boxes		[]		0	7	0	honihitak	ensambleboxes
3274	3274	nsk_image_search3_man7c		[]		0	2	0	motono0223	nsk-image-search3-man7c
3275	3275	Softbank Startup Investments		[]		1	14	0	peretzcohen	softbank-startup-investments
3276	3276	flagaaa		[]		0	11	0	kuba99p	flagaaa
3277	3277	coco image		[]		0	17	0	tom526	coco-image
3278	3278	Grocery Store Prices, Mongolia	Prices for largest markets/supermarkets in Ulaanbaatar, Mongolia	['asia', 'beginner', 'data visualization', 'time series analysis', 'food']	"Context
The National Statistics Office of Mongolia goes to each major market to record food prices each week in Ulaanbaatar, the capital city of Mongolia. The main purpose for this is to monitor a common basket of goods for use in consumer price index (CPI) calculations. 
Content
The data is in a long-form, with date, market, product, and price recorded. All prices are in Mongolian Tugriks. As of 2021 the USD to MNT is about 2850 MNT = 1 USD.
Acknowledgements
This dataset is possible thanks to the hard work of the people of the National Statistics Office of Mongolia.
Inspiration
Often people choose supermarkets over the open markets (called a ""zakh""). Mostly this is for convenience, but it is notable how much money people could save by choosing a different market!
This would be a great dataset for EDA or looking at how prices change over time."	215	1982	13	robertritz	ub-market-prices
3279	3279	geofiles	holoviz geo libraries in ubuntu	[]		5	15	0	adahmed	geofiles
3280	3280	YH-CN2-0		[]		0	14	0	neverstoppredicting	yhcn20
3281	3281	yhrn20		[]		0	10	0	neverstoppredicting	yhrn20
3282	3282	augment		[]		1	107	0	yochino	augment
3283	3283	20220130-0701-Model-LightGBMwithCats		[]		0	1	0	tomohiroyazaki	ubiquant-market-prediction
3284	3284	BTC_Dataset		[]		0	13	0	oussamaeboujbair	btc-dataset
3285	3285	homes for sale in Riyadh, Saudi Arabia 2021	homes for sale in Riyadh, Saudi Arabia 2021	[]		3	84	2	mansourhussain	homes-for-sale-in-riyadh-saudi-arabia-2021
3286	3286	BdSlHD-2300		['computer vision', 'deep learning', 'cnn', 'binary classification']	"Please email kanchon.k.podder@bmpt.du.ac.bd to get full access of the dataset.
you can find dataset from here: https://drive.google.com/file/d/1fgIY_azbGJr95T1p-ovrupwC3kqNmNW8/view?usp=sharing"	1	269	3	kanchonkantipodder	bdslhd2300
3287	3287	soft_teacher_git		[]		0	2	0	itaynivtau	soft-teacher-git
3288	3288	Evaluating students writing - balanced dataset		[]		0	21	0	alicematusheva	evaluating-students-writing-balanced-dataset
3289	3289	Tweets_Presidentielles_2022		[]		0	5	0	mathurinache	tweets-presidentielles-2022
3290	3290	dayes.csv		[]		2	37	0	ahmedelafndy	dayescsv
3291	3291	trained_weights		[]		1	39	0	raminanush	trained-weights
3292	3292	pycocotools	latest wheel for kaggle	['computer science']		0	27	1	greendolphin	pycocotools
3293	3293	mmseg_0210		[]		0	10	0	itaynivtau	mmseg-0210
3294	3294	det_softteacher_R101_faster_rcnn_iter14568		[]		0	9	0	itaynivtau	det-softteacher-r101-faster-rcnn-iter14568
3295	3295	Stock Analysis Project		['business']		1	13	0	carrie535	stock-analysis-project
3296	3296	The Boeing Company(BA) Stock Price		['transportation']		0	27	1	ifashion	the-boeing-companyba-stock-price
3297	3297	February Flight Delay Prediction		['currencies and foreign exchange']		1	89	1	priyankakuklani	february-flight-delay-prediction
3298	3298	Hide Dataset Image Deblurring Dataset		['earth and nature']		0	22	1	darthvader4067	hideblur
3299	3299	David King Applied AI Assignment 1 Connect 4 Level		['earth and nature']		0	11	0	davidkingrutgers	david-king-applied-ai-assignment-1-connect-4-level
3300	3300	Statistical Discussion In Python		[]		0	9	0	kakamana	statistical-discussion-in-python
3301	3301	Python statistical discussion		[]		0	7	0	kakamana	python-statistical-discussion
3302	3302	GoProDataset		[]		0	11	0	darthvader4067	gopro
3303	3303	DVD (Deep Video Deblurring)		['movies and tv shows']		1	21	1	darthvader4067	deepvideodeblurring
3304	3304	sampletest		[]		5	15	0	alankaruniyal	sampletest
3305	3305	Hitters		['baseball']		0	7	0	elifkoc1	hitters1
3306	3306	yolov5-3000	It is for reef v4.0 inference.	['earth and nature']		95	351	1	calvchen	yolov5-1920-4
3307	3307	men100		[]		0	2	0	rikdifos	men100
3308	3308	impostor_2		[]		0	7	0	domsgr	impostor-2
3309	3309	Neutrophil Activation Dataset	Neutrophil phase contrast images and DHR-123 stained microscopy images 	['biology', 'image data']	"Context
Monitoring the immune system provides crucial information in informing treatment strategies and assessing the effect of therapies. While measures such as complete blood count to determine the leukocyte subsets are extensively used clinically, our ability to assess leukocyte function is limited, especially for the cells of the innate immune system, such as neutrophils. Neutrophils are well known to undergo subtle morphological changes upon activation resulting from a complex cascade of signaling and gene expression. Here we create a carefully curated ground truth dataset of primary human neutrophil activation using abundance of reactive oxygen species (ROS) as a quantitative metric of activation.
Content
For detail information, please refer to the original publication.
Original publication
The original publication is: 
The Bibtex is: 
Acknowledgements
The project is funded by MIT Jameel Clinic"	2	51	2	weiliaomit	neutrophil-activation-dataset
3310	3310	Police productivity in the capital of São Paulo	Statistical data on police performance	['cities and urban areas', 'earth and nature', 'crime', 'public safety', 'social issues and advocacy']	"Context
Statistical data on police productivity in the city of São Paulo, SP
Content
The dataset contains statistical data from 2001 to 2021 with the number of occurrences carried out in all police stations in the city of São Paulo, among the occurrences: illegal weapons seized, cars recovered, arrests made.
Acknowledgements
Data were obtained from the website of the secretary of public security of the state of São Paulo: https://www.ssp.sp.gov.br/Estatistica/Pesquisa.aspx
The data were simplified to facilitate their manipulation."	2	17	3	mateusferro	police-productivity-in-the-capital-of-so-paulo
3311	3311	timeseriesus		[]		0	0	0	ayusha232	timeseriesus
3312	3312	seg_upernetswin_epoch7		[]		0	3	0	itaynivtau	seg-upernetswin-epoch7
3313	3313	yolox-s-coco		[]		0	1	0	sangayb	yolox-s-coco
3314	3314	Animal Shelter Intake and Outcome		[]		2	9	0	carrie535	animal-shelter-intake-and-outcome
3315	3315	dataset		[]		0	7	1	m4ur1c10	dataset
3316	3316	predicted_with_yearid		[]		0	6	0	dianhuanlin	predicted-with-yearid
3317	3317	Gun YoloV5 Detection		[]		2	11	0	hossamfakher	gun-yolov5-detection
3318	3318	The DoomsDay	The Detail About Doomsday 	['astronomy', 'intermediate', 'exploratory data analysis', 'data visualization', 'news']	"Context
This Database Shows The Data About Doomsday Till Date..
Content
Nothing
Acknowledgements
This Database is only Learning Purpose
Inspiration
Data Visualization"	25	527	10	sandipdevre	the-doomsday
3319	3319	nsk_image_search3_man7b		[]		0	2	0	motono0223	nsk-image-search3-man7b
3320	3320	single_24		[]		0	22	0	diveintoai	single-24
3321	3321	ump tf record time series split 10 fold		[]		0	18	3	lonnieqin	ump-tf-record-time-series-split-10-fold
3322	3322	mydataset		[]		0	20	0	eishatirraziaumt	mydataset
3323	3323	gr20182		[]		1	2	0	matrixneo	gr20182
3324	3324	fresh and stale apple yoloV5 Detection 		[]		3	13	0	hossamfakher	fresh-and-stale-apple-yolov5-detection
3325	3325	fresh and stale banana yoloV5 Detection 		[]		5	25	0	hossamfakher	fresh-and-stale-banana-yolov5-detection
3326	3326	Fire in Forest yoloV5 Detection 		[]		0	7	0	hossamfakher	fire-in-forest-yolov5-detection
3327	3327	Smoking YoloV5 Detection		[]		3	17	0	hossamfakher	smoking-yolov5-detection
3328	3328	FE_MACD_one_year		[]		0	8	0	hangy132	fe-macd-one-year
3329	3329	superimage		[]		0	1	0	shanmuhapriyaaraju	superimage
3330	3330	NBATopShots 01/29/2022		[]		0	0	0	erikaswang	nbatopshots-01292022
3331	3331	satellite_water YoloV5 Detection		[]		0	2	0	hossamfakher	satellite-water-yolov5-detection
3332	3332	"""C:\Users\Cavin lobo\Downloads\heart.csv"""		[]		0	9	0	cavinlobo	cuserscavin-lobodownloadsheartcsv
3333	3333	apple and orange Yolo Detection		[]		5	23	0	hossamfakher	apple-and-orange-yolo-detection
3334	3334	COVID-19 India- statewise 	Statewise updates of covid-19 in India upto 29 Jan 2022	['india', 'diseases', 'tabular data', 'covid19']	"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The disease has since spread worldwide, leading to an ongoing pandemic.
This dataset contains updates of covid-19 in India up to 29th January 2022.
No. of confirmed cases, no. of active cases,  no of persons who are cured/discharged, and no of deaths in different states of India are included.
Data is collected from government sources."	125	969	38	aryakrishnanar	covid-data-india-statewise
3335	3335	deeplabv3-plus-pytorch-main		[]		0	26	0	xuduoteng	deeplabv3-plus-pytorch-main
3336	3336	German Real Estate Auction Data	Data about German real estate Auctions from 2020-2021	['europe', 'real estate', 'data cleaning', 'feature engineering', 'tabular data']	"Introduction
An interesting characteristic about auctions is the so-called Winner's Curse. The winners of an auction oftentimes overpay for the property that they get. This is mainly due to the different perceptions of the property's value. Some individuals are rather optimistic (or maybe more risk-affine) while others might be more pessimistic (or risk-averse) which leads to different value estimations. Because there are many bidders in an auction and the highest bid wins, he/she will therefore usually overpay, especially in a setting where the objects to be auctioned are typically unique.
This dataset can therefore be used to examine the extent to which the winner's curse effect is existent in this type of environment. Of course, for this to work we need some way of assessing whether an individual actually overpaid for the property which could be implemented in separate ways. For example, one can estimate the value with the information available ex-ante or with the ex-post information.
This dataset is not yet cleaned and there can be some feature engineering performed with it. It contains information about auction results in Saxony, Germany from the years 2020 and 2021.
General Ideas:
Clean the data
Perform feature engineering
Use property evaluation models to check for overpaying
Check if any of the objects that have been auctioned off have been sold on the market already.
Which objects tend to be ""overpaid"" more than others?
How have auctions prices evolved throughout the years?
Notes
I am planning to add information about the prior years (~2015-2021) in the upcoming weeks.
There are 4 auctions each year, held in Dresden and Leipzig.
Pictures and more information about the objects will also follow soon."	10	75	3	laurinbrechter	german-real-estate-auction-data
3337	3337	Automotive directions - Telematics	Data from 9027 unique VINs received for a time period of 2 months	['automobiles and vehicles', 'intermediate', 'data analytics', 'feature engineering', 'dimensionality reduction']	"This Data is collected from more than  9000 unique VINs received for a time period of 2 months. 
Use Case: Engine Health Monitoring
EHS(Engine Health Score) can be used to monitor the health of all engines in a population in real-time and identify
engines that are already faulty or likely to be faulty in the near future.
Estimate engine health (up to the current time instant) in real-time, and
Forecast engine health (into the future, e.g. 1 week ahead) in real-time"	1	60	1	adhilpk	automotive-directions-telematics
3338	3338	Box Diet - Warsaw Concept	The data was collected by Warsaw Concept  from miodmalina.eu, Mar-Nov 2020	[]		1	26	0	warsawconcept	box-diet-warsaw-concept
3339	3339	Klasifikasi kanker Payudara		[]		3	23	1	wildanizzuddinfaza	klasifikasi-kanker-payudara
3340	3340	DataSet_Of_CryptocurrencyDoge_Source_YahooFinance	This information used in Exploratory Data Analysis & Price prediction further!	['business']		0	14	0	ajaygaur09	dataset-of-cryptocurrencydoge-source-yahoofinance
3341	3341	subaru temp 2		['automobiles and vehicles']		0	74	0	wuliaokaola	subaru-temp-2
3342	3342	dnnmodel		[]		11	31	0	monolith0456	dnnmodel
3343	3343	motorcycle rider helmet license plate 		[]		7	74	0	majorprojectssuu	motorcycle-rider-helmet-license-plate
3344	3344	breakhis2	train and validation dataset for multiclassification having 8 classes	['business']		0	9	0	deepindersingh2	breakhis2
3345	3345	tree detection model 	pre-trained model to detect urban trees in aerial images	['cities and urban areas', 'cnn']	pre-trained torchvision.models.detection.retinanetresnet50fpn model to detect urban trees in aerial images	5	30	2	easzil	dataset
3346	3346	Crypto News +	Crypto news articles containing title, text, and the sentiment analysis. 	['finance', 'nlp', 'text data', 'investing']	"Context
Crypto news data (2021-10-12 / 2021-01-29) in a structured format including title, text, source, subject, and sentiment analysis. 
Get live data using CryptoNews+ API
Content
Crypto news is scraped from the web. The sentiment analysis is performed using textblob. 
Updates
We are trying to provide monthly updates on the dataset. 
Acknowledgements
Sources used:
- cryptonews.com
Inspiration
This dataset can be an addition to all kind of crypto models one can think of (e.g. BTC price prediction). 
An API to get the latest crypto news + in the same format is available at RapidAPI"	30	444	5	oliviervha	crypto-news
3347	3347	Fire detection dataset	This dataset contains images with fire and without fire in it. 	['arts and entertainment']		3	52	4	kabilan03	fire-detection-dataset
3348	3348	face classifier		['earth and nature']		2	106	0	zenbot99	face-classifier
3349	3349	TugasAI		[]		0	6	0	haidarbagiralfahmi	tugasai
3350	3350	ddbo137		[]		0	2	0	movie112	ddbo137
3351	3351	International Chess Statistics 2022	Dataset of all chess players around the world along with country's chess stats.	['games', 'board games', 'video games', 'sports', 'internet']	"Context
This dataset chess statistics w.r.t. countries as well as their player base.
Content
File 1 : Complete Player Database
- Data of all the players around the world with rating more than 1000.
File 2 : International chess stats
- Stats of all the countries with chess players.
Acknowledgements
The data was scraped from chess-ranking.com\
Inspiration
Magnus Carlsen's YouTube channel."	192	1642	22	deepcontractor	international-chess-statistics-2022
3352	3352	SportsCelebrity		[]		0	9	0	wahidulhasanabir	sportscelebrity
3353	3353	20newsg		[]		0	4	0	fernandobordi	20news
3354	3354	email-classifier-dataset-desafio2		['computer science']		1	22	0	rywgar	emailclassifierdatasetdesafio2
3355	3355	232323		[]		0	4	0	qcq123456789	232323
3356	3356	tf_epoch_20		[]		0	2	0	darknesszx	tf-epoch-20
3357	3357	all information you need in tmdb movies		['movies and tv shows']		2	18	0	ahmedabdelhamied2501	all-information-you-need-in-tmdb-movies
3358	3358	COTS_balaji_models		[]		15	191	0	dhakshiin1601	cots-balaji-models
3359	3359	pretrained-pca-rf		[]		2	32	0	danielrose1995	pretrainedpcarf
3360	3360	mymusicalprefrences		[]		0	6	0	xianchaoliu	mymusicalprefrences
3361	3361	Traffic stops by police officers on Rhode Island	Traffic stops by police officers on Rhode Island state in USA	['beginner', 'data visualization', 'data analytics', 'pandas', 'seaborn']	"Context
Traffic stops data in USA Rhode Island State. The data contain more than 90,000 of rows
Data Dictionary
| Attributes | Definition | example |
| --- | --- | ---|
| stop_date | date of stop dd-mm-yy | 1/23/2005 |
| stop_time | time of stop hour:min | 1:55|
| driver_gender | Sex/Gender | M/F|
| driver_age | Age | 20 |
| driver_race | a person's race ('White', 'Black', 'Asian', 'Hispanic', 'Other') | 'Black' |
| violation | which violation ('Speeding', 'Other', 'Equipment', 'Moving violation', 'Registration/plates', 'Seat belt') | 'Speeding' |
| search_conducted | is the officers search for taboo | True |
| stop_outcome | action that the officer take ('Citation', 'Arrest Driver', 'N/D', 'Warning', 'Arrest Passenger', 'No Action') | 'Citation' |
| is_arrested | the person is arrested or not | False |
| stop_duration | duration for the stop (0-15 Min, 16-30 Min, 30+ Min) | '0-15 Min' |
| drugs_related_stop | is the stop related to drugs or not | False |
Acknowledgements
All thanks to  THE STANFORD OPEN POLICING PROJECT For making this data set
Inspiration
Get the data and try to create your own questions. Good luck ❤️"	68	354	6	ibrahimelsayed182	stanford-open-policing
3362	3362	Covid_data	Pandemic Report of Each Country 	['health']		2	19	0	gourangthakurr	covid-data
3363	3363	carlane		[]		0	4	0	xuduoteng	carlane
3364	3364	Linear Regression	Randomly created dataset for Linear Regression	['beginner', 'linear regression', 'tabular data']	"Context
This dataset is created using the sources from this dataset.
Content
Single variable regression model:
$$ y = mx + c $$
Both training dataset and testing dataset contain 1 Million rows. 
1) x-values are numbers between 1 and 100. 
2) y-values are created using this excel function: NORMINV(RAND(), x, 3).
License: feel free to use"	3	135	0	fareedkhan557	linear-regression
3365	3365	GBR Starfish TFRecords Class 1X 2 1		[]		0	2	0	mmelahi	gbr-starfish-tfrecords-class-1x-2-1
3366	3366	GBR Starfish TFRecords Class 1X 2 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-class-1x-2-0
3367	3367	GBR Starfish TFRecords Class 1X 1 1		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-class-1x-1-1
3368	3368	GBR Starfish TFRecords Class 1X 1 0		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-class-1x-1-0
3369	3369	GBR Starfish TFRecords Class 1X 0 1		[]		0	0	0	mmelahi	gbr-starfish-tfrecords-class-1x-0-1
3370	3370	GBR Starfish TFRecords Class 1X 0 0		[]		0	1	0	mmelahi	gbr-starfish-tfrecords-class-1x-0-0
3371	3371	myanimelist top 15k manga	This dataset is taken from myanimelists top manga section	['anime and manga']	"Context
This dataset is taken from myanimelist by using the myanimelist api https://myanimelist.net/apiconfig/references/api/v2.
Content
This dataset have all the information that is in myanimelist manga details except for the private information to user
Missing Values
Missing values are specified as empty strings and NaN values.
Inspiration
I created this dataset to make a manga recommendation system."	77	659	6	fatihoge	myanimelist-top-12k-manga
3372	3372	create_magic		[]		0	6	0	nabanichowdhury	create-magic
3373	3373	dialing		['internet']		2	23	6	sapanmankad	dialing
3374	3374	jigsaw-public-code		[]		1	15	0	mathurinache	jigsaw-public-code
3375	3375	United Nations voting database	Voting records on the General Assembly and the Security Council since 1946	['government', 'politics', 'international relations']	"Context
This database contains all logged UN resolutions that have votes.
Content
This database contains 7855 records. 
Each record is either a General Assembly (GA) vote, or a Security Council (SC) vote. 
The votes are from 1946 until 2021. 
Each nation can vote: 
Y (YES)
N (NO)
X (Chose not to vote)
A (Absent)
[EMPTY] the nation is not relevant to the vote. 
Acknowledgements
The data was scrape from the very well organized UN digital library at: 
https://digitallibrary.un.org/
Inspiration
This database was constructed to better understand relationship and biases between nations"	26	326	10	guybarash	un-resolutions
3376	3376	Model1		[]		0	7	0	majedalatawi	model1
3377	3377	진중문고예시		[]		0	6	0	kevinsunpark	militarybooks
3378	3378	Dataset_Olive		[]		0	2	0	majedalatawi	dataset-olive
3379	3379	YH-CN2-1		[]		0	9	0	neverstoppredicting	yhcn21
3380	3380	ubiquant models public	Models for the Ubiquant market prediction competition.	[]		3	42	0	slawekbiel	ubiquant-models-public
3381	3381	twitter-roberta-base-sentiment		['social networks']		0	15	0	dzisandy	twitterrobertabasesentiment
3382	3382	bert-base-uncased-hatexplain		[]		0	8	0	dzisandy	bertbaseuncasedhatexplain
3383	3383	bert-base-uncased-hatexplain-rationale-two		[]		0	11	0	dzisandy	bertbaseuncasedhatexplainrationaletwo
3384	3384	twitter-xlm-roberta-base-sentiment		[]		0	9	0	dzisandy	twitterxlmrobertabasesentiment
3385	3385	Lonely Kids Club Product Comments from Judge.me	1852 lines of user comments	['australia', 'clothing and accessories', 'marketing', 'text mining', 'tabular data', 'text data']	"Context
I was researching an Australian fashion company called The Lonely Kids Club (https://lonelykidsclub.com/). Then I find that 97% of its comments are five stars (out of five) despite the fact that all products are hand-printed by the business owner and the business is not the owner's major focus. Therefore, I decided to take a look at its product feedback. 
Data are crawled from Judge.me and the official website.
The python code of the crawler will be posted as well.
Content
Variable and descriptions
user: user name
time: when is the comment made, the values are either 'x month(s) ago' or '1 year ago.
score: number of stars given to the product, range from 0 to 5
title: title of the comment body
comment: comment body
product_name: name of the product that is commented
product_description: description of the product provided on the official website of the brand
href: link to the product website
photo: link of photo that is embodied within the comment. Values are each 'None' or a link.
Inspiration
Have the company purposely filtered the comment?
If their T-shirts are so good, why?"	15	125	5	pengpengwang1999	lonely-kids-club-product-comments-from-judgeme
3386	3386	Filipino Translated Random Dataset		[]		6	15	0	benzajtil	filipino-translated-random-dataset
3387	3387	High Resolution Cat-Dog-Bird Image Dataset (13000)	+13000 high resolution images scrabbed from Instagram and manually filtered	['animals', 'classification', 'deep learning', 'cnn', 'image data']	"Data Description
The training archive contains +13,000 High Resolution images of dogs, cats and birds each of them is contained in its own folder read the data from them and train test split your data.
Note
don't forget to use the data augmentation approach so that you can have even larger dataset
about the Data
birds =&gt; 4149 image ------- indexed from 0 =&gt; 4148
cat =&gt; 4015 image ------- indexed from 0 =&gt; 4014
dog =&gt; 5180 image ------- indexed from 0 =&gt; 5179"	26	139	2	mahmoudnoor	high-resolution-catdogbird-image-dataset-13000
3388	3388	nyu-v2		['universities and colleges']		0	11	0	mohyfahim	nyuv2
3389	3389	SkolkovoInstitute/roberta_toxicity_classifier_v1		[]		0	6	0	dzisandy	skolkovoinstituteroberta-toxicity-classifier-v1
3390	3390	[jigsaw] pre-trained RoBERTa (MLM)		[]		13	9	0	mikezz11	jigsaw-pretrained-roberta-mlm
3391	3391	train_sample		[]		0	5	0	toniean	train-sample
3392	3392	car_dataset		[]		1	4	1	bhavyachandrasala	car-dataset
3393	3393	imdb_movies_dataset		[]		0	21	1	bhavyachandrasala	imdb-movies-dataset
3394	3394	Imagesdata		[]		0	2	0	diribachali	imagesdata
3395	3395	Bugs Reports		[]		0	16	1	innocentcharles	bugs-reports
3396	3396	Kpop Artists and Full Spotify Discography	273 Kpop Artists from 1990s to 2021	['popular culture', 'music', 'beginner', 'tabular data', 'korea']		12	271	2	ericwan1	kpop-artists-and-full-spotify-discography
3397	3397	68personbmp		[]		0	13	0	natthaweenn	68personbmp
3398	3398	comic_character		[]		0	10	0	swayamjathar	comic-character
3399	3399	Scaler		[]		1	9	0	mbonyani	scaler
3400	3400	Flag pose for wind speed estimation	contains windspeeds associated with respective frames for regression task	['environment', 'beginner', 'intermediate', 'computer vision', 'video data']		13	97	3	imsiddhant07	flag-pose-for-wind-speed-estimation
3401	3401	weapon_detection		[]		12	44	0	nikitaosovskiy	weapon-detection
3402	3402	Greece's Earthquakes	A list of earthquakes that have occurred in Greece between 1965 and 2021	['earth and nature', 'earth science', 'geology', 'computer science', 'data visualization']	"Context
An updated and expanded earthquake inventory only for Greece, dating back to 1965, which will be updated annually with the preceding year's earthquake events.
Content
The first column is titled 'DATETIME,' and it indicates when the earthquake occurred. Then there are the 'LAT' (Latitude) and 'LONG' (Longitude) coordinates, which tell us where the earthquake occurred. Finally, the earthquake is described by 'DEPTH' (km) and 'MAGNITUDE' (Richter Scale).
Acknowledgements
National Observatory of Athens / Institute of Geodynamics"	188	1303	18	nickdoulos	greeces-earthquakes
3403	3403	TMS traffic counts		[]		0	7	1	farhanzafrani	tms-traffic-counts
3404	3404	subaru ds train 0129 trim		[]		0	2	0	wuliaokaola	subaru-ds-train-0129-trim
3405	3405	RSNA SeriesIDs		[]		71	20	0	fereshtej	rsna-seriesids
3406	3406	5_lang_train_test_new		[]		1	7	0	omkarghadge	5-lang-train-test-new
3407	3407	ddbo136		[]		0	2	0	movie112	ddbo136
3408	3408	corpus-prensa-educativa-arg		[]		0	2	0	fernandobordi	corpusprensaeducativaarg
3409	3409	bottlenecks		['business']		1	24	0	raminanush	bottlenecks
3410	3410	Efaecium_AMRC_data		[]		0	0	0	saiprajwalreddy	efaecium-amrc-data
3411	3411	Efaecium_AMRC		[]		0	0	0	saiprajwalreddy	efaecium-amrc
3412	3412	ddbo134		[]		0	1	0	movie112	ddbo134
3413	3413	largecnn		[]		1	12	0	gmhost	largecnn
3414	3414	tugas_visualisasi		[]		0	2	0	asadhumam	tugas-visualisasi
3415	3415	fifa_predictions		[]		0	12	1	areebameerkhan	fifa-predictions
3416	3416	sea-thru		[]		2	14	1	umepon0626	seathru
3417	3417	depthmap		[]		0	7	1	umepon0626	depthmap
3418	3418	mouse_walking_data		[]		0	24	0	raminanush	mouse-walking-data
3419	3419	competition		[]		0	8	0	peilingliu2021	competition
3420	3420	starfish_yolov5		[]		2	97	0	panouyang	starfish-yolov5
3421	3421	Ad Click Data		['categorical data', 'classification', 'tabular data', 'text data']		10	73	1	mafrojaakter	ad-click-data
3422	3422	Time Series, Aggregation and Filters 2022		['internet']		0	11	0	ks9521	time-series-aggregation-and-filters-2022
3423	3423	Abstract Art Gallery	Create your own abstract piece of art using GANs	['arts and entertainment', 'art', 'deep learning', 'gan', 'image data']	"Deep Learning is the new Vassily Kandinsky
Content
This dataset contains 2782 files of abstract images.
I scrapped these images from <a href=""https://www.wikiart.org/"">this website</a> to try to generate abstract images based on this whole dataset using GANs.
If you like implementing research papers from scratch, you can get started from <a href=""https://www.kaggle.com/getting-started/150948"">this topic</a> on GANs as reference.
Inspiration
The painter Vassily Kandinsky is considered the founder of abstract art. He painted his first abstract watercolour Untitled in 1913.
<br>"	2362	16562	127	bryanb	abstract-art-gallery
3424	3424	GreatBarrierReefEffdet-D3B8E50R1-GPU	Used to debug the results of the great barrier reef	[]		3	29	0	heyrict	greatbarrierreefeffdetd3b8e50r1gpu
3425	3425	AIMC-MIMIC-CXR-samples		[]		0	17	0	yisakkim	aimc-mimic-cxr-samples
3426	3426	Bellabeat device tracker dataset	Dataset was collated from the fitbit fitness tracker	['exercise']		0	5	0	blessingalabie	bellabeat-device-tracker-dataset
3427	3427	roberta-base-tokenizer		[]		0	1	0	cwren0110	robertabasetokenizer
3428	3428	NoContextroBERTa		[]		0	7	0	cwren0110	nocontextroberta
3429	3429	NIFTY INDICES DATA Jan22	Daily level historical dataset of major NIFTY indices	['business', 'time series analysis', 'investing']	"Context
The National Stock Exchange of India Limited (NSE) is the leading stock exchange of India, located in Mumbai. The NIFTY 50 index is National Stock Exchange of India's benchmark broad based stock market index for the Indian equity market.
Content
This dataset has day level information on major NIFTY indices starting from 01 January 2000.
Two new indices are added: 
- Nifty Private Bank
- Nifty Oil&Gas
Each file represents an index and has the following columns
Date - date of observation
Open - open value of the index on that day
High - highest value of the index on that day
Low - lowest value of the index on that day
Close - closing value of the index on that day
Volume - volume of transaction
Turnover - turn over
P/E - price to earnings ratio
P/B - price to book value
Div Yield - dividend yield
Acknowledgements
The data is obtained from NSE website and updated from old dataset. 
https://www.kaggle.com/sudalairajkumar/nifty-indices-dataset
Photo credits: piqsels
Inspiration
Use this data to predict future stock prices or analyze the trend."	26	205	10	atrisaxena	nifty-indices-data
3430	3430	Google Data Analytics Certification Capstone		[]		3	21	0	michaelbrueckmann	google-data-analytics-certification-capstone
3431	3431	DualAxis Sales Vs Profit 2022		['business']		2	12	0	ks9521	dualaxis-sales-vs-profit-2022
3432	3432	wt-competition		[]		3	39	0	witoldt	wtcompetition
3433	3433	stylumia_complete		[]		1	13	0	enkrish259	stylumia-complete
3434	3434	nsk_image_search3_man7a		[]		0	2	0	motono0223	nsk-image-search3-man7a
3435	3435	nsk_image_search3_man7		[]		0	3	0	motono0223	nsk-image-search3-man7
3436	3436	UPI Payments India	UPI Payments India Dataset	['business', 'finance']	"Context
I was interested in doing some dashboards and finding out how Indian digital payments have evolved and how India is performing at Global level
Content
You'd find the UPI data month-wise from 2016 to 2021. You'd find the count of banks that allowed UPI, the value of the UPI (how much money was transacted), and the volume i.e. the count of the transactions being done.
Acknowledgements
We wouldn't be here without the help of others. The data was taken from the NPCI website https://www.npci.org.in/"	21	100	2	bhatnagardaksh	upipaymentsindia
3437	3437	Office Supplies2022		['business']		0	12	0	ks9521	office-supplies2022
3438	3438	Telecom Churn Case Study using ML 	Observing the different parameters affecting churning in the Telecom company	['business']	"Business problem overview
In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.
For many incumbent operators, retaining high profitable customers is the number one business goal.
To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.
In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."	5	40	1	siddharthaborgohain	telecom-churn-case-study-using-ml
3439	3439	Soybean operation at Port of Santos	Soybean loading at Port of Santos from 2016 to 2022	[]		0	4	1	josefontebassoneto	soybean-loading-at-santos-port-from-2016-to-2022
3440	3440	CNN_olive_dataset1		[]		0	2	0	majedalatawi	cnn-olive-dataset1
3441	3441	Olive_dataset0		[]		0	0	0	majedalatawi	olive-dataset0
3442	3442	Planes Vs Trains Ticket Price In India	Planes Ticket Price Vs Train Ticket Price In India In 7 Major Citys	['india', 'intermediate', 'data visualization', 'text data', 'travel']	"Context
This database shows the 7 major city's in India Travel Expense.
Content
Nothing Much More
Acknowledgements
Thanks to web
Inspiration
My personnel"	90	656	16	sandipdevre	planes-vs-trains-in-ticket-price-in-india
3443	3443	cardiffnlp/twitter-roberta-base-hate	Twitter-roBERTa-base for Hate Speech Detection	['email and messaging']		0	22	0	roumaissaa	twitterrobertabasehate
3444	3444	Hotel Bookings		['hotels and accommodations']	"Hello, i found this description on a website, so I modified it and posted it here.
This dataset for two hotels, one of the hotels is resort hotel and the other is a city hotel.
Dataset features:
ADR --&gt; Average Daily Rate
Adults --&gt; Number of adults
Agent --&gt; ID of the travel agency that made the booking
ArrivalDateDayOfMonth --&gt; Day of the month of the arrival date
ArrivalDateMonth --&gt; Month of arrival 
ArrivalDateWeekNumber --&gt; Week number of the arrival date
ArrivalDateYear --&gt; Year of arrival date
AssignedRoomType --&gt; Code for the type of room assigned to the booking
Babies --&gt; Number of babies
BookingChanges --&gt; Number of changes that made to the booking
Children --&gt; Number of children
Company --&gt; ID of the company
Country --&gt; Country of origin
CustomerType --&gt; Type of booking
DaysInWaitingList --&gt; Number of days the booking was in the waiting list before it was confirmed to the customer
DepositType --&gt; Indication on if the customer made a deposit to guarantee the booking
DistributionChannel --&gt; Booking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”
IsCanceled --&gt; Value indicating if the booking was canceled (1) or not (0)
IsRepeatedGuest --&gt; Value indicating if the booking name was from a repeated guest (1) or not (0)
LeadTime --&gt; Number of days that elapsed between the entering date of the booking and the arrival date
MarketSegment --&gt; Market segment designation, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”
Meal --&gt; Type of meal booked, Undefined/SC – no meal package
                                                    BB – Bed & Breakfast
                                                    HB – Half board (breakfast and one other meal – usually dinner)
                                                    FB – Full board (breakfast, lunch and dinner)
PreviousBookingsNotCanceled --&gt; Number of previous bookings not cancelled by the customer prior to the current booking
PreviousCancellations --&gt; Number of previous bookings that were cancelled by the customer prior to the current booking
RequiredCardParkingSpaces --&gt; Number of car parking spaces required by the customer
ReservationStatus --&gt; Reservation last status
ReservationStatusDate --&gt; Date at which the last status was set
ReservedRoomType --&gt; Code of room type reserved
StaysInWeekendNights    --&gt; Number of weekend nights the guest stayed or booked to stay at the hotel
StaysInWeekNights --&gt; Number of week nights the guest stayed or booked to stay at the hotel
TotalOfSpecialRequests --&gt; Number of special requests made by the customer"	1	32	0	mohammedhassan11	hotel-bookings
3445	3445	dt1234		['automobiles and vehicles']		2	2	0	choudharyhardiknmims	dt1234
3446	3446	Diabetes_Technical		[]		0	1	0	choudharyhardiknmims	diabetes-technical
3447	3447	SEA Building Energy Benchmarking		['energy']		4	70	1	mickaelnarboni	sea-building-energy-benchmarking
3448	3448	SemEval_task_11		[]		3	237	0	darthmanav	semeval-task-11
3449	3449	Loan Default Dataset	Loan Default Classification Problem	['finance', 'banking', 'beginner', 'classification', 'investing']	"Description:
Banks earn a major revenue from lending loans. But it is often associated with risk. The borrower's may default on the loan. To mitigate this issue, the banks have decided to use Machine Learning to overcome this issue. They have collected past data on the loan borrowers & would like you to develop a strong ML Model to classify if any new borrower is likely to default or not.
The dataset is enormous & consists of multiple deteministic factors like borrowe's income, gender, loan pupose etc. The dataset is subject to strong multicollinearity & empty values. Can you overcome these factors & build a strong classifier to predict defaulters? 
Acknowledgements:
This dataset has been referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build classification model to predict weather the loan borrower will default or not.
Also fine-tune the hyperparameters & compare the evaluation metrics of vaious classification algorithms."	730	5208	30	yasserh	loan-default-dataset
3450	3450	UK Petrol Prices	UK petrol Prices Up and Down	['united states', 'beginner', 'text data', 'oil and gas', 'python']	"Context
This database About How diesel and petrol Prices Rising day By day in UK
Content
Diesel Price And Petrol Price
Acknowledgements
Just For Learning
Inspiration
Another One"	202	1205	9	sandipdevre	uk-petrol-prices
3451	3451	analytics data		['business']		0	6	0	abdellahnasreddine	analytics-data
3452	3452	spektral		[]		0	4	0	mmelahi	spektral
3453	3453	mengklasifikasi kanker data		[]		0	1	0	muhrifqi	mengklasifikasi-kanker-data
3454	3454	Ecom Analytics		['business']		5	34	0	abdellahnasreddine	ecommdataan
3455	3455	klasifikasi kanker payudara		[]		1	12	0	alimmusyaffa	klasifikasi-kanker-payudara
3456	3456	Employee_Attrition		[]	"About the data set (Employee data)
The dataset contains information about employees. The aim is to find which employees might undergo attrition.
Attribute information:
Age: Age of the employee
BusinessTravel: How much travel is involved in the job for the employee:No Travel, Travel Frequently, Tavel Rarely
Department: Department of the employee: Human Resources, Reserach & Development, Sales
Commute: Number of miles of daily commute for the employee
Education: Employee education field: Human Resources, Life Sciences, Marketing, Medical Sciences, Technical, Others
EnvironmentSatisfaction: Satisfaction of employee with office environment
Gender: Employee gender
JobInvolvement: Job involvement rating
JobLevel: Job level for employee designation
JobSatisfaction: Employee job satisfaction rating
MonthlyIncome: Employee monthly salary
OverTime: Has the employee been open to working overtime: Yes or No
PercentSalaryHike: Percent increase in salary
PerformanceRating: Overall employee performance rating
YearsAtCompany: Number of years the employee has worked with the company
Attrition: Employee leaving the company: Yes or No"	1	13	2	hariprasath94	empattr94
3457	3457	ToxicCommentsVectors		[]		0	20	0	jmrludan	toxiccommentsvectors
3458	3458	datacancer		[]		0	0	0	muhammadridwan23	datacancer
3459	3459	xView Tiled		[]		0	33	0	hassanmojab	xview-tiled
3460	3460	Lebanese Customs (Private Trade)	Private Trade (Import and Export) 2012 - 2021	['transportation', 'tabular data', 'middle east']	"Data source | http://www.customs.gov.lb
Interactive Report | https://bit.ly/LebaneseCustomsReport 📈📉📊"	8	102	1	najielkotob	lebanese-customs-private-trade
3461	3461	Covid 19 in Ontario		[]		20	641	6	roydatascience	covid-19-in-ontario
3462	3462	G_r_finaldataset		[]		1	7	0	matrixneo	g-r-finaldataset
3463	3463	lxrmodels11		[]		0	12	0	lxr1204	lxrmodels11
3464	3464	Keith Galli's Sales Analysis Exercise	Data for the 'Solving real world data science tasks with Python Pandas!' video	['internet', 'beginner', 'exploratory data analysis', 'data visualization', 'matplotlib', 'pandas']	"Context
This is the dataset required for Keith Galli's 'Solving real world data science tasks with Python Pandas!' video. Where he analyzes and answers business questions for 12 months worth of business data. The data contains hundreds of thousands of electronics store purchases broken down by month, product type, cost, purchase address, etc. 
I decided to upload the data here so that I can carry out the exercise straight on Kaggle Notebooks. Making it ready for viewing as a portfolio project. 
Content
12 .csv files containing sales data for each month of 2019. 
Acknowledgements
Of course, all thanks goes to Keith Galli and the great work he does with his tutorials. He has several other amazing tutorials that you can follow and subscribe at his channel."	52	261	4	zulkhaireesulaiman	sales-analysis-2019-excercise
3465	3465	Allopathic Facility in Jammu and Kashmir 2019	Patient attendance in various district wise allopathic facility of J& K state	['health']		0	29	6	raj401	allopathic-facility-in-jammu-and-kashmir-2019
3466	3466	ridge_ucblab_cleanexp2_fasttext_single		[]		0	5	0	shobhitupadhyaya	ridge-ucblab-cleanexp2-fasttext-single
3467	3467	COVID-19 SOLES Tables	COVID-19 evidence from primary research studies	['research', 'literature', 'medicine', 'nlp', 'covid19']	"Context
It is difficult to keep up with the rapid rate at which COVID-19 research gets published. 
Content
What is COVID-19-SOLES?  https://camarades.shinyapps.io/COVID-19-SOLES/
Using an online systematic review platform SyRF [Camarades] are categorizing primary COVID-19 research studies with the help of a crowd of trained reviewers. Each record is annotated by two independent reviewers and disagreements are reconciled by a 3rd senior reviewer to maintain a high level of accuracy.
Primary research on COVID-19
[Camarades] define primary COVID-19 research as studies on either COVID-19 disease or the SARS-CoV-2 virus spanning the clinical, in vivo animal, in vitro, and in silico literature. [Camarades] annotate all primary COVID-19 studies with objectives, methodology and the subjects/samples used. [Camarades] also annotate secondary research (e.g. news articles, editorials) related to COVID-19 but do not extract any further information about these articles. At this stage, [Camarades] are not including any studies performed on other coronavirus strains or in other disease contexts.
Target audience
This web application is intended for use by all stakeholders in COVID-19 research, including researchers working within the field or performing rapid or systematic reviews of COVID-19 literature.
Acknowledgements
Banner Photo by Vincent Ghilione on Unsplash
Data was released under a CC 4.0 license at https://camarades.shinyapps.io/COVID-19-SOLES/"	71	5186	21	paultimothymooney	covid19-soles
3468	3468	NSE Data	Daily NSE Data for all Securities	['business', 'investing']		19	374	0	abhilashanil	india-vix
3469	3469	Nifty 500 Stock List	Indian Stock name and method to download the data(Notebook link attached)	['business']	"I have added this file to show how anyone can download the Indian Stock Data
Code is here https://www.kaggle.com/amarlove/stock-data-downloader please upvote if you liked it"	3	21	1	amarlove	nifty-500-stock-list
3470	3470	448tfds		[]		0	8	0	huydnglquang	448tfds
3471	3471	ADNI Y1 MCI		[]		12	11	0	lightchaser	adni-y1-mci
3472	3472	ADNI Y1 CN		[]		6	12	0	lightchaser	adni-y1-cn
3473	3473	Dataset		[]		0	4	0	vaasubisht	dataset
3474	3474	ADNI Y1 AD		[]		8	18	0	lightchaser	adni-y1-ad
3475	3475	BejaiaRegion		[]	"Data Set Information:
The dataset includes 244 instances that regroup a data of two regions of Algeria,namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region located in the northwest of Algeria.
122 instances for each region.
The period from June 2012 to September 2012.
The dataset includes 11 attribues and 1 output attribue (class)
The 244 instances have been classified into â€˜fireâ€™ (138 classes) and â€˜not fireâ€™ (106 classes) classes.
Attribute Information:
Date : (DD/MM/YYYY) Day, month ('june' to 'september'), year (2012)
Weather data observations
Temp : temperature noon (temperature max) in Celsius degrees: 22 to 42
RH : Relative Humidity in %: 21 to 90
Ws :Wind speed in km/h: 6 to 29
Rain: total day in mm: 0 to 16.8
FWI Components
Fine Fuel Moisture Code (FFMC) index from the FWI system: 28.6 to 92.5
Duff Moisture Code (DMC) index from the FWI system: 1.1 to 65.9
Drought Code (DC) index from the FWI system: 7 to 220.4
Initial Spread Index (ISI) index from the FWI system: 0 to 18.5
Buildup Index (BUI) index from the FWI system: 1.1 to 68
Fire Weather Index (FWI) Index: 0 to 31.1
Classes: two classes, namely â€œFireâ€ and â€œnot Fireâ€
References: https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++#"	0	3	1	volkanmazlum	bejaiaregion
3476	3476	PPRD Duterte Script	a President Duterte Transcript when Tulfo inverviewed	['government']		27	7	0	benzajtil	pprd-duterte-script
3477	3477	Colors of Van Gogh	Datasets for colors of 1931 of Van Gogh's paintings.	['arts and entertainment', 'art', 'earth and nature']	"Content
This dataset contains 3 dataframes and 1 folder with images. 
- The dataframe called ""color_space"" contains 988 colors (Names and RGB values) extracted from this website. It is used to reduce the amount of colors we have in the ""df"" dataset.
- The dataframe called ""df"" contains the unedited data. Web-scraped from WikiArt It contains information for 1931 paintings of Van Gohg including Name, Year painted, Genre, Style, Link for the image and 5 colors extracted from the image using Adobe Color.
- The dataframe called ""df_reduced"" is the edited version of ""df"". It contains exactly the same information as ""df"" but the 5 colors for each painting are now picked from the ""color_space"" dataframe.
- Lastly, the folder contains all of Van Gogh's paintings sorted by year in subfolders. They were acquired from WikiArt using this tool by ""lucasdavid"".
How to use this dataset
Exploratory Data Analysis
Create graphs
Inspiration
This dataset came to be as a result from my ""Starry Network"" project which can be found on my Github. It was used to create nodes and edges datasets and then using the Gephi software a graph of ""color-coexistences"" was made.
Acknowledgements
If you use this dataset please credit the author"	222	3175	28	pointblanc	colors-of-van-gogh
3478	3478	acc-model		['clothing and accessories']		0	4	0	kookheejin	accmodel
3479	3479	detoxic_original		[]		1	4	0	iamownt	detoxify-toxic-original
3480	3480	Chess Pieces Detection Images Dataset	Image Dataset for Chess Pieces Identification.	['games', 'intermediate', 'computer vision', 'deep learning', 'image data']	"Context
This can be a great Computer Vision or Multi-Class Deep Learning Project.
Content
The Folder names are self-explanator, they contain the names of the Pieces and the Images are singular images so that the dataset tidy. Multiple object images have been deleted making it an easy dataset to work with.
Inspiration
I am an amateur chess player and a Chess fan, plus I did not come across any great datasets like this on the Internet and so decided to make this one."	382	4114	41	anshulmehtakaggl	chess-pieces-detection-images-dataset
3481	3481	gambar kepala buddha		[]		7	193	0	faruqaziz	gambar-kepala-buddha
3482	3482	Edge AI Course Supplementary	supplementary materials for the Edge AI course	['education']		0	5	1	yanghaojin	edge-ai-supplementary
3483	3483	xiamen3th-b-Leaderboard		[]		1	29	2	tianwaifeixian	xiamen3th-b-leaderboard
3484	3484	helmet		['retail and shopping']		0	21	0	ryanvarghese	helmet
3485	3485	dataset		[]		3	203	0	mingchou	dataset
3486	3486	EURUSD_M15_20150101_20220125		[]		0	2	0	jassonfan	eurusd-m15-20150101-20220125
3487	3487	Medicaid Spending by Drug	 Information on spending for covered outpatient drugs	['united states', 'healthcare', 'health', 'data visualization', 'tabular data']	"The Medicaid by Drug dataset presents information on spending for covered outpatient drugs prescribed to beneficiaries enrolled in Medicaid by physicians and other healthcare professionals. 
The dataset focuses on average spending per dosage unit and change in average spending per dosage unit over time. Units refer to the drug unit in the lowest dispensable amount. It also includes spending information for manufacturer(s) of the drugs as well as consumer-friendly information of drug uses and clinical indications.
Drug spending metrics for Medicaid represent the total amount reimbursed by both Medicaid and non-Medicaid entities to pharmacies for the drug. Medicaid drug spending contains both the Federal and State reimbursement and is inclusive of any applicable dispensing fees. In addition, this total is not reduced or affected by Medicaid rebates paid to the states."	74	537	17	shiv28	medicaid-spending-by-drug
3488	3488	Campeonato Brasileiro de futebol	Campeonato Brasileiro de Futebol	['categorical data', 'data cleaning', 'text data']	"Campeonato Brasileiro de Futebol
18 anos de campeonato brasileiro de futebol
Conteúdo
No total 7645 partidas de 2003 à 2021
Github do projeto
https://github.com/adaoduque/Brasileirao_Dataset"	3084	20487	82	adaoduque	campeonato-brasileiro-de-futebol
3489	3489	data_analyst_jobs	data analyst jobs data taken from naukri.com	['india', 'text data', 'jobs and career']	"The data
this data contains jobs and skills required for those jobs in the area DATA ANALYST/DATA SCINTIST
Source
the data is taken from - 'https://www.naukri.com/' via web scrapping using python  selenium package"	22	218	4	prabinraj	data-analyst-jobs
3490	3490	Bitcoin Price History	Historical data of Bitcoin (Latest)	['artificial intelligence', 'time series analysis', 'deep learning', 'currencies and foreign exchange']	"Context
I had started to trade, learn about Cryptocurrencies and was interested to explore this data for better insights into this crpyto.
Content
The data consists of historical changes in Bitcoin price till today.
Acknowledgements
The data is from Yahoo Finance. and analysis purpose
Inspiration
Explore the Bitcoin historical data for better insights and decide whether investing in bitcoin is good for the future!"	122	633	7	pavan9065	bitcoin-price-history
3491	3491	electra-base-discriminator		[]		0	4	0	liger82	electrabasediscriminator
3492	3492	svd-3000		['sports']		0	12	0	architjain128	svd3000
3493	3493	landuse		[]		1	37	0	sergiovitale	landuse
3494	3494	VAERS - Vaccine Adverse Event Reporting System	VAERS is an early warning system to detect possible safety problems in vaccines.	['public health', 'public safety']		30	1184	1	andrej1a	vaers-vaccine-adverse-event-reporting-system
3495	3495	1000_companies_profit	1000 Companies operating cost sample data list for building regression usecases	['business', 'finance', 'marketing', 'linear regression', 'regression']	"The dataset includes sample data of 1000 startup companies operating cost and their profit. Well-formatted dataset for building ML regression pipelines.
Includes
R&D Spend          float64
Administration     float64
Marketing Spend    float64
State               object
Profit             float64"	39	220	3	rupakroy	1000-companies-profit
3496	3496	labelslai		[]		0	4	0	reichsrat	labelslai
3497	3497	Dobble High Resolution	The dataset contains images and OD annotations in COCO format for Dobble game	[]		4	55	1	atugaryov	dobble-object-detection
3498	3498	ubiquant-tabnet-baseline		[]		10	55	0	monolith0456	ubiquanttabnetbaseline
3499	3499	bhaswar-bruno		[]		0	2	0	bhaswargupta1	bhaswar-bruno-malis-1
3500	3500	funnel small		['earth and nature']		0	7	0	shigeria	funnel-small
3501	3501	2017 Uttar Pradesh Assembly Elections	Highlighting individual metric	['india', 'politics', 'tabular data']		38	221	13	sid3945	2017-uttar-pradesh-assembly-elections
3502	3502	WHOData		[]		0	9	0	ayusha232	whodata
3503	3503	US Clothing Retail Sales Dataset	Monthly clothing retail sales data from 1992	['retail and shopping']		182	3272	1	anuraggupta29	us-clothing-retail-sales-dataset
3504	3504	justin		['arts and entertainment']		3	18	0	jakin1612	justin
3505	3505	WHOData1		[]		0	1	0	ayusha232	whodata1
3506	3506	vi common voice	Vietnamese Common Voice	['text data', 'audio data']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	13	0	tuannguyenvananh	vi-common-voice
3507	3507	digitDataset		[]		0	1	0	enkhmanlaibmmn	digitdataset
3508	3508	Dataset		[]		4	2	0	randhirgittebhau	dataset
3509	3509	tabnetmodel		[]		0	10	0	peixiangong	tabnetmodel
3510	3510	roberta-large-att models		[]		1	8	0	gmhost	robertalargeatt-models
3511	3511	SmallTest		[]		0	6	0	patnareesri	smalltest
3512	3512	TOP 8 Crypto Currency Data		['currencies and foreign exchange']		8	19	1	ralarellanolopez	top-8-crypto-currency-data
3513	3513	I Don't Work Here Lady	Humor, sarcasm and fun customers stories from r/IDontWorkHereLady subreddit	['nlp', 'text data', 'retail and shopping', 'social networks']	"Context
I Don't Work Here Lady (r/IDontWorkHereLady), is a  derivative of subreddits like Tales From Retail that involves stories about folks that go into other businesses and have irate customers mistake them for employees. The collection of texts included in this subreddit is a thesaurus of English language creative usage, full of humor and sarcasm, therefore might be very interesting as a material to test your advanced NLP skills.
The data is not filtered.
Collection
Reddit posts from subreddit r/IDontWorkHereLady, downloaded from https://www.reddit.com/r/IDontWorkHereLadyusing praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
* title - relevant for posts
 score - relevant for posts - based on impact, number of comments
 id - unique id for posts/comments
 url - relevant for posts - url of post thread
 commns_num - relevant for post - number of comments to this post
 created - date of creation
 body - relevant for posts/comments - text of the post or comment
* timestamp - timestamp
Acknowledgements
All merit goes to the contributors to the posts of subreddit r/IDontWorkHereLady. I only collects them daily.
Inspiration
You can use the data to:
* Perform sentiment analysis;
* Identify discussion topics;"	26	1470	10	gpreda	i-dont-work-here-lady
3514	3514	Antiwork: Unemployment for all, not just the rich!	r/antiwork - A subreddit for those who want to end work	['demographics', 'nlp', 'text data', 'social networks']	"Context
Antiwork: Unemployment for all, not just the rich! (r/antiwork), is a subreddit for those who want to end work, are curious about ending work, want to get the most out of a work-free life, want more information on anti-work ideas and want personal help with their own jobs/work-related struggles.
<img src=""https://cdn.actorsequity.org/images/public/news/PR/DNW-MET.jpg"">
The data is not filtered.
Collection
Reddit posts from subreddit r/antiwork(Antiwork: Unemployment for all, not just the rich!), downloaded from https://www.reddit.com/r/antiwork using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
<img src=""https://workingnotworking.com/assets/share-logo-1d7fda747e04a749fc8ece04cac803bb7420bed778048216f1a2d7bd6d6cd8b5.jpg"">"	45	1084	13	gpreda	antiwork-unemployment-for-all-not-just-the-rich
3515	3515	Matches dataset		['online communities']		0	3	0	nithyalakshmis	matches-dataset
3516	3516	covid19-updated		[]		0	7	0	ayusha232	covid19-updated
3517	3517	covid19		[]		0	0	0	ayusha232	covid19
3518	3518	Diabetes data set		['diabetes']		0	4	1	nithyalakshmis	diabetes-data-set
3519	3519	datafile		['business']		0	0	0	jaswanthchakka	datafile
3520	3520	fictionlit		[]		0	11	0	fictionlit	fictionlit
3521	3521	valeur_foncier_study		[]		0	0	0	moussasavane	valeur-foncier-study
3522	3522	Never going to happen		[]		0	4	0	adriennenicole	never-going-to-happen
3523	3523	Submission1 For Text Evaluation		[]		2	16	0	electrospirit	submission1-for-text-evaluation
3524	3524	train1		[]		0	10	0	sudujr	train1
3525	3525	result		['standardized testing']		0	6	0	plyplyp	result
3526	3526	Modern Family TV Show: All Episodes Data	Data for all Modern Family episodes	['arts and entertainment', 'movies and tv shows', 'people and society', 'internet', 'tabular data']	"Context
This is one of my favorite TV shows 😄 
It ran for 11 seasons and has an IMDB rating of 8.4/10
Content
Title: The name of each episode in the show.
Airdate: The date the episode aired on TV.
Season-Episode: Season and episode number.
Rating: Rating of the episode.
Total Votes: The total number of votes the episode received.
Description: A short summary of the plot of each episode.
Image Link: Contains the image link to the cover image of each episode.
Objectives
Identify the best and worst season and episode respectively.
Identify the highest and lowest rated episodes of the show.
Identify months which released the highest and lowest number of episodes.
Look at the distributions of rating and total votes.
Generate a word cloud for the titles and descriptions of the episodes.
Display the cover images from the links in the Image Link column."	56	585	9	rprkh15	modern-family-dataset
3527	3527	urban_sound_8k	The latest dataset includes  urban sounds for machine learning audio analysis.	['music', 'cities and urban areas', 'earth and nature', 'artificial intelligence', 'classification', 'deep learning']	"UrbanSound8K
Created By
Justin Salamon^, Christopher Jacoby and Juan Pablo Bello*
* Music and Audio Research Lab (MARL), New York University, USA
^ Center for Urban Science and Progress (CUSP), New York University, USA
http://serv.cusp.nyu.edu/projects/urbansounddataset
http://marl.smusic.nyu.edu/
http://cusp.nyu.edu/
Version 1.0
Description
This dataset contains 8732 labeled sound excerpts (&lt;=4s) of urban sounds from 10 classes: air_conditioner, car_horn, 
children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, and street_music. The classes are 
drawn from the urban sound taxonomy described in the following article, which also includes a detailed description of 
the dataset and how it was compiled:
J. Salamon, C. Jacoby and J. P. Bello, ""A Dataset and Taxonomy for Urban Sound Research"", 
22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.
All excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds
(folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results
reported in the article above.
In addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided.
Audio Files Included
8732 audio files of urban sounds (see description above) in WAV format. The sampling rate, bit depth, and number of 
channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file).
Meta-data Files Included
UrbanSound8k.csv
This file contains meta-data information about every audio file in the dataset. This includes:
slice_file_name: 
The name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:
[fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken
[classID] = a numeric identifier of the sound class (see description of classID below for further details)
[occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording
[sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence
fsID:
The Freesound ID of the recording from which this excerpt (slice) is taken
start
The start time of the slice in the original Freesound recording
end:
The end time of slice in the original Freesound recording
salience:
A (subjective) salience rating of the sound. 1 = foreground, 2 = background.
fold:
The fold number (1-10) to which this file has been allocated.
classID:
A numeric identifier of the sound class:
0 = air_conditioner
1 = car_horn
2 = children_playing
3 = dog_bark
4 = drilling
5 = engine_idling
6 = gun_shot
7 = jackhammer
8 = siren
9 = street_music
class:
The class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, 
siren, street_music.
Please Acknowledge UrbanSound8K in Academic Research
When UrbanSound8K is used for academic research, we would highly appreciate it if scientific publications of works 
partly based on the UrbanSound8K dataset cite the following publication:
J. Salamon, C. Jacoby and J. P. Bello, ""A Dataset and Taxonomy for Urban Sound Research"", 
22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.
The creation of this dataset was supported by a seed grant by NYU's Center for Urban Science and Progress (CUSP).
Conditions of Use
Dataset compiled by Justin Salamon, Christopher Jacoby and Juan Pablo Bello. All files are excerpts of recordings
uploaded to www.freesound.org. Please see FREESOUNDCREDITS.txt for an attribution list.
The UrbanSound8K dataset is offered free of charge for non-commercial use only under the terms of the Creative Commons
Attribution Noncommercial License (by-nc), version 3.0: http://creativecommons.org/licenses/by-nc/3.0/
The dataset and its contents are made available on an ""as is"" basis and without warranties of any kind, including 
without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or 
completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, NYU is not 
liable for, and expressly excludes, all liability for loss or damage however and whenever caused to anyone by any use of
the UrbanSound8K dataset or any part of it.
Feedback
Please help us improve UrbanSound8K by sending your feedback to: justin.salamon@nyu.edu or justin.salamon@gmail.com
In case of a problem report please include as many details as possible."	18	322	8	rupakroy	urban-sound-8k
3528	3528	price_tag_lenta		[]		0	2	1	octoded	price-tag-lenta
3529	3529	Market Basket Example		['business']		0	9	0	psugunnasil	market-basket-example
3530	3530	test-2		[]		1	11	0	zin371	test-2
3531	3531	2014NissanAltimaPrices		[]		0	7	0	grannysmithapples	2014nissanaltimaprices
3532	3532	Bowler_IPL_2019		[]		1	25	0	lightninginbrain	bowleripl2019
3533	3533	Ekush - Bangla Handwritten Characters dataset	Multipurpose Multitype Comprehensive dataset for Bangla Handwritten Characters	['linguistics', 'computer vision', 'deep learning', 'image data']	"Ekush Dataset
Ekush: A Multipurpose and Multitype Comprehensive Database for Online Off-line Bangla Handwritten Characters. Ekush has several features:
<br>
&nbsp; 1. Characters Recognition
  &nbsp; 2. Recognition in context
  &nbsp; 3. Gender Identification
  &nbsp; 4. Forensic Investigation
  &nbsp; 5. 367,018 character instances
  &nbsp; 6. 122 character class
  &nbsp; 7. 4 captions per image
  &nbsp; 8. 186,729 Female data
  &nbsp; 9. 180,289 Male data
<br>
Dataset Credit - Ekush - Bangla Handwritten Characters Dataset
Description Credit - Shahriar Rabbi
Cover Image Credit - Ekush Logo"	0	49	0	abdullahsaihan	ekush-bangla
3534	3534	cbisddsmtfds		[]		0	3	0	huydnglquang	cbisddsmtfds
3535	3535	COVID19_CT_7930		[]		0	39	0	mustai	covid19-ct-7930
3536	3536	temp_data		[]		10	58	0	phngnamdng	temp-data
3537	3537	Efaecium_AMRC		[]		1	6	0	mswamina	efaecium-amrc
3538	3538	Dogecoin INR Dataset 2017-2022	Cryptocurrency Dogecoin price in Indian Currency Dataset	['business', 'intermediate', 'exploratory data analysis', 'time series analysis', 'currencies and foreign exchange']	"Context
This is a Dataset for Crypto Currency called Dogecoin, Currency in INR(Indian Rs).
This dataset start from 11 Nov 2017 to 28 Jan 2022 .
Source:
It was collected from Yahoo Finance
Task:
You can perform Time Series Analysis and EDA on data."	150	810	17	meetnagadia	dogecoin-inr-dataset-20172020
3539	3539	Newyork Yellow Taxi Trip Data	The Dataset consist of NYC yellow taxi trip data. 	['finance', 'travel']	"Context
The yellow  taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts. The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers authorized under the Taxicab & Livery Passenger Enhancement Programs (TPEP/LPEP). 
Content
Column Description
VendorID : A code indicating the TPEP provider that provided the record.
---- 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.
tpep_pickup_datetime : The date and time when the meter was engaged.
tpep_dropoff_datetime : The date and time when the meter was disengaged.
Passenger_count : The number of passengers in the vehicle.( This is a driver-entered value )
Trip_distance : The elapsed trip distance in miles reported by the taximeter.
PULocationID : TLC Taxi Zone in which the taximeter was engaged
DOLocationID :TLC Taxi Zone in which the taximeter was disengaged
*RateCodeID :  The final rate code in effect at the end of the trip.
---- 1= Standard rate
---- 2=JFK
---- 3=Newark
---- 4=Nassau or Westchester
---- 5=Negotiated fare
---- 6=Group ride
Store_and_fwd_flag :  This flag indicates whether the trip record was held in vehicle
memory before sending to the vendor, aka “store and forward,”
because the vehicle did not have a connection to the server.
---- Y= store and forward trip
---- N= not a store and forward trip
Payment_type A numeric code signifying how the passenger paid for the trip.
---- 1= Credit card
---- 2= Cash
---- 3= No charge
---- 4= Dispute
---- 5= Unknown
---- 6= Voided trip
Fare_amount : The time-and-distance fare calculated by the meter.
Extra : Miscellaneous extras and surcharges. Currently, this only includes
the $0.50 and $1 rush hour and overnight charges.
MTA_tax : $0.50 MTA tax that is automatically triggered based on the metered
rate in use.
Improvement_surcharge : $0.30 improvement surcharge assessed trips at the flag drop. The
improvement surcharge began being levied in 2015.
Tip_amount : Tip amount – This field is automatically populated for credit card
tips. Cash tips are not included.
Tolls_amount : Total amount of all tolls paid in trip.
Total_amount : The total amount charged to passengers. Does not include cash tips.
Acknowledgements
Data is obtained from NYCTaxi & Limousine Commission website.
https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"	1088	7596	18	microize	newyork-yellow-taxi-trip-data-2020-2019
3540	3540	leadsdata		[]		0	0	0	davidlee2022	leadsdata
3541	3541	CoNLL2003 Dataset		['earth and nature']		1	5	0	juliangarratt	conll2003-dataset
3542	3542	Romanian Baccalaureate	National exam data from years 2014-2021	['education', 'standardized testing']	Source: data.gov.ro	0	11	1	edwardliv	romanian-baccalaureate
3543	3543	dog_vid_frame		[]		1	21	0	derekcai	dog-vid-frame
3544	3544	FBI NICS Firearm Background Check Data	Number of firearm background checks initiated through the NICS 	['crime', 'data analytics', 'tabular data']	"Context
Mandated by the Brady Handgun Violence Prevention Act of 1993 and launched by the FBI on November 30, 1998, NICS is used by Federal Firearms Licensees (FFLs) to instantly determine whether a prospective buyer is eligible to buy firearms or explosives. Before ringing up the sale, cashiers call in a check to the FBI or to other designated agencies to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. More than 100 million such checks have been made in the last decade, leading to more than 700,000 denials.
Acknowledgements
The data source is the FBI's National Instant Criminal Background Check System.
https://www.fbi.gov/about-us/cjis/nics"	78	1368	9	saurabhshahane	fbi-criminal-background
3545	3545	Datos Notebook 1		[]		0	10	0	sumitkumarjethani	datos-notebook-1
3546	3546	distilroberta-finetuned-tweets-hate-speech		[]		0	8	0	dzisandy	distilrobertafinetunedtweetshatespeech
3547	3547	distilbert-base-uncased-finetuned-sst-2-english		[]		0	12	0	dzisandy	distilbertbaseuncasedfinetunedsst2english
3548	3548	twitter-roberta-base-offensive		['social networks']		0	9	0	dzisandy	twitterrobertabaseoffensive
3549	3549	www.kaggle.com/moneebkhaled/		[]		0	2	0	moneebkhaled	wwwkagglecommoneebkhaled
3550	3550	2021divvytripdataDecember		[]		0	9	0	leetdum	2021divvytripdatadecember
3551	3551	2021divvytripdataNovember		[]		0	2	0	leetdum	2021divvytripdatanovember
3552	3552	2021divvytripdataSeptember		[]		0	3	0	leetdum	2021divvytripdataseptember
3553	3553	2021divvytripdataAugust		[]		0	4	0	leetdum	2021divvytripdataaugust
3554	3554	2021divvytripdataJuly		[]		0	4	0	leetdum	2021divvytripdatajuly
3555	3555	2021divvytripdataJune		[]		0	2	0	leetdum	2021divvytripdatajune
3556	3556	2021divvytripdataMay		[]		0	6	0	leetdum	2021divvytripdatamay
3557	3557	2021divvytripdataApril		[]		0	12	0	leetdum	2021divvytripdataapril
3558	3558	2021divvytripdataMarch		[]		0	2	0	leetdum	2021divvytripdatamarch
3559	3559	2021divvytripdataFebruary		[]		0	2	0	leetdum	2021divvytripdatafebruary
3560	3560	2021divvytripdataJanuary		[]		0	10	0	leetdum	2021divvytripdatajanuary
3561	3561	Edge-IIoTset Cyber Security Dataset of IoT & IIoT	Edge-IIoTset Cyber Security Dataset 	['artificial intelligence', 'deep learning', 'dnn', 'multiclass classification', 'multilabel classification']	In this project, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the proposed testbed is organized into seven layers, including, Cloud Computing Layer, Network Functions Virtualization Layer, Blockchain Network Layer, Fog Computing Layer, Software-Defined Networking Layer, Edge Computing Layer, and IoT and IIoT Perception Layer. In each layer, we propose new emerging technologies that satisfy the key requirements of IoT and IIoT applications, such as, ThingsBoard IoT platform, OPNFV platform, Hyperledger Sawtooth, Digital twin, ONOS SDN controller, Mosquitto MQTT brokers, Modbus TCP/IP, ...etc. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, ...etc.). However, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches in both centralized and federated learning modes.	75	473	3	mohamedamineferrag	edgeiiotset-cyber-security-dataset-of-iot-iiot
3562	3562	Heartattack		[]		2	13	2	parisanahmadi	heartattack
3563	3563	test dataset	 This is a test dataset for my youtube channel.	[]	This is a test dataset for my youtube channel.	0	6	1	amirshnll	test-dataset
3564	3564	housed1		[]		0	1	0	assaadchiboub	housed1
3565	3565	Data of Credit_EDA_Data_Set		['business']		1	10	0	suryamanoj	data-of-credit-eda-data-set
3566	3566	Nigeria Districts Dataset	Nigeria Districts Dataset	[]		0	6	0	adamdini	nigeria-districts-data
3567	3567	mlp model for ump		[]		0	17	0	takezo	mlp-model-for-ump
3568	3568	Credit_EDA		[]		0	16	0	suryamanoj	credit-eda
3569	3569	NLP_sample_text		[]		0	8	0	virajoak	nlp-sample-text
3570	3570	Oxford-IIIT Pet Dataset		[]		0	10	0	vedantvijaydalimkar	oxford-iiit-pet-dataset
3571	3571	mermaids test		['religion and belief systems']		1	376	1	wgdesign2	mermaids-test
3572	3572	Car Price	Database for Regression	['random forest', 'xgboost', 'regression', 'ratings and reviews']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	8	65	1	domenicomorabito	carprice
3573	3573	Cancer Photos Dataset		['cancer']		0	8	0	rasankpatro	cancer-photos-dataset
3574	3574	class-playlist		['education']		1	10	0	aengelmann	classplaylist
3575	3575	Ratings of the most popular anime	Ratings of 100 most popular anime on myanimest.net	['comics and animation', 'anime and manga']	"Context
myanimelist.net is the most popular site where fans of Japanese animated films and series share their opinions on various productions. The portal works in a similar way to IMDb and allows users to rate various positions and then create various types of rankings based on them. In this database you will find a distribution of votes on a scale from 1 to 10 for the 100 most popular (most often cast) anime votes from a specific point in time.
Content
The data was obtained using webscraping. The Python language with the ""BeautifulSoup"", ""requests"", ""re"", ""pandas"" and ""numpy"" packages was used for this process and ""SelectorGadet"" add-on, which made the work with the site easier. For each movie or series, we have 10 lines in turn with each rating and number of votes assigned to it, and some other information related to the series / movie.
Inspiration
The data can be used to check the distribution of ratings between individual series / movies. We can check whether the final average results from a large, relatively high score or maybe from the bi-larity of the distribution. Dana can be an addition to other, often already old, datasets for subordinate topics (e.g. Anime recommendations database and Anime dataset).
Features
Votes (Number of votes)
Score (Score: 10 - the best, 1 - the worst)
English_name (Title in english)
Favourites_count (Number of users who add series to favourites)
Popularity_ranking (Position in popularity ranking - the lowest the more users votes)
Photo by <a href=""https://unsplash.com/@dexezekiel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Dex Ezekiel</a> on <a href=""https://unsplash.com/s/photos/anime?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	159	1688	20	michau96	ratings-of-the-most-popular-anime
3576	3576	Horse Survival Dataset	Horse Survival Prediction - Classification Problem	['healthcare', 'health', 'beginner', 'tabular data', 'health conditions']	"Description:
Predict whether or not a horse can survive based upon past medical conditions.
Noted by the ""outcome"" variable in the data.
Content:
All of the binary representation have been converted into the words they actually represent. However, a fuller description is provided by the data dictionary (datadict.txt).
There are a lot of NA's in the data. This is the real struggle here. Try to find a way around it through imputation or other means.
Attribute Information:
1: surgery? 
1 = Yes, it had surgery 
2 = It was treated without surgery 
2: Age 
1 = Adult horse 
2 = Young (&lt; 6 months) 
3: Hospital Number 
- numeric id 
- the case number assigned to the horse (may not be unique if the horse is treated &gt; 1 time) 
4: rectal temperature 
- linear 
- in degrees celsius. 
- An elevated temp may occur due to infection. 
- temperature may be reduced when the animal is in late shock 
- normal temp is 37.8 
- this parameter will usually change as the problem progresses, eg. may start out normal, then become elevated because of the lesion, passing back through the normal range as the horse goes into shock 
5: pulse 
- linear 
- the heart rate in beats per minute 
- is a reflection of the heart condition: 30 -40 is normal for adults 
- rare to have a lower than normal rate although athletic horses may have a rate of 20-25 
- animals with painful lesions or suffering from circulatory shock may have an elevated heart rate 
6: respiratory rate 
- linear 
- normal rate is 8 to 10 
- usefulness is doubtful due to the great fluctuations 
7: temperature of extremities 
- a subjective indication of peripheral circulation 
- possible values: 
1 = Normal 
2 = Warm 
3 = Cool 
4 = Cold 
- cool to cold extremities indicate possible shock 
- hot extremities should correlate with an elevated rectal temp. 
8: peripheral pulse 
- subjective 
- possible values are: 
1 = normal 
2 = increased 
3 = reduced 
4 = absent 
- normal or increased p.p. are indicative of adequate circulation while reduced or absent indicate poor perfusion 
9: mucous membranes 
- a subjective measurement of colour 
- possible values are: 
1 = normal pink 
2 = bright pink 
3 = pale pink 
4 = pale cyanotic 
5 = bright red / injected 
6 = dark cyanotic 
- 1 and 2 probably indicate a normal or slightly increased circulation 
- 3 may occur in early shock 
- 4 and 6 are indicative of serious circulatory compromise 
- 5 is more indicative of a septicemia 
10: capillary refill time 
- a clinical judgement. The longer the refill, the poorer the circulation 
- possible values 
1 = &lt; 3 seconds 
2 = &gt;= 3 seconds 
11: pain - a subjective judgement of the horse's pain level 
- possible values: 
1 = alert, no pain 
2 = depressed 
3 = intermittent mild pain 
4 = intermittent severe pain 
5 = continuous severe pain 
- should NOT be treated as a ordered or discrete variable! 
- In general, the more painful, the more likely it is to require surgery 
- prior treatment of pain may mask the pain level to some extent 
12: peristalsis 
- an indication of the activity in the horse's gut. As the gut becomes more distended or the horse becomes more toxic, the activity decreases 
- possible values: 
1 = hypermotile 
2 = normal 
3 = hypomotile 
4 = absent 
13: abdominal distension 
- An IMPORTANT parameter. 
- possible values 
1 = none 
2 = slight 
3 = moderate 
4 = severe 
- an animal with abdominal distension is likely to be painful and have reduced gut motility. 
- a horse with severe abdominal distension is likely to require surgery just tio relieve the pressure 
14: nasogastric tube 
- this refers to any gas coming out of the tube 
- possible values: 
1 = none 
2 = slight 
3 = significant 
- a large gas cap in the stomach is likely to give the horse discomfort 
15: nasogastric reflux 
- possible values 
1 = none 
2 = &gt; 1 liter 
3 = &lt; 1 liter 
- the greater amount of reflux, the more likelihood that there is some serious obstruction to the fluid passage from the rest of the intestine 
16: nasogastric reflux PH 
- linear 
- scale is from 0 to 14 with 7 being neutral 
- normal values are in the 3 to 4 range 
17: rectal examination - feces 
- possible values 
1 = normal 
2 = increased 
3 = decreased 
4 = absent 
- absent feces probably indicates an obstruction 
18: abdomen 
- possible values 
1 = normal 
2 = other 
3 = firm feces in the large intestine 
4 = distended small intestine 
5 = distended large intestine 
- 3 is probably an obstruction caused by a mechanical impaction and is normally treated medically 
- 4 and 5 indicate a surgical lesion 
19: packed cell volume 
- linear 
- the # of red cells by volume in the blood 
- normal range is 30 to 50. The level rises as the circulation becomes compromised or as the animal becomes dehydrated. 
20: total protein 
- linear 
- normal values lie in the 6-7.5 (gms/dL) range 
- the higher the value the greater the dehydration 
21: abdominocentesis appearance 
- a needle is put in the horse's abdomen and fluid is obtained from 
the abdominal cavity 
- possible values: 
1 = clear 
2 = cloudy 
3 = serosanguinous 
- normal fluid is clear while cloudy or serosanguinous indicates a compromised gut 
22: abdomcentesis total protein 
- linear 
- the higher the level of protein the more likely it is to have a compromised gut. Values are in gms/dL 
23: outcome 
- what eventually happened to the horse? 
- possible values: 
1 = lived 
2 = died 
3 = was euthanized 
24: surgical lesion? 
- retrospectively, was the problem (lesion) surgical? 
- all cases are either operated upon or autopsied so that this value and the lesion type are always known 
- possible values: 
1 = Yes 
2 = No 
25, 26, 27: type of lesion 
- first number is site of lesion 
1 = gastric 
2 = sm intestine 
3 = lg colon 
4 = lg colon and cecum 
5 = cecum 
6 = transverse colon 
7 = retum/descending colon 
8 = uterus 
9 = bladder 
11 = all intestinal sites 
00 = none 
- second number is type 
1 = simple 
2 = strangulation 
3 = inflammation 
4 = other 
- third number is subtype 
1 = mechanical 
2 = paralytic 
0 = n/a 
- fourth number is specific code 
1 = obturation 
2 = intrinsic 
3 = extrinsic 
4 = adynamic 
5 = volvulus/torsion 
6 = intussuption 
7 = thromboembolic 
8 = hernia 
9 = lipoma/slenic incarceration 
10 = displacement 
0 = n/a 
28: cp_data 
- is pathology data present for this case? 
1 = Yes 
2 = No 
- this variable is of no significance since pathology data is not included or collected for these cases
Acknowledgements:
This dataset was originally published by the UCI Machine Learning Database:\
http://archive.ics.uci.edu/ml/datasets/Horse+Colic
Objective:
Understand the Dataset & cleanup (if required).
Build classification model to predict weather the horse will survive or not.
Also fine-tune the hyperparameters & compare the evaluation metrics of vaious classification algorithms."	135	1080	15	yasserh	horse-survival-dataset
3577	3577	ridge_ucblab_cleanexp2_fasttext_oof		[]		0	2	0	shobhitupadhyaya	ridge-ucblab-cleanexp2-fasttext-oof
3578	3578	Data Science Job Listings - Australia - 2019-2021	Data Scientist Job Listings from Seek.com.au	['computer science', 'programming', 'beginner', 'data visualization']	"Context
Which programming language do you think is most vital for a data scientist to master? Is it best to know a bit of many languages, or specialise? I wondered the same things, so I decided to ask the job market -  this dataset comprises every Australian data science job listing for a year! 
The dataset is a collection of every search result for data scientist along with each of 25 programming languages and software applications. It provides over 50 columns of rich numeric, geographic and text data to explore.
Content
This dataset answers interesting questions you've always wanted to know but didn't want to ask, like:
Which industries are data scientists happiest in? 
How much do data scientists earn? 
Can I come from overseas and work in Australia as a data scientist? 
This dataset is great for:
Text analysis / NLP (see the Job Description column),
Geospatial analysis (every job has a location)
Visualisation
Note that this dataset is very beginner-friendly, but also has some challenging wrangling to keep advanced scientists challenged and entertained.
Inspiration
This dataset is inspired by a desire to know more about the world we live in."	387	3517	12	nomilk	data-science-job-listings-australia-20192020
3579	3579	G-Research Crypto Forecasting Data		[]		0	32	0	mmelahi	gresearch-crypto-forecasting-data
3580	3580	Train_data PICKLE FORMAT Ubiquant (3.5 GB)		['food']		1	15	0	maherelouahabi	train-data-pickle-format-ubiquant-35-gb
3581	3581	mymusicalprefrences		[]		0	27	0	chengbin1998	mymusicalprefrences
3582	3582	dataset		[]		0	9	0	shabazz	dataset
3583	3583	validation2-3		[]		3	17	0	yuzhoudiyishuai	validation23
3584	3584	Relationships		[]		0	41	0	anapowell	relationships
3585	3585	Olive_dataset2		[]		0	6	0	majedalatawi	olive-dataset2
3586	3586	stock_trading_data	The dataset contains stock trading data properly formatted with datetime.	['advanced', 'time series analysis', 'deep learning', 'investing', 'datetime']	"The dataset contains stock trading data properly formatted with datetime. Helpful for quick Time Series debugging and sequential learning process. 
The dataset contains:
DateTime.
Opening Stock
High.
Low
Closing Stock."	164	1355	11	rupakroy	stock-trading-data
3587	3587	Nuclio Prophet TS DF		[]		1	2	0	npscul	nuclio-prophet-ts-df
3588	3588	refereescareerMadridexplainingvariables		[]		0	6	0	ialiende	refereescareermadridexplainingvariables
3589	3589	Song popularity 5 folds		[]		0	27	3	sahilrajpal121	song-popularity-5-folds
3590	3590	Ankurs_Finance_Stock_investment		[]	"You are provided with the following information for 24 stocks of leading companies listed in New York Stock Exchange(NYSE):
Date
Open price: Price of stock at the start of the day
Close price: Price of stock at the end of the day
High price: Highest price reached by the stock on that day
Low price: Lowest price reached by the stock on that day
Adjusted close price: Stock price adjusted to include the annual returns (dividends) that the company offers to the shareholders
Volume traded: Number of stocks traded on the day
The information for every stock ranges from 1st October 2010 to 30th September 2020.
The stocks belong to different domains:
Technology/IT
Travel/Aviation/Hospitality
Banking/Financial Services and Insurance
Pharmaceuticals/Healthcare/Life Sciences
To help you with the market benchmark, you are given the S&P 500 index prices for the same period."	0	10	0	ankurnapa	ankurs-finance-stock-investment
3591	3591	Divvy_bike_data_clean_2021	2021 Bike Share Data from Divvy Bike	['united states', 'cycling', 'beginner', 'exploratory data analysis', 'tabular data']		0	9	0	swatsonds	divvy-bike-data-clean-2021
3592	3592	airline_passengers	Well formatted time series dataset for time series or any sequential learning	['computer science', 'advanced', 'time series analysis', 'simulations', 'datetime']	"Well structured and formatted time series dataset for easy time-series debugging  or any sequential learning.
Dataset includes:
DateTime: in the year and month format.
Count of Passengers"	87	882	9	rupakroy	airline-passengers
3593	3593	tez-fb-large		['online communities']		17	26	2	devanshchowdhury	tezfblarge
3594	3594	longformerlarge4096		[]		20	11	1	devanshchowdhury	longformerlarge4096
3595	3595	pool_Horario		[]		0	6	0	caralosal	pool-horario
3596	3596	fblongformerlarge1536		[]		13	13	0	devanshchowdhury	fblongformerlarge1536
3597	3597	Military and Civilian Vehicles		['military']		2	24	0	thynguynthu	military-and-civilian-vehicles
3598	3598	challenge		['arts and entertainment']		4	7	0	vigneshirtt	challenge
3599	3599	longformer-large		[]		0	15	0	devanshchowdhury	longformerlarge
3600	3600	pbvs_sar		[]		1	5	0	adityakane	pbvs-sar
3601	3601	pbvs_eo		[]		2	6	0	adityakane	pbvs-eo
3602	3602	g2-2-30		[]		0	7	0	thisisshahzadkhan	g2230
3603	3603	tez_fb_large		[]		0	5	0	devanshchowdhury	tez-fb-large
3604	3604	tez-lib		[]		1	5	0	devanshchowdhury	tezlib
3605	3605	fblongformerlarge		[]		0	2	0	devanshchowdhury	fblongformerlarge
3606	3606	cbnetv2-dc		[]		0	23	0	ccvipchenbin	cbnetv2dc
3607	3607	original		[]		0	0	0	abrahamanderson	original
3608	3608	changed_views		[]		0	1	0	abrahamanderson	changed-views
3609	3609	emails data		['email and messaging']		0	8	0	marwanmakhlouf	emails-data
3610	3610	nothingcanbeexplained		[]		0	162	0	yus002	nothingcanbeexplained
3611	3611	Overall percentage	Overallpercentage of class	[]		5	24	1	sandhyasekar01	overall-percentage
3612	3612	catdataset		[]		0	6	0	hichammoujahid	catdataset
3613	3613	dataset cat		['earth and nature']		0	3	0	hichammoujahid	dataset-cat
3614	3614	test_dataset		[]		0	0	0	hichammoujahid	test-dataset
3615	3615	cell-seg-pth		[]		0	12	0	banbeipi	cellsegpth
3616	3616	dataset traffic signs		[]		2	14	0	elmehdianiq	dataset-traffic-signs
3617	3617	Survey_Monkey_Initial_Data		[]		0	1	0	alexleustean	survey-monkey-initial-data
3618	3618	ridge_ucblab_noclean_fasttext_oof		[]		0	4	0	shobhitupadhyaya	ridge-ucblab-noclean-fasttext-oof
3619	3619	shapefile		[]		0	1	0	kashishgajwani	shapefile
3620	3620	attention_mask_rcnn		[]		4	8	0	ccvipchenbin	attention-mask-rcnn
3621	3621	cadiffnlp/bertweet-base-offensive		[]		0	10	2	datafan07	cadiffnlpbertweetbaseoffensive
3622	3622	Natural Disaster Analysis		[]		9	39	0	kashishgajwani	natural-disaster-analysis
3623	3623	lukisan sang seniman		[]		0	9	0	yogieeka	lukisan-sang-seniman
3624	3624	traincsv		[]		2	15	0	seongjin24	traincsv
3625	3625	Art Images		['art']		2	22	0	yogieeka	art-images
3626	3626	twitter_tx_bert		[]		0	3	1	datafan07	twitter-tx-bert
3627	3627	cycleGAN_32000		[]		1	8	0	huangkailong	cyclegan-32000
3628	3628	Wipro's Sustainability Machine Learning Challenge		['earth and nature', 'education']		4	111	5	gauravduttakiit	wipros-sustainability-machine-learning-challenge
3629	3629	datasetscat		[]		0	0	0	meryamassermouh	datasetscat
3630	3630	dacon_image_256		[]		1	16	0	eunsongcheon	dacon-image-256
3631	3631	ubiquant-autocorr		[]		0	10	0	farcii	ubiquantautocorr
3632	3632	Maharashtra 2014 Assembly Election details	Partywise and constituency statistics	['india', 'politics', 'tabular data']	"Context
This dataset is made public by the Election Commission of India. It is compiled in its current form by the team of TCPD (the center for political data)."	19	144	12	sid3945	maharashtra-2014-assembly-election-details
3633	3633	Telecom_churn		[]		0	1	0	whiteoran	telecom-churn
3634	3634	Credit EDA Assignment		['lending']		0	16	1	crysllobo	credit-eda-assignment
3635	3635	pokemanyolox5		[]		0	9	0	rundonggao	pokemanyolox5
3636	3636	fakefaces_cropped64		[]		0	9	0	batart	fakefaces-cropped64
3637	3637	stations_info		[]		0	7	0	himanshikapurwar	stations-info
3638	3638	unitary unbiased toxic roberta model		[]		0	13	0	temple520	unitary-unbiased-toxic-roberta-model
3639	3639	ihmxia		[]		0	11	0	gaspardgoupy	ihmxia
3640	3640	tf-xlm-roberta-large-download		['arts and entertainment']		0	3	0	vigneshirtt	tfxlmrobertalargedownload
3641	3641	Football referees by postcode in Madrid	Registered referees 1991-2021	['football']		0	5	0	ialiende	football-referees-by-postcode-in-madrid
3642	3642	Petrol Prices In India 	Petrol Prices Of last 4 Years Of India	['india', 'beginner', 'data visualization', 'text data', 'oil and gas']	"Context
Data file Shows The data about fuel rate in india
Content
nothing simple data set
Acknowledgements
thanks to petrolprice .com to provide data
Inspiration
nothing"	688	3751	19	sandipdevre	petrol-prices-in-india
3643	3643	User-Behavior eCommerce - Sequential Session Data		['e-commerce services']		15	214	2	hariwh0	userbehavior-ecommerce-sequential-session-data
3644	3644	BookRecommendation	Dataset for data analysis, prediction, and filtering 	[]		0	6	0	sabingautam	bookre
3645	3645	Youtube WorldWide Trends		[]		4	23	1	jeevaananth	youtube-worldwide-trends
3646	3646	[TR-EN] Dictionary	Turkish English Dictionary	['languages', 'text data']	"Context
Turkish - English dictionary in different usage cases. Translations were obtained from tureng which is a reliable source.
Content
The dataset contains three columns which are;
turkish: The Turkish word/saying
english: The translation of the Turkish word/saying
category: Usage category"	8	104	5	oktayozturk010	tren-dictionary
3647	3647	scaler9		[]		3	13	0	valentinzastrozhny	scaler9
3648	3648	model9		[]		1	7	0	valentinzastrozhny	model9
3649	3649	pcap_asterisk_kamailio		[]		3	24	0	huthytrn	pcap-asterisk-kamailio
3650	3650	ipyplot_install	ipyplot offline installation	['computer science']	"Context
Steps to install:
1. Import this dataset in notebook
2. Execute !pip install /kaggle/input/ipyplot-install/ipyplot-1.1.0-py3-none-any.whl
3. import ipyplot"	0	23	3	mahipalsingh	ipyplot-install
3651	3651	pytorch_train		[]		0	8	0	devanshchowdhury	pytorch-train
3652	3652	Youtube Country Codes		[]		0	11	0	jeevaananth	youtube-country-codes
3653	3653	Titanic dataset	Thisi Titanic dataset	['exploratory data analysis', 'data visualization', 'matplotlib', 'pandas', 'seaborn']	"Overview
This is Titanic dataset 
Data Dictionary
| Attributes | Definition | Key |
| --- | --- | ---|
| sex |  Sex/Gender| male/female |
| age | Age |  |
| sibsp | siblings of the passenger | 0/1 /2 ... |
| parch | parents / children aboard the Titanic | 0/1/2 ... |
| fare | Passenger fare |  |
| embarked | Port of Embarkation | C : Cherbourg, Q : Queenstown, S : Southampton |
| class | Ticket class | First / Second / Third |
| who | categories to passengers | male, female, child |
| alone | he was alone in ship or no | 0/1 |
| survived | | 0/1 |"	91	585	3	ibrahimelsayed182	titanic-dataset
3654	3654	TamilNadu_Covid 19 Tracker	Number of cases (by date) Till 27-01-2022 	['law']		0	13	1	logeshkumar04	tn-covid-cases-27-01-2022
3655	3655	68personBMP		[]		0	15	0	patnareesri	68personbmp
3656	3656	68Person		[]		2	3	0	amphancm	68person
3657	3657	axion_ds_mass_250k_model_stage1		[]		0	11	0	renessmi2017	axion-ds-mass-250k-model-stage1
3658	3658	MyTestPic		[]		0	0	0	kugolashih	mytestpic
3659	3659	10 Basic Strides to create appealing essay		[]		0	27	0	klaraloft	10-basic-strides-to-create-appealing-essay
3660	3660	Malaysian Image Classification		[]		0	12	0	naqibaqil	malaysian-image-classification
3661	3661	ner_relationship_extraction		[]		1	9	0	swapnilpote	ner-relationship-extraction
3662	3662	AirPassengers		[]		0	5	0	donghansohn	airpassengers
3663	3663	titanic_labels		[]		0	1	0	jackgong	titanic-labels
3664	3664	test_test		[]		2	105	1	bigcat111	test-test
3665	3665	Object Detection and Drivable Lane Masking	Dataset for Self-Driving Car	['artificial intelligence', 'software', 'automobiles and vehicles', 'image data']	"The original dataset is BDD100k.
This dataset contains object detection and drivable lane masking for 100k images in the original dataset."	58	67	1	nguyentrongquocdat	object-detection-and-drivable-lane-masking
3666	3666	player_matches_500		[]		2	10	0	olegdudnik	player-matches-500
3667	3667	AlgoSeerData2		[]		6	22	0	kanakalathavemuru	algoseerdata2
3668	3668	mnist-2d.csv	2d version of MNIST dataset which consist of 784 dimensions, reduced using t-SNE	['computer science']		0	9	1	egyfirst	mnist-2d
3669	3669	bert-sa-pretrained		[]		0	12	0	mlcovidresearch	bertsapretrained
3670	3670	road anomaly outputs		[]		4	16	0	shashwatnaidu07	road-anomaly-outputs
3671	3671	Fire and Smoke BBox COCO Dateset	Dataset to detect indoor fire and smoke with bounding box	['arts and entertainment']		19	49	1	deeplearn1	fire-and-smoke-bbox-coco-dateset
3672	3672	Tabular Playground Series		['earth and nature']		0	14	0	coulson1922	tabular-playground-series
3673	3673	bilstm		[]		0	0	0	yejinhwang	bilstm
3674	3674	Online Retail For Market Basket Analysis		['business']	"Data Set Information:
This is a transnational data set that contains all the transactions occurring `between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Most customers of the company are wholesalers**.
Attribute Information:
InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.
StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.
Description: Product (item) name. Nominal.
Quantity: The quantities of each product (item) per transaction. Numeric.
InvoiceDate: Invice  Date and time. Numeric, the day and time when each transaction was generated.
UnitPrice: Unit price. Numeric, Product price per unit in sterling.
CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.
Country: Country name. Nominal, the name of the country where each customer resides.
Source
http://archive.ics.uci.edu/ml/datasets/online+retail#"	20	230	4	yekahaaagayeham	online-retail-for-market-basket-analysis
3675	3675	WORLD ECONOMIC AND FINANCIAL	World Economic Database	['economics', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	The World Economic Outlook (WEO) database contains selected macroeconomic data series from the statistical appendix of the World Economic Outlook report, which presents the IMF staff's analysis and projections of economic developments at the global level, in major country groups and in many individual countries. The WEO is released in April and September/October each year.	69	288	1	kuntalmaity	world-economic-and-financial
3676	3676	fs static outputs		[]		2	12	0	shashwatnaidu07	fs-static-outputs
3677	3677	News Headlines from Malaysia	A set of 63,692 news headlines from Malaysia.	['asia', 'categorical data', 'lstm', 'news', 'keras']	"Context
A set of 63,692 news headlines from Malaysia. There are 2 columns: news headline column and sentiment column. Specifically, there are 3 types of sentiments:
- Negative (-1)
- Neutral (0)
- Positive (1)
Distribution
The distribution of number of news headline with sentiments according to the training, validation and test sets is as follows:
| Sentiments | Training Set | Validation Set | Test Set | Total |
| --- | --- | --- | --- | --- |
| Negative | 13,037 | 237 | 256 | 13,530 |
| Neutral | 24,033 | 217 | 305 | 24,555 |
| Positive | 13,037 | 546 | 439 | 25,607 |
| Total | 61,692 | 1,000 | 1,000 | 63,692 |"	3	20	0	haojie98	news-headline
3678	3678	fs lost and found outputs		[]		6	9	0	shashwatnaidu07	fs-lost-and-found-outputs
3679	3679	Credit_data	The data includes credit default behavior highly useable for debugging ML models	['finance', 'banking', 'computer science', 'advanced', 'ensembling', 'tabular data']	The dataset includes well-distributed statistics of income, age, previous loan amount as well their past behaviors with reference to defaults, for faster machine learning debugging processes like stacking regressor, stacking classifier, also included as notebook with the dataset	510	3217	19	rupakroy	credit-data
3680	3680	longformer-v1		[]		1	29	0	ningyt	longformerv1
3681	3681	roberta_v2		[]		0	7	0	tonymarkchris	roberta-v2
3682	3682	caltech_birds2010		[]		0	6	0	isurualagiyawanna	caltech-birds2010
3683	3683	Suduko Image with Solution	data Contains the Suduko Image and Solutions in the csv file	['online communities']		3	39	3	amarlove	suduko-image-with-solution
3684	3684	corpuscomenta		[]		0	1	0	fernandobordi	corpuscomenta
3685	3685	corpusnoticiasLDA		[]		0	7	0	fernandobordi	corpusnoticiaslda
3686	3686	pokeman-yolox-4		[]		0	1	0	rundonggao	pokemanyolox4
3687	3687	Suduko_Template		[]		0	2	2	amarlove	suduko-template
3688	3688	255645		[]		0	0	0	qcq123456789	255645
3689	3689	325648		[]		0	0	0	qcq123456789	325648
3690	3690	simple toy data for pla		[]		0	10	0	llyy88	simple-toy-data-for-pla
3691	3691	imageblending2		[]		0	5	0	mxtson	imageblending2
3692	3692	blending-test		[]		0	9	0	mxtson	blendingtest
3693	3693	325652		[]		0	7	0	qcq123456789	325652
3694	3694	Proyecto CoderHouse.		[]		0	6	0	axelsanchez	proyecto-coderhouse
3695	3695	MUSHROOM CLASSIFICATION DATASET		['biology']		0	21	0	mustai	mushroom-8910
3696	3696	Self_driving_car_dataset		[]	"In this dataset, you will find two varieties; one from the benchmark KITTI dataset that contains complex road scenes and a high level of occlusion and truncation in road objects. On the other hand, the Waymo dataset gives versatile scenes from extreme weather and night timings. 
All relevant information regarding data usage is given in the readMe.txt file.
In KITTI all frames are saved in one place with different scenes; however, for Waymo, we have collected data from different segments to make the variety so data can be seen in 12 different folders with all necessary files. Around 2400 frames are considered from each dataset.
Data is collected from two sensors camera and LiDAR. For information about labeling format, calibration and mapping, please check readMe.txt.
For complete KITTI dataset please see the link http://www.cvlibs.net/datasets/kitti/raw_data.php.
For complete Waymo dataset please see the link https://waymo.com/intl/en_us/dataset-download-terms"	4	55	0	sabeeha	self-driving-car-dataset
3697	3697	Chinese Calligraphy Characters Image Set	For Script Type And Character recognition	['arts and entertainment']	"Context
Original calligraphies come from The Metropolitan Museum of Art. All the calligraphies are claimed to be in public domain.
Seal （篆书）
吳煕載 篆書詩經南山有臺
https://www.metmuseum.org/art/collection/search/75814?searchField=All&sortBy=Relevance&ft=seal-script&offset=80&rpp=20&pos=95
clerical (隶书)
北宋 佚名 雜阿含經卷第二十五 卷
https://www.metmuseum.org/art/collection/search/39914
cursive script (草書)
huang-tingjian
https://www.metmuseum.org/art/collection/search/39918
mi-fu
https://www.metmuseum.org/art/collection/search/39919
semi-cursive script (行書)
zhao-mengjian(趙孟堅)
https://www.metmuseum.org/art/collection/search/40285
regular script (楷書)
zhong-shaojing
https://www.metmuseum.org/art/collection/search/36439"	1	43	0	bai224	chinese-calligraphy-characters-image-set
3698	3698	CovidSentiPreprocessed		[]		0	0	0	sundasrukhsar	covidsentipreprocessed
3699	3699	Turkish Poems	Popular Turkish Poems from different poets with popularity ranks	['culture and humanities', 'literature', 'europe', 'nlp', 'text data']	"Context
This dataset includes poems from popular Turkish poets. All the data is scraped from poetry pages. ( Bu veri seti popüler şairklerin Türkçe şiirlerden oluşmaktadır. Şiirler internet sitelerinden toplanmıştır.)
Disclaimer: The file might contain missing poems, poem text in different languages, or incorrect poems.
Content
Poems are a crucial part of a culture. It represents the language of emotions which makes this dataset interesting for answers of many questions.
Inspiration
This dataset is a great resource for applying NLP techniques. Many different methods from basic analysis like wordcloud to lyrics generation with LSTM neural networks can be applied."	0	26	0	emreokcular	turkish-poems
3700	3700	lets get to work		['business']		0	14	0	evanbedford2	lets-get-to-work
3701	3701	sample		[]		0	16	0	derekcai	sample
3702	3702	Unitat2		[]		0	18	0	lorenzoescutia	unitat2
3703	3703	covid data from south america and brazil		[]		2	35	1	calvimaoyama	covid-data-from-south-america-and-brazil
3704	3704	EasyEnsembleClassifier		[]		0	6	0	lionelbottan	easyensembleclassifier
3705	3705	Font Styles Dataset		[]		0	1	0	dschettler8845	font-styles-dataset
3706	3706	RandomForest_ro		[]		0	8	0	lionelbottan	randomforest-ro
3707	3707	Flower Classification App		[]		1	4	0	nadaalashi	flower-classification-app
3708	3708	Telecom_Churn_Case_Study		[]		0	14	0	rahulsrivastavapro	telecom-churn-case-study
3709	3709	sirius		[]		0	9	0	acforvs	sirius
3710	3710	Promo_Sveta_Nedelja Brezje/Bestovje sequel	Nema granice kad su tu dve oran'ce	['atmospheric science', 'business', 'text data']		3	124	6	leoklis	restorani
3711	3711	svm_ru		[]		0	6	0	lionelbottan	svm-ru
3712	3712	randomforest_ru		[]		0	14	0	lionelbottan	randomforest-ru
3713	3713	disaster		[]		6	6	0	faustino2020	disaster
3714	3714	adaBoostClassifier_ru		[]		0	3	0	lionelbottan	adaboostclassifier-ru
3715	3715	djone1		[]		0	0	0	mtang123	djone1
3716	3716	dt_clf_ru		[]		0	8	0	lionelbottan	dt-clf-ru
3717	3717	LogRef_ru		[]		0	5	0	lionelbottan	logref-ru
3718	3718	LogRef_1		[]		0	6	0	lionelbottan	logref-1
3719	3719	bruhElec		[]		0	19	0	aliibrahim522	bruhelec
3720	3720	IBM Customer Data		[]		2	32	0	lingyichen0924	ibm-customer-data
3721	3721	dashboard		[]		0	4	0	mithranseralathan	dashboard
3722	3722	non_hispanic_american_indian	CDC’s Adult Physical Inactivity Prevalence Data	['exercise', 'health']	"Context
Prevalence of Self-Reported Physical Inactivity Among Non-Hispanic American Indian/Alaska Native Adults by State and Territory, BRFSS, 2017–2020
Content
The content of this dataset reveals valuable information about physical inactivity among non-Hispanic American Indian/Alaska Native Adults by state and territory.
Acknowledgements
Content source: Division of Nutrition, Physical Activity, and Obesity, National Center for Chronic Disease Prevention and Health Promotion
Inspiration
This dataset helped me to get more insights in order to analyze FitBit Fitness Tracker Data for my Bellabeat Analysis"	2	9	0	parvanekhosravizade	non-hispanic-american-indian
3723	3723	non_hispanic_black	Prevalence of Self-Reported Physical Inactivity Among Non-Hispanic Black Adults 	['exercise', 'health']	"Context
Prevalence of Self-Reported Physical Inactivity Among Non-Hispanic Black Adults by State and Territory, BRFSS, 2017–2020
Content
The content of this dataset reveals valuable information about physical inactivity among non-Hispanic black adults by state and territory.
Acknowledgements
Content source: Division of Nutrition, Physical Activity, and Obesity, National Center for Chronic Disease Prevention and Health Promotion
Inspiration
This dataset helped me to get more insights in order to analyze ""FitBit Fitness Tracker Data"" notebook for my Bellabeat Analysis"	1	9	0	parvanekhosravizade	non-hispanic-black
3724	3724	Divvy_Bike_data_2021_raw	All 2021 Bike Share Data from Divvy Bike combined into one csv file	['united states', 'sampling', 'transportation', 'beginner', 'tabular data']		0	6	0	swatsonds	divvy-bike-data-2021-raw
3725	3725	hispanic_adults	CDC’s Adult Physical Inactivity Prevalence Dataset	['health']	"Context
Prevalence of Self-Reported Physical Inactivity Among Hispanic Adults by State and Territory, BRFSS, 2017–2020
Content
The content of this dataset reveals valuable information about physical inactivity among Hispanic Adults by state and territory.
Acknowledgements
Content source: Division of Nutrition, Physical Activity, and Obesity, National Center for Chronic Disease Prevention and Health Promotion
Inspiration
This dataset helped me to get more insights in order to analyze ""FitBit Fitness Tracker Data"" notebook for my Bellabeat Analysis;"	1	15	0	parvanekhosravizade	hispanic-adults
3726	3726	Global threatened species	Countrywise timeseries data of threatened terrestrial and marine species	['global', 'environment', 'data cleaning', 'data visualization', 'time series analysis', 'data analytics']	"To understand the biodiversity problem we need to know how many species are under pressure; where they are; and what the threats are. To do this, the Organisation for Economic Co-operation and Development (OECD) evaluates species across the world for their level of extinction risk. It does this evaluation every year, and continues to expand its coverage.
The presented global time-series datasets contain precentage values of preserved terrestrial and marine region, Red List index*, and the count of vunerable, threatened, endangered, and critically endangered species for each country. 
Red List index is based on categorisations of species on the IUCN Red List of Threatened Species (http://www.iucnredlist.org/), defined following IUCN (2012a)"	281	2041	19	sarthakvajpayee	global-species-extinction
3727	3727	Cyclistic		[]	"Context
Cyclistic data analysis from Divvy Trips, from Q2 2019 to Q1 2020
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	12	0	amilcarrubio	cyclistic
3728	3728	overall_inactivity	Overall Physical Inactivity Among US Adults; BRFSS, 2017–2020	['exercise', 'health']	"Context
Prevalence of Self-Reported Overall Physical Inactivity Among US Adults by State and Territory, BRFSS, 2017–2020
Content
The content of this dataset reveals valuable information about overall physical inactivity among US adults by state and territory.
Acknowledgements
Content source: Division of Nutrition, Physical Activity, and Obesity, National Center for Chronic Disease Prevention and Health Promotion
Inspiration
This dataset helped me to get more insights in order to analyze FitBit Fitness Tracker Data notebook for my Bellabeat Analysis"	1	13	0	parvanekhosravizade	overall-inactivity
3729	3729	race_ethnicity	Physical Inactivity by Race/Ethnicity	['exercise']	"Context
Prevalence of Self-Reported Physical Inactivity by Race/Ethnicity, BRFSS, 2017–2020
Content
The content of this dataset reveals valuable information about prevalence of self-reported physical inactivity among US adults by race/ethnicity
Acknowledgements
Content source: Division of Nutrition, Physical Activity, and Obesity, National Center for Chronic Disease Prevention and Health Promotion
Inspiration
This dataset helped me to get more insights in order to analyze FitBit Fitness Tracker Data notebook for my Bellabeat Analysis"	1	8	0	parvanekhosravizade	race-ethnicity
3730	3730	Hackerone Hacktivity		['beginner', 'data cleaning', 'data analytics', 'numpy', 'pandas']		0	1	0	mohammedmahdiali	hackerone-hacktivity
3731	3731	FridayDDoS		[]		0	12	0	aliyilmaz65	fridayddos
3732	3732	Travel (Trip) Reviews Data	Google reviews on attractions from 24 categories across Europe are considered.	['research', 'computer science', 'data analytics', 'ratings and reviews', 'travel']	"Data Set Information:
This data set is populated by capturing user ratings from Google reviews. Reviews on attractions from 24 categories across Europe are considered. Google user rating ranges from 1 to 5 and average user rating per category is calculated.
Attribute Information:
Attribute 1 : Unique user id
Attribute 2 : Average ratings on churches
Attribute 3 : Average ratings on resorts
Attribute 4 : Average ratings on beaches
Attribute 5 : Average ratings on parks
Attribute 6 : Average ratings on theatres
Attribute 7 : Average ratings on museums
Attribute 8 : Average ratings on malls
Attribute 9 : Average ratings on zoo
Attribute 10 : Average ratings on restaurants
Attribute 11 : Average ratings on pubs/bars
Attribute 12 : Average ratings on local services
Attribute 13 : Average ratings on burger/pizza shops
Attribute 14 : Average ratings on hotels/other lodgings
Attribute 15 : Average ratings on juice bars
Attribute 16 : Average ratings on art galleries
Attribute 17 : Average ratings on dance clubs
Attribute 18 : Average ratings on swimming pools
Attribute 19 : Average ratings on gyms
Attribute 20 : Average ratings on bakeries
Attribute 21 : Average ratings on beauty & spas
Attribute 22 : Average ratings on cafes
Attribute 23 : Average ratings on view points
Attribute 24 : Average ratings on monuments
Attribute 25 : Average ratings on gardens"	7	73	1	travelmania	travel-trip-reviews-data
3733	3733	glasses-detection-model		[]		0	9	0	yvesbischofberger	glassdetectionmodel
3734	3734	titanic		[]		0	4	0	bagusoktavian	titanic
3735	3735	Imgur image data	Imgur's top images for every day since 2018	['beginner', 'intermediate', 'advanced']		1	23	1	nikitricky	imgur-image-data
3736	3736	Chennai house price prediction	Chennai house price prediction	[]		2	37	0	praveen3ravi	trainchennaisalecsv
3737	3737	train-last-hour-average		[]		0	7	1	oviazlo	train-last-hour-average
3738	3738	French applicants for Post-secondary 	Summary of applicants by higher education institutions (ParcourSup) 	['europe', 'education', 'data analytics', 'clustering', 'scipy']	"Context
This dataset is built from the Parcoursup data of the 2020 campaign. The Parcoursup application is the national pre-registration platform set up by the Ministry of Higher Education, Research and Innovation to enable students to apply for entry into higher education.
It covers all applicants who have at least one validated orientation wish in the main and/or complementary phase, among the 12,760 courses offered, excluding apprenticeships. It thus covers 985 538 candidates.
Reference
Please refer to the French Gouvernement for any work : ©Gouvernement.français
Source
https://www.data.gouv.fr/fr/datasets/parcoursup-2020-voeux-de-poursuite-detudes-et-de-reorientation-dans-lenseignement-superieur-et-reponses-des-etablissements/"	2	27	0	baptistemartin	french-applicants-for-postsecondary
3739	3739	England Premier League Players Stats Data	A unique dataset containing FPL data, popularity and market values	['football', 'sports', 'computer science', 'statistical analysis', 'data analytics']	"Context
For most football fans, May - July represents a lull period due to the lack of club football. What makes up for it, is the intense transfer speculation that surrounds all major player transfers today. Their market valuations also lead to a few raised eyebrows, lately more than ever. I was curious to see how good a proxy popularity could be for ability, and the predictive power it would have in a model estimating a player's market value.
Content
name: Name of the player
club: Club of the player
age : Age of the player
position : The usual position on the pitch
position_cat :
1 for attackers
2 for midfielders
3 for defenders
4 for goalkeepers
market_value : As on transfermrkt.com on July 20th, 2017
page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017
fpl_value : Value in Fantasy Premier League as on July 20th, 2017
fpl_sel : % of FPL players who have selected that player in their team
fpl_points : FPL points accumulated over the previous season
region:
1 for England
2 for EU
3 for Americas
4 for Rest of World
nationality
new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July)
age_cat
club_id
big_club: Whether one of the Top 6 clubs
new_signing: Whether a new signing for 2017/18 (till 20th July)
Inspiration
To statistically analyse the beautiful game."	7	67	1	eplstatsmaster	england-premier-league-players-stats-data
3740	3740	National Physical Inactivity	America's Health Ranking Annual Report	['sports', 'public health', 'people', 'health', 'tabular data']	"Context
US National Physical Inactivity; 2021
Content
The content of this dataset reveals valuable information about national physical inactivity in the United States
Acknowledgements
Source: America’s Health Rankings 2021 Annual Report. ©2021 United Health Foundation. All Rights Reserved.
Inspiration
This dataset helped me to get more insights in order to analyze FitBit Fitness Tracker Data notebook for my Bellabeat Analysis"	2	24	0	parvanekhosravizade	national-physical-inactivity-report
3741	3741	kiba_davis_dataset_txt		[]		1	8	0	parthoghosh	kiba-davis-dataset-txt
3742	3742	Narrativa/byt5-base-tweet-hate-detection		[]		0	13	0	wallykop	narrativabyt5basetweethatedetection
3743	3743	competitors	Bellabeat's competitors 	['united states', 'exercise', 'beginner', 'data analytics', 'tabular data']	"Context
This dataset gives more information about the FitBit Fitness Tracker notebook. 
Content
The ""competitors"" dataset contains the data which is collected from the  websites of five fitness tracker companies, along with using online search engines.
Acknowledgements
Any comments would be highly appreciated.
Inspiration
This dataset is provided to present some extra information for analyzing Bellabeat Analysis project."	1	19	0	parvanekhosravizade	competitors
3744	3744	distilroberta-finetuned		[]		0	10	0	dzisandy	distilrobertafinetuned
3745	3745	Population of England Since 1086! (930 years)	Yearly population of England over 930 years, from 1086-2016	['europe', 'social science', 'beginner', 'intermediate', 'time series analysis']	"This file contains the yearly population of England from 1086 to 2016. These are estimates from the Bank of England, and the data was compiled by the Federal Reserve of St. Louis in the US. Can you find the years when the plague hit? Can you spot any significant wars? What are some possible projections for England's population in the future?
England is currently the largest country in the United Kingdom, containing over 80% of its population."	14	106	2	axeltorbenson	population-of-england-since-1086
3746	3746	tennis		[]		0	5	0	fabriciomartins	tennis
3747	3747	Survey for AI		['business']		0	13	1	vishnupragash	survey-for-ai
3748	3748	The Rocket propellant Data	simple linear regression	['regression', 'python']		9	118	7	faezehghaffari	the-rocket-propellant-data
3749	3749	Canadian Brazilian Jiu Jitsu Gyms	A pre-covid list of Brazilian Jiu Jitsu in Canada	['martial arts', 'canada']		1	20	1	ryanjrfleming	canadian-brazilian-jiu-jitsu-gyms
3750	3750	unbiased_toxic_roberta		[]		0	7	0	dzisandy	unbiased-toxic-roberta
3751	3751	analisis		[]		0	5	1	jonathancastros	analisis
3752	3752	pokeman-yolox-l-3		[]		0	6	2	derrick01	pokemanyoloxl3
3753	3753	train_suppl		[]		0	4	0	yuyiping2020	train-suppl
3754	3754	BankNifty		[]		0	4	0	abhinavmukherjee22	banknifty
3755	3755	spam-data	classification for nlp	['psychology']		0	29	0	vimalpillai	spamdata
3756	3756	temp_files		[]		0	10	0	xiaoa5	temp-files
3757	3757	VGGface2_HQ_cropped		[]		0	11	0	zenbot99	vggface2-hq-cropped
3758	3758	Cats and Dogs Detection(classification) Using CNN 		[]		0	22	0	sanjoymondal0	cats-and-dogs-detectionclassification-using-cnn
3759	3759	London Borough Carbon Dioxide Emissions		['environment', 'beginner', 'advanced', 'data visualization']	"Context
Local and Regional CO2 Emissions Estimates for 2005-2011 plus, subset data of CO2 emissions within the scope of influence of Local Authorities (previously called National Indicator 186: Per capita CO2 emissions in the LA area) by sector
Acknowledgements
https://www.gov.uk/government/publications/local-authority-emissions-estimates"	2	28	0	davidemarcantoni	london-borough-carbon-dioxide-emissions
3760	3760	KDDM2 Data		[]		1	15	0	alepru	kddm2
3761	3761	CompaniesDataset from Ambition Box	Dataset extracted using web scraping method from ambition box.	[]		0	7	0	jokervb	companiesdataset-from-ambition-box
3762	3762	Detoxify (Source&Models)		['health']		5	106	10	steubk	detoxify-sourcemodels
3763	3763	Global Corruption Index (2021)	Global Corruption Index by Transparency International	['data visualization', 'news']		9	90	8	atrisaxena	global-corruption-index-2021
3764	3764	corpus-comentarios		[]		0	23	0	fernandobordi	corpuscomentarios
3765	3765	UnityEyes classified images 720*1280		[]		0	4	0	dorianabadtovardiaz	unityeyes-classified-region-images
3766	3766	Dataset		[]		0	2	0	pratyayroy	dataset
3767	3767	Bert_SA		[]		0	6	0	mlcovidresearch	bert-sa
3768	3768	TITANIC_ALL_DATASETS	My_Complete_Pipeline_for_any_ML_Competition	[]		3	20	6	raj401	titanic-all-datasets
3769	3769	Annual Ticket Sales		['retail and shopping']		5	44	1	pratyayroy	annual-ticket-sales
3770	3770	Property Prices Index By City 2009 to 2021	I scrapped the data using Python. Code and Source in the description.	['websites', 'housing', 'computer science', 'programming', 'pandas']	"Context
I wanted to see how affordable housing is across countries and wanted to compare the price of housing. But I could not find a properly documented and easily downloaded dataset hence I created one with the help of web-scraping with Python and Pandas.
Content
I spent a lot of time searching for a source for the information I wanted in order to compare affordability. I stumbled upon a great website which was exactly what I was looking for Numbeo The website has a lot of details like affordability index, prime to income ratio, price to rent ratios in and out of city centre and more!
Now I had the data, I needed to download it. Since I couldn't get the raw form of the data, I did web scraping in order to get details in the table for 2009 to 2021 using a for loop to go through all links and create csv files for every year.
What's in the dataset?
Details of columns
Note: There are a few null values in the 2009 dataset (mortgage and Affordability Index columns.
Check out the code I used on Github.
Acknowledgements
I couldn't have gotten the data without Numbeo!
Inspiration
I was working on a project trying to see if Price of Housing in Singapore can be justified and wanted more data that's global instead of just from Singapore. Let me know if you have any questions!"	212	1326	11	jolenech	property-prices-index-by-city-2009-to-2021
3771	3771	h3spectrum		[]		4	17	0	rasterbunny	h3spectrum
3772	3772	covid19_3		[]		0	6	1	arifurrahmanarif	covid19-3
3773	3773	Cycling Path Length 2013-2020		[]		2	14	1	xxre34	cycling-path-length-20132020
3774	3774	language classification 		[]		2	19	0	nazmuddhohaansary	language-classification
3775	3775	Horse2zebra Dataset		[]		0	10	0	huangkailong	horse2zebra-dataset
3776	3776	Breast Cancer Classification By Machine Learning		['cancer']		6	40	1	mrugrajkumpavat	breast-cancer-classification-by-machine-learning
3777	3777	pooling-model		[]		0	1	0	kookheejin	poolingmodel
3778	3778	swissdata		[]		0	1	0	stark09	swissdata
3779	3779	old_train.csv		[]		0	7	0	duckfalcon	old-traincsv
3780	3780	Morgan Fence & Awning		[]	We provide a full range of fencing products, from vinyl fencing to commercial chain link fencing. Through our experience and dedication to customer service, we have made a name for ourselves in the fencing business. We take pride in delivering quality products and earning the trust of our customers.	0	4	0	morganfenceawning	morgan-fence-awning
3781	3781	GEN4_Retest_datasets		[]		0	17	0	pterkrtvlyesi	project1
3782	3782	lenaimg		[]		0	0	0	daredevil555	lenaimg
3783	3783	Pakistani House Price Prediction		['religion and belief systems']		1	24	0	altafk	pakistani-house-price-prediction
3784	3784	comicsface		[]		1	8	0	karethdy	comicsface
3785	3785	Song_Popularity_SKF		[]		0	5	0	kirtichauhan11	song-popularity-skf
3786	3786	dataset classification animal zoo	over 100 data different animal	['animals']	"over 100 data different animal, for classification animal type.
1 missing value, u can try to fix with filter replacemissingvalue or something else."	0	30	0	sweedendataset	dataset-classification-animal-zoo
3787	3787	dftyuiuhgfgh		[]		0	1	0	lubitelanime	dftyuiuhgfgh
3788	3788	sdfghjkl;lkj		[]		0	1	0	lubitelanime	sdfghjkllkj
3789	3789	DenseNet121 Backbone Weight		[]		0	7	0	fereshtej	densenet121-backbone-weight
3790	3790	ssssssssssssssssssssssss		[]		0	7	0	lubitelanime	ssssssssssssssssssssssss
3791	3791	448-Eff-Swin-224-weights		[]		2	38	0	niliangliang	448-eff-swin-224-weights
3792	3792	money_detection		[]		2	18	0	kenakai16	money-detection
3793	3793	Invent Help Is it Right For You		['law']	"Invent Help can be an invaluable resource for new inventors. These organizations can provide you with the support you need to get your invention to market. Many people turn to InventHelp for help because they don't have the time to devote to researching and writing patent applications themselves. If you're among these individuals, they can help you navigate the patent process. If you're unsure whether to hire InventHelp or find another solution, here are some tips for getting started.
Visit here https://www.jpost.com/special-content/hire-inventhelp-experts-for-patent-services-694280 for invent help.
A: You need to find a company with experience in helping inventors. This is important because a new company may not have as much experience as an established one. Invent Help has been helping inventors for 35 years and has expanded their offices and staff over the years. They have people who understand the needs of inventors. The staff at InventHelp is friendly and professional, and they are happy to help. You'll never have to worry about the legalities or costs associated with a new business, here about how do you patent an idea with InventHelp.
Experience is important in the industry. You don't want to work with a company that is too new. A company that has been in business for 35 years is likely to have a vast amount of knowledge and experience. Its employees are familiar with the needs of inventors and have worked to develop effective solutions. This is an advantage when working with an Invent Help agent. They'll guide you along the way. You'll know exactly what to do next.
A professional company will have experience in the field you're interested in. They will be able to guide you through the entire process and answer any questions you might have. Once your prototype is complete, you can then get a company to manufacture your product if it meets all of your specifications. This will help you protect your intellectual property and ensure that you can profit from your invention. You'll need a lawyer and a patent attorney.
Invent Help uses high-pressure sales tactics to get you to sign a contract with them. They can manipulate your information to persuade you to sign a fraudulent contract. If your idea is a good one, the company will offer you patent protection. It's not necessary to pay for this service. The services offered by InventHelp are a great deal of help for you, but they should be repaid after they have been used.
The cost of InventHelp can be astronomical. This company will push you to sign a contract to their program that's based on fake promises. Aside from high pressure, the company will also push you to purchase a product without a guarantee. It's a scam that will take your hard earned money and leave you stranded. Fortunately, InventHelp will pay you a fee if you decide to go with them."	0	21	0	sashagreg	invent-help-is-it-right-for-you
3794	3794	ASL signs		[]		0	8	0	muhammedjaabir	asl-signs
3795	3795	feedback ner train		[]		12	47	3	kishalmandal	feedback-ner-train
3796	3796	Image Regression from CAD to CAE, on Airbag Volume	CAD screenshots are used as input with Volume simulation as verified CAE outputs	['earth and nature', 'science and technology', 'engineering', 'feature engineering', 'cnn', 'image data']	"Context
Engineering-based datasets have special characteristics and therefore need special treatments in terms of algorithms that are being used. But to develop such algorithms, we need better data. This database presents airbags geometry before their deployment and the question is to be able to predict the volume of each shape after deployment.
Content
There are 60000  screenshots obtained from the CATIA sketch environment. All pictures are single-channel and cropped to have the same object placement. The CSV file contains the volume (Liter) simulation results that are obtained by dynamic relaxation and verified by finite element. 
There are two more pickled and zipped databases in a smaller size for ease of access (In these smaller files the regression problem is changed to a classification problem).
Acknowledgements
For citing the database please either cite our paper on CAD2022 or cite directly by http://dx.doi.org/10.13140/RG.2.2.19470.02887
Inspiration
How to attack engineering-based databases that are skewed like this? 
How to customize algorithms to get better performance on this kind of image regression problem?"	1	37	1	mohammadrad	airbag-cad-to-cae-volume
3797	3797	Quality of life Norway, Sweden, Finland		[]		4	37	0	khaymon24	quality-of-life-norway-sweden-finland
3798	3798	Purchasing power of Norway, Sweden, Finland		[]		1	19	0	khaymon24	purchasing-power-of-norway-sweden-finland
3799	3799	Harry Potter Dataset		[]		18	12	0	cocwithme	harry-potter-dataset
3800	3800	coral-coco		[]		3	11	0	tunminhhunh	coralcoco
3801	3801	Filter and Fire Neuron	Data and Code to fully replicate all F&F paper results	['earth and nature', 'biology', 'neuroscience', 'science and technology', 'computer science']	"The Filter and Fire (F&F) Neuron Model
This dataset and notebook collection is part of the work behind the paper:
""Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons""  
Multiple Synaptic Contacts combined with Dendritic Filtering <br>enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons
David Beniaguev, Sapir Shapira, Idan Segev, Michael London
Abstract:
A cortical neuron typically makes multiple synaptic contacts on the dendrites of a post-synaptic target neuron. The functional implications of this apparent redundancy are unclear. The dendritic location of a synaptic contact affects the time-course of the somatic post-synaptic potential (PSP) due to dendritic cable filtering. Consequently, a single pre-synaptic axonal spike results with a PSP composed of multiple temporal profiles. Here, we developed a ""filter-and-fire"" (F&F) neuron model that captures these features and show that the memory capacity of this neuron is threefold larger than that of a leaky integrate-and-fire (I&F) neuron, when trained to emit precisely timed output spikes for specific input patterns. Furthermore, the F&F neuron can learn to recognize spatio-temporal input patterns, e.g., MNIST digits, where the I&F model completely fails. Multiple synaptic contacts between pairs of cortical neurons are therefore an important feature rather than a bug and can serve to reduce axonal wiring requirements.
Content
In the code secion of this dataset are 6 kaggle notebooks that replicate all results in the paper (Figures 1,2,3,4,5 and Supplementary Figure 2)
For more details about the work, please visit the github repository and the manuscript
For fine-grained details please read the file descriptions in the data explorer section below
Acknowledgements
This paper is part of my PhD work at the ELSC PhD program, under the supervision of Idan Segev and Mickey London
I would like to thank all lab members of the Segev and London Labs for many fruitful discussions and valuable feedback regarding this work.
In particular I would like to thank Sapir Shapira that skillfully collected all data and created Supplementary Figure S2 in the paper.
If you use this code or dataset, please cite the following work:
- David Beniaguev, Sapir Shapira, Idan Segev and Michael London. ""Multiple Synaptic Contacts combined with Dendritic Filtering enhance Spatio-Temporal Pattern Recognition capabilities of Single Neurons ."" bioRxiv 2022.01.28.478132; doi: https://doi.org/10.1101/2022.01.28.478132"	4	191	2	selfishgene	fiter-and-fire-paper
3802	3802	Rice Leaf Diseasess		['food']		3	11	0	auliyaa	rice-leaf-diseasess
3803	3803	data eindopdracht		['business']		0	11	0	nathanvankrevelen	data-eindopdracht
3804	3804	jigsaw——cleaned_data		[]		6	23	1	banbeipi	jigsawcleaned-data
3805	3805	rice leaf diseases		['health conditions']		1	17	2	auliyaa	rice-leaf-diseases
3806	3806	Aquarium starfishes		[]		1	24	0	ks2019	aquarium-starfishes
3807	3807	Road Traffic Accidents	Road Traffic Accident Dataset of Addis Ababa City	['transportation', 'beginner', 'intermediate', 'advanced', 'tabular data']	"Context
This data set is collected from Addis Ababa Sub city police departments for Masters research work. 
Content
The data set has been prepared from manual records of road traffic accident of the year 2017-20. All the sensitive information have been excluded during data encoding and finally it has 32 features and 12316 instances of the accident. Then it is preprocessed and for identification of major causes of the accident by analyzing it using different machine learning classification algorithms algorithms. RTA Dataset.csv is the dataset before preprocessing and cleaned.csv is the preprocessed dataset.
Acknowledgements
Bedane, Tarikwa Tesfa (2020), “Road Traffic Accident Dataset of Addis Ababa City”, Mendeley Data, V1, doi: 10.17632/xytv86278f.1"	149	1097	0	saurabhshahane	road-traffic-accidents
3808	3808	Hotel-Booking		['hotels and accommodations']		0	11	0	hosseingolmohammadi	hotelbooking
3809	3809	exp file		['software']		0	2	0	grandhisaidivya	exp-file
3810	3810	Loan Prediction		[]		4	68	0	altafk	loan-prediction
3811	3811	tmdb_topratedmovies	top rated movies dataset from tmdb movies website.	['arts and entertainment']		1	20	0	jokervb	tmdb-topratedmovies
3812	3812	lfw_people_flatten_data		[]		0	1	0	ravikumarmn	lfw-people-flatten-data
3813	3813	1-boys		['arts and entertainment']		0	2	1	wallacefqq	1boys
3814	3814	beirbiencoderresults3		[]		0	3	0	muennighoff	beirbiencoderresults3
3815	3815	beirbiencoderresults2		[]		0	5	0	muennighoff	beirbiencoderresults2
3816	3816	after2017		[]		0	2	0	yixuliu	after2017
3817	3817	pokeman-yolox-l-2		[]		0	7	0	derrick01	pokemanyoloxl2
3818	3818	MPII Human Pose	MPII Human Pose Dataset for Human Pose Estimation	['earth and nature']		9	221	0	harshpatel66	mpii-human-pose
3819	3819	MH_wipro_sustainable		[]		0	44	0	tirthankardas	mh-wipro-sustainable
3820	3820	JS hateBERT		[]		9	5	0	aishikai	js-hatebert
3821	3821	Classification 		[]		6	30	0	humzaiftikhar	classification
3822	3822	add_dcic	ocr1111111111111111111111111	[]		1	19	0	panfei748	add-dcic
3823	3823	Subang Jaya Weather for 2020-2021		[]		0	2	0	thinesk89	subang-jaya-weather-for-20202021
3824	3824	NHL2018-19odds		[]		0	2	1	bhavishsalia	nhl201819odds
3825	3825	Sentiment tweets		['online communities']		1	29	0	mlcovidresearch	sentiment-tweets
3826	3826	IPL2018Odds		[]		4	16	2	bhavishsalia	ipl2018odds
3827	3827	Transformer_qianyi		[]		0	3	0	bigcat111	transformer-qianyi
3828	3828	NBA2019odds		[]		1	3	1	bhavishsalia	nba2019odds
3829	3829	HYDE population and land-use since 10000BC	ANTHROPOGENIC LAND-USE ESTIMATES FOR THE HOLOCENE; HYDE 3.2	['social science']		0	19	1	ilyenkov	hyde-population
3830	3830	UMP Norm Lag 1 Features		['sports']		1	29	3	takamichitoda	ump-norm-lag-1-features
3831	3831	world data		[]		1	5	0	saimmohd	world-data
3832	3832	Volcanoes		['geology']		0	8	0	saimmohd	volcanoes
3833	3833	yolo best		[]		2	32	0	light367	yolo-best
3834	3834	datafinal		[]		0	0	0	selene009	datafinal
3835	3835	qianyan_textsim		[]		0	4	0	turkeymz	qianyan-textsim
3836	3836	base_model		[]		0	0	0	zhangrenjie	base-model
3837	3837	PART FEEDER - SATISFY ALL YOUR NEEDS	 Read more: https://temas.vn/en/blog/part-feeder-satisfy-all-your-needs	['business']		1	15	0	roboticsolution	part-feeder-satisfy-all-your-needs
3838	3838	ridge_j_multi_data_cleanexp2_fasttext_oof		[]		0	1	0	shobhitupadhyaya	ridge-j-multi-data-cleanexp2-fasttext-oof
3839	3839	pokeman_yolox_l_1		[]		0	7	0	derrick01	pokeman-yolox-l-1
3840	3840	UMP Lag 1 Features f200~f299		[]		0	7	0	takamichitoda	ump-lag-1-features-f200f299
3841	3841	UMP Lag 1 Features f100~f199		[]		1	12	0	takamichitoda	ump-lag-1-features-f100f199
3842	3842	Data of Brazilian Federal Revenue (July-2021)	Data from Brazilian Federal Revenue organized in SQLite File	['brazil', 'business', 'intermediate', 'data analytics', 'sql']	"Context:
This dataset was used for me to update all client's data in the company i'm working. So i think this could be usefull for other people in Brazil.
Content:
In this file you'll find a database SQL format with some tables that contains data from establishments, companies and status from this accounts. Also that file contain CNAE information.
Acknowledgements
I would like to thanks a strange guy in github that developed a script that convert all csv files into a SQLite file. Unfortunately, I couldn't found his GutHub. 
Also, that data can be found at: http://200.152.38.155/CNPJ/"	26	447	6	juniorfazzio	data-of-brazilian-federal-revenue-july2021
3843	3843	yolox_v1.0		[]		0	16	0	chiangtyrol	yolox-v10
3844	3844	The world machine from oneshot the real one		['literature']		3	16	2	zipperxyz	the-world-machine-from-oneshot-the-real-one
3845	3845	Google Mobility Report Extract (AU, SG, JP, US)	Extract from Google Mobility Report for select countries	['software', 'tabular data', 'covid19']		0	21	0	alcheng10	gmr-extract-au-sg-jp
3846	3846	fire detection	fire detection dataset that contains training and testing 	['public safety']	"I am a master student, I have to make a model for fire detection using computer vision, so I decided to help the new researchers by providing dataset that I gathered along my trip.
This dataset contains two folders fire and non-fire, each one contains training and testing folders.
I collected this data from different sources even real life by capturing fire with my Nikon3200 and iPhone camera."	1	18	1	mykaggle193	fire-detection
3847	3847	Belgium Largest Companies	Dataset of the world's largest publicly listed corporations	['business', 'finance', 'economics', 'investing']	"About this dataset
&gt; <p>From the Forbes Global 2000 list​ last updated on May 2013. Forbes publishes an annual list of the world's 2000 largest publicly listed corporations. ​The Forbes Global 2000 weigh​s​ sales, profits, assets and market value​ equally​ so companies can be ranked by size. Figures for all companies are in US dollars.</p>
<p>​Source: <a href=""%E2%80%8Bhttp://www.economywatch.com/companies/forbes-list/"" target=""_blank"" rel=""nofollow"">Economy Watch</a></p>
This dataset was created by Finance and contains around 0 samples along with Profits ($billion), Profits ($billion), technical information and other features such as:
- Profits ($billion)
- Profits ($billion)
- and more.
How to use this dataset
&gt; - Analyze Profits ($billion) in relation to Profits ($billion)
- Study the influence of Profits ($billion) on Profits ($billion)
- More datasets
Acknowledgements
If you use this dataset in your research, please credit Finance 
Start A New Notebook!"	39	283	5	yamqwe	belgium-largest-companiese
3848	3848	UMP Lag 1 Features f0~f99		[]		1	30	0	takamichitoda	ump-lag-1-features-f0f99
3849	3849	dcic_model	111111111111111111111111111	[]		0	3	0	panfei748	dcic-model
3850	3850	WHTsign00		[]		0	1	0	haitahwong	whtsign00
3851	3851	Boston Housing Price	Predict house price with regression techniques	['housing', 'regression', 'sklearn']		1	40	0	domenicomorabito	boston-housing-price
3852	3852	SolPunks Truth Project	All 10,000 CryptoPunks, SolPunks, Fake SolPunks, Dumb SolPunks, ...	['art', 'categorical data', 'finance', 'tabular data', 'image data']	We are declassifying the world's largest Punks database. We want the world's top data scientists & AI researchers to use advanced butt analysis and anal clustering techniques to understand the nature of Tractor's Fake SolPunks.	11	407	7	solpunks	solpunks-truth-project
3853	3853	after2016		[]		0	2	0	yixuliu	after2016
3854	3854	ap-yolo		[]		1	14	0	annprzyb	apyolo
3855	3855	model_data		[]		1	16	0	raminanush	model-data
3856	3856	Lab2_python		[]		0	1	0	nicholasgrant38	lab2-python
3857	3857	Axion new Sim 23k 		['automobiles and vehicles']		0	7	0	renessmi2017	axion-new-sim-23k
3858	3858	imagenet		[]		0	7	0	anditenriawaruasnur	imagenet
3859	3859	Morn_Click_WAV		[]		0	12	0	markusunterdechler	morn-click-wav
3860	3860	LogHub - Windows Log Data	A dataset of logs from Windows instances	['computer science', 'programming', 'text mining', 'text data']		10	582	3	omduggineni	loghub-windows-log-data
3861	3861	nyu_d_cfw		[]		5	7	0	dariushbahrami	nyu-d-cfw
3862	3862	DWDM_lab2		[]		0	4	0	nicholasgrant38	dwdm-lab2
3863	3863	Illicit Drug Database with Descriptors		[]		0	7	0	dschettler8845	illicit-drug-database-with-descriptors
3864	3864	Breaking Bad Episode Data	Episodes and ratings for Breaking Bad	['arts and entertainment', 'movies and tv shows', 'beginner', 'tabular data']	"Context
Series created by: Vince Gilligan
Number of seasons: 5
Number of episodes: 62
Original air dates: January 20, 2008 – September 29, 2013
Content
Data was acquired through downloading IMDb TV episodes datasets and scraping information from Wikipedia.
Acknowledgements
Thanks to IMDb, Wikipedia, and community curators.
Use
It should be easy to join these data files together on Title and Air Date fields to compare (for example) US viewers and IMDb ratings.
Motivation
I wanted to share a dataset about Breaking Bad,  one of my favorite TV shows to binge watch."	23	223	5	bcruise	breaking-bad-episode-data
3865	3865	Cyclistic		[]		0	2	0	juancordovavazquez	cyclistic
3866	3866	best_mask0		[]		0	2	0	light367	best-mask0
3867	3867	1 year data summary		[]		0	6	0	linhho1	1-year-data-summary
3868	3868	SQL draft		['business']		0	16	0	linhho1	sql-draft
3869	3869	Nasir Samael - shuffling songs playlist		[]		0	2	0	nasirsamael	nasir-samael-shuffling-songs-playlist
3870	3870	JantoDec2021.cyclistictripdataclean		[]		0	2	0	linhho1	jantodec2021cyclistictripdataclean
3871	3871	DataMiningDataset	Ensemble Dataset for Data Mining	['business']		0	23	0	chipmunkatdev	weathercsv
3872	3872	Zingat House Sale Dataset	Web Scraped data from Zingat	['business', 'real estate', 'economics', 'beginner', 'regression']	"This dataset is created by web scraping the Zingat web site. Zingat is one of the most popular house selling /renting web sites in Turkey. The scraped data is from the houses that for sale in Istanbul.
The dataset is prepared for those who want to create machine learning models with small amount of variables.
nsm = 'Net Square Meters'
gsm = 'Gross Square Meters'
bathrooms = Number of bathrooms the house has
rooms = Number of rooms the house has
prices = Value Of the house"	1	12	0	ekremzturk	zingat-house-sale-dataset
3873	3873	English Premier League Match Events and Results	2001-2002 season onwards	['football', 'sports']	"The Data
There are two csvs with match information, matches.csv and events.csv, that contain information about each match from the 2001-2002 season through roughly current during the 2021-2022 season. matches.csv contains information such as the teams playing, final score, date, and lineups. events.csv contains events that happened in a game, at what time, and in what game. 
There is one csv with table information, all_tables.csv, that contains the tables from the 2001-2002 season through roughly current during the 2021-2022 season.
There is 3 csvs with aggregated stats in the agg_stats folder. They have data from 2002 through present. 
I plan on updating this dataset with data approximately weekly while a season is ongoing.
Note: The Year column in matches.csv contains the year that the season started in, not the year that the match took place.
Note: 107 of 380 matches in the 2001-2002 season have no commentary.
Data Source
Match data was scrapped by me from: https://www.espn.com/soccer/fixtures//date/20210413/league/eng.1
Tables were scrapped by me from: https://www.espn.com/soccer/standings//league/ENG.1/season/2020
Image
Code used to scrap the data is located here.
Inspiration
How have having crowds/the long break effected teams
Teams that win/lose by the most goals
If a team is more likely to get yellow cards against certain other teams"	270	2315	8	josephvm	english-premier-league-game-events-and-results
3874	3874	English Premier League Logo Detection (20k Images)	Large Scale Image Classification - Classify Soccer Team Logos!	['football', 'sports', 'computer science', 'computer vision', 'image data', 'multiclass classification']	"Context
Detecting logos from brands in images can help with targeted advertising. Try out this technique with a simple image classification task to classify English Premier League Football/Soccer Club Team Logos.
If you love watching soccer and computer vision tasks this is a good beginner dataset for you!
Content
The companion to this dataset is my Top 5 Football Leagues Club Logos Dataset, where I compiled the basic logo images. For this dataset, I just created 1000 augmented images for each of the 20 team logos. 
Coming Soon!!! : Once you've trained your models with the images from '.../epl-logos-big/', test out if your models can go on to detect the logos in real world mages found in the '/test/' directory. Explore why this approach works or does not work and what could be changed about the dataset itself.
Summary of variables:
list of the teams = ['arsenal', 'aston-villa', 'brentford', 'brighton', 'burnley', 'chelsea', 'crystal-palace', 'everton', 'leeds', 'leicester-city', 'liverpool', 'manchester-city', 'manchester-united', 'newcastle', 'norwich', 'southampton', 'tottenham', 'watford', 'west-ham', 'wolves']
dictionary of the teams and corresponding class number = {'arsenal':0, 'aston-villa':1, 'brentford':2, 'brighton':3, 'burnley':4, 'chelsea':5, 'crystal-palace':6, 'everton':7, 'leeds':8, 'leicester-city':9, 'liverpool':10, 'manchester-city':11, 'manchester-united':12, 'newcastle':13, 'norwich':14, 'southampton':15, 'tottenham':16, 'watford':17, 'west-ham':18, 'wolves':19}
Acknowledgements
I do not own the rights to the team logos, but they are publicly available and can be used in this context. It should go without saying, but do not use the logos from this dataset for your own business or commercial endeavors. Perfectly fine to use for this project though!
Inspiration
MNIST digit classifiers are overdone by now. Looking to provide simple image classification datasets here on Kaggle that use different and useful data. Also I love watching EPL - Go Man U!"	64	897	9	alexteboul	english-premier-league-logo-detection-20k-images
3875	3875	UCF101 dataset	Models on the UCF101 dataset.	['earth and nature']		0	16	0	ziadfellahidrissi	ucf101-dataset
3876	3876	Maharashtra's Cities and Towns Census	census of 1991,2001,2011	['social science']		0	4	0	samarthgangurde	maharashtras-cities-and-towns-census
3877	3877	pyspellchecker		[]	"Source
https://pypi.org/project/pyspellchecker"	1	18	1	atamazian	pyspellchecker
3878	3878	InsuranceLM		[]		1	10	0	garr3ttmj	insurancelm
3879	3879	Champions League era stats	UEFA Champions League satistics	['football']	"Version-info
This data includes statistics up to the group stage of the 2021/22 season. 
Context
The UEFA Champions League (abbreviated as UCL) is an annual club football competition organized by the Union of European Football Associations (UEFA) and contested by top-division European clubs, deciding the competition winners through a group and knockout format. It is one of the most prestigious football tournaments in the world and the most prestigious club competition in European football, played by the national league champions (and, for some nations, one or more runners-up) of their national associations. (*** From wikipidea)
Note: This doesn't have any information about the European cup competition (1950-1992). It starts with the beginning of the Champions league (1992/93) season.
Content
So far this data has the following:
 1- Each club's participation record in the competition
 2- Each country's clubs participation records in the competition (summary of #1)
 3- Top Player Appearances by club (i.e. number of times played for a club in the competition)
 4- Top Player Appearances Total games (summary of #3)
 5- Top Goal scorer by club (i.e. number of goals scored by a player for a club in the competition)
 6- Top Goal scorer Totals (summary of #5)
 7- Top Coach Appearances by club (i.e. number of times coached for a club in the competition)
 8- Top Coach Appearances Total games (summary of #7)
 9- Top Goal Scorer for each season in the competition with # of appearances
10- Number of goals scored per round per group in each season
Acknowledgements
All this data was provided by UEFA.com.  All I did was download the PDF and then scrape the data and put it in csv format."	155	831	2	basharalkuwaiti	champions-league-era-stats
3880	3880	NC-clean		[]		0	1	0	lrgao420	ncclean
3881	3881	www.kaggle.com/vindieselmark/		[]		0	5	0	vindieselmark	wwwkagglecomvindieselmark
3882	3882	Ownership Data		[]		0	7	0	aviral23	ownership-data
3883	3883	Cyclistic bike-share analysis 	(Dec2020 to Nov2021)	[]		0	4	0	mithranseralathan	cyclistic-bikeshare-analysis-dec2020-to-nov2021
3884	3884	Daily Moon Illumination 1800 to 2100	Calculated percentage of moon illuminated daily at midnight	['astronomy']	Out of curiosity, I wanted to look at moon illumination in relation to my sleep quality. I found datasets that contained phases for certain dates, but what I wanted was a percentage of moon illumination around the time I would be asleep. Using a python script created by Kevin Turner, which was derived from a tool created by John Walker, I generated the percentages that I needed—I then decided to broaden the date range and share them in case anybody else was looking for a similar dataset. These are entirely calculated values and not observed. Enjoy!	1	12	0	petermenzies	daily-moon-illumination-1800-to-2100
3885	3885	Google Analytics Capstone: Bellabeat 	How Can A Wellness Technology Company Play It Smart?	[]		0	34	0	filipdusek	google-analytics-capstone-bellabeat
3886	3886	ITP New Dataset		[]		1	10	0	aritrabrahma	itp-new-dataset
3887	3887	Cyclistic Case Study	Capstone Project for Google Analytics Certificate	['education']	"Context
I made some assumptions with cleaning out some data point for this case study. I removed all rides that were greater than 6 hours and less than 0 minutes. Per the source companies policies, no ride should be over 3 hours without significant usage fees. At 6 hours, most of the usage fees pass a reasonable about for a day of riding a bike verging in on an annual subscription fee. 
Content
The data is from public data from Divvy Bike share. The data I used was from the last 12 months, so calendar year 2021. I removed columns in excel that was not pertinent to my analyses (longitude, latitude).
Acknowledgements
I used the case study outline provided by Coursera. Also, inspiration to start, from the completed and public case study report from Kaggle user JulianAranguren. 
Inspiration
This was my first attempt at this. I spent a long time in a different career using data analysis to progress. I am new to these tools and I know I have plenty to improve on. I want to look back at this, and see the first wobbly stone of my new path."	0	24	0	joebiel	cyclistic-case-study
3888	3888	India's GDP data_World_Development_Indicator		[]		0	4	0	himanshusirsat	indias-gdp-data-world-development-indicator
3889	3889	Fig1 Number of records per year 	Number of records per year (1968-2021). Source: own elaboration based on Scopus.	['law', 'data analytics', 'image data']		0	5	0	edisoncarrascojimnez	database
3890	3890	VGGface2_HQ		[]		0	29	0	zenbot99	vggface2-hq
3891	3891	nsk_image_search3_man6y		[]		0	0	0	motono0223	nsk-image-search3-man6y
3892	3892	InstaCart Online Grocery Basket Analysis Dataset	InstaCart Online Grocery Market Basket Analysis	['beginner', 'clustering', 'retail and shopping', 'online communities']	"Description:
Whether you shop from meticulously planned grocery lists or let whimsy guide your grazing, our unique food rituals define who we are. Instacart, a grocery ordering and delivery app, aims to make it easy to fill your refrigerator and pantry with your personal favorites and staples when you need them. After selecting products through the Instacart app, personal shoppers review your order and do the in-store shopping and delivery for you.
Instacart’s data science team plays a big part in providing this delightful shopping experience. Currently they use transactional data to develop models that predict which products a user will buy again, try for the first time, or add to their cart next during a session. Recently, Instacart open sourced this data - see their blog post on 3 Million Instacart Orders, Open Sourced.
In this competition, Instacart is challenging the Kaggle community to use this anonymized data on customer orders over time to predict which previously purchased products will be in a user’s next order. They’re not only looking for the best model, Instacart’s also looking for machine learning engineers to grow their team.
Acknowledgements:
This dataset is taken from Kaggle, \
https://www.kaggle.com/c/instacart-market-basket-analysis/data
Objective:
Understand the Dataset & cleanup (if required).
Build classification model to recommend groceries based on users past purchases."	237	1940	16	yasserh	instacart-online-grocery-basket-analysis-dataset
3893	3893	ToxicCommentTraining		[]		0	11	0	jmrludan	toxiccommenttraining
3894	3894	ToxicCommentTraining		[]		0	11	0	jmrludan	toxiccommenttraining
3895	3895	aaaaaabbbb		[]		97	9	0	devanshchowdhury	aaaaaabbbb
3896	3896	dataset		[]		0	6	0	devanshchowdhury	dataset
3897	3897	USA Monster Job Data	This dataset includes job data from Monster USA	['jobs and career']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 438090  Domain Name : monster.usa  Date Range : 01st Apr 2021 - 30th Jun 2021   File Extension : ldjson
Available Fields : uniq_id, crawl_timestamp, url, job_title, category, company_name, city, state, country, post_date, job_description, job_type, apply_url, job_board, geo, valid_through, html_job_description, is_remote, test_contact_email, contact_email, test1_cities, test1_states, test1_countries, site_name, domain, postdate_yyyymmdd, predicted_language, inferred_iso3_lang_code, test1_inferred_city, test1_inferred_state, test1_inferred_country, inferred_city, inferred_state, inferred_country, has_expired, last_expiry_check_date, latest_expiry_check_date, dataset, postdate_in_indexname_format, segment_name, duplicate_status, fitness_score   
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud, DataStock and live job data from JobsPikr.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	16	224	6	promptcloud	usa-monster-job-data
3898	3898	tfrecord cots		[]		0	4	0	ashusma	tfrecord-cots
3899	3899	Cyclistic Bike-share GDA case study 1		[]		0	9	0	pavangfadnees	cyclistic-bikeshare-gda-case-study-1
3900	3900	ttttttt1		[]		0	11	0	devanshchowdhury	ttttttt1
3901	3901	gresarch_train_final		[]		2	3	0	matrixneo	gresarch-train-final
3902	3902	homeLoan		[]		0	11	0	arieljumba	homeloan
3903	3903	testdata		['business']		0	2	0	gamingnation	testdata
3904	3904	Public Transport Average Ridership 2015-2020	Singapore Public Transport:Public Transport Average Ridership 2015-2020	['transportation', 'automobiles and vehicles', 'cycling', 'rail transport', 'water transport']	"Context
This dataset is fully from the LTA.Just sorting the data and converting it from pdf to xlsx file
Acknowledgements
Land Transport Authority (2020) PUBLIC TRANSPORT RIDERSHIP .Available at https://www.lta.gov.sg/content/dam/ltagov/who_we_are/statistics_and_publications/statistics/pdf/PT_Ridership_2015-2020.pdf [Accessed on 27/01/2021]"	5	62	0	hanci16	public-transport-average-ridership-20152020
3905	3905	flowers2art		[]		0	48	0	jiccrlla	flowers2art
3906	3906	journeys in peak undertaken on public transport		['transportation']		0	5	0	shtjyg	journeys-in-peak-undertaken-on-public-transport
3907	3907	Homelessness in Ireland (Jan 2019 - Oct 2021)		['intermediate', 'data cleaning', 'statistical analysis', 'data analytics', 'hotels and accommodations']		1	22	1	susharanhonnal	homelessness-in-ireland-jan-2019-oct-2021
3908	3908	v1sads		[]		0	4	0	vijayindalkar	v1sads
3909	3909	vegg_data		[]		1	37	2	kenials	vegg-data
3910	3910	University dataset	University Dataset for learning the clustering 	['universities and colleges']	"Context
Hi, guys i'm new at Kaggle and this is my 1st dataset. so plz support by giving me feedback regarding my work.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	25	0	ritwiksingh99	university-dataset
3911	3911	jigsaw_toxic_comment		[]		0	4	0	sunilchoudhary1	jigsaw-toxic-comment
3912	3912	The Android App Market on Google Play		['tabular data', 'ratings and reviews', 'matplotlib', 'numpy', 'pandas']		4	23	1	malvirex	the-android-app-market-on-google-play
3913	3913	datasetB3		[]		0	15	0	flutterdevelopers	datasetb3
3914	3914	flood_risk_maps	Flood height maps for the Netherlands 	['geography', 'weather and climate', 'geospatial analysis']	"Acknowledgements
These floodmaps are created by WCN (Watermanagementcentrum Nederland) and are distributed via LIWO. These maps are version 2021_0_3 and will be updated in the future. If you want to have an updated map please go to: https://basisinformatie-overstromingen.nl/ where you can find the source data"	5	57	1	mnijhuis	flood-risk-maps
3915	3915	Naladovky		[]		0	2	0	silvia3377	naladovky
3916	3916	MH - Wipro Sustainable ML Challenge		['earth and nature']		2	112	1	rajatranjan	mh-wipro-sustainable-ml-challenge
3917	3917	Sydney House Prices	Greater Sydney Property Sample from 2016 - 2021	['australia', 'real estate', 'economics', 'tabular data']	Residential property information web-scraped off https://domain.com.au, joined with suburb data and economic data.	9	32	0	alexlau203	sydney-house-prices
3918	3918	snappfood-reviews		['ratings and reviews']		3	6	0	no3ignalno3ignal	snappfoodreviews
3919	3919	bornes-irve		[]		0	8	0	wadietorjmane	bornesirve
3920	3920	TMDB Movies 		['movies and tv shows']		3	38	1	mohammedmustafa13	tmdb-movies
3921	3921	simple regression sample	simple regression dataset 800 rows 5 cols	['beginner', 'tabular data', 'regression']	"simple regresssion dataset 
800 rows 5 columns (id,target,feature*3)
id - id 
target - target
column - col1,col2,col3"	33	395	4	rhythmcam	simple-regression-dataset-2002-01-25
3922	3922	Market		['business']		0	9	0	harshshaw5	market
3923	3923	Самые честные новости	"Архив ""Самых честных новостей"" Артемия Лебедева с комментами и субтитрами"	['russia', 'nlp', 'news']	"По-русски получается так что мат — это инструмент честности. Люди очень хорошо слышат это. Просто когда ты включаешь телек и видишь, как какой-нибудь чиновник комментирует что либо — они не могут просто сказать, что “преступник убежал в лес”, они скажут что-то вроде “подозреваемый выдвинулся в сторону лесного массива” — почему ты разучился говорить как нормальный человек?? Ну а я бы сказал еще короче. © Артемий Лебедев
Context
TBD
Content
TBD
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
TBD
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Подсмотреть тут:
Как писать посты в стиле Артемия Лебедева? Подробный анализ телеграм-канала и кое-что еще
Сравнительный анализ тональности комментариев в YouTube"	7	373	2	stassl	lebedev-news
3924	3924	jigsaw classification bert 5folds		['puzzles']		1	118	0	yuzhoudiyishuai	jigsaw-classification-bert-5folds
3925	3925	rfmodel		[]		0	0	0	gamingnation	rfmodel
3926	3926	Word association network with PyDictionary	List of words and the words that are associated with by PyDictionary	['artificial intelligence', 'beginner', 'intermediate', 'text data', 'pandas']	"Context
My purpose of creating this dataset was to imitate a hypothesis about how our brain represents abstract objects. The theory suggests that synaptic connections of many neurons are from now on a piece of representation of the real world and we can create a simulation of real world in our mind by associating those representations with each other. This dataset represents a one dimensional associtave network which is only consisting of english words.
Content
This file is consisting of words and their frequency of use for defining each other in the PyDictionary. Basically a word is defined by a bunch of other words. We assume that those words are connected to the current word and we create associations between each of them. 
word_column : A word
meaning : This column includes the words that are used to define the word and frequencies of the words"" use.
Inspiration
I would like people to create more datasets starting from different words and use these datasets on different Natural Language processing projects and tasks."	3	126	2	b3d1rhan	word-association-network-with-pydictionary
3927	3927	tgbr-train-ftl-ub		[]		0	0	0	prateekagnihotri	tgbr-train-ftl-ub
3928	3928	ubiquanttrainpicklefile	pickle file of train.csv in Ubiquant competition	[]		1	20	0	shashimo	ubiquanttrainpicklefile
3929	3929	Cleaned datasets		[]		1	20	1	gamingnation	cleaned-datasets
3930	3930	List of the Best Locations for Rentals in Pakistan		[]		2	36	1	faizankamran	blogs
3931	3931	liberrex		[]		0	4	0	rjouba2	liberrex
3932	3932	Sir. Gopalakrishnan Kumar	Dataset for analysis of BMI	['movies and tv shows']		1	21	1	gopalkk2	sirgopalakrishnankumar
3933	3933	tgbr-train-ftl-w		['transportation']		0	2	0	prateekagnihotri	tgbr-train-ftl-w
3934	3934	ubiquant_folds		[]		0	1	1	nischaydnk	ubiquant-folds
3935	3935	filllw		[]		0	6	0	murtadhayaseen	filllw
3936	3936	tgbr-train-ax2-w		['transportation']		0	1	1	prateekagnihotri	tgbr-train-ax2-w
3937	3937	Best VFX rotoscoping services		[]		0	1	0	rotomakervfxservices	best-vfx-rotoscoping-services
3938	3938	Human Activity Recognition Using Smartphone Data		[]		0	15	0	devyansh30gupta	human-activity-recognition-using-smartphone-data
3939	3939	Body Fat Estimator	Dataset to estimate the fat percentage in one's body	['health', 'decision tree']		6	48	1	aggle6666	body-fat-estimator
3940	3940	nba_21-22_player_stats		['basketball', 'united states', 'sports']		0	24	1	zgrbalbay	nba-21-22
3941	3941	microsoft-malware-prediction		[]		7	53	1	mdsadikurrahman	microsoftmalwareprediction
3942	3942	Predict Clearsky Global Horizontal Irradiance(GHI)		['categorical data', 'atmospheric science', 'statistical analysis', 'regression']	"Data Attributes:
‘Year',
'Month',
'Day',
'Hour',
'Minute',
'Temperature', 0C
'Clearsky DHI', w/m2
'Clearsky DNI', w/m2
'Clearsky GHI', w/m2
'Cloud Type',
    Cloud Type 0    Clear
    Cloud Type 1    Probably Clear
    Cloud Type 2    Fog
    Cloud Type 3    Water
    Cloud Type 4    Super-Cooled Water
    Cloud Type 5    Mixed
    Cloud Type 6    Opaque Ice
    Cloud Type 7    Cirrus
    Cloud Type 8    Overlapping
    Cloud Type 9    Overshooting
    Cloud Type 10    Unknown
    Cloud Type 11    Dust
    Cloud Type 12    Smoke
    Cloud Type -15    N/A
'Dew Point', C
'Fill Flag',
    Fill Flag 0    N/A
    Fill Flag 1    Missing Image
    Fill Flag 2    Low Irradiance
    Fill Flag 3    Exceeds Clearsky
    Fill Flag 4    Missing CLoud Properties
    Fill Flag 5    Rayleigh Violation
    Fill Flag any   N/A
'Relative Humidity', %
'Solar Zenith Angle', Degree to calculate cos(θ)
'Surface Albedo', flux per unit area
'Pressure', mbar
'Precipitable Water', cm
'Wind Direction', Degrees
'Wind Speed' m/s"	8	171	6	varun23	predict-clearsky-global-horizontal-irradianceghi
3943	3943	yolov5-font		[]		0	5	0	kmurano	yolov5font
3944	3944	facemask		[]		1	6	0	namanshr1403	facemask
3945	3945	gutenberg-dataset		['culture and humanities']		0	8	2	annas1301	gutenberg-dataset
3946	3946	ubiquant-market-tfdataset		[]		0	24	0	tchaye59	ubiquantmarkettfdataset
3947	3947	Employee Attrition	Predicting Employee Turnover	['employment', 'business', 'classification']	Employee Attrition is&nbsp;when an employee leaves the company through any method, including voluntary resignations, layoffs, failure to return from a leave of absence, or even illness or death. Every year a lot of companies hire a number of employees. The companies invest time and money in training those employees, not just this but there are training programs within the companies for their existing employees as well. The concept of these programs is to increase the effectiveness of their employees. If there is any model that can predict employee attrition it will be easy to analyze the attrition process and improving the performance of employees.	57	346	2	sonalishanbhag	employee-attrition
3948	3948	modelresultsfold5b7tt2		[]		0	8	0	mollelmichael	model1resultsfold5b7tt2
3949	3949	vishleshan		[]		0	11	0	harshasharawala	vishleshan
3950	3950	EMSIG_train_label		[]		1	9	0	bingliangli	emsig-train-label
3951	3951	EMSIG_spectrum		[]		2	10	0	bingliangli	emsig-spectrum
3952	3952	Cyclistic Data		[]		1	13	0	bonnieyew	cyclistic-data
3953	3953	Assembly Election Results Kerala 2021	Constituencywise Assembly Election Results of Kerala in 2021	['politics']	"This dataset contains the details of Assembly Constituency election in Kerala in the year 2021.
Attributes
AC Name - Name of the assembly constituency
AC No - Constituency number
Type - Type of the constituency whether general or reserved for various categories
District - District of Kerala in which the constituency is present
Winning candidate - Name of the candidate who won in the election
Party - Political party of the winning candidate
Total Electors - Total no of voters in the constituency
Total votes - Total number of votes polled in the election
Poll% - polling percentage
Margin - difference between share of votes cast for the winning candidate and the second place candidate in the election
Margin% - percentage of margin
Source
https://www.indiavotes.com/"	54	660	28	aryakrishnanar	assembly-election-results-kerala-2021
3954	3954	Narrative Essay Spreading out Method		[]		0	30	0	deneenbruss	narrative-essay-spreading-out-method
3955	3955	Managing Discoursed in a Story Piece		[]		0	32	0	deneenbruss	managing-discoursed-in-a-story-piece
3956	3956	Midwest Cities Weather Data 2021	Daily weather data of top 5 midwest cities in the year of 2021.	['united states', 'weather and climate', 'beginner', 'time series analysis', 'tabular data']	"Context
This dataset contains weather data for top 5 midwest cities in the US for the year of 2021.
Content
This dataset includes temperature numbers, as well as other weather elements including snowdepth, windspeed, visibility, etc.
Acknowledgements
The data is downloaded as a CSV using Python with the Visual Crossing Weather API.
Link to the API: https://www.visualcrossing.com/weather-data
Inspiration
Weather forecasting;
Sunrise / sunset schedule patterns with season changes;
Moon-phase cycle;"	10	98	1	chuyuchen	midwest-cities-weather-data-2021
3957	3957	Unsupervised Brain Tumor Weights		['biology']		0	7	0	danieldelro	unsupervised-brain-tumor-weights
3958	3958	2022-01-25		[]		1	22	0	maruihanrobin	20220125
3959	3959	Convincing Approaches to making an  Article		[]		0	43	0	deneenbruss	convincing-approaches-to-making-an-article
3960	3960	Splendid Kinds of Papers with Models - 2022 		[]		0	36	0	deneenbruss	splendid-kinds-of-papers-with-models-2022
3961	3961	Brilliant Record Article and Certain Essays 		['literature']		0	42	0	deneenbruss	brilliant-record-article-and-certain-essays
3962	3962	Best Machine Learning coding books		['education', 'computer science']		0	8	0	codingvidya	machine-learning-books
3963	3963	XGBoost_first_testing		[]		0	20	0	richieouo	xgboost-first-testing
3964	3964	cow_seg		[]		2	15	0	aimanlim0	cow-seg
3965	3965	OSOCR-training		['education']		0	15	0	vsdf2898kaggle	osocrtraining
3966	3966	amazingrobertaman		[]		0	4	0	thispathml	amazingrobertaman
3967	3967	TOP10 STEAM GAME REVIEWS DATASET(Last 3 Months)	Predict whether use recommends the game or not	['games', 'video games', 'beginner', 'intermediate', 'data cleaning', 'text data']	"E-Sports is one of the fastest growing industries in current times. But with every industry, there are flaws that needs to be filled in order to cover the issues and increase even more rapidly. 
This dataset contain all the useful reviews that were scraped from steamcommunity.com. The reviews are from the games that had highest number of active users on 18th of January. The game names are:
- DOTA 2
- CSGO
- Apex legends
- Team Fortress 2
- GTA V
- Naraka:Bladepoint
- Monster Hunter Rise
- MIR4
- PUBG
- RUST
As a student who is very enthusiast for the data analyst, I wanted to began my work on sentiment analysis. The special Thanks to steamcommunity for making there reviews easier to extract. These Reviews will surely help you understand sentiment analysis in depth. Moreover you can perform different data visualization on this dataset for future works"	1	44	1	muhammadiqbalmukati	top10-steam-games-dataset
3968	3968	word2vec_google_news_300d_pretrained_embeddings		[]		0	1	0	ikaytas	word2vec-google-news-300d-pretrained-embeddings
3969	3969	The Lick	Examples of a single instrument playing 'The Lick' motif	[]		1	18	0	tylerweir	the-lick
3970	3970	Landslide Prediction for Muzaffarabad-Pakistan	Predict landslide in muzaffarabad region	[]	"Context
The data set is for landslide prediction of Muzaffarabad, Pakistan.
Acknowledgements
Aslam, Bilal, Adeel Zafar, and Umer Khalil. ""Development of integrated deep learning and machine learning algorithm for the assessment of landslide hazard potential."" Soft Computing 25, no. 21 (2021): 13493-13512."	37	222	8	adizafar	landslide-prediction-for-muzaffarabadpakistan
3971	3971	UMP lag freatures		[]		1	51	1	takamichitoda	ump-lag-freatures
3972	3972	Eye Iris Dataset		[]	"This is the preprocessed version of the iris dataset of IITD. 
Each row correspond to an iris.
There are 22 columns from which 21 are the attributes and the last column is the class column.
The class column numbers between 1 to 224, iris data belonging to class 1 is basically iris data obtained by processing different images of the same person.
I hereby request you to apply some classification method so that this data is classified into 224 classes.
And also give the accuracy of the model."	1	27	0	sack0hli	eye-iris-dataset
3973	3973	SongPopularityPrediction|Blending,Fold		['beginner', 'intermediate']		2	19	4	venkatkumar001	songpopularityprediction-blending-fold
3974	3974	Foods Dataset	Foods from SEA region	['nutrition']		0	20	1	johnsmith0007	foods1
3975	3975	pengcheng2022		[]		3	25	1	yuanzhezhou	pengcheng2022
3976	3976	speech2text		[]		11	34	1	datlq98	speech2text
3977	3977	lexp00224		[]		0	5	0	phoenix9032	lexp00224
3978	3978	yoloxx_1280x768_ap59_fold4_best		[]		1	5	0	dragonzhang	yoloxx-1280x768-ap59-fold4-best
3979	3979	grsupple		[]		0	5	0	csleet	grsupple
3980	3980	Trainingset		[]		0	5	0	sitimujilahwati	trainingset
3981	3981	试一下怎么用		[]		0	7	0	wanheiqiyihu	11111
3982	3982	nsk_image_search3_man6e		[]		0	1	0	motono0223	nsk-image-search3-man6e
3983	3983	feature_map_np1		[]		0	1	0	ananyagr	feature-map-np1
3984	3984	feature_map_cn1		[]		0	0	0	ananyagr	feature-map-cn1
3985	3985	Original synthesis + icnet final		['earth and nature']		0	5	0	laxminaidu07	original-synthesis-icnet-final
3986	3986	heart_data		[]		0	5	0	jackchenlalala	heart-data
3987	3987	longformer_zn		[]		0	7	0	yankuoaaagmailcom	longformer-zn
3988	3988	Dashboard by HR dep about the employees		[]		3	12	0	shaimaamadkour	dashboard-by-hr-dep-about-the-employees
3989	3989	Top Tech Youtuber Video Statistics Data	Get Views, Comments and Likes for entire life span of top 8 tech Ytube Channels	['celebrities', 'tabular data', 'video data', 'ratings and reviews', 'social networks']		17	143	3	ashishmhatre927	top-tech-youtuber-video-statistics-data
3990	3990	Rihanna Lyrics	Lyrics From Rihanna Albums	['exploratory data analysis']	"Context
This dataset contains Rihanna lyrics from Music of the Sun, A Girl like Me, Good Girl Gone Bad, Rated R, Loud, Talk That Talk, Unapologetic and ANTI albums.
Content
+100 rows and 5 columns.
Columns' description are listed below.
album : Album title
song : Song title
lyrics : Song lyrics
year : Album release year
views : View count of lyrics on genius.com
Acknowledgements
Data from Genius.
Image from Rihanna.
If you're reading this, please upvote."	28	458	9	vivovinco	rihanna-lyrics
3991	3991	G2Net Gravitational Wave Detection TFRecords		['physics']		0	9	0	ningliu	g2net-gravitational-wave-detection-tfrecords
3992	3992	Google_Capstone_Bike_Share		[]		0	11	0	ryanneilscott	google-capstone-bike-share
3993	3993	Audio npy		['arts and entertainment']		0	2	0	evergreen001	audio-npy
3994	3994	pokemon_imagesx512		[]		0	7	0	reelord42069	pokemon-imagesx512
3995	3995	holidays		['travel']		2	5	0	khurana0112	holidays
3996	3996	 Curso regular mayo 2019 ESPOL		[]		2	23	0	rubenhuntervera	curso-regular-mayo-2019-espol
3997	3997	Stellar (XLM) Cryptocurrency Dataset	Data from September 17, 2014 to November 29, 2021	['history', 'finance', 'time series analysis', 'investing', 'currencies and foreign exchange']	"Context
Stellar (XLM) is an open network that allows money to be moved and stored. When it was released in July 2014, one of its goals was boosting financial inclusion by reaching the world’s unbanked — but soon afterwards, its priorities shifted to helping financial firms connect with one another through blockchain technology.
The network’s native token, lumens, serves as a bridge that makes it less expensive to trade assets across borders. All of this aims to challenge existing payment providers, who often charge high fees for a similar service.
If all of this sounds familiar, it is worth noting that Stellar was originally based on the Ripple Labs protocol. The blockchain was created as a result of hard fork, and the code was subsequently rewritten.
Learn more: https://www.stellar.org/lumens
Content
Three CSVs of trading data for Stellar XLM cryptocurrency from September 17, 2014 to November 29, 2021:
1.  Daily trading data, approx. 2602 rows.
2. Weekly trading data, approx. 85 rows.
3. Monthly trading data, approx. 372 rows.
Column Attributes (all prices are in USD):
- Date      - Start date of the trades.
- Open      - Price of the first trade.
- High      - Maximum price of trading day.
- Low       - Minimum price of trading day.
- Close     - Price of the last trade.
- Adj Close - Closing price adjusted to reflect the value after accounting for any corporate actions.
- Volume    - Number of units traded.
Acknowledgements
Stellar XLM dataset downloaded from Yahoo Finance.
Inspiration
DeFi, crypto and blockchain."	152	1380	15	adebayo	stellar-cryptocurrency-dataset
3998	3998	dataset3		[]		0	4	0	flutterdevelopers	dataset3
3999	3999	Iris Flower		[]		1	9	0	ciyakhan	iris-flower
4000	4000	Evaluatung students writing - labelled sentences		[]		0	7	0	alicematusheva	evaluatung-students-writing-labelled-sentences
4001	4001	flavors of cocoa Rating		['food']		2	14	1	ciyakhan	flavors-of-cocoa-rating
4002	4002	Warith akbar the Owner 		['arts and entertainment']		0	1	0	warithakbar	warith-akbar-the-owner
4003	4003	Employees		['art', 'dplyr', 'matplotlib', 'numpy', 'pandas']		7	30	8	qusaybtoush	employees
4004	4004	Cars Large Dataset		['automobiles and vehicles']		2	20	1	ciyakhan	cars-large-dataset
4005	4005	yolox-l-coco		[]		0	1	0	sangayb	yolox-l-coco
4006	4006	Driving Lane Detection 		['law']		21	498	5	srinjoybhuiya	driving-lane-detection
4007	4007	sales records		[]		0	15	0	dahabb	sales-records
4008	4008	reef-yolox-m-v0		[]		0	6	0	sangayb	reef-yolox-m-v0
4009	4009	European Club Football Dataset	Data for over 23k matches from various top football/soccer leagues in Europe	['football', 'europe', 'sports', 'tabular data']	"Includes data for over 23000 matches and over 2 million events for those matches!
Content
This dataset contains information on 6 of the top European football/soccer leagues. I plan on updating this dataset weekly/biweekly with data for new matches played as well as gradually going backwards for game data as well. 
(All data listed below is through roughly present during the current season.)
Data start years:
English Premier League
 Game Data - 2001
 Aggregate Stats - 2002 
** Tables - 2001
Spanish La Liga
 Game Data - 2004
 Aggregate Stats - 2002
** Tables - 2000
German Bundesliga
 Game Data - 2002
 Aggregate Stats - 2002
** Tables - 2000
Italian Serie A
 Game Data - 2016
 Aggregate Stats - 2001
** Tables - 2000
Dutch Eredivisie
 Game Data - 2018
 Aggregate Stats - 2001
** Tables - 2000
French Ligue 1
 Game Data - 2018
 Aggregate Stats - 2002
** Tables - 2002
Some notes:
* Year as a column refers to the year a season started in. So if a match was played in January 2021, it's value for year would be 2020 because that season started in 2020. 
* Some older matches have no commentary, but they do have one row in events.csv to denote such
Acknowledgements
ESPN, as that's where this data is scraped from
Image
Inspiration
How do the leagues compare in things like goals per game and red cards per team per season?
Which teams across the leagues foul/get fouled the most and the least per year?
SkillCorner has some interesting data here that may be worth a bit of your time to check out."	252	2215	17	josephvm	european-club-football-dataset
4010	4010	Bank Marketing Case-Study		['business']		3	36	0	ciyakhan	bank-marketing-casestudy
4011	4011	Dummy Water Tracking Data Set		['business', 'education']	"Context
This dataset created for USKUDAR UNIVERSITY DATA MINING EXERCISE
Content
it has simple water tracking onboarding informations and overall daily_drink for each user"	1	25	0	berkaykzgr	watertrackingdataset
4012	4012	snapshot-640		[]		0	11	1	ionafinasnfsdsoifso	snapshot640
4013	4013	Saint Lucia Monthly Temperatures 1973 to 2017	Data from Saint Lucia's Open Data Portal	['earth and nature', 'beginner', 'data visualization', 'dplyr']		0	15	3	janaislu	saint-lucia-monthly-temperatures-1973-to-2017
4014	4014	Cars Crash Test		[]		0	20	0	ciyakhan	cars-crash-test
4015	4015	map_chicago		[]		0	11	0	slaryc	map-chicago
4016	4016	Admission Prediction		[]		0	17	0	ciyakhan	admission-prediction
4017	4017	US Arrests		[]		1	13	0	ciyakhan	us-arrests
4018	4018	Police recorded crimes of England and Wales	Exploratory data analysis of crimes	['public safety']	"Context
It will show how many crimes were reported by police by Community Safety Partnership area, England and Wales, in the year ending June 2021. However, new IT systems were put in place in July 2019 and Greater Manchester Police have not had access to data for the months of July 2019 to June 2021. Therefore, figures for Greater Manchester are not included in the totals. Caution must be taken when interpreting small numbers of offences.
Content
24 attributes; This shows the number of crimes recorded by police in  under different offences and police area. Some of them are Total recorded crime (excluding fraud), Violence against the person, Homicide, Death or serious injury caused by illegal driving etc,.
43 observations; This shows  the ""Police Force Area name"" such as Avon and Somerset, Bedfordshire, Cambridgeshire, Cheshire, City of London, Cleveland, Cumbria etc,.
Acknowledgements
Home Office - Police recorded crime
Inspiration
You should find out through exploratory data analysis that which regions are safest and which are not. Which offences are the lowest in which regions."	5	54	3	alikashif1994	police-recorded-crimes-of-england-and-wales
4019	4019	Analyzing traffic stops weather policing activity		[]		0	8	0	kakamana	analyzing-traffic-stops-weather-policing-activity
4020	4020	Credit Score		[]		6	44	0	ciyakhan	credit-score
4021	4021	TripDetails		[]		1	5	1	ciyakhan	tripdetails
4022	4022	hateBert-model		['clothing and accessories']		0	10	0	chiranthans23	hatebert-model
4023	4023	Sales of cars production and distribuation		[]		1	29	0	shaimaamadkour	sales-of-cars-production-and-distribuation
4024	4024	Hyperspectral Atmospheric Compensation		['atmospheric science']		0	22	0	billbasener	hyperspectral-atmospheric-compensation
4025	4025	sample_sales_records		[]		0	5	0	dahabb	sample-sales-records
4026	4026	meat_data		[]		1	13	1	kenials	meat-data
4027	4027	Just Dance @ YouTube: Multi-label Text + Analytics	User's comments. Usability. User Experience. Health. QoL. Sentiment Analysis.	['universities and colleges', 'arts and entertainment', 'video games', 'nlp', 'text mining', 'data analytics', 'text data', 'social networks']	"Context
With the growth of social media and the spread of the Internet, the user's opinion become accessible in public forums. It became then possible to analyse and extract knowledge based on the textual data published by users, through the application of Natural Language Processing and Text Mining techniques. In this dissertation, these techniques are used to, based on comments posted by users on YouTube, extract information about Usability, User Experience (UX), and Perceived Health Impacts related to Quality of Life (H-QoL). This analysis focus on videos about the Just Dance series, one of the most popular interactive dance video games.
Just Dance belongs in a category of games whose purpose goes beyond entertainment - serious games - among which there is a specific type of games, exergames, which aim is to promote physical activity. Despite their positive influence on the health of their users, these often stop playing after a short period of time, leading to the loss of benefits in the medium and long term. It is in this context that the need to better understand the experience and opinions of players arises, especially how they feel and how they like to interact, so that the knowledge generated can be used to redesign games, so that these can increasingly address the preferences of end-users.
It is with this purpose, that in a serious game it is necessary to assure not only the fundamental characteristics of the functioning system, but also to provide the best possible experience and, at the same time, to understand if these positively impact players' lives. In this way, this work analyses three dimensions, observing, besides Usability and UX aspects, also H-QoL, in the corpus extrated.
To meet the objectives, a tool was developed that extracts information from user comments on YouTube, a social media network that despite being one of the most popular, still has been little explored as a source for opinion mining. To extract information about Usability, UX and H-QoL, a pre-established vocabulary was used with an approach based on the lexicon of the English idiom and its semantic relations. In this way, the presence of 38 concepts (five of Usability, 18 of UX, and 15 of H-QoL) was annotated, and the sentiment of each comment was also analysed. Given the lack of a vocabulary that allowed for the analysis of the dimension related to H-QoL, the concepts identified in the World Health Organization's WHOQOL-100 questionnaire were validated for user opinion mining purposes with ten specialists in the Health and Quality of Life domains.
The results of the information extration are displayed in a public dashboard that allows visitors to explore and analyse the existing data. Until the moment of this work, 543 405 comments were collected from 32 158 videos, in which about 52% contain information related to the three dimensions. The performance of this annotation process, as measured through human validation with eight collaborators, obtained an general efficacy of 85%.
Content
There are three datasets related with Just Dance game on YouTube, with:
- All the user's comments extracted, with some informations about them and with sentiment analysis
- Analytics collected from YouTube, related with comments, videos and channels 
- All the data analyzed in the work, with the annotation of the 38 concepts under study
Project
Developed by Renato Santos in the context of the Master Degree in Informatics Engineering, DEI-FCTUC, dissertation titled ""Analysing Usability, User Experience, and Perceived Health Impacts related to Quality of Life based on Users' Opinion Mining"", under the supervision of Paula Alexandra Silva and Joel Perdiz Arrais.
More information
Check more about this project: https://linktr.ee/justdanceproject
Contact
If you have any questions or suggestions, please e-mail us on renatojms@student.dei.uc.pt"	60	1326	6	renatojmsantos	just-dance-on-youtube
4028	4028	Pytorch Forecasting WHL		[]		0	4	1	steveroberts	pytorch-forecasting-whl
4029	4029	Retail Price Optimization	“The moment you make a mistake in pricing, you’re eating into your reputation” 	['business', 'exploratory data analysis', 'data visualization', 'model comparison', 'regression', 'optimization']	"Context
Price optimization is using historical data to identify the most appropriate price of a product or a service that maximizes the company’s profitability. There are numerous factors like demography, operating costs, survey data, etc that play a role in efficient pricing, it also depends on the nature of businesses and the product that is served. The business regularly adds/upgrades features to bring more value to the product and this obviously has a cost associated with it in terms of effort, time, and most importantly companies reputation.
As a result, it is important to understand the correct pricing, a little too high, you lose your customers and slight underpricing will result in loss of revenue. Price optimization helps businesses strike the right balance of efficient pricing, achieving profit objectives, and also serve their customers.
Content
The data contains the demand and corresponding average unit price at a product - month_year level
Tasks
Exploratory data analysis
Data visualization
Demand forecasting
Price optimization"	166	1437	12	suddharshan	retail-price-optimization
4030	4030	Churn_Modelling		[]		1	3	0	nirmalthomas13	churn-modelling
4031	4031	20000 iterations plate detection		['programming']		0	16	0	romainsr	20000-iterations-plate-detection
4032	4032	datasets from ThinkStats by Allen B. Downey		[]	"Datasets to work through  ""Think Stats: Exploratory Data Analysis in Python"" by Allen B. Downey.
My notebooks using these datasets:
part 1
part 2
part 3"	0	8	0	andronikova	thinkstats
4033	4033	Sample Products test		[]		0	6	0	dahabb	sample-products-test
4034	4034	feature_map_nc		[]		0	0	0	ananyagr	feature-map-nc
4035	4035	feature_map_pn		[]		0	0	0	ananyagr	feature-map-pn
4036	4036	datasetB		[]		0	2	0	flutterdevelopers	datasetb
4037	4037	FDM_LinearAdvance		[]		2	27	1	kseniazimenko	fdm-linearadvance
4038	4038	Imbalanced-data 		[]		0	13	0	gamingnation	imbalanced-data
4039	4039	feature_map_np		[]		0	6	0	ananyagr	feature-map-np
4040	4040	feature-map_cn		[]		0	0	0	ananyagr	featuremap-cn
4041	4041	mini_speech_commands	minified speech commands dataset	['earth and nature']		1	11	1	joseamaya22	mini-speech-commands
4042	4042	AIdea Speech Enhancement		[]		1	39	0	zenbot99	aidea-speech-enhancement
4043	4043	jigsaw toxic Comments annotations	Jigsaw Toxic Comments augumetnation of original data with Severity Rate	['earth and nature', 'tabular data', 'text data']	"Context
Augumentation of initial dataset:For using it in Jigsaw Rate Severity of Toxic Comments Example usage: ☣️ Jigsaw - Super Simple Naive Bayes by dataista0 (Julián Peller) Data scientist at Toptal Buenos Aires, Buenos Aires, Argentina
first file contain classification in neutral, offensive or hate
second file contain the agreement between annotators for classification in neutral, offensive or hate
Content
For using it in Jigsaw Rate Severity of Toxic Comments Example usage: ☣️ Jigsaw - Super Simple Naive Bayes by dataista0 (Julián Peller) Data scientist at Toptal Buenos Aires, Buenos Aires, Argentina
agreement probabilities data from:
@inproceedings{hateoffensive, title = {Automated Hate Speech Detection and the Problem of Offensive Language}, author = {Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar}, booktitle = {Proceedings of the 11th International AAAI Conference on Web and Social Media}, series = {ICWSM '17}, year = {2017}, location = {Montreal, Canada}, pages = {512-515} }
the second dataset has an MIT license
Acknowledgements
thanks to dataista0 (Julián Peller) for posting original dataset
and 
Davidson, Thomas for posting Automated Hate Speech Detection and the Problem of Offensive Language
ALSO TO:
Warmsley, Dana and Macy, Michael and Weber, Ingmar
Inspiration
Is Inter-Annotator Agreement relevant for offensiveness detection? Can be predicted? And applied to another datasets?"	6	52	1	crischir	jigsaw-toxic-coomens-annotations
4044	4044	iris.csv		['internet']		0	1	0	smithzhao	iriscsv
4045	4045	vowpal_files_v1		[]		0	5	0	konradb	vowpal-files-v1
4046	4046	Jigsaw_toxic_annotation_copy		[]		1	6	1	crischir	jigsaw-toxic-annotation-copy
4047	4047	covid-190		[]		1	6	0	ananyagr	covid190
4048	4048	Food Recognition 2022: Dataset	Detecting & Segmenting various kinds of food from an image	['earth and nature', 'computer vision', 'food']	"🍕 Food Recognition 2022
Problem Statement
Detecting & Segmenting various kinds of food from an image. For ex. Someone got into new restaurent and get a food that he has never seen, well our DL model is in rescue, so our DL model will help indentifying which food it is from the class our model is being trained on!
Dataset
We will be using data from Food Recognition Challenge - A benchmark for image-based food recognition challange which is running since 2020.
https://www.aicrowd.com/challenges/food-recognition-benchmark-2022#datasets
We have a total of 39k training images with 3k validation set and 4k public-testing set. All the images are RGB and annotations exist in MS-COCO format.
Evaluation
The evaluation metrics is IOU aka. Intersection Over Union ( more about that later ).
The actualy metric is computed by averaging over all the precision and recall values for IOU which greater than 0.5.
https://www.aicrowd.com/challenges/food-recognition-challenge#evaluation-criteria"	41	728	9	awsaf49	food-recognition-2022-dataset
4049	4049	Unicorn Startups 2015 - 2022 (8 Years)	Complete List Of Unicorn Startups Tracked by CB Insights 	['business']	Complete list of unicorn startups tracked and published by CB insights from Dec 2015 to Jan 2022.	5	50	1	cheeann290	unicorn
4050	4050	ft_mohen_bert_uncased_toxicity		[]		2	10	0	hangy132	ft-mohen-bert-uncased-toxicity
4051	4051	covid_19		[]		1	4	0	ananyagr	covid-19
4052	4052	house_tiny		[]		0	2	0	ppcastle	house-tiny
4053	4053	ratings		['ratings and reviews']		0	11	0	andreagironda	ratings
4054	4054	Roberta-folds		['music']		0	10	0	vigneshirtt	robertafolds
4055	4055	Forbes Billionaire 2020		['business']		7	41	0	eptehalnashnoush	forbes-billionaire-2020
4056	4056	convnext		[]		0	10	0	mmelahi	convnext
4057	4057	modeles2		[]		0	2	0	samuelguerin	modeles2
4058	4058	CUB Médio Brasil	CUB Médio Brasil - Custo Unitário Básico de Construção por m²	['economics']	"PT-BR
CUB Médio Brasil - Custo Unitário Básico de Construção por m²
O Custo Unitário Básico de Construção (CUB) é um indicador de custos no setor da construção calculado e divulgado pelos Sinduscons estatuais e regido pela Lei Federal 4.591/64.
O CUB Brasil é uma média ponderada dos indicadores de alguns dos principais estados da federação. Este tópico contém informações sobre a evolução do CUB Brasil e dos estados que o compõem.
Colunas:
state: unidade federativa do brasil
global_value: valor total
cm_value: valor componentes materiais
cmo_value: valor componentes mão de obra
cda_value: valor componentes despesa administrativa
ce_value: valor componentes equipamento
(global_value = cm_value + cmo_value  + cda_value + ce_value )
Todas as colunas  de valor utilizam a unidade R$/m²
ca50_value: evolução do valor do aço ca50 10mm no período (R$/Kg)
EN
Average CUB Brazil - Basic Unit Cost of Construction per m²
The Basic Unit Cost of Construction (CUB) is an indicator of costs in the construction sector calculated and published by the state-owned Sinduscons and governed by Federal Law 4,591/64.
CUB Brasil is a weighted average of the indicators of some of the main states of the federation. This topic contains information about the evolution of CUB Brasil and the states that comprise it.
Columns:
state: federative unit of brazil
global_value: total value
cm_value: material components value
cmo_value: labor components value
cda_value: administrative expense component value
ce_value: equipment components value
(global_value = cm_value + cmo_value + cda_value + ce_value )
All value columns use the R$/m² unit
ca50_value: evolution of the value of ca50 10mm steel in the period (R$/Kg)
Fonte / Source
http://www.cbicdados.com.br/menu/custo-da-construcao/cub-medio-brasil-custo-unitario-basico-de-construcao-por-m2
http://www.cbicdados.com.br/menu/materiais-de-construcao/aco-10mm-e-produtos-de-aco-longo"	41	756	18	felipefiorini	cub-mdio-brasil
4059	4059	Simulation Machine - 1198888 claims (R 3.5)		[]	"1198888 Non-Life Insurance Claims Cash Flows for a Synthetic Portfolio
Data Generator:
""An Individual Claims History Simulation Machine"" by Andrea Gabrielli and Mario V. Wüthrich
Data generated with:
""A Neural Network Boosted Double Over-Dispersed Poisson Claims Reserving Model""
Data Generation.R by  Andrea Gabrielli, 28.05.2020
Path: https://github.com/gabrielliandrea/neuralnetworkdoubleODP/blob/master/RCodeNeuralNetworkDoubleODP.zip
Remark: Random number generation has changed with R Version 3.6
To get published results (pre 3.6) the following option is needed:
RNGkind(sample.kind = ""Rounding"")
(source: https://stackoverflow.com/questions/47199415/is-set-seed-consistent-over-different-versions-of-r-and-ubuntu/56381613#56381613)
Count Claims per LoB (1 to 6):
     1      2      3      4      5      6 Sum
250040 250197  99969 249683 249298  99701 1198888
Read:
""Claims Reserving and Neural Networks"" (Doctorial Thesis) by Andrea Gabrielli, ETHZ 2020."	2	14	0	floser	simulation-machine-1198888-claims-r-35
4060	4060	326547		[]		0	0	0	qcq123456789	326547
4061	4061	Modeles		[]		0	2	0	samuelguerin	modeles
4062	4062	Moroccan Stock Prices	Over 70 moroccan company stocks along with the MASI index	['africa', 'business', 'time series analysis', 'clustering', 'k-means']	"Context
Hey! we don't always have to forecast time series am I right ?
We use k-means to cluster about 70 moroccan stock prices to see their influence on market trends indicated by the MASI (Moroccan All Shares Index) index, for the case of time series k-means uses DTW (Dynamic Time Warping) metric which is a better indicator for similiarity for time series data
Our analysis leads us to find out about the companies that flourished despite the pandemic, the ones that did suffer but managed to recover and the ones that suffered the most.
Content
The dataset contains one file with over 75 moroccan companies stocks and the MASI index
Data Source:
this dataset was scraped from LeBoursier"	9	130	9	aymanlafaz	moroccan-stock-prices
4063	4063	final_assign_2022		[]		0	2	0	lishhaasakpal	final-assign-2022
4064	4064	nsk_image_search3_man6d		[]		0	2	0	motono0223	nsk-image-search3-man6d
4065	4065	episode		[]		0	4	0	ayaabdalsalam	episode
4066	4066	my-data-set67		[]		0	5	0	ekpreetsandhu	mydataset67
4067	4067	fractalrecords		[]		0	6	0	blue0620	fractalrecords
4068	4068	lexp00124		[]		0	4	0	phoenix9032	lexp00124
4069	4069	states_india		[]		0	8	1	muskanbhasin	states-india
4070	4070	Israeli Parliament (Knesset) Members 	All members of the Israeli Parliament with affiliation	['politics', 'middle east']	"Context
Data on the Israeli Knesset from the first Knesset to the 24th. 
Content
For Knesset 1-24 this contains:
- Names of all politicians
- The Party they are affiliated with
- The number(s) of parliaments each politician was active in 
- Gender
- Place of Birth
- Place of Death
- image link
- date of birth
- primary language 
etc.
Source:
https://main.knesset.gov.il/mk/Pages/current.aspx?pg=mklist
Wikipedia
Wikidata
Inspiration
During 2019-2020 isreal was stuck in a political deadlock with election following election. This contribution was done in the hope to allow easier access to political data which will enable verifying news as fake or real news."	10	181	5	guybarash	israeli-parliament-knesset-members
4071	4071	eval_student_writing		[]		0	13	0	mermaid	eval-student-writing
4072	4072	train csv		[]		5	5	0	devanshchowdhury	train-csv
4073	4073	configconfigconfig		[]		0	0	0	crained	configconfigconfig
4074	4074	Machine Failure Predictions	machine failure data 	['business', 'programming', 'automobiles and vehicles', 'text data', 'python']		11	69	0	dineshmanikanta	machine-failure-predictions
4075	4075	Carvana		[]		6	29	0	childsparks	carvana
4076	4076	WebQAdata		[]		0	18	0	solitudecosmos	webqadata
4077	4077	TenMins - 20220124		[]		0	17	0	naoyayokohama	tenmins-20220124
4078	4078	Flipkart Headphones Dataset	Dataset containing details of headphone products in flipkart.com 	['websites', 'mobile and wireless', 'electronics', 'tabular data']	"This dataset containing details of various headphones products in India scraped from an e-commerce website 'Flipkart.com'. This dataset has 1000 samples with 7 attributes.
Attributes
Title of the headphone product (title)
Color of the headphone product (color)
Type of the headphone product (type)
Average rating of the product (avg_rating)
Number of ratings available (num_of_ratings)
Selling price of the headphone at the time of scraping (selling_price)
Maximum Retail Price (MRP)
Inspiration
You can use this dataset to answer some interesting questions like-
- Different price range segments for headphones in India
- Brand with most product offerings for the Indian Market
- Brand catering to all different segments (low price range, mid price  range, high price range)
- Most common color and type (on the ear/in the ear etc) offered by various brands 
- Compare Two Brands based on details
- Are higher rated headphones always high-priced or expensive?
- Does a brand have better than 4 ratings for all its products?
   and so on…
If you are interested in how this dataset was created, you may refer to my blogpost here."	143	653	12	shrishailgajbhar	flipkart-headphones-dataset
4079	4079	target-tmp		[]		0	16	0	hayden234	target-tmp
4080	4080	LIFE EXPECTATION	Predict the life expectancy  based on GDP, fertility rate, and population. 	['geography', 'environment', 'people and society', 'social science', 'demographics']		16	51	5	ankumagawa	life-expectation
4081	4081	umap-learn	Uniform Manifold Approximation and Projection (UMAP)	['data visualization', 'clustering', 'dimensionality reduction', 'python']	"UMAP
Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data:
The data is uniformly distributed on a Riemannian manifold;
The Riemannian metric is locally constant (or can be approximated as such);
The manifold is locally connected.
From these assumptions, it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.
The details for the underlying mathematics can be found in our paper on ArXiv:
McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018"	13	1360	4	kozistr	umaplearn
4082	4082	Crypto_Gate		[]		7	191	2	hao0o0o	crypto-gate
4083	4083	Iris Dataset for EDA	Iris Dataset for EDA	['earth and nature', 'computer science']	Iris dataset for EDA. This dataset consists petal length and width , sepal length and width and name of species.	8	82	3	mdjafrilalamshihab	iris-dataset-for-eda
4084	4084	GE Soccer Clubs News	News from the largest brazilian sports site about soccer clubs (pt-br)	['football', 'brazil', 'sports', 'nlp', 'news']	"Context
This dataset is intended to provide a very real-world data sample covering a reasonable time frame. It contains a column with the club name, which can be considered as a class. 
Content
The news was extracted from the GE website considering all available articles for a team but in a random order, so it covers the whole period: 2015-2020. It's expected to extract all the available articles for the given clubs present in the dataset and adding new clubs to it. 
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Known issue: there's almost no news between 2015-12 and 2016-07. They are not in the scraped list. I'm looking for a workaround. 
Notice:
- A same article can appear to n teams, and it's represented by n different rows with the column ""club"" being different
Acknowledgements
All the article contents are owned by GE and this dataset is solely an effort to put everything together to be easily available for data science experiments and research. 
Inspiration
This dataset has a nice time frame so it's very good to check how things evolve over time. It's possible to run sentiment analysis and also a classification using the ""club"" column as a target - it might be interesting to exclude the club name from the articles then.
Citation
Remember to fix the date and version when citing it.
@unpublished{Moneda2020genews,
  title={Globo Esporte News dataset},
  author={Moneda, Luis},
  year={2020},
  note={Version 11. Retrieved March 31, 2021 from https://www.kaggle.com/lgmoneda/ge-soccer-clubs-news}
}"	173	7079	25	lgmoneda	ge-soccer-clubs-news
4085	4085	training		['computer science']	"Dataset made up of thousands of images of 59 handwritten digits, which are listed in the ""label_dict"" (label dictionary) inside dataset
4 different convolutional neural networks ADAPTED to this project: AlexNet, MyLeNet, SqueezeNet and ZFNet
Goal: trying to get the best accuracy in handwritten recognition
Currently training in Kaggle notebooks. How? Calling the class ""AlexNet"", ""LeNet"", ""SqueezeNet"" or ""ZFNet"" as a model and running ""train"" method
To Do's: optimize these neural networks and see how results improve. Approaches:
Manual adjustment: LR, batch size, number of epochs, number of convolutions, filter size
Reduce LR on pleateau
Add dropout layers
DATA BALANCING (current dataset is very imbalanced)"	0	85	1	mariaros1	training
4086	4086	tfgbr-stage2-v3-yolov5m-64		[]		0	8	0	ks2019	tfgbr-stage2-v3-yolov5m-64
4087	4087	Spliting_dataset_according_to_Sir		[]		0	1	0	ragibmehediakash	spliting-dataset-according-to-sir
4088	4088	Divvy Bikeshare Data 2021		[]		0	4	0	kimmosca	divvy-bikeshare-data-2021
4089	4089	Data-Lamia-hassan		[]		0	4	0	lamiaazzouzi	datalamiahassan
4090	4090	InsureMe Historical Insurance Claims		['insurance']		13	131	0	computingschool	insureme-historical-insurance-claims
4091	4091	ESR+code		['earth and nature']		0	11	0	omkar445	esrcode
4092	4092	video of traffic		[]		0	14	0	romainsr	video-of-traffic
4093	4093	tfgbr-stage2-v4-yolov5m-64		[]		0	10	0	ks2019	tfgbr-stage2-v4-yolov5m-64
4094	4094	YOLOX_update		[]		0	2	0	lebornjian	yolox-update
4095	4095	Heart Failure Prediction : Comparing models		['heart conditions']		4	33	0	adarshkl08	heart-failure-prediction-comparing-models
4096	4096	TPH-Yolov5_Paper		[]		1	14	0	dwchen	tphyolov5-paper
4097	4097	Understanding Different File Formats	csv, json, ods, xml, xls	['india', 'agriculture', 'computer science', 'software', 'tabular data', 'json', 'pandas']	"Working with different File Formats
I have created supporting notebook to explain all these file formats. You can find the notebook here:- https://www.kaggle.com/raj401/working-with-different-file-formats
The Datasets
datafile.csv
datafile.json
datafile.ods
datafile.xls
All the above Datasets contain same data in different file formats.
The data shows pattern of land utilisation under categories such as orest, permanent pastures and another grazing lands, land not available for cultivation etc.
The data contains following features:-
'Year (Col.1)'
 'Geographical Area (Col.2)'
 'Reporting area for Land utilisation statistics (Col.3 = Col.4+Col.7+ Col.11+Col.14+Col.15)'
 'Forests (Col.4)'
 'Not available for cultivation - Area under non-agricultural uses (Col.5)'
 'Not available for cultivation - Barren and unculturable Land (Col.6)'
 'Not available for cultivation - Total (Col.7 = Col.5+Col.6)'
 'Other uncultivated Land excluding Fallow Land - Permanent pastures & other Grazing Lands (Col.8)'
 'Other uncultivated Land excluding Fallow Land - Land under Misc. tree crops & groves (not incl. in net area sown) (Col.9)'
 'Other uncultivated Land excluding Fallow Land - Culturable waste Land (Col.10)'
 'Other uncultivated Land excluding Fallow Land - Total (Col.11 = Col.8 to Col.10)'
 'Fallow Lands - Fallow Lands other than current fallows (Col.12)'
 'Fallow Lands - Current fallows (Col.13)'
 'Fallow Lands - Total Col.14 = (Col.12+Col.13)'
 'Net area Sown (Col.15)'
 'Total cropped area (Col.16)'
 'Area sown more than once (Col.17 = Col.16-Col.15)'
 'Agricultural Land/Cultivable Land/Culturable Land/Arable Land (Col.18 = Col.9+Col.10+Col.14+Col.15)'
 'Cultivated Land (Col.19 = Col.13+Col.15)'
 'Cropping Intensity (Col.20 = % of Col.16 over Col.15)'
Acknowledgements
I am really thankful to Indian government for storing these valuable data. Source:- https://data.gov.in/
Inspiration
I am inspired by everyone here on Kaggle for the level of their dedication and hard work."	4	88	6	raj401	understanding-different-file-formats
4098	4098	Email spam detection		['email and messaging']		1	33	1	poojatomar	email-spam-detection
4099	4099	jigsaw original data		[]		32	85	2	gonnbe	jigsaw-original-data
4100	4100	tfgbr-stage2-v4-yolov5l-64		[]		0	3	0	ks2019	tfgbr-stage2-v4-yolov5l-64
4101	4101	HomePrice		[]		0	15	0	selimreza12020	homeprice
4102	4102	DCIC_ocr	1111111111111111111111111111	[]		0	21	0	panfei748	dcic-ocr
4103	4103	wordbased_bilstm		[]		0	1	0	stutipasricha	wordbased-bilstm
4104	4104	tensorflow-great-barrier-reef		['earth and nature']		0	10	0	kangth1993	tensorflowgreatbarrierreef
4105	4105	Weighted-Boxes-Fusion		[]	new Weighted-Boxes-Fusion.	0	22	0	rishengyang	weightedboxesfusion
4106	4106	BTC_tweets.csv		['internet']		0	1	0	senaada	btc-tweetscsv
4107	4107	Wordle Answers	A list of all the answers from the Wordle game (so far)	['games', 'puzzles', 'text data']	"Context
Wordle is a popular free online word game. To play, you get up to six guesses to guess the secret five-letter word. In each guess, you are given clues as to which of the letter used in your guess feature in the secret word.
Content
This file contains all of the daily Wordle puzzle answers that have been created so far. Any updates to the file will be correct up to that particular day.
Inspiration
This list could be used to evaluate the effectiveness of different strategies that a player could employ to crack the secret word.
Acknowledgements
Data has been obtained from the following links:
- https://gist.github.com/potluck/4ff27425a1896100279a2552f7a062da
- https://progameguides.com/wordle/all-wordle-answers-in-2022-updated-daily/
Thank you to Wordle's developer Josh Wardle for creating this awesome game and making it free for all!"	34	590	5	shamiljamion	wordle-answers
4108	4108	Sharp Test		[]		0	11	0	kshitijmohan	sharp-test
4109	4109	spam or non-spam		[]		1	11	0	saivarun777	spam-or-nonspam2
4110	4110	Sugarcane Disease Dataset	Leaf Disease Detection	[]		32	176	0	prabhakaransoundar	sugarcane-disease-dataset
4111	4111	Blood_AI data set		[]		0	13	2	vgg16rukia	blood-ai-data-set
4112	4112	spam or non-spam		[]		0	16	0	saivarun777	spam-or-nonspam
4113	4113	rtdl_data_lyhue		[]		0	1	0	lyhue1991	rtdl-data-lyhue
4114	4114	VoxConverse Dataset	50+ Hours of Annotated Multi-speaker Audio	['computer science', 'beginner', 'intermediate', 'advanced', 'audio data', 'news']	"The Vox Converse dataset was created by the Visual Geometry Group at Oxford. It contains 50+ hours of multi-speaker audio pulled from YouTube videos, usually in a political debate or news segment context to ensure multi-speaker dialogue. Read the paper here.
This dataset is provided on Kaggle for simple ease of access and processing. vosconverse_dev_wav/audio/ contains .wav files corresponding to .rttm (rich transcription time-marked) files in labels/dev/. Read about .rttm file organization in Appendix A of this document. You can read a text representation of a .rttm file with the pydiarization library. Each .rttm file marks the start and duration of when a unique speaker is talking in the associated audio clip.
You can use this dataset to identify when a particular person is speaking, detect when the speaker changes, and other audio applications.
Notes from the Visual Geometry Group:
&gt; The VoxConverse dataset is available to download for research purposes under a Creative Commons Attribution 4.0 International License. The copyright remains with the original owners of the video.
&gt; In order to obtain videos with a large amount of overlapping speech, we used data consisting of political debates and news segments. The views and opinions expressed by speakers in the dataset are those of the individual speakers and do not necessarily reflect positions of the University of Oxford, Naver Corporation, or the authors of the paper.
&gt; We would also like to note that the distribution of identities in this dataset may not be representative the global human population. Please be careful of unintended societal, gender, racial, linguistic and other biases when training or deploying models trained on this data.
All credit goes to the VGG."	0	21	0	washingtongold	voxconverse-dataset
4115	4115	COTS-IB		[]		18	27	0	truonghuymai	cotsib
4116	4116	hateBERT		[]		0	9	0	yangsuoly	hatebert
4117	4117	mymodel1		[]		0	95	0	mohammadhosseina	mymodel1
4118	4118	yoloxx_ap37_fold1_1280x768_best		[]		0	0	0	dragonzhang	yoloxx-ap37-fold1-1280x768-best
4119	4119	talib-binary		[]		5	15	0	axzhang	talibbinary
4120	4120	Top 50 action movies according to IMDB		['movies and tv shows']		3	14	0	rajdeepchakravorty	top-50-action-movies-according-to-imdb
4121	4121	al_ap922_v5s6		[]		1	2	0	dragonzhang	al-ap922-v5s6
4122	4122	csv-data		['internet']		0	4	0	baburajmadathil	csvdata
4123	4123	images		[]		0	7	0	baburajmadathil	images
4124	4124	num_models		[]		0	3	0	kookheejin	num-models
4125	4125	The Weeknd's Full Discography Dataset	Contains information about tracks from all albums extracted from Spotify API	['arts and entertainment', 'music', 'audio data']	"Context
As one of The Weeknd's diehard fans, information about all songs from all albums released over the last decade is extracted from Spotify API for non-commercial, educational purposes. 
Content
Data from all tracks in all the albums released up to ""Dawn FM"" (2022). The dataset excludes tracks featuring the artist and live versions. Spotify provides data on the features of a song, such as mood, properties and more. To find out more, please visit: https://developer.spotify.com/discover/
Acknowledgements
Huge thanks to Steven Morse for providing instructions and Python codes to make this happen. Please visit https://stmorse.github.io/journal/spotify-api.html for full instructions on extracting data from Spotify using Python.
Inspiration
I created the dataset to conduct analysis for educational purposes. It's only fitting to give back love to the world - much like the artist has done for us.  #XOTWOD"	2	21	1	paulbaek	theweeknddiscography
4126	4126	Temperature and cherry blossom status	Data in Hirosaki Park,Aomori,Japan	['computer science']	"Context
Temperature and flowering status data to predict the flowering of cherry blossoms.
Content
Average daily temperature and flowering status of cherry blossoms at Hirosaki Park in Hirosaki City, Aomori Prefecture, Japan from January 1, 1997 to December 31, 2019.
2020 cherry blossom festival was canceled due to COVID-19, and no flowering information has been published.
Acknowledgements
Past temperature data is provided by the Japan Meteorological Agency, and data on cherry blossom flowering is provided by the Hirosaki Green Association.
Japan Meteorological Agency
Hirosaki City Green Association
Flower Status Data
Inspiration
There is a causal relationship between temperature and cherry blossom status.
Based on these data, and based on past and future temperatures this year, how accurate can you predict when the cherry blossoms will bloom this year?
It is also important to predict how the temperature will change in the future, based on the change in temperature this year."	240	3067	9	akioonodera	temperature-and-flower-status
4127	4127	Bangladesh COVID-19 Testing Centers & Kiosks	JSON and CSV Datasets of Testing Locations	[]		15	607	0	arifnezami	bangladesh-covid19-testing-centers-kiosks
4128	4128	dq13dpjv f610		[]		0	0	0	reighns	dq13dpjv-f610
4129	4129	dq13dpjv f15		[]		0	0	0	reighns	dq13dpjv-f15
4130	4130	Maori Symbols dataset for YOLOv5		[]		2	42	0	changjianli	maori-symbols-dataset-for-yolov5
4131	4131	Power1	solar data for arizona area	['renewable energy']		0	35	0	forsythkadingdi	power1
4132	4132	Braille dataset for scene text recognition		['computer vision', 'image data']		3	79	0	changjianli	braille-dataset-for-scene-text-recognition
4133	4133	FastText-cc-en-300D		[]		0	13	0	mkt0309	fasttextccen300d
4134	4134	Uso de Suelo		[]		0	6	0	veronica14	uso-de-suelo
4135	4135	LoL Spring Split 2022 Match Data	League of Legends esports match data from spring split 2022.	['video games']	"Context
I spent a while trying to find an online dataset for this data that was being updated regularly. Since I couldn't find one, I made one. I plan to update this dataset weekly and eventually add the matches from other regions. 
Content
week: This column refers to the week the game took place in the split.
day: This column refers to the day in the week that the game took place, some weeks have 3 days of games and some just have 2.
blue_: All the columns that start with blue_ refer to stats on the blue team.
red_: All the columns that start with red_ refer to stats on the blue team.
winning_team: The team that wins the match, either red or blue.
game_time_mins: This column has the total game time in minutes. 
Acknowledgements
Gathered this data from the VODs on lolesports.
Inspiration
A couple of things that I think could be done with this data:
- See if over the split if being on one side has benefits over the other.
- See if certain teams are more dominant than others."	4	97	0	thomaslazarus	league-of-legends-spring-split-2022-match-data
4136	4136	yolox-m-coco		[]		0	8	0	sangayb	yolox-m-coco
4137	4137	qmnist		[]		3	6	0	yakeworld126	qmnist
4138	4138	Festivities in Finland, Norway, Sweden (TSP 01-22)	Tabular Playground Series - Jan 2022	['europe']	"Tabular Playground Series - Jan 2022
The dataset contains dates of festivities in Finland, Norway and Sweden in the years used by the competitions."	38	236	16	lucamassaron	festivities-in-finland-norway-sweden-tsp-0122
4139	4139	Predict students' dropout and academic success		['education']	"A dataset created from a higher education institution (acquired from several disjoint databases) related to students enrolled in different undergraduate degrees, such as agronomy, design, education, nursing, journalism, management, social service, and technologies.
The dataset includes information known at the time of student enrollment (academic path, demographics, and social-economic factors) and the students' academic performance at the end of the first and second semesters.
The data is used to build classification models to predict students' dropout and academic success. The problem is formulated as a three category classification task (dropout, enrolled, and graduate) at the end of the normal duration of the course.
Publications
M.V.Martins, D. Tolledo, J. Machado, L. M.T. Baptista, V.Realinho. (2021) ""Early prediction of student's performance in higher education: a case study"" Trends and Applications in Information Systems and Technologies, vol.1, in Advances in Intelligent Systems and Computing series. Springer. DOI: 10.1007/978-3-030-72657-7_16
Funding
We acknowledge support of this work by the program ""SATDAP - Capacitação da Administração Pública under grant POCI-05-5762-FSE-000191, Portugal"""	11	67	0	vrealinho	predict-students-dropout-and-academic-success
4140	4140	IDBM Combined Dataset		[]	Data Source: official idbm website. Shortened the rows and then combined all 9 datasets	0	8	0	haidershah	idbm-combined
4141	4141	Disaster_data		[]		0	11	0	saamarthrastogi	disaster-data
4142	4142	Uso de Suelo		[]		12	16	0	vernicacristina	uso-suelo
4143	4143	Causes of Death in Asian Indians in USA Data		[]		7	71	0	adityasimha	causes-of-death-in-asian-indians-in-usa-data
4144	4144	Marketing (Sales) data	Practice Dataset for EDA on Marketting and Sales (Retail)	[]		15	68	0	rahultheogre	markettingdata
4145	4145	Uso de Suelo		[]		1	10	0	vernicacristina	suelo
4146	4146	Easy TS CV to Feather		[]		0	6	1	lucasmorin	easy-ts-cv-to-feather
4147	4147	veriList		[]		0	3	0	beyenernt	verilist
4148	4148	univ-data	Algerian Universities, Institutes, and College Centers.	['universities and colleges', 'africa', 'education', 'data cleaning', 'text data', 'json']	"DZ-Univ-Data
A Dataset for Algerian Universities, Institues, and College Centers. Name in Arabic, French, English, Web site and Region, are the collected Data."	6	291	1	kerneler	dzuniv
4149	4149	anger_dir		[]		0	25	1	dianaspahieva	anger-dir
4150	4150	pokemon_imagesx128		[]		0	14	0	reelord42069	pokemon-imagesx128
4151	4151	igm-selffocus		[]		39	76	0	unerriar	igmselffocus
4152	4152	Heart Disease Bangladesh Data		['heart conditions']		6	39	0	rafatashrafjoy	heart-disease-bangladesh-data
4153	4153	North American Grand Lodge Websites	Preferred CMS and SSL Certificate Providers	['websites', 'north america', 'internet', 'beginner', 'text data']		0	11	0	larrygwilson	north-american-grand-lodge-websites
4154	4154	Zekel Preprocessed		[]		0	6	0	anupamad	zekel-preprocessed
4155	4155	Myntra_Fashion_Products	10,761 products on Myntra	['clothing and accessories', 'business', 'internet', 'retail and shopping']	"Context
Myntra is a major Indian fashion e-commerce company. Crawl Feeds team extracted more than 1.16M+ records for research and analysis purposes. Last extracted on 29th July 2021. Here we are using only 12,491 rows.
Contact crawl feeds team to customize dataset as per your needs like format changes, data frequency, and adding or removing fields.
Content
This dataset contains the following fields : 
name : Name of the product
sku : Stock keeping unit
mpn : Manufacturer part number
price : Price of the product
in_stock : Product in stock or not
currency : Currency type
brand : Brand of the product
description : Product description
images : urls of the product images
gender : Gender to which product belongs to
Acknowledgements
This dataset was created by [crawlfeeds] (https://www.kaggle.com/fedesoriano/heart-failure-prediction)
Inspiration
The dataset can be used for different analysis."	19	100	0	nirokey	myntra-fashion-products
4156	4156	cots-fasterrcnn_v2		[]		3	18	0	kyawlin	cotsfasterrcnn-v2
4157	4157	Heart dataset input		['heart conditions']		2	10	0	ayushsarraf0731	heart-dataset-input
4158	4158	Heart dataset		['heart conditions']		0	5	0	ayushsarraf0731	heart-dataset
4159	4159	COTS_Torch_FasterRCNN_weights		[]		0	6	0	kyawlin	cots-torch-fasterrcnn-weights
4160	4160	output-for-gresearch		[]		0	1	0	puppakcheera	outputforgresearch
4161	4161	Bikeshare-Datasets	USA - Bikeshare datasets - egFWD Professional Track - Data Analysis	[]		0	16	1	karimhussam	bikesharedatasets
4162	4162	EBL_files		[]		0	7	0	kneha75	ebl-files
4163	4163	The Oxford-IIIT Pet Dataset With Annotations	Classify and segment images of pets	['animals', 'classification', 'deep learning', 'image data']	"Attribution
The dataset was created by Omkar M Parkhi and Andrea Vedaldi and Andrew Zisserman and C. V. Jawahar and was downloaded from https://www.robots.ox.ac.uk/~vgg/data/pets/ on January 23, 2022
Context
This is a 37 category pet dataset with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too."	15	82	0	julinmaloof	the-oxfordiiit-pet-dataset
4164	4164	simclr		[]		0	15	0	becky84	simclr
4165	4165	lol data		['arts and entertainment']		0	7	0	gabrielbecton	lol-data
4166	4166	Bone Marrow Cell Classification	170,000 images of hematologic diseases, including leukemia and lymphomas.	['biology', 'health', 'image data', 'health conditions', 'cancer']	"About this dataset
&gt; Bone marrow biopsy is procedure applied to collect and examine bone marrow — the spongy tissue inside some of your larger bones.
This biopsy can show whether your bone marrow is healthy and making normal amounts of blood cells. Doctors use these procedures to diagnose and monitor blood and marrow diseases, cancers, as well as fevers of unknown origin. 
&gt; The dataset contains a collection of over 170,000 de-identified, expert-annotated cells from the bone marrow smears of 945 patients stained using the May-Grünwald-Giemsa/Pappenheim stain. The diagnosis distribution in the cohort included a variety of hematological diseases reflective of the sample entry of a large laboratory specialized in leukemia diagnostics. Image acquisition was performed using a brightfield microscope with 40x magnification and oil immersion.
&gt; All samples were processed in the Munich Leukemia Laboratory (MLL), scanned using equipment developed at Fraunhofer IIS and post-processed using software developed at Helmholtz Munich.
How to use this dataset
&gt; - Create a multi-classification model to predict cell abnormalities;
- Create a binary-classification model to predict if a cell is normal or not.
Highlighted Notebooks
&gt; - Your kernel can be featured here!
- Related Dataset: Leukemia Classification
- More datasets
Acknowledgements
If you use this dataset in your research, please credit the authors.
&gt; ### Citation
&gt; Matek, C., Krappe, S., Münzenmayer, C., Haferlach, T., & Marr, C. (2021). An Expert-Annotated Dataset of Bone Marrow Cytology in Hematologic Malignancies [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.AXH3-T579
&gt; Matek, C., Krappe, S., Münzenmayer, C., Haferlach, T., and Marr, C. (2021). Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a large image dataset. https://doi.org/10.1182/blood.2020010568
&gt; ### License
CC BY 4.0
&gt; ### Splash banner
Icon by Freepik."	158	2705	32	andrewmvd	bone-marrow-cell-classification
4167	4167	Mypytorchmodels		[]		3	11	0	arafatasim	mypytorchmodels
4168	4168	Skin Cancer Detection using CNN	To build a CNN based model which can accurately detect melanoma.	['deep learning', 'cnn', 'multiclass classification', 'cancer']	"Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.
The dataset consists of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC). 
The data set contains the following diseases:
Actinic keratosis
Basal cell carcinoma
Dermatofibroma
Melanoma
Nevus
Pigmented benign keratosis
Seborrheic keratosis
Squamous cell carcinoma
Vascular lesion"	65	455	5	jaiahuja	skin-cancer-detection
4169	4169	Amazon Product Reviews Dataset	Amazon Product Reviews Dataset - Topic Modelling Problem	['earth and nature', 'business', 'beginner', 'nlp', 'ratings and reviews', 'e-commerce services', 'online communities']	"Description:
The dataset consists of samples from Amazon Ratings for select products. The reviews are picked randomly and the corpus has nearly 1.6k reviews of different customers.\
Amazon aims to understand what are the main topics of these reviews to classify them for easier search.\
Can you build a strong model that differentiates the topics based on the reviews corpus? 
Acknowledgements
The dataset is referred from Kaggle.
Objective:
Understand the Dataset & perform the necessary cleanup.
Build a strong Topic Modelling Algorithm to classify the topics."	827	6893	36	yasserh	amazon-product-reviews-dataset
4170	4170	nsk_image_search3_man6c		[]		0	0	0	motono0223	nsk-image-search3-man6c
4171	4171	deberta-v2-xl-fast-tokenizer	Can use with fast tokenizer	['computer science', 'programming', 'nlp', 'transformers']	"The is microsoft/deberta-v2-xlarge. I have added files to load the tokenizer as a fast version instead of the base version. Here is the code that must be added at the top before importing any transformers components.
```
import shutil
from pathlib import Path
transformers_path = Path(""/opt/conda/lib/python3.7/site-packages/transformers"")
input_dir = Path(""../input/debertav2xlfasttokenizer"")
convert_file = input_dir / ""convert_slow_tokenizer.py""
conversion_path = transformers_path/convert_file.name
if conversion_path.exists():
    conversion_path.unlink()
shutil.copy(convert_file, transformers_path)
deberta_v2_path = transformers_path / ""models"" / ""deberta_v2""
for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:
    filepath = deberta_v2_path/filename
    if filepath.exists():
        filepath.unlink()
shutil.copy(input_dir/filename, filepath)
```
After that you can load the fast tokenizer with the following code:
from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast
tokenizer = DebertaV2TokenizerFast.from_pretrained(model_checkpoint)
Below is copied from the Hugging Face model card:
DeBERTa: Decoding-enhanced BERT with Disentangled Attention
DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.
Please check the official repository for more details and updates.
This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.
Fine-tuning on NLU tasks
We present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.
| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |
|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|
|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |
| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |
| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |
| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |
| DeBERTa-Large(1)| 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |
| DeBERTa-XLarge(1)| -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|
| DeBERTa-V2-XLarge(1)|95.8/90.8| 91.4/88.9|91.7/91.6| 97.5| 95.8|71.1|93.9|92.0/94.2|92.3/89.8|92.9/92.9|
|DeBERTa-V2-XXLarge(1,2)|96.1/91.4|92.2/89.7|91.7/91.9|97.2|96.0|72.0| 93.5| 93.1/94.9|92.7/90.3 |93.2/93.1 |
Notes.
Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on DeBERTa-Large-MNLI, DeBERTa-XLarge-MNLI, DeBERTa-V2-XLarge-MNLI, DeBERTa-V2-XXLarge-MNLI. The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.
To try the XXLarge model with HF transformers, you need to specify --sharded_ddp
cd transformers/examples/text-classification/
export TASK_NAME=mrpc
python -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge --task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4 --learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16
Citation
If you find DeBERTa useful for your work, please cite the following paper:
latex
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}"	6	28	2	nbroad	debertav2xlfasttokenizer
4172	4172	wilayaapi	Algerian States, Provinces Data	['africa', 'data cleaning', 'data analytics', 'tabular data', 'standardized testing', 'json']	"DZ-Wilaya-Data
DZ-Wilaya-Data, is a set of collected Data, that containins the list of the Algerian States (Wilaya) and Provinces (Baladiya). Wialaya codes, and Baladiya codes, post codes and phones codes, are all included in this Data set."	10	203	2	kerneler	wilayaapi
4173	4173	Isekai Anime Light Novel Titles and Descriptions	1300+ rows in isekai light novel titles and descriptions	['nlp', 'text data', 'multiclass classification', 'anime and manga', 'multilabel classification']	"Context
Inspired by Gigguk's This Light Novel Does Not Exist video, I wanted to make my own isekai light novel title generating AI. I thought it would also be interesting to do it with the description/blurb or the novel too.
Content
The data was collected from novelupdates.com, filtered by tags ""Fantasy World"" and ""Alternate World"". A python script using selenium and bs4 was used to scrape the data from the website. 
Since some titles are very short, it is recommended to filter them out for best results. 
The link is also provided if additional metadata is needed such as cover image, tags and recommendations. 
Acknowledgements
Data collected from https://www.novelupdates.com/ by @andy8744 
Inspiration
Try generate your own light novel titles or descriptions. Or how about both? 
Generate text with your LSTMs then try retraining GPT-2 to do the same. How does the results differ? 
Predict the genre based on the title and description. (multi-class classification)This Light Novel Does Not Exist."	29	476	6	andy8744	isekai-light-novel-titles-and-descriptions
4174	4174	pytorchmodel		[]		1	13	0	arafatasim	pytorchmodel
4175	4175	fincompliants100k		[]		0	3	0	fernandobordi	fincompliants100k
4176	4176	pickbandataset		[]		0	12	0	olegdudnik	pickbandataset
4177	4177	pickbandota2heroes		[]		1	2	0	olegdudnik	pickbandota2heroes
4178	4178	mioFiore		[]		0	3	1	giulianabarberis	fiore
4179	4179	fincompliants30009		[]		0	2	0	fernandobordi	fincompliants30009
4180	4180	iopath		[]		0	10	0	riadalmadani	iopath
4181	4181	paperDataset		[]		2	11	0	mengyangyang1	paperdataset
4182	4182	fvcore		[]		0	0	0	riadalmadani	fvcore
4183	4183	3008_all_v5s6_ap86		[]		2	8	0	dragonzhang	3008-all-v5s6-ap86
4184	4184	setuptools		[]		0	0	0	riadalmadani	setuptools
4185	4185	fincompliants-15000		[]		0	1	0	fernandobordi	fincompliants15000
4186	4186	antlr4		[]		0	1	0	riadalmadani	antlr4
4187	4187	GDP DATA		['business']		0	9	0	balaponugupati	gdp-data
4188	4188	timm_0_5_4		[]		0	4	0	riadalmadani	timm-0-5-4
4189	4189	Indigenous figures	Prominent indigenous peoples in the Americas	['culture and humanities', 'history', 'demographics']		0	7	3	fireballbyedimyrnmom	indigenous-figures
4190	4190	pytorch_lightning		[]		0	0	0	riadalmadani	pytorch-lightning
4191	4191	fincompliants-10000		[]		0	6	0	fernandobordi	fincompliants10000
4192	4192	ipykernel		[]		0	0	0	riadalmadani	ipykernel
4193	4193	pokemonx32		[]		1	14	0	reelord42069	pokemonx32
4194	4194	retrain		['health']		0	6	0	chenyt263	retrain
4195	4195	fincompliants-10000		[]		0	4	0	fernandobordi	fincompliants-10000
4196	4196	resnest		[]		0	0	0	riadalmadani	resnest
4197	4197	div2k-tfrecords		[]		0	4	0	akshathmahajan	div2ktfrecords
4198	4198	River Water Level Data		['energy']		0	39	0	jasonkung98	public-infobanjir-past-records
4199	4199	omegaconf		[]		0	2	0	riadalmadani	omegaconf
4200	4200	Cars data		[]		1	15	0	vivekzs	cars-data
4201	4201	fin-compliants		[]		0	2	0	fernandobordi	fincompliants
4202	4202	fire_0_4_0		[]		0	0	0	riadalmadani	fire-0-4-0
4203	4203	training_data		[]		0	6	0	mananshijayminvyas	training-data
4204	4204	loguru		[]		0	2	0	riadalmadani	loguru
4205	4205	mymodel3		[]		0	173	0	mhashouri	mymodel3
4206	4206	tryttr		[]		0	3	0	looooooooow	tryttr
4207	4207	Python Recipes	Functions, Classes, & Other Usefulities	['earth and nature', 'business', 'software', 'python']	"$$\color{#ff35fe}{\mathbb{Context}}$$
The main idea is to create collections with standard code recipes.
$$\color{#ff35fe}{\mathbb{Content}}$$
Files with the .py (and similar) formats.
$$\color{#ff35fe}{\mathbb{Acknowledgments}}$$
Many thanks for the user comments.
$$\color{#ff35fe}{\mathbb{Inspiration}}$$
Could this data be a time saver in data processing?"	48	2615	10	olgabelitskaya	python-recipes
4208	4208	albumentations		[]		0	0	0	riadalmadani	albumentations
4209	4209	Wine Reviews Data	Dataset from scrape wine reviews	['alcohol']	"Context
Part of a bigger project. See here for more information: https://github.com/SamuelAdamsMcGuire/wine_data_clean"	131	1290	16	samuelmcguire	wine-reviews-data
4210	4210	US pollution		[]		5	45	0	tobijoshua	pollution
4211	4211	Ubiquant Market Prediction half precision Pickle		['investing']		271	695	18	lonnieqin	ubiquant-market-prediction-half-precision-pickle
4212	4212	My dataset	This is a pth file dataset	['categorical data']	This is a pth file	5	63	4	chesteranimation	my-dataset
4213	4213	thirdeditionChestXray		[]		0	10	0	chaitrapatwardhan	thirdeditionchestxray
4214	4214	mmdetection utils		[]		3	15	0	ks2019	mmdetection-utils
4215	4215	Planets		['astronomy']		3	18	0	satakshikrishna	planets
4216	4216	yolov5-lib-f2score	to calculate f2score as CV for GBR competition	['software', 'image data']	"From yolov5 github
edit along this discussion"	5	33	0	stgkrtua	yolov5libf2score
4217	4217	tfgbr-stage2-v3-yolov5l-64		[]		0	2	0	ks2019	tfgbr-stage2-v3-yolov5l-64
4218	4218	MediumTransformer		[]		0	3	0	darthvader4067	medium-tranformer
4219	4219	federated learning		['education']		3	22	0	mbonyani	federatedlearning
4220	4220	Highest grossing movies of Golden Age Hollywood	Top movies from 1930-1960, general info and their main cast	['arts and entertainment', 'movies and tv shows', 'exploratory data analysis']	"Context
The dataset contains general info of the top 487 highest-grossing movies from year 1930 - 1960. The information was scraped from IMDB using Python.
Content
Movie information includes: Title, Year of release, IMDB score, number of votes on IMDB, gross (according to IMDB, up until April 2021), movie ID on IMDB, genre(s), director, main cast (6 top-billed actors).
Inspiration
I made this dataset to investigate the interaction between actors in old Hollywood blockbusters, and to see if there is a connection between movies' success and an actor's connectivity in Hollywood.
Disclaimer
The box office gross information on IMDB is not always reliable, especially for older movies (missed figures, accounted for inflation incorrectly, etc.)"	4	51	0	phamgiang	highestgrossing-movies-of-golden-age-hollywood
4221	4221	Highest grossing movies of modern Hollywood	Top movies from 2000-2020, general info and their main cast	['movies and tv shows']	"Context
The dataset contains general info of the top 2000 highest-grossing movies from year 2000 until 2020. The information was scraped from IMDB using Python. 
Content
Movie information includes: Title, Year of release, IMDB score, number of votes on IMDB, gross (according to IMDB, up until April 2021), movie ID on IMDB, genre(s), director, main cast (10 top-billed actors).
Inspiration
I made this dataset to investigate the interaction between actors in modern blockbusters, and to see if there is a connection between movies' success and an actor's connection in Hollywood."	19	58	0	phamgiang	highest-grossing-movies-of-the-21st-century
4222	4222	Irie dataset		[]		0	9	0	rovinewanjala	irie-dataset
4223	4223	Prediction of demand for shared bikes		['online communities']		1	34	0	abhikbarman	prediction-of-demand-for-shared-bikes
4224	4224	airplane		[]		0	17	0	kevin33824	airplane
4225	4225	snomed2		[]		1	4	0	openj012	snomed2
4226	4226	Board Games Dataset	Fantastic Board Games and How to Predict Them	['games']		2	44	1	bananalee67	board-games-dataset
4227	4227	f1-roberta-base-5fold		['auto racing']		0	17	2	tonymarkchris	f1robertabase5fold
4228	4228	pyhton exercises		['exercise']		0	7	1	bestesakar	pyhton-exercises
4229	4229	International Patent Classification Matrix	International Patent Classification of Japanese Pharmaceutical Company	['health', 'law', 'k-means', 'IPython', 'sklearn']		2	34	3	ankumagawa	ipc-subclass-matrix
4230	4230	Modelo		[]		1	47	0	ngelaso	modelo
4231	4231	holtwinters_input		[]		0	3	0	yuyiping2020	holtwinters-input
4232	4232	MySteamDB		[]		2	12	0	zeeenb	mysteamdb
4233	4233	my_train		[]		0	1	0	lolitawu	my-train
4234	4234	100 Tourist Cities In India	Incredible India, picnic spot in India, 	['india', 'text data', 'travel']		11	60	1	samarthgangurde	100-tourist-cities-in-india
4235	4235	bert_weights		[]		0	6	0	regressionanalysisa	bert-weights
4236	4236	Oxford 17 Flowers dataset	Flowers dataset belonging to 17 different flower species	['forestry']		4	17	0	datajameson	oxford-17-flowers-dataset
4237	4237	Cython_3_0_0		[]		0	0	0	riadalmadani	cython-3-0-0
4238	4238	Petfinder-public		[]		0	13	0	shigemitsutomizawa	petfinderpublic
4239	4239	Ffaker		[]		0	5	0	magorzatamikoajewicz	ffaker
4240	4240	dbnet weights		['exercise']		1	7	0	riadhossainapsis	dbnet-weights
4241	4241	affectnet_helper		['psychology', 'signal processing', 'cnn', 'image data']	"This dataset is preprocessed data for affectnet sample dataset
, affectnet is rgb image dataset for face emotion recognition"	0	15	2	sarabhian	affectnet-helper
4242	4242	VIT_Small		[]		0	4	0	darthvader4067	vit-small
4243	4243	Datase		[]		1	3	0	chaitanyakumar001	datase
4244	4244	pycocotools		[]		0	2	0	riadalmadani	pycocotools
4245	4245	Image_data		[]		0	1	0	pavancharagondla	image-data
4246	4246	amundi_pred		[]		1	365	0	hado0601	amundi-pred
4247	4247	flightlegs		[]		0	12	0	magorzatamikoajewicz	flightlegs
4248	4248	ubiquant_2G_dataset		[]		0	14	0	hongyishao	ubiquant-2g-dataset
4249	4249	ridge_toxic_cleanexp2_fasttext_more_fe_oof_v2		[]		2	4	0	shobhitupadhyaya	ridge-toxic-cleanexp2-fasttext-more-fe-oof-v2
4250	4250	ridge_toxic_cleanexp1_fasttext_more_fe_oof_v2		[]		0	2	0	shobhitupadhyaya	ridge-toxic-cleanexp1-fasttext-more-fe-oof-v2
4251	4251	ridge_toxic_noclean_fasttext_more_fe_oof_v2		[]		0	2	0	shobhitupadhyaya	ridge-toxic-noclean-fasttext-more-fe-oof-v2
4252	4252	ziped bert		[]		0	6	1	crischir	ziped-bert
4253	4253	Flight Fare Prediction		[]		4	34	0	karan842	flight-fare-prediction
4254	4254	pycocotools_2_0_4		[]		0	0	0	riadalmadani	pycocotools-2-0-4
4255	4255	Netflix Dataset 		[]		4	59	0	vermakeshav	netflix-dataset
4256	4256	addict_2_4_0		[]		0	0	0	riadalmadani	addict-2-4-0
4257	4257	yapf_0_32_0		[]		0	0	0	riadalmadani	yapf-0-32-0
4258	4258	mmcv_cuda11		[]		0	2	0	riadalmadani	mmcv-cuda11
4259	4259	grocery_stores_to_open_10		[]		0	4	0	abdull4h	grocery-stores-to-open-10
4260	4260	Japanese Rice Seeds Agewise Classification 	Famous three rice varieties in Japan with the age of cultivation	['food']		0	13	0	namalrathnayake1990	japanese-rice-seeds-agewise-classification
4261	4261	Wind Speed along years 1947-2007 (reorganized)		['geography', 'physical science', 'physics', 'time series analysis', 'data analytics']		0	17	2	mostafaalaa123	wind-speed-along-years-19472007-reorganized
4262	4262	Adults		[]		0	10	1	rainlane	adults
4263	4263	SAHI-HUB	SAHI library needed for installation n KAggle	['business']	SAHI - Slicing Aided Hyper Inference. SAHI is a vision library for large-scale object detection & instance segmentation. Now I introduced it on Kaggle for Yolov5 (YoloX will be soon - testing it). This dataset provides modules for SAHI installation on Kaggle (without Internet access).	22	115	2	remekkinas	sahihub
4264	4264	tfgbr-stage2-v3-yolo-64		[]		0	10	0	ks2019	tfgbr-stage2-v3-yolo-64
4265	4265	AUTHORS_SYNBOOST_MODELS		[]		1	6	0	cshanmukhsivasai	authors-synboost-models
4266	4266	Literacy Ratio		['education']		2	10	0	roeebicher	literacy-ratio
4267	4267	Bank Loan Application EDA		[]		11	68	0	abhikbarman	bank-loan-application-eda
4268	4268	spleen–CT		['biology']		0	7	0	mazhuangxuan	spleenct
4269	4269	IMDB 5000 movies dataset		['movies and tv shows']		10	48	0	yaminyasinmech	imdb-5000-movies-dataset
4270	4270	reef_all_yolov5l6_pth		[]		2	6	0	dragonzhang	reef-all-yolov5l6-pth
4271	4271	Food Object Detection Dataset	Object Detection Dataset in COCO JSON format , to localize food .	['business', 'computer science']		16	43	2	deepking	food-object-detection-dataset
4272	4272	testData		['business']		0	2	0	frivolousfellow	testdata
4273	4273	yolov5-pip	Fixed fork of https://github.com/fcakyon/yolov5-pip for Yolov5 offline use	[]		0	14	0	hey24sheep	yolov5pip
4274	4274	brain_progression_masktumor_nii		[]		0	10	0	sosonger	brain-progression-masktumor-nii
4275	4275	pytorch_image_models	pytorch image models (timm)	['earth and nature', 'business', 'computer science', 'computer vision', 'image data', 'gpu', 'pytorch']	"PyTorch Image Models
Sponsors
What's New
Introduction
Models
Features
Results
Getting Started (Documentation)
Train, Validation, Inference Scripts
Awesome PyTorch Resources
Licenses
Citing
Sponsors
A big thank you to my GitHub Sponsors for their support!
In addition to the sponsors at the link above, I've received hardware and/or cloud resources from
* Nvidia (https://www.nvidia.com/en-us/)
* TFRC (https://www.tensorflow.org/tfrc)
I'm fortunate to be able to dedicate significant time and money of my own supporting this and other open source projects. However, as the projects increase in scope, outside support is needed to continue with the current trajectory of hardware, infrastructure, and electricty costs.
What's New
Aug 18, 2021
Optimizer bonanza!
Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ timm bits branch)
Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)
Some cleanup on all optimizers and factory. No more .data, a bit more consistency, unit tests for all!
SGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself).
EfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -&gt; 1k weights are very sensitive and less robust than the 1k weights.
Added PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested.
July 12, 2021
Add XCiT models from official facebook impl. Contributed by Alexander Soare
July 5-9, 2021
Add efficientnetv2_rw_t weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res)
top-1 82.34 @ 288x288 and 82.54 @ 320x320
Add SAM pretrained in1k weight for ViT B/16 (vit_base_patch16_sam_224) and B/32 (vit_base_patch32_sam_224)  models.
Add 'Aggregating Nested Transformer' (NesT) w/ weights converted from official Flax impl. Contributed by Alexander Soare.
jx_nest_base - 83.534, jx_nest_small - 83.120, jx_nest_tiny - 81.426
June 23, 2021
Reproduce gMLP model training, gmlp_s16_224 trained to 79.6 top-1, matching paper. Hparams for this and other recent MLP training here
June 20, 2021
Release Vision Transformer 'AugReg' weights from How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
.npz weight loading support added, can load any of the 50K+ weights from the AugReg series
See example notebook from official impl for navigating the augreg weights
Replaced all default weights w/ best AugReg variant (if possible). All AugReg 21k classifiers work.
Highlights: vit_large_patch16_384 (87.1 top-1), vit_large_r50_s32_384 (86.2 top-1), vit_base_patch16_384 (86.0 top-1)
vit_deit_ renamed to just deit_
Remove my old small model, replace with DeiT compatible small w/ AugReg weights
Add 1st training of my gmixer_24_224 MLP /w GLU, 78.1 top-1 w/ 25M params.
Add weights from official ResMLP release (https://github.com/facebookresearch/deit)
Add eca_nfnet_l2 weights from my 'lightweight' series. 84.7 top-1 at 384x384.
Add distilled BiT 50x1 student and 152x2 Teacher weights from  Knowledge distillation: A good teacher is patient and consistent
NFNets and ResNetV2-BiT models work w/ Pytorch XLA now
weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered)
eps values adjusted, will be slight differences but should be quite close
Improve test coverage and classifier interface of non-conv (vision transformer and mlp) models
Cleanup a few classifier / flatten details for models w/ conv classifiers or early global pool
Please report any regressions, this PR touched quite a few models.
June 8, 2021
Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 block variant, 79.2 top-1.
Add ResNet51-Q model w/ pretrained weights at 82.36 top-1.
NFNet inspired block layout with quad layer stem and no maxpool
Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288
May 25, 2021
Add LeViT, Visformer, ConViT (PR by Aman Arora), Twins (PR by paper authors) transformer models
Add ResMLP and gMLP MLP vision models to the existing MLP Mixer impl
Fix a number of torchscript issues with various vision transformer models
Cleanup input_size/img_size override handling and improve testing / test coverage for all vision transformer and MLP models
More flexible pos embedding resize (non-square) for ViT and TnT. Thanks Alexander Soare
Add efficientnetv2_rw_m model and weights (started training before official code). 84.8 top-1, 53M params.
May 14, 2021
Add EfficientNet-V2 official model defs w/ ported weights from official Tensorflow/Keras impl.
1k trained variants: tf_efficientnetv2_s/m/l
21k trained variants: tf_efficientnetv2_s/m/l_in21k
21k pretrained -&gt; 1k fine-tuned: tf_efficientnetv2_s/m/l_in21ft1k
v2 models w/ v1 scaling: tf_efficientnetv2_b0 through b3
Rename my prev V2 guess efficientnet_v2s -&gt; efficientnetv2_rw_s
Some blank efficientnetv2_* models in-place for future native PyTorch training
May 5, 2021
Add MLP-Mixer models and port pretrained weights from Google JAX impl
Add CaiT models and pretrained weights from FB
Add ResNet-RS models and weights from TF. Thanks Aman Arora
Add CoaT models and weights. Thanks Mohammed Rizin
Add new ImageNet-21k weights & finetuned weights for TResNet, MobileNet-V3, ViT models. Thanks mrT
Add GhostNet models and weights. Thanks Kai Han
Update ByoaNet attention modules
Improve SA module inits
Hack together experimental stand-alone Swin based attn module and swinnet
Consistent '26t' model defs for experiments.
Add improved Efficientnet-V2S (prelim model def) weights. 83.8 top-1.
WandB logging support
April 13, 2021
Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer
April 12, 2021
Add ECA-NFNet-L1 (slimmed down F1 w/ SiLU, 41M params) trained with this code. 84% top-1 @ 320x320. Trained at 256x256.
Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training.
Add ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs
Lambda Networks - https://arxiv.org/abs/2102.08602
Bottleneck Transformers - https://arxiv.org/abs/2101.11605
Halo Nets - https://arxiv.org/abs/2103.12731
Adabelief optimizer contributed by Juntang Zhuang
April 1, 2021
Add snazzy benchmark.py script for bulk timm model benchmarking of train and/or inference
Add Pooling-based Vision Transformer (PiT) models (from https://github.com/naver-ai/pit)
Merged distilled variant into main for torchscript compatibility
Some timm cleanup/style tweaks and weights have hub download support
Cleanup Vision Transformer (ViT) models
Merge distilled (DeiT) model into main so that torchscript can work
Support updated weight init (defaults to old still) that closer matches original JAX impl (possibly better training from scratch)
Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids
Fix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants
nn.Sequential for block stack (does not break downstream compat)
TnT (Transformer-in-Transformer) models contributed by author (from https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT)
Add RegNetY-160 weights from DeiT teacher model
Add new NFNet-L0 w/ SE attn (rename nfnet_l0b-&gt;nfnet_l0) weights 82.75 top-1 @ 288x288
Some fixes/improvements for TFDS dataset wrapper
March 17, 2021
Add new ECA-NFNet-L0 (rename nfnet_l0c-&gt;eca_nfnet_l0) weights trained by myself.
82.6 top-1 @ 288x288, 82.8 @ 320x320, trained at 224x224
Uses SiLU activation, approx 2x faster than dm_nfnet_f0 and 50% faster than nfnet_f0s w/ 1/3 param count
Integrate Hugging Face model hub into timm create_model and default_cfg handling for pretrained weight and config sharing (more on this soon!)
Merge HardCoRe NAS models contributed by https://github.com/yoniaflalo
Merge PyTorch trained EfficientNet-EL and pruned ES/EL variants contributed by DeGirum
March 7, 2021
First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc).
Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation.
Tested with PyTorch 1.8 release. Updated CI to use 1.8.
Benchmarked several arch on RTX 3090, Titan RTX, and V100 across 1.7.1, 1.8, NGC 20.12, and 21.02. Some interesting performance variations to take note of https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f
Feb 18, 2021
Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl.
Models are prefixed with dm_. They require SAME padding conv, skipinit enabled, and activation gains applied in act fn.
These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly 1/2 the inference speed of my SiLU PyTorch optimized s variants.
Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated).
Matching the original pre-processing as closely as possible I get these results:
dm_nfnet_f6 - 86.352
dm_nfnet_f5 - 86.100
dm_nfnet_f4 - 85.834
dm_nfnet_f3 - 85.676
dm_nfnet_f2 - 85.178
dm_nfnet_f1 - 84.696
dm_nfnet_f0 - 83.464
Feb 16, 2021
Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171. Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py.
AGC w/ default clipping factor --clip-grad .01 --clip-mode agc
PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0
PyTorch value clipping of 10, --clip-grad 10. --clip-mode value
AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet.
Feb 12, 2021
Update Normalization-Free nets to include new NFNet-F (https://arxiv.org/abs/2102.06171) model defs
Feb 10, 2021
First Normalization-Free model training experiments done,
nf_resnet50 - 80.68 top-1 @ 288x288, 80.31 @ 256x256
nf_regnet_b1 - 79.30 @ 288x288, 78.75 @ 256x256
More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks')
GPU-Efficient-Networks (https://github.com/idstcv/GPU-Efficient-Networks), impl in byobnet.py
RepVGG (https://github.com/DingXiaoH/RepVGG), impl in byobnet.py
classic VGG (from torchvision, impl in vgg.py)
Refinements to normalizer layer arg handling and normalizer+act layer handling in some models
Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not.
Fix a few bugs introduced since last pypi release
Feb 8, 2021
Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352.
ecaresnet26t - 79.88 top-1 @ 320x320, 79.08 @ 256x256
ecaresnet50t - 82.35 top-1 @ 320x320, 81.52 @ 256x256
ecaresnet269d - 84.93 top-1 @ 352x352, 84.87 @ 320x320
Remove separate tiered (t) vs tiered_narrow (tn) ResNet model defs, all tn changed to t and t models removed (seresnext26t_32x4d only model w/ weights that was removed).
Support model default_cfgs with separate train vs test resolution test_input_size and remove extra _320 suffix ResNet model defs that were just for test.
Jan 30, 2021
Add initial ""Normalization Free"" NF-RegNet-B* and NF-ResNet model definitions based on paper
Jan 25, 2021
Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer
Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer
ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support
NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning
Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit
Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes
Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script
Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2
Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar
Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp
Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling
Jan 3, 2021
Add SE-ResNet-152D weights
256x256 val, 0.94 crop top-1 - 83.75
320x320 val, 1.0 crop - 84.36
Update results files
Introduction
PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.
The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.
Models
All model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated. Here are some example training hparams to get you started.
A full version of the list below with source links can be found in the documentation.
Aggregating Nested Transformers - https://arxiv.org/abs/2105.12723
Big Transfer ResNetV2 (BiT) - https://arxiv.org/abs/1912.11370
Bottleneck Transformers - https://arxiv.org/abs/2101.11605
CaiT (Class-Attention in Image Transformers) - https://arxiv.org/abs/2103.17239
CoaT (Co-Scale Conv-Attentional Image Transformers) - https://arxiv.org/abs/2104.06399
ConViT (Soft Convolutional Inductive Biases Vision Transformers)- https://arxiv.org/abs/2103.10697
CspNet (Cross-Stage Partial Networks) - https://arxiv.org/abs/1911.11929
DeiT (Vision Transformer) - https://arxiv.org/abs/2012.12877
DenseNet - https://arxiv.org/abs/1608.06993
DLA - https://arxiv.org/abs/1707.06484
DPN (Dual-Path Network) - https://arxiv.org/abs/1707.01629
EfficientNet (MBConvNet Family)
EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252
EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665
EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946
EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html
EfficientNet V2 - https://arxiv.org/abs/2104.00298
FBNet-C - https://arxiv.org/abs/1812.03443
MixNet - https://arxiv.org/abs/1907.09595
MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626
MobileNet-V2 - https://arxiv.org/abs/1801.04381
Single-Path NAS - https://arxiv.org/abs/1904.02877
GhostNet - https://arxiv.org/abs/1911.11907
gMLP - https://arxiv.org/abs/2105.08050
GPU-Efficient Networks - https://arxiv.org/abs/2006.14090
Halo Nets - https://arxiv.org/abs/2103.12731
HardCoRe-NAS - https://arxiv.org/abs/2102.11646
HRNet - https://arxiv.org/abs/1908.07919
Inception-V3 - https://arxiv.org/abs/1512.00567
Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261
Lambda Networks - https://arxiv.org/abs/2102.08602
LeViT (Vision Transformer in ConvNet's Clothing) - https://arxiv.org/abs/2104.01136
MLP-Mixer - https://arxiv.org/abs/2105.01601
MobileNet-V3 (MBConvNet w/ Efficient Head) - https://arxiv.org/abs/1905.02244
NASNet-A - https://arxiv.org/abs/1707.07012
NFNet-F - https://arxiv.org/abs/2102.06171
NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692
PNasNet - https://arxiv.org/abs/1712.00559
Pooling-based Vision Transformer (PiT) - https://arxiv.org/abs/2103.16302
RegNet - https://arxiv.org/abs/2003.13678
RepVGG - https://arxiv.org/abs/2101.03697
ResMLP - https://arxiv.org/abs/2105.03404
ResNet/ResNeXt
ResNet (v1b/v1.5) - https://arxiv.org/abs/1512.03385
ResNeXt - https://arxiv.org/abs/1611.05431
'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187
Weakly-supervised (WSL) Instagram pretrained / ImageNet tuned ResNeXt101 - https://arxiv.org/abs/1805.00932
Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet/ResNeXts - https://arxiv.org/abs/1905.00546
ECA-Net (ECAResNet) - https://arxiv.org/abs/1910.03151v4
Squeeze-and-Excitation Networks (SEResNet) - https://arxiv.org/abs/1709.01507
ResNet-RS - https://arxiv.org/abs/2103.07579
Res2Net - https://arxiv.org/abs/1904.01169
ResNeSt - https://arxiv.org/abs/2004.08955
ReXNet - https://arxiv.org/abs/2007.00992
SelecSLS - https://arxiv.org/abs/1907.00837
Selective Kernel Networks - https://arxiv.org/abs/1903.06586
Swin Transformer - https://arxiv.org/abs/2103.14030
Transformer-iN-Transformer (TNT) - https://arxiv.org/abs/2103.00112
TResNet - https://arxiv.org/abs/2003.13630
Twins (Spatial Attention in Vision Transformers) - https://arxiv.org/pdf/2104.13840.pdf
Vision Transformer - https://arxiv.org/abs/2010.11929
VovNet V2 and V1 - https://arxiv.org/abs/1911.06667
Xception - https://arxiv.org/abs/1610.02357
Xception (Modified Aligned, Gluon) - https://arxiv.org/abs/1802.02611
Xception (Modified Aligned, TF) - https://arxiv.org/abs/1802.02611
XCiT (Cross-Covariance Image Transformers) - https://arxiv.org/abs/2106.09681
Features
Several (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP:
All models have a common default configuration interface and API for
accessing/changing the classifier - get_classifier and reset_classifier
doing a forward pass on just the features - forward_features (see documentation)
these makes it easy to write consistent network wrappers that work with any of the models
All models support multi-scale feature map extraction (feature pyramids) via create_model (see documentation)
create_model(name, features_only=True, out_indices=..., output_stride=...)
out_indices creation arg specifies which feature maps to return, these indices are 0 based and generally correspond to the C(i + 1) feature level.
output_stride creation arg controls output stride of the network by using dilated convolutions. Most networks are stride 32 by default. Not all networks support this.
feature map channel counts, reduction level (stride) can be queried AFTER model creation via the .feature_info member
All models have a consistent pretrained weight loader that adapts last linear if necessary, and from 3 to 1 channel input if desired
High performance reference training, validation, and inference scripts that work in several process/GPU modes:
NVIDIA DDP w/ a single GPU per process, multiple processes with APEX present (AMP mixed-precision optional)
PyTorch DistributedDataParallel w/ multi-gpu, single process (AMP disabled as it crashes when enabled)
PyTorch w/ single GPU single process (AMP optional)
A dynamic global pool implementation that allows selecting from average pooling, max pooling, average + max, or concat([average, max]) at model creation. All global pooling is adaptive average by default and compatible with pretrained weights.
A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved performance doing inference with input images larger than the training size. Idea adapted from original DPN implementation when I ported (https://github.com/cypw/DPNs)
Learning rate schedulers
Ideas adopted from
AllenNLP schedulers
FAIRseq lr_scheduler
SGDR: Stochastic Gradient Descent with Warm Restarts (https://arxiv.org/abs/1608.03983)
Schedulers include step, cosine w/ restarts, tanh w/ restarts, plateau
Optimizers:
rmsprop_tf adapted from PyTorch RMSProp by myself. Reproduces much improved Tensorflow RMSProp behaviour.
radam by Liyuan Liu (https://arxiv.org/abs/1908.03265)
novograd by Masashi Kimura (https://arxiv.org/abs/1905.11286)
lookahead adapted from impl by Liam (https://arxiv.org/abs/1907.08610)
`fused"	9	635	5	kozistr	pytorch-image-models
4276	4276	mm-S3S6	grid corpus(speech process)	['social science']		0	128	0	anniezz	mms1s3
4277	4277	Pakistan District Profile		['cities and urban areas', 'beginner', 'intermediate', 'tabular data']	"Context
Pakistan districts profile data set with key seven attributes.
Content
This dataset contains district profile of 134 districts of Pakistan including Islamabad. The dataset contains following information about each district of Pakistan.
•   Name and introduction
•   Background Information ('Area (Sq. km)', 'Forest Area (acres)', 'Total Housing Units', 'No. of Tehsils', 'No. of Union Councils')
•   Population ('Population', 'Population Density per Sq. Km', 'Ratio Male per hundred females', 'Urban Population', 'Rural Population', 'Male', 'Female', 'Transgender')
•   Economic Profile ('Labou force', 'Number of Printing Presses', 'Number of Television Sets', 'Operational Bank Branches', 'Sale of fertilizers (tonnes)', 'Number of Animals', 'Number of Cattle', 'Value produced by manufacturing industries', 'Average daily persons engaged in industry (No)', 'Employment cost', 'Wages & Salaries', 'Multi Dimensional Poverty Index')
•   Transport and Communication ('Road Kilometrage', 'Railway Route Kilometrage', 'Motor Vehicles Registered', 'Number of Exchanges', 'Telephone Connections', 'Public Call offices', 'Number of Post Offices')
•   Health and Sanitation Profile ('Govt. Health Institutions', 'Bed Strength', 'Number of Doctors', 'Registered Private Medical Practitioners', 'Number of patients treated', 'Percentage of children fully immunized Urban', 'Immunized rural', 'Percentage of households with tap water Urban', 'Rural', 'Percentage of households with toilet facility', , 'Private Health Institutions')
•   Education Profile ('Number of primary schools', 'Number of middle schools', 'Number of high schools', 'Number of higher secondary schools', 'Learning Score Percentage', 'Edu Inst Availability of Electricity', 'Edu Inst Availability of Water', 'Edu Inst Availability of Toilet')
Acknowledgements
The data was extracted from public dataset available on https://opendata.com.pk/dataset/district-profiles-all-districts-of-pakistan
Inspiration
I’d like to call the attention of my fellow Kagglers to use Machine Learning and Data Sciences to  utilize dataset for useful insights and learning."	3	114	3	alikhan83	pakistan-district-profile
4278	4278	unswnb		[]		1	19	0	haroldyhyin	unswnb
4279	4279	M2L-Dataset for Four Languages		['earth and nature']		0	3	0	mohammednaif	m2ldataset-for-four-languages
4280	4280	6-airpassenger		[]		0	8	0	wallacefqq	6airpassenger
4281	4281	Mendeteksi Kanker 		[]		0	16	1	haidarbagiralfahmi	mendeteksi-kanker
4282	4282	ddbo133		[]		0	0	0	movie112	ddbo133
4283	4283	airdata		[]		2	21	0	rahulbhowmick9680	airdata
4284	4284	ridge_un_cleanexp2_fasttext_multiseeds_oof		[]		2	6	0	shobhitupadhyaya	ridge-un-cleanexp2-fasttext-multiseeds-oof
4285	4285	ridge_un_cleanexp1_multiseeds_oof		[]		0	30	0	shobhitupadhyaya	ridge-un-cleanexp1-multiseeds-oof
4286	4286	noisy speech_synthesizer interspeech 2020		[]		1	17	0	zenbot99	noisy-speech-synthesizer-interspeech-2020
4287	4287	3lcg145y_F6-10		[]		0	2	0	reighns	3lcg145y-f610
4288	4288	MELI_Cars_Col		[]		0	125	1	juandag97	meli-cars-col
4289	4289	Devanagari CAPTCHA	A length 5 Devanagari Script CAPTCHA Dataset	[]		0	12	0	aditibihade	devanagari-captcha
4290	4290	dacon농업환경변화train,test zip파일		[]		1	22	1	yelim421	dacontraintest-zip
4291	4291	Software 	Redes Concolcionales	[]		13	23	1	vernicacristina	software
4292	4292	3lcg145y_F1-5		[]		0	5	0	reighns	3lcg145y-f15
4293	4293	ohsumed		[]		2	3	0	jackhua0597	ohsumed
4294	4294	r52_dataset		[]		0	1	0	jackhua0597	r52-dataset
4295	4295	r8_dataset		[]		0	4	0	jackhua0597	r8-dataset
4296	4296	ekh-dacon-zip		[]		0	2	0	eomkyeongho	ekhdaconzip
4297	4297	MICCF600		[]		0	4	0	nishaahin	miccf600
4298	4298	Evaluating students writing - trained model		[]		1	6	0	alicematusheva	evaluating-students-writing-trained-model
4299	4299	WSB posts 2021		[]		0	14	1	pnguy985	wsb-posts-2021
4300	4300	test3juju		[]		0	4	0	juliencardon	test3juju
4301	4301	Cyclistic_2020-11_2021-11	Cyclistic's bike rides in Chicago from Nov 2020 to Nov 2021	[]		0	12	0	benjaminestrada	cyclistic-202011-202111
4302	4302	England spinners with Test wickets	England men's Test spinners with at least 1 wicket	['cricket']		2	22	2	jbomitchell	england-spinnerswithtestwickets
4303	4303	fukuda step test berengueres		['movies and tv shows']		1	7	0	harriken	fukuda-step-test-berengueres
4304	4304	Demo of 7 Useful Custom Keras Callbacks	Small image classification dataset to demo callbacks	['classification', 'image data', 'multiclass classification', 'keras', 'tensorflow']		0	18	1	gpiosenka	demo-of-7-useful-custom-keras-callbacks
4305	4305	1000valid		[]		0	6	0	navidmoradi	1000valid
4306	4306	Economic Reporting Data (2007–2021) 	Econometric forecast, outcome, and reporting dates. 	['business', 'economics']	"Context
economic reported data & economic reported date: The data was collected from Forexfactory and processed with Julia lang. It contains US economic reporting dates, forecasts, and results for various economic reports such as ISM PMI, Non-farm payroll, FOMC, OPEC, CPI, building permits, and others. The purpose is to identify the influence of these econometric reports on market prices. These data will be used to complement the financial market data. Example, if the a specific economic report fails the traders' and investors' expectation, how would the market react? 
santa rally & triple witching week: The data was algorithmically processed with Julia lang by identifying and filtering the dates based on a set of conditions. For example, Santa Rally occurs 5 days before and 2 days after the end of December, and Triple Witching Week occurs on the 3rd week of the end of each quarter. The purpose is to identify the influence of these period on market prices. These data will be used to complement the financial market data.
observance dates ext: The data was algorithmically processed with Julia lang by identifying and filtering the dates based on a set of conditions. For example, Labor Day occurs on every Monday within a specific range of days depending on each specified year. The dates are also extend 3 days before and after the event date. For example, the extended days for Christmas will look like this: 2020-12-22, 2020-12-23, 2020-12-24, Christmas, 2020-12-28, 2020-12-29, 2020-12-30. The idea of the date extension is derived from a book called ""Almanac"", authored by Jeffrey A. Hirsch. The purpose is to identify the influence of these observance period on market prices. These data will be used to complement the financial market data."	11	133	0	lioneltyd	economic-reporting-data-20072021
4307	4307	MICCF200		[]		1	2	0	nishaahin	miccf200
4308	4308	Casiaedited		[]		3	2	0	sivasathguru	casiaedited
4309	4309	GDP_FIN_NOR_SWE_2015-2019_Quarterly_IMF		['europe', 'economics', 'beginner', 'tabular data']	"Content
Quarterly GDP in domestic currency (LCU) for Finland, Norway and Sweden from 2015 to 2019.
Data source: IMF"	0	8	0	siukeitin	gdp-fin-nor-swe-20152019-quarterly-imf
4310	4310	weatherAUS_Rev1		[]		0	2	0	lionelbottan	weatheraus-rev1
4311	4311	Ken Jee YouTube Data	Dataset for analyzing Ken Jee's YouTube Channel. Data by video and over time	['arts and entertainment', 'categorical data', 'education', 'exploratory data analysis', 'time series analysis', 'social networks']	"Context
I've been creating videos on YouTube since November of 2017 (https://www.youtube.com/c/KenJee1)  with the mission of making data science accessible to more people. One of the best ways to do this is to tell stories and working on projects. This is my attempt at my first community project. I am making my YouTube data available for everyone to help better understand the growth of my YouTube community and think about ways that it could be improved! I would love for everyone in the community feel like they had some hand in contributing to the channel. 
Announcement Video: https://youtu.be/YPph59-rTxA
I will be sharing my favorite projects in a few of my videos (with permission of course), and would also like to give away a few small prizes to the top featured notebooks. I hope you have fun with the analysis, I'm interested in seeing what you find in the data! 
For those looking for a place to start, some things I'm thinking about are:
- What are the themes of the comment data?
- What types of video titles and thumbnails drive the most traffic?
- Who is my core audience and what are they interested in? 
- What types of videos have lead to the most growth?
- What type of content are people engaging with the most or watching the longest? 
Some advanced projects could be:
- Creating a chat bot to respond to common comments with videos where I have addressed a topic
- Pulling sentiment from thumbnails and titles and comparing that with performance 
Data I would like to add over time
- Video descriptions
- Video subtitles
- Actual video data 
Content
There are four files in this repo. The relevant data included in most of them is from Nov 2017 - Jan 2022. I gathered some of this data via the YouTube API and the rest from my specific analytics. 
1) Aggregated Metrics By Video - This has all the topline metrics from my channel from its start (around 2015 to Jan 22 2022). I didn't post my first video until around 
2) Aggregated Metrics By Video with Country and Subscriber Status - This has the same data as aggregated metrics by video, but it includes dimensions for which country people are viewing from and if the viewers are subscribed to the channel or not.
3) Video Performance Over Time - This has the daily data from each of my videos. 
4) All Comments - This is all of my comment data gathered from the YouTube API. I have anonymized the users so don't worry about your name showing up! 
Acknowledgements
This obviously wouldn't be possible without all of the wonderful people who watch and interact with my videos! I'm incredibly grateful for you all and I'm so happy I can share this project with you! 
License
I collected this data from the YouTube API and through my own google analytics. Thus use of it must uphold the YouTube API's terms of service: https://developers.google.com/youtube/terms/api-services-terms-of-service"	243	4069	84	kenjee	ken-jee-youtube-data
4312	4312	wordcloud		[]		0	2	1	nikbamne	wordcloud
4313	4313	penguins		['hockey']		1	6	0	srahabib	penguins
4314	4314	ridge models1		[]		0	70	0	alexander1980	ridge-models1
4315	4315	Forecasting given time series problem	Time Series Practice Data For Beginners	['education']		3	55	2	ashisparida	petrichor-forecasting-given-time-series-problem
4316	4316	Regression Problem	Regression Problem for Beginners	['beginner', 'intermediate', 'data cleaning', 'data visualization', 'regression']		1	34	2	ashisparida	petrichor-regression-problem
4317	4317	Classification Problem	Classification Task for Beginners	[]		1	29	1	ashisparida	petrichor-classification-problem
4318	4318	Sample Sales Records		[]		3	29	6	pouryaazar1994	sample-sales-records
4319	4319	ridge_un_noclean_fasttext_multiseeds_oof		[]		3	23	0	shobhitupadhyaya	ridge-un-noclean-fasttext-multiseeds-oof
4320	4320	Stephen Curry stats 2009-2021 in NBA	Stephen who is the best 3 points player 	['basketball', 'sports', 'exploratory data analysis']	"🎉 Stephen Curry overtakes Ray Allen for NBA's all-time 3-point lead! 🎉
Context
Data sets containing Stephen Curry stats 2009-2022 (January 23)
Content
Stephen Curry Stats.csv
    Stats from ALL NBA matches involve Postseason and Preseason
    Total 21 columns about Stephen's stats and Season data, W/N, Team Score, Other team name, etc
Stephen Curry Postseason stats.csv
Stephen Curry Regularseason stats.csv
Stephen Curry Preseason stats.csv
Features
Season_year : Season, ex) 2020-2021 (str)
Season_div : Pre season, Regular season, Post Season (str)
Date : Month and day (str)
OPP : Opposite (str)
Result : Won or Loss (str)
T Score : Score of the Team involved Stephen Curry (int)
O Score : Score of the other (int)
MIN : Minutes played in game (float) 
FG : Field goals made - attempted (str)
FG% : Field goals made, attempted and percentage (float)
3PT : Three-point field goals made - attempted (str)
3PT% : Three-point field goals made, attempted and percentage (float)
FT : Free throws made - attempted (str)
FT% : Free throws made, attempted and percentage (float)
REB : Rebounds in game (int)
AST : Assists in game (int)
BLK : Blocks in game (int)
STL : Steals in game (int) 
PF : Personal fouls in game (int)
TO : Turnover in game (int)
PTS : Points in game (int)
Acknowledgements
Reference ESPN NBA
Inspiration
Exploratory data analysis"	288	1981	12	mujinjo	stephen-curry-stats-20092021-in-nba
4321	4321	ddbo132		[]		0	3	0	movie112	ddbo132
4322	4322	yolox_darknet		[]		0	1	0	crained	yolox-darknet
4323	4323	Online retail		['retail and shopping']		2	33	0	hamidosharaf	online-retail
4324	4324	patient		[]		0	10	1	stamatelou	patient
4325	4325	Stroke		['health conditions']		0	32	0	muhammadumerfayyaz	stroke
4326	4326	keras_vgg16		[]		0	5	0	feipan2021	keras-vgg16
4327	4327	user_rating		[]		3	12	0	sourav044	user-rating
4328	4328	Turkey Covid19 Dataset	Turkey Covid19 Data between 11/03/2020-22/01/2022(my first web scraping project)	['categorical data', 'government', 'text data', 'covid19', 'pandas']	"content
this dataset includes Turkey's 683 daily covid-19 cases and others.
why is some data empty
because the turkish government does not share data consistently"	35	261	5	echtr0	trkiye-covid19-verileri-dataset
4329	4329	tfgbr-stage2-v2-64		[]		0	4	0	ks2019	tfgbr-stage2-v2-64
4330	4330	Turkey's monthly natural gas data		['oil and gas']		4	24	1	naturalgas	turkeys-monthly-natural-gas-data
4331	4331	tfgbr-stage2-v2-128		[]		0	1	0	ks2019	tfgbr-stage2-v2-128
4332	4332	dataset 30_7 b40 nside 64		[]		0	3	0	lino08	dataset-30-7-b40
4333	4333	ridge_ruddit_cleanexp2_fasttext_multiseeds_oof		[]		1	19	0	shobhitupadhyaya	ridge-ruddit-cleanexp2-fasttext-multiseeds-oof
4334	4334	ridge_ruddit_noclean_fasttext_multiseeds_oof		[]		4	8	0	shobhitupadhyaya	ridge-ruddit-noclean-fasttext-multiseeds-oof
4335	4335	ridge_ruddit_cleanexp1_fasttest_multiseeds_oof		[]		1	5	0	shobhitupadhyaya	ridge-ruddit-cleanexp1-fasttest-multiseeds-oof
4336	4336	data_gen_k		[]		0	30	1	minhvlthin	data-gen-k
4337	4337	Dogs Vs Cats		[]		3	20	1	tharun369	dogs-vs-cats
4338	4338	mymodel7		[]		0	28	0	mhashoorii	mymodel7
4339	4339	House Price Estimation		[]		0	24	0	kvsandeepmoudgalya	house-price-estimation
4340	4340	electra_small_discriminator		[]		0	1	0	ksherho	electra-small-discriminator
4341	4341	funnel_transformer_small_base		[]		0	2	0	ksherho	funnel-transformer-small-base
4342	4342	albert_base_v2		[]		0	1	0	ksherho	albert-base-v2
4343	4343	Daily minimum temperatures	Data for time series analysis and forecasting	['weather and climate', 'time series analysis', 'tabular data', 'regression']	"Content
This data set contains information about daily minimum temperatures from 1981 to 1990.
Acknowledgements
The original data can be found here: https://github.com/upul/WhiteBoard/blob/master/data/daily-minimum-temperatures-in-me.csv"	10	57	1	suprematism	daily-minimum-temperatures
4344	4344	Reber Grammar	Reber Grammar Sequences; Valid and Invalid	['computer science', 'programming', 'text data', 'python', 'r']	"A Reber sequence is a grammar string made of finite states, in simple words, formed by using a confined set of characters. In the research paper that proposed the LSTM, the authors use embedded Reber grammar due to its short time lags.
Here is the chapter from my Master's thesis that briefly explains the dataset, and also includes various plots about its statistics.
Data visualiations available at about_reber."	5	237	2	harshildarji	reber
4345	4345	Bangladesh-filters-price		[]		0	4	1	nabidbhuia	bangladesh-filters-price
4346	4346	Pakistan's Top 50 Celebrities on Instagram	Pakistani celebrities on Instagram	['celebrities', 'asia', 'beginner', 'data cleaning', 'text data']	"The digital race is becoming increasingly competitive
So it's no surprise that celebrities all over the world are turning to Instagram to transform their already-existing recognition into tremendous popularity! Pakistani celebrities, too, are following in their footsteps, and you can't help but see them posting updates on social media on a regular basis.
Which begs the question: who exactly are the ruling Pakistani superstars that have monopolised the Instagram world and pushed devotees to follow each and every move they make?"	0	9	0	patriotboy112	pakistans-top-50-celebrities-on-instagram
4347	4347	yolox_cots_models_1		[]		0	8	0	zengkuikui	yolox-cots-models-1
4348	4348	Test data		[]		0	2	0	haranr	test-data
4349	4349	sgns.target.word		[]		0	15	0	zhenhoblngjia	sgnstargetword
4350	4350	2021 Food Trade Data		['business']	This data is the import and export data of the world's major food crops in 2021. The value represents the quantity imported.	5	28	0	yitaoranxiao	2021-food-trade-data
4351	4351	diacritic-restoration		[]		0	40	0	welcomehere	diacritic-restoration
4352	4352	FaceImages-StyleGan2		['gan', 'image data']		1	16	0	dat0chin	faceimagesstylegan2
4353	4353	2021-2022 NBA Player Stats	2021-2022 Regular Season NBA Player Stats	['basketball', 'classification', 'regression']	"Context
This dataset contains 2021-2022 regular season NBA player stats per game.
Content
+500 rows and 30 columns.
Columns' description are listed below.
Rk : Rank
Player : Player's name
Pos : Position
Age : Player's age
Tm : Team
G : Games played
GS : Games started
MP : Minutes played per game
FG : Field goals per game
FGA : Field goal attempts per game
FG% : Fiel goal percentage
3P : 3-point field goals per game
3PA : 3-point field goal attempts per game
3P% : 3-point field goal percentage
2P : 2-point field goals per game
2PA : 2-point field goal attempts per game
2P% : 2-point field goal percentage
eFG% : Effective field goal percentage
FT : Free throws per game
FTA : Free throw attempts per game
FT% : Free throw percentage
ORB : Offensive rebounds per game
DRB : Defensive rebounds per game
TRB : Total rebounds per game
AST : Assists per game
STL : Steals per game
BLK : Blocks per game
TOV : Turnovers per game
PF : Personal fouls per game
PTS : Points per game
Acknowledgements
Data from Basketball Reference.
Image from NBA.
If you're reading this, please upvote."	854	4238	29	vivovinco	nba-player-stats
4354	4354	AdditionalDatasets	We4Tech - Saurabh Karbhajan	['games']		1	22	0	saurabhkarbhajan	additionaldatasets
4355	4355	ddbo131		[]		0	2	0	movie112	ddbo131
4356	4356	nsk_image_search3_man6b		[]		0	3	0	motono0223	nsk-image-search3-man6b
4357	4357	nsk_image_search3_man6		[]		0	2	0	motono0223	nsk-image-search3-man6
4358	4358	India Aviation Grievance Dataset	Airline wise  Aviation Grievance Dataset	['government', 'aviation', 'exploratory data analysis', 'data visualization', 'data analytics', 'automl']	"Content
Aviation grievance dataset is available to all Kagglers for data analysis and visualization. All columns  are self-explanatory.
Acknowledgements
Dataset is fetched from Public API available at Data.gov.in
Dataset is licensed under GODL."	30	227	7	shashwatwork	india-aviation-grievance-dataset
4359	4359	raw_tweets		[]		2	8	0	playermzfast	raw-tweets
4360	4360	fma_mel_new		[]		0	40	0	giuseppemagazz	fma-mel-new
4361	4361	pklfies		[]		0	11	0	ludaykumar	pklps
4362	4362	roberta_base_reset4_weight		[]		0	3	0	kintaro1	roberta-base-reset4-weight
4363	4363	AudioSet Valid Dataset		[]		0	7	0	benimaru069	audioset-valid-dataset
4364	4364	chaise_dataset		[]		0	0	0	machacalisto	chaise-dataset
4365	4365	AudioSet Train Dataset		['transportation']		0	8	0	benimaru069	audioset-train-dataset
4366	4366	tfgbr-stage2-v1		[]		0	1	0	ks2019	tfgbr-stage2-v1
4367	4367	The Android App Market on Google Play		[]		16	111	0	housseinihadia	the-android-app-market-on-google-play
4368	4368	AudioSet Dataset		['earth and nature']		7	11	0	benimaru069	audioset-dataset
4369	4369	jan222022		[]		0	3	0	crained	jan222022
4370	4370	austin_weather		[]		6	16	0	arslannurmukhametov	austin-weather
4371	4371	2nd Hand car price prediction		[]		2	18	0	rajeshmalayanuru	2nd-hand-car-price-prediction
4372	4372	peanut-soybean		[]		5	12	0	auliyaa	peanut-soybean
4373	4373	cnn_model		[]		0	2	0	playermzfast	cnn-model
4374	4374	Document Classification - Desafio 1		[]		0	14	0	rywgar	document-classification-desafio-1
4375	4375	mypthGB		[]		10	56	0	naturezhang	mypthgb
4376	4376	Russian Emoji		[]		48	618	3	shonenkov	russian-emoji
4377	4377	lung_cancer_dataset	It an Lung Disease Dataset 	['health conditions']		12	178	6	balasubramaniamv	lung-cancer-dataset
4378	4378	YoloV3EbikeDetect		[]		0	8	0	jackcobra	yolov3ebikedetect
4379	4379	FakeNewsDetection		[]		0	17	0	civitasv	fakenewsdetection
4380	4380	hiiiiii		[]		0	9	0	sanketsss	hiiiiii
4381	4381	EfficientNetB7TPU		[]		4	16	0	tiquasar	efficientnetb7tpu
4382	4382	cots_non_verified_license_yet		[]		32	55	3	hengck23	cots-non-verified-license-yet
4383	4383	Feedback Prize - RoBERTa Tokens 1024	Token ids, attention masks and labels for Feedback Prize Competition	['games', 'education', 'nlp', 'text data', 'primary and secondary schools']	"Content
This is a dataset of Feedback Prize Competition texts converted to tokens with RoBERTa tokenizer. All texts are cropped/padded to 1024 tokens.
For usage example, see Training RoBERTa in 10 minutes.
Acknowledgements
Photo by lilartsy from Unsplash"	4	52	0	nickuzmenkov	feedback-prize-roberta-tokens-1024
4384	4384	byte-track		[]		1	19	2	slavkoprytula	bytetrack
4385	4385	Avocado sales 2015-2021 (US centric)	made possible thanks to HassAvocadoBoard	['united states', 'categorical data', 'agriculture', 'bigquery', 'food']	"I looked at the ""Avocado Prices"" dataset but that only was from 2015 to 2018. So I went to HassAvocadoBoard website and downloaded the data from 2018 (again) to 2021 for better comparisons :)
It represents the sales of 3 different types of avocado in the U.S based on varieties, total volume, average price, region
I formatted everything in a table so it should be fine. Careful though, even though the data is categorized by region, one region is ""TOTAL U.S"" so that can cause errors in the data.
As I'd mentioned, this data was inspired by the ""Avocado Prices"" made by Justin Kiggins.
I hope you like it :) it is a great tool to practice some sparkSQL, R and even Python."	74	457	3	valentinjoseph	avocado-sales-20152021-us-centric
4386	4386	scikit_1.0.2		[]		0	3	0	zaakciiru	scikit-102
4387	4387	WIDS 2022		[]		15	169	1	justvikram	wids-2022
4388	4388	hotel_booking.csv		[]		0	11	0	maryamahmadizadeh	hotelbooking
4389	4389	https://faleasecanna.disqus.com/admin/		[]		0	2	0	feliciaannkelleyta	httpsfaleasecannadisquscomadmin
4390	4390	Airline_passenger_satisfication_data		[]		1	13	0	sandhiyakumar	airline-passenger-satisfication-data
4391	4391	loandata		[]		0	0	0	tusarsingh	loandata
4392	4392	Taps_Apps		[]		1	20	0	eneaceoliniqa	taps-apps
4393	4393	Bellabeat		[]		0	6	0	vishwajeetpm	bellabeat
4394	4394	Telecom_Churn_Dataset		[]		0	11	0	veenitankam	telecom-churn-dataset
4395	4395	yunbert		[]		13	35	0	aronbryant	yunbert
4396	4396	Antispoofing		[]		1	30	0	huyngqc	antispoofing
4397	4397	Tweets		['online communities']		0	1	0	dianfarahbinteriduan	tweets
4398	4398	friends image		['social networks']		0	10	1	muhammadammarjamshed	friends-image
4399	4399	google logo		['internet']		0	7	1	muhammadammarjamshed	google-logo
4400	4400	aERIAL IMAGE		['art']		0	6	1	muhammadammarjamshed	aerial-image
4401	4401	DatasetMariagesInseeDataviz		[]		0	3	0	carpiliengpatbaul	datasetmariagesinseedataviz
4402	4402	picture__skimage		[]		0	2	1	muhammadammarjamshed	picture-skimage
4403	4403	Basic arithmetics dataset	Digits, brackets, multiplication, division, subtraction, addition	['beginner', 'classification', 'image data', 'multiclass classification']		0	17	0	simepavlic	basic-arithmetics-dataset
4404	4404	DE_data		[]		0	1	0	dnyaneshpainjane	de-data
4405	4405	NBA_injuries_season_2019/2018		[]		0	2	0	mawkley	web-scraped-repository
4406	4406	swin_large_patch4_window7_224_7_folds_xyftxueh		[]		0	7	0	reighns	swin-large-patch4-window7-224-7-folds-xyftxueh
4407	4407	Diabetes Prediction	Support Vector Machine	['svm', 'matplotlib', 'numpy', 'pandas', 'seaborn']		7	48	0	reshmashinde123	diabetes-prediction
4408	4408	Precipitation Prediction in LA	Basic Dataset for a Basic Project	['education', 'beginner', 'exploratory data analysis', 'classification', 'tabular data']	"Context
This Dataset is part of a basic DIY Machine Learning project offered by my college, Indian Institute of Technology, Guwahati (IIT G). The main aim of this project was to get familiar with the workflow and various techniques involved in a Machine Learning project.
Content
The dataset is fairly simple and contains various features regarding precipitation.
PRCP = Precipitation (tenths of mm)
TMAX = Maximum temperature (tenths of degrees C)
TMIN = Minimum temperature (tenths of degrees C)
PGTM = Peak gust time (hours and minutes, i.e., HHMM)
AWND = Average daily wind speed (tenths of meters per second)
TAVG = Average temperature (tenths of degrees C)
WDFx = Direction of fastest x-minute wind (degrees)
WSFx = Fastest x-minute wind speed (tenths of meters per second)
WT = Weather Type
Acknowledgements
All Credits go to the Coding Club of Indian Institute of Technology, Guwahati (IIT Guwahati).
Instagram: https://www.instagram.com/codingclubiitg/
LinkedIn : https://www.linkedin.com/company/coding-club-iitg/ 
Inspiration
Hope that this dataset + my notebook (https://www.kaggle.com/varunnagpalspyz/precipitation-prediction/notebook) helps all beginners like me."	36	278	8	varunnagpalspyz	precipitation-prediction-in-la
4409	4409	road anomaly icnet original synthesis		['earth and nature']		2	4	0	synboost	road-anomaly-icnet-original-synthesis
4410	4410	Speech separation		[]		0	12	0	zmzm123	speech-separation
4411	4411	lost and found icnet original synthesis 		[]		1	5	0	synboost	lost-and-found-icnet-original-synthesis
4412	4412	Satisfaction2		[]		0	2	0	chemicalburn09	satisfaction2
4413	4413	Fine_Tuned_CXR_Classifier_CovidPneumoniaNormal		[]		0	4	0	metformin	fine-tuned-cxr-classifier-covidpneumonianormal
4414	4414	MSVD VGG Extracted		[]		4	27	0	steveandreasimmanuel	msvd-extracted
4415	4415	nhanes-data-files		['video games']		0	14	0	maransk	nhanes-data-files
4416	4416	5-Yr SSE Index as of 2022-01-22	------------------------------	[]		1	10	0	kingychiu	5yr-sse-composite-index-as-of-20220122
4417	4417	c_yolov5		[]		0	7	0	chao7777	c-yolov5
4418	4418	titanic		[]		0	8	0	yukkes74	titanic
4419	4419	Stat_Holiday_Mondays_Fin_Nor_Swe_2015_2019		[]		1	18	1	khbreslauer	stat-holiday-mondays-fin-nor-swe-2015-2019
4420	4420	ponzi_detect		[]		0	22	0	lantian123	ponzi-detect
4421	4421	cifar_dataset		[]		0	4	0	shabhu18	cifar-dataset
4422	4422	landmark_additional_packages		[]		32	375	0	steamedsheep	landmark-additional-packages
4423	4423	MBLLENV		[]		0	4	0	nuo0503	mbllenv
4424	4424	ddbo130		[]		0	4	0	movie112	ddbo130
4425	4425	UPI transactions	Monthly UPI transaction data from Jan 2020 	['finance', 'banking', 'tabular data', 'investing', 'news']	This dataset contains monthly transaction data using UPI ( Unified Payments Interface ).	25	366	4	nishchalnishant	upi-transactions
4426	4426	tugasbayes		[]		0	1	0	skrnagrh	tugas
4427	4427	dataset		[]		0	5	0	skrnagrh	dataset
4428	4428	Multiparamss		[]		0	3	0	mammadabbasli	multiparamss
4429	4429	UMP_Pickle_Trainfile		[]		1	11	0	venkatkumar001	ump-pickle-trainfile
4430	4430	fish-datasets	Attempt to generate Fish Datasets: Segmentations & labelling [All +3000 species]	['earth and nature', 'fish and aquaria']		7	476	1	pablogod	fishdatasets
4431	4431	swimming-pool-satellite-imagery-yolov5		[]		0	5	0	hruthiksivakumar	swimmingpoolsatelliteimageryyolov5
4432	4432	Top 10 video and mobile marketing online courses		[]		2	29	0	mirkobronzi	top-10-video-and-mobile-marketing-online-courses
4433	4433	Home Improvement Online Courses		[]		0	4	0	mirkobronzi	home-improvement-online-courses
4434	4434	pix2pixv3		[]		0	10	0	juliencardon	pix2pixv3
4435	4435	datasetkecerdasanbuatan		[]		0	0	0	riyadsalih	datasetkecerdasanbuatan
4436	4436	hnubci2022dataset		[]		1	16	0	houyao	hnubci2022dataset
4437	4437	swin_large_patch4_window7_224_7_folds_14h8rv8p		[]		0	2	0	reighns	swin-large-patch4-window7-224-7-folds-14h8rv8p
4438	4438	cots_yolov5_10folds		[]		0	4	0	huyidao	cots-yolov5-10folds
4439	4439	DS Bootcamp - Akreditasi Sekolah		[]		45	169	1	dionisiusdh	ds-bootcamp-akreditasi-sekolah
4440	4440	datasetkaggle		[]		0	1	0	fikriearizal	datasetkaggle
4441	4441	Turkish Sentences Dataset	Türkçe Cümleler Veriseti	['languages', 'text mining', 'text data']		40	411	3	mahdinamidamirchi	turkish-sentences-dataset
4442	4442	Data_chaise_2		[]		1	9	0	bbniccccccc	data-chaise-2
4443	4443	HDFC Stock Price (2000-2021)	Here is HDFC Bank stock Price from year 2000 to 2021	['india', 'time series analysis', 'statistical analysis', 'linear regression', 'investing', 'sklearn']	"Context
This is a dataset of HDFC Bank stock price from 2000 to 2021, its has more than 5000 rows. So we can perform EDA and various Regression Techniques to predict future price and how well the Bank stock are performing.
About Dataset:
Date: A date is a particular day of the month.
Open: It is the price at which the financial security opens in the market when trading begins. It may or may not be different from the previous day's closing price. Thus, the price in the beginning of trading sessions is called open price or simply open.
High: Today's high refers to a security's intraday highest trading price. It is represented by the highest point on a day's stock chart. This can be contrasted with today's low, which is the trading day's intraday low price.
Low: The low is the minimum price of a stock in a period, while high is the maximum value reached by the stock in the same period.
Close: The close is a reference to the end of a trading session in the financial markets when the markets close for the day. The close can also refer to the process of exiting a trade or the final procedure in a financial transaction in which contract documents are signed and recorded.
Adj Close:  The adjusted closing price amends a stock's closing price to reflect that stock's value after accounting for any corporate actions. The closing price is the raw price, which is just the cash value of the last transacted price before the market closes.
Volume: In capital markets, volume, or trading volume, is the amount of a security that was traded during a given period of time. In the context of a single stock trading on a stock exchange, the volume is commonly reported as the number of shares that changed hands during a given day."	61	417	10	meetnagadia	hdfc-stock-price-20002021
4444	4444	Goblet from Tang Dynasty		['arts and entertainment']		0	10	1	qingyueyang	goblet-from-tang-dynasty
4445	4445	AsDataset	First Dataset For A-Soul	['china ', 'beginner', 'image data', 'anime and manga', 'cv2']	"A-Soul DataSet
由Asdb维护,用于自动化workflow.
遵循CC协议开源
Maintenance By Asdb,Using for object detection.
Opensource by CC License."	3	67	2	petepei	asdataset
4446	4446	traffic6		[]		0	1	0	abrahamanderson	traffic6
4447	4447	traffic5		[]		0	0	0	abrahamanderson	traffic5
4448	4448	traffic3		[]		0	1	0	abrahamanderson	traffic3
4449	4449	traffic1		[]		0	0	0	abrahamanderson	traffic1
4450	4450	traffic2		[]		0	0	0	abrahamanderson	traffic2
4451	4451	Ubiquant downscaled data	fdgdfgdfgdfgdfgdfgdfgdfg	[]		3	18	0	munumbutt	ubiquant-downscaled-data
4452	4452	jovenes		[]		0	1	0	abrahamanderson	jovenes
4453	4453	Titanic Story		['history']		1	14	0	nickhull	titanic-story
4454	4454	Chess players	All Time FIDE Rated Chess Player Dataset	['games', 'sports', 'categorical data', 'beginner', 'text data']	"History
Chess is very popular nowadays, there are more than 600 million people in the world who play it. Everyone has heard of them at least once. But how difficult is it to become a professional player? This dataset features amateur and professional players who have ever played at the international level. Yes, yes, Magnus Carlsen and Garry Kasparov are also here.
Try
I think you are also interested to know how the best players differ from amateurs, which country has the most professional players and who is the youngest player now? Analyze and visualize the data, maybe you will find the secret of this wonderful game."	72	502	4	goldian	chess
4455	4455	carvshuman		[]		0	1	0	abrahamanderson	carvshuman
4456	4456	Ann2Chall		[]		3	62	0	giancarlosorrentino	ann2chall
4457	4457	humanvscar		[]		0	0	0	abrahamanderson	humanvscar
4458	4458	objects		['software']		0	6	0	abrahamanderson	objects
4459	4459	chinese-poetry	Ancient Chinese poetry 	[]		6	148	2	jiaminggogogo	chinesepoetry
4460	4460	IMDB 2021		['movies and tv shows']	Dataset with titles, user rating, number of votes, genre, runtime, US certificates, metascore and US box office with all the films of 2020.	3	15	0	erickjbgarcia	imdb-2021
4461	4461	arabic_stop_words		[]		0	7	0	torliko	arabic-stop-words
4462	4462	CasiaV2_revised		[]		3	4	0	nishaahin	casiav2revised
4463	4463	 2020-21 NBA Player Stats: Per Game 	NBA player stats of the 2020 - 2021 season, source:www.basketball-reference.com	['basketball']	"Context
NBA player statistics from the 2021 season
Acknowledgements
https://www.basketball-reference.com/leagues/NBA_2021_per_game.html"	0	24	0	arafbeatz	202021-nba-player-stats-per-game
4464	4464	Evaluation of financial statements	Deliverable: Financial report.	['finance', 'exploratory data analysis', 'statistical analysis']	"📄 Data Context
Instead of a data set, this is an exercise on financial analysis made with R vectors. The intention is to demonstrate the practicality of this tool when interpreting vector calculations, matrices or data frames. This is due to its character of being a high-level language, which allows algorithms to be expressed in a way that is appropriate to human cognitive capacity, instead of the capacity with which machines execute them."	2	34	0	arianrios	evaluacin-de-estados-financieros
4465	4465	CasiaV21Jan		[]		2	2	0	sivasathguru	casiav21jan
4466	4466	Canciones Spotify 1920-2020	Recopilación de canciones extraidas de la API de Spotify	[]	"Primarias
id: (Id de la canción en Spotify)
Numericas:
acousticness: Cuan acústica es la canción (De 0 a 1)
danceability: Describe como de ""bailable"" es una canción basado en una combinación de elementos  músicales como el tempo, el ritmo, la fuerza del beat y la regularidad (De 0 a 1)
energy: Representa una medida percentual de intensidad y actividad. Normalmente, las canciones     energéticas se perciben más rápidas y ruidosas (De 0 a 1)
duration_ms: Duración de la canción en milisegundos
instrumentalness: Cantidad de partes vocales en la canción (De 0 a 1)
valence: Mide la ""positividad músical"". Canciones con alta valencia suelen ser más felices y eufóricas.(De 0 a 1)
popularity: Mide la popularidad de la canción.(De 0 a 100)
tempo: Mide el tempo en beats por minuto (BPM).
liveness: Mide la probabilidad de que la canción haya sido grabada con público en vivo. Según la documentación oficial, un valor superior a 0.8 indica que altamente probable que la canción haya sudo grabada en vivo.(De 0 a 1)
loudness: Mide el volumen de una canción en decibelios
speechiness: Detecta la presencia de discursos hablados en la canción.(De 0 a 1)
Dummy:
mode: Indica la modalida (0 = Menor, 1 = Mayor)
explicit: Indica si contiene lenguaje agresivo (malas palabras, se habla de drogas, etc) (0 = No  tiene, 1 = Tiene)
Categóricas:
key: Todas la clave de la canción en codificación numérica, de 0 a 11, empezando con C = 0 y así sucesivamente
timesignature: Inidca el compás de la canción.
artists: Lista de artistas
id_artists: Ids aritstas
release_date: Fecha de salida de la canción
name: Nombre de la canción"	3	56	1	javivaleiras	spotify-tracks-19202020
4467	4467	mymodel6		[]		0	84	0	keyhanashoori	mymodel6
4468	4468	Ubiquant train.feather		[]		6	45	1	danielkorth	ubiquant-trainfeather
4469	4469	Tesco Stock Prices 1988 -2022		['business']		6	31	0	russell031089	tesco-stock-prices-1988-2022
4470	4470	HDFC.csv		['finance']		0	2	1	mahmoudhassanmahmoud	hdfccsv
4471	4471	Bitcoin Prices Dataset	Bitcoin Historical Price Records	['beginner', 'time series analysis', 'tabular data', 'currencies and foreign exchange']	"Context
Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus ""chained"" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining!
Acknowledgements
The dataset is referred from Kaggle"	159	1117	12	yasserh	bitcoin-prices-dataset
4472	4472	Glove_Word		[]		0	0	0	sarashahin	glove-word
4473	4473	wqdasdasfwadasasd		[]		0	8	0	donggeonhan	wqdasdasfwadasasd
4474	4474	Youtube Transcript Index	Youtube video transcript data	['websites', 'text data', 'video data', 'audio data']		4	5	1	devmohanty	youtube-transcript-index
4475	4475	an2dl-imageclass		[]		1	69	0	andrealentini	an2dlimageclass
4476	4476	Forest fire data set		['business']		0	39	0	ultronstark	forest-fire-data-set
4477	4477	landmark-448-train-clean-x-8		[]		0	1	0	kittenraidrua	landmark-448-train-clean-x-8
4478	4478	Human ds		[]		0	7	0	uroojfatima123	human-ds
4479	4479	Hong Kong's Top 300 YouTubers( 香港 YouTubers 数据集)	香港 300 强 Youtubers 数据集	['intermediate', 'data cleaning', 'data analytics', 'pandas', 'seaborn']	"Who are the top YouTubers in Hong Kong?
According to the number of subscribers, these are the top 300 channels on YouTube.
HK's 300 most popular YouTube channels, together with the channel's name and the number of subscribers, are included in this dataset.
Interested in what Hong Kong People is watching?
Zeeshan-ul-hassan Usmani is to be credited as the source of inspiration."	5	82	4	patriotboy112	hks-top-300-youtubers
4480	4480	Cyclistic Data 2021	Fictional company Cyclistic’s historical trip data for the full year 2021	['united states', 'business', 'automobiles and vehicles', 'beginner', 'data analytics']	"Context
You can use this dataset - Cyclistic’s historical trip data for the year 2021 - to analyze and identify trends. 
Please note that the dataset provides data for Cyclistic which is a fictional company. 
Content
This is public data that you can use to explore how different customer types are using Cyclistic bikes. 
The dataset was acquired here, for the purposes of succeeding the capstone project of the Google Data Analytics Certificate.
The time period for this dataset is the full year 2021."	6	199	3	helddata	cyclistic-data-2021
4481	4481	COVID-19-geographic-disbtribution-worldwide-		[]		1	10	0	ahmedmostafaelgamal	covid19geographicdisbtributionworldwide
4482	4482	horse2zebra		[]		0	4	0	jiccrlla	horse2zebra
4483	4483	gyros-images		[]	"Context
A dataset consisting only of gyros/tylixta images."	0	19	0	tzalex	gyros-images
4484	4484	images_annotations_cars		[]		0	7	0	romainsr	images-annotations-cars
4485	4485	Careerbuilder USA Job Data	This dataset includes job data from CareerBuilder USA	['jobs and career']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 1924405  Domain Name : careerbuilder.usa  Date Range : 01st Apr 2021 - 30th Jun 2021   File Extension : ldjson
Available Fields : uniq_id, crawl_timestamp, url, job_title, company_name, city, state, post_date, job_description, job_type, apply_url, job_board, geo, job_post_lang, inferred_iso2_lang_code, is_remote, test1_cities, test1_states, site_name, html_job_description, domain, postdate_yyyymmdd, predicted_language, inferred_iso3_lang_code, test1_inferred_city, test1_inferred_state, test1_inferred_country, has_expired, last_expiry_check_date, latest_expiry_check_date, dataset, postdate_in_indexname_format, segment_name, duplicate_status, fitness_score  
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud, DataStock and live job data from JobsPikr.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	3	19	0	promptcloud	careerbuilder-usa-job-data
4486	4486	Human Faces	This is a human facial detection dataset.	[]		8	16	0	garidziracrispen	human-faces
4487	4487	pt_starfish	cb_net_mask_rcnn_swin_tiny	[]		9	19	0	deeeeeeeplearning	pt-starfish
4488	4488	yolox_cots_models		[]		0	5	0	zengkuikui	yolox-cots-models
4489	4489	photos		[]		1	8	0	evernax95	photos
4490	4490	landmark-448-train-clean-x-4		[]		0	1	0	kittenraidrua	landmark-448-train-clean-x-4
4491	4491	fake news		['news']		0	7	0	saeedakbarkhan	fake-news
4492	4492	Breast cancer		['cancer']		2	23	0	lingga	breast-cancer
4493	4493	geneset.txt		[]		1	27	0	tarikuzzaman	genesettxt
4494	4494	Song_Popularity_Folds		[]		1	8	0	manabendrarout	song-popularity-folds
4495	4495	License Plate Dataset For YOLO		[]		9	24	0	doganozcan	license-plate-dataset-for-yolo
4496	4496	EfficientNet_tfKeras	EfficientNet_tfKeras	[]		1	812	3	mlneo07	efficientnet-keras-and-tensorflow-keras
4497	4497	DR_preprocessed_full	preprocessed images into subcategories	[]		2	42	0	gajipuvi	dr-preprocessed-full
4498	4498	20-Bn Jester TFRecord (9 Classes)	20bn Jester Dataset with 9 classes TFRecord Format..	['education', 'intermediate', 'computer vision', 'classification', 'tensorflow', 'transformers']	"Context
Loading Jester Dataset for personal project in limited hardware was problematic. Facing producer-consumer problem I decided to create TFRecord for a limited set of classes.
Content
20BN Jester Dataset was filtered and a small set of gestures were extracted and converted to TFRecordFormat.
Image size 100x100 with 36 frames.
Gestures = [""Swiping Down"", ""Swiping Up"", ""Swiping Left"", ""Swiping Right"", ""Pushing Hand Away"", ""Pulling Hand In"", ""Turning Hand Clockwise"", ""Turning Hand Counterclockwise"", ""No gesture""]
Train Samples: 36297
Validation Samples: 4389"	0	13	0	sahasprajapati	jester-tfrecord-9class
4499	4499	best_10epoch_ckpt		[]		0	7	0	sakura1986	d8-train09-yoloxl-adam
4500	4500	vocal_denoising		[]		0	13	0	lalalalex	vocal-denoising
4501	4501	Hindi Text Image Dataset	Hindi Text image dataset for Scene Text Recognition	['cities and urban areas', 'linguistics', 'computer vision', 'image data', 'email and messaging']	"This sample dataset is collected by DataCluster Labs, India.
To download full datasets or to submit a request for your dataset needs, please email on: sales@datacluster.ai
Introduction
This dataset can be used to detect or recognize Hindi texts. It can be trained for scene text detection or optical character recognition(OCR) usecase. It contains text in various fonts, size and in colors.
Dataset Features
Captured by 2000+ unique users
Rich in diversity
Mobile phone view point
Various lighting conditions
Text in various fonts
Different Font Size and Sentences.
Dataset Features
Recognition and detection annotations available
Multiple category annotations possible
COCO, PASCAL VOC and YOLO formats
To download full datasets or to submit a request for your dataset needs, please ping us on sales@datacluster.ai**
Visit www.datacluster.in to know more.
Note:
All the images are manually verified and are contributed by the large contributor base on DataCluster platform"	3	128	1	dataclusterlabs	hindi-text-image-dataset
4502	4502	vocal_denoising		[]		0	13	0	lalalalex	vocal-denoising
4503	4503	Hindi Text Image Dataset	Hindi Text image dataset for Scene Text Recognition	['cities and urban areas', 'linguistics', 'computer vision', 'image data', 'email and messaging']	"This sample dataset is collected by DataCluster Labs, India.
To download full datasets or to submit a request for your dataset needs, please email on: sales@datacluster.ai
Introduction
This dataset can be used to detect or recognize Hindi texts. It can be trained for scene text detection or optical character recognition(OCR) usecase. It contains text in various fonts, size and in colors.
Dataset Features
Captured by 2000+ unique users
Rich in diversity
Mobile phone view point
Various lighting conditions
Text in various fonts
Different Font Size and Sentences.
Dataset Features
Recognition and detection annotations available
Multiple category annotations possible
COCO, PASCAL VOC and YOLO formats
To download full datasets or to submit a request for your dataset needs, please ping us on sales@datacluster.ai**
Visit www.datacluster.in to know more.
Note:
All the images are manually verified and are contributed by the large contributor base on DataCluster platform"	3	128	1	dataclusterlabs	hindi-text-image-dataset
4504	4504	Telecom_churn.csv		['internet']		2	8	0	keyush06	telecom-churncsv
4505	4505	Dobble Small Resolution		[]		1	12	0	atugaryov	dobble-small-resolution
4506	4506	landmark-448-train-clean-x-6		[]		0	7	0	kittenraidrua	landmark-448-train-clean-x-6
4507	4507	Horses		[]		2	11	0	uroojfatima123	horses
4508	4508	toxic_roberta		[]		1	3	0	kintaro1	toxic-roberta
4509	4509	reducehiggs4lep		[]		2	7	0	marcelagarca	reducehiggs4lep
4510	4510	analisa kanker		[]		1	16	0	alimmusyaffa	analisa-kanker
4511	4511	Tp Jan 2022		[]		0	13	0	piyushm28	tp-jan-2022
4512	4512	IBM1.csv		['internet']		2	7	0	vishnu393831	ibm1csv
4513	4513	new chest xray		[]		4	34	0	chaitrapatwardhan	new-chest-xray-for-disease
4514	4514	LINAIGE	Low-resolution INfrared-array data for AI on the edGE	['people and society', 'classification', 'deep learning', 'image data', 'covid19']	"Data Description
The dataset consists of low-resolution multi-pixel infrared sensor samples, to be used for AI regression and classification applications (person counting, presence detection, etc.) and of the corresponding ground truth values. The main purpose of the dataset is to support the development and validation of the infrared sensor demonstrator in the project.
Data Format
The dataset consists of a set of Comma-Separated Value (CSV) files, where each row stores the information about one collected infrared frame, in particular:
•   Frame timestamp
•   Frame infrared pixels (8x8 array, stored in row-major, left to right, order).
•   Room temperature, measured by a thermistor
•   Ground truth label (e.g., person count in the frame).
•   Label confidence (a letter, ""e"" for ""easy-to-label"", ""h"" for ""hard-to-label""), used for indicating human labeller's confidence to assign the label to specific frame, because of the difficulty of exactly matching the alignment and viewing angles of the infrared sensor and an optical camera used as reference.
Samples in each CSV file are collected at 10Hz and ordered by timestamp. Different CSV files refer to different data collection sessions (e.g., different environments, different days or times of the day, etc.).
Data Size
The dataset will grow throughout the project. Currently, it includes a total of 25000 samples collected over 6 different sessions. The total file size is 13.2MB. 
Acknowledgements
If you use LINAIGE in your experiments, please make sure to cite our paper:
@inproceedings{xie2022privacy,
    author = {Xie, Chen and Daghero, Francesco and Chen, Yukai and Castellano, Marco and Gandolfi, Luca and Calimera, Andrea and Macii, Enrico and Poncino, Massimo and Jahier Pagliari, Daniele},
    title = {Privacy-preserving Social Distance Monitoring on Microcontrollers with Low-Resolution Infrared Sensors and CNNs},
    year = {2022},
    publisher = {IEEE},
    booktitle = {Proceedings of the 2022 IEEE International Symposium on Circuits and Systems (ISCAS)},
    series = {ISCAS 2022}
}"	1	123	6	francescodaghero	linaige
4515	4515	FRC_Balls_Training		[]		3	17	1	christoshen	frc-balls-training
4516	4516	eurosat_numpy_final		[]		1	9	0	adithyansukumar	eurosat-numpy-final
4517	4517	538 character		['arts and entertainment']		0	1	0	acidrafiea	538-character
4518	4518	finalproject		[]		0	11	0	ndhu411011327	finalproject
4519	4519	GitHubRecSys1		[]		24	291	0	ayanvishwakarma12	githubrecsys1
4520	4520	finalwork		[]		1	3	0	ndhu411011327	finalwork
4521	4521	f3spss		[]		0	3	0	acidrafiea	f3spss
4522	4522	my_data		[]		0	6	0	amjadalkadi	my-data
4523	4523	retrain_id_yolov5		[]		0	0	0	zengkuikui	retrain-id-yolov5
4524	4524	faster_rcnn_mmdetection	Faster RCNN model based on mmdetection.	[]		4	35	0	dinghuixiang	faster-rcnn-mmdetection
4525	4525	landmark-448-train-clean-x-2		[]		0	3	0	kittenraidrua	landmark-448-train-clean-x-2
4526	4526	landmark-448-train-clean-x-5		[]		0	10	0	kittenraidrua	landmark-448-train-clean-x-5
4527	4527	landmark-448-train-clean-x-3		[]		0	6	0	kittenraidrua	landmark-448-train-clean-x-3
4528	4528	landmark-448-train-clean-x-7		[]		0	3	0	kittenraidrua	landmark-448-train-clean-x-7
4529	4529	word2vec		[]		0	6	0	mlcovidresearch	word2vec
4530	4530	LinkedIn Jobs		['jobs and career']		11	85	0	samssj10	linkedin-jobs
4531	4531	yeast_data		[]		0	4	0	drangos	yeast-data
4532	4532	retrain-models-0121_1		[]		0	1	0	zengkuikui	retrainmodels0121-1
4533	4533	Sentiment analysis tweets		['online communities']		5	40	0	mlcovidresearch	sentiment-analysis-tweets
4534	4534	longformer-linear5ep-last-model-f4-cv-0.6306		[]		0	6	0	atharvaingle	longformer-linear5ep-last-model-f4
4535	4535	fs static icnet original synthesis		['earth and nature']		1	10	0	synboost	fs-static-icnet-original-synthesis
4536	4536	pneumonia-xray-images		[]		4	11	0	logulg	pneumoniaxrayimages
4537	4537	ump1.95GB		[]		23	74	4	sytuannguyen	ump195gb
4538	4538	Synthetic 10M x 1M (1B) matrix with zipf(1.1) skew	1B revealed cells, distributed to cols+rows according to a zipf(1.1) distr.	['recommender systems']	"This is a synthetic rectangular matrix with 10 million rows and 1 million columns and 1 billion revealed (i.e., non-zero) cells.
To generate the set of revealed cells, we sampled 1 billion random row and column indices from zipf(1.1, 10M) (for rows) and zipf(1.1, 1M) (for columns)
distributions (rejecting duplicate coordinates until reaching 1 billion unique cells).
Due to its size (32GB uncompressed), we compressed the file train.mmc using xz. To uncompress, you can use tar xf train.mmc.tar.xz, for example
This is the matrix hat was used in the paper NuPS: A Parameter Server for Machine Learning with Non-Uniform Parameter Access (https://arxiv.org/abs/2104.00501)."	1	33	0	alexrenz	syncthetic-zipf-1-1-matrix
4539	4539	yolov5m6		[]		16	106	0	w3579628328	yolov5m6
4540	4540	embedding		[]		1	4	0	kofi1984	embedding
4541	4541	Measuring Hate Speech		['nlp', 'text mining', 'text data']	"Description
This is a public release of the dataset described in Kennedy et al. (2020), consisting of 39,565 comments annotated by 7,912 annotators, for 135,556 combined rows. The primary outcome variable is the ""hate speech score"" but the 10 constituent labels can also be treated as outcomes. 
The original paper can be found here: Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application
Original dataset link at HuggingFace: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech
Acknowledgemen to the original work:
@article{kennedy2020constructing,
  title={Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application},
  author={Kennedy, Chris J and Bacon, Geoff and Sahn, Alexander and von Vacano, Claudia},
  journal={arXiv preprint arXiv:2009.10277},
  year={2020}
}"	137	666	17	andre112	measuring-hate-speech
4542	4542	weights_barlow_twins		[]		0	18	0	clementapa	weights-barlow-twins
4543	4543	Linkedin_datasets		[]		0	26	0	bhushanpande	linkedin-datasets
4544	4544	Formula 1 Stats 1998-2021	Driver, win and championship statistics	['sports']	"An excel file containing the following on the seasons 1998 to 2021:
-Personal stats of drivers (championship finishes, wins/season, total wins, podiums, points, fastest laps and pole positions)
-Championship stats (drivers and teams, with colours, and their championship positions at the end of each season)
-Table with the wins per circuit per year (also with colours) and the wins per team per year
This dataset was mainly made for fun / nice looking visualization so first open it in excel to see the colours as well. If you want to use it for more complex purposes, I would recommend to do some data-prepping"	5	52	1	nickpeterselie	formula-1-stats-19982021
4545	4545	dataset2		[]		0	3	0	yanngrandgirard	dataset2
4546	4546	pig sale		[]		0	15	0	ruiyu0731	pig-sale
4547	4547	dataset1		[]		0	3	0	yanngrandgirard	dataset1
4548	4548	headmodels		[]		4	5	0	xixi6196	headmodels
4549	4549	ubiquant_mlp_20folds		[]		0	2	2	mathurinache	ubiquant-mlp-20folds
4550	4550	landmark-448-train-clean-x-1		[]		0	5	0	kittenraidrua	landmark-448-train-clean-x-1
4551	4551	Sartorius - train v2		[]		1	4	0	prateekagnihotri	sartorius-train-v2
4552	4552	Sartorius - train v1		[]		1	20	0	prateekagnihotri	sartorius-train-v1
4553	4553	Sartorius - train v0		['arts and entertainment']		1	14	0	prateekagnihotri	sartorius-train-v0
4554	4554	hiehiehi 		[]		0	3	0	zhuoyangwang	bumeibule
4555	4555	full_log_spec_dataset		[]		0	15	0	lorenzoconcina	full-log-spec-dataset
4556	4556	mymodel9		[]		0	8	0	mohammadiashoori	mymodel9
4557	4557	Dataset		[]		0	1	0	nilaksheerajule	dataset
4558	4558	Vaccine record in Germany		['europe', 'public health', 'geography', 'drugs and medications', 'covid19']		4	334	2	shashankpandey0949	vaccine-record-in-germany
4559	4559	face&mel		[]		0	14	1	xiahuadong1981	facemel
4560	4560	lte-dataset1		[]		0	1	0	nilaksheerajule	ltedataset1
4561	4561	sarcasm prepro		['arts and entertainment']		0	8	0	michaeloster	preprocessed
4562	4562	My Dataset		[]		0	0	0	nilaksheerajule	my-dataset
4563	4563	LTE-Dataset		[]		0	5	0	nilaksheerajule	ltedataset
4564	4564	catboost_models_ubiq		[]		0	2	0	kryzhikov	catboost-models-ubiq
4565	4565	Investigating Singapore's Birth Rate		[]		8	24	0	sherrysoh	investigating-singapores-birth-rate
4566	4566	LTE dataset		['internet']		0	14	0	nilaksheerajule	lte-dataset
4567	4567	twCounty2010.geo.json		[]		0	1	0	richard0218	twcounty2010geojson
4568	4568	Customer Spend Dataset	Customer's Spending over different items	['categorical data', 'clustering', 'e-commerce services']	"Context
This is the dataset to analyze the consumer behavior when they visit the departmental store. To perform the analysis of the spending of every customer over different Item Categories
Content
The dataset contains information about the customer's spending over different item categories. Additionally the ERP system of the departmental store keeps track of the customer's visit and their monthly spending. It is useful to categorize the customers spending"	2	35	0	anandchauhan	customer-spend-dataset
4569	4569	UMP-Exp001-ColabTraining		[]		6	12	0	columbia2131	ump-exp001-colabtraining
4570	4570	Mask worn incorrectly		[]		1	8	0	temporarytext7	mask-worn-incorrectly
4571	4571	No mask		['arts and entertainment']		1	6	0	temporarytext7	no-mask
4572	4572	Mask worn correctly		[]		1	7	0	temporarytext7	mask-worn-correctly
4573	4573	Mask only		[]		1	10	0	temporarytext7	mask-only
4574	4574	Electra_large		[]		0	2	0	masatakaaoki	electra-large
4575	4575	ai_spark_total		[]		4	5	0	junghokim	ai-spark-total
4576	4576	DS Project: Salary Data Cleaned		[]		0	16	0	muhtaufiqfirmansyah	ds-project-salary-data-cleaned
4577	4577	templemap		[]		0	10	0	amanda410714218	templemap
4578	4578	kitti-odometry-woo	kitti odometry dataset that I have removed moving objects from images.	['robotics', 'computer science']	"Context
This Dataset contains KITTI Visual Odometry / SLAM Evaluation 2012 benchmark, created by
Andreas Geiger, Philip Lenz and Raquel Urtasun in the Proceedings of 2012 CVPR ,"" Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite"".
This is KITTI Visual Odometry dataset first 11 sequences that I have removed moving object like person and cars from images.
I first trained the yoloV5 network on the Argoverse dateset to find objects that could move.With the help of this network, moving objects were found, then all the pixels inside the square of the detected object were set to (0,0,0).
Acknowledgements
Ruslan Baynazarov  had downloaded this dataset from the link above and uploaded it on kaggle unmodified.
Thank you Ruslan"	1	29	0	aminabazari	kitti-odometry-woo
4579	4579	opt_final		[]		1	7	0	caizd2012	opt-final
4580	4580	Kaggle Notebooks Ranking	Top 3000 Kaggle Notebooks Ranking	['computer science', 'regression']	"Context
This dataset contains Kaggle ranking of notebooks.
Content
+3000 rows and 8 columns.
Columns' description are listed below.
Rank : Rank of the user
Tier : Grandmaster, Master or Expert
Username : Name of the user
Join Date : Year of join
Gold Medals : Number of gold medals
Silver Medals : Number of silver medals
Bronze Medals : Number of bronze medals
Points : Total points
Acknowledgements
Data from Kaggle.
Image from Wikiwand.
If you're reading this, please upvote."	37	661	20	vivovinco	kaggle-notebooks-ranking
4581	4581	TPS Jan 2022 Pseudo Labels	Pseudo Labels for blend and stacking models	['tabular data']	Hopefully these files will be of use in the Tabular Playground Series - Jan 2022 competition.	11	20	4	hiro5299834	tps-jan-2022-pseudo-labels
4582	4582	Augmented_img_full		[]		0	3	0	fidanmusazade	augmented-img-full
4583	4583	TaipeiSansTC		[]		0	10	0	ivanechen	taipeisanstc
4584	4584	Dataset_librable		[]		0	4	0	pierrealaintietz	dataset-librable
4585	4585	kanker_payudara		[]		0	2	0	muhrifqi	kanker-payudara
4586	4586	captcha_6		[]		0	11	0	nirnayroy	captcha-6
4587	4587	OECD Consumer Confidence Index	From https://data.oecd.org/leadind/consumer-confidence-index-cci.htm	[]	"This consumer confidence indicator provides an indication of future developments of households’ consumption and saving, based upon answers regarding their expected financial situation, their sentiment about the general economic situation, unemployment and capability of savings. An indicator above 100 signals a boost in the consumers’ confidence towards the future economic situation, as a consequence of which they are less prone to save, and more inclined to spend money on major purchases in the next 12 months. Values below 100 indicate a pessimistic attitude towards future developments in the economy, possibly resulting in a tendency to save more and consume less. 
Source: https://data.oecd.org/leadind/consumer-confidence-index-cci.htm"	12	53	4	ambrosm	oecd-consumer-confidence-index
4588	4588	Chest Xray Images		['health']		14	26	0	chaitrapatwardhan	chestxrayimages
4589	4589	d8_train06_yoloxl_adam_10_15epoch		[]		0	11	0	sakura1986	d8-train06-yoloxl-adam
4590	4590	Shamdatacol		[]		0	5	0	pavankandru	shamdatacol
4591	4591	Near earth objects observed by NASA(1900-2021)	Dataset is about the NEOS which past or will pass through the earth	['earth science', 'astronomy', 'exploratory data analysis', 'data analytics']	"<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Asteroids-KnownNearEarthObjects-Animation-UpTo20180101.gif/600px-Asteroids-KnownNearEarthObjects-Animation-UpTo20180101.gif"">
A near-Earth object is an asteroid or comet which passes close to the Earth's orbit. In technical terms, a NEO is considered to have a trajectory that brings it within 1.3 astronomical units of the Sun and hence within 0.3 astronomical units, or approximately 45 million kilometers, of the Earth's orbit.
NEOS represent potentially catastrophic threats to our planet. The International Asteroid Warning Network (IAWN) and the Space Mission Planning Advisory Group (SMPAG) are two entities established in 2014 as a result of United Nations-endorsed recommendations, and represent important mechanisms at the global level for strengthening coordination in the area of planetary defense.TThe scientific interest in comets and asteroids is due largely to their status as the relatively unchanged remnant debris from the solar system formation process some 4.6 billion years ago. The giant outer planets (Jupiter, Saturn, Uranus, and Neptune) formed from an agglomeration of billions of comets, and the leftover bits and pieces from this formation process are the comets we see today. Likewise, today’s asteroids are the bits and pieces left ove from the initial agglomeration of the inner planets that include Mercury, Venus, Earth, and Mars.
<img src=""https://image.slidesharecdn.com/cometsasteroids-and-meteors-171013071324/95/comets-asteroids-and-meteors-2-638.jpg?cb=1581516590"">
As the primitive, leftover building blocks of the solar system formation process, comets and asteroids offer clues to the chemical mixture from which the planets formed some 4.6 billion years ago. If we wish to know the composition of the primordial mixture from which the planets formed, then we must determine the chemical constituents of the leftover debris from this formation process - the comets and asteroids."	49	558	16	ramjasmaurya	near-earth-objects-observed-by-nasa
4592	4592	flickr8k-textd		[]		0	3	0	ludaykumar	flickr8k-textd
4593	4593	geng_2	Normal to pix generator	[]		0	5	0	electrikfire7	geng-2
4594	4594	taichungwin		[]		0	3	0	yihanwangyihan	taichungwin
4595	4595	Tredence_machinehack_2022		[]		0	10	0	ashwathshetty	tredence-machinehack-2022
4596	4596	trident	Make PyTorch and TensorFlow two become one.	['computer science']		25	1876	0	allanyiinai	trident
4597	4597	Intrusion_data_wsn	Prediction of the k-barriers for intrusion detection in WSNs using ML.	['computer science', 'beginner', 'svm', 'tabular data', 'regression']	"This dataset is consist of five columns. The first four columns represent the input features (i.e., area, sensing range, transmission range, and the number of sensors).  The last column represents the response variable or target variable (i.e., number of barriers). Through this dataset, we can set up a regression model to predict the number of barriers required for fast intrusion detection and prevention in machine learning.
For more details please refer to the published paper;
https://www.mdpi.com/1424-8220/22/3/1070
If you are using this data then please cite the following paper;
Singh, A., Amutha, J., Nagar, J., Sharma, S., & Lee, C. C. (2022). LT-FS-ID: Log-Transformed Feature Learning and Feature-Scaling-Based Machine Learning Algorithms to Predict the k-Barriers for Intrusion Detection Using Wireless Sensor Network. Sensors, 22(3), 1070."	2	164	1	abhilashdata	intrusion-data-wsn
4598	4598	cities1		[]		0	1	0	yudanzhang3	cities1
4599	4599	H&M Stores Dataset (2022)	H&M Global Stores Location	['geography', 'clothing and accessories', 'geospatial analysis', 'tabular data', 'retail and shopping']	"Context
Hennes & Mauritz AB (H&M) is a Swedish multinational clothing company headquartered in Stockholm. It is known for its fast-fashion clothing for men, women, teenagers, and children. H&M operates in 75+ countries with over 5,000 stores under the various company brands, with 126,000 full-time equivalent positions. It is the second-largest global clothing retailer, behind Spain-based Inditex (parent company of Zara). Founded by Erling Persson and run by his son Stefan Persson and Helena Helmersson, the company makes its online shopping available in 33 countries. 
This dataset contains global locations and meta data of H&M Stores."	641	4433	31	shivamb	hm-stores-dataset
4600	4600	telecom customer churn dataset		['business']		1	25	0	hajarkhagd	telecom-customer-churn-dataset
4601	4601	IDA_Statement_Of_Credits_and_Grants		['global', 'india', 'china ', 'asia', 'economics']	"Context
International Development Association Statement of Credits and Grants from World Bank web page
Data covers since 1961 to 2021.
Description from its page:
""The International Development Association (IDA) credits are public and publicly guaranteed debt extended by the World Bank Group. IDA provides development credits, grants and guarantees to its recipient member countries to help meet their development needs. Credits from IDA are at concessional rates. Data are in U.S. dollars calculated using historical rates. This dataset contains historical snapshots of the IDA Statement of Credits and Grants including the latest available snapshot. The World Bank complies with all sanctions applicable to World Bank transactions."""	0	6	0	felipeea	ida-credits-grants
4602	4602	Movie Recommendation	Movie Recommendation / Recommender system / Movies and Series Data	['arts and entertainment', 'celebrities', 'movies and tv shows']		7	86	5	lucifierx	movie-recommendation
4603	4603	handposture2(splited)		[]		0	5	0	xiongxiongzhizhi	handposture2splited
4604	4604	MLProjectData	Tiny ImageNet Dataset cleaned for a Support Vector Classification Model	['computer science', 'sklearn']		0	9	0	trevortrwa4172	mlprojectdata
4605	4605	klustera		[]		0	2	0	nocturnalmoon	klustera
4606	4606	TL21 States and Places	TIGER Line States and Places	[]		1	12	0	jorgegarciainiguez	tl21-states-and-places
4607	4607	school		['primary and secondary schools']		0	6	0	hjchan520	school
4608	4608	Testdataset		[]		0	17	0	gieon91	testdataset
4609	4609	d9d922		[]		0	0	0	melodyelect	d9d922
4610	4610	326541		[]		0	0	0	qcq123456789	326541
4611	4611	signtestset		[]		0	16	0	gieon91	signtestset
4612	4612	signtrainset		[]		0	4	0	gieon91	signtrainset
4613	4613	fontproperties		[]		0	2	0	hsiaochingkao	fontproperties
4614	4614	HDB Prices		[]		0	6	0	sherrysoh	hdb-prices
4615	4615	pog youtube like thumbnail feature embeddings		['arts and entertainment']		2	62	5	robikscube	pog-youtube-like-thumbnail-feature-embeddings
4616	4616	Hospital Admissions Data	Two Year Hospital Admissions and Discharge Data from Hero DMC Heart Institute	['india', 'healthcare', 'public health', 'pollution', 'heart conditions']	"This dataset is being provided under creative commons License (Attribution-Non-Commercial-Share Alike 4.0 International (CC BY-NC-SA 4.0)) https://creativecommons.org/licenses/by-nc-sa/4.0/
Context
This data was collected from patients admitted over a period of two years (1 April 2017 to 31 March 2019) at Hero DMC Heart Institute, Unit of Dayanand Medical College and Hospital, Ludhiana, Punjab, India. This is a tertiary care medical college and hospital. During the study period, the cardiology unit had 14,845 admissions corresponding to 12,238 patients. 1921 patients who had multiple admissions. 
Specifically, data were related to patients ; date of admission; date of discharge; demographics, such as age, sex, locality (rural or urban); type of admission (emergency or outpatient); patient history, including smoking, alcohol, diabetes mellitus (DM), hypertension (HTN), prior coronary artery disease (CAD), prior cardiomyopathy (CMP), and chronic kidney disease (CKD); and lab parameters corresponding to hemoglobin (HB), total lymphocyte count (TLC), platelets, glucose, urea, creatinine, brain natriuretic peptide (BNP), raised cardiac enzymes (RCE) and ejection fraction (EF). Other comorbidities and features (28 features), including heart failure, STEMI, and pulmonary embolism, were recorded and analyzed.
Shock was defined as systolic blood pressure &lt; 90 mmHg, and when  the cause for shock was any reason other than cardiac.  Patients in shock due to cardiac reasons were classified into cardiogenic shock. Patients in shock due to multifactorial pathophysiology (cardiac and non-cardiac) were considered for both categories. 
The outcomes indicating whether the patient was discharged or expired in the hospital were also recorded.
Further details about this dataset can be found here:
https://doi.org/10.3390/diagnostics12020241 
If you use this dataset in academic research all publications arising out of it must cite the following paper:
Bollepalli, S.C.; Sahani, A.K.; Aslam, N.; Mohan, B.; Kulkarni, K.; Goyal, A.; Singh, B.; Singh, G.; Mittal, A.; Tandon, R.; Chhabra, S.T.; Wander, G.S.; Armoundas, A.A. An Optimized Machine Learning Model Accurately Predicts In-Hospital Outcomes at Admission to a Cardiac Unit. Diagnostics 2022, 12, 241. https://doi.org/10.3390/diagnostics12020241
If you intend to use this data for commercial purpose explicit written permission is required from data providers.
Content
table_headings.csv has explanatory names of all columns.
Acknowledgements
Data was collected from Hero Dayanand Medical College Heart Institute Unit of Dayanand Medical College and Hospital, Ludhiana, Punjab, India.
Inspiration
For any questions about the data or collaborations please contact ashish.sahani@iitrpr.ac.in"	71	1071	2	ashishsahani	hospital-admissions-data
4617	4617	lsun_church		[]		0	12	0	hamzabenmendil	lsun-church
4618	4618	thaifont		[]		0	0	0	plenoi	thaifont
4619	4619	ClassMusic		[]		0	15	0	coreyjen	classmusic
4620	4620	Vietnamese News Datasets (2021-2022)	Vietnamese fake and real news datasets for fake news detect	['news']		0	27	1	flap1812	fake-vietnamese-news-datasets-20212022
4621	4621	retrain_models_0121		[]		0	4	0	zengkuikui	retrain-models-0121
4622	4622	inputdata1		[]		0	2	0	hayden234	inputdata1
4623	4623	CDC NVSS	National Vital Statistics System CDC data	['health']		0	48	0	allegrac	cdc-nvss
4624	4624	ubiquant parquet		['computer science', 'programming']	"Code to create parquet
def transform_to_parquet(path, usecols, dtype):
    train = pd.read_csv(
        path,
        usecols=usecols,
        dtype=dtypes
    )
    train.to_parquet('train_low_mem.parquet')
path = '../input/ubiquant-market-prediction/train.csv'
basecols = ['row_id', 'time_id', 'investment_id', 'target']
features = [f'f_{i}' for i in range(300)]
dtypes = {
    'row_id': 'str',
    'time_id': 'uint16',
    'investment_id': 'uint16',
    'target': 'float32',
}
for col in features:
    dtypes[col] = 'float32'
transform_to_parquet(path, basecols + features, dtypes)"	16	41	0	currypurin	ubiquant-parquet-low-mem
4625	4625	eurusd15m		[]		0	2	0	joe199	eurusd15m
4626	4626	huggingfaceText		[]		0	23	1	jinbao1	huggingfacetext
4627	4627	Arabic Speech Corpus	Arabic Speech Corpus	[]	"Context
This Speech corpus has been developed as part of PhD work carried out by Nawar Halabi at the University of Southampton. The corpus was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.
It is released here under the creative commons license specified below. In case further rights are required, or you require consultancy for building Arabic speech corpora, please contact Nawar Halabi by email. Thank you for your interest.
Content
1813 .wav files containing spoken utterances.
1813 .lab files containing text utterances.
1813 .TextGrid files containing the phoneme labels with time stamps of the boundaries where these occur in the .wav files. These files can be opened using Praat software.
phonetic-transcript.txt which has the form ""[wav_filename]"" ""[Phoneme Sequence]"" in every line.
orthographic-transcript.txt which has the form ""[wav_filename]"" ""[Orthographic Transcript]"" in every line. Orthography is in Buckwalter Format which is friendlier where there is software that does not read Arabic script. It can be easily converted back to Arabic.
There is an extra 18 minutes of fully annotated corpus (separate from above but with the same structure as above) which was used to evaluted the corpus (see PhD thesis). Feel free to use it in your applications.
Acknowledgements
Thank you very much to Taha Zerrouki, Ahmad Barqawi, Karim Hemina and Oussama Hemina for their work to produce this TTS:
Festival for Arabic
Mishkal Diacritiser
Shakkala Diacritiser"	19	381	4	haithemhermessi	arabic-speech-corpus
4628	4628	chanisss		[]		0	7	0	s410711217	chanisss
4629	4629	Bodyperformance		[]		2	5	1	hamza256	bodyperformance
4630	4630	Playlist		['arts and entertainment']		0	3	0	jairpozocoronel	playlist
4631	4631	2019 Flight and weather data		[]		9	30	0	pateldhyey	2019-flight-and-weather-data
4632	4632	rappenhofmap		[]		0	2	0	anfauglir	rappenhofmap
4633	4633	NLP on gutenberg textual data 	Natural Language Processing project on Ebook data of Gutenberg textual Data	[]		1	11	1	virajoak	nlp-on-gutenberg-textual-data
4634	4634	yolo-baseline-weights		[]		0	8	0	anhnguyen811	yolobaselineweights
4635	4635	INT20H-2022-dataset		['earth and nature']		0	12	0	kryvokhyzha	int20h2022dataset
4636	4636	Playlist		['arts and entertainment']		0	3	0	kekwong2	playlist
4637	4637	Class playlist 		['education']		0	3	1	rowanlau	class-playlist
4638	4638	Playlists		[]		0	6	0	kekwong2	playlists
4639	4639	Playlist		['arts and entertainment']		0	6	0	ginodeynaco	playlist
4640	4640	Playlist		['arts and entertainment']		0	2	0	daniellely	playlist
4641	4641	srtest		[]		0	10	0	amanda410714218	srtest
4642	4642	testModel1		[]		0	2	0	ruslanomelchenko	testmodel1
4643	4643	Waymo-Open Validation Dataset 4		[]		0	3	0	mohammedosama	waymo-open-validation-dataset-4
4644	4644	DeepMIMO dataset	Communication data used for DL applications	['research', 'internet', 'electronics', 'engineering', 'simulations']	"Context
DeepMIMO dataset: Massive MIMO (Multi-Input Multi-Output) dataset of O1 scenario for 3.5GHz (Sub 6Ghz) and 28GHz (mmWave) frequencies.
Content
• DeepMIMO_dataset{b}.user{u}.channel accesses the M × |K| channel matrix between the active base station b and user u. Here active_BS=3 and the active users rows active_user_first=700 and active_user_last=1300 then DeepMIMO_dataset{1}.user{1}.channel accesses the channel between the first active BS, which is BS 3, and the first active user, which is the first user in the row R700 (each row has 181 users).
• DeepMIMO_dataset{b}.user{u}.loc accesses the position vector u of the uth active user.
Acknowledgements
A. Alkhateeb,""DeepMIMO: A Generic Deep Learning Dataset for Millimeter Wave and Massive MIMO Applications"" (https://www.deepmimo.net/)"	83	781	1	sagarkaushik98	deepmimo-dataset
4645	4645	Mouse on Treadmill Frames		['exercise']		1	27	0	raminanush	mouse-on-treadmill-frames
4646	4646	measuring hate speech		[]	"This is a public release of the dataset described in Kennedy et al. (2020). The dataset contains one row for each comment that an annotator reviews; each comment is rated multiple times. The primary outcome variable is the ""hate speech score"" but the 10 constituent labels can also be treated as outcomes.
This dataset card is a work in progress and will be improved over time.
Key dataset columns
hate_speech_score - continuous hate speech measure, where higher = more hateful and lower = less hateful
text - lightly processed text of a social media post
comment_id - unique ID for each comment
annotator_id - unique ID for each annotator
sentiment - ordinal label that is combined into the continuous score
respect - ordinal label that is combined into the continuous score
insult - ordinal label that is combined into the continuous score
humiliate - ordinal label that is combined into the continuous score
status - ordinal label that is combined into the continuous score
dehumanize - ordinal label that is combined into the continuous score
violence - ordinal label that is combined into the continuous score
genocide - ordinal label that is combined into the continuous score
attack_defend - ordinal label that is combined into the continuous score
hatespeech - ordinal label that is combined into the continuous score
annotator_severity - annotator's estimated survey interpretation bias
in this dataset i only keep hatespeech,annotator_severity,text and sentiment columns
Citation
@article{kennedy2020constructing,
  title={Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application},
  author={Kennedy, Chris J and Bacon, Geoff and Sahn, Alexander and von Vacano, Claudia},
  journal={arXiv preprint arXiv:2009.10277},
  year={2020}
}
references : 
1. https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech
2. https://hatespeech.berkeley.edu/
3. https://www.kaggle.com/c/jigsaw-toxic-severity-rating/discussion/302060"	2	32	1	mobassir	measuring-hate-speech
4647	4647	testove_int20h_uklon_churn		[]		1	5	0	zekamrozek	testove-int20h-uklon-churn
4648	4648	Waymo-Open Validation Dataset 3		[]		0	2	0	mohammedosama	waymo-open-validation-dataset-3
4649	4649	longformer-linear5ep-last-model-f2-cv-0.6223		[]		0	4	0	atharvaingle	longformer-linear5ep-last-model-f2
4650	4650	longformer-linear5ep-last-model-f3-cv-0.626		[]		0	3	0	atharvaingle	longformer-linear5ep-last-model-f3
4651	4651	test_data		[]		1	2	0	amanda410714218	test-data
4652	4652	embedding		[]		0	0	0	sarashahin	embedding
4653	4653	COVID-19 image data collection	This is not a kaggle competition dataset.	['earth and nature', 'computer science']	"Background
The 2019 novel coronavirus (COVID-19) presents several unique features. While the diagnosis is confirmed using polymerase chain reaction (PCR), infected patients with pneumonia may present on chest X-ray and computed tomography (CT) images with a pattern that is only moderately characteristic for the human eye Ng, 2020. COVID-19’s rate of transmission depends on our capacity to reliably identify infected patients with a low rate of false negatives. In addition, a low rate of false positives is required to avoid further increasing the burden on the healthcare system by unnecessarily exposing patients to quarantine if that is not required. Along with proper infection control, it is evident that timely detection of the disease would enable the implementation of all the supportive care required by patients affected by COVID-19.
In late January, a Chinese team published a paper detailing the clinical and paraclinical features of COVID-19. They reported that patients present abnormalities in chest CT images with most having bilateral involvement Huang 2020. Bilateral multiple lobular and subsegmental areas of consolidation constitute the typical findings in chest CT images of intensive care unit (ICU) patients on admission Huang 2020. In comparison, non-ICU patients show bilateral ground-glass opacity and subsegmental areas of consolidation in their chest CT images Huang 2020. In these patients, later chest CT images display bilateral ground-glass opacity with resolved consolidation Huang 2020.
COVID is possibly better diagnosed using radiological imaging Fang, 2020 and Ai 2020.
Motivation
While PCR tests offer many advantages they are physical things that require shipping the test or the sample. X-ray machines can be plugged in to screen patients as long as they have electricity.
Imagine a future where we run out of tests and then the majority of radiologists get sick. AI tools can help general practitioners to triage and treat patients.
Companies are developing AI tools and deploying them at hospitals Wired 2020. We should have an open database to develop free tools that will also provide assistance.
Context
The authors are building a database of COVID-19 cases with chest X-ray or CT images. They are looking for COVID-19 cases as well as MERS, SARS, and ARDS.
All images and data will be released publicly in this GitHub repo. Currently authors are building the database with images from publications as those are images that are already available.
Content
This is a database of COVID-19 cases with chest X-ray or CT images. We are looking for COVID-19 cases as well as MERS, SARS, and ARDS.
All images and data will be released publicly in this GitHub repo (https://github.com/ieee8023/covid-chestxray-dataset). The author are building the database with images from publications as they are images that are already available.
Acknowledgements
Thanks to Joseph Paul Cohen. Postdoctoral Fellow, Mila, University of Montreal for making this dataset public for everyone.
Inspiration
The goal is to use these images to develop AI based approaches to predict and understand the infection."	153	4449	2	akorenvais	covid19-image-data-collection
4654	4654	Ex - Employees Data		[]		1	8	0	divyasuperhero	ex-employees-data
4655	4655	Melanoma	Melanoma--7p-dataset	['cancer']		0	7	0	saschamet	melanoma-7p
4656	4656	INDIAN_AIR_QUALITY_DATASET	MULTICLASSIFICATION PROBLEM DATASET	[]		2	10	1	anjalisharma123	indian-air-quality-dataset
4657	4657	feefff		[]		0	4	0	wujunwww	feefff
4658	4658	dddddd		[]		0	5	0	hamzakhtatnah	dddddd
4659	4659	plantpathology_21_classes		[]		13	17	0	laxmikantnishad	plantpathology-21-classes
4660	4660	IMDB Top 250 Lists (1996 - 2021)	Internet Movie Database Top 250 movies lists, from 1996 to present	['popular culture', 'movies and tv shows', 'internet', 'data visualization', 'tabular data']	"Context
IMDB (Internet Movie Database) is one of the most popular web sites, or databases, about movies, TV shows and similar.
IMDB's Top 250 lists also important feature for considering good movies. Rankings are calculated with users' votes. For more
IMDB's pollmaster account  shares previous years IMDB Top 250 lists. Top 250 lists changes all the time, so that the lists are created for December 31st, midnight PST of that year.
Content
This dataset contains IMDB Top 250 lists from 1996 to 2020 with every movie's basic information; release year, ranking, score, stars, etc.
Acknowledgements
This data scraped from IMDB, and you can reach scraping part from here
Inspiration
Time travel... You can look into lists for last 25 years. Analyze best movies for voters, genre preferences, most successful directors, stars, ranking changings over time et cetera. There are lots of things to do. Be creative and visualize them."	586	2834	13	mustafacicek	imdb-top-250-lists-1996-2020
4661	4661	soildataset		[]		15	16	1	meghaljambhale	soildataset
4662	4662	Restaurant Promotion Spam Detection	\	['business']		1	15	0	ayansengupta17	restaurant-promotion-spam-detection
4663	4663	ubiquant raw		['food']		1	44	5	abaojiang	ubiquant-raw
4664	4664	Blender Dataset	Loops, Abstracts, Characters, Environments, Renders	['arts and entertainment', 'art', 'games', 'environment', 'beginner']	"Blender is an open-source 3D creation suite that supports the 3D pipeline, modeling, rigging, animation, simulation, rendering, compositing, and motion tracking, video editing, and game creation.
Find Game assets, loops, environment materials, and shaders created using the open-source blender ❤ toolkit."	14	589	5	victorsullivan	blender-dataset
4665	4665	Psychotherapy	including client and therapist datasets	[]		4	25	0	nasim77	psychotherapy
4666	4666	Manavgat Tourist Attractions Geo		[]		3	13	0	kursatguzel	manavgat-tourist-attractions-geo
4667	4667	Playlist		['arts and entertainment']		0	4	0	naokimatsuno	playlist
4668	4668	Netflix Top 10 Weekly Dataset	Netflix Top 10 Movies and TV Dataset updated Weekly	['movies and tv shows', 'beginner', 'tabular data']	"Context
OTT platforms are growing in the last few years. Netflix is one of the top OTT platforms with maximum subsriber and viewership. Netflix has released Top 10 Movies and TV across weeks where we can analyze the viewership and movie content.
Content
The data is present in the excel sheets and it was directly downloaded from the website and will be updated on weekly basis.
We have two files in the dataset.
1) All Weeks Global
Global Top 10 viewership counts across the weeks.
2) All Weeks Countries
Per Countrywise Top 10 List of Movies and TV
Acknowledgements
Last week Netflix has started publishing its data to the public domain. The data is available on https://top10.netflix.com/
Inspiration
What are the viewership distribution across top 10 movies and TV and change on the weekly basis?
We can find which countries have similar viewership?"	1376	8379	30	mikitkanakia	netflix-top-10-weekly-dataset
4669	4669	Face_Mask_Detection_annotation.csv		[]		0	2	0	light367	face-mask-detection-annotationcsv
4670	4670	Waymo-Open Validation Dataset 2		[]		1	9	0	mohammedosama	waymo-open-validation-dataset-2
4671	4671	 Ubiquant train.feather 32 bit	Ubiquant Market Prediction data converted to feather format with 32 bit columns	[]		26	230	9	slawekbiel	ubiquant-trainfeather-32-bit
4672	4672	us-statewise-covid data	This data set contains united states-statewise covid dataset. 	['united states', 'business', 'beginner', 'intermediate', 'text data', 'covid19']	"About the data
This is a covid19 data set from United States. It includes date, Number of cases, Number of deaths. The other countries data are also available in my Kaggle and Github profile. The links are provided below
- Github
- Kaggle
If you want to read more about the data 
Click here"	54	324	1	dhamur	usstatewisecovid-data
4673	4673	itd_loss		[]		0	1	0	kookheejin	itd-loss
4674	4674	Handwritten Dzongkha Digits		[]		0	28	0	yjvision	handwritten-dzongkha-digits
4675	4675	PCA-model		['clothing and accessories']		0	12	0	puppakcheera	pcamodel
4676	4676	dataset12		[]		0	6	0	ekpreetsandhu	dataset12
4677	4677	data89		[]		0	7	0	ekpreetsandhu	data89
4678	4678	IT job vacancies and requirements	IT job data and requirements dataset	['computer science', 'programming']	"The data
This dataset contains data about jobs taken from 
https://jobs.prathidhwani.org/jobs    on Jan 20, 2021
The selenium package in python is used to extract data from the website"	187	1320	6	prabinraj	it-job-vacancies-and-requirements
4679	4679	GithubURL	Testing of of GithubURL 	['python']		3	470	1	vijayakishoredusi	githuburl
4680	4680	2nd-indi-ver2		[]		0	8	0	gabrieltangzy	2ndindiver2
4681	4681	ISIC2017 512*512		[]		4	23	0	hossamelkady	isic2017-512512
4682	4682	COVID-19 India dataset	This dataset contains everyday data and cummilatative data.	['india', 'online communities', 'covid19']	"This data set contains the data of covid-19 Conformed, Recovered and Deaths in India. This data is took from the non-governmental organization.
Website
<a href=""https://covid-19-india-ae23d.firebaseapp.com/"">COVID-19 Daily updates</a>
My Github
Profile
Data Set
Data collected from
COVID19-India - The data from 31-Jan-2020 to 31-Oct-2021.
Remaining data from"	84	677	7	dhamur	covid19-india-dataset
4683	4683	DataBISINDO		[]		1	7	0	adifahmi	databisindo
4684	4684	mmcv-1_3_18_cp37	mmcv-1_3_18_cp37_is_all_you_need	[]		0	27	0	deeeeeeeplearning	mmcv1-3-18-cp37
4685	4685	dqwdqw		[]		0	0	0	zhangtianwei	dqwdqw
4686	4686	nasbench201-v1_1-096897_extracted_cifar10_valid		[]		0	20	0	tahsintariq	nasbench201-v1-1-096897-extracted-cifar10-valid
4687	4687	L3DAS22 Challenge	Machine Learning for 3D Audio Signal Processing	['signal processing', 'classification', 'deep learning', 'audio data', 'python']	"L3DAS22: MACHINE LEARNING FOR 3D AUDIO SIGNAL PROCESSING
This dataset supports the L3DAS22 IEEE ICASSP Gand Challenge. The challenge is supported by a Python API that facilitates the dataset download and preprocessing, the training and evaluation of the baseline models and the results submission.
Scope of the Challenge
The L3DAS22 Challenge aims at encouraging and fostering research on machine learning for 3D audio signal processing.
3D audio is gaining increasing interest in the machine learning community in recent years. The range of applications is incredibly wide, extending from virtual and real conferencing to autonomous driving, surveillance and many more. In these contexts, a fundamental procedure is to properly identify the nature of events present in a soundscape, their spatial position and eventually remove unwanted noises that can interfere with the useful signal. To this end, L3DAS22 Challenge presents two tasks: 3D Speech Enhancement and 3D Sound Event Localization and Detection, both relying on first-order Ambisonics recordings in reverberant office environments.
Each task involves 2 separate tracks: 1-mic and 2-mic recordings, respectively containing sounds acquired by one 1st order Ambisonics microphone and by an array of two ones. The use of two Ambisonics microphones represents one of the main novelties of the L3DAS22 Challenge. We expect higher accuracy/reconstruction quality when taking advantage of the dual spatial perspective of the two microphones. Moreover, we are very interested in identifying other possible advantages of this configuration over standard Ambisonics formats.
Interactive demos of our baseline models are available on Replicate.
Top 5 ranked teams can submit a regular paper according to the ICASSP guidelines.
Prizes will be awarded to the challenge winners thanks to the support of Kuaishou Technology.
Tasks
Tasks
The tasks we propose are:
  * 3D Speech Enhancement
The objective of this task is the enhancement of speech signals immersed in the spatial sound field of a reverberant office environment. Here the models are expected to extract the monophonic voice signal from the 3D mixture containing various background noises. The evaluation metric for this task is a combination of short-time objective intelligibility (STOI) and word error rate (WER).
  * 3D Sound Event Localization and Detection
The aim of this task is to detect the temporal activities of a known set of sound event classes and, in particular, to further locate them in the space. Here the models must predict a list of the active sound events and their respective location at regular intervals of 100 milliseconds. Performance on this task is evaluated according to the location-sensitive detection error, which joins the localization and detection error metrics.
Dataset Info
The L3DAS22 datasets contain multiple-source and multiple-perspective B-format Ambisonics audio recordings. We sampled the acoustic field of a large office room, placing two first-order Ambisonics microphones in the center of the room and moving a speaker reproducing the analytic signal in 252 fixed spatial positions. Relying on the collected Ambisonics impulse responses (IRs), we augmented existing clean monophonic datasets to obtain synthetic tridimensional sound sources by convolving the original sounds with our IRs. We extracted speech signals from the Librispeech dataset and office-like background noises from the FSD50K dataset. We aimed at creating plausible and variegate 3D scenarios to reflect possible real-life situations in which sound and disparate types of background noises coexist in the same 3D reverberant environment. We provide normalized raw waveforms as predictors data and the target data varies according to the task.
The dataset is divided in two main sections, respectively dedicated to the challenge tasks.
The first section is optimized for 3D Speech Enhancement and contains more than 60000 virtual 3D audio environments with a duration up to 12 seconds. In each sample, a spoken voice is always present alongside with other office-like background noises. As target data for this section we provide the clean monophonic voice signals. For each subset we also provide a csv file, where we annotated the coordinates and spatial distance of the IR convolved with the target voice signals for each datapoint. This may be useful to estimate the delay caused by the virtual time-of-flight of the target voice signal and to perform a sample-level alignment of the input and ground truth signals.
The other sections, instead, is dedicated to the 3D Sound Event Localization and Detection task and contains 900 30-seconds-long audio files. Each data point contains a simulated 3D office audio environment in which up to 3 simultaneous acoustic events may be active at the same time. In this section, the samples are not forced to contain a spoken voice. As target data for this section we provide a list of the onset and offset time stamps, the typology class, and the spatial coordinates of each individual sound event present in the data-points.
We split both dataset sections into a training set and a development set, paying attention to create similar distributions. The train set of the SE section is divided in two partitions: train360 and train100, and contain speech samples extracted from the correspondent partitions of Librispeech (only the sample) up to 12 seconds. The train360 is split in 2 zip files for a more convenient download. All sets of the SELD section are divided in: OV1, OV2, OV3. These partitions refer to the maximum amount of possible overlapping sounds, which are 1, 2 or 3, respectively.
L3DAS22 Challenge Supporting API
The gitHub supporting API is aimed at downloading the dataset, pre-processing the sound files and the metadata, training and evaluating the baseline models and validating the final results. We provide easy-to-use instruction to produce the results included in our paper. Moreover, we extensively commented our code for easy customization.
For further information please refer to the challenge website and to the challenge documentation."	433	1870	6	l3dasteam	l3das22
4688	4688	nas_bench_201_info_cifar100		[]		2	9	0	tahsintariq	nas-bench-201-info-cifar100
4689	4689	nas_bench_201_info_ImageNet16-120		[]		0	13	0	tahsintariq	nas-bench-201-info-imagenet16120
4690	4690	nas_bench_201_info_cifar10		[]		0	12	0	tahsintariq	nas-bench-201-info-cifar10
4691	4691	DC Comic Books Dataset	A complete collection of all dc comicbooks	['arts and entertainment', 'beginner', 'recommender systems', 'tabular data', 'comics and animation']	"Context
The dataset contains info on all the comic books ever released in the DC Universe.
Content
DC Comics is one of the largest and oldest American comic book companies, with their first comic under the DC banner being published in 1937. The majority of its publications take place within the fictional DC Universe and feature numerous culturally iconic heroic characters, such as Superman, Batman, and Wonder Woman. It is widely known for some of the most famous and recognizable teams including the Justice League, the Justice Society of America, and the Teen Titans. 
Acknowledgements
DC Fandom Website
Inspiration
This is my second submission to the comic book series. if you like this check out marvel comic books dataset"	125	1181	20	deepcontractor	dc-comic-books-dataset
4692	4692	dataset		[]		0	5	0	sumitkumarjethani	dataset
4693	4693	my-data1		['software']		0	4	0	ekpreetsandhu	mydata1
4694	4694	En-Ta-Dataset		[]		0	13	0	skoushik	entadataset
4695	4695	Happy and Sad Emotion Classification		['social networks']		3	22	0	chadgarratt	happy-and-sad-emotion-classification
4696	4696	2nd_indi		[]		0	2	0	gabrieltangzy	2nd-indi
4697	4697	CO2_Emission_cause_prediction_data	You can predict the cause and amount of Co2 emissions 	[]		2	23	0	atreyakatnam	co2-emission-prediction-data
4698	4698	Air Pollution Forecasting - LSTM Multivariate	Lstm multivariate sample dataset for architecture design and orchestration 	['earth and nature', 'computer science', 'time series analysis', 'lstm', 'datetime']	"THE MISSION
The story behind the dataset is how to apply LSTM architecture to understand and apply multiple variables together to contribute more accuracy towards forecasting.
THE CONTENT
Air Pollution Forecasting
The Air Quality dataset.
This is a dataset that reports on the weather and the level of pollution each hour for five years at the US embassy in Beijing, China.
The data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete feature list in the raw data is as follows:
No: row number
year: year of data in this row
month: month of data in this row
day: day of data in this row
hour: hour of data in this row
pm2.5: PM2.5 concentration
DEWP: Dew Point
TEMP: Temperature
PRES: Pressure
cbwd: Combined wind direction
Iws: Cumulated wind speed
Is: Cumulated hours of snow
Ir: Cumulated hours of rain
We can use this data and frame a forecasting problem where, given the weather conditions and pollution for prior hours, we forecast the pollution at the next hour."	819	5592	22	rupakroy	lstm-datasets-multivariate-univariate
4699	4699	2nd_indi_noiseVer		[]		0	6	0	gabrieltangzy	2nd-indi-noisever
4700	4700	Indian-God-Names	Indian god names with Sanskrit meanings	['culture and humanities', 'india', 'religion and belief systems']	"Idea
The idea for making this dataset is came to me when I was searching project for submitting on jovian.ml platform as a part of their task. (This is very good site for beginners who want to learn data science python skills, they arranged course in collaboration with freecodecamp). I thought to make unique project for that I need my own dataset. That's why I created this dataset.
What's Inside
Contains God names with meaning in Sanskrit , translated in English for better understanding. I collected this data from different scriptures & Sanskrit literatures. More will be added soon.
Acknowledgements
Thanks to my school Sanskrit teacher Mr. V. B. Patil Sir, which introduced us to this language."	63	1000	4	avadhootk	names-df
4701	4701	Writing tips		[]		1	19	0	theresahutchinson	writing-tips
4702	4702	cbnet_custom	cbnet_custom_aug_is_all_you_need	[]		1	16	0	deeeeeeeplearning	cbnet-custom
4703	4703	retrain_model_0120		[]		0	6	0	zengkuikui	retrain-model-0120
4704	4704	Finance dataset		[]		0	11	0	okohlawrence	finance-dataset
4705	4705	Chessboard Pictures for Stereocamera Calibration	i.e. for a camera calibration with OpenCV	['arts and entertainment', 'earth and nature', 'computer vision']	"Stereocamera Chessboard Dataset
Within this Dataset you find 40 pictures of the same checkerboard pattern, produced with two equal cameras. (so you have 20 pictures per camera). The Data is divided into two folders leftcamera for the left camera and rightcamera for the right one. The images are numbered. Image 1 of the left camera (Im_L_1.png) therefore belongs to image 1 of the right camera (Im_R_1.png). They show the same scenario from different points of view.
With this Dataset you are able to get started with the OpenCV Calibration process and warm up with the parameters and techniques. You can look at the geometrical dependencies of the Stereocamera, learn something about rectification, epipolars, intrinsic, extrinsic, distortion, etc.
To get started you have to know a view things about the given data. The square size of the checkerboard is 30mm. The board size is 11x7 (counted by the corners of the squares)."	5	60	0	danielwe14	stereocamera-chessboard-pictures
4706	4706	mmcv_full		[]		0	2	0	riadalmadani	mmcv-full
4707	4707	CelebA_Smaller_20k		[]		0	2	0	paulrohan2020	celeba-smaller-20k
4708	4708	Real and fake news		['news']		3	52	0	nikhilghase	real-and-fake-news
4709	4709	Valiya Babji		[]		0	9	0	skvaliyababji	valiya-babji
4710	4710	Divvy_2019	The full year for Cyclistic 	['business', 'beginner', 'data cleaning', 'data visualization', 'statistical analysis', 'data analytics']	"Overview
This dataset is based on a bike renting company called Cyclistic which takes place in Chicago , their director of marketing  believes the company’s future success depends on maximizing the number of annual memberships . Therefore, Our team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights, Our team will design a new marketing strategy to convert casual riders into annual members
Content
the data after further inspections shows a bit of unexplained data where some rides too months which didn't make sense so we added a criteria of 120 mins maximum rides so we get a real understanding of the data , our goal is to clean , organize , filter & sort so we can make observations on the data , i acquired the data from : https://ride.divvybikes.com/system-data ,which is an open source data with the license agreement :https://ride.divvybikes.com/data-license-agreement , this data set takes place in 2019 whole year . 
Acknowledgements
can't disregard the help of the people around me makes it much easier to have other insights regarding the points of view and other ways to tackle the problem on hand ."	5	88	0	moazmohamedhassan	divvy-2019
4711	4711	torchtext_0_11_0		[]		0	0	0	riadalmadani	torchtext-0-11-0
4712	4712	German Sign Language (DGS) Alphabet	A dataset containing hand landmark vectors of all static DGS alphabet signs	['languages', 'categorical data', 'education', 'linguistics', 'computer science']	"Context
This dataset was created to train a neural network for real time sign detection, which would be used as automated feedback for a learning application.
The dataset ist based on the normalized hand landmark vectors provided by mediapipe's handpose in order to make the trained NN invariant to lighting situations or skin colors, which could not be represented in a diverse enough fashion in the dataset.
The dataset is therefore designed to train a NN which categorizes the MULTI_HAND_LANDMARK output of the handpose solution.
Content
The dataset contains 64 columns with the first column being the sample's label. All static signs (meaning signs not involving movement) of the German Sign language alphabet are represented as 24 classes ('a'-'y', excluding 'j').
All other columns represent the  21 linearized, three-dimensional hand landmarks provided by handpose in their normalized ([0.0, 1.0]) state.
In total the dataset contains ca. 7300 samples with at least 250 samples per class, recorded by 7 different non-native signers.
The dataset is purely made up of recorded samples and does not make use of data augmentation.
Acknowledgements
This dataset was inspired by the desire to create a German version of the Sign Language MNIST dataset with a stronger focus on practical applicability.
Inspiration
Our team is interested in providing a foundation for all kinds of practical applications involving sign language recognition. As with our own work, we appreciate a focus on applications challenging non-signers to engage with sign language in a way that promotes inclusion.
Ethical considerations
We are aware of the ethical implications of such a dataset and encourage developers to seriously consider research on the ethics of machine learning and sign language to avoid harmful outcomes of well intended projects. For more information on this topic we recommend Bragg, D., Caselli, N., Hochgesang, J. A., Huenerfauth, M., Katz-Hernandez, L., Koller, O., Kushalnagar, R., Vogler, C., & Ladner, R. E. (2021). The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective. In ACM Transactions on Accessible Computing (14th ed., Vol. 2, pp. 1-45). Association for Computing Machinery. 10.1145/3436996 as a starting point."	4	80	0	moritzkronberger	german-sign-language
4713	4713	 (COVID-19 variant)	Omicron daily cases by country	[]		8	74	0	sandy117	covid19-data
4714	4714	opencv_python_headless		[]		0	0	0	riadalmadani	opencv-python-headless
4715	4715	ubiquant_simplemlp_5folds		[]		3	30	0	hengzheng	ubiquant-simplemlp-5folds
4716	4716	icedata		[]		0	2	0	riadalmadani	icedata
4717	4717	torchvision_0_11_1_cu111		[]		0	0	0	riadalmadani	torchvision-0-11-1-cu111
4718	4718	30days_folds		[]		0	1	0	danishkumar12	30days-folds
4719	4719	torch_1_10_0_cu111		[]		0	0	0	riadalmadani	torch-1-10-0-cu111
4720	4720	Flipkart E-commerce Dataset	20000 Rows Data for Product Category on Flipkart	['india', 'clothing and accessories', 'intermediate', 'classification', 'e-commerce services']	"About Dataset
This is an E-commerce Flipkart Dataset with exactly 20,000 samples. It has 15 columns with a lot of information. You can use is to predict which category it might fall under, considering “Description” for a product, analyze it."	317	2345	13	atharvjairath	flipkart-ecommerce-dataset
4721	4721	ARXIV Articles: Titles & Abstracts.	ARXIV articles about ML, Stats, Nonlinear Modelling & Neurobiology.	['earth and nature', 'education']	"ARXIV articles metadata (most importantly titles and abstracts).
Shout out to Mahdi Sadjadi (2017). arxivscraper: Zenodo. http://doi.org/10.5281/zenodo.889853 for the Arxiv scraper package."	0	11	3	mahmoudlimam	arxiv-articles
4722	4722	Car Prices Poland	Predict sale prices for cars in Poland 	['europe', 'computer science', 'programming', 'automobiles and vehicles', 'deep learning', 'tabular data', 'regression']	"Hello
I am a Python developer and train the ability to parse, analyze data and create neural networks. I decided to share the dataset I'm currently working with. Maybe it will be useful to someone. Why a car? I love cars! As well as programming. If you are interested in me as a specialist, please contact me glotoffalexandr@gmail.com (I'm looking for a new job)
The dataset was assembled in January 2022. Data from a well-known car sale site in Poland (which is public). Selenium and request were used for parsing (python of course)
The dataset contains information about the make, model, generation, year of production, mileage, engine type and volume, localization and price
I have ideas for expanding the model: add body type, configuration, color, power, etc.
I see many ways to use models created on the basis of this dataset, I will describe them in notebooks"	1061	5543	30	aleksandrglotov	car-prices-poland
4723	4723	Spotify Dataset With Genres		[]		16	101	1	shivamshrivastava21	spotify-dataset-with-genres
4724	4724	goodCroppedSortedProditecDatasetTeam6		[]		1	11	0	carpiliengpatbaul	goodcroppedsortedproditecdatasetteam6
4725	4725	mangal		[]		0	1	0	cvprneeraj	mangal
4726	4726	Deep Noise Suppression Challenge Interspeech 2020		['earth and nature']		3	26	0	zenbot99	deep-noise-suppression-challenge-interspeech-2020
4727	4727	Cyclistic 2021 Data	Cyclistic (Chicago's Divvy) trip data from January 2021 to December 2021	['cycling', 'data visualization', 'data analytics', 'text data', 'tidyverse']	"Context
This data set is a part of the Google Data Analytics Certificate Program capstone project.
Cyclistic is a fictional company 'created' for this project for students to use. The data itself is real and is of a company called Divvy which operates a bike-sharing service in Chicago, IL.
Content
The data has been made available by Motivate International Inc. under this license. This is public data that you can use to explore how different customer types are using Cyclistic bikes.
The data includes the ride IDs, start and end station names and their respective IDs and latitude and longitude coordinates, timestamps of the beginning and end of every ride, and whether a rider is a casual rider (using pay-per-ride or full-day pass) or a member (paying for an annual subscription)."	4	77	0	elinuss	cyclistic-2021-data
4728	4728	data-science-london		['earth and nature']		1	7	0	kdijfwr	datasciencelondon
4729	4729	yolov5-data-v1		[]		0	17	0	jialuuuuuu	yolov5datav1
4730	4730	Surface match dataset	Based on 29k generated images which provide 21.7M pairs in summary	['computer vision', 'online communities']	"Detail description in the post ""Neural networks in the photogrammetry"" [RU ver.]
Context
A photogrammetry workflow require detecting similar images for later calculatating as a part of whole process.
This dataset created to solve this problem. It contains pairs of images and a value in range 0..1 which shows how many common surfaces their have.
Content
Dataset contains jpeg images and json files (in zip) with describing pairs of images and value in range 0..1."	18	970	1	olegpostoev	surface-match-dataset
4731	4731	Urdu Language Speech Dataset	Urdu Language Speech Emotional Corpus from GitHub	['languages', 'text data']	"Context
URDU dataset contains emotional utterances of Urdu speech gathered from Urdu talk shows. It contains 400 utterances of four basic emotions: Angry, Happy, Neutral, and Emotion. There are 38 speakers (27 male and 11 female).
Content
This data is collected from Youtube content. Speakers are selected randomly. Nomenclature followed while naming the files in the dataset is to provide information about the speaker, gender, number of the file for that speaker and overall numbering of the file in a particular emotion. Files are named as follows:
General Name: SGX_FXX_EYY
The name can be divided into three segments which are segregated by underscore(_) sign.
Where,
In SGX, G indicates the gender of the speaker either it can be M for male speaker and F for female speaker, while X represents the speaker ID which remains the same for a particular speaker in all the emotions.
In FXX, F is a keyword presenting file and XX indicates the number of files for a particular speaker.
In EYY, E provides the information about emotion i.e., A, H, N and S for Angry, Happy, Neutral and Sad. respectively.
For example, file name SM1_F01_A12 indicates that this is 1st file recorded by speaker No. 1 and A12 indicates that this is the 12th file of angry emotion.
Acknowledgements
For more details about the dataset, please refer to the complete paper ""Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages"". https://arxiv.org/pdf/1812.10411.pdf"	300	4912	23	bitlord	urdu-language-speech-dataset
4732	4732	flickr8k		[]		0	7	2	nitishroat	flickr8k
4733	4733	useCaseProditecCroppedSortedTeam6		[]		1	15	0	carpiliengpatbaul	usecaseproditeccroppedsortedteam6
4734	4734	iris-dataset		[]		0	1	0	mohamedgamal1	irisdataset
4735	4735	dataset123		[]		0	0	0	zhangtianwei	dataset123
4736	4736	Sentiment_dataset_downsampling		[]		4	11	0	kengofujii	sentiment-dataset-downsampling
4737	4737	NBA Hall of Famers as for 2021	NBA all-star selections as for 2021	['basketball']	The NBA has selected players for the All-Star Game. The intent was to combine this dataset and find the probability of getting into the NBA HOF based on various criteria such as position, number of all-star appearances by a player, and others.	4	34	0	simplyab	nba-hall-of-famers-as-for-2021
4738	4738	Indian Companies		[]		1	15	1	arjunachu	indian-companies
4739	4739	taxi_data		['education']	"Context
Just for a school project.
Content
Taxi-driving data"	1	6	0	huisit	taxi-data
4740	4740	gpt2-largw		[]		1	12	0	vanle73	gpt2largw
4741	4741	Annual reports		[]		2	6	0	samarthagarwal23	annual-reports
4742	4742	Covid measures timeline	A list of measures taken by all governments and the timestamps these were active	['public health', 'government', 'public safety', 'covid19']		0	9	0	leonardojulcamargo	covid-measures-timeline
4743	4743	barrier_reef		[]		0	8	0	ageage	barrier-reef
4744	4744	libiray		[]		0	0	0	zhangtianwei	libiray
4745	4745	Body Parts Dataset	128x128 images of human body parts from Google Images	['biology', 'computer vision', 'classification', 'image data']		9	134	1	linkanjarad	body-parts-dataset
4746	4746	fastai	https://github.com/fastai/fastai	[]		0	1480	1	manyregression	fastai
4747	4747	Cholera Outbreak	Cholera outbreak in 1983	[]		5	44	7	brijlaldhankour	cholera-outbreak
4748	4748	weightfile		[]		0	12	0	phuepyaemaung	weightfile
4749	4749	skindisease		[]		0	15	0	mdsiyamulislam	skindisease
4750	4750	d8_train02_yoloxl_9_15_20epoch		[]		0	8	0	sakura1986	d8-train02-yoloxl-9-15-20epoch
4751	4751	d8_train04_yoloxl_Adam		[]		1	16	0	sakura1986	d8-train04-yoloxl-adam
4752	4752	myimage		[]		0	3	0	homesky	myimage
4753	4753	Criminal incidents in Brazil	Criminal incidents divided by municipality, region and date	['brazil', 'crime']		13	106	4	henriqueliberato	criminal-incidents-by-municipality-brazil
4754	4754	Song popularity		['music']		1	58	1	chandanhariharan	song-popularity
4755	4755	xmllabel0047		[]		0	8	0	vainglorylx	xmllabel0047
4756	4756	PAIDataset		[]		0	2	0	tanqiwen	paidataset
4757	4757	Temperature Uniformity/Chamber Mapping	Calibration and Validation of Laboratory Equipment	['earth and nature', 'science and technology', 'biotechnology', 'time series analysis', 'statistical analysis', 'linear regression']	"Context
Here are several real-life examples of temperature mapping/uniformity testing of various laboratory equipment.
Temperature mapping is a common Quality Control(QC) activity performed for the verification and/or validation and calibration of equipment when temperature is of critical importance to the process.
Content
Each file containers columns for the time stamp and loggers separately numbered 1-5. Temperatures are in Celsius and logging frequency was typically set to 1 minute.
Freezers, Fridges, and Incubators are mapped in 3-D with approximate Logger Placement (In order of #):
Top-Left-Back (Corner)
Top-Right-Front (Corner)
Middle (Approximate center of the chamber)
Bottom-Left-Front (Corner)
Bottom-Right-Back (Corner)
Waterbaths are mapped in 2-D with the equivalents above. I will see about adding 2-D heating block and thermal cycler mappings in the future.
Let me know if you have any questions!"	2	28	0	anthonykirby	temperature-uniformitychamber-mapping
4758	4758	Cryptocurrency extra data - Dogecoin	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	596	3870	18	yamqwe	cryptocurrency-extra-data-dogecoin
4759	4759	Urdu Word Vectors	🎡Urdu NLP Word Embeddings Collection	['earth and nature']	"Urdu Word Vector Models
Following vector models are available as of now.
Word2Vec
Web News Dataset (linguistic variation 1)
We trained this on huge News dataset. The model knows 64653 different Urdu words.
Linguistic Preprocessing: Split into sentences, Word tokenization
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""min_count"": 50, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 5}
Model Performance:
Google Analogy: 48%
Download (78 MB)
Web News Dataset (linguistic variation 2)
We trained this on huge News dataset. The model knows 64315 different Urdu words.
Linguistic Preprocessing: Split into sentences, Word tokenization, Stop word removed
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""min_count"": 50, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 5}
Model Performance:
Google Analogy: 52%
Download (70 MB)
Wikipedia Dataset (linguistic variation 1)
We trained this model on whole Wikipedia dataset. The model knows 17794 different Urdu words.
Linguistic Preprocessing: Split into sentences, Word tokenization
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""min_count"": 50, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 10}
Model Performance:
Google Analogy: 62%
Download (21 MB)
Wikipedia Dataset (linguistic variation 2)
We trained this model on whole wikipedia dataset. The model knows 17465 different Urdu words.
Linguistic Preprocessing: Split into sentences, Word tokenization, Stop word removed
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""min_count"": 50, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 10} 
Model Performance:
Google Analogy: 62%
Download (21 MB)
Fasttext
Web News Dataset (linguistic variation 1)
We trained this on huge News dataset.
Linguistic Preprocessing: Split into sentences, Word tokenization
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""max_n"": 5, ""min_count"": 50, ""min_n"": 2, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 5, ""word_ngrams"": 1}
Model Performance:
Google Analogy: 44%
Download (437 MB)
Web News Dataset (linguistic variation 2)
We trained this on huge News dataset.
Linguistic Preprocessing: Split into sentences, Word tokenization, Stop word removed
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""max_n"": 5, ""min_count"": 50, ""min_n"": 2, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 5, ""word_ngrams"": 1}
Model Performance:
Google Analogy: 50%
Download (436 MB)
Wikipedia Dataset (linguistic variation 1)
We trained this model on whole Wikipedia dataset.
Linguistic Preprocessing: Split into sentences, Word tokenization
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""max_n"": 5, ""min_count"": 50, ""min_n"": 2, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 10, ""word_ngrams"": 1}
Model Performance:
Google Analogy: 59%
Download (166 MB)
Wikipedia Dataset (linguistic variation 2)
We trained this model on whole wikipedia dataset.
Linguistic Preprocessing: Split into sentences, Word tokenization, Stop word removed
Parameters: {""alpha"": 0.05, ""hs"": 0, ""iter"": 15, ""max_n"": 5, ""min_count"": 50, ""min_n"": 2, ""negative"": 20, ""sample"": 0.0001, ""sg"": 1, ""size"": 300, ""window"": 10, ""word_ngrams"": 1} 
Model Performance:
Google Analogy: 58%
Download (163 MB)"	26	1787	2	akkefa	urdu-word-vectors
4760	4760	data123		[]		0	13	0	ekpreetsandhu	data123
4761	4761	subway_datasets		[]		2	21	0	kogeonwoo	subway-datasets
4762	4762	English-Korean Multitarget Ted Talks Task (MTTT)	Parallel English-Korean Corpus	['nlp']	"English-Korean Parallel Corpus provided by: https://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/
Corpus contains 166k train / 2k val / 2k test English-Korean sentence pairs.
Sentences were first transcribed into English from various Ted Talks and then translated into Korean by TED translators."	1	8	0	msarmi9	englishkorean-multitarget-ted-talks-task-mttt
4763	4763	vqgan-imagenet-f16-16384-mindall-e		['computer science']		3	12	0	annas1301	vqgan-imagenet-f16-16384-mindall-e
4764	4764	Cryptocurrency extra data - Maker	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	428	2479	8	yamqwe	cryptocurrency-extra-data-maker
4765	4765	g2net_blocks		[]		0	14	0	ml1liu	g2net-blocks
4766	4766	sahi-0.8.22		[]		3	11	0	zzy990106	sahi0822
4767	4767	Tmdb Top Rated Movies		['movies and tv shows']		0	6	0	tanmayshinde2002	tmdb-top-rated-movies
4768	4768	Ultralytics YOLOv5 Git Repository (Monthly)	Dump of the Ultralytics' YOLOv5 Git repository (GPLv3, updates monthly)	['computer science', 'computer vision', 'transfer learning', 'torchvision']	"This is a dump of the YOLOv5 Git repository, updated by Kaggle monthly. You may use this to submit kernels utilizing YOLOv5 without Internet access. All required dependencies are already installed in the default virtualenv, so no extra packages are needed. The model alone is not very useful without pretrained weights, which are also available as a Kaggle dataset.
Example usage:
ipynb
%cd /kaggle/working
!cp -r /kaggle/input/ultralyticsyolov5a yolov5
This repository is licensed under the General Public License, Version 3 (GPLv3)."	9	407	4	dragoonaethis	ultralyticsyolov5a
4769	4769	USDA Rural Development Resale Properties 	Single family homes and ranches for sale by the U.S. Federal Government.	[]		19	2070	2	marchman	usda-rural-development-resale-properties
4770	4770	freeCodeCamp - New Coder Surveys		['computer science']		17	880	0	anirudhvaranasi	freecodecamp-new-coder-surveys
4771	4771	xgraphinns2		[]		0	7	0	shanahussain	xgraphinns2
4772	4772	Cryptocurrency extra data - IOTA	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	437	2357	12	yamqwe	cryptocurrency-extra-data-iota
4773	4773	ascdfv		[]		0	2	0	panchaljay	ascdfv
4774	4774	githubcovid	live updated data in github for coiv in india	[]		2	1317	0	adiravin	githubcovid
4775	4775	bert ja spm	Scripts of BERT with SentencePiece for Japanese text	[]	"Scripts of BERT with SentencePiece for Japanese text
https://github.com/yoheikikuta/bert-japanese
The pretrained model is here."	3	1224	0	vochicong	bert-ja-spm
4776	4776	VADER Sentiment	Dataset and code From GitHub	[]		41	2276	1	mrgodsay	vader-sentiment
4777	4777	Data Versioning	Sentiment Lexicon from GitHub training for data pipleines	[]		4	1605	0	mehdi16	data-versioning
4778	4778	Cryptocurrency extra data - Cardano	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	493	2712	13	yamqwe	cryptocurrency-extra-data-cardano
4779	4779	synthesis original path		['earth and nature']		0	10	0	shashwatnaidu07	synthesis-original-path
4780	4780	audio 2 lip		[]		1	7	0	xiahuadong1981	audio-2-lip
4781	4781	vg-sales-number1		['news']		0	12	0	connorboyce	vgsalesnumber1
4782	4782	Bitcoin Historical Price Dataset	Bitcoin Prices in USD from Sept 2014 to Jan 2022	['beginner', 'intermediate', 'data visualization', 'tabular data', 'currencies and foreign exchange']	This dataset contains the bitcoin price values in USD from Sept 2014 to Jan 2022.	47	287	4	oddasparagus11	bitcoin-historical-price-dataset
4783	4783	Aksara Jawa / Hanacaraka	AksaraJawa/Hanacaraka datasets. Digital handwritten of Hanacaraka.	['popular culture', 'cnn']	"Context
Kumpulan tulisan tangan hanacaraka yang dibuat secara digital oleh berbagai manusia yang bersliweran di Internet. Tulisan tangan dibuat secara digital dengan memanfaatkan sebuah papan digital yang dikirimkan melalui kuesioner online. Tidak hanya itu juga, ada beberapa yang dibuat secara manual dan dipindai secara digital.
Content
Apa aja ? berisi sekitar 500+ tulisan tangan digital dalam format JPG. Setiap huruf pada aksara jawa hanacaraka berisi sekitar 75+ yang memiliki resolusi beragam tetapi dibawah 500x500 pixel.
Contributor
Kami ucapkan terimakasih kepada seluruh partisipan dalam proyek ini! Love you all!
Kami ucapkan terima kasih sebesar-besarnya kepada:
Aditya Bayu Perdana ( @by.abay ) sebagai kontributor pertama dalam manual handwritting.
Warga SMA Negeri 1 Wonoayu | Mereka yang banyak membantu kami dalam mengumpulkan data
Sakura Fukoka Studio Data Branch | Hey, Kalian yang bantu benerin semuanya kan ? Iya kan heheh
dan kalian! Tanpa kalian semua, dataset ini tidak akan pernah terwujud!"	277	6225	33	vzrenggamani	hanacaraka
4784	4784	Russian_Oil_Refining_Research		[]		3	12	0	cyrillburov	russian-oil-refining-research
4785	4785	109 school		['primary and secondary schools']		0	13	0	lanewww	109-school
4786	4786	ubiquant_parquet_file	ubiquant dataset csv to parquet	['business']	Convert ubiquan dataset from csv file to parquet file.	4	107	2	wanngide	ubiquan-parquet-dataset
4787	4787	similar-loss		[]		0	2	0	kookheejin	similarloss
4788	4788	The Lick (External Examples, Non-strict)		['audio data']		1	26	0	andychamberlain	the-lick-external-examples-nonstrict
4789	4789	fastai_audio	https://github.com/mogwai/fastai_audio	[]		1	1324	1	manyregression	fastai-audio
4790	4790	resemblyzer Github repository	Resemblyzer Github repository is now available as a Kaggle Dataset!	['computer science']	"Resemblyzer repository
I added this repository to the Kaggle datasets since I found it very useful in the audio features extraction. All credits goes to the creator of this amazing package. For references please visit the repository in Github."	6	1503	0	mawanda	resemblyzer-github-repository
4791	4791	FaceDetectionDataset	Faces annotated from Labeled Faces in the Wild (LFW)	['art']		49	1821	0	sunilgo	facedetectiondataset
4792	4792	USEP and WEP electricity prices 2021-2022		['electricity']		1	13	0	symevelyn	usep-and-wep-electricity-prices-20212022
4793	4793	cities.csv		[]		1	15	0	davinnatanael	citiescsv
4794	4794	for_yolor		[]		0	4	0	yhoung	for-yolor
4795	4795	Cryptocurrency extra data - TRON	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	447	2514	13	yamqwe	cryptocurrency-extra-data-tron
4796	4796	Mushroom Classification Dataset	Automatic Mushroom Species Classification Dataset	['biology']		4	51	0	mustai	mushroom-12-9528
4797	4797	Ten Thousand German News Articles Dataset	10kGNAD for Topic Classification	['computer science', 'programming', 'nlp', 'classification', 'news']	"(see https://tblock.github.io/10kGNAD/ for the original dataset page)
This page introduces the 10k German News Articles Dataset (10kGNAD) german topic classification dataset. 
The 10kGNAD is based on the One Million Posts Corpus and avalaible under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You can download the dataset here.
Why a German dataset?
English text classification datasets are common.
Examples are the big AG News, the class-rich 20 Newsgroups and the large-scale DBpedia ontology datasets for topic classification and for example the commonly used IMDb and Yelp datasets for sentiment analysis.
Non-english datasets, especially German datasets, are less common.
There is a collection of sentiment analysis datasets assembled by the Interest Group on German Sentiment Analysis. 
However, to my knowlege, no german topic classification dataset is avaliable to the public.
Due to grammatical differences between the English and the German language, a classifyer might be effective on a English dataset, but not as effectiv on a German dataset.
The German language has a higher inflection and long compound words are quite common compared to the English language. 
One would need to evaluate a classifyer on multiple German datasets to get a sense of it's effectivness.
The dataset
The 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset.
It consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.
These articles are a till now unused part of the One Million Posts Corpus.
In the One Million Posts Corpus each article has a topic path. For example Newsroom/Wirtschaft/Wirtschaftpolitik/Finanzmaerkte/Griechenlandkrise.
The 10kGNAD uses the second part of the topic path, here Wirtschaft, as class label.
In result the dataset can be used for multi-class classification.
I created and used this dataset in my thesis to train and evaluate four text classifyers on the German language.
By publishing the dataset I hope to support the advancement of tools and models for the German language.
Additionally this dataset can be used as a benchmark dataset for german topic classification.
Numbers and statistics
As in most real-world datasets the class distribution of the 10kGNAD is not balanced.
The biggest class Web consists of 1678, while the smalles class Kultur contains only 539 articles.
However articles from the Web class have on average the fewest words, while artilces from the culture class have the second most words.
Splitting into train and test
I propose a stratifyed split of 10% for testing and the remaining articles for training.
To use the dataset as a benchmark dataset, please used the train.csv and test.csv files located in the project root.
Code
Python scripts to extract the articles and split them into a train- and a testset avaliable in the code directory of this project.
Make sure to install the requirements.
The original corpus.sqlite3 is required to extract the articles (download here (compressed) or here (uncompressed)).
License
This dataset is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
Please consider citing the authors of the One Million Post Corpus if you use the dataset."	696	9841	32	tblock	10kgnad
4798	4798	Cryptocurrency extra data - Monero	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	438	2379	10	yamqwe	cryptocurrency-extra-data-monero
4799	4799	80% MNIST		['computer science']		1	9	1	aymuos10	80-mnist
4800	4800	Cryptocurrency extra data - Litecoin	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	501	2463	11	yamqwe	cryptocurrency-extra-data-litecoin
4801	4801	Ubiquant_low_mem_data		[]		0	7	0	masuken1994	ubiquant-low-mem-data
4802	4802	retrain_models_1		[]		0	3	0	zengkuikui	retrain-models-1
4803	4803	retrain_models		[]		0	3	0	zengkuikui	retrain-models
4804	4804	retrain_model		[]		0	0	0	zengkuikui	retrain-model
4805	4805	Cryptocurrency extra data - Ethereum	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	800	5067	39	yamqwe	cryptocurrency-extra-data-ethereum
4806	4806	longformer-linear5ep-last-model-f0-cv-0.6224		[]		0	5	0	atharvaingle	longformer-linear5ep-last-model-f0
4807	4807	Cryptocurrency extra data - Ethereum Classic	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	458	2560	9	yamqwe	cryptocurrency-extra-data-ethereum-classic
4808	4808	movielens20M v2		[]		0	15	0	superrock	movielens20m-v2
4809	4809	Portuguese text generator Dataset	PT text generation Dataset with Desciclopedia text	[]		0	106	0	eduapps	pt-text-generation-dataset-with-desciclopedia-text
4810	4810	Cryptocurrency extra data - EOS.IO	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	520	3500	29	yamqwe	cryptocurrency-extra-data-eos-io
4811	4811	stewards		[]		0	7	0	pavfedotov	stewards
4812	4812	efficientnetv2_tf	efficientnetv2 github repo	['computer science', 'computer vision', 'image data', 'tensorflow']		2	224	1	kozistr	efficientnetv2-tf
4813	4813	interace_meta		[]		0	6	1	thomasmeiner	interace-meta
4814	4814	formulaic		[]		0	2	1	thomasmeiner	formulaic
4815	4815	ngboost_e2e		[]		0	5	1	thomasmeiner	ngboost-e2e
4816	4816	lifelines		[]		0	2	1	thomasmeiner	lifelines
4817	4817	Cryptocurrency extra data - Bitcoin	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	1165	7994	49	yamqwe	cryptocurrency-extra-data-bitcoin
4818	4818	Winter Olympics Prediction - Fantasy Draft Picks	Creating a predictive model for the 2022 Winter Olympics	['sports']	"Olympic Draft Predictive Model
Our family runs an Olympic Draft - similar to fantasy football or baseball - for each Olympic cycle.  The purpose of this case study is to identify trends in medal count / point value to create a predictive analysis of which teams should be selected in which order.
There are a few assumptions that will impact the final analysis:
Point Value - Each medal is worth the following:
    Gold - 6 points
    Silver - 4 points
    Bronze - 3 points
For analysis reviewing the last 10 Olympic cycles.
Winter Olympics only.
All GDP numbers are in USD
My initial hypothesis is that larger GDP per capita and size of contingency are correlated with better points values for the Olympic draft.
All Data pulled from the following Datasets:
Winter Olympics Medal Count - https://www.kaggle.com/ramontanoeiro/winter-olympic-medals-1924-2018
Worldwide GDP History - https://data.worldbank.org/indicator/NY.GDP.MKTP.CD?end=2020&start=1984&view=chart
GDP data was a wide format when downloaded from the World Bank.  Opened file in Excel, removed irrelevant years, and saved as .csv.
Process
In RStudio utilized the following code to convert wide data to long:
install.packages(""tidyverse"")
library(tidyverse)
library(tidyr)
Converting to long data from wide
long &lt;- newgdpdata %&gt;% 
  gather(year, value, -c(""Country Name"",""Country Code""))
Completed these same steps for GDP per capita.
Primary Key Creation
Differing types of data between these two databases and there is not a good primary key to utilize.  Used CONCAT to create a new key column in both combining the year and country code to create a unique identifier that matches between the datasets.
SELECT *,
CONCAT(year,country_code) AS ""Primary""
FROM medal_count
Saved as new table ""medals_w_primary""
Utilized Excel to concatenate the primary key for GDP and GDP per capita utilizing:
=CONCAT()
Saved as new csv files.
Uploaded all to SSMS.
Contingent Size
Next need to add contingent size.
No existing database had this information.
Pulled data from Wikipedia.
2018 - No problem, pulled existing table.
2014 - Table was not created.  Pulled information into excel, needed to convert the country NAMES into the country CODES.
Created excel document with all ISO Country Codes.
Items were broken down between both formats, either 2 or 3 letters.  Example:
AF/AFG
Used =RIGHT(C1,3) to extract only the country codes.
For the country participants list in 2014, copied source data from Wikipedia and pasted as plain text (not HTML).
Items then showed as:
Albania (2)
Broke cells using ""("" as the delimiter to separate country names and numbers, then find and replace to remove all parenthesis from this data.
We were left with:
Albania  2
Used VLOOKUP to create correct country code:
=VLOOKUP(A1,'Country Codes'!A:D,4,FALSE)
This worked for almost all items with a few exceptions that didn't match.  Based on nature and size of items, manually checked on which items were incorrect.
Chinese Taipei  3   #N/A
Great Britain   56  #N/A
Virgin Islands  1   #N/A
This was relatively easy to fix by adding corresponding line items to the Country Codes sheet to account for future variability in the country code names.
Copied over to main sheet.
Repeated this process for additional years.
Once complete created sheet with all 10 cycles of data.  In total there are 731 items.
Data Cleaning
Filtered by Country Code since this was an issue early on.
Found a number of N/A Country Codes:
Serbia and Montenegro
FR Yugoslavia
FR Yugoslavia
Czechoslovakia
Unified Team
Yugoslavia
Czechoslovakia
East Germany
West Germany
Soviet Union
Yugoslavia
Czechoslovakia
East Germany
West Germany
Soviet Union
Yugoslavia
Appears to be issues with older codes, Soviet Union block countries especially.  Referred to historical data and filled in these country codes manually.  Codes found on iso.org.
Filled all in, one issue that was more difficult is the Unified Team of 1992 and Soviet Union.  For simplicity used code for Russia - GDP data does not recognize the Soviet Union, breaks the union down to constituent countries.  Using Russia is a reasonable figure for approximations and analysis to attempt to find trends.
From here created a filter and scanned through the country names to ensure there were no obvious outliers.  Found the following:
Olympic Athletes from Russia[b] -- This is a one-off due to the recent PED controversy for Russia.  Amended the Country Code to RUS to more accurately reflect the trends.
Korea[a] and South Korea -- both were listed in 2018.  This is due to the unified Korean team that competed.  This is an outlier and does not warrant standing on its own as the 2022 Olympics will not have this team (as of this writing on 01/14/2022).  Removed the COR country code item.
Confirmed Primary Key was created for all entries.
Ran minimum and maximum years, no unexpected values.
Ran minimum and maximum Athlete numbers, no unexpected values.
Confirmed length of columns for Country Code and Primary Key.
No NULL values in any columns.  Ready to import to SSMS.
SQL work
We now have 4 tables, joined together to create the master table:
SELECT [OlympicDraft].[dbo].[medals_w_primary].[year], host_country, host_city, [OlympicDraft].[dbo].[medals_w_primary].[country_name], [OlympicDraft].[dbo].[medals_w_primary].[country_code], Gold, Silver, Bronze, [OlympicDraft].[dbo].[gdp_w_primary].[value] AS GDP, [OlympicDraft].[dbo].[convertedgdpdatapercapita].[gdp_per_capita], Atheletes
FROM medals_w_primary
INNER JOIN gdp_w_primary
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[gdp_w_primary].[year_country]
INNER JOIN contingency_cleaned
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[contingency_cleaned].[Year_Country]
INNER JOIN convertedgdpdatapercapita
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[convertedgdpdatapercapita].[Year_Country]
    ORDER BY year DESC
This left us with the following table:
Performed some basic cleaning tasks to ensure no outliers:
Checked GDP numbers:
1992 North Korea shows as null.  Updated this row with information from countryeconomy.com - $12,458,000,000
Checked GDP per capita:
1992 North Korea again missing.  Updated this to $595, utilized same source.
UPDATE [OlympicDraft].[dbo].[gdp_w_primary]
    SET [OlympicDraft].[dbo].[gdp_w_primary].[value] = 12458000000
    WHERE [OlympicDraft].[dbo].[gdp_w_primary].[year_country] = '1992PRK'
UPDATE [OlympicDraft].[dbo].[convertedgdpdatapercapita]
    SET [OlympicDraft].[dbo].[convertedgdpdatapercapita].[gdp_per_capita] = 595
    WHERE [OlympicDraft].[dbo].[convertedgdpdatapercapita].[year_country] = '1992PRK'
Liechtenstein showed as an outlier with GDP per capita at 180,366 in 2018.  Confirmed this number is correct per the World Bank, appears Liechtenstein does not often have atheletes in the winter olympics.  Performing a quick SQL search to verify this shows that they fielded 3 atheletes in 2018, with a Bronze medal being won.  Initially this appears to be a good ratio for win/loss.
Finally, need to create a column that shows the total point value for each of these rows based on the above formula (6 points for Gold, 4 points for Silver, 3 points for Bronze).
Updated query as follows:
SELECT [OlympicDraft].[dbo].[medals_w_primary].[year], host_country, host_city, [OlympicDraft].[dbo].[medals_w_primary].[country_name], [OlympicDraft].[dbo].[medals_w_primary].[country_code], Gold, Silver, Bronze, [OlympicDraft].[dbo].[gdp_w_primary].[value] AS GDP, [OlympicDraft].[dbo].[convertedgdpdatapercapita].[gdp_per_capita], Atheletes,
    (Gold6) + (Silver4) + (Bronze*3) AS 'Total_Points'
FROM [OlympicDraft].[dbo].[medals_w_primary]
INNER JOIN gdp_w_primary
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[gdp_w_primary].[year_country]
INNER JOIN contingency_cleaned
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[contingency_cleaned].[Year_Country]
INNER JOIN convertedgdpdatapercapita
    ON [OlympicDraft].[dbo].[medals_w_primary].[primary] = [OlympicDraft].[dbo].[convertedgdpdatapercapita].[Year_Country]
    ORDER BY [OlympicDraft].[dbo].[convertedgdpdatapercapita].[year]
Spot checked, calculating correctly.
Saved result as winter_olympics_study.csv.
We can now see that all relevant information is in this table:
RStudio Work
To continue our analysis, opened this CSV in RStudio.
install.packages(""tidyverse"")
library(tidyverse)
library(ggplot2)
install.packages(""forecast"")
library(forecast)
install.packages(""GGally"")
library(GGally)
install.packages(""modelr"")
library(modelr)
View(winter_olympic_study)
Finding correlation between gdp_per_capita and Total_Points
ggplot(data = winter_olympic_study) + geom_point(aes(x=gdp_per_capita,y=Total_Points,color=country_name)) +
facet_wrap(~country_name)
cor(winter_olympic_study$gdp_per_capita, winter_olympic_study$Total_Points, method = c(""pearson""))
Result is .347, showing a moderate correlation between these two figures.
Looked next at GDP vs. Total_Points
ggplot(data = winter_olympic_study) + geom_point(aes(x=GDP,y=Total_Points,color=country_name))+
facet_wrap(~country_name)
cor(winter_olympic_study$GDP, winter_olympic_study$Total_Points, method = c(""pearson""))
This resulted in 0.35, statistically insignificant difference between this and GDP Per Capita
Next looked at contingent size vs. total points
ggplot(data = winter_olympic_study) + geom_point(aes(x=Atheletes,y=Total_Points,color=country_name)) +
  facet_wrap(~country_name)
cor(winter_olympic_study$Atheletes, winter_olympic_study$Total_Points, method = c(""pearson"")) 
This has a much better correlation, at 0.736, plot also looks much more linear:
0.736 is better, however there are some notable outliers such as Norway, which don't have as many athletes but still perform exceptionally well.
Clearly there's something additional at play.  Norway has been at or near the top of the medal count but doesn't fit neatly within this correlation.  Additional factor that might need to be reviewed is which sports have the most medals awarded, etc.  This will be an analysis for another time.  For now, looking at correlation with this notable outlier.
Performed predictive analysis using relevant factors.
input &lt;- winter_olympic_study[c(""Total_Points"",""gdp_per_capita"",""Atheletes"")]
View(input)
model &lt;- lm(Total_Points~gdp_per_capita+Atheletes, data = input)
View(model)
summary(model)$coefficient
Estimate   Std. Error   t value     Pr(&gt;|t|)
(Intercept)    -9.4913725212 3.770775e+00 -2.517088 1.274746e-02
gdp_per_capita  0.0002632879 8.869442e-05  2.968483 3.419816e-03
Atheletes       0.4966503345 3.761424e-02 13.203786 6.143128e-28
confint(model)
sigma(model)/mean(winter_olympic_study$Total_Points)
1 0.6478761
This shows that between Athletes and GDP a reasonable correlation can be found.
Testing regression models
plot(Total_Points ~ Atheletes, data = winter_olympic_study)
Athletes.lm &lt;- lm(Total_Points ~ Atheletes, data = winter_olympic_study)
summary(Athletes.lm)
predict(model, newdata = p)
points.lm &lt;- lm(Total_Points ~ gdp_per_capita + Atheletes, data = winter_olympic_study)
summary(points.lm)
Multiple Regression for both GDP Per Capita and Contingency Size
par(mfrow=c(2,2))
plot(points.lm)
par(mfrow=c(1,1))
summary(model)
Test data to test the predictions
athletes.df &lt;- data.frame(Atheletes=c(50,113,150))
predict(Athletes.lm,newdata = athletes.df)
Data appears to predict correctly using the one variable.
Attempted with 2
predict(Athletes.lm,newdata = athletes.df,interval = 'confidence')
athgdp.df &lt;- data.frame(Atheletes=c(50,113,150),gdp_per_capita=c(10000,20000,50000))
view(athgdp.df)                    
predict(Athletes.lm,newdata = athgdp.df,interval = 'confidence')
2 variables appear to work correctly as well.
Tested against 2018 data.  Did not include dataframe as cumbersome.
predict(Athletes.lm,newdata = df2018,interval = 'confidence')
predict_2018 &lt;- predict(Athletes.lm,newdata = df2018)
Predicted point values align very well with actual 2018 data.  Can determine which teams under and over performed based on prior data.  Appears this model will be a good statistical model to choose future teams based on GDP Per Capita and Contingent Size.
Reviewing the predictions most are within reason.  Norway is again the exception.  This appears to be the only extreme example of this.  While additional data might be able to be reviewed to determine why this is, suffice it to say Norway would be expected to continue getting significant points, perhaps due to sports they compete in, perhaps other factors are at play.
Potential overall variance could be explained with team sports such as Hockey which have a large number of athletes, but only 6 medals (Women and Mens).
2022 Predictions
Now, all that's left is predicting the 2022 Olympics.  I plan on updating this, but team sizes were pulled on 01/19/2022 from the individual countries 2022 Olympics Wikipedia page.
View(df2022)
Table has been created for 2022.  Now time to create predictions:
predict2022 &lt;- predict(Athletes.lm,newdata = df2022,interval = 'confidence')
View(predict2022)
Predictions input.  Now a simple matter of exporting the information to a .csv and inputting for visualizations in Tableau.  Easiest tool is Excel for this.  Uploaded data into excel and saved as csv, then added country_names for processing.
write.csv(predict2022,""2022_predictions.csv"", row.names = FALSE)
Acknowledgements
I'd like to call out Alex Freberg for his amazing Youtube series with excellent technical knowledge in an easy to understand manner.
I'd also like to thank Google and Coursera for their Data Analysis Certificate Program which I recently completed.
Dashboard
Published dashboard:
https://public.tableau.com/app/profile/eric1949/viz/OlympicDraftPicks/Dashboard1"	9	57	0	ericsbrown	winter-olympics-prediction-fantasy-draft-picks
4819	4819	little_help		[]		0	4	0	soutanbasak	little-help
4820	4820	Video Games Data		['video games']		3	72	0	albertmarrero	video-games-data
4821	4821	Stadia Game Data	Attributes of games listed in Stadia	['games', 'video games', 'tabular data', 'text data']		1	14	0	jayasooryantm	stadia-game-data
4822	4822	2021 CSGO Save Data		[]		2	18	0	ralphiisports	2021-csgo-save-data
4823	4823	data_preparation_task2_		[]		1	6	0	phoonyein	data-preparation-task2
4824	4824	Cycling Count Berlin	Bicycle traffic data in Berlin 2012-2020	['cities and urban areas', 'transportation', 'time series analysis', 'lstm']	"Context
Cycling is an essential part in the journey towards a more sustainable mobility and transportation. To prove the importance of usage of bicycles, Berlin installed several counting stations at different locations. Since 2012 the administration of Berlin collects and publish data about the number of cyclists in Berlin. The original raw data can be found here. I transformed the data to a more convient structure to work with (in my opinion).
Content
The provided Excel file has multiple measurement tables (data_20XX). Each table contains data for one year (2012 until 2020). One row contains data for one hour. In the first years there is only one station providing data; since 2016 there are more stations with data. Error and missing values are indicated by the value '-1'.
- DateTime: Date and time of the measurement
- Station: acronym of the measurement station (reference to the data_location table, see below)
- cyclists: Sum of cyclists in this hour at this station
The table data_location contains following data about the station
- Zaehlstelle: acronym of the measurement station
- Beschreibung - Fahrtrichtung: real name and location of the measurement station
- Breitengrad: latitude of station
- Laengengrad: longitude of station
- Installationsdatum: date of installation of station
Acknowledgements and Licence
This data is originally provided by ""Senatsverwaltung für Umwelt, Mobilität, Verbraucher- und Klimaschutz - Abteilung Verkehr, Am Köllnischen Park 3,
10179 Berlin"" or Administation of Berlin under the license dl-de/by-2-0. The original raw data can be found here. I transformed the data to a more convient structure to work with (in my opinion).
Inspiration
I love cycling and this data set provides some insights of cycling traffic at different locations in Berlin. This data set is well suited for time series analysis and forecasting! More over you can practice your feature engeineering skills by augmenting the data with more features..."	3	39	0	phisinger	bike-counting-berlin
4825	4825	ridge_toxic_cleanexp1_fasttext_more_fe_oof		[]		1	7	0	shobhitupadhyaya	ridge-toxic-cleanexp1-fasttext-more-fe-oof
4826	4826	Train kek		[]		1	11	0	panser	train-kek
4827	4827	ridge_toxic_cleanexp2_fasttext_more_fe_oof		[]		1	7	0	shobhitupadhyaya	ridge-toxic-cleanexp2-fasttext-more-fe-oof
4828	4828	!Kung San Height data	Based on Howell’s observations of the !Kung San	['arts and entertainment']		2	6	0	tylerbonnell	kung-san-height-data
4829	4829	flair-ner		[]		4	7	0	ganeshn88	flairner
4830	4830	weatherAUS_Rev1		[]		4	4	0	samuelguerin	weatheraus-rev1
4831	4831	Cryptocurrency extra data - Binance Coin	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	877	5122	33	yamqwe	cryptocurrency-extra-data-binance-coin
4832	4832	image_to_predict_cnn		[]		0	1	0	emrhn1031	image-to-predict-cnn
4833	4833	Augmented_Set_of_Chemical_Structures		[]		3	17	1	narminj	augmented-set-of-chemical-structures
4834	4834	RSNA Normal Images PNG Part5		[]		0	11	0	fereshtej	rsna-normal-images-png-part5
4835	4835	Cryptocurrency extra data - Bitcoin Cash	[Auto Updating] Market data collection for G-Research Crypto forecasting comp	['finance', 'banking', 'investing', 'currencies and foreign exchange']	"Context:
This dataset is an extra updating dataset for the G-Research Crypto Forecasting competition.
Introduction
This is a daily updated dataset, automaticlly collecting market data for G-Research crypto forecasting competition. 
The data is of the 1-minute resolution, collected for all competition assets and both retrieval and uploading are fully automated. see discussion topic.
The Data
For every asset in the competition, the following fields from Binance's official API endpoint for historical candlestick data are collected, saved, and processed.
```
timestamp - A timestamp for the minute covered by the row.
Asset_ID - An ID code for the cryptoasset.
Count - The number of trades that took place this minute.
Open - The USD price at the beginning of the minute.
High - The highest USD price during the minute.
Low - The lowest USD price during the minute.
Close - The USD price at the end of the minute.
Volume - The number of cryptoasset u units traded during the minute.
VWAP - The volume-weighted average price for the minute.
Target - 15 minute residualized returns. See the 'Prediction and Evaluation section of this notebook for details of how the target is calculated.
Weight - Weight, defined by the competition hosts here
Asset_Name - Human readable Asset name.
```
Indexing
The dataframe is indexed by timestamp and sorted from oldest to newest. 
The first row starts at the first timestamp available on the exchange, which is July 2017 for the longest-running pairs.
Usage Example
The following is a collection of simple starter notebooks for Kaggle's Crypto Comp showing PurgedTimeSeries in use with the collected dataset. 
Purged TimesSeries is explained here. There are many configuration variables below to allow you to experiment. Use either GPU or TPU. You can control which years are loaded, which neural networks are used, and whether to use feature engineering. You can experiment with different data preprocessing, model architecture, loss, optimizers, and learning rate schedules. The extra datasets contain the full history of the assets in the same format as the competition, so you can input that into your model too.
Baseline Example Notebooks:
Neural Network Starter
LightGBM Starter
Catboost Starter
XGBoost Starter
TabNet Starter
Reinforcement Learning (PPO) Starter
These notebooks follow the ideas presented in my ""Initial Thoughts"" here. Some code sections have been reused from Chris' great (great) notebook series on SIIM ISIC melanoma detection competition here
Loose-ends:
This is a work in progress and will be updated constantly throughout the competition. At the moment, there are some known issues that still needed to be addressed:
VWAP: - At the moment VWAP calculation formula is still unclear. Currently the dataset uses an approximation calculated from the Open, High, Low, Close, Volume candlesticks. [Waiting for competition hosts input]
Target Labeling: There exist some mismatches to the original target provided by the hosts at some time intervals. On all the others - it is the same. The labeling code can be seen here. [Waiting for competition hosts] input] 
Filtering: No filtration of 0 volume data is taken place.
Example Visualisations
Opening price with an added indicator (MA50):
Volume and number of trades:
License
This data is being collected automatically from the crypto exchange Binance."	491	2692	10	yamqwe	cryptocurrency-extra-data-bitcoin-cash
4836	4836	dataset1		[]		0	4	0	madadshah	dataset1
4837	4837	WINE QUALITY RED DATASET		['alcohol']		3	49	0	shivamchaudhary11	wine-quality-red-dataset
4838	4838	altegrad 2021		[]		0	13	1	antocad	altegrad-2021
4839	4839	dataset		[]		0	3	0	madadshah	dataset
4840	4840	BoardGameGeek Reviews	13M reviews by 290k users	['board games', 'internet']	"EDIT: UPDATED JANUARY 2022! New version with 4M additional reviews, since last update! With enough upvotes I'll may refresh the dataset again next year :)
2020:
New version with 2M additional reviews (up from 13M to 15M in about 15 months). 
Number of unique users has grown from 290k to 350k.
Do you like games? Do you like data science? In that case this could be a dataset for you. I tried to sharpen my data science skills on the reviews users posted on boardgamegeek, the world's largest board game site.
In the csv file are all the reviews of users. Comments are included. The download was made by querying the BGG API2 at 2nd of May 2019. All the game IDs I took from Beefsack's Github, which basically includes all games with &gt;30 reviews.
Thanks obviously to the wonderful folks at BoardGameGeek. 
Disclaimer: I don't own this data, it's property of BoardGameGeek and should be used under their terms
Photo by JESHOOTS.COM on Unsplash
What can you find out? Any hidden patterns? Give it a go!"	2389	23100	97	jvanelteren	boardgamegeek-reviews
4841	4841	COUNTBOOKS		[]		0	0	0	fahedshaikh	countbooks
4842	4842	madrid_weather		[]		1	6	0	mustafabozka	madrid-weather
4843	4843	MANGA Comparison		['business', 'finance', 'data analytics', 'investing']		4	49	5	mostafaalaa123	manga-comparison
4844	4844	Iris_dataset		[]		0	5	0	mdwasimakhtar03	iris-dataset
4845	4845	Manga Panels Dataset 817 Mangas	A curated collection of panels from 817 unique different mangas	['anime and manga']		7	56	1	mrviolet	manga-panels-dataset-817-mangas
4846	4846	tfgbr-masks-br		[]		0	3	0	ks2019	tfgbr-masks-br
4847	4847	Global terrorism 		[]		4	17	0	jonathanparker69	global-terrorism
4848	4848	IndianCurrency_for_classification		[]		0	21	0	najiaboo	indiancurrency-for-classification
4849	4849	DAQUAR Dataset (Processed) for VQA	DAtaset for QUestion Answering on Real-world images	['intermediate', 'nlp', 'computer vision', 'tabular data', 'image data']	"Context
The first significant Visual Question Answering (VQA) dataset was the DAtaset for QUestion Answering on Real-world images (DAQUAR). It contains 6794 training and 5674 test question-answer pairs, based on images from the NYU-Depth V2 Dataset. That means about 9 pairs per image on average.
This dataset is a processed version of the Full DAQUAR Dataset where the questions have been normalized (for easier consumption by tokenizers) & the image IDs, questions & answers are stored in a tabular (CSV) format, which can be loaded & used as-is for training VQA models.
Content
This dataset contains the processed DAQUAR Dataset (full), along with some of the raw files from the original dataset.
Processed data:
- data.csv: This is the processed dataset after normalizing all the questions & converting the {question, answer, image_id} data into a tabular format for easier consumption.
- data_train.csv: This contains those records from data.csv which correspond to images present in train_images_list.txt
- data_eval.csv: This contains those records from data.csv which correspond to images present in test_images_list.txt
- answer_space.txt: This file contains a list of all possible answers extracted from all_qa_pairs.txt (This will allow the VQA task to be modelled as a multi-class classification problem)
Raw files:
- all_qa_pairs.txt
- train_images_list.txt
- test_images_list.txt
Acknowledgements
Malinowski, Mateusz, and Mario Fritz. ""A multi-world approach to question answering about real-world scenes based on uncertain input."" Advances in neural information processing systems 27 (2014): 1682-1690."	2	38	0	tezansahu	processed-daquar-dataset
4850	4850	shandong_preweight		[]		0	4	0	wjfwjf	shandong-preweight
4851	4851	RuneScape Usernames	55 million RuneScape MMO usernames	['video games', 'demographics', 'internet', 'text data', 'online communities']	"Context
In 2014 RuneScape removed the character names of dormant accounts, making them available to the public.
(180 MB compressed, 600 MB uncompressed)
Content
In 2014 RuneScape removed the character names of dormant accounts, making them available to the public.
Download All (100 MB compressed, 600 MB uncompressed)
Official News Posts:
 0
 1
 2
 3
 4
 5
 6
Currently missing names that started with numbers
Username Rules
Max length: 12
Allowed characters: a-z, 0-9, _
Cannot start or end with _
Acknowledgements
Data compiled and downloaded from https://github.com/RuneStar/name-cleanup-2014
Inspiration
What names are popular? How do they relate? 
How unique are the names in Runescape vs real life, or other MMOs?"	27	822	11	danofer	runescape-usernames
4852	4852	breast-cancer		['cancer']		3	23	0	nayefahmad	breastcancer
4853	4853	cartoonset100k		['comics and animation']	"Source -  https://google.github.io/cartoonset/download.html
The cartoon images are named csX.png, where X is a hash computed from the cartoon's attribute configuration.
Each cartoon image has an accompanying csX.csv file that lists the attributes for that cartoon. Each line in the attribute file contains the attribute name, the index of the selected variant, and the total number of variants for that attribute, for example:
  ""face_shape"", 4, 7
  ""facial_hair"", 14, 15
  ""hair"", 29, 111
  ""eye_color"", 2, 5
  ...
This data is licensed by Google LLC under a Creative Commons Attribution 4.0 International License."	1	9	0	paulrohan2020	cartoonset100k
4854	4854	Tech Support Scams Dataset	Tech-Support-Scams and their popups from 2018-2021	['websites', 'beginner', 'tabular data', 'email and messaging']	"Context
The PopupDB Project has been collecting data about Tech-Support-Scams and their popups since 2018 and stopped collecting around the end of the year 2021. This dataset is a small sample of the data collected by the project.
Tech support scammers try to scam innocent people into trusting them to give access to their device citing Malware and other issues with the intention of scamming the user, This can appear in the form of pop-ups and users should be well aware of the potential risks they can cause.
Content
The dataset Contains 11375 Rows and 14 Columns and is arranged in tabular format. It contains various details of malicious websites
Acknowledgements
(© PopupDB.org | choozn / 2021)- Attribution 4.0 International (CC BY 4.0)"	109	939	14	jehanbhathena	tech-support-scams-dataset
4855	4855	Census		['social science']		1	15	0	lightout99	census
4856	4856	Longitude and latitude		[]		0	9	0	ouoouoouo	longitude-and-latitude
4857	4857	Amazon product data dump		['business']		1	28	0	vvkiscool	amazon-product-data-dump
4858	4858	Kaggle Datasets Ranking	Top 1000 Kaggle Datasets Ranking	['computer science', 'regression']	"Context
This dataset contains Kaggle ranking of datasets.
Content
+800 rows and 8 columns.
Columns' description are listed below.
Rank : Rank of the user
Tier : Grandmaster, Master or Expert
Username : Name of the user
Join Date : Year of join
Gold Medals : Number of gold medals
Silver Medals : Number of silver medals
Bronze Medals : Number of bronze medals
Points : Total points
Acknowledgements
Data from Kaggle.
Image from The Guardian.
If you're reading this, please upvote."	115	1373	17	vivovinco	kaggle-datasets-ranking
4859	4859	wiki_sents_datasets		[]	"Context
来自wiki中文的1258282句
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	1	7	0	terrychanorg	wiki-sents-datasets
4860	4860	TRAIN001		[]		0	2	0	ilingtsou	train001
4861	4861	Waste Classification Image Dataset		['energy']		3	40	1	nandankakadiya	waste-classification-image-dataset
4862	4862	roberta_data		[]		0	4	0	vigneshirtt	roberta-data
4863	4863	FVC 2000		['education']		0	12	1	mayanksinghk	fvc-2000
4864	4864	Finding The greater and lesser number using Python		['computer science']		0	1	0	learntechwithavan	finding-the-greater-and-lesser-number-using-python
4865	4865	STONE PAPER SCISSOR game using PYTHON language		['games']		1	10	0	learntechwithavan	stone-paper-scissor-game-using-python-language
4866	4866	ffc for image inpainting		[]		1	17	0	tom99763	ffc-for-image-inpainting
4867	4867	song_popularity_5_folds		[]		0	2	1	snikhil17	song-popularity-5-folds
4868	4868	RSNA Normal Images PNG Part4		[]		1	17	0	fereshtej	rsna-normal-images-png-part4
4869	4869	THIRTYNP		[]		1	7	0	duriani	thirtynp
4870	4870	TRAIN01		[]		0	8	0	ilingtsou	train01
4871	4871	libis-100k		[]		0	13	0	senvaitis	libis100k
4872	4872	naic2021		[]		0	4	0	buptxj	naic2021
4873	4873	5-LakeHuron		[]		0	0	0	wallacefqq	5lakehuron
4874	4874	temple		['religion and belief systems']		0	5	0	amanda410714218	temple
4875	4875	cc.ar.300.bin		[]	"This dataset from FASTTEXT Library was Uploaded from here Download 6GB
You can see the code of how it works and use it . How To Use"	0	7	2	murtadhayaseen	ccar300bin
4876	4876	WeedCountImages_v1		[]		0	5	0	itqfnu	weedcountimages-v1
4877	4877	yolov5l6 trained		[]		1	5	0	crained	yolov5l6-trained
4878	4878	external_data		[]		0	20	0	woohyeokmoon	external-data
4879	4879	Song Popularity Predictions		['music']		1	73	1	gomohit	song-popularity-predictions
4880	4880	Single cell RNAseq data U2OS cell line	GSE146773 Spatiotemporal dissection of the cell cycle with single-cell proteogen	['genetics', 'biology', 'biotechnology', 'cancer']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
The data from: 
https://www.nature.com/articles/s41586-021-03232-9
Spatiotemporal dissection of the cell cycle with single-cell proteogenomics
Emma Lundberg et.al.
Single cell RNA sequencing data - https://en.wikipedia.org/wiki/Single_cell_sequencing
See many related datasets on kaggle here: https://www.kaggle.com/general/203136"	2	280	0	alexandervc	single-cell-rnaseq-data-related-to-cell-cycle
4881	4881	dblp_publication_data_for_mgp_researchers		[]		0	3	0	curiousaniruddha	dblp-publication-data-for-mgp-researchers
4882	4882	NASDAQ-100 Stock Price Data	2010 to till date daily trends of NASDAQ-100 Stocks	['business', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics', 'tabular data']	"&gt; - The Nasdaq Stock Market ) is an American stock exchange based in New York City. It is ranked second on the list of stock exchanges by market capitalization of shares traded, behind the New York Stock Exchange.
&gt; - The exchange platform is owned by Nasdaq, Inc., which also owns the Nasdaq Nordic stock market network and several U.S. stock and options exchanges.
&gt; - ""Nasdaq"" was initially an acronym for the National Association of Securities Dealers Automated Quotations. 
&gt; - It was founded in 1971 by the National Association of Securities Dealers (NASD), now known as the Financial Industry Regulatory Authority (FINRA).
&gt; - On February 8, 1971, the Nasdaq stock market began operations as the world's first electronic stock market.
&gt; - At first, it was merely a ""quotation system"" and did not provide a way to perform electronic trades.
Context
NASDAQ is one of the most popular stock exchanges in the world and the data trend determines the world economy in a way
Content
Stock prices of all NASDAQ-100 index stocks (as on Sep 2021) from 2010
Acknowledgements
Yahoo Finance API development team
Inspiration
All the Kagglers and Data Science Enthusiasts"	254	2267	16	kalilurrahman	nasdaq100-stock-price-data
4883	4883	IMDB Movie Ratings Sentiment Analysis	IMDB Movie Ratings Sentiment Analysis	['movies and tv shows', 'beginner', 'nlp', 'classification', 'binary classification']	"Description:
The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.
train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.
test.tsv contains just phrases. You must assign a sentiment label to each phrase.
The sentiment labels are:
0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive
The dataset can be downloaded here: https://archive.ics.uci.edu/ml/datasets/spambase
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict the ratings of the movie.
Compare the evaluation metrics of vaious classification algorithms."	624	5920	31	yasserh	imdb-movie-ratings-sentiment-analysis
4884	4884	RSNA Normal Images PNG Part3		[]		1	13	0	fereshtej	rsna-normal-images-png-part3
4885	4885	Octopus		[]		0	6	0	fsociety00	octopus
4886	4886	ubiquant_lgb_models		[]		3	10	0	yosukeyama	ubiquant-lgb-models
4887	4887	mtcars sl dataset		[]		1	7	0	ashish2693	mtcars-sl-dataset
4888	4888	Using RoBERTa (FEEDBACK comp)		['arts and entertainment']		22	68	0	temporaryaccabhi	using-roberta-feedback-comp
4889	4889	stm.csv		[]		0	8	0	aftabsama	stmcsv
4890	4890	Automobile Data Exploration SL		[]		0	8	0	ashish2693	automobile-data-exploration-sl
4891	4891	Battel		[]		0	7	0	aftabsama	battel
4892	4892	lgb_simple		[]		0	5	0	jellyz9	lgb-simple
4893	4893	jacuzzi-fireplace		[]		0	2	0	dharini1708	jacuzzifireplace
4894	4894	mmdet_2.17.0		[]		0	0	0	riadalmadani	mmdet-2170
4895	4895	Bay Electric San Francisco		[]	We are San Francisco’s trusted and proud Electrical Contractors for all commercial and residential projects. Our goal is to work closely with our clients to determine and create an environment that is a reflection of their needs. Whether it is a single room, an entire home, an outlet repair or a custom lighting installation, we can provide quality service. Our services include assistance with panel, breakers and fuses, 110/220 volts circuits, recessed can lights, switches and outlets, indoor/outdoor lighting, tenant improvements, security cameras, code violations corrections and locally owned businesses. We believe open communication and accurate scheduling are the keys to achieving your desired results.	0	11	0	bayelectricsf	bay-electric-san-francisco
4896	4896	final_extra_data		[]		0	33	0	songeunmoon	final-extra-data
4897	4897	data.statistiek.csv		['internet']		0	5	0	senabarhan	datastatistiekcsv
4898	4898	covid_470		[]		0	11	0	ananyagr	covid-470
4899	4899	yolos_transformer		[]		0	12	0	dwchen	yolos-transformer
4900	4900	lightgbm-5folds-gkp		[]		6	13	0	hengzheng	lightgbm5foldsgkp
4901	4901	NIFTY Sectoral & Thematic Indices [1990-2021]	Stock price data of all sectoral and thematic NIFTY indices from NSE India	['india', 'business', 'intermediate', 'time series analysis', 'tabular data', 'investing']	"This dataset contains stock price data of all sectoral and thematic NIFTY indices from NSE India:
Structure of Dataset:
Sectoral Indices [contains 15 different indices]
AUTO 
BANK
CONSUMER DURABLES
FINANCIAL SERVICES 20/50
FINANCIAL SERVICES 
FMCG
HEALTHCARE
IT
MEDIA
METAL
OIL AND GAS
PHARMA
PRIVATE BANK
PSU BANK
REALTY
Thematic Indices [contains 17 different indices]
ADITYA BIRLA GROUP
COMMODITIES
CPSE
ENERGY
INDIA CONSUMPTION
INDIA DIGITAL
INDIA MANUFACTURING
INFRASTRUCTURE
MAHINDRA GROUP
MNC
MOBILITY
NON-CYCLICAL CONSUMER
PSE
SERVICES SECTOR
SME EMERGE
TATA GROUP
TATA GROUP 25 CAP
All files contain 5 columns that are Date, Open, Close, High, Low
Date: Market open date
Open: Price at which market/sector/theme opened
Close: Price at which market/sector/theme closed
High: Highest Price of the index on a particular date
Low: Lowest Price of the index on a particular date"	14	211	10	shubamsumbria	nifty-sectoral-thematic-indices-19902021
4902	4902	H²O Interaction dataset	Human-to-Human-or-Object Interaction dataset	['earth and nature', 'business', 'artificial intelligence', 'computer science', 'computer vision', 'deep learning', 'image data']	"H²O: Human-to-Human-or-Object Interaction Dataset
H²O is an image dataset annotated for Human-to-Human-or-Object interaction detection. 
The dataset has been introduced in this paper: Orcesi, A., Audigier, R., Toukam, F. P., & Luvison, B. (2021, December). Detecting Human-to-Human-or-Object (H 2 O) Interactions with DIABOLO. In 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021) (pp. 1-8). IEEE. 
The annotations were made with Pixano, an opensource, smart annotation tool for computer vision applications: https://pixano.cea.fr/
Dataset description
Images
H²O is composed of the 10 301 images from V-COCO dataset to which are added 3 635 images which mostly contain interactions between people.
Extra-images sources:
MS-COCO dataset
Human-Interaction-Images dataset
BIT-Interaction dataset
SBU-Kinect-Interaction dataset
TV-Human-interaction dataset
Pascal-Voc dataset
ShakeFive2 dataset
UT-Interaction dataset
Web images
Annotations
All H²O images have been annotated with a new taxonomy of verbs including human-to-object and human-to-human interactions.
This taxonomy is composed of 51 verbs divided into 5 categories:
Verbs decribing the general posture of the subject
Verbs related to the way the subject is moving
Verbs used for interactions with object
Verbs describing human-to-human interactions
Verbs of interactions involving strength or violence
Dataset download
Please download and unzip the H2O.zip file.
Images
To get images, please download V-COCO images and split it in trainval set and test set as defined in the V-COCO split files. Then rename it as HO[vcoco_id.zfill(10)].jpg.
Then launch the download_HH_images.py script to get the additional images.
The script first download the dataset from which are taken extra-images and then copy or download all images in a new directory H2O next to the script.
Annotations
In H²O, each interacting object is annotated even if its category is not in the 80 COCO classes.
We provide 3 annotation folders:
initial_annotations/ where objects outside the 80 COCO classes are labeled with their real label.
other_annotations/ where objects outside the 80 COCO classes are grouped under the label ""other"".
vcocolike_annotations/ where objects outside the 80 COCO classes are not annotated: the interactions with such objects are thus considered as if they had no target object.
In each of these files, you can find 4 folders:
trainval and test where the structure is as follows:
```
{
""entities"":[
    {
        ""sourceId"":     # Image name
        ""category"":     # Object category
        ""geometry"":
        {
            ""geometrytype"": 1,
            ""isNormalized"": true,
            ""vertices"": [xmin, ymin, xmax, ymax]    # Normalized coordinates
        }
        ""id"":           # A uniq object Id
        ""actions"":
        [
            {
                ""value"":        # Interaction verb
                ""targetId"":     # Target object Id / entity Id if the interaction has no target
                ""instrumentId"": # Instrument object Id used to achieve the interaction
                                # / entity Id if the interaction has no instrument
            }
        ]
    }
]
}
`
    -trainval_vcocoandtest_vcoco which follows the V-COCO annotation file structure.
Evaluation
To run evaluation and compute the agent AP and role AP, get the V-COCO evaluation code and replace vsrl_eval.py file by h2o_vsrl_eval.py file provided in the H2O.zip file.
This new version allows the evaluation of a list of target objects for a given interaction.
As for the V-COCO evaluation, store your predictions as a pickle file (detections.pkl) in the following format:
[
    {
        'image_id':         # The H²O image name
        'person_box':       # [xmin, ymin, xmax, ymax] the box prediction for the person
        '[action]_agent':   # The score for action corresponding to the person prediction
                            # [action] is a verb from the list provided in H2O_verb_list.json file
        '[action]_role':    # [[x1, y1, x2, y2, s]], list of the predicted boxes for role and 
                            # associated score for the action-role pair
                            # [action] is a verb from the list provided in H2O_verb_list.json file                      
    }
]
To launch the evaluation, run:
```
from h2o_vsrl_eval import VCOCOeval
vcocoeval = VCOCOeval(vsrl_annot_file, coco_file, split_file)
For the original scenario
vsrl_annot_file:  vcocolike_annotations/test_vcoco/interactions_test.json
coco_file:        vcocolike_annotations/test_vcoco/instances_test.json
split_file:       images_test.ids
For the objectness scenario
vsrl_annot_file:  other_annotations/test_vcoco/interactions_test.json
coco_file:        other_annotations/test_vcoco/instances_test.json
split_file:       images_trainval.ids
vcocoeval._do_eval(detections.pkl, ovr_thresh=0.5)
License
Data annotations are under Creative Commons Attribution Non Commercial 4.0 license (see LICENSE file in H2O.zip file).
Evaluation code is under MIT license.
Citation
A. Orcesi, R. Audigier, F. Poka Toukam and B. Luvison, “Detecting Human-to-Human-or-Object (H2O) Interactions with DIABOLO"", 2021 IEEE International Conference on Automatic Face and Gesture Recognition (FG2021), https://arxiv.org/abs/2201.02396
@INPROCEEDINGS{h2o_dataset_2021,
  author={Orcesi, Astrid and Audigier, Romaric and Poka Toukam, Fritz and Luvison, Bertrand},
  booktitle={2021 IEEE International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={Detecting Human-to-Human-or-Object (H2O) Interactions with DIABOLO}, 
  year={2021},
  volume={},
  number={},
  pages={},
  doi={}}
```
Contact
If you have any question about this dataset, you can contact us by email at: h2o@cea.fr"	0	38	0	angeliqueloesch	ho-interaction-dataset
4903	4903	Autism tweets data	Autism tweets dataset from 2008	['universities and colleges', 'healthcare', 'education', 'beginner']	"Autism tweets dataset
Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. The tweets are extracted using Twitter Official API for freelance work.
Attribute Information
author_id: Unique Author id
created_at: Timestamp of tweet created
geo: Geo-coordinates of tweets
id: Unique identifier for individual tweets
lang: Language of a tweet
like_count: like count of a tweet
quote_count: Quote retweet count of a tweet
reply_count: Reply count of a tweet
retweet_count: Retweet count of a tweet
source: Device used for Tweet
tweet: tweet text"	22	403	2	saurabhjoshi24	autism-tweets-data
4904	4904	Detoxify	Toxic Comment Classification with ⚡ Pytorch Lightning and 🤗 Transformers	['nlp']	"About this dataset
Trained models to predict toxic comments on all 3 Jigsaw Toxic Comment Challenges.
https://github.com/unitaryai/detoxify
How to use
python
import transformers
original
model_path = ""../input/detoxify/original""
self.tokenizer = transformers.BertTokenizer.from_pretrained(model_path)
self.encoder = transformers.BertForTokenClassification.from_pretrained(
    f""{model_path}/pytorch_model.bin"",
    config = transformers.BertConfig.from_pretrained(f""{model_path}/config.json"")
).bert
unbiased
model_path = ""../input/detoxify/unbiased""
self.tokenizer = transformers.RobertaTokenizer.from_pretrained(model_path)
self.encoder = transformers.RobertaForSequenceClassification.from_pretrained(
    f""{model_path}/pytorch_model.bin"",
    config = transformers.RobertaConfig.from_pretrained(f""{model_path}/config.json"")
).roberta
multilingual
model_path = ""../input/detoxify/multilingual""
self.tokenizer = transformers.XLMRobertaTokenizer.from_pretrained(model_path)
self.encoder = transformers.XLMRobertaForSequenceClassification.from_pretrained(
    f""{model_path}/pytorch_model.bin"",
    config = transformers.XLMRobertaConfig.from_pretrained(f""{model_path}/config.json"")
).roberta"	0	30	0	maruyama	detoxify
4905	4905	Battels.csv		[]		1	4	0	aftabsama	battelscsv
4906	4906	ump-lgbm-models		[]		1	9	0	sishihara	umplgbmmodels
4907	4907	some employeeDATA	person who works for somebody	['music', 'categorical data', 'data cleaning', 'data visualization', 'corrplot']		0	27	1	amritanshusharma23	some-employeedata
4908	4908	book dataset	written work that is published as printed pages fastened together inside a cover	['research', 'artificial intelligence', 'computer science', 'python', 'r']		1	24	1	amritanshusharma23	book-dataset
4909	4909	demo xml	connected with people or population	['computer science', 'programming']		18	37	1	amritanshusharma23	demo-xml
4910	4910	ID dataset	identification; identity	['categorical data', 'internet', 'statistical analysis', 'dplyr', 'ggplot2']		2	43	1	amritanshusharma23	id-dataset
4911	4911	Rainfall prediction dataset	Rainfall dataset from year 1901 to 2015	['india', 'earth and nature', 'intermediate', 'tabular data']		10	87	6	zwartfreak	rainfall-prediction-dataset
4912	4912	Title_category classification		['business', 'beginner', 'intermediate', 'advanced', 'naive bayes', 'text data']	"This classification dataset emerges the title of the dataset, based on the title the category has to be classified.
A distributor who has billing of the things of his shop to track and wish to categorize the items by putting them in a specific category. Based on the given train test dataset, build a model to put the items in correct category.
with this given dataset what good accuracy can be achieved as I am still finding a way to solve this problem, kindly find this article."	1	30	0	ruchiyadav22	title-category-classification
4913	4913	2022 Capstone Project Bellabeat	Project for Coursera	['education']		0	15	0	vigneshpalanisamy	2022-capstone-project-bellabeat
4914	4914	mymodel		[]		14	16	0	ahmedmsoliman	mymodel
4915	4915	RSNA Normal Images PNG Part2		[]		2	10	0	fereshtej	rsna-normal-images-png-part2
4916	4916	mmcv_full_cpu		[]		0	0	0	riadalmadani	mmcv-full-cpu
4917	4917	4_mtcars		[]		0	1	0	wallacefqq	4-mtcars
4918	4918	Dino Game	Popular dinosaur game in chrome	['games', 'deep learning', 'dnn', 'cnn', 'image data']		0	35	1	darkangel24	dino-game
4919	4919	myubiquant		[]		0	2	0	yus002	myubiquant
4920	4920	Call of Duty - Mobile, Weapons Stats	Weapons impact data to try out the best weapons combo in Call of Duty—Mobile	['video games', 'categorical data', 'beginner', 'intermediate', 'tabular data']	"Context
Call of Duty – Mobile is one of the popular mobile games and contains large amount of weapon sets and combinations. As a great fan of the game, I always wanted to figure out the best weapons and skills combo. The game data wasn't available outside, and going through the video game to collect data of hundreds of weapons wasn't a good option that I could try. Recently, I came across a website which contained posts about weapons in CODM. The data below provided is collected using web-scrapping, the website. Still, the website doesn't contain much of useful information about other data like perks and streaks. I'm trying to figure out a way to get useful data out of these posts.
Content
Basically, there are 3 types of weapons - Primary, Secondary, Throwable.&nbsp;
These types are further categorized into various categories.
We have large verities for Primary weapons since these are the main tool for the player
The Secondary weapon is considered a backup weapon. Like when primary weapon has to reload or run out of ammo, to do a silent kill or when you need to use like rocket launchers, you can use secondary weapon.
You can mainly carry 2 throwables with you, Lethal and Tactical. Lethal can kill the enemies, and Tactical can be used to slow down your enemy.
The columns like Damage, Accuracy, Range, Fire Rate... are self-explanatory. They do not have any specific unit. The higher the value, the better.
Also Included the available images in folder,&nbsp;
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
What are the best attacking weapons combo, with maximum impact
Which are the best Defensive weapons combination to try in Multiplayer modes like Domination and Hard point
Which are the weapons combo, which offers maximum flexibility along with maximum damage for modes like Free-for-All or Gun&nbsp;"	4	58	1	shagalsajid	call-of-duty-mobile-weapons-stats
4921	4921	ridge_toxic_noclean_more_fe_oof		[]		1	16	0	shobhitupadhyaya	ridge-toxic-noclean-more-fe-oof
4922	4922	Automl	up to date automl repo	['computer vision', 'image data', 'tensorflow']		0	70	1	jobayerhossain	automl
4923	4923	Advertising		[]		0	7	0	mdwasimakhtar03	advertising
4924	4924	my advertising 1		[]		0	2	0	kuldipsharma	my-advertising-1
4925	4925	tfrecs_new	TFRecords from PetFinder.my containing images along with categorical features	[]		0	41	0	jannish	tfrecs-new
4926	4926	ubiquant_lightgbm_5folds		[]		16	10	0	hengzheng	ubiquant-lightgbm-5folds
4927	4927	roberta_base_reg		[]		0	3	0	kintaro1	roberta-base-reg
4928	4928	RSNA Normal Images PNG Part1		[]		2	12	0	fereshtej	rsna-normal-images-png-part1
4929	4929	Market Mix Modelling Using Casual Impact Analysis		[]		5	44	0	anjalibakliwal	market-mix-modelling-using-casual-impact-analysis
4930	4930	likefire		[]		0	15	1	hamzehalshehab	likefire
4931	4931	RSNA Abnormal Images PNG 512		['health']		5	69	0	fereshtej	rsna-abnormal-images-png-512
4932	4932	Gameplay Images	10000 images of 10 video games	['games', 'video games', 'beginner', 'intermediate', 'image data']	"This is a dataset of 10 very famous video games in the world.
These include
Among Us
Apex Legends
Fortnite
Forza Horizon
Free Fire
Genshin Impact
God of War
Minecraft
Roblox
Terraria
There are 1000 images per class and all are sized 640 x 360. They are in the .png format.
ThisDataset was made by saving frames every few seconds from famous gameplay videos on Youtube"	27	561	22	aditmagotra	gameplay-images
4933	4933	movedata		[]		0	8	0	leedongchel	movedata
4934	4934	historydata		[]		0	3	0	leedongchel	historydata
4935	4935	ubiquant		[]		2	40	2	ernnnn4u	ubiquant
4936	4936	Urtext 2		['music']		0	1	0	hammadurrahman	urtext-2
4937	4937	Pawpularity_Weight	trained models for petfinder competition	[]		1	32	0	yamash73	pawpularity-weight
4938	4938	yolov5l6_B1_epoch18		[]		3	11	0	dragonzhang	yolov5l6-b1-epoch18
4939	4939	aaptsss		[]		1	2	0	yizhenglin2002	aaptsss
4940	4940	Simple Linear regression dataset		[]		8	38	2	ghadiyaayush	simple-linear-regression-dataset
4941	4941	Loan Defaulters Prediction		[]		6	24	3	ghadiyaayush	loan-defaulters-prediction
4942	4942	Kết quả tuyển sinh DUE (Đà Nẵng) 2021	Kết quả tuyển sinh của trường Đại học Kinh tế Đà Nẵng (DUE) năm 2021.	[]		0	5	0	kwonhoang	kt-qua-tuyn-sinh-due-a-nng-2021
4943	4943	Dare in Reality 		['arts and entertainment']		1	6	4	ghadiyaayush	dare-in-reality
4944	4944	Kaggle_Data		[]		5	14	0	yizhenglin2002	kaggle-data
4945	4945	Song Prediction 5 Stratified Folds		['music']		2	25	2	prikshitsingla	song-prediction-5-stratified-folds
4946	4946	Ubiquant Market Prediction Train In Feather		['finance']		17	166	6	mithilsalunkhe	ubiquant-market-prediction-train-in-feather
4947	4947	axion_250k_mmep		[]		0	10	0	renessmi2017	axion-250k-mmep
4948	4948	Optical_Clear_Resin	OCR material development data set	[]		1	29	1	waterfirst	optical-clear-resin
4949	4949	augmented leaf disease dataset	created image data set of 256x256 jpg's using keras ImageDataGenerator	['earth and nature', 'agriculture', 'classification', 'image data', 'keras']	"Context
while a lot of datasets on leaf disease containing segmented leaf only without background. but when using those data with images with backgrounds models get off balance and skewed to those images which have backgrounds when model user inputs images with backgrounds to the model. to avoid that scenario I have to create a dataset to mix with that segmented data to give more general nature to the model.
Content
this dataset contains 256x256 colour images which are randomly changed t their original properties to create more images form already exists using the Keras image augmentation technique."	8	177	2	asheniranga	augmented-leaf-dataset
4950	4950	covid19deta		[]		0	9	0	qscwdv952	covid19deta
4951	4951	ET Top 500 Indian Companies (2009-2021)	Dataset is about ET.com top 500 Indian companies every year since 2009	['people', 'business', 'finance', 'economics', 'investing']	"<img src=""https://img.etimg.com/photo/msid-83805015,quality-100/graph1.jpg"">
The Economic Times is an Indian English-language business-focused daily newspaper. It is owned by The Times Group. The Economic Times began publication in 1961. As of 2012, it is the world's second-most widely read English-language business newspaper, after The Wall Street Journal, with a readership of over 800,000. It is published simultaneously from 14 cities: Mumbai, Bangalore, Delhi, Chennai, Kolkata, Lucknow, Hyderabad, Jaipur, Ahmedabad, Nagpur, Chandigarh, Pune, Indore, and Bhopal. Its main content is based on the Indian economy, international finance, share prices, prices of commodities as well as other matters related to finance. This newspaper is published by Bennett, Coleman & Co. Ltd. The founding editor of the paper when it was launched in 1961 was P. S. Hariharan. The current editor of The Economic Times is Bodhisattva Ganguli.
The Economic Times is sold in all major cities in India. In June 2009, it launched a television channel called ET Now."	92	521	12	ramjasmaurya	et-top-500-indian-companies-since-2009
4952	4952	Insect Classification		['biology']		6	44	1	harshalkhachane	insect-classification
4953	4953	reef-baseline-fold12-v2		[]		2	28	0	anhnguyen811	reefbaselinefold12v2
4954	4954	ump train picklefile		[]		199	774	36	columbia2131	ump-train-picklefile
4955	4955	496projectASL		[]		3	4	0	eceshell	496projectasl
4956	4956	HR Dashboard 		[]		2	9	0	shaimaamadkour	hr-dashboard
4957	4957	TF2 CenterNet HourGlass104	TensorFlow 2 Detection Model Zoo - CenterNet HourGlass104	[]		0	13	0	duythanhng	tf2-centernet-hourglass104
4958	4958	West Africa And India - Hydropower Dams		['renewable energy']	"Context
A dam is a structure built across a river or stream to hold back water. Dams has been the major source of Power/Electricity production in India as well as across the world. Harnessing the power of nature (water) to produce electricity is a much needed activity to run major economies across the world.
Content
This database contains parameters gathered after an assessment of the technological potential for development of floating solar photovoltaic (PV) projects on existing hydropower dams and other reservoirs, starting with a pilot in FY18 focused on West Africa and India. This dataset has two files as given below - 
- Metadata file - This file has metadata about the dataset and other reference sources to understand more on how the final dataset was collected
- wb_database_dams.csv - This is the dataset file having relevant information about the dams of West Africa and India. Dataset can be used for exploring potential power projects analytics 
Acknowledgements
Dataset is openly made available by World Bank Data Catalog
Inspiration
Let us keep this open for now. I will add them once I start asking questions to this data"	0	17	0	aashaymaheshwari	west-africa-and-india-hydropower-dams
4959	4959	second-hand-car-trade		[]		3	67	0	zymaoo	secondhandcartrade
4960	4960	T1_KDD_MACIEL	TAREA 1 - QUEJAS, SUGERENCIAS, AGRADECIMIENTOS	[]		2	47	0	josemaciel	t1-kdd-maciel
4961	4961	Pseuodo Labels		[]		3	16	0	edwardakalarrywelch	pseuodo-labels
4962	4962	Family_guy		[]		25	38	1	masterbabyyoda	family-guy
4963	4963	International Debt Statistics Jan 2022	International Debt Statistics, successor to Global Development Finance 	['business', 'finance', 'government', 'lending']	"Context
Now in its forty-eighth year, International Debt Statistics (IDS) supports policymakers and analysts by monitoring aggregate and country-specific trends in external debt in low- and middle-income countries. It provides a comprehensive picture of external borrowing and sources of lending by type of borrower and creditor with information on data availability and comparability. To further enhance debt transparency, this year’s report introduces additional features, such additional information on average lending terms by creditor country and the currency composition of debt stock. The Central Bank is also featured separately in the borrower composition along with its debt instruments. In addition, the IDS-DSSI database includes the actual debt service deferred in 2020 by each bilateral creditor and the projected monthly debt-service payments owed to all bilateral creditors for year 2021. The IDS 2022 publication provides a select set of indicators, with an expanded data set available online.
International Debt Statistics (IDS), successor to Global Development Finance and World Debt Tables, is designed to respond to user demand for timely, comprehensive data on trends in external debt in low- and middle-income countries. The World Bank’s Debtor Reporting System (DRS), from which the aggregate and country tables presented in this report are drawn, was established in 1951. World Debt.
Source
https://datacatalog.worldbank.org/search/dataset/0038015/international-debt-statistics
Download Links:
API
Excel
CSV
Metadata
Coverage & Extent
Geographical Coverage : East Asia and Pacific, Europe and Central Asia, Middle East and North Africa, South Asia
Granularity List : National
Temporal Coverage: 1970 - 2027
Periodicity : Annual
Additional Metadata
Acronym                                               IDS
Recommended Citation              International Debt Statistics, The World Bank
Languages Supported                       English
Source Type                                         World Bank Group
Source:                                          International Debt Statistics, The World Bank
Harvest Source                                World Bank Data API
Harvest System ID                                            6
Dates
First Published Date: Dec 19, 2012
Update Frequency: Annual
Update Schedule: October"	82	707	13	meetnagadia	international-debt-statistics-jan-2022
4964	4964	Fault_stats		[]		0	6	0	phngnamdng	fault-stats
4965	4965	Inria Aerial Image Labeling Dataset		['earth and nature']		3	13	0	huanranye	inria-aerial-image-labeling-dataset
4966	4966	Faces Dataset		[]		21	20	0	muhammadhananasghar	faces-dataset
4967	4967	sdcnet+spade synboost		[]		0	8	0	shashwatnaidu	sdcnetspade-synboost
4968	4968	saddleminist784		[]		2	1	0	zmjjiang	saddleminist784
4969	4969	saddlemnist4096		[]		2	2	0	zmjjiang	saddlemnist4096
4970	4970	BASE_finale_csv		[]		2	2	0	emmanueltalba	base-finale-csv
4971	4971	ssl_jigsaw		[]		0	27	0	shanshilei	ssl-jigsaw
4972	4972	15 Years of Inflation (IPC) in Latino America	15 years of historical inflation rates for 18 countries in Latin America	['brazil', 'south america', 'economics', 'mexico']	"Context
Already more than 15 years of history, this compilation of the main results regarding the cost of living in some Latin American countries allows a clearer view of more than a decade of results inflation in the region
Acknowledgements
I collected all these figures from each individual official authority on each of the countries, you could see the data source on the file for each row
Inspiration
I always I was looking for this data, but usually, some places only consolidate estimations or predictions from global financial authorities rather than go directly to the country sources"	119	1375	3	jaforero	inflation-ipc-in-latam
4973	4973	difference		[]		0	2	0	jackmcmenamin	difference
4974	4974	 News articles through text sentiment analysis 	Correlation with Tesla stock price through sentiment analysis of news articles	['news']	"Context
News articles through text sentiment analysis
a study on the relationship and change with the Tesla stock price
Content
Predicting the direction of Tesla's stock price through sentiment analysis of news articles
Inspiration
https://github.com/farooq96/News-Sentiment-Analysis-in-Python"	4	99	0	junghwan22	news-articles-through-text-sentiment-analysis
4975	4975	longformer-cosine-last-model-f0-cv-0.6197		['earth and nature']		0	6	0	atharvaingle	longformer-cosine-last-model-f0
4976	4976	hhhhhh		[]		0	2	0	mhdhd04	hhhhhh
4977	4977	Google Capstone Project - Divvy Bike-Share		[]	"Context
These pre-cleaned datasets are for Google Data Analytics Capstone Project Case Study 1.
Content
In the capstone project, you will work for a fictional company, Cyclistic. The datasets uploaded here are the previous 12 months of trip data from December 2020 to November 2021. The original source of these datasets can be found at https://divvy-tripdata.s3.amazonaws.com/index.html. 
Acknowledgements
The data has been made available by Motivate International Inc. under this license. (https://ride.divvybikes.com/data-license-agreement)"	1	25	0	arielfelices	google-capstone-project-divvy-bikeshare
4978	4978	WHO COVID Data Jan 18 2022	For educational purposes only. Combined case, death, and vaccination data.	['public safety']		13	71	0	alexandrahiggins	who-covid-data-jan-18-2022
4979	4979	email classifier dataset	Spam or not spam email classification	['email and messaging']		4	72	0	williambruno	email-classifier-dataset
4980	4980	jokes dataset		[]		4	21	0	yaroslav62	jokes-dataset
4981	4981	RoombaV2		[]		0	19	0	fghjkgcfxx56u	roombav2
4982	4982	fyptest		[]		0	10	0	asadaliakbar	fyptest
4983	4983	Covid19_tweets	Covid19 tweets 30k English	['classification', 'clustering', 'text data', 'covid19']	"Data collection
The data is collected using tweepy Python package to access Twitter API.
Inspiration
You can perform multiple operations on the covid tweets. Here are few possible suggestions:
Study the subjects of recent tweets about the vaccine made by various producers;
Perform various NLP tasks on this data source (topic modelling, sentiment analysis);"	3	26	1	ziadfellahidrissi	covid19-tweets
4984	4984	Country Data	Basic data for all countries from country.io	['global', 'geography']	"Content
Contains basic JSON data for countries, such as ISO2, ISO3 codes, continents, capitals, phone prefixes and currency codes.
Acknowledgements
Data taken form http://country.io/data, updating monthly."	2242	23558	52	timoboz	country-data
4985	4985	Celebs	Celebs Dataset for testing 	['arts and entertainment', 'celebrities', 'news']		0	8	0	asadaliakbar	testfyp
4986	4986	stanza resources		[]		2	16	0	ganeshn88	stanza-resources
4987	4987	childe		[]		14	16	0	richielleisart	childe
4988	4988	Violence		[]		0	35	0	mehmedbesimdemirhan	violence
4989	4989	Snapchat Ads		[]		3	18	0	hansikumar	snapchat-ads
4990	4990	PublicThings	Just testing with some data	[]		0	30	0	goefft	publicthings
4991	4991	headmodel4		[]		0	15	0	xiwuhan	headmodel4
4992	4992	Orders4		[]		0	1	0	jackmcmenamin	orders4
4993	4993	SARD_YOLO	SARD Dataset converted to YOLOv5 format with the labels	['earth and nature']		5	21	0	kushagrgoyal	sard-yolo
4994	4994	Orders3		[]		0	11	0	jackmcmenamin	orders3
4995	4995	orders2		[]		0	0	0	jackmcmenamin	orders2
4996	4996	Supermarket Data		[]		6	60	1	isaiahamwata	supermarket-data
4997	4997	submit_baseline.csv		[]		1	21	0	donnaontiveros	submit-baselinecsv
4998	4998	Song Shuffling Simulation BHE 		['arts and entertainment']		0	4	0	baileyhigginseaton	song-shuffling-simulation-bhe
4999	4999	Shopify's 2022 Data Science Summer Challenge		['education']		2	25	0	samuelakintajuwa	shopifys-2022-data-science-summer-challenge
5000	5000	datecount3		[]		0	0	0	jackmcmenamin	datecount3
5001	5001	BERT dataset		[]		5	6	0	ajipandas	bert-dataset
5002	5002	Playlist		['arts and entertainment']		0	5	0	ryyu5123	playlist
5003	5003	Playlist		['arts and entertainment']		0	8	0	jonasgirsang	playlist
5004	5004	Fitbit_Data_Partially_Cleaned_2016		[]		0	8	1	sudheenamisra	fitbit-data-partially-cleaned-2016
5005	5005	Class Playlist		['education']		0	3	0	justinguan2	class-playlist
5006	5006	Class Playlist		['education']		0	16	1	yumamcallister	class-playlist
5007	5007	tfgbr-segmentation-v2-3		['business']		0	8	0	ks2019	tfgbr-segmentation-v2-3
5008	5008	tfgbr-segmentation-v2-1		['business']		0	11	1	ks2019	tfgbr-segmentation-v2-1
5009	5009	Playlist		['arts and entertainment']		0	4	0	daluong3	playlist
5010	5010	Online_Retail	The dataset contains the transactional behavior for association rule modeling	['business', 'marketing', 'recommender systems', 'reinforcement learning', 'retail and shopping', 'social networks']	"Content
The file contains: 
InvoiceNo, 
StockCode, 
Description, 
Quantity, 
UnitPrice, 
InvoiceDate, 
CustomerID, 
Country. 
Acknowledgements
The online retail dataset is an open-source dataset that contains transactional behaviors for association rule modeling.
Inspiration
Association Rule Learning: Apriori is one of the powerful algorithms to understand association among the products. Take an example of a supermarket where most of the person buys egg also buys milk and also baking soda. Probably the reason is they want to bake a cake for new year's eve.
So we can see there is an association between eggs, milk as well as baking soda. Now after knowing such association we simply put all the 3 things together in the shelf and that definitely will increase our sales.. Let's find out."	163	1375	10	rupakroy	online-retail
5011	5011	food101_tfrecords	A dataset of 101 food categories with 101000 images	['computer vision', 'image data', 'multiclass classification', 'food']		1	195	0	omarchevska	food101-tfrecords
5012	5012	tfgbr-segmentation-v2-6		['business']		0	14	0	ks2019	tfgbr-segmentation-v2-6
5013	5013	tfgbr-segmentation-v2-4		['business']		0	12	1	ks2019	tfgbr-segmentation-v2-4
5014	5014	tfgbr-segmentation-v2-5		['business']		0	3	0	ks2019	tfgbr-segmentation-v2-5
5015	5015	tfgbr-segmentation-v2-0		['business']		0	11	0	ks2019	tfgbr-segmentation-v2-0
5016	5016	v5m6_7epoch		[]		0	10	0	dragonzhang	v5m6-7epoch
5017	5017	tfgbr-segmentation-v2-2		['business']		0	20	0	ks2019	tfgbr-segmentation-v2-2
5018	5018	Bundestag (German parliament)	Historical data of MPs from first to current legislative period	['europe', 'government', 'politics', 'exploratory data analysis', 'data cleaning', 'data visualization']	"Content
Data of all MPs that served in the Bundestag (German parliament). Includes data for all legislative periods from 1949 to now. Columns contain information such as: name, birth date, marital status, party affiliation, number of terms in the Bundestag, birth place, birth country, occupation etc."	11	123	11	greengamma	bundestag
5019	5019	ACL dataset		[]		0	11	0	ch4itanyap4ndey	acl-dataset
5020	5020	cdm_beta_250k_model_stage1		[]		0	28	0	renessmi2017	cdm-beta-250k-model-stage1
5021	5021	seattle_weather_1948-2017	seattle_weather_1948-2017	[]		1	11	0	tobijoshua	seattle-weather-19482017
5022	5022	Brain tumor dataset includes the mask and images 		['cancer']		9	100	3	tinashri	brain-tumor-dataset-includes-the-mask-and-images
5023	5023	elephant1		[]		0	1	0	sarashahin	elephant1
5024	5024	More ASL Alphabet test pics	Thanks Ciro for sharing your hand for the community	[]		6	30	1	vivianapentangelo	ciropics
5025	5025	homework		['education']		2	11	0	hsiaochingkao	homework
5026	5026	DATECOUNT2		[]		0	5	0	jackmcmenamin	datecount2
5027	5027	nmt_opnmt_dataset		[]		1	6	1	oldbirdaz	nmt-opnmt-dataset
5028	5028	tfgbr-adverserial-val-tfr		[]		0	6	0	ks2019	tfgbr-adverserial-val-tfr
5029	5029	Covid_ECG		[]		1	22	0	ananyagr	covid-ecg
5030	5030	datecount1		[]		0	0	0	jackmcmenamin	datecount1
5031	5031	DATECOUNT		[]		0	2	0	jackmcmenamin	datecount
5032	5032	IMDb Movie Reviews Preprocessed	Preprocessed version of IMDb 50K movie reviews dataset for Sentiment Analysis	['movies and tv shows', 'classification', 'text data', 'binary classification', 'nltk']		30	189	1	ayanwap7	imdb-movie-reviews-preprocessed
5033	5033	height_image		[]		0	3	0	johnfrancis1995	image
5034	5034	Prophet Easy Function	Prophet Easy Function(TimeSeries Prediction)	['beginner', 'time series analysis', 'datetime']	"usage) 
DATA_PATH = ""https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv""
DS_COL = 'Month'
DATA_COL = 'Sales'
model,new_df = makeProphetModel(DATA_PATH,DS_COL,DATA_COL)"	5	253	8	rhythmcam	propheteasyfuction
5035	5035	nmt_ende_bpe_out		[]		0	1	0	oldbirdaz	nmt-ende-bpe-out
5036	5036	homework_cvpr		[]		2	31	0	jianzhang1111	homework-cvpr
5037	5037	photos to test		[]		0	0	0	ngnishchay191	photos-to-test
5038	5038	Tinkers construct 	Material Stats and Traits for all the materials involved in Tinker's Construct.	['software']	"Introduction
In search of Tinkers data to decide the best possible weapon in my Minecraft server I decided to create this dataset to be used in applications to better the selection process and to help perform analytics on the materials.  
What to expect
The data has the stats and traits of most of the materials on parts like head, extras and handles.
Inspiration
In search of the best possible weapon in a modded Minecraft world. Thats why i made this --- https://github.com/PranavMurali/Tinkers-Selector
https://tinkerer-ashen.vercel.app/ , be sure to check it out!
refer to https://tinkers-construct-2.fandom.com/wiki/Material_Stats"	2	218	9	pranavmurali1	tinkers-construct
5039	5039	Brain tumor segmentation	Segmentation with architecture	['cancer']		3	72	1	tinashri	brain-tumor-segmentation-datasets
5040	5040	foodtext	food_Texture refers to those qualities of a food that can be felt with teeth)	['categorical data', 'artificial intelligence', 'tabular data', 'image data', 'text data']	"Context
Nothing much 
There's a story behind every dataset and here's your opportunity to share yours.
Content
About Food texture 
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
Soverssingh
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
From the college 
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	9	117	5	amritanshusharma23	foodtext
5041	5041	Naruto Shippuden IMDb ratings	IMDb ratings along with number of IMDb votes and Japanese episode air date	['movies and tv shows', 'tabular data', 'text data', 'anime and manga']	"NARUTO SHIPPUDEN
Naruto Shippuden is a famous anime popular in the youths all over the world. Set up in the ninja/shinobi world, this anime focuses on the story of an ambitious  shinobi Naruto Uzumaki, a citizen of Konohagakure, who dreams to be the Hokage of the village hoping that the villagers who had shunned him because of the demon fox spirit sealed inside him, would someday acknowledge him as the individual he is.
Content of data
The data consists of following columns:
episode_number _overall : The overall episode number of the anime
season : Season number of the episode
episode_number _in _ season : The episode number within the corresponding season
Title : Title of the episode
Directed by : Director of the episode
Written by : Writer of the episode
original_air _date : The original Japanese air date of the episode
english_air _date : The English air date of the episode (Only available upto episode 370 since the English dub versions of episodes 371-500 were not aired at the time of data creation)
rating : IMDb rating of each individual episode
votes : Number of votes on each episode on IMDb
description : Short description about the contents of the episode
Data sources
The data is obtained using web scraping from Wikipedia and IMDb website pages of Naruto Shppuden as given below : 
Wikipedia Naruto Shippuden episode list webpage
IMDb Naruto Shippuden webpage
Thank you
Data last updated on : January 18, 2022
Version History
Version 1  (Uploaded on January 9, 2022) : (Now deleted)
Version 2 (Uploaded on January 18, 2022) : Earlier version had English episode air dates only up to episode 368 whereas this version has the English episode air dates up to episode 370. Also the earlier missing Director and Writer names for some episodes have been added in this version."	59	655	3	greninja1729	naruto-shippuden-imdb-ratings
5042	5042	fruitobject		[]		0	1	0	ansuld	fruitobject
5043	5043	membership inference on cifar and gpt-2	semester project at EPFL's SPRING lab	['education']	"Adversarial features extracted from ResNet20 models on cifar-10 and cifar-100.
Log-perplexities from GPT-2 fine-tuned on WikiCorpus dataset"	0	6	0	eliottzemour	mia-cifar-gpt2
5044	5044	torchez	PyTorch Wrapper for easy reproducibility 	['earth and nature']		4	80	4	kishalmandal	torchez
5045	5045	unbiased-toxic-roberta		[]		0	17	0	shawndong98	unbiasedtoxicroberta
5046	5046	checkpointsanime		[]		0	2	0	ravinash218	checkpointsanime
5047	5047	COTS-YOLOv5-weights		[]	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	7	36	1	myintzu	cotsyolov5weights
5048	5048	xml-label	a car plate xml dataset	['law', 'computer science', 'programming']		6	19	0	vainglorylx	xmllabel
5049	5049	data2-0		[]		0	3	0	xxbxhj	data20
5050	5050	saved_model		[]		0	6	0	runxiaudreyzhao	saved-model
5051	5051	DATASET		[]		1	6	0	adifahmi	dataset
5052	5052	zhangiii		[]		0	0	0	zhangtianwei	zhangiii
5053	5053	CNN_Saved		[]		0	6	0	quickfingers555	cnn-saved
5054	5054	zhangtianwei123		[]		0	0	0	zhangtianwei	zhangtianwei123
5055	5055	wuehqownoqw		[]		0	1	0	zhangtianwei	wuehqownoqw
5056	5056	zhangtianwei		[]		0	1	0	zhangtianwei	zhangtianwei
5057	5057	MBA - 1 - Data Wrangling		[]		6	72	1	curiel	mba-1-data-wrangling
5058	5058	finalHugo		[]		0	3	0	ppdolphinyy	finalhugo
5059	5059	stable-baseline-offline		['software']	https://github.com/Stable-Baselines-Team/stable-baselines	1	10	1	kirderf	stablebaselineoffline
5060	5060	TF1.15OfflineInstall		[]		2	4	0	kirderf	tf115offlineinstall
5061	5061	Gast-0.2.2		[]	https://github.com/serge-sans-paille/gast/	3	6	0	kirderf	gast022
5062	5062	Electrical Product Sample Sales Data	 500 Thousand Rows of Sales Data	['business']		29	124	4	muratmutlubi	electrical-product-sample-sales-data
5063	5063	2021 GreatBarrierReef Prep Data	my preprocessed data for the GreatBarrierReef Competition	['earth and nature', 'computer science', 'video data', 'standardized testing']	"Context
This data was collected and preprocessed from the Kaggle Competition TensorFlow - Help Protect the Great Barrier Reef (link: https://www.kaggle.com/c/tensorflow-great-barrier-reef)."	139	1953	22	andradaolteanu	2021-greatbarrierreef-prep-data
5064	5064	SalamFoodNet	Image Dataset of 9 types of food items for use in Convolutional Neural Networks	['computer science']		3	40	1	abdulsalamaziz	salamfoodnet
5065	5065	Movies (TMDB)	Movies Recommendation	['movies and tv shows']		4	23	0	kkqwerty	movies-tmdb
5066	5066	Airline fare prediction	Multiple attributes of airlines	['automobiles and vehicles', 'intermediate', 'tabular data', 'regression']	"Context
For those who are curious to understand how the pricing differs of different airlines, why they don't have the same pricing then this dataset is best for you and I am one of you xD.
Content
It consists of multiple attributes like route, stoppage, etc to provide you with everything related to airlines.
Acknowledgements
Thanks to you!
Inspiration
Find an effective way to predict the price/fare of airlines."	412	2497	19	zwartfreak	airline-fare-prediction
5067	5067	Retail Dataset	This data is data on retail stores	['business']		3	64	0	novitasaril	datastokbarang
5068	5068	piclan		[]		0	58	0	shamankovnikolay	piclan
5069	5069	TMDB Top Rated Movies	TMDB Top Rated Movies	['movies and tv shows']		1	9	1	arjunachu	tmdb-top-rated-movies
5070	5070	dataset		[]		9	21	0	younisbashir	dataset
5071	5071	TITANIK		[]		3	21	0	qwerty0214	titanik
5072	5072	greeen		[]		0	2	0	subinjang	greeen
5073	5073	GBR COTS dataset for TPU		['internet']	"Context
Dataset for TPU for TensorFlow - Help Protect the Great Barrier Reef competition 
Content
TFrecords for train, validation split. 
Model checkpoint and config for reference.
This dataset may change or be deleted, made public temporarily for testing with TPU using KaggleDatasets().get_gcs_path.
Private dataset requires Kaggle Secrets credentials - see https://www.kaggle.com/product-feedback/163416 for more details on this. 
Acknowledgements
TFrecords - Based on https://www.kaggle.com/khanhlvg/tensorflow-prepare-cots-dataset-for-tpu
TF Models see https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md 
Inspiration
Using TPU for competition."	0	16	1	something4kag	gbr-cots-dataset-for-tpu
5074	5074	detoxify		['health']		5	48	2	atamazian	detoxify
5075	5075	Swiggy restaurants dataset		[]		7	33	0	praveenmzzzz876	swiggy-restaurants-dataset
5076	5076	yolox_models		[]		0	2	0	gaozhi0202	yolox-models
5077	5077	house-prices-advanced-regression-techniques 		['earth and nature']		3	44	0	deepaksethi	housepricesadvancedregressiontechniques
5078	5078	USA Indeed Job Data	This dataset includes job data from Indeed USA	['jobs and career']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 3933743  Domain Name : indeed.usa  Date Range : 01st Apr 2021 - 30th Jun 2021   File Extension : ldjson
Available Fields : uniq_id, crawl_timestamp, url, job_title, category, company_name, city, state, country, post_date, job_description, job_type, apply_url, job_board, geo, job_post_lang, inferred_iso2_lang_code, is_remote, test1_cities, test1_states, test1_countries, site_name, html_job_description, domain, postdate_yyyymmdd, predicted_language, inferred_iso3_lang_code, test1_inferred_city, test1_inferred_state, test1_inferred_country, inferred_city, inferred_state, inferred_country, has_expired, last_expiry_check_date, latest_expiry_check_date, dataset, postdate_in_indexname_format, segment_name, duplicate_status, fitness_score  
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud, DataStock and live job data from JobsPikr.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	34	446	5	promptcloud	usa-indeed-job-data
5079	5079	Football Data : Top 5 Leagues	Football Data From Top 5 leagues across europe (2014-2020)	['football', 'beginner', 'intermediate', 'exploratory data analysis', 'data visualization']		397	2368	21	sanjeetsinghnaik	football-data-top-5-leagues
5080	5080	amazonReviews		[]		9	21	5	mammadabbasli	amazonreviews
5081	5081	The Benefits of Invention Help From a Third Party		['law']	"Many inventors find themselves in need of Invention Help. A third party can provide valuable guidance and provide a roadmap for their invention. A third party can provide the necessary financial resources, and may have years of experience working with inventors. Having a third party can make all the difference in a new inventor's success, and they can also help protect their intellectual property. Read on to learn more about the benefits of hiring an invention help firm, go here https://vinnews.com/2022/01/01/get-help-with-patent-protection-from-inventhelp/ for invent help.
First, it's important to protect your intellectual property. Your idea is unique and will stand the test of time, so you need to protect it. Invention help from an IP attorney will ensure that your idea is protected. If it's worth patenting, a patent attorney will advise you on the best strategy to pursue. These attorneys are experts at patenting and can help you get the most out of your idea. They won't try to charge you monthly fees, and they'll never try to rip you off, here about how to file a patent with InventHelp
After your idea has been protected, it's time to start marketing your invention. A patent attorney will conduct a search of your idea and can tell you how to proceed. They won't try to sell you a monthly subscription service, and won't try to scam you out of thousands of dollars. A patent search will help you determine how unique your idea is, and will help you decide if you should pursue it or not.
An IP attorney is another important resource for new inventors. A good IP attorney can help you with patenting your idea. An IP attorney will conduct a patent search on your idea. If your idea is already in the public domain, an IP attorney can guide you on the best course of action. With an attorney's expertise, you can be sure that your idea has the potential to be patented. You can look forward to increased chances of success if you hire a good patent lawyer.
In addition to patenting your invention, it's essential for you to market and promote your idea. A good IP attorney will do more than just file your patent. They will help you get your idea noticed by the right people. This can be vital for your career. You can contact an IP attorney by visiting a local law office. If your idea is not patented, you might need to consider working with an IP attorney. Your IP attorney can perform a patent search on your behalf, and will not charge you a dime.
How to patent a product with InventHelp and can also help you protect your intellectual property. The legal assistance of a patent attorney is vital for an inventor to protect their idea. It is essential to hire an attorney if you want to ensure that your idea is protected. An IP attorney will be able to run a patent search on your ideas and will advise you on the best course of action. A professional patent attorney can help you protect your intellectual property. If you hire an IP lawyer, you can also receive advice and counsel on marketing and promoting your invention."	0	27	0	sashagreg	the-benefits-of-invention-help-from-a-third-party
5082	5082	detoxify_models		[]	"Source
https://github.com/unitaryai/detoxify
Usage
Here's an example for the 'original' model:
!mkdir -p /root/.cache/torch/hub/checkpoints/
!cp ../input/detoxify-models/toxic_original-c1212f89.ckpt /root/.cache/torch/hub/checkpoints/toxic_original-c1212f89.ckpt"	1	38	2	atamazian	detoxify-models
5083	5083	Diabetes_Classification		[]		0	12	0	faezeparsa	diabetes-classification
5084	5084	yolox_kaggle_fix_for_demo_inference1		[]		0	7	0	gaozhi0202	yolox-kaggle-fix-for-demo-inference1
5085	5085	TGBR yolov5 video2 dataset		['earth and nature']		5	9	0	gmhost	tgbr-yolov5-video2-dataset
5086	5086	yolox-kaggle-fix-for-demo-inference		['computer science']		2	7	0	zengkuikui	yoloxkagglefixfordemoinference
5087	5087	demo_to_test		[]		0	0	0	gaozhi0202	demo-to-test
5088	5088	Car Park Dataset	Car Park datasets contains randomly generate data using Faker package in Python	['automobiles and vehicles', 'exploratory data analysis', 'data cleaning', 'data visualization', 'image data']		10	166	7	cankatsrc	car-park-dataset
5089	5089	Dogecoin Historical Data	Historical Price Data of Dogecoin	['education', 'finance', 'economics', 'internet', 'currencies and foreign exchange', 'news']	"Introduction
Dogecoin is an open source peer-to-peer digital currency, favored by Shiba Inus worldwide. It is qualitatively more fun while being technically nearly identical to its close relative Bitcoin. This dataset contains its historical stock price in USD on a daily frequency starting from 17 September 2014.
For more information refer to https://dogecoin.com/
Credits
Image Credits: Unsplash - claybanks"	1876	13115	71	dhruvildave	dogecoin-historical-data
5090	5090	Dataset for Predicting the Happiness Score		['research', 'global', 'intermediate', 'tabular data', 'pandas']		7	53	0	yashyk	dataset-for-predicting-the-happiness-score
5091	5091	USA Study Visa Consultants in Chandigarh		['universities and colleges']	By any university ranking, American colleges are among the best in the world, seamlessly mixing academics, research, and extracurricular activities. We as USA Study Visa Consultants in Chandigarh will assist you with admission to such universities. Although the American education system draws a large number of overseas students, colleges have a high student satisfaction rate due to the numerous student services they offer. Your qualification is recognized all over the world. Anyone, especially future employers, will be impressed if you complete your degree or even a portion of it in the United States. Higher education in the United States is regarded for producing highly qualified professionals; employers will see you as a candidate with an international mentality and a problem solver. In the United States, there is a clear gap between education and employability, with companies viewing universities and colleges as gatekeepers of worker talent while those same institutions fail to prioritize job skills and career preparedness.	1	16	0	flytouchoverseas	usa-study-visa-consultants-in-chandigarh
5092	5092	predict_iris		[]		0	13	0	wallacefqq	predict-iris
5093	5093	OWMmod_3		[]		1	0	0	ding314	owmmod-3
5094	5094	viet clean air parnertship	mạng lưới không khí sạch việt nam	[]	viet clean air parnertship	0	131	0	gridog	wordpress-content
5095	5095	OWM_mod_3		[]		0	1	0	ding314	owm-mod-3
5096	5096	eightclassweight		[]		0	0	0	sodhana	eightclassweight
5097	5097	2_airmiles		[]		0	13	0	wallacefqq	2-airmiles
5098	5098	state-funded influenza vaccines in Kaohsiung		['public safety']		0	16	0	hungjing	statefunded-influenza-vaccines-in-kaohsiung
5099	5099	income		['business']		0	17	0	subinjang	income
5100	5100	raihanabdila		[]		4	47	1	raihanafdilah	raihanabdila
5101	5101	First time		[]		0	2	0	steveneuyung	first-time
5102	5102	ft_mohen_rbt		[]		0	3	0	hangy132	ft-mohen-rbt
5103	5103	Swiggy restaurants in Metro-Cities Dataset		['restaurants']		8	26	0	praveenmzzzz876	swiggy-restaurants-in-metrocities-dataset
5104	5104	longformer-baseline-sfold-f4-cv-0.6218		[]		0	4	0	atharvaingle	longformer-baseline-sfold-f4
5105	5105	Swiggy Metro-Cities Dataset		[]		5	22	0	praveenmzzzz876	swiggy-metrocities-dataset
5106	5106	OWM_mod_3_xsp		[]		0	0	0	ding314	owm-mod-3-xsp
5107	5107	Swiggy Dataset of the Metro Cities		[]		4	26	0	praveenmzzzz876	swiggy-dataset-of-the-metro-cities
5108	5108	Swiggy Bangalore Restaurants		['restaurants']		7	46	0	infoanalysis	swiggy-bangalore-restaurants
5109	5109	maskrcnn1		[]		0	0	0	ansuld	maskrcnn1
5110	5110	Vivo phones prices from Flipkart		[]		2	6	0	vivekjain111	vivo-phones-prices-from-flipkart
5111	5111	maskrcnn2		[]		0	0	0	ansuld	maskrcnn2
5112	5112	onesqiyu		[]		0	1	0	ansuld	onesqiyu
5113	5113	imageref		[]		0	1	0	harsh1040puri	imageref
5114	5114	bmpimg		[]		0	0	0	harsh1040puri	bmpimg
5115	5115	Crime Data - Auto Clustering 	The famous UCI dataset is used to demonstrate the auto clustering method	['clustering', 'tabular data', 'optimization', 'automl']		40	283	5	rupakroy	crime-data
5116	5116	transaksi_parfum		[]		1	13	0	muhamadkomarhidayat	transaksi-parfum
5117	5117	roberta_weights		[]		1	11	0	mlcovidresearch	roberta-weights
5118	5118	embedding		[]		0	4	0	bholanath84	embedding
5119	5119	yolov4		[]		0	18	0	vainglorylx	yolov4
5120	5120	pets photo		[]		0	9	0	jennishw	pets-photo
5121	5121	Story_viability		[]		0	6	0	ankushkamble	story-viability
5122	5122	Yearly Sales - Lstm Multistep	Apply Lstm for multistep forecasting of sales or any data using this template	['computer science', 'time series analysis', 'lstm', 'tabular data', 'datetime']		11	167	2	rupakroy	lstmmultistep
5123	5123	jigsaw2021-wat080-data		[]		0	1	0	wataoka	jigsaw2021-wat080-data
5124	5124	shandongecgyuzhi_posweight_labelsm_dropout1		[]		1	3	0	buptxj	shandongecgyuzhi-posweight-labelsm-dropout1
5125	5125	jigsaw2021-wat074-data		[]		0	1	0	wataoka	jigsaw2021-wat074-data
5126	5126	shandongecgyuzhi_posweight_labelsm_dropout0		[]		1	4	0	buptxj	shandongecgyuzhi-posweight-labelsm-dropout0
5127	5127	jigsaw2021-wat073-data		[]		0	2	0	wataoka	jigsaw2021-wat073-data
5128	5128	jigsaw2021-wat078-data		[]		0	8	0	wataoka	jigsaw2021-wat078-data
5129	5129	book_books_2022		[]		0	1	0	bimoldanmagzhan	book-books-2022
5130	5130	feepayment		[]		1	0	0	abhimanyujena	feepayment
5131	5131	ParSQuAD	The first Persian QA dataset	['business', 'advanced', 'nlp', 'deep learning', 'text data', 'json']		2	20	1	neginabadani	parsquad
5132	5132	RNA data preprocessing toolkits		['biology']		8	92	7	ratthachat	rna-data-preprocessing-toolkits
5133	5133	jigsaw2021-wat062-data		[]		0	1	0	wataoka	jigsaw2021-wat062-data
5134	5134	vode11		[]		0	24	0	zhenhoblngjia	vode11
5135	5135	pathology_wsi_capstone		[]		132	43	1	lenicejackson	pathology-wsi-capstone
5136	5136	split fold [cots]		[]		2	38	0	hieuhq	split-fold-cots
5137	5137	yolov5_package		[]		0	8	0	vincentwang25	yolov5-package
5138	5138	University of Ottawa Workout Session Demand	Workout session demand for University of Ottawa gym facilities	['exercise']	"Context
The squash courts at uOttawa were being used through COVID for extra workout space
I was told the squash courts would be re-opened when the demand for workout sessions could be met
This led me to start tracking the demand for workout sessions
The squash courts were re-opened late November 2021 for the first time since the beginning of the pandemic.
However, all gym facilities in Ontario are now closed due to new restrictions in January 2022"	11	154	1	danielholmes	university-of-ottawa-workout-enrolment
5139	5139	fitbitdata		[]		0	4	0	yutaparks	fitbitdata
5140	5140	SAHI_0.8.22		[]		0	16	0	vincentwang25	sahi-0822
5141	5141	boosted-cache		[]		4	8	1	chasembowers	boostedcache
5142	5142	newdata		[]		2	16	0	hnghilong	newdata
5143	5143	seq-classifiers		['biology']		3	13	1	chasembowers	seqclassifiers
5144	5144	hands plam		[]		0	5	0	haohaoxuexiba	hands-plam
5145	5145	Harry Potter Movies Dataset	Scripts/Transcripts of the Harry Potter movies saga	['movies and tv shows']	"Harry Potter movies datasets
Content
This repo contains scripts/transcripts of the Harry Potter movie saga.
<h3>movies.csv</h3>
This file contains several infos of the movies

movie: movie name
released_year: the year of the movie release
running_time: the running time of the movie in minutes
budget: budget of the movie in $
box_office: movie box office in $
<h3>movies.csv</h3>
This file contains dialogs of the movie

movie: movie name
chapter: chapter of the movie according to the script
character: character speaking
dialog: dialog of the character speaking
Notes
I'm not totally sure that the scripts are 100% complete and sometimes scenes from the script did not make it to the movie.
Feel free to notify if you see missing parts !"	456	2580	8	kornflex	harry-potter-movies-dataset
5146	5146	loss-model-3		['clothing and accessories']		0	25	0	kookheejin	lossmodel3
5147	5147	TikTok - Data from most followed accounts (2022)	12k+ videos from the 59 top TikTok accounts	[]	"Schema description:
- internal_id: bigint, not null
- account_name: character varying, not null
- tiktok_id: bigint, not null
- description, character varying, not null
- comment_count, integer, not null
- digg_count, integer, not null
- play_count, integer, not null
- share_count, integer, not null
- create_time, timestamp without time zone, not null 
- saved_time, timestamp without time zone, not null
Columns available:
- account_name: name of the TikTok account
- tiktok_id: ID of the TikTok video
- description: title / description for the video / post
- comment_count
- digg_count: number of likes / reactions
- share_count
- create_time: when the video was posted to TikTok
Metadata:
- internal_id: row number in the database
- saved_time: when the video was saved to database"	25	100	1	moraesvic	tiktok-data-from-most-followed-accounts-2022
5148	5148	swin_transformer_tf	SwinTransformer in Tensorflow 2.x	['computer science', 'programming', 'computer vision', 'classification', 'tpu', 'gpu', 'transformers']	"Swin Transformer (Tensorflow)
Tensorflow reimplementation of Swin Transformer model. 
Based on Official Pytorch implementation.
Requirements
tensorflow &amp;gt;= 2.4.1
Pretrained Swin Transformer Checkpoints
ImageNet-1K and ImageNet-22K Pretrained Checkpoints
| name | pretrain | resolution |acc@1 | #params | model |
| :---: | :---: | :---: | :---: | :---: | :---: |
|swin_tiny_224 |ImageNet-1K |224x224|81.2|28M|github|
|swin_small_224|ImageNet-1K |224x224|83.2|50M|github|
|swin_base_224 |ImageNet-22K|224x224|85.2|88M|github|
|swin_base_384 |ImageNet-22K|384x384|86.4|88M|github|
|swin_large_224|ImageNet-22K|224x224|86.3|197M|github|
|swin_large_384|ImageNet-22K|384x384|87.3|197M|github|
Examples
Initializing the model:
python
from swintransformer import SwinTransformer
model = SwinTransformer('swin_tiny_224', num_classes=1000, include_top=True, pretrained=False)
You can use a pretrained model like this:python
import tensorflow as tf
from swintransformer import SwinTransformer
model = tf.keras.Sequential([
  tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=""torch""), input_shape=[*IMAGE_SIZE, 3]),
  SwinTransformer('swin_tiny_224', include_top=False, pretrained=True),
  tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])
If you use a pretrained model with TPU on kaggle, specify `use_tpu` option:python
import tensorflow as tf
from swintransformer import SwinTransformer
model = tf.keras.Sequential([
  tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=""torch""), input_shape=[*IMAGE_SIZE, 3]),
  SwinTransformer('swin_tiny_224', include_top=False, pretrained=True, use_tpu=True),
  tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])
Example: TPU training on Kaggle
Citation
```
@article{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}"	5	435	1	kozistr	swin-transformer-tf
5149	5149	US bikeshare datanalysis	bikeshare project NYc, Chicago and Washington	[]	"Bikeshare Project
In this project we explore data related to bike share systems for three major cities in the United States—Chicago, New York City, and Chicago, we use data provided by&nbsp;Motivate, a bike-share system provider, to uncover bike share usage patterns.
It is a two-phase project, the First about filtering and loading the specified data, and the Second about analyzing data considering four main categories:
* Popular times of travel
* Popular stations and trip
* Trip duration
* User information         
During working on the project, there were some points I've searched and found 
different methods to deal with them and I have applied them in different parts of the script, there are some points treated with different methods for the same result like in get_filter function and in duration_trip function."	0	8	1	modhelal	us-bikeshare-datanalysis
5150	5150	ADP_KR_p1		[]		0	7	0	doeungim	adp-kr-p1
5151	5151	TPS Jan 2022 GDP Data Long Format		[]		6	37	2	ifashion	tps-jan-2022-gdp-data-long-format
5152	5152	Playlist		['arts and entertainment']		0	3	0	jasondmurphythomas	playlist
5153	5153	SLIM SHOCK		[]		0	10	0	duriani	slim-shock
5154	5154	model_best		[]		1	12	0	helloggfss	model-best
5155	5155	immigration		[]		4	32	0	ivymarie	immigration
5156	5156	Preferred Interpersonal Distances	Compare preferred interpersonal distances across the world	[]	"Context
These data were collected by Sorokowska et al. (2017), for the paper called: ""Preferred Interpersonal Distances: A Global Comparison."" You can find more information about the data by reading their research paper. You can also find the full reported data here.
Content
The data reported here is just the mean distance reported to strangers, acquaintances, and someone close. 
Acknowledgements
All thanks should be given to Sorokowska et al. (2017)."	3	15	0	tylerbonnell	preferred-interpersonal-distances
5157	5157	Z boson mass		['physics']		1	15	1	dabansherwani	z-boson-mass
5158	5158	NVIDIA Apex		[]		1	1471	0	syoya1997	nvidia-apex
5159	5159	Text Cleaning & Profanity Detection		[]		2	13	0	ryanrudes	text-cleaning-profanity-detection
5160	5160	scRNASeq processed cell line datasets		[]	"Set of scRNASeq datasets - processed data, ready to be used to test some algorithms.
The title is misleading - it reflects only the first file in the collection"	3	95	2	andreizinovyev	chla9-renorm-proch5ad
5161	5161	nsk_image_search3_man5		[]		1	3	0	motono0223	nsk-image-search3-man5
5162	5162	BaselineResult		[]		0	4	0	axzhang	baselineresult
5163	5163	Board Game Database from BoardGameGeek	22k Games, 411K users, 19mil ratings - for EDA, modeling, recommender systems	['games', 'board games', 'intermediate', 'exploratory data analysis', 'recommender systems', 'linear regression']	"Context
Feature-rich multi-table dataset full of interesting information about Board Games which can be used for tasks such as exploratory EDA, predictive modeling, or recommender systems. 
Content
This data set includes nine potential files for exploration and/or modeling. The files are as follows:
GAMES - the basic information file with 47 features about each of 22k board games. Primary key is BGGId which is the BoardGameGeek game id.
RATINGS_DISTRIBTION - includes full ratings distribution for each BGGId
THEMES - table of themes for each BGGId
MECHANICS - table of mechanics with binary flags per BGGId
SUBCATEGORIES - table of subcategories with binary flags per BGGId
ARTISTS_REDUCED - gives artist information for each BGGId. This file is reduced to artists with &gt;3 works, with a binary flag indicating the game included an artist with &lt;= 3 works
DESIGNERS_REDUCED - gives designer information for each BGGId. This file is reduced to designers with &gt;3 works, with a binary flag indicating the game included a designer with &lt;= 3 works
PUBLISHERS_REDUCED - gives publisher information for each BGGId. This file is reduced to publishers with &gt;3 works, with a binary flag indicating the game included a publisher with &lt;= 3 works
USER_RATINGS - has all ratings for all BGGId with username. There are over 411k unique users and ~19 million ratings. Use in recommender system.
Inspiration
This dataset has potential for a wide range of tasks for the novice to the expert. 
- Use the GAMES data set in conjunction with one or more of the themes, mechanics, artists etc tables to perform EDA on the past and current state of board gaming. 
- Use modeling to develop insights about games for the future, or unlock existing BGG ""mysteries"" (Can you crack BGG's bayes weighted average formula?). 
- For the deepest challenge, utilize the included user ratings to develop a recommender system, either via Content Recommendation or Collaborative Filtering.
Like this dataset? See my other datasets!
Acknowledgements
Sources: 
BoardGameGeek:  https://boardgamegeek.com/
All data was sourced and shared in accordance with the XML API Terms of Use: https://boardgamegeek.com/wiki/page/XML_API_Terms_of_Use"	269	2375	26	threnjen	board-games-database-from-boardgamegeek
5164	5164	better autocorrect		[]		0	3	1	kennyxie	better-autocorrect
5165	5165	stockStatistics&Fundamentals_Latest_Quarter_2021		['finance', 'economics', 'investing']		0	32	0	smc345	stockstatisticsfundamentals-latest-quarter-2021
5166	5166	Branch Business Hours		['banking']	"Instruction
This is an example of branch operational hours list. Use this and the high-level information provided by fictional stakeholder below to ""role play"" Business Requirement gathering process.
Business Analyst Interview / Role Play
Report Name: Branch Business Hours Report
- Background:
CyberBank is a brick-and-mortar bank providing consumer banking products and services. The types of products and services provided are checking accounts, saving accounts, CD accounts, and bill payment.
- Problem Statement:
The growth of CyberBank has been phenomenal and the bank currently has more than 1000 branches across Canada. To keep track of branch locations and operational hours, we would like to create a recurring report. The main users of this report will be the branch managers and the service & operations team. The main information needed would be the branch name and transit, the region and district the branch belong to, the main contact people in the branch (usually the branch manager and assistant manager’s name and contact), the branch detailed physical address and branch phone number, the branch open and close hours and whether it is open workdays only (5 days/week) or also open on weekends (6 days/week or 7 days/week). 
- Source of Information:
The report will require two main data sources. The branch information such as the operational hours will need to be pulled from the branch intranet system. The employee information such as the job role (branch manager, assistant manager) and employee contact details (phone and email) will need to be pulled from the HR system.
- The Ask:
The first step of the project is to identify and document the requirements for this branch business hours report.
Business Analysis Best Practice - What to keep in mind?
Here are a few guidelines for conducting business requirements gathering:
- Keep Enterprise Analysis in mind - remember to think about the big picture, the current infrastructure and how this solution must fit into it.
- Find out the Business rules - understand the existing business process. E.g. CyberBank footprint, how it operates, etc.
- Find out the Process steps - first you have to know the as-is before you can focus on the to-be. E.g. how do the stakeholder currently getting this type of information, etc.
- Find out ""what"" you need it to do. Based on the “what” later you will need to discuss with the IT team and decide on ""how"" it will be done.
Think about the requirement from multiple different perspectives:
- Regulatory Requirements. If applicable, consider from both inside and outside of the company
- Business Requirements. Generally business requirements are driven by:
1. Strategic need - mostly qualitative, these are big words like ""superior service"" and ""lightning speed"". Get your stakeholder to quantify him/herself.
2. Tactical need - these focus on integration with other systems.
3. Operational need - these focus on management's needs.
- User Requirements
1. Who the users are, who the stakeholders are.
2. The black box experience for the user. Less on how it will do what it needs to do, focus more on what does it need to do for the user.
3. What results or value immediately visible to the end-user. E.g. how the client will use this report, how frequent it is needed, etc.
4. “In-Scope” vs “Not In-Scope”. E.g. geography or area of focus, what if there are new branches planned to be opened but is not officially open yet, what to do with old branches which are no longer operating, what will the report show if the branch manager position is vacant, etc.
5. Functional - This is a discrete requirement that focuses on a function as it relates to a business result or deliverable. E.g. purpose, use case/user story, prototype of actual output, the detailed column-by-column data mapping, etc.
6. Non-functional - Other non-functional requirements such as usability, reliability, etc.
Above all remember - each requirement must be visible (if not written does not exist), accountable (someone must agree to it), specific (how far, how fast, how soon), measurable (how will we know when the requirement is met). and rated (how important is this requirement)."	4	40	1	lissadora	branch-business-hours
5167	5167	Historical BTC Prices [01/05/2021 — 31/08/2021]		['economics', 'time series analysis', 'statistical analysis', 'currencies and foreign exchange', 'IPython']		0	12	0	imranurabid	historical-btc-prices-01052021-31082021
5168	5168	cats_dogs_data		[]		0	1	0	alexandersaquinga	cats-dogs-data
5169	5169	darknet		[]		0	3	0	drew1984	darknet
5170	5170	DBpedia		[]		1	2	0	chiritacatalinaelena	dbpedia
5171	5171	nlpaug		[]		0	0	1	kennyxie	nlpaug
5172	5172	Case Study for Google Data Analytics Professional	How Does a Bike-Share Navigate Speedy Success?	['cycling']	"""INTRODUCTION”
Cyclistic bike-share analysis case study!
In this case study, I perform many of the real-world tasks of a junior data analyst,
for a fictional company, Cyclistic, and I meet different characters and team members. To answer key business questions, I followed the steps of the data analysis process: ask, prepare, process, analyze, share, and act.
Along the way, the case study roadmap tables - including guiding questions and key tasks helped me stay on track. By the end of this lesson, I will have a portfolio-ready case study.
Following the script recommended by the course, I downloaded the files and consulted the details of this case study at any time.
As I begin my job search, this case study will be a tangible way to demonstrate my knowledge and skills to potential employers.
Context
“You are a junior data analyst working in the marketing analyst team at Cyclistic, a bike-share company in Chicago. The director of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore, your team wants to understand how casual riders and annual members use Cyclistic bikes differently. From these insights, your team will design a new marketing strategy to convert casual riders into annual members. 
But first, Cyclistic executives must approve your recommendations, so they must be backed up with compelling data insights and professional data visualizations.
CHARACTERS AND TEAMS
•   Cyclistic: A bike-share program that features more than 5,800 bicycles and 600 docking stations. Cyclistic sets itselfapart by also offering reclining bikes, hand tricycles, and cargo bikes, making bike-share more inclusive to people with disabilities and riders who can’t use a standard two-wheeled bike. The majority of riders opt for traditional bikes; about 8% of riders use the assistive options. Cyclistic users are more likely to ride for leisure, but about 30% use them commute to work each day.
•   Lily Moreno: The director of marketing and your manager. Moreno is responsible for the development of campaign and initiatives to promote the bike-share program. These may include email, social media, and other channels.
•   Cyclistic marketing analytics team: A team of data analysts who are responsible for collecting, analyzing, and reporting data that helps guide Cyclistic marketing strategy. You joined this team six months ago and have been busy learning about Cyclistic’s mission and business goals — as well as how you, as a junior data analyst, can help Cyclistic achieve them.
•    Cyclistic executive team: The notoriously detail-oriented executive team will decide whether to approve the recommended marketing program.
ABOUT THE COMPANY
In 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime.
Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments.
One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers who purchase annual memberships are Cyclistic members.
Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is avery good chance to convert casual riders into members. 
She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs.
Moreno has set a clear goal: 
Design marketing strategies aimed at converting casual riders into annual members. In order todo that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.”
Contents
Os dados apresentados neste estudo fazem parte da produção do meu portfolio online relativo ao meu trabalho de conclusão de curso do certificado profissional de analista de dados.
https://coursera.org/share/deb70d54977be77a836621ea82497698
Todo este trabalho teve a duração de 20 dias
Data Source:
http://divvy-tripdata.s3.amazonaws.com/index.html
Reconhecimentos
Todo o conhecimento adquirido para a produção deste estudo foi adquirido durante urso de analise de dados da Google.
Não tenho a referenciar a colaboração de outras pessoas. Apenas segui o roteiro proposto
Inspiration
Seus dados estarão na frente da maior comunidade de ciência de dados do mundo. Quais perguntas você quer ver respondidas?
Ask
Three questions will guide the future marketing program:
1. How do annual members and casual riders use Cyclistic bikes differently?
2. Why would casual riders buy Cyclistic annual memberships?
3. How can Cyclistic use digital media to influence casual riders to become members?
Moreno has assigned you the first question to answer: How do annual members and casual riders use Cyclistic bikes differently?
You will produce a report with the following deliverables:
1. A clear statement of the business task
A description of all data sources used
•   http://divvy-tripdata.s3.amazonaws.com/index.html
Documentation of any cleaning or manipulation of data
A summary of your analysis
From Excell:
From SQL:
From RStudio:
Supporting visualizations and key findings
From Excell:
From SQL:
From RStudio:
Your top three recommendations based on your analysis"	0	28	0	marioazevedo	cyclistic-bikeshare
5173	5173	QS World University Rankings with 2022 details 		['universities and colleges']		8	43	1	jaimepalmahernndez	qs-world-university-rankings-with-2022-details
5174	5174	cdm_250k_mmep_map		[]		0	9	0	renessmi2017	cdm-250k-mmep-map
5175	5175	patch_casia		[]		0	9	0	itachi0906	patch-casia
5176	5176	pytorch-image-models		['arts and entertainment']		2	206	0	bloodaxe	pytorchimagemodels
5177	5177	MISO Regional MTLF vs Actual	Medium-term Load Forecast 2014-12-20 to Present	[]		2	13	0	chatki	miso-load
5178	5178	bent from the hit game genshin impact		['video games']		0	48	0	rebbatshctorc	bent-from-the-hit-game-genshin-impact
5179	5179	Dataset for TSE chat in Telegram groups	دیتاست چت کاربران در گروه های تلگرامی درباره بورس تهران	['online communities']		22	654	1	rezaali	dataset-for-tse-chat-in-telegram-groups
5180	5180	diamonds	Dataset from Github  - mispricing in the diamond market	[]		70	2360	0	jonnyasher	diamonds
5181	5181	student		['universities and colleges']		0	22	0	yuhsin123	student
5182	5182	KaggleMug		[]		0	3	0	monikalca	kagglemug
5183	5183	THIRTYTHICKE		[]		2	6	0	duriani	thirtythicke
5184	5184	Alltestdata		[]		0	6	0	nohelnath013	alltestdata
5185	5185	Augmented_Osteoarthritis_dataset		[]		0	8	0	eishahassan	augmented-osteoarthritis-dataset
5186	5186	ahmed majedr		[]		0	3	0	ahmedmjed	ahmed-majedr
5187	5187	fa_ro_opus-mt-mul-en	Persian to Romanian weights for the Helsinki-NLP/opus-mt-mul-en model; BLEU 35.5	[]		2	24	0	ilikehaskell	ro-en-final
5188	5188	AllAudio		[]		0	5	1	nohelnath013	allaudio
5189	5189	Covid time series data india till 31oct21		[]		8	34	1	rishaabkarthik1	covid-time-series-data-india-till-31oct21
5190	5190	67835model		[]		0	2	0	liuyunqing	67835model
5191	5191	POLLEN20L-det	Pollen dataset of 20 species (7750 pollen grains) annotated for the detection	['global', 'europe', 'biology', 'computer vision', 'deep learning', 'image data']	"Context
Flenley et al. stated the problem of automatic pollen images recognition more than 50 years ago. It requires solving two tasks: detection and classification. Nowadays, both tasks can be successfully handled using computer vision (CV) methods. However, the main obstacle in these tasks solving is the absence of big open datasets for benchmarking. Existing open datasets for pollen classification task (POLEN23E, POLLEN73S) are quite small and represent different domains, thus, their merging is not straightforward. Moreover, there is no open pollen dataset annotated for the detection task.
Thus, here we present the largest open pollen dataset obtained from the bright-field microscope, including 20 plant species, 2117 images containing 7750 single pollen grains, annotated for the classification and detection tasks.
Content
The images in the dataset are obtained using lighting microscope Olympus BX51. Pollen is stained with Fuchsin. The dataset is made using the Olympus DP71 image viewing system. All the pollen species were collected from Perm Krai, Russia, but typical for Europe in common.
The dataset is related to two domains: allergenic-specific palynology and mellisopalynology. Hence, it contains 13 taxa of allergenic plants (willow
linden, alder, birch, nettle, pigweed, plantain, sorrel, grass, pine, maple, hazel, mugwort) and 8 taxa of honey plants (linden, buckwheat, clover, angelica wild, angelica garden, hill mustard, meadow pink, fireweed). 
The allergenic dataset we call POLLEN13L-det. We set the baseline for the detection and recognition on this dataset, see the paper [link to be provided]. We achieved 96.3% of average precision for the detection task and 98.34% of F1 measure for the classification task.
Cite
To cite our dataset, please use Khanzhina, Natalia, et al. ""Combating data incompetence in pollen images detection and classification for pollinosis prevention."" Computers in biology and medicine 140 (2022): 105064.
or
@article{khanzhina2022combating,
  title={Combating data incompetence in pollen images detection and classification for pollinosis prevention},
  author={Khanzhina, Natalia and Filchenkov, Andrey and Minaeva, Natalia and Novoselova, Larisa and Petukhov, Maxim and Kharisova, Irina and Pinaeva, Julia and Zamorin, Georgiy and Putin, Evgeny and Zamyatina, Elena and others},
  journal={Computers in biology and medicine},
  volume={140},
  pages={105064},
  year={2022},
  publisher={Elsevier}
}
Acknowledgements
This dataset is collected with the great help of Larisa Novoselova, Irina Kharisova, Julia Pinaeva and Georgiy Zamorin.
Inspiration
Researchers are welcome to solve the detection and classification tasks :) Although we set relatively high baseline scores on these tasks, there is still the room for improvement. For example, there are two species that belong to one genus - angelica wild and angelica garden - its shape is almost the same even to most of palynologists, which significantly complicates the recognition. 
Make pollen recognition great again!"	8	97	1	nataliakhanzhina	pollen20ldet
5192	5192	techtarget30		[]		1	4	0	duriani	techtarget30
5193	5193	AirPassengers+statistics_small		[]		2	3	0	ujoshi076	airpassengersstatistics-small
5194	5194	books_read	Every book I've read since 2016	['arts and entertainment', 'literature', 'beginner', 'data analytics', 'text data']	"In 2016, I started keeping track of each book I finished reading. In that time, I've read nearly 100.  Now, I'm using the list to help me learn R programming (and you can too)
This dataset provides the titles, authors, years I read the books, genres, subgenres, language, approximate page counts, and some more info. Some books, like The Alchemist, were great. Others, like Churchill in the Trenches, were not, but I enjoyed all of them the same (and some of them were even read in a foreign language)
Shoutout to all the great writers, alive or dead, and to Goodreads for reminding me of things like author names, page counts, etc."	20	214	3	dcgonk	books-read
5195	5195	Passenger_Statistics		[]		2	23	0	ujoshi076	passenger-statistics
5196	5196	Online Learning		[]		3	37	0	jyotikumarrout	online-learning
5197	5197	crowdfunding 		['finance']		0	18	0	yuichikuriyama	crowdfunding
5198	5198	meilds4		[]		21	246	0	tombutton	meilds4
5199	5199	img_clf_cs		[]		1	46	0	lonelvino	img-clf-cs
5200	5200	orders_autumn_2020_test		[]		0	12	0	jackmcmenamin	orders-autumn-2020-test
5201	5201	orders_autumn_2020_training		[]		0	0	0	jackmcmenamin	orders-autumn-2020-training
5202	5202	Metastatic Breast Cancer Genomic data	Targeted Sequencing of 1365 metastatic Breast Cancer tumor/normal pairs	['cancer']	"Metastatic breast cancer
Metastatic breast cancer (also called stage IV) is breast cancer that has spread to another part of the body, most commonly the liver, brain, bones, or lungs.
Cancer cells can break away from the original tumor in the breast and travel to other parts of the body through the bloodstream or the lymphatic system, which is a large network of nodes and vessels that works to remove bacteria, viruses, and cellular waste products.
Breast cancer can come back in another part of the body months or years after the original diagnosis and treatment. Nearly 30% of women diagnosed with early-stage breast cancer will develop metastatic disease.
Data
Targeted Sequencing of 1365 metastatic Breast Cancer tumor/normal pairs via MSK-IMPACT to understand the role of INK4 on CDK4/6 resistance.
There are several different types of breast cancer.
1. Ductal Carcinoma in Situ (DCIS) also known as intra-ductal carcinoma
- Non-invasive breast cancer
- Abnormal cells have not spread through the ducts into surrounding breast tissue
- Has not yet spread outside of breast
- May become invasive breast cancer
Lobular Carcinoma in Situ (LCIS) also called lobular neoplasia
Abnormal cell growth starts in the lobules of the breast
Not considered a true cancer because it is not likely to spread to surrounding tissues
LCIS is an indicator that a woman may be more likely to develop invasive cancer in either breast
3.Invasive Ductal Carcinoma (IDC) also known as infiltrating ductal carcinoma
- The most common type of breast cancer
- Starts in the ducts of the breast then grows into fatty breast tissue
- May also spread to other parts of the body through the lymph system and bloodstream
- Approximately 8 out of 10 invasive breast cancers are invasive ductal carcinomas
Invasive Lobular Carcinoma (ILC) also known as infiltrating lobular carcinoma
Starts in the lobules of the breast
Can spread to other parts of the body
May be harder to detect with a mammogram
Approximately 1 in 10 invasive breast cancers are invasive lobular carcinomas
Acknowledgements
Data from cBioPortal.
- https://www.cbioportal.org/study/summary?id=breast_ink4_msk_2021
- Whttps://www.breastcancer.org/symptoms/types/recur_metast
- https://www.abcf.org/about-breast-cancer/types-of-breast-cancer/?gclid=CjwKCAiAxJSPBhAoEiwAeO_fP3SJXjyU4lO4iE2Umrpxe3n0WBadoG7_JK27fSh49eatMGGBUl3kcBoCuHkQAvD_BwE
Inspiration
Inspiration came from the research on the effect of genetic mutations on breast cancer and its progression."	9	85	0	mahyahemmat	metastatic-breast-cancer-genomic-data
5203	5203	SARS-CoV-2 ITALY (UPDATED!) 	Daily report on Covid-19 in Italy	['healthcare', 'health', 'exploratory data analysis', 'data cleaning', 'data visualization', 'news']	"Context
I want to share with you the dataset concerning the COVID-19 pandemic, updated day by day, so that I can offer you an in-depth logical and graphic analysis on the evolution of the pandemic in Italy.
Content
Inside there are 3 datasets:
- covidAndamentoNazionale.csv
- covidAndamentoRegionale.csv
- covidAndamentoProviciale.csv
Acknowledgements
Thanks to all those who work at all levels to fight this pandemic.
And... Thanks to Umberto Rosini for making this data available
Inspiration
I enjoy data analytics and I love sharing my passion with other people. Also, I always like to learn, and so I thank you for the comparison."	18	100	0	snocco	sarscov2-italy-updated
5204	5204	train_avito		[]		1	22	0	arsdav	train-avito
5205	5205	My dataset CNN proiect		[]		3	16	0	georgegabrielpopescu	my-dataset-cnn-proiect
5206	5206	Cornell movie_dialogs corpus 		[]		0	7	0	saeedomar	cornell-movie-dialogs-corpus
5207	5207	sdcnet with spade		[]		0	2	0	laxminaidu07	sdcnet-with-spade
5208	5208	makanan ringan		[]		2	18	1	anisahwulandari	makanan-ringan
5209	5209	ft_Sk_rtc		[]		0	11	0	hangy132	ft-sk-rtc
5210	5210	Hyperspectral Library of Agricultural Crops (USGS)	GHISACONUS Global Hyperspectral library of Agricultural crops United States	['categorical data', 'agriculture', 'geospatial analysis', 'classification', 'deep learning']	"Description
The Global Hyperspectral Imaging Spectral-library of Agricultural crops (GHISA) is a comprehensive compilation, collation, harmonization, and standardization of hyperspectral signatures of agricultural crops of the world. This hyperspectral library of agricultural crops is developed for all major world crops and was collected by United States Geological Survey (USGS) and partnering volunteer agencies from around the world. Crops include wheat, rice, barley, corn, soybeans, cotton, sugarcane, potatoes, chickpeas, lentils, and pigeon peas, which together occupy about 65% of all global cropland areas. The GHISA spectral libraries were collected and collated using spaceborne, airborne (e.g., aircraft and drones), and ground based hyperspectral imaging spectroscopy.
The GHISA for the Conterminous United States (GHISACONUS) Version 1 product provides dominant crop data in different growth stages for various agroecological zones (AEZs) of the United States. The GHISA hyperspectral library of the five major agricultural crops (e.g., winter wheat, rice, corn, soybeans, and cotton) for CONUS was developed using Earth Observing-1 (EO-1) Hyperion hyperspectral data acquired from 2008 through 2015 from different AEZs of CONUS using the United States Department of Agriculture (USDA) Cropland Data Layer (CDL) as reference data.
GHISACONUS is comprised of seven AEZs throughout the United States covering the major agricultural crops in six different growth stages: emergence/very early vegetative (Emerge VEarly), early and mid vegetative (Early Mid), late vegetative (Late), critical, maturing/senescence (Mature Senesc), and harvest. The crop growth stage data were derived using crop calendars generated by the Center for Sustainability and the Global Environment (SAGE), University of Wisconsin-Madison.
Provided in the CSV file is the spectral library including image information, geographic coordinates, corresponding agroecological zone, crop type labels, and crop growth stage labels for the United States."	9	154	0	billbasener	hyperspectral-library-of-agricultural-crops-usgs
5211	5211	Google Data Analytics Case Study		[]		1	23	0	danelleduardo	google-data-analytics-case-study
5212	5212	The Economist's Big Mac Index	Data and methodology for the Big Mac index 	['economics', 'software']	"Context
Data and methodology for the Big Mac index https://www.economist.com/news/2018/07/11/the-big-mac-index.
Updated yearly, with data from 1986 to 2019.
Content
big-mac-raw-index.csv contains values for the “raw” index
big-mac-adjusted-index.csv contains values for the “adjusted” index
big-mac-full-index.csv contains both
| variable      | definition                                            | source                     |
| ------------- | ----------------------------------------------------- | -------------------------- |
| date          | Date of observation                                   |
| iso_a3        | Three-character [ISO 3166-1 country code][iso 3166-1] |
| currency_code | Three-character [ISO 4217 currency code][iso 4217]    |
| name          | Country name                                          |
| local_price   | Price of a Big Mac in the local currency              | McDonalds; The Economist |
| dollar_ex     | Local currency units per dollar                       | Reuters                  |
| dollar_price  | Price of a Big Mac in dollars                         |
| USD_raw       | Raw index, relative to the US dollar                  |
| EUR_raw       | Raw index, relative to the Euro                       |
| GBP_raw       | Raw index, relative to the British pound              |
| JPY_raw       | Raw index, relative to the Japanese yen               |
| CNY_raw       | Raw index, relative to the Chinese yuan               |
| GDP_dollar    | GDP per person, in dollars                            | IMF                        |
| adj_price     | GDP-adjusted price of a Big Mac, in dollars           |
| USD_adjusted  | Adjusted index, relative to the US dollar             |
| EUR_adjusted  | Adjusted index, relative to the Euro                  |
| GBP_adjusted  | Adjusted index, relative to the British pound         |
| JPY_adjusted  | Adjusted index, relative to the Japanese yen          |
| CNY_adjusted  | Adjusted index, relative to the Chinese yuan          |
Acknowledgements
Banner Photo by amirali mirhashemian on Unsplash
This software is published by The Economist under the MIT licence. The data generated by The Economist are available under the Creative Commons Attribution 4.0 International License.
The licences include only the data and the software authored by The Economist, and do not cover any Economist content or third-party data or content made available using the software. More information about licensing, syndication and the copyright of Economist content can be found here.
From https://github.com/TheEconomist/big-mac-data"	406	5332	35	paultimothymooney	the-economists-big-mac-index
5213	5213	autogluon		[]		1	23	0	sinrinosekai	autogluon
5214	5214	useCaseProditecTeam6		[]		4	47	0	carpiliengpatbaul	usecaseproditecteam6
5215	5215	Meadows vs Orchards	Sentinel-2 time series	['europe', 'agriculture', 'tabular data', 'news', 'numpy', 'pandas']		0	21	0	baptistel	meadows-vs-orchards
5216	5216	timemachine.txt		[]		0	4	0	commandls	timemachinetxt
5217	5217	TV series and Movies on Netflix (separately)	seperate csv for tv series and movies	['movies and tv shows', 'categorical data', 'beginner', 'tabular data', 'text data']	"Movies and tv series dataset
The original dataset is combined both movies and tv series. 
I wanted to separate out Movies and TV series.
The notebook is here: https://www.kaggle.com/fanbyprinciple/getting-tv-series-and-movies-dataset-from-netflix
This dataset is sourced from :https://www.kaggle.com/shivamb/netflix-shows"	96	775	4	fanbyprinciple	tv-series-and-movies-on-netflix
5218	5218	images		[]		0	6	0	hosseindahaei	images
5219	5219	ft_imvladikon_bert_base_uncased_jigsaw		[]		1	32	0	hangy132	ft-imvladikon-bert-base-uncased-jigsaw
5220	5220	CO2 ppm daily	From GitHub Atmospheric Carbon Dioxide Dry Air Mole at Mauna Loa, Hawaii 	['earth science']		94	2186	0	frank06	co2-ppm-daily
5221	5221	gta_2205khz		[]		0	6	0	dunky11	gta-2205khz
5222	5222	label04		[]		0	11	0	effys777	label04
5223	5223	Crime Against Women (India, Tamil Nadu, Dindigul)		['india', 'crime', 'beginner', 'data visualization', 'time series analysis']	"Context
Datasets of Crime against women in India, Tamil Nadu and India
.
Content
 The datasets contain crime entry for the past decade, with the variable number of crime types based on the crime occurrence which has been registered either in the police station or in any one of the women commissions. The dataset was collected by the first week of October 2021. The overlapping crimes have been merged in the pre-processing phase.
The dataset of crime against women in India and Tamil Nadu has been extracted from the National Crime Record Bureau’s (NCRB) publication section. The data has been collected and summarized once a year by the concerned authority.
The dataset of crime against women in Dindigul has been extracted from the Tamil Nadu Police - Citizen Portal’s Crime Review Tamil Nadu section. The data has been collected and summarized once a year by the concerned authority.
The dataset of crime against women in India has been scarped/extracted from the National Commission for Women’s (NCW) portal. The data has been collected and summarized once a month by the concerned authority."	16	131	2	adhithyaharshan	crime-against-women-india-tamil-nadu-dindigul
5224	5224	ILO report on Unemployment	Unemployment report of labour organisatiion	['employment', 'international relations', 'human rights', 'covid19']		6	26	0	vaibhav2025	ilo-report-on-unemployment
5225	5225	123456		[]		0	10	0	aokoqkq	123456
5226	5226	MachineTranslation		[]		0	6	0	andreeabodea	machinetranslation
5227	5227	PLOS Machine Learning Articles	"Articles from plos.org with ""Machine Learning"" in the article title."	['earth and nature']		12	1724	3	nkarasch	plos-machine-learning-articles
5228	5228	partial_mnist		[]		0	6	0	katotetsuro	partial-mnist
5229	5229	nlqwqqwq		[]		0	3	0	gene410822020	nlqwqqwq
5230	5230	Indian Licence Plates	Indian Vechile Number plate data	['india', 'law', 'automobiles and vehicles', 'image data']	"This dataset consists of licence plate images for indian vehicles.
Introduction
This dataset is a complication of multiple dataset where the images are manually downloaded from google images and manually labeled."	37	416	8	bellair	indian-licence-plates
5231	5231	PyCaret Classification Easy Function	PyCaret Classification Blend Model	['beginner', 'classification', 'tabular data', 'automl']		0	23	1	rhythmcam	pycaret-classification-model
5232	5232	heroin-EEG-features		[]		0	10	0	zhengkeliu	heroineegfeatures
5233	5233	全台露營場資料 Let Me Down Slowly 		[]		0	6	0	www6qwww	-let-me-down-slowly
5234	5234	web-scraper	Youtube, github projects and ICC data scraped	[]		28	1972	0	prankshaw	webscraper
5235	5235	data_db_keras	panfei的数据。。。。。。。。。。。。。。。。。。。。。	[]		0	11	0	panfei748	data-db-keras
5236	5236	yearair		[]		0	7	0	eheka923	yearair
5237	5237	CQ500 Hemorrhage DICOM		[]		2	22	0	fereshtej	cq500-hemorrhage-dicom
5238	5238	bigbaojiao		[]		0	7	1	februarysea	bigbaojiao
5239	5239	Uber_Drives_DATASET		[]		2	5	1	priy998	uber-drives-dataset
5240	5240	Financial News Sentiment Classification Dataset	Sentiment classification	['finance', 'nlp', 'text data', 'news', 'sklearn']	"Context
Due to the limited sentiment classification dataset online, I labeled more than 200 news title(from well-known financial websites such as CNBC, Financial times etc.) with 3 sentiment categories. This dataset contains relative new information which may be helpful for you in predicting new trends such as COVID-19）. The standard that how I labeled is based on the other two already exist datasets. So when you judge the sentences you might have some different feelings. Hope if you also do this job you can share your data with us if you can! Also looking forward to have a thumb up from you!"	73	793	9	percyzheng	sentiment-classification-selflabel-dataset
5241	5241	imdb-sqlite-DATASET		['business']		4	31	1	priy998	imdbsqlitedataset
5242	5242	lbpnew		[]		0	13	0	widhiwinata	lbpnew
5243	5243	boredapeyachtclub		[]		1	6	0	jamesjeon	boredapeyachtclub
5244	5244	2015 Canadian Federal Election Results	A suite of official voting results of Canada's 42nd general election	['politics']		14	762	0	ijensen	2015-canadian-federal-election-results
5245	5245	SatelliteData		[]		1	8	0	mikhailkharin	satellitedata
5246	5246	DineroFelicidad	Relación por Comunidad autónoma renta hogar por indice de bienestar	[]		8	66	0	joerobcia	dinerofelicidad
5247	5247	maskdata		[]		0	7	0	luhanyu	maskdata
5248	5248	subaru ds train 0117 disparity		['automobiles and vehicles']		0	7	2	wuliaokaola	subaru-ds-train-0117-disparity
5249	5249	insurance dataset		['insurance']		2	28	1	priy998	insurance-dataset
5250	5250	use sentiment140		[]		0	6	0	panser	use-sentiment140
5251	5251	YOLO-Tiling output		[]		1	12	0	hassanmojab	yolo-tiling-output
5252	5252	CQ500 Non-Hemorrhage DICOM		[]		0	14	0	fereshtej	cq500-nonhemorrhage-dicom
5253	5253	divvy trips data 20192020	Divvy Bikes Trip Data 2019 - 2020 /w Weather Analysis	['cycling', 'data cleaning', 'data visualization', 'data analytics', 'tabular data']	"Content
The data has been downloaded from https://divvy-tripdata.s3.amazonaws.com/index.html. 2019 datasets are already present in quarterly format. 2020 datasets were downloaded individually and then merged in RGUI v4.1.2 as quarterly data.
The weather data was downloaded from https://www.kaggle.com/sobhanmoosavi/us-weather-events.
Acknowledgements
I would like to thank Mr. Sobhan Moosavi (for the weather dataset), Mr. Julen Aranguren, Mr. David Areso, Mr. Wesley Liao, and Mr. Cody Freeman for their inspirational work on this topic. It was through researching their notebooks on kaggle.com I was able to continue to work through and be inspired to complete the Capstone Project for Google Data Analytics Professional Certificate Program.
I would like to thank Mr. Sobhan Moosavi (for the weather dataset), Mr. Julen Aranguren, Mr. David Areso, Mr. Wesley Liao, and Mr. Cody Freeman for their inspirational work on this topic. It was through researching their notebooks on kaggle.com I was able to continue to work through and be inspired to complete the Capstone Project for Google Data Analytics Professional Certificate Program.
I would also like to thank Mr. Zulkhairee Sulaiman for his work in Bellabeat's Capstone Project. It was his completed project that motivated me to complete my own even though I had seriously lagged behind due to external pressures."	3	26	0	mdmasumomarjashim	divvy-trips-data-20192020
5254	5254	COVID Freq characteristics Comparison Data		['ratings and reviews']		0	10	0	himanshu007121	covid-freq-characteristics-comparison-data
5255	5255	COVID Cough WAVs		[]		6	44	2	himanshu007121	covid-cough-wavs
5256	5256	yolov5	deeplearning study about yolo	['universities and colleges']		1	35	0	vainglorylx	yolov5
5257	5257	lbpmod		[]		0	2	0	widhiwinata	lbpmod
5258	5258	Underwater Image Enhancement Model		[]		9	39	0	seshurajup	underwater-image-enhancement-model
5259	5259	boredape		[]		2	3	0	jamesjeon	boredape
5260	5260	London Air Quality	This dataset contains monthly-averages.csv and time-of-day-per-month.csv.	[]		338	4226	8	zsn6034	london-air-quality
5261	5261	1_wineind		[]		0	29	0	wallacefqq	1-wineind
5262	5262	bored_ape		[]		1	5	0	jamesjeon	bored-ape
5263	5263	images	a car plate datasets	[]		3	13	0	vainglorylx	images
5264	5264	Earthquake in Japan	USGS Earthquake Data in Japan since 2019	['geology']	"Datasource
The data was acquired from the USGS Earthquake Catalog
Columns
['time', 'latitude', 'longitude', 'depth', 'mag', 'magType', 'nst', 'gap', 'dmin', 'rms', 'net', 'id', 'updated', 'place', 'type', 'horizontalError', 'depthError', 'magError', 'magNst', 'status', 'locationSource', 'magSource']
Time period
2019/1/1-2021/12/03
Region
[25.513, 46.702] Latitude
[127.441, 150.293] Longitude
Ispiration
On 2011/03/11, Japan experienced a huge earthquake. At that time, there were no predictive information about that.  However, analyzing the data until the event, some characteristic features were found. It is possible to predict a future large earthquake by analyzing recent data.
Focus
Find signs of a huge aftershock following the 2011 Tohoku Earthquake
Find signs of Nankai Trough earthquake<p></p>"	175	1764	16	stpeteishii	earthquake-in-japan
5265	5265	The beauty of the forecast		['news']		2	15	0	wallacefqq	the-beauty-of-the-forecast
5266	5266	Cyberbullying Classification	47k tweets belonging to 6 balanced classes.	['people and society', 'nlp', 'text data', 'social issues and advocacy', 'social networks']	"Abstract
&gt; With rise of social media coupled with the Covid-19 pandemic, cyberbullying has reached all time highs. We can combat this by creating models to automatically flag potentially harmful tweets as well as break down the patterns of hatred.
About this dataset
&gt; As social media usage becomes increasingly prevalent in every age group, a vast majority of citizens rely on this essential medium for day-to-day communication. Social media’s ubiquity means that cyberbullying can effectively impact anyone at any time or anywhere, and the relative anonymity
of the internet makes such personal attacks more difficult to stop than traditional bullying.
&gt; On April 15th, 2020, UNICEF issued a warning in response to the increased risk of cyberbullying during the COVID-19 pandemic due to widespread school closures, increased screen time, and decreased face-to-face social interaction. The statistics of cyberbullying are outright alarming: 36.5% of middle and high school students have felt cyberbullied and 87% have observed cyberbullying, with effects ranging from decreased academic performance to depression to suicidal thoughts.
&gt; In light of all of this, this dataset contains more than 47000 tweets labelled according to the class of cyberbullying:
- Age;
- Ethnicity;
- Gender;
- Religion;
- Other type of cyberbullying;
- Not cyberbullying
&gt; The data has been balanced in order to contain ~8000 of each class.
&gt; Trigger Warning These tweets either describe a bullying event or are the offense themselves, therefore explore it to the point where you feel comfortable.
How to use this dataset
&gt; - Create a multiclassification model to predict cyberbullying type;
- Create a binary classification model to flag potentially harmful tweets;
- Explore words and patterns associated with each type of cyberbullying.
Highlighted Notebooks
&gt; - Cyberbullying on Twitter Visualization 🤬 by Liza Konopelko
- Your kernel can be featured here!
- More datasets
Acknowledgements
If you use this dataset in your research, please credit the authors.
&gt; ### Citation
&gt; J. Wang, K. Fu, C.T. Lu, “SOSNet: A Graph Convolutional Network Approach to Fine-Grained Cyberbullying Detection,” Proceedings of the 2020 IEEE International Conference on Big Data (IEEE BigData 2020), December 10-13, 2020.
&gt; ### License
CC BY 4.0
&gt; ### Splash banner
Icons by Freepik and Juicy Fish."	650	6470	47	andrewmvd	cyberbullying-classification
5267	5267	Tamil_first_ready_for_sentiment		[]		0	3	0	mayaabasu	tamil-first-ready-for-sentiment
5268	5268	vent_hs		[]		0	12	0	saidobak	vent-hs
5269	5269	Abnormal temperature rise		['health conditions']		0	24	0	xiangdongyuan	abnormal-temperature-rise
5270	5270	modelsisi		[]		0	8	0	widhiwinata	modelsisi
5271	5271	higgs4lep		[]		3	32	0	marcelagarca	higgs4lep
5272	5272	loadings-STCN		['earth and nature']		0	27	0	fabienmerceron	loadingsstcn
5273	5273	GBR_TRAINING_MODEL		[]		0	0	0	codinguy	gbr-training-model
5274	5274	debate_1		[]		2	8	0	shimizuiori	debate-1
5275	5275	YOLOX_COTs_CV_Patch		[]		0	5	0	toshikiharaguchi	yolox-cots-cv-patch
5276	5276	tph-yolov5_ds		[]		1	29	0	dwchen	tphyolov5-ds
5277	5277	test_csv_injection		[]		3	27	0	choithuthoi	test-csv-injection
5278	5278	dataDADA		[]		4	38	0	yuhaeun	datadada
5279	5279	Spellcorpus		[]		35	8	0	daominhkhanh	spellcorpus
5280	5280	ExampleDomain		[]		36	15	0	daominhkhanh	exampledomain
5281	5281	test dataset		[]		1	4	1	xstarr	test-dataset
5282	5282	Cancer Rate by Countries	Cancer rate for 50 countries	['text data', 'cancer']	"This dataset contains information about cancer rate for 50 countries in the world.
The data was obtained by doing web scraping from Wikipedia using BeautifulSoup in Python.
Wikipedia link:
https://en.wikipedia.org/wiki/List_of_countries_by_cancer_rate"	512	3653	22	dianapratiwi	cancer-rate-by-countries
5283	5283	validationdata		[]		2	11	0	mlcovidresearch	validationdata
5284	5284	starfish_fold		[]		11	55	0	kevin1742064161	starfish-fold
5285	5285	distilbert-base-uncased		[]		0	3	0	shawndong98	distilbertbaseuncased
5286	5286	swin transformer		[]		0	9	0	jennishw	swin-transformer
5287	5287	letterdata	Alphabet's data in CSV form 	['classification', 'svm']		3	25	0	anandchauhan	letterdata
5288	5288	loss_model_2		[]		0	5	0	kookheejin	loss-model-2
5289	5289	UAS ALGORITMA DAN PEMROGRAMAN_M.ADRA AL ICHSAN		[]		0	6	0	adraalichsan	uas-algoritma-dan-pemrograman-madra-al-ichsan
5290	5290	practice		[]	"This dataset comes from https://www.kaggle.com/abhishek
Thank you!"	0	7	0	wallacefqq	practice
5291	5291	Custom Data Yolov5		['business']		0	6	0	canhnguyninh	custom-data-yolov5
5292	5292	beirquorapromptsresults		[]		0	6	0	muennighoff	beirquorapromptsresults
5293	5293	Teddy & Machine		[]		0	9	0	wengkeatho	teddy-machine
5294	5294	Aeona Dataset		['earth and nature']		0	24	0	deepsarda	aeona-dataset
5295	5295	Test Matches played from 1877 - Jan 2022	Dataset containing basic information of Test matches played from 1877-Jan 2022.	['cricket', 'india', 'sports', 'data cleaning']	"Context
Test cricket is regarded as the purest form of cricket. Test cricket has given fans some of the most memorable moments in history, such as the 2001 Kolkata Test, 2005 Ashes series, 2020-21 Border Gavaskar Series, etc. Analyzing Test cricket data reveals exciting new information, establishes past trends, and helps in predicting the outcome of an upcoming match/series.
Content
The dataset contains a single CSV file, containing data of all the Test matches played till Jan 2022, and multiple other folders containing Ground-wise, Team-wise, Year-wise, and Host-wise data of Test matches.
Special care has been taken to appropriately accord for the matches hosted by Pakistan and Afghanistan but hosted in other countries.
Also certain matches, part of a bigger tournament, played at a neutral venue have also been appropriately placed.
Acknowledgements
The data is scraped from the ESPNCricinfo Stats website using BeautifulSoup and rendered into multiple CSV files using Pandas.
Link to the Source page: https://stats.espncricinfo.com/ci/content/records/307847.html
Inspiration
The main inspiration in creating this dataset was to be able to analyze the data to see how the performance of a team varies from conditions to conditions and ground to ground.
This directory contains an All_Matches.csv file, which contains basic information of all the Test matches played till January 2022. 
The data is initially arranged year-wise. So using web scraping, first the links to individual years are extracted a and then web scraping is performed on those links to get the data of all Test Matches.
The data is further segregated into 4 parts - By_Ground, By_Hosting_Nation, By_Team, and By_Year using Pandas.
Now a single directory containing separate CSV files or a single CSV file having all matches can be called and utilized in a project/notebook."	132	594	10	bong952	test-matches-played-from-1877-jan-2022
5296	5296	Face Data Yolov5		[]		2	14	0	canhnguyninh	face-data-yolov5
5297	5297	niko-dream-dataset		[]		0	11	0	annas1301	niko-dream-dataset
5298	5298	xmd prediction sample data		['religion and belief systems']		4	5	0	rdhenkawat	xmd-prediction-sample-data
5299	5299	Evaluate_Writing_updated_train_dataset		[]		0	4	0	ashwinvijayanpillai	evaluate-writing-updated-train-dataset
5300	5300	pokemon	A dataset of images containing pokemon	['business']	"Pokemon dataset : A dataset of images containing pokemon
Vesion : 2022.01.17
Content
The following pokemons and are included :
Charmander, Diglett, Ditto, Grimer, Growlithe, Jigglypuff, Mew, Pikachu, Snorlax, Squirtle
Dataset properties
The total number of images: 3,869
Training set size: 3,227 
Test set size: 642
The number of classes: 10
update
22.01.14 Upload dataset
22.01.17 Data Preprocessing (Delete invalid files)"	12	85	1	kkitokki	pokemon-data-set
5301	5301	b43600phypf012		[]		0	9	0	gyanendradas	b43600phypf012
5302	5302	CountriesWithGDP		[]		0	14	0	maurasateriale	countrieswithgdp
5303	5303	TGBR yolov5 dataset		['earth and nature']		10	21	0	gmhost	tgbr-yolov5-dataset
5304	5304	videofile		[]		0	10	0	debojit23	videofile
5305	5305	petfinder_models		[]		0	89	0	sinpcw	petfinder-models
5306	5306	FBR POS Winners Jan 2022 	FBR POS Winners (Pakistan)	['business']		0	3	1	imranqamer	fbr-pos-winners-jan-2022
5307	5307	EdX Courses Dataset 2021	Data collected on Courses available on EdX	['websites', 'education', 'recommender systems', 'text data', 'online communities']	"Context
2021 has seen a boom in the MOOCs due to the Covid-19 Pandemic. With the availability of numerous paid and free resources on the internet, it becomes overwhelming for students to learn new skills. Therefore, this dataset can be used to create Recommender Systems and recommend courses to students based on the Skills and Difficulty Level entered by the student. The Course Link is also provided, which can be offered by the Recommender System for easy access.
Content
This dataset was scraped off the publicly available information on the EdX website in September 2021 and manually entered in the case where the data was improperly scraped. It can be used in Recommender Systems to promote EdX courses based on the Difficulty Level and the Skills needed. 
Acknowledgements
The dataset was obtained from the publicly available information on the EdX website. I do not own any information."	227	1708	30	khusheekapoor	edx-courses-dataset-2021
5308	5308	Microsoft Video Description Dataset (MSVD)	All videos have been converted into frames and splitted into training, val, test	['arts and entertainment', 'software']		1	26	0	steveandreasimmanuel	msvd-video-caption
5309	5309	Grocery Data	Data collected from Jio Mart Website using Web Scrapping	['nutrition', 'categorical data', 'beginner', 'image data', 'text data']	"Category         : Category of the Product
Sub Category : Sub Category of the Product
Name              : Name of the Product
Price               : Price of the Product
Image URL      : URL link image representing the product
Image              : Image Extracted from corresponding Image URL"	94	700	3	jeffryjames	groceries-data
5310	5310	Conv_SEresnet_fishion_10L		[]		0	0	0	ding314	conv-seresnet-fishion-10l
5311	5311	Icevision_lib		[]		0	4	0	riadalmadani	icevision-lib
5312	5312	eChineseLearning learn chinese online	1-on-1 Chinese live lesson online	['education']		0	32	3	allenwull	1on1-chinese-live-lesson-online
5313	5313	WEATHER PREDICTION	Predicting it will Rain or not using some Weather Conditons..	['categorical data', 'weather and climate']	"Using the Columns :
                                  *  precipitation
                                  *  temp_max
                                  *  temp_min
                                  *  wind
We are going to predict the weather condition  :
                                   * drizzle
                                  * rain
                                  * sun
                                  * snow 
                                  * fog"	467	3419	23	ananthr1	weather-prediction
5314	5314	rightwing-profiles	hate symbols in profile pictures on steam and soundcloud	['online communities']		1	24	0	bindestrich	rightwingprofiles
5315	5315	sephora-dataset		['make-up and cosmetics']		16	80	1	ramanathanannamalai	sephoradataset
5316	5316	mountainbike		[]		2	5	0	zmjjiang	mountainbike
5317	5317	citybike		[]		1	6	0	zmjjiang	citybike
5318	5318	yolo_for_train		[]		0	7	0	pulishen	yolo-for-train
5319	5319	pypi autocorrect		[]		0	4	1	kennyxie	pypi-autocorrect
5320	5320	uas prak algoritma & pemrograman		[]		0	7	0	safiraasari	uas-prak-algoritma-pemrograman
5321	5321	uas algoritma		[]		0	5	0	safiraasari	uas-algoritma
5322	5322	uas algo		[]		0	2	0	safiraasari	uas-algo
5323	5323	Large Swin Models		[]		0	19	0	ni7san	large-swin-models
5324	5324	Top NFT Collections 	A compilation of the top 600 NFT's by Sales Volume	['arts and entertainment', 'music', 'art', 'finance', 'internet', 'text data', 'currencies and foreign exchange']	"About NFT's
NFT stands for ""non-fungible token."" They are unique, digital assets that are non-interchangeable and bought and sold online. According to Forbes:
&gt; An NFT is a digital asset that represents real-world objects like art, music, in-game items and videos. They are bought and sold online, frequently with cryptocurrency, and they are generally encoded with the same underlying software as many cryptos. Although they’ve been around since 2014, NFTs are gaining notoriety now because they are becoming an increasingly popular way to buy and sell digital artwork. A staggering $174 million has been spent on NFTs since November 2017. 
Dataset Information
This dataset was created via Python using the requests, json, and pandas libraries. The information was pulled on January 16, 2022, and represents all time information for the top NFT collections. As an example, the Sales column represents all sales under a specified NFT collection from its creation up until January 16, 2022. 
The dataset consists of the following information:
Index: The index of the file.
Name: The name of the NFT collection. 
Volume: The volume of sales from the NFT collection in Solana (SOL). 
Volume_USD: The volume of sales from the NFT collection in United States Dollar (USD). 
Market_Cap: The market capitalization—total value of the collection's items in circulation—in Solana (SOL). 
Market_Cap_USD: The market capitalization—total value of the collection's items in circulation—in United States Dollar (USD). 
Sales: The number of sales from the NFT collection. 
Floor_Price: The lowest price of any NFT in the collection in Solana (SOL). 
Floor_Price_USD: The lowest price of any NFT in the collection in United States Dollar (USD). 
Average_Price: The average price of an NFT in the collection in Solana (SOL). 
Average_Price_USD: The average price of an NFT in the collection in United States Dollar (USD). 
Owners: The number of owners of NFT's in the collection. 
Assets: The number of items in the collection. 
Owner_Asset_Ratio: The ownership percentage of all items in the collection. 
Category: The category of the NFT collection. 
Website: The associated website of the NFT collection. 
Logo: The associated image of the NFT collection. 
Sources
This data was scraped from https://coinmarketcap.com/nft/collections/ 
The cover image was downloaded from https://unsplash.com/@tezos"	700	5722	38	nenamalikah	nft-collections-by-sales-volume
5325	5325	gensaddle		[]		2	10	0	zmjjiang	gensaddle
5326	5326	Google Data Analytics Capstone Project	Bellabeat Capstone Project	['business']		0	6	0	gloriamillan	google-data-analytics-capstone-project
5327	5327	Meta Kaggle Prize Money	Details on competition prizes and the Kagglers who won them	['websites', 'research', 'celebrities', 'computer science', 'tabular data', 'text data']	"Have you ever wondered how much prize money gets distributed through Kaggle competitions? Or how much top earners have won? Here's the data to help answer such questions. Money awarded for each competition is itemized by leaderboard rank and matched with the teams/users at that rank. It's assumed that teams evenly split their winnings among members.
The dataset captures 90% of the $14M awarded for top leaderboard finishes. Not every competition is included, mainly because prize breakdowns were scraped from Kaggle web pages and there are many different page formats/wording for competitions, especially before 2017. The data also is missing prizes awarded for early phases, milestones, using TPUs, etc., as those winners are found only in discussion posts and not recorded in Meta Kaggle.
Non-cash prizes don't appear for a similar reason. That includes really nice prizes like a DGX Workstation or high-end GPU that can exceed the value of cash awards for a competition. It also includes lame prizes like swag or paying travel expenses to a conference. 
The  <a href=""https://www.kaggle.com/jpmiller/moneyboard-dataset-maker"" target=""_blank""> MoneyBoard Dataset Maker </a> notebook contains code used to build the dataset. There are sure to be errors and omissions somewhere. Feel free to contact me with corrections or post them here.
Last update: 16 Jan 2022, up through the Santa competition"	59	1115	29	jpmiller	meta-kaggle-moneyboard
5328	5328	res34gcn		[]		0	2	0	sodhana	res34gcn
5329	5329	longformer-baseline-sfold-f3-cv-0.6271		[]		0	4	0	atharvaingle	longformer-baseline-sfold-f3
5330	5330	fetal-ultrasound-brain		[]		10	103	0	rahimalargo	fetalultrasoundbrain
5331	5331	longformer-baseline-sfold-f2-cv-0.621		[]		0	4	0	atharvaingle	longformer-baseline-sfold-f2
5332	5332	Sales Data (Project1 IIITD)	Writing programs and analyzing sales data using Python	['exploratory data analysis', 'data cleaning', 'data analytics', 'numpy', 'pandas']		5	47	0	rahultheogre	iiitd-project1
5333	5333	24,169 Pitchfork Reviews	Pitchfork reviews from Jan 5, 1999 to Dec 12, 2021	['music', 'ratings and reviews']	"Context
In 2017, as I was just stepping into the data science space, I wrote a web scraper which populated a SQLite database of Pitchfork reviews. That dataset became quite popular on Kaggle; as of now there are 1750 notebooks exploring the data.
More than five years have passed and I have gotten a lot better at data modeling. I thought it might be fun to rewrite the scraper to see what I did differently, with the hopes of writing a blog post on the mistakes I naively made long ago.
Content
This dataset is the result of having run the scraper in the middle of December 2021. Included are 24,169 reviews set up within a new (improved?) data model. The database was designed using DBT, you can find the source models here.
Analytics workloads are best handled by a view, standard_reviews_flat, in which reissues, retrospectives, etc, are excluded. That view provides well-typed columns (e.g., score as a float) ready for plotting and modelling. Its model is:
review_url: String primary key.
artist_count: Integer.
artists: String. Comma-delimited list of artists (in case the review has multiple).
title: String.
score: Float.
best_new_music: Boolean/integer (as sqlite3 has no native boolean type). 
authors: String. Comma delimited list of author names.
genres: String. Comma delimited list of genre names.
labels: String. Comma delimited list of label names.
pub_date: Datetime.
release_year: Integer.
body: String."	5	37	0	nolanbconaway	24169-pitchfork-reviews
5334	5334	Russian_electricity_market_price_1_zobe		[]		1	27	0	arsdav	russian-electricity-market-price-1-zobe
5335	5335	IPL Matches (2008to2019)		['cricket', 'exploratory data analysis', 'numpy', 'pandas', 'seaborn']		8	84	1	rahultheogre	ipl-matches-2008to2019
5336	5336	BankChurnHI		[]		3	11	0	pinarelcinmann	bankchurnhi
5337	5337	PolicyCOVID		[]		0	20	1	akshatmundra	policycovid
5338	5338	applicant2.csv		[]		0	1	0	ojaswayadav	applicant2csv
5339	5339	Handwritten Russian Letters	Symbol Images for Detection, Recognition, & Generation	['languages', 'education', 'computer science', 'software', 'image data']	"History
I have made the database from my own photos of Russian letters written by hand. 
Content
The GitHub repository with examples
GitHub
The main dataset
zip files with color images (32x32x3) of 33 letters. 
The additional dataset
zip files with base color photos of many symbols. 
Letter Types =&gt; Letter Labels
'lowercase'=&gt;0, 'uppercase'=&gt;1
Letter Symbols =&gt; Letter Labels
а=&gt;0, б=&gt;1, в=&gt;2, г=&gt;3, д=&gt;4, е=&gt;5, ё=&gt;6, ж=&gt;7, з=&gt;8, и=&gt;9,
й=&gt;10, к=&gt;11, л=&gt;12, м=&gt;13, н=&gt;14, о=&gt;15, п=&gt;16, р=&gt;17, с=&gt;18, т=&gt;19,
у=&gt;20, ф=&gt;21, х=&gt;22, ц=&gt;23, ч=&gt;24, ш=&gt;25, щ=&gt;26, ъ=&gt;27, ы=&gt;28, ь=&gt;29,
э=&gt;30, ю=&gt;31, я=&gt;32
Image Backgrounds =&gt; Background Labels
single-colored paper=&gt;0, striped paper=&gt;1, squared paper=&gt;2, graph paper=&gt;3
Acknowledgments
As an owner of this database, I have published it for absolutely free usage by any site visitor.
Usage
Symbol classification, image generation, etc. in the case of handwritten letters are common and useful exercises.
Data Building. Practice Project 1_0: Letter Detection
The additional folder ""finger letters"" contains images with drawing symbols by finger in applications.
Just for fun, we could try to classify images in this direction: how it was written (on paper or on-screen). 
It seems to me for this goal it needs to clear photo images from any noise and then try to compare with application images.
Improvement
There are lots of ways for increasing this set and the machine learning algorithms applied to it. 
 - For example: add the same images but written by another person."	40	1447	10	olgabelitskaya	handwritten-russian-letters
5340	5340	Indonesia Restaurant Reviews		[]		6	35	0	nadiarizkyh	indonesia-restaurant-reviews
5341	5341	House Prices With Advanced Feature Engineering		[]		2	21	0	itai2468	house-prices-with-advanced-feature-engineering
5342	5342	NBA Highest-Paid Player by Season	NBA Salaries Since 1984 	['basketball']		1	69	0	bazbabar	nba-highestpaid-player-by-season
5343	5343	cardio-disease-dataset		['health conditions']		28	43	0	rahimalargo	cardiodiseasedataset
5344	5344	sun_094-a		[]		4	20	0	randalljarrell	sun-094a
5345	5345	pytorch_toolbelt	Latest version of pytorch-toolbelt for offline kernels	[]		1	690	0	bloodaxe	pytorch-toolbelt
5346	5346	cuencariolujan		[]		0	11	0	fernandobordi	cuencariolujan
5347	5347	embedding		[]		0	5	0	heyman1984	embedding
5348	5348	Sign Language Recognition (Alphabets)	This dataset contains pixel values of Sign Language Alphabets from A-Z 	['intermediate', 'cnn', 'image data', 'multiclass classification', 'tensorflow']		19	234	4	anirudhchauhan	sign-language-recognition-alphabets
5349	5349	DCRNN Bangkok		[]		11	6	0	aishikai	dcrnn-bangkok
5350	5350	imadeskd	asdfghjwterfhclsdchbc	[]		0	3	0	abeeromar	imadeskd
5351	5351	AH Mon prov cnts deaths Causes Death Age Race CDC	CDC 2020-2021 Deaths by Age (COVID-19)	['covid19']	"Provisional counts of deaths by the month the deaths occurred, by age group and race/ethnicity, for select underlying causes of death for 2020-2021. Final data is provided for 2019. The dataset also includes monthly provisional counts of death for COVID-19, coded to ICD-10 code U07.1 as an underlying or multiple cause of death.
What Age Groups are dying from COVID Alone?
What races suffer from COVID 19 most?
Are deaths from COVID-19 Decreasing?
https://healthdata.gov/dataset/AH-Monthly-Provisional-Counts-of-Deaths-for-Select/bj4f-mcqz"	5	21	0	andrewaldrich	ah-mon-prov-cnts-deaths-causes-death-age-race-cdc
5352	5352	Speech Sentiment Dataset		['social science']		2	13	0	henrychibueze	speech-sentiment-dataset
5353	5353	Printed Digits Dataset	Printed Numerical Digits Image Dataset. 	['beginner', 'computer vision', 'classification', 'cnn', 'image data']	"Contains around 3000 images of digitally printed numeric dataset.
Each image is of dimension 28x28 and is grayscale. This dataset was purposely created for sudoku digits classification hence it shows blank image for 0 (zeros).
This dataset originally had 177 images, afterwards augmentation was performed."	173	1744	14	kshitijdhama	printed-digits-dataset
5354	5354	variables		['earth and nature']		0	4	0	sumitkumarjethani	variables
5355	5355	Arknights Operator Stats	Stats evolution as operators level up and get promoted	['video games', 'tabular data', 'anime and manga']	"Context
Inspired by other fan-made Arknights portals and tools, I have decided to scrape the Arknights wiki from Gamepress. This scraping includes going through each operator's individual page to scrape their stats.
Content
This dataset includes the stats for each operator in Arknights, at each level of each promotion rank.
So, taking Amiya as an example, there are 50 rows for her stats at each level of Elite 0 promotion, 70 rows for her stats at each level of Elite 1 promotion, and finally 80 rows for her stats at each level of Elite 2 promotion.
The Python script used to scrape data is available on GitHub here.
Acknowledgements
All this data was scraped programmatically from the Arknights wiki using Python and its implementation of Selenium. 
Inspiration
The dataset includes the complete stat evolution for all operators. This means it is possible to analyse operators across classes and understand things such as which classes excel at which stats or find outliers (i.e. operators that break the stats ""norm""). Moreover, it will open a glimpse into how Arknights' progression is balanced. As an ongoing game that receives new content and characters every couple of weeks, it is important to understand trends in operator releases, character progression and even balancing across rarity and classes."	37	771	6	ze1598	arknights-operator-stats
5356	5356	LenguajeManos		[]		2	12	0	miguelespinozac	lenguajemanos
5357	5357	Proffesors' scholar information	Keywords, cites, publications, profiles	['universities and colleges', 'education']	"Context
I'm about to study abroad but I have several choices which could definitely change my life path. Therefore I decided to use my experiences as a data analyst to make this choice. These data are my attempts to know my target professors.
Content
These data are including professors' information: in which my target professors are interested, how they are cited, full content about their publications.
Acknowledgements
Andreas Handel"	5	138	2	rezajafariraviz	proffesors-scholar-information
5358	5358	timeseriescv		[]		0	2	0	joseantonioalatorre	timeseriescv
5359	5359	Car Evaluation Database	Car Evaluation Database was derived from a simple hierarchical decision model 	[]	Car Evaluation Database was derived from a simple hierarchical decision model	5	34	0	dekeltv	car-evaluation-database
5360	5360	WiDS Datathon 2022 with Catboost output		['standardized testing']		6	208	4	lonnieqin	wids-datathon-2022-with-catboost-output
5361	5361	yolo-voc-weights		[]		1	14	0	romainsr	yolovocweights
5362	5362	Amazon Women's Shirts dataset	To collect the Data Amazon Rainforest API has been Utilized.	['beginner', 'computer vision', 'image data', 'text data', 'e-commerce services']	In order to collect the Data Amazon Rain forest API has been Utilized. This Dataset was originally in Json Format. Other Necessary Steps have been utilized to process and use the Data Efficiently.	8	65	0	owaiskhan9654	amazon-womens-shirts-dataset
5363	5363	chroma		[]		0	9	1	nkegke	chroma
5364	5364	CenterNet2_DLA_BiFPN_P3_4x_weight_file		[]		2	17	0	sahilchachra	centernet2-dla-bifpn-p3-4x-weight-file
5365	5365	diabetes		['diabetes']		42	59	1	buseyldrm	diabetes
5366	5366	longformer-baseline-sfold-f1-cv-0.6222		[]		0	3	0	atharvaingle	longformer-baseline-sfold-f1
5367	5367	FIFA22 OFFICIAL DATASET	From FIFA17 to FIFA22 statistics for each football player	['sports', 'exploratory data analysis', 'data visualization', 'feature engineering', 'online communities']	"[Updated 01/2022]
FIFA concerned: From FIFA17 to FIFA22
Dataset
The dataset contains +17k unique players and more than 60 columns, general information and all KPIs the famous videogame offers. As the esport scene keeps rising espacially on FIFA, I thought it can be useful for the community (kagglers and/or gamers)
Context
The data was retrieved thanks to a crawler that I implemented to retrieve: 
- Aggregated data such as name of the players, age, country
- Detailed data such as offensive potential, defense, acceleration
I like football a lot and this dataset is for me the opportunity to bring my contribution for the realization of projects that can go from simple analysis to elaboration of strategies on optimal composition under constraints... 
Acknowledgements
We wouldn't be here without the help of others. I would like to thanks @karangadiya who I got inspiration from, check his repo <a href=""https://github.com/amanthedorkknight/fifa18-all-player-statistics/tree/master/2019"">here</a> !
&gt; FIFA19 dataset: https://www.kaggle.com/karangadiya/fifa19
FIFA18 dataset: https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset
More details on the crawler
I used beautifulsoup to scrap https://sofifa.com/. First, I scrap the main page to get all general information and then, I scraped each player's webpage that is associated. I defined a batch size so I can parallelize the retrieving of the data. Then I merge all dataframes and cleaned the merged one. I have only 4 CPU and defined 5 batches:
- Without batch:  5h12
- With batch: 1h39
If you have any question or suggestion, feel free to comment !
Last update
I added concatenation of all dataframes. !!! Disclaimer !!! Id column is no longer primary key. the primary key would be Id + source together"	2438	16270	75	bryanb	fifa-player-stats-database
5368	5368	Rain volume 2021 in the Paris		[]		3	17	0	kidkuro	rain-volume-2021-in-the-paris
5369	5369	longformer-baseline-sfold-f0-cv-0.6232		[]		0	4	0	atharvaingle	longformer-baseline-sfold-f0
5370	5370	Spotify Play Count for Billboard's 1990 Top 100	Comparing streaming numbers of Top 100 most played songs on US radio in 1990 	['music', 'united states', 'beginner', 'data analytics', 'text data']	"Comparing the contemporary popularity (in terms of streaming frequency) of the top 100 most played songs on US radio in 1990. What genres of 1990 continue to captivate modern audiences, which songs have been forgotten by time? Genres sourced from Rateyourmusic. Chart positions sourced from Billboard. ""Girls Nite Out"" by Tyler Collins and ""Whole Wide World"" by A'Me Lorain are unavailable on Spotify.
I intend to evaluate subsequent year-end Billboard top 100 charts in the same manner."	344	2596	21	zacharykauz	spotify-play-count-for-billboards-1990-top-100
5371	5371	test_tobedeleted		[]		0	1	0	obrinkmann	test-tobedeleted
5372	5372	Covid19		[]		2	6	0	prinskumar	covid19
5373	5373	IMDB Reviews Dataset	IMDB Reviews Sentiment Prediction - NLP Problem	['movies and tv shows', 'psychology', 'beginner', 'nlp', 'classification', 'text data']	"Description
Understanding the sentiment behind the reviews of movies on IMDB.
Acknowledgements
The dataset is referred from Kaggle
Inspiration
NLP Research"	47	558	6	yasserh	imbd-reviews-dataset
5374	5374	APS 2022 Dataset		['education']		0	15	0	asta071	aps-2022-dataset
5375	5375	Mydata		['business']		0	38	0	tranngocdu	mydata
5376	5376	How (un)popular is Donald Trump?	An updating calculation of the ex-president's approval rating	['research', 'popular culture', 'politics', 'beginner', 'tabular data']		0	14	0	jiggy01	donald-trumps-popularity-using-approval-ratings
5377	5377	sd74.8		[]		0	1	0	iwillfindthatgirl	sd748
5378	5378	giga_fren		[]		15	8	0	nasheqlbrm	giga-fren
5379	5379	raga student		['music']		0	4	0	permanaraga	raga-student
5380	5380	Cough Audio Dataset		[]	"The dataset used consists of 135 cough files and 52 non-cough files from Google's AudioSet, 40 cough files and 1,960 non-cough files from the ESC-50 dataset, and 256 cough files and 10,801 non-cough files from the FSDKaggle2018 dataset.
Total Cough recordings: 451
Dataset Source: https://github.com/mdabdk/Cough-Detection"	1	45	0	himanshu007121	cough-audio-dataset
5381	5381	student		['universities and colleges']		0	4	0	permanaraga	student
5382	5382	StAnD (Small problems)	Small problems from the Static Analysis Dataset	['earth and nature', 'engineering']	"The Static Analysis Dataset (StAnD) is a large collection of solved linear static analysis problems on frame structures.
This dataset contains the subset of StAnD composed of small problems.
[Abstract] [Paper] [Code]
Context
Many algorithms for solving sparse linear systems are published at a great pace, but currently there is no standard dataset to compare their running time on real problems. To best of our knowledge there is no existing large dataset of sparse linear systems. 
A few sparse matrices (often less than 10) are published for many engineering problems in the Matrix Market or in the SuiteSparse matrix collection, but their limited number is not sufficient to measure the running time in the average case or to measure reliably how the resolution algorithm scales with the size and the number of non-zeros in the sparse matrix. Additionally, no constant term is provided in conjunction with the matrices. The constant term, is maybe not fundamental when direct methods are used, but it becomes important if we want to measure the effectiveness of iterative methods, whose behavior depends on the relationship between the initialization of the solution and the real solution.
Content
StAnD is composed of 303.000 static analysis problems of frame structures divided into 6 parts: 100.000 small training problems, 100.000 medium training problems, 100.000 large training problems, 1.000 small test problems, 1.000 medium test problems and 1.000 large training problems. The size of a problem is determined by the number of degrees of freedom (DOFs) of the structure model or equivalently by the number of rows or columns of the stiffness matrix associated to the structure. Small problems have 2115 DOFs on average (and at most 5166 DOFs), medium problems have about 7000 DOFs (and at most 14718 DOFs), while large problems can have up to 31770 DOFs with about 15500 DOFs on average.
The dataset is programmatically created using OpenSeesPy, the Python interface to the OpenSees finite element solver. For each structural model with its loading configuration, a static analysis is performed in order to compute nodal displacements. Therefore, every problem in the dataset is a tuple (K, f, u), where K is the sparse (symmetric positive-definite) stiffness matrix associated to the structure, f is a load vector and u is the ground-truth displacement vector such that K · u = f.
In the training set, for the same structure (i.e. the same stiffness matrix K), we apply different load configurations. In the test set, we use a single load configuration for every structure to maximize the variability.
Every solved system is saved in a separate .npz file. The name of the file follows the scheme {seed}{K hash}{u f hash}.npz. Problems in files with the same seed and the same K hash come from the same structure. The sparse stiffness matrix is saved in COO format.
Let us call n is the number of rows (and columns) of K and nnz is the number of non-null elements of K, then the keys of the .npz file are:
- A_indices: (2, nnz)-shaped array of row and column coordinates of non-null elements of K;
- A_values: nnz-shaped array of coefficients of non-null elements of K;
- x: n-shaped array containing to the linear system solution u;
- b: n-shaped array containing to the linear system constant term f.
Inspiration
We introduce StAnD to ease the comparison and the evaluation of new sparse linear system solvers and to spur research in resolution methods specifically tailored to structural engineering and static analysis problems."	0	71	0	zurutech	stand-small-problems
5383	5383	StAnD (Medium problems)	Medium problems from the Static Analysis Dataset	['earth and nature', 'engineering']	"The Static Analysis Dataset (StAnD) is a large collection of solved linear static analysis problems on frame structures.
This dataset contains the subset of StAnD composed of medium problems.
[Abstract] [Paper] [Code]
Context
Many algorithms for solving sparse linear systems are published at a great pace, but currently there is no standard dataset to compare their running time on real problems. To best of our knowledge there is no existing large dataset of sparse linear systems. 
A few sparse matrices (often less than 10) are published for many engineering problems in the Matrix Market or in the SuiteSparse matrix collection, but their limited number is not sufficient to measure the running time in the average case or to measure reliably how the resolution algorithm scales with the size and the number of non-zeros in the sparse matrix. Additionally, no constant term is provided in conjunction with the matrices. The constant term, is maybe not fundamental when direct methods are used, but it becomes important if we want to measure the effectiveness of iterative methods, whose behavior depends on the relationship between the initialization of the solution and the real solution.
Content
StAnD is composed of 303.000 static analysis problems of frame structures divided into 6 parts: 100.000 small training problems, 100.000 medium training problems, 100.000 large training problems, 1.000 small test problems, 1.000 medium test problems and 1.000 large training problems. The size of a problem is determined by the number of degrees of freedom (DOFs) of the structure model or equivalently by the number of rows or columns of the stiffness matrix associated to the structure. Small problems have 2115 DOFs on average (and at most 5166 DOFs), medium problems have about 7000 DOFs (and at most 14718 DOFs), while large problems can have up to 31770 DOFs with about 15500 DOFs on average.
The dataset is programmatically created using OpenSeesPy, the Python interface to the OpenSees finite element solver. For each structural model with its loading configuration, a static analysis is performed in order to compute nodal displacements. Therefore, every problem in the dataset is a tuple (K, f, u), where K is the sparse (symmetric positive-definite) stiffness matrix associated to the structure, f is a load vector and u is the ground-truth displacement vector such that K · u = f.
In the training set, for the same structure (i.e. the same stiffness matrix K), we apply different load configurations. In the test set, we use a single load configuration for every structure to maximize the variability.
Every solved system is saved in a separate .npz file. The name of the file follows the scheme {seed}{K hash}{u f hash}.npz. Problems in files with the same seed and the same K hash come from the same structure. The sparse stiffness matrix is saved in COO format.
Let us call n is the number of rows (and columns) of K and nnz is the number of non-null elements of K, then the keys of the .npz file are:
- A_indices: (2, nnz)-shaped array of row and column coordinates of non-null elements of K;
- A_values: nnz-shaped array of coefficients of non-null elements of K;
- x: n-shaped array containing to the linear system solution u;
- b: n-shaped array containing to the linear system constant term f.
Inspiration
We introduce StAnD to ease the comparison and the evaluation of new sparse linear system solvers and to spur research in resolution methods specifically tailored to structural engineering and static analysis problems."	0	29	0	zurutech	stand-medium-problems
5384	5384	student graduation		['education']		1	25	0	permanaraga	student-graduation
5385	5385	graduation student		['business', 'education']		4	33	2	permanaraga	graduation-student
5386	5386	pd_raw_db		[]		0	2	0	patrykzemla	pd-raw-db
5387	5387	ckpt GAN		[]		0	9	0	boost3000	ckpt-gan
5388	5388	inputdata		[]		0	2	0	hayden234	inputdata
5389	5389	salesd		[]		2	6	0	fiatreus786	salesd
5390	5390	Preprocessing Tool Kit		[]		0	21	0	nirmaldash	preprocessing-tool-kit
5391	5391	roberta-base-5fold-2 linear 256		[]		187	755	8	yuzhoudiyishuai	robertabase5fold2-linear-256
5392	5392	Cricket Team Records 2021	Dataset contains the cricket team records for T20 matches played in 2021	['cricket', 'beginner', 'tabular data', 'regression', 'sklearn']	"Context
The dataset encapsulates Cricket Team Records/Stats for the year 2021
Content
The dataset contains the following features gathered from ESPN stats for T20s played by each team in the year 2021- 
- Team
- Matches: T20 matches played this year to date
- Won: Matches won by the team this year
- Lost: Matches lost by the team this year
- Tied: Matches tied, played by the team
- NR: Matches with no result, played by the team
- W/L: Win to loss ratio
- Ave: Average runs per wicket
- RPO: Average runs per over (6 balls)
- Inns: Number of team innings
- HS: Highest Team Score
- LS: Lowest Completed Score
Acknowledgements
Data gathered from ESPN site - link here
Inspiration
The year 2020 and partly 2021 was a bad year for the sport. This dataset throws an insight into how teams are performing and possibly helps provide a predictive model to the upcoming matches this year."	53	744	13	ggsri123	cricket-team-records-2021
5393	5393	Real_time_mask_dataset	Real_time_mask_images for three classes 	['computer vision', 'image data', 'covid19']		4	191	14	balasubramaniamv	real-time-mask-dataset
5394	5394	MaskedFace-Net	Dataset of human faces with a correctly and incorrectly worn mask based on FFHQ	['computer vision', 'covid19']	Please refer to this https://github.com/cabani/MaskedFace-Net and the following paper https://arxiv.org/pdf/2008.08016.pdf for detailed information about this dataset. All credits to the authors.	3	129	1	charitarth	maskedfacenet
5395	5395	train_NER.csv		['internet']		0	3	0	keisuke07	train-nercsv
5396	5396	roberta_scratch_v6		[]		0	8	0	alexander1980	roberta-scratch-v6
5397	5397	sports		['sports']		0	11	0	madanmanikumarjaldu	sports
5398	5398	Anime DataSet 2022	Web scrape data from anime planet with 18000+ animes.	['arts and entertainment', 'business', 'exploratory data analysis', 'data cleaning', 'anime and manga', 'pandas']	"Context
My first Project on Web Scrapping.
Content
This Data is scrapped from https://www.anime-planet.com/
which consists of 18495 rows and 17 columns.
Acknowledgements
Thank You Anime Planet for providing such vast  Data on Animes.
Inspiration
Use this information to build a better anime recommended system."	1096	7204	62	vishalmane10	anime-dataset-2022
5399	5399	ft_cameron_bert_jigsaw		[]		0	2	0	hangy132	ft-cameron-bert-jigsaw
5400	5400	410612030 化學5 杜柏勳1		[]		0	2	0	duboxun	410612030-5-1
5401	5401	subset		['earth and nature']		0	1	0	madanmanikumarjaldu	subset
5402	5402	trial1_credit_score		[]		0	1	0	cvs2022	trial1-credit-score
5403	5403	twores		[]		0	2	0	sodhana	twores
5404	5404	Omicron Daily Cases	Per Google Data Analytic Capstone	['business']		19	126	1	ging21	project
5405	5405	Photo_livrable		[]		0	4	0	karimsalhi	photo-livrable
5406	5406	RSNA_CQ500_Abnormal_Data		[]		14	222	1	fereshtej	rsna-cq500-abnormal-data
5407	5407	train_df2		[]		0	3	0	teitlax	train-df2
5408	5408	Gamestop product reviews dataset	Gamestop product reviews	['video games', 'retail and shopping']	"Context
Gamestop reviews dataset is extracted by crawl feeds team for research and analysis purposes.
Content
Gamestop reviews dataset stats
total records: 4500
Fields: url, name, brand, sku, reviewer_name, review_title, review_description, recommended_review, verifed_purchaser, helpful_count, not_helpful_count, reviewed_at, images, rating, average_rating, reviews_count, reviews_link, comment_id, uniq_id, scraped_at
Fields count: 20
Acknowledgements
Crawl Feeds in house team extracted dataset from crawl feeds.
Inspiration
Get complete dataset crawlfeeds with more than 55K+ records. Large datasets helps to analyse things more better and accurate. Download complete dataset from crawl feeds."	20	108	3	crawlfeeds	gamestop-product-reviews-dataset
5409	5409	UPI apps Transactions in 2021	Dataset is about the transactions happened in 2021 by UPI apps.	['india', 'business', 'finance', 'e-commerce services', 'currencies and foreign exchange']	"<img src=""https://miro.medium.com/max/1400/1*94MvdhxeCQHoD7A4K1vlWg.png"">
Unified Payments Interface (UPI) is an instant real-time payment system developed by National Payments Corporation of India (NPCI) facilitating inter-bank peer-to-peer (P2P) and person-to-merchant (P2M) transactions.NPCI is umbrella organisation for all digital payments. The interface is regulated by the Reserve Bank of India (RBI) and works by instantly transferring funds between two bank accounts on a mobile platform. As of November 2021, there are 274 banks available on UPI with a monthly volume of 4.18 billion transactions and a value of ₹7.1 trillion (US$94 billion) UPI witnessed 68 billion transactions till November 2021. The mobile-only payment system helped transact a total of ₹34.95 lakh crore (US$460 billion) during the 67 months of operation starting from 2016. As of May 2021, the platform has 150 million monthly active users in India with plans to achieve 500 million by 2025. IIT Madras is also working to integrate voice command feature that can support English and Indian vernacular language in future. The proportion of UPI transactions in total volume of digital transactions grew from 23% in 2018-19 to 55% in 2020-21 with an average value of ₹1,849 per transaction"	551	2750	22	ramjasmaurya	upi-apps-transactions-in-2021
5410	5410	Images	These are my image for openCV	['computer science', 'programming', 'computer vision', 'deep learning', 'image data']	"Context
These are my images which I have uploaded to use them for opencv and Computervision"	5	77	5	mukeshmanral	images
5411	5411	yolo weights final		['exercise']		0	13	0	taoufikelkhaouja	yolo-weights-final
5412	5412	GlobalFood		[]		0	11	0	lauretasvarca	globalfood
5413	5413	Feedback-Prize-folds		[]		61	11	0	atharvaingle	feedbackprizefolds
5414	5414	Osteoarthritis_Augmented		[]		1	4	0	eishahassan	osteoarthritis-augmented
5415	5415	YoloR Kaggle HUB		['computer science']		31	128	1	remekkinas	yolor-kaggle-hub
5416	5416	Weather_data_2012-2019_krasnodar		[]		0	9	0	suslovalexey	weather-data-20122019-krasnodar
5417	5417	fun88asia		[]		0	2	0	fun88ben150	fun88asia
5418	5418	rdocuments	Scanned docs with skewness and angle of skewness	['business', 'computer science', 'programming']	"Context
Data with skewed documents and their angle of rotation.
Content
Data with skewed documents and their angle of rotation.
Inspiration
Skew correction with opencv works well but if we try for deep learning model it might yield even better results"	3	24	0	vishnunkumar	rdocuments
5419	5419	EV-dataset	Collection of 9504 pictures of Eeveelution art with annotations	['art', 'image data']	"Context
I don't see any labeled Eeveelution dataset out there so I decided to compose one. I hand-labeled thousands of the images myself.
Content
Artworks of eight types of evolved Eevee form and Eevee itself. Top folders denote the collected source. Single eeveelution art is directory-labeled and multiple eeveelution art is annotated in Metadata in .json files.
Acknowledgements
I acknowledge artists that draw them. And many thanks to Sap1231#1231 on Discord who provided me the first 6000 images for me to create the first EV-classifier. As well as the Eeveelution community that shares many Eeveelution images of which has contributed to the collection of this dataset.
Inspiration
Eevee detector
Eevee GAN"	18	598	5	longng0611	evdataset
5420	5420	sf-dst-car-price-prediction		['automobiles and vehicles']		1	8	0	olgamarkhai	sfdstcarpriceprediction
5421	5421	GDP_FIN_NOR_SWE_2015-2019_Multiple_Sources		['europe', 'government', 'economics', 'beginner', 'tabular data']	"Content
GDP data for Finland, Norway and Sweden between the years 2015 and 2019 from multiple sources:
UN data, IMF, OECD, World Bank
Hopefully these would be useful in the TPS Jan 2022 competition."	4	28	0	siukeitin	gdp-fin-nor-swe-20152019-multiple-sources
5422	5422	localnb002t-vae-regression-sub		['software']		0	9	0	riow1983	localnb002t-vae-regression-sub
5423	5423	byte_bigram		[]		0	11	0	ary0nk	byte-bigram
5424	5424	CQ500 Normal Images and Labels		[]		19	319	1	fereshtej	cq500-normal-images-and-labels
5425	5425	PyCaret Regression Blend Model	PyCaret Regression Blend Model	['beginner', 'optimization', 'automl']	"usage: 
TRAIN_PATH = ""../input/tabular-playground-series-jan-2022/train.csv""
TEST_PATH = ""../input/tabular-playground-series-jan-2022/test.csv""
SAMPLE_SUBMISSION_PATH = ""../input/tabular-playground-series-jan-2022/sample_submission.csv""
ID = ""row_id""
TARGET = ""num_sold""
SEED = 2022
CRITERIA = 'mape'
FOLD = 5
BLEND_FOLD = 5
BLEND_MODELS = ['rf','xgboost','catboost']
TUNE_FOLD = 5
model = PyCaretRegressionModel()
model.initPath(TRAIN_PATH,TEST_PATH,SAMPLE_SUBMISSION_PATH)
model.initSetting(ID,TARGET,SEED,CRITERIA)
model.initModel(FOLD,BLEND_FOLD,BLEND_MODELS,TUNE_FOLD)
model.buildModel()"	2	172	6	rhythmcam	pycaret-regression-auto-model
5426	5426	SDUchenfu		[]		1	19	0	sduchenfu	sduchenfu
5427	5427	taxi_data		[]		0	4	0	abrahamanderson	taxi-data
5428	5428	sdu-beef1		[]		0	2	0	sduchenfu	sdubeef1
5429	5429	Covid-19 WorldWide	Data Cleaning and Data Analysis	['business']		1	16	0	mdmodassirathar	covid19-worldwide
5430	5430	kick_off_data		[]		0	2	0	madtitan0210	kick-off-data
5431	5431	fooddata		[]		0	11	0	komalsunilkhachane	fooddata
5432	5432	data01		[]		1	6	0	antoniohong	data01
5433	5433	5gram_nlp_project		[]		10	91	0	jinkaido	5gram-nlp-project
5434	5434	E-Commerce Sales Dataset 🛒	Sales details of an E-Commerce platform	['economics', 'beginner', 'intermediate', 'tabular data', 'e-commerce services']	"Context
This dataset contains sales details of an E-Commerce platform. It covers 20.000 unique customers and 150.000 basket transactions.
Datasets
Customer Details
customer_id: Unique ID given to the Customer.
sex: Sex of customer.
customer_age: Age of customer.
tenure: tenure of customer as month
Basket Details
customer_id: Unique ID given to the Customer.
product_id: Unique ID of the product which has been added to the basket by the customer.
basket_date: Date of transaction.
basket_count: How many times this event occured during this date?"	777	5369	28	berkayalan	ecommerce-sales-dataset
5435	5435	data-set-auto-2022		['automobiles and vehicles']		1	18	0	olgamarkhai	datasetauto2022
5436	5436	xlm-roberta-base_tokenizer	xlm-roberta-base_tokenizer pretrained model saved here...	['arts and entertainment']		3	17	0	nanditab35	xlmrobertabase-tokenizer
5437	5437	synthesized images		[]		0	4	0	synboost	synthesized-images
5438	5438	indian liver		['health']		4	16	0	sakshigarg14	indian-liver
5439	5439	Data_set_auto_2022		[]		0	7	0	aleksandrnischik	data-set-auto-2022
5440	5440	Covid-19 Public Repository Data	Dataset containing details of projects hosted on GitHub regarding Covid-19	['earth and nature', 'text data', 'covid19']	"A comprehensive versioned dataset of the repositories and relevant related metadata about public projects hosted on GitHub related to the 2019 Novel Coronavirus and associated COVID-19 disease.
GitHub had received a number of enquiries from researchers and the community surrounding open collaboration on projects on the platform related to the disease COVID-19 caused by the SARS-CoV-2 virus. Many projects, ordered by star count, can be found using the covid-19 topic on GitHub, however, discovery of other important projects is difficult due to differences in the way users self identify their work.
to hear about it so that we can help ensure it becomes more prominently featured. Please open a PR against the file USER_SUBMISSIONS.md with a link to your research. We are especially interested in highlighting the most promising and impactful projects in need of community help and support.
Open data
Open source is bigger than any company or community. The dataset is released under CC0-1.0 for anyone to use and learn from.
There are two main sets of files, released via TSV and json formats for public consumption in the directory data/. A comprehensive data dictionary that explains the contents of these files is here. The files are sorted in descending order by the count of distinct contributors at the time of extract.
The files have been versioned based on a weekly snapshot of identified repositories from the week of 2020-01-20 onward.
We will update this repository with new data files on a monthly basis, generally on the first Tuesday of a month. We will revisit this each month and provide an update on continuing this commitment.
Identification methodology
Rather than relying on any one GitHub topic to identify potential COVID-19 related projects, the data set is produced using a more comprehensive set of search criteria to identify projects likely to be COVID-19 related.
Note: This has the potential to include a small number of false positives however we figured we were better to cast a wide net and allow consumers of the data to perform additional cleaning if they desire.
Furthermore, since this data is versioned based on the week the repo was initially created, there may exist data that are included for repos that were originally public that have been made private and are currently inaccessible.
The following parts of public metadata are currently being used to identify public projects (those licensed and not) as COVID-19 related:
The repo's description
The name of the repo
The topics associated with the repo
The organization bio description where that exists
Search terms against these metadata include variations of: covid, coronavirus, ncov and sars-cov-2"	1	28	0	niravdas	covid19-open-source-project-github-repositories
5441	5441	Kaggle Competitions Ranking	Top 5000 Kaggle Competitions Ranking	['computer science', 'regression']	"Context
This dataset contains Kaggle ranking of competitions.
Content
5000 rows and 8 columns.
Columns' description are listed below.
Rank : Rank of the user
Tier : Grandmaster, Master or Expert
Username : Name of the user
Join Date : Year of join
Gold Medals : Number of gold medals
Silver Medals : Number of silver medals
Bronze Medals : Number of bronze medals
Points : Total points
Acknowledgements
Data from Kaggle.
Image from Olympics.
If you're reading this, please upvote."	56	600	11	vivovinco	kaggle-competitions-ranking
5442	5442	train housing		['business']		2	16	0	shivamchaudhary11	train-housing
5443	5443	train dataset		['transportation']		0	0	0	shivamchaudhary11	train-dataset
5444	5444	Leaf Classification	tensorflow Sequential	[]		1	37	1	novinict1	leaf-classification
5445	5445	ten-kfold-8-test		[]		0	3	0	aliyuibrahimtetengi	tenkfold8test
5446	5446	first image		[]		0	4	0	shashwatnaidu07	first-image
5447	5447	Formula One Cars	This dataset contains Formula One cars image data curated by teams.	['auto racing', 'automobiles and vehicles', 'image data', 'multiclass classification', 'online communities']	"Context
Hello fellow F1 fanatics, I've created this dataset which contains formula one team wise image data of their respective racing cars.
Content
The images of the F1 cars have been acquired by scraping DuckDuckGo's search results. The dataset has been cleaned using Fastai's ImageClassifierCleaner.
Inspiration
You guys can use this dataset to create Image Classification models."	33	541	4	vesuvius13	formula-one-cars
5448	5448	Kaggle ranking datasets	User data of top 1000 in Kaggle rankings in the each of the month	['computer science', 'tabular data']	"What's these datasets?
This is the top 1000 user data of the four types of rankings (i.e., Compeition, Datasets, Notebooks, and Discussion) from October 2021. The data will be scraped from the Kaggle Ranking every month. The scraping code is in GitHub.
Note: The raked users in Datasets ranking are less than 1,000 users, so technically there will be less than 1,000 users in the top_1000_datasets_####.csv file.
Dates the ranking data scraped
In 2021:
Competitions ranking: Oct. 4, Nov. 21, Dec. 16
Datasets ranking: Oct. 12, Nov. 21, Dec. 16
Notebooks ranking: Oct. 13, Nov. 23, Dec. 16
Discussion ranking: Oct. 17, Nov. 23, Dec. 16
In 2022:
Competitions ranking: Jan. 16
Datasets ranking: Jan. 16
Notebooks ranking: Jan. 16
Discussion ranking: Jan. 16"	52	918	6	hdsk38	comp-top-1000-data
5449	5449	ExploratoryDataAnalysisWithPython	National Survey of Family Growth	['business']		3	15	0	kakamana	pythonworkingwithdatesandtimes
5450	5450	FIFA - Football World Cup Dataset	Records from all the Football World Cups (1930 to 2018)	['football', 'sports', 'categorical data', 'beginner', 'tabular data']	"Context
The FIFA World Cup, often simply called the World Cup, is an international association football competition contested by the senior men's national teams of the members of the Fédération Internationale de Football Association (FIFA), the sport's global governing body. The championship has been awarded every four years since the inaugural tournament in 1930, except in 1942 and 1946 when it was not held because of the Second World War. The current champion is France, which won its second title at the 2018 tournament in Russia.
The current format involves a qualification phase, which takes place over the preceding three years, to determine which teams qualify for the tournament phase. In the tournament phase, 32 teams, including the automatically qualifying host nation(s), compete for the title at venues within the host nation(s) over about a month.
The 21 World Cup tournaments have been won by eight national teams. Brazil have won five times, and they are the only team to have played in every tournament. The other World Cup winners are Germany and Italy, with four titles each; Argentina, France, and inaugural winner Uruguay, with two titles each; and England and Spain, with one title each.
The World Cup is the most prestigious association football tournament in the world, as well as the most widely viewed and followed single sporting event in the world. The cumulative viewership of all matches of the 2006 World Cup was estimated to be 26.29 billion with an estimated 715.1 million people watching the final match, a ninth of the entire population of the planet.
17 countries have hosted the World Cup. Brazil, France, Italy, Germany, and Mexico have each hosted twice, while Uruguay, Switzerland, Sweden, Chile, England, Argentina, Spain, the United States, Japan, and South Korea (jointly), South Africa, and Russia have each hosted once. Qatar will host the 2022 tournament, and 2026 will be jointly hosted by Canada, the United States, and Mexico, which will give Mexico the distinction of being the first country to host games in three World Cups.
Content
This Dataset consists of Records from all the previous Football World Cups (1930 to 2018)
Acknowledgements
For more, please visit - https://www.fifa.com/"	565	3507	39	iamsouravbanerjee	fifa-football-world-cup-dataset
5451	5451	exp059-059-convnext-224-		[]		0	4	0	kurokurob	exp059-059-convnext-224-
5452	5452	combinded_data		[]		0	1	0	wallacefqq	combinded-data
5453	5453	coatnet		[]		0	2	0	tmtrngt	coatnet
5454	5454	Braille_swaraj		[]		0	10	0	swaraj6955	braille-swaraj
5455	5455	Alzheimer		[]		0	47	0	hariprakashk18csr053	alzheimer
5456	5456	Car Price dataset		[]		1	27	1	chitrabirole	car-price-dataset
5457	5457	new_refined_s1_data		[]		0	3	0	iiitbh56	new-refined-s1-data
5458	5458	effdet models		['clothing and accessories']		0	2	1	prateekagnihotri	effdet-models
5459	5459	District wise MSME in India	Number of MSME in Indian Districts from Manufacturing and Service Sector	['india', 'business']	"The Dataset contain District wise Micro, Small and Medium Enterprises in Service Sector and Manufacturing Sector in India. 
Total MSME in every District visualized.
Last Updated: 16-01-2022
Source: https://data.gov.in/"	10	53	1	harshitatiwari23	district-wise-manufacturing-and-service-msme
5460	5460	indian_startup_funding_2022jan16		[]		2	6	0	harnestroysantiago	indian-startup-funding-2022jan16
5461	5461	fashion-mnist_train		['clothing and accessories']		1	27	0	antoniohong	fashionmnist-train
5462	5462	Detectron-Swin Transformer	Detectron-Swin Transformer	[]		0	31	1	forcewithme	yydsyyds
5463	5463	Jazbreaux		['beginner']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	2	14	1	josephbreaux80	jazbreaux
5464	5464	toxic_model_v2		[]		0	1	0	hawzen	toxic-model-v2
5465	5465	yolov5l-xs weights		['exercise']		0	11	0	gigggggge	yolov5lxs-weights
5466	5466	train.csv		[]		0	2	0	saminkhann	traincsv
5467	5467	toxic_model		[]		0	3	0	hawzen	toxic-model
5468	5468	Hand_Gesture_for_Numbers	gesture for numbers(0-9)	[]		0	9	0	xiongxiongzhizhi	hand-gesture-for-numbers
5469	5469	gasCar_03k dataset		['earth and nature']		0	18	1	rochel	gascar-03k-dataset
5470	5470	Manga Faces Dataset	Manga Faces Dataset classified with facial expressions classes	['gan', 'anime and manga', 'mexico', 'numpy', 'pytorch']	"Context
This database is focused on manga face generation using architectures like Gans.
This dataset is an extension of the original Manga Facial Expressions, adding more classes and images, extending it to 11 classes and 670 images.
Content
670 manga faces images, divided in 11 classes.
• Angry (53)
• Blushed (55)
• Confused (5)
• Crying (64)
• Embarrassed (68)
• Happy (119)
• Normal (92)
• Pleased (37)
• Pout (16)
• Sad (56)
• Shock/Surprised (106)
Acknowledgements
This database is an extension of the original database Manga Facial Expressions
[Original creator] (https://www.kaggle.com/mertkkl)
Invitation
If you want me to extend this database please contact me to create a robust database for future projects."	4	113	2	davidgamalielarcos	manga-faces-dataset
5471	5471	Mushaf Madinah		[]		0	1	1	mohamedmoaz	mushaf-madinah
5472	5472	CXR_autuencoder_256		[]		1	5	0	metformin	cxr-autuencoder-256
5473	5473	ro_fa_clean_translate		[]		6	33	0	ilikehaskell	ro-fa-clean-translate
5474	5474	The Quran and it's Numeric Miracles	القرآن الكريم و الإعجاز العددي	['religion and belief systems', 'text data']	"Context
الاعجاز العددي في القرآن و حساب الجمل
The Miracle of Quran and Alphabet Numerals
Allah Said: (4:82) Then do they not reflect upon the Qur'an? If it had been from [any] other than Allah, they would have found within it much contradiction
قال تعالى &gt; أَفَلَا يَتَدَبَّرُونَ الْقُرْآنَ ۚ وَلَوْ كَانَ مِنْ عِندِ غَيْرِ اللَّهِ لَوَجَدُوا فِيهِ اخْتِلَافًا كَثِيرًا 82&gt; سورة النساء
For the last twenty years or more, different scholars, such as Abderrazzaq Nufal, Bassám Jarrar, Abdu A-Daim Kuhail, among others, have studied the miracle of numbers of the Holy Quran. The truth is that the use of computers and statistics programs have made this task much easier, leading to the acknowledgment of the miracle of numbers in the Quran. These results speak for themselves, the numbers don't lie, and show the divine origin of this Sacred Text
I hope you use this dataset to dig deeper and find your way to yet another astonishing number miracle of The Quran.
Content
For the sensitive nature of this dataset and the work that really depend on the percise of the digtal text of Quran I only use a well know sources and highly accurate text of Hafs narration of Quran. 
sources:
1. The King Fahd Glorious Qur’an Printing Complex
2. Quran Metadata from Tanzil Project 
I followed Noon Islamic Center Rules for calculating Alphabet Numerals (حساب الجمل)
For more information about my dataset I have notebook that guides you step by step of how I created and developed this data and you can even create one by yourself. notebook for creating this dataset
Acknowledgements
I really wouldn't be interested in The numeric miracles of Quran without the effort of Noon Islamic Center and sheikh Bassam Jarrar
so all thanks goes to them and their Youtube channel
Inspiration
We are in the digital era and we are just in the begging of findings of Numerals Miracle in Quran so do your best and find your own questions and answers. Here are some question to help you start
Could you find any new number Miracle?
What are the patterns of numbers and their colorations?"	13	200	5	mohamedmoaz	numeric-quran
5475	5475	PlantSegDatasets	Two plant image datasets with ground-truth segmentation 	['earth and nature', 'plants', 'computer vision', 'image data']	"Presented within the dataset are two sets of images of weed plants and sweetpotato slips/vine cuttings, which were acquired under natural field conditions and indoors, respectively. The weed data consists of 50 color images (.jpg) and pixel-wise annotated plant masks (.png), and the vine cutting data contains 60 color images (.jpg) and corresponding plant masks (.png). These datasets were originally prepared as part of image data used for evaluating plant segmentation methods as described in this paper. 
If you use the dataset(s) in your research, you may consider citing:
Yuzhen Lu, Sierra Young, 2022. Robust plant segmentation of color images based on image contrast optimization. Computers and Electronics in Agriculture. https://doi.org/10.1016/j.compag.2022.106711."	3	90	1	yuzhenlu	plantsegdatasets
5476	5476	The Bachelor 	The Bachelor and Contestants from seasons 1-2, 5, 9-26	['arts and entertainment', 'celebrities']		4	18	0	aditirajagopal	the-bachelor
5477	5477	C02 Emission by Prod/Cons, GDP and Population	C02 Emission by Country, Consumption, Production, GDP and Population	['public health', 'environment', 'pollution', 'energy', 'oil and gas']	"The world is becoming more modernized by the year, and with this becoming all the more polluted.
This data was pulled from the US Energy Administration and joined together for an easier analysis. Its a collection of some big factors that play into C02 Emissions, with everything from the Production and Consumption of each type of major energy source for each country and its pollution rating each year. It also includes each countries GDP, Population, Energy intensity per capita (person), and Energy intensity per GDP (per person GDP). All the data spans  all the way from the 1980's to 2020.
Feature Descriptions:
* Country - Country in question
* Energy_type - Type of energy source
* Year - Year the data was recorded
* Energy_consumption - Amount of Consumption for the specific energy source, measured (quad Btu)
* Energy_production - Amount of Production for the specific energy source, measured (quad Btu)
* GDP - Countries GDP at purchasing power parities, measured (Billion 2015$ PPP)
* Population - Population of specific Country, measured (Mperson)
* Energy_intensity_per_capita - Energy intensity is a measure of the energy inefficiency of an economy. It is calculated as units of energy per unit of capita (capita = individual person), measured (MMBtu/person)
* Energy_intensity_by_GDP - Energy intensity is a measure of the energy inefficiency of an economy. It is calculated as units of energy per unit of GDP, measred (1000 Btu/2015$ GDP PPP)
* CO2_emission - The amount of C02 emitted, measured  (MMtonnes CO2)"	33	230	0	lobosi	c02-emission-by-countrys-grouth-and-population
5478	5478	NYC Homeless Service Requests (2019)	311 Service Requests Recieved By The Department of Homeless Services In 2019	['united states', 'housing', 'public safety', 'social issues and advocacy']	311 is a non-emergency number that provides access to non-emergency municipal services. The following dataset was collected from New York City's public dataset via Google's marketplace.	8	102	5	donnetew	nyc-homeless-service-requests-2019
5479	5479	yolor_pytorch	You Only Learn One Representation: Unified Network for Multiple Tasks	['computer vision', 'cnn', 'gpu', 'pytorch']	"YOLOR
implementation of paper - You Only Learn One Representation: Unified Network for Multiple Tasks
paper : https://arxiv.org/pdf/2105.04206.pdf
Citation
@article{wang2021you,
  title={You Only Learn One Representation: Unified Network for Multiple Tasks},
  author={Wang, Chien-Yao and Yeh, I-Hau and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2105.04206},
  year={2021}
}"	5	319	0	kozistr	yolor-pytorch
5480	5480	kodim_shivam		[]		0	10	0	shivampal2112	kodim-shivam
5481	5481	Eurostat - Deaths in EU countries 2000-2021	Total numbers of weekly deaths	['europe', 'people', 'tabular data', 'news']	Through the dataset one can get insight into change of weekly death over last two decades, and what most interesting interesting anomalies in last two years.	7	224	0	jarxrr	eurostat-weekly-deaths-in-eu-countries-2000
5482	5482	Bangalore house price-Updated		[]		0	12	1	blessontomjoseph	bangalore-house-priceupdated
5483	5483	Orders_Autumn_2020		[]		2	4	0	jackmcmenamin	orders-autumn-2020
5484	5484	Sports Stadium Locations	Latitude and Longitude coordinates of MLB, NFL, NBA, NHL, and MLS stadiums	['football', 'baseball', 'basketball', 'sports', 'tabular data']	"Content
Contains the latitude and longitude coordinates in decimal format of every Major League Baseball (MLB), National Football League (NFL), National Basketball Association (NBA), National Hockey League (NHL), and Major League Soccer (MLS) team's home stadium. Also includes information about each team's division.
Note that as teams change names, new stadiums are built, and sports league realign divisions this information will become out of date.
Credit to Mick Haupt via Unsplash for the banner photo."	27	280	5	logandonaldson	sports-stadium-locations
5485	5485	Mecari_Notebook_price-Predication		[]		1	16	0	antonyaragland	mecari-notebook-pricepredication
5486	5486	datasetlivrable2		[]		0	3	0	adriendneirda	datasetlivrable2
5487	5487	covid19		[]		1	11	0	mayarmansour	covid19
5488	5488	dataset-livrable-1		[]		0	5	0	adriendneirda	datasetlivrable1
5489	5489	shopify_data		[]		8	40	0	zhansi	shopify-data
5490	5490	scRNA-seq collection of cancer cell lines	Broad Institute scRNA-seq data	['genetics', 'biology', 'biotechnology', 'cancer']	"Data and Context
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes (csv file is vice versa).
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Particular data:
GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE157220
BROAD: https://singlecell.broadinstitute.org/single_cell/study/SCP542/pan-cancer-cell-line-heterogeneity
Paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8135089/
Pan-cancer single cell RNA-seq uncovers recurring programs of cellular heterogeneity
Gabriela S. Kinker,,1,4 Alissa C. Greenwald,,1 Rotem Tal,1 Zhanna Orlova,1 Michael S. Cuoco,2 James M. McFarland,3 Allison Warren,3 Christopher Rodman,2 Jennifer A. Roth,3 Samantha A. Bender,3 Bhavna Kumar,5 James W. Rocco,5 Pedro ACM Fernandes,4 Christopher C. Mader,3 Hadas Keren-Shaul,6,7 Alexander Plotnikov,6 Haim Barr,6 Aviad Tsherniak,3 Orit Rozenblatt-Rosen,2 Valery Krizhanovsky,1 Sidharth V. Puram,8 Aviv Regev,2 and Itay Tirosh1,#
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	7	218	2	alexandervc	scrnaseq-collection-of-cancer-cell-lines
5491	5491	phishing		[]		2	18	1	johnadeniyi	phishing
5492	5492	SciPy for beginners data		['computer science', 'programming']		1	14	3	bhavinmoriya	scipy-for-beginners-data
5493	5493	Nuclear disaster		[]		3	17	0	bhavinmoriya	nuclear-disaster
5494	5494	abohavaha		[]		0	0	0	salehkheiri	abohavaha
5495	5495	1234532		[]		0	11	0	chenchu123	1234532
5496	5496	Air Quality Across Countries in COVID-19	Air Quality Parameters data across geographies during COVID-19	['cities and urban areas', 'environment', 'pollution', 'weather and climate', 'covid19']	"Air Quality Parameters data across geographies during COVID-19
The dataset contains information on Air Quality Parameters across geographies. Each CSV file holds the details for a single country of the Air Quality Values for the years 2020-2021. Within each CSV file data for multiple cities located within the country is made available. 
The following are the Air Quality Parameters whose data is present in the files (Along with their notations mentioned in brackets):
Carbon Monoxide (CO)
Dew (dew)
Humidity (humidity)
Nitrogen Dioxide (NO2)
Ozone (O3)
Particulate Matter 10 (pm10)
Particulate Matter 2.5 (pm25)
Pressure (pressure)
Sulphur Dioxide (SO2)
Temperature
Wind Gusts
Wind Speed"	483	3287	32	aestheteaman01	air-quality-across-countries-in-covid19
5497	5497	Wine Quality Dataset	Wine Quality Prediction - Classification Prediction	['alcohol', 'earth and nature', 'beginner', 'classification', 'tabular data', 'food']	"Description:
This datasets is related to red variants of the Portuguese ""Vinho Verde"" wine.The dataset describes the amount of various chemicals present in wine and their effect on it's quality. The datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).Your task is to predict the quality of wine using the given data.
A simple yet challenging project, to anticipate the quality of wine.
The complexity arises due to the fact that the dataset has fewer samples, & is highly imbalanced.
Can you overcome these obstacles & build a good predictive model to classify them?
This data frame contains the following columns:
Input variables (based on physicochemical tests):\
1 - fixed acidity\
2 - volatile acidity\
3 - citric acid\
4 - residual sugar\
5 - chlorides\
6 - free sulfur dioxide\
7 - total sulfur dioxide\
8 - density\
9 - pH\
10 - sulphates\
11 - alcohol\
Output variable (based on sensory data):\
12 - quality (score between 0 and 10)
Acknowledgements:
This dataset is also available from Kaggle & UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality.
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict the wine quality.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	3380	20596	135	yasserh	wine-quality-dataset
5498	5498	xfbfbg		[]		0	12	0	artemtaganov	xfbfbg
5499	5499	petfinder-data		[]		3	68	1	titericz	petfinderdata
5500	5500	satellite images		['government', 'computer vision', 'image data', 'urban planning']		2	41	0	aminebensalah	satellite-images
5501	5501	Precipitação de Chuva em Belo Horizonte	Precipitação de Chuva em Belo Horizonte-MG (Brasil) de 2006 a 2021	['weather and climate']		2	18	2	diegomariano	chuva-belo-horizonte-brasil-2006-2021
5502	5502	EvsMeshcutoff		[]		0	14	0	renatacardinot	evsmeshcutoff
5503	5503	youtube video titles		[]		0	27	0	pushpaksaraf	youtube-video-titles
5504	5504	products sales report		[]		4	33	0	shaimaamadkour	products-sales-report
5505	5505	Inertial data for dog behaviour classification	Wearable IMU data from dogs	['animals', 'classification', 'tabular data']	"Context
The dataset includes movement sensor data from sensors placed on the collar and the harness of a dog and recorded while the dog is given tasks or activities to perform. The task are: galloping, lying on chest, sitting, sniffing, standing, trotting, and walking. The movement sensors used are: ActiGraph GT9X Link (ActiGraph LLC, Florida, USA) and they include 3D accelerometer and 3D gyroscope. The sampling rate used is 100 Hz. 
Content
DogMoveData.csv - IMU output (xyz axes for both gyro and accelerometer) plus behaviour labels 
DogInfo.csv - Information on the subjects 
Data_description.txt - Further information on column names etc
Acknowledgements
The dataset is described in more detail in the data description article: Vehkaoja, A., Somppi, S., Törnqvist, H., Valldeoriola Cardó, A.,  Kumpulainen, P., Väätäjä, H., Majaranta, P., Surakka, V.,  Kujala, M. V., Vainio, O., Description of Movement Sensor Dataset for Dog Behavior Classification, Data in Brief, 2022.
The behavior classification results obtained with the dataset are published in: Kumpulainen, P., Valldeoriola Cardó, A., Somppi, S., Törnqvist, H., Väätäjä, H., Majaranta, P., Gizatdinova, Y., Hoog Antink, C., Surakka, V., Kujala, M. V., Vainio, O., and Vehkaoja, A., Dog behaviour classification with movement sensors placed on the harness and the collar, Applied Animal Behavior Science, 241 (2021): 105393. https://doi.org/10.1016/j.applanim.2021.105393
The authors of the dataset request researchers to refer to the aforementioned publications when using the data and publishing results produced using it.
Data downloaded from https://data.mendeley.com/datasets/vxhx934tbn/2 
Inspiration
Use this dataset to classify dog movements! 
Licence
CC BY 4.0 license
The files associated with this dataset are licensed under a Creative Commons Attribution 4.0 International license.
What does this mean?
You can share, copy and modify this dataset so long as you give appropriate credit, provide a link to the CC BY license, and indicate if changes were made, but you may not do so in a way that suggests the rights holder has endorsed you or your use of the dataset. Note that further permission may be required for any content within the dataset that is identified as belonging to a third party."	26	344	4	benjamingray44	inertial-data-for-dog-behaviour-classification
5506	5506	yyyyyy		[]		0	7	0	fernandobordi	yyyyyy
5507	5507	laptop_price_prediction_dataset	A dataset of laptop specifications which are used to predict the laptop price	['business', 'intermediate', 'linear regression', 'tabular data']		8	96	0	aggle6666	laptop-price-prediction-dataset
5508	5508	Diluc Lines Genshin		[]		22	48	0	erinkoch	diluc-lines-genshin
5509	5509	Stellar Classification Dataset - SDSS17	Classification of Stars, Galaxies and Quasars. Sloan Digital Sky Survey DR17	['earth and nature', 'astronomy', 'physics', 'beginner', 'classification', 'multiclass classification']	"Similar Datasets
CERN Proton Collision Dataset: LINK
Airfoil Self-Noise Dataset: LINK
CERN Electron Collision Data: LINK
Context
In astronomy, stellar classification is the classification of stars based on their spectral characteristics. The classification scheme of galaxies, quasars, and stars is one of the most fundamental in astronomy. The early cataloguing of stars and their distribution in the sky has led to the understanding that they make up our own galaxy and, following the distinction that Andromeda was a separate galaxy to our own, numerous galaxies began to be surveyed as more powerful telescopes were built. This datasat aims to classificate stars, galaxies, and quasars based on their spectral characteristics.
Content
The data consists of 100,000 observations of space taken by the SDSS (Sloan Digital Sky Survey). Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.
1. obj_ID = Object Identifier, the unique value that identifies the object in the image catalog used by the CAS
1. alpha = Right Ascension angle (at J2000 epoch)
1. delta = Declination angle (at J2000 epoch)
1. u = Ultraviolet filter in the photometric system
1. g = Green filter in the photometric system
1. r = Red filter in the photometric system
1. i = Near Infrared filter in the photometric system
1. z = Infrared filter in the photometric system
1. run_ID = Run Number used to identify the specific scan
1. rereun_ID = Rerun Number to specify how the image was processed
1. cam_col = Camera column to identify the scanline within the run
1. field_ID = Field number to identify each field
1. spec_obj_ID = Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)
1. class = object class (galaxy, star or quasar object)
1. redshift = redshift value based on the increase in wavelength
1. plate = plate ID, identifies each plate in SDSS
1. MJD = Modified Julian Date, used to indicate when a given piece of SDSS data was taken
1. fiber_ID = fiber ID that identifies the fiber that pointed the light at the focal plane in each observation
Citation
&gt; fedesoriano. (January 2022). Stellar Classification Dataset - SDSS17. Retrieved [Date Retrieved] from https://www.kaggle.com/fedesoriano/stellar-classification-dataset-sdss17.
Acknowledgements
The data released by the SDSS is under public domain. Its taken from the current data release RD17.
- More information about the license: http://www.sdss.org/science/image-gallery/
SDSS Publications: 
- Abdurro’uf et al., The Seventeenth data release of the Sloan Digital Sky Surveys: Complete Release of MaNGA, MaStar and APOGEE-2 DATA (Abdurro’uf et al. submitted to ApJS) [arXiv:2112.02026]"	871	8800	64	fedesoriano	stellar-classification-dataset-sdss17
5510	5510	tripdata		[]		0	4	0	gomesneto98	tripdata
5511	5511	Electronic_sales_data	Sales Data of Electronic products	['business']		8	56	1	shreyhenry	electronic-sales-data
5512	5512	Top 1000 Highest Grossing Movies	Top 1000 Highest Grossing Holywood Movies	['movies and tv shows', 'beginner', 'intermediate', 'exploratory data analysis', 'data visualization']	"Context
This dataset contains information about the top 1000 highest grossing holywood films. It is up to date as of 10th January 2022.
Acknowledgements
This data has been scraped from multiple site and has been added together for performing various datat operations. The data has been taken from idmb, rotten tomatoes and many other sites.
UPDATE:
The original data contained information about movies and some additional have been added by me. 'None' refers to datapoints that I could not scrape. If you wish to contribute to this dataset. Do contact me :)"	2150	11447	89	sanjeetsinghnaik	top-1000-highest-grossing-movies
5513	5513	efficientdetv2-init-weights		['exercise']		0	8	0	dolphin15	efficientdetv2initweights
5514	5514	Eye Disease Deep Learning Dataset	712 ocular staining images of flaky corneal ulcers	['healthcare', 'diseases', 'artificial intelligence', 'cnn', 'image data']	"Context
I think there are extremely lack of open datasets and algorithms for accelerating medical AI. So, I'm researching to make a global baseline classifier for hospitals which has not enough data and AI capability. I hope you are interested in, too.
Content
The labels of this dataset consists of 3 categories, 5 types and 5 grades.
It can be 75 multi-labels. 
Category
point-like corneal ulcers
point-flaky mixed corneal ulcers
flaky corneal ulcers
Types
type 0 : No ulcer of the corneal epithelium
type 1 : Micro punctate
type 2 : Macro punctate
type 3 : Coalescent macro punctate
type 4 : Patch (&gt;=1 mm)
Grade
grade 0 : No ulcer of the corneal epithelium
grade 1 : Corneal ulcers involve only one surrounding quadrant
grade 2 : Corneal ulcers involve two surrounding quadrants
grade 3 : Corneal ulcers involve three or four surrounding quadrants
grade 4 : Corneal ulcers involve the central optical zone of the cornea
Acknowledgements
Deng, L., Lyu, J., Huang, H. et al. The SUSTech-SYSU dataset for automatically segmenting and classifying corneal ulcers. Sci Data 7, 23 (2020). https://doi.org/10.1038/s41597-020-0360-7"	56	727	5	bongsang	eye-disease-deep-learning-dataset
5515	5515	Flipkart Dataset		[]		2	17	0	subhajit2001	flipkart-dataset
5516	5516	models_stage_2		[]		0	2	0	qinyukun	models-stage-2
5517	5517	Google Play Store		[]		0	14	0	churros247	google-play-store
5518	5518	NYC Motor Vehicle Collisions to Person	Killed, Injured Case at 2021	['law', 'automobiles and vehicles', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics', 'classification']	"Context
It is data of a collision accident between a person and a motor vehicle that occurred in New York in 2021. Only cases of injury or death over $1,000 were filtered out. The date and time of the incident, the location of the injury, and the police description of the incident are summarized.
Content
CRASH_DATE 
CRASH_TIME    
PERSON_INJURY  : Injured, killed, unspecified
PERSON_AGE     : Automatically calculated based on date of birth
BODILY_INJURY : Injured body area (i.e. head, face, neck, etc.)
SAFETY_EQUIPMENT : Safety equipment being used (i.e. lap belt, harness, child restraint, air bag, etc.)
PERSON_SEX 
PERSON_TYPE : Bicyclist, Motor Vehicle Occupant, Pedestrian
PED_LOCATION : Location of the pedestrian (i.e. at intersection, not at intersection)
CONTRIBUTING_FACTOR_2 : Factors contributing to the collision for designated vehicle
EJECTION : Indicates the following: Not ejected, partially ejected, or ejected from the vehicle
COMPLAINT : Type of physical complaint (ex. Concussion, severe burn, severe bleeding, etc.)
EMOTIONAL_STATUS : Apparent death, unconscious, semiconscious, etc.
VEHICLE_ID : Unique vehicle record associated with person. Foreign Key to the vehicle table
PERSON_ID : Person identification code assigned by system
CONTRIBUTING_FACTOR_1 : Factors contributing to the collision for designated vehicle
POSITION_IN_VEHICLE : Seating position #1-#8 (i.e. driver, front passenger, etc.)
PED_ROLE :    Pedestrian, witness, in-line skater, other, etc.
UNIQUE_ID : Unique record code generated by system. Primary Key for Person table.
PED_ACTION : What the pedestrian was doing at time of crash (i.e., walking with the signal, against the signal, etc.)
COLLISION_ID :    Crash identification code. Foreign Key, matches unique_id from the Crash table.
Inspiration
Through EDA, you can check the characteristics of each traffic incident by date and by type of incident.    
Source
https://data.cityofnewyork.us/"	520	4120	44	kukuroo3	nyc-motor-vehicle-collisions-to-person
5519	5519	Cyclistic-Divvy-Trips-2021	Divvy Trip Data (January 2021 - December 2021)	['travel']	"Context
This dataset was created in order to complete Case Study 1 of the Google Data Analytics Certificate.
Content
This dataset contains 12 months of trip data from Chicago's Divvy Ride Share Service for the period January to December 2021. The dataset was acquired from here.
The data has been made available by Motivate International Inc. under this license.
Inspiration
I hope this will help others to also complete their case study."	19	132	3	michaeljohnsonjr	divvytrips2021
5520	5520	[PetFinder.my]107th solution		[]		4	9	0	xxllxl	petfindermy107th-solution
5521	5521	MH_Fruit_properties		[]		0	5	0	tirthankardas	mh-fruit-properties
5522	5522	imgs0021		[]		1	17	0	dunaizhuan	imgs0021
5523	5523	IMDb Scores for Netflix Movies and TV Shows	IMDb scores column for Netflix Movies and TV Shows dataset.	['movies and tv shows']	"Context
Это расширение для датасета Netflix Movies and TV Shows, добавляющие IMDb рейтинг произведений. Данные собирались и чистились посредством моего notebook'а."	12	95	0	sergeyzelenovskiy	netflix-films-tv-shows-imdb-scores
5524	5524	DataSet		[]		0	13	0	nimaabdpoor	dataset
5525	5525	animal_competition		[]		12	34	0	hongseokho	animal-competition
5526	5526	Covid-19		[]		0	5	0	saweera	covid19
5527	5527	BITEhack		[]		2	20	0	rafanojek	bitehack
5528	5528	dacon_crop_jpg_data		[]		0	62	0	jeonghj	dacon-crop-jpg-data
5529	5529	ekh_dacon		[]		0	27	0	eomkyeongho	ekh-dacon
5530	5530	MIcrosoft News Recommendation Dataset Train	https://msnews.github.io/	['news']		1	23	0	bninopaul	microsoft-news-recommendation-dataset-train
5531	5531	DCASE2016	dataset for Acoustic scene classification	['earth and nature']		0	13	0	mrudulajadhav	dcase2016
5532	5532	speech emotion recognition		[]		1	23	0	zmzm123	speech-emotion-recognition
5533	5533	NetflixData_ViewingActivity	ViewingActivity csv file from Netflix	[]		15	62	0	jolenech	netflixdata-viewingactivity
5534	5534	Google playstore- Case study 		[]		4	27	0	vermakeshav	google-playstore-case-study
5535	5535	SkinDiseases		[]		1	49	0	syedaun	skindiseases
5536	5536	Predicting Weather Using Alien Fruit Properties		['astronomy']	"Problem Statement:
The year is 2050 and a team of astronauts from all over the world went on a mission to an Exoplanet and discovered a vast amount of life and awesome weather. The scientists began collecting data samples of fruits found in their landing site and were curious by their shape and size. They collected data for more than a solar year of the planet to understand the fruit growing conditions in different weathers. 
To analyze data and grow fruits similar to earth, they began transmitting data back to the Earth, however, due to solar radiation, some data got corrupted and got lost in transmission. Back on Earth, the scientists figured they need to identify the type of climate the exoplanet has based on the properties of the fruit with the existing challenge of missing data. Help the scientists identify the earth-like season in which the fruit must have grown using the data collected.
About Data
Columns: [‘edible-poisonous’, 'cap-diameter', 'cap-shape', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', 'gill-color', 'stem-height', 'stem-width', 'stem-color', 'has-ring', 'ring-type', 'habitat', 'season']
Train: 42,748 rows x 14 columns
Test: 18,321 rows x 14 columns
Data Dictionary:
Independent Variables
edible-poisonous: edible=e, poisonous=p
cap-diameter: float number in cm
cap-shape: bell=b, conical=c, convex=x, flat=f, sunken=s, spherical=p, others=o
cap-color: brown=n, buff=b, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y, blue=l, orange=o, black=k
does-bruise-bleed: bruises-or-bleeding=t,no=f
gill-attachment: adnate=a, adnexed=x, decurrent=d, free=e, sinuate=s, pores=p, none=f
gill-color: see cap-color + none=f
stem-height: float number in cm
stem-width: float number in mm
stem-color: see cap-color + none=f
has-ring: ring=t, none=f
ring-type: cobwebby=c, evanescent=e, flaring=r, grooved=g, large=l, pendant=p, sheathing=s, zone=z, scaly=y, movable=m, none=f
habitat: grasses=g, leaves=l, meadows=m, paths=p, heaths=h, urban=u, waste=w, woods=d
Dependent variable
season: spring=s, summer=u, autumn=a, winter=w"	4	61	1	manishtripathi86	predicting-weather-using-alien-fruit-properties
5537	5537	Dermatology		[]		3	21	0	syedaun	dermatology
5538	5538	Missing Migrants Project	Data recorded between January 2014 and December 2021	['demographics', 'international relations', 'mortality', 'social issues and advocacy']	"Context
Missing Migrants Project tracks deaths of migrants, including refugees and asylum-seekers, who have gone missing along mixed migration routes worldwide. The research behind this project began with the October 2013 tragedies, when at least 368 individuals died in two shipwrecks near the Italian island of Lampedusa. Since then, Missing Migrants Project has developed into an important hub and advocacy source of information that media, researchers, and the general public access for the latest information.
With a count surpassing 60,000 over the last two decades, IOM calls on all the world’s governments to address what it describes as “an epidemic of crime and abuse.""
Missing Migrants Project is a joint initiative of IOM's Global Migration Data Analysis Centre (GMDAC) and Media and Communications Division (MCD). GMDAC has also published three reports on this issue: Fatal Journeys: Tracking Lives Lost during Migration, Fatal Journeys Volume 2: Identification and Tracing of Dead and Missing Migrants. A third volume was published in two parts in 2017, Fatal Journeys Volume 3 Part 1: Improving Data on Missing Migrants,  and Volume 3 Part 2: Improving Data on Missing Migrants.
Missing Migrants Project is licensed under a Creative Commons Attribution 4.0 International License. This means that Missing Migrants Project data are free to share and adapt, as long as the appropriate attribution is given. This includes stating that the source is ""IOM's Missing Migrants Project"", and indicating if changes were made to the data. Ideally, a link to this website should also be included.
Missing Migrants Project is made possible by funding by UK Aid from the Government of the United Kingdom; however, the views expressed do not necessarily reflect the Government of the United Kingdom’s official policies.
Content
Missing Migrants Project tracks deaths of migrants, including refugees and asylum-seekers, who have died or gone missing in the process of migration towards an international destination. Please note that these data represent minimum estimates, as many deaths during migration go unrecorded. 
We strongly encourage users to read more about our methodology before using this data.
Acknowledgements
The downloads below are licensed under a Creative Commons Attribution 4.0 International License. This means that Missing Migrants Project data are free to share and adapt, as long as the appropriate attribution is given. This includes stating that the source is 'IOM's Missing Migrants Project', and indicating if changes were made to the data. Ideally, a link to this website should also be included.
For more info: https://missingmigrants.iom.int/
Inspiration
I hope that monitoring these tragic events and studying the data relating to them can help to avoid a recurrence of these. We are all human beings."	5047	31251	140	snocco	missing-migrants-project
5539	5539	Fluorescent Neuronal Cells	283 high-res images of mice brain slices from a fluorescence microscope	['biology', 'neuroscience']	"Fluorescent Neuronal Cells
Uploaded to Kaggle by @nbroad, but all the research and work was done by the authors at the bottom of the description.
Quick Start
This dataset contains 283 images of mice brain slices obtained through fluorescent microscopy, plus the correspondent
ground-truth masks for semantic segmentation and object detection.
The folder all_images/images contains the original images acquired during the experiment described in 1, while the
correspondent ground-truth masks were generated in 2.
Terms of Use
All images in this archive are licensed under the Creative Commons Attribution Share Alike 4.0 International License,
available at: https://creativecommons.org/licenses/by-sa/4.0/legalcode
Specific copyright statements are reported separately for the images and the masks.
When using this dataset, please cite the work that introduced it 2.
Abstract
By releasing this dataset, we aim at providing a new testbed for computer vision techniques using Deep Learning. The main peculiarity is the shift from the domain of ""natural images"" proper of common benchmark dataset to biological imaging.
We anticipate that the advantages of doing so could be two-fold: i) fostering research in biomedical-related fields - for which popular pre-trained models perform typically poorly - and ii) promoting methodological research in deep learning by addressing peculiar requirements of these images.
Possible applications include but are not limited to semantic segmentation, object detection and object counting.
The data consist of 283 high-resolution pictures (1600x1200 pixels) of mice brain slices acquired through a fluorescence microscope. The final goal is to individuate and count neurons highlighted in the pictures by means of a marker, so to assess the result of a biological experiment. 
The corresponding ground-truth labels were generated through a hybrid approach involving semi-automatic and manual semantic segmentation. The result consists of black (0) and white (255) images having pixel-level annotations of where the stained neurons are located. For more information, please refer to 2.
Description
The images depict neurons of interest as objects of different size and shape appearing as yellow-ish spots of variable
brightness and saturation over a composite, generally darker background. For more details refer to Dataset section in 2.
A summary table with average measures<strong>*</strong> of main cell characteristics is reported below:
|   area  | minor axis | major axis | equivalent diameter | maximum feret diameter | mean diameter |
|:-------:|:----------:|:----------:|:-------------------:|:----------------------:|:-------------:|
| 1206.43 |    29.39   |    50.43   |        36.50        |          55.34         |     47.42     |
<div>
<strong>*</strong> obtained with <i>skimage 0.18.1</i> python package
</div>

Challenges
Several relevant challenges are present:
variability in brightness and contrast causes some fickleness in the pictures overall appearance
cells exhibit varying saturation levels due to the natural fluctuation of the fluorescent emission properties
substructures of interest have a fluid nature, so the shape of the stained cells may change significantly
artifacts, bright biological structures -- like neurons' filaments -- and non-marked cells are present
cells are sometimes clumping together and/or overlapping each other
broad shift in the number of target cells from image to image, from no stained cells to several dozens
All of these factors make the segmentation/recognition task harder, sometimes creating borderline cases that lead to a
subjective interpretation.
Ground-truth labels
Since generating annotations requires a great effort in terms of time and human resources, we resorted to an automatic
procedure to speed up the labeling. We started from a large subset of 252 images and applied gaussian blurring to remove
noise. The cleaned images were then subjected to a thresholding operation based on automatic histogram shape-based
methods. The goal was to obtain a loose selection of the objects that may seem good candidates to be labeled as neuronal
cells. After that, acknowledged operators reviewed the results to discard the false positives introduced with the
previous procedure, taking care of excluding irrelevant artifacts and misleading biological structures. The remaining 31
images were segmented manually by domain experts. We included significant pictures with peculiar traits -- such as
artifacts, filaments and crowded areas -- in the latter set to have highly reliable masks for the most challenging
examples.
The list of manually segmented images is reported below, all the remaining were obtained from the semi-automatic
labeling procedure:
Mar31bS2C1R2_VLPAGr_200x_y.png, Mar33bS1C4R2_DMl_200x_y.png, Mar40S1C2R2_DMl_200x_o.png, Mar41S3C1R1_DMl_200x_o.png,
MAR38S1C3R1_LHR_20_o.png, MAR38S1C3R1_DML_20_o.png, Mar42S2C2R2_DMr_200x_o.png, Mar36bS1C6R2_DMl_200x_y.png,
39.png, Mar40S1C2R2_DMr_200x_o.png, Mar41S3C1R1_DMr_200x_o.png, Mar33bS2C1R1_DMl_200x_y.png, Mar43S1C5R3_DMr_200x_o.png,
Mar37S1C2R1_DMr_200x_o.png, Mar40S3C4R2_VLPAGr_200x_o.png, 37.png, Mar37S1C2R1_DMl_200x_o.png, Mar36bS1C6R2_DMr_200x_y.png,
MAR55S3C2R2_VLPAGL_20_o.png, MAR39S2C2R2_DMR_200x_o.png, 38.png, MAR55S3C2R2_VLPAGR_20_o.png, Mar31bS2C3R4_DMr_200x_y.png,
Mar32bS2C2R2_DMl_200x_y.png, MAR39S2C2R2_DML_200x_o.png, MAR52S2C1R3_LHL_20_o.png, Mar42S2C4R2_VLPAGr_200x_o.png,
MAR55S1C5R3_DMR_20_o.png, MAR38S1C3R1_DMR_20_o.png, Mar33bS1C4R2_DMr_200x_y.png, Mar41S3C3R3_VLPAGl_200x_o.png'
Fundings
The collection of original images was supported by funding from the University of Bologna (RFO 2018) and the
European Space Agency (Research agreement collaboration 4000123556).
Ethical approval
All the experiments were conducted following approval by the National Health Authority (decree: No.141/2018 -
PR/AEDB0.8.EXT.4), in accordance with the DL 26/2014 and the European Union Directive 2010/63/EU, and under the
supervision of the Central Veterinary Service of the University of Bologna. All efforts were made to minimize the number
of animals used and their pain and distress.
References
1 Hitrec, T., Luppi, M., Bastianini, S., Squarcio, F.,
Berteotti, C., Martire, V.L., Martelli, D., Occhinegro, A., Tupone, D., Zoccoli, G. and Amici, R., 2019. Neural control
of fasting-induced torpor in mice. Scientific reports, 9(1), pp.1-12.
2 Morelli, R., Clissa, L., Amici, R., Cerri, M., Hitrec, T.,
Luppi, M., Rinaldi, L., Squarcio, F. and Zoccoli, A., 2021. Automating cell counting in fluorescent microscopy through 
deep learning with c-ResUnet. Scientific reports, (in press)."	60	996	20	nbroad	fluorescent-neuronal-cells
5540	5540	Sample Superstore		['business']		3	22	0	riddhimaa	sample-superstore
5541	5541	Customer Counts of ISPs of Nepal	Total subscriber counts of each internet service providers of Nepal	['internet', 'time series analysis']	"Description
Total subscriber counts of ISPs of Nepal.
Source
https://nta.gov.np/en/mis-reports/
Context
Inspiration"	2	33	0	nischallal	customer-counts-of-isps-of-nepal
5542	5542	VAERSCleaned		[]		0	17	2	jeffatennis	vaerscleaned
5543	5543	spotify_data		[]		12	147	0	abdghuff	spotify-data
5544	5544	MH Predicting Weather Using Alien Fruit Properties		[]		0	15	0	a45632	mh-predicting-weather-using-alien-fruit-properties
5545	5545	subaru ds train 0115 anno		['automobiles and vehicles']		0	6	2	wuliaokaola	subaru-ds-train-0115-anno
5546	5546	Hourly Weather Data in Gallipoli (2008-2021)	Hourly weather data between 2008 and 2021 for Gallipoli, Turkey.	['weather and climate', 'time series analysis', 'datetime']	"Context
This dataset contains hourly weather data between 2008 and 2021 for Gallipoli, Turkey.
Content
+120.000 rows and 10 columns.
Columns' description are listed below.
DateTime : Datetime in ""dd.mm.yyyy hh:mm"" format
Temperature : Temperature at 2 m in °C
Sunshine Duration : Sunshine duration in min
Shortwave Radiation : Shortwave radiation in W/m²
Relative Humidity : Relative Humidity at 2 m in %
Mean Sea Level Pressure : Mean Sea Level Pressure (MSL) in hPa
Soil Temperature : Soil temperature at 0-10 cm down in °C
Soil Moisture : Soil moisture at 0-10 cm down in m³/m³
Wind Speed : Wind speed at 10 m in km/h
Wind Direction : Wind direction at 10 m in degrees
Acknowledgements
Data from Meteoblue.
Image from Anadolu Agency.
If you're reading this, please upvote."	46	480	12	vivovinco	hourly-weather-data-in-gallipoli-20082021
5547	5547	Tomato_leaf_final_dataset	Tomato leaf after augmentation(pickle file)	['beginner', 'cnn', 'image data']		5	44	0	saroj12dangol	final-dataset
5548	5548	Dataconvert		[]		0	1	0	widhiwinata	dataconvert
5549	5549	Dry etch process data	molybdenum dry etch with CF4 and O2 gas	['business']		3	28	2	waterfirst	dry-etch-process-data
5550	5550	MLB Statcast data	MLB Statcast tracking data	['baseball']	Statcast is a high-speed, high-accuracy, automated tool developed to analyze player movements and athletic abilities in Major League Baseball (MLB).Statcast was introduced to all thirty MLB stadiums in 2015.	359	2974	9	s903124	mlb-statcast-data
5551	5551	sports-dataset		['sports', 'classification', 'image data', 'multiclass classification']		12	126	2	jyothishkumar	sportsdataset
5552	5552	ADNI Y1		[]		4	43	1	lightchaser	adni-y1
5553	5553	iris data classification		[]		2	5	0	prakashinisrivastava	iris-data-classification
5554	5554	TimeSeriesDataSet		[]		3	22	0	mohamedisbaine	timeseriesdataset
5555	5555	store sales historical data	historical sales data for 1,115 stores	['earth and nature', 'business']	"Content
DATASETS:
1. sales_data : Dataset containing historical sales, customer count and other day-wise information.                             
2. store : Supplemental information about the stores.
DATA FIELDS:
Store:  a unique Id for each store                    
Sales:  the turnover for any given day (this is what you are predicting)                      
Customers:  the number of customers on a given day                    
Open:   &nbsp;an indicator for whether the store was open: 0 = closed, 1 = open                   
StateHoliday:   &nbsp;indicates&nbsp;a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public: holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None                      
SchoolHoliday:  indicates&nbsp;if the&nbsp;(Store, Date)&nbsp;was affected by the closure of public schools. 1: schools closed, 0: schools open.                      
StoreType:  &nbsp;differentiates between 4 different store models: a, b, c, d                     
Assortment: describes an assortment level: a = basic, b = extra,&nbsp;c = extended                    
CompetitionDistance:    distance in meters to the nearest competitor store                    
CompetitionOpenSince[Month/Year]:   gives the approximate year and month of the time the nearest competitor was opened                    
Promo:  indicates whether a store is running a promo on that day                      
Promo2: Promo2&nbsp;is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating                     
Promo2Since[Year/Week]: describes the year and calendar week when the store started&nbsp;participating in&nbsp;Promo2                     
PromoInterval:&nbsp;    describes&nbsp;the consecutive intervals Promo2&nbsp;is started, naming the months the promotion is started anew. E.g. ""Feb,May,Aug,Nov"" means each round starts in February, May, August, November of any given year for that store"	6	42	0	archis777	store-sales-historical-data
5556	5556	HANA Database	HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition 	['business', 'artificial intelligence', 'deep learning', 'neural networks', 'image data', 'transfer learning']	"This is the first version of the HANA database. The minipics are from the police register sheets from Copenhagen which cover all adults (above the age of 10) residing in the capital of Denmark, Copenhagen, in the period from 1890 to 1923.
The labels in the .csv files refers to the main character on the original register sheets. Each row contain a reference to the corresponding image as the first element and the name as the second element. The HANA database consist of 1,105,904 files and labels. The last name is always only one word and if multiple last names were transcribed, the last of these were chosen as the last name, while the remaining were moved to the end of the first names. The first names can be up to 9 individual words.
All names are written in lower case letters and contain only characters which are used in Danish words, which implies 29 alphabetic characters i.e. this database include the letters æ, ø, and å.
If anything is missing or if you are interested in the original documents from Copenhagen Archives for improving on the cropouts, feel free to write me at sfw@sam.sdu.dk.
More information can be found in: HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition
We wish you the best of luck."	15	252	0	sdusimonwittrock	hana-database
5557	5557	yolo transformer pretrained weights		['exercise']		4	34	1	ks2019	yolo-transformer-pretrained-weights
5558	5558	yolov5_corts		[]		2	26	0	manyuli	yolov5-corts
5559	5559	Forbes_BillionairesList		[]		3	6	0	robertpaires	forbes-billionaireslist
5560	5560	FullDataCentric	2010 IMDB Collection (1.4GB) INEX 2010	['movies and tv shows']		0	16	0	imanebelahyane	fulldatacentric
5561	5561	yolov5 transformer head		[]		0	25	0	ks2019	yolov5-transformer-head
5562	5562	20+ Skin Disease Directories with Face Images	Smart Disease Detection of Skin Diseases through Face	[]		19	136	0	syedaun	20-skin-disease-directories-with-face-images
5563	5563	GIB-UVa ERP-BCI dataset		['business']	"About this dataset:
This dataset contains EEG signals from 73 subjects (42 healthy; 31 disabled) using an ERP-based speller to control different brain-computer interface (BCI) applications. The demographics of the dataset can be found in info.txt.
Grupo Ingeniería Biomédica, Universidad de Valladolid (GIB-UVa)
Code
Check the code section to find the dataset exploration script. 
Additionally, in the official repository you will find the original code of EEG-Inception, a novel CNN for EEG classification tasks that was presented in 1. This model significantly ourperformed EEGNet ShallowConvNet and DeepConvNet in this dataset 1.
Official repository: https://github.com/esantamariavazquez/EEG-Inception
Original article
1 Santamaría-Vázquez, E., Martínez-Cagigal, V., Vaquerizo-Villar, F., & Hornero, R. (2020). EEG-Inception: A Novel Deep Convolutional Neural Network for Assistive ERP-based Brain-Computer Interfaces. IEEE Transactions on Neural Systems and Rehabilitation Engineering. https://doi.org/10.1109/TNSRE.2020.3048106
Instructions:
This dataset contains data from 73 subjects using an ERP-based speller with the row-column paradigm (RCP) as stimulation paradigm. You can find complete information about the experiments in 1.
The dataset contains the following variables:
features [nstimuli x nsamples x n_channels] -&gt; EEG epochs [0, 1000] ms after stimulus onset. The EEG is already
preprocessed (i.e., FIR bandpass filter order 1000 [0.5-45] Hz, Common average reference (CAR), baseline normalization [-200, 0] ms). Channel order: ['FZ', 'CZ', 'PZ', 'P3', 'P4', 'PO7', 'PO8', 'OZ'].
erp_labels [nstimuli x 1] -&gt; ERP labels (i.e., 0 for non-target stimulus, 1 for target stimulus)
codes [n_stimuli x 1] -&gt; Code of the row or column that was highlighted
trials [n_stimuli x 1] -&gt; Array that relates each stimulus to its trial. Use example: features[trials==10] returns the EEG epochs of the 10th trial.
sequences [n_stimuli x 1] -&gt; Array that relates each stimulus to its number of sequence. Use example: features[sequences&lt;=10] returns the EEG epochs from sequences 1-10.
subjects [n_stimuli x 1] -&gt; Array that relates each stimulus to its subject. Use example: features[subjects==10] returns the EEG epochs of subject 10.
database_ids [n_stimuli x 1] -&gt; Array that relates each stimulus to its database (there are 3 different databases, as explained in the original study)
run_indexes [n_stimuli x 1] -&gt; Array that relates each stimulus to its run.
matrix_indexes [n_stimuli x 1] -&gt; Array that relates each stimulus to its matrix_index.
target [n_trials x 1] -&gt; Trial target. Column information [databaseid, subject, trial, run, matrix_index, row, col]
matrix_dims [n_runs x 1] -&gt; Matrix dimmensions for each run"	5	94	0	esantamaria	gibuva-erpbci-dataset
5564	5564	test_html		[]		0	1	0	wallacefqq	test-html
5565	5565	COT_YOLOV5_detected_frames		[]		0	3	0	georgeteo89	cot-yolov5-detected-frames
5566	5566	Titanic Dataset with Solution	with added correct Survival Prediction	[]	"In this Dataset you find the original titanic csv-files train and test. Special in this dataset is, that I added the right (100%) Survival Solution to the test data. This is only for better and faster evaluation of your own solution. Please don't upload this solution as a Submission to the official Competition!
Please be fair to the other Kagglers!"	1	24	0	danielwe14	titanic-dataset-with-solution
5567	5567	housing_pdf		[]		0	11	0	wallacefqq	housing-pdf
5568	5568	Monthly precipitation totals for the entire world	GeoTIFF images of Monthly Precipitation Totals from 2010 to 2018	['weather and climate', 'computer vision', 'image data']	"Content
GeoTIFF images of monthly precipitation totals for the period 2010-2018. These images are generated from observed data, with Global Climate Models (also know as General Circulation Models)
The monthly precipitation totals (mm) is the climatic variable represented in the images (That is, if it is January, we compute the sum of all the daily precipitations for the 31 days...)
The spatial resolution is 2.5 minutes (~21 km2). The Folder cotains 109 GeoTiff (.tif) files, for each month of the year (January is 1; December is 12), for the period 2010-2018.
Here, a notebook of A Quick Look at GeoTiff Precipitation Images
Acknowledgements
Thanks to the Climatic Research Unit, University of East Anglia (United Kingdom) and the  International Journal of Climatology.
Inspiration
These GeoTIFF images have the potential to be used in computer vision applications for the climate and beyond. For a more extensive and multivariable analysis, consult the Dataset Monthly maximum temperature GeoTIFF pictures for 2010-2018."	44	1097	17	abireltaief	monthly-precipitation-totals-for-the-entire-world
5569	5569	credit_card_fraud		[]		3	17	1	odinjfwang	credit-card-fraud
5570	5570	wu_data		[]		0	23	0	wallacefqq	wu-data
5571	5571	train_folds		[]		0	10	0	kesisour	train-folds
5572	5572	movies		['movies and tv shows']		1	6	0	wallacefqq	movies
5573	5573	cots-yoloxs-test_2		[]		5	76	0	junwoonlee	cotsyoloxstest-2
5574	5574	panini		['sports']		8	9	0	aulges	panini
5575	5575	blankline		[]		0	1	0	wallacefqq	blankline
5576	5576	housing		['social issues and advocacy']		0	5	0	wallacefqq	housing
5577	5577	population: 2015-2019 Finland, Norway, Sweden		['social science']		6	75	0	naokisugimura	population-20152019-finland-norway-sweden
5578	5578	skipfooter		[]		0	9	0	wallacefqq	skipfooter
5579	5579	skiprows		[]		0	8	0	wallacefqq	skiprows
5580	5580	yolov5		[]		2	47	0	kevin1742064161	yolov5
5581	5581	SYSUCD		[]		0	4	0	firepatpat	sysucd
5582	5582	Weighted_Boxes_Fusion		[]		0	4	0	ykkkk13	weighted-boxes-fusion
5583	5583	ULAP_Processing		[]		1	4	0	viiiiiiiiiiiiiiiiic	ulap-processing
5584	5584	Baylor Religion Survey		[]		0	41	0	panagiotisgia	baylor-religion-survey
5585	5585	CSV_EX_3		[]		0	28	0	wallacefqq	csv-ex-3
5586	5586	CSV_EX_2		[]		0	1	0	wallacefqq	csv-ex-2
5587	5587	CSV_EX_1		[]		0	1	0	wallacefqq	csv-ex-1
5588	5588	tfrtaskmatekenyanzindi		[]		0	19	0	mikemollel	tfrtaskmatekenyanzindi
5589	5589	Supermarket Sale		[]		1	26	0	ravialam	supermarket-sale
5590	5590	Netflix subscription fee in different countries	Which countries pay the most and least for Netflix in 2021?	['movies and tv shows', 'exploratory data analysis', 'regression']	"Context
Which countries pay the most and least for Netflix in 2021?
Acknowledgements
Data source: https://www.comparitech.com/blog/vpn-privacy/countries-netflix-cost/
Cover image credit: https://www.pexels.com/photo/light-man-people-woman-5112410/"	3353	16857	111	prasertk	netflix-subscription-price-in-different-countries
5591	5591	Wholesale Agricultural Products Market	Wholesale Agricultural Produce Assembling Markets in India - 2004	['india', 'agriculture', 'tabular data']	"Context
The Directorate of Marketing and Inspection had brought out three Directories of “Wholesale Agricultural Produce Assembling Markets in India” in the years 1963, 1992, and 2000. These Directories were found to be of immense use by the Government departments, research scholars, other markets functionaries and t and the farmers. The Directory on Agricultural Produce Markets facilitates and provides authentic and supportive information to farmers for competitive selling of their produce at an optimal price. The dissemination of complete and accurate market information plays key role in improving both operational and
pricing efficiency in the agricultural marketing system. 
Content
Efforts have been made to incorporate the information on the newly established markets, their postal addresses, telephone and fax numbers, the name of the nearest railway station with distance, infrastructural facilities available, and commodities transacted in the markets. The listing of markets has been compiled state-wise and district-wise on the basis of information collected from the State Agricultural Marketing Departments / State Agricultural Marketing Boards through the Regional / Sub-offices of this Directorate located in various parts of the country.
Acknowledgements
The data was acquired from the Government Website for AG Market."	2	34	0	rithurajnambiar	wholesale-agricultural-products-market
5592	5592	linear regression		[]		0	11	0	linyungchao	linear-regression
5593	5593	Air Travel Dataset		[]		6	57	1	heisenz	air-travel-dataset
5594	5594	longformer_settings		[]		15	13	0	devanshchowdhury	longformer-settings
5595	5595	full_2020	Données de Valeurs foncières	[]		1	525	0	ekane3	full-2020
5596	5596	Consultants Survey Dataset	What consulting firms offers to consultants?	['employment', 'business', 'survey analysis', 'tabular data', 'jobs and career']	"Context
Data set was created via survey of Consulting Humor community on Instagram (@consultinghumor). Below are some comments from the creator of Consulting Humor, Mo Yang. Hope this is helpful. Best, Heinrich
Data:
'Timestamp'
'Do you CURRENTLY work at a consulting firm?'
'Which firm do you CURRENTLY work for?'
'Are you CURRENTLY in a Commercial or Federal practice? '
'What is your CURRENT title (or equivalent)? '
'What COUNTRY are you CURRENTLY based out of?'
'What is your CURRENT annual base compensation in USD (not including bonuses, perks, or other incentives)? Please consult Google for conversion tools for local currency into USD. Enter number values only.'
'What is the total amount in USD of BONUSES you estimate you will receive in 2021? Enter number only.'
'Expected total compensation (calculated)'
'How many hours per week on AVERAGE do you work (including non-billable time)?'
'Have you gotten an OFFER from another company this year?'
'Was the offer from another CONSULTING FIRM?'
'Which firm was the HIGHEST (base compensation) OFFER from?'
'Was the OFFER for a Commercial or Federal practice? '
'What was the OFFERED title (or equivalent)? '
'What COUNTRY was the OFFER based out of?'
'What was the HIGHEST OFFERED annual base compensation in USD (not including bonuses, perks, or other incentives)? Please consult Google for conversion tools for local currency into USD. Enter a number only.'
'What was the total annual BONUS OFFERED?'
'Did you accept the offer?'
Acknowledgements
Thank you to everyone who participated in this survey. I appreciate y'all taking time out of your busy lives to contribute to something that will hopefully benefit many people.
Purpose
The goal of this self-reported survey is to assess the overall competitiveness of the current consulting industry. Due to the increase in competing firm offers, we want to know about competing offers as well as current base compensation. Hopefully this will help folks assess what their worth is in the market, and drive people to seek more equitable compensation
Footnotes
So first, this has been a great learning experience for me - I think I will put out future surveys that focus on equity and consulting demographics, but for now, lets talk about some of the limitations of this data set.
Data is self-reported. This one's pretty obvious.
Data may have duplicate entries. So there was an option on google forms to only allow one submission per person, but it would require people to enter in their email addresses, and I felt like many people wouldn't be comfortable with it. Due to the global nature of this survey, I wanted to make sure we got as many data points as possible, and that accessibility barriers were minimal.
Data needs to be cleaned - this is a raw data set of the responses exported into excel from google. The data validation tools that google forms has, at least the free version, arent great, so I've noticed blank cells, for example, from questions that were multiple choice...this also might have been due to me making live edits while people were in the survey, but I'll try to avoid doing that next time.
A big thank you, once again to everyone who participated in this survey. I hope you find these results interesting and compelling.
Banner Image Credit: Trust Consultancy"	66	497	9	asimzahid	consultants-survey-dataset
5597	5597	cars dataset		['automobiles and vehicles']		1	17	0	taddisrinivasarao	cars-dataset
5598	5598	Graduate Admission predictions		['universities and colleges']		4	38	1	tevintemu	graduate-admission-predictions
5599	5599	Supermarket Sales		[]		7	43	0	ravialam	supermarket-sales
5600	5600	dataset		[]		0	7	0	riyadsalih	dataset
5601	5601	NBA_data with bet365(2009-2011)	profitable when accuracy over .508 %	['sports', 'gambling']	"The dataset is NBA statistical data, each column is average statistic number for past ten games.  we combine it with bet365s handicap data.  Last two columns are response variables.
Firstly, we have to understand how sportsbooks earn the money. Generally speaking, most time they earn their profit by collecting commissions on bets. For instance, if you wager a 100 dollars bet and odd is 0.97, you will get 197 dollars where sportsbook gets 3 dollars profit. If there is a Basketball game Team A versus Team B, they will try to do is make mounts of betting money are balanced paced on Team A versus team B, which will maximize their profit; otherwise they may face a significant risk.
How sportsbooks do to balance the betting money is very simple, which is handicap. For example, they will turn the A versus B in A needs to win B by 3 points handicap. If bettors keep on betting the same side, sportsbooks will keep increasing the Handicap until bets balancing on both sides. 
Although it seems like sportsbooks have a hundred percent chance of winning, many of them ended up bankrupt every year. This is due to the reasons below. Since the sports betting market is fairly competitive, a sportsbook would not change their odds obviously different from others. Otherwise, they will lose their customers overnight. The only option they have is to change the handicap of games to balance their bets. However, if one sportsbook's handicap is significantly different from its competitors, it would be another problem which would make them bankrupt;  Furthermore, big sports leagues like NFL, NBA and MLB, their information is public and real-time spread. It cannot be manipulated like the stock market.
Marcus
melo15best@hotmail.com"	156	42773	5	marcuslin	marcuslin-nbadata
5602	5602	Coursera Courses Dataset 2021	Data collected on Courses available on Coursera	['websites', 'education', 'recommender systems', 'text data', 'online communities']	"Context
2021 has seen a boom in the MOOCs due to the Covid-19 Pandemic. With the availability of numerous paid and free resources on the internet, it becomes overwhelming for students to learn new skills. Therefore, this dataset can be used to create Recommender Systems and recommend courses to students based on the Skills and Difficulty Level entered by the student. The Course Link is also provided, which can be offered by the Recommender System for easy access.
Content
This dataset was scraped off the publicly available information on the Coursera website in September 2021 and manually entered in the case where the data was improperly scraped. It can be used in Recommender Systems to promote Coursera courses based on the Difficulty Level and the Skills needed. 
Acknowledgements
The dataset was obtained from the publicly available information on the Coursera website. I do not own any information."	920	6609	69	khusheekapoor	coursera-courses-dataset-2021
5603	5603	Synced dataset example		['software']		0	15	0	mrisdal	synced-dataset-example
5604	5604	CO₂ and Greenhouse Gas Emissions	How are emissions changing in each country?	[]		0	19	0	tulasiram574	co-and-greenhouse-gas-emissions
5605	5605	Death Rate Per Sex		[]		2	20	1	john9988	death-rate-per-sex
5606	5606	Morte DArthur		['arts and entertainment']		0	4	0	gabrielferrer	morte-darthur
5607	5607	Point2Skeleton		[]		14	22	0	niliangliang	point2skeleton
5608	5608	Test Model		['earth and nature']		0	24	0	frederickmorlock	test-model
5609	5609	unetmodel2		[]		1	7	0	sunchch55	unetmodel2
5610	5610	yolov5-font		[]		0	11	0	hngbiquc	yolov5font
5611	5611	Telugu Lyrics	Tollywood song lyrics at song level data. 	['languages', 'music', 'art', 'india', 'data analytics']	"Context
The secrets of a demography is hidden in the language they speak. Telugu being a resource-scarce language often has limited scope when it comes to any digital humanities based work. This is an attempt to accumulate song lyrics from Tollywood as a source of Telugu material for any NLP/text analytics work & to boost online resources for the language.
There is still an acute shortage in language resources such as spell-check, stemmers, parts of speech identifiers and lemmatizers when it comes to Telugu. Hope is that this dataset can act as training data for all/some of the above mentioned resources.
Content
The data is at a song level. Movie name, release date of the movie, song lyricist are mentioned for each song. If anybody wants to add to this so we reach a lot more songs, please do ping me. Larger the data, greater the uses.
Acknowledgements
I owe a lot to - https://lovelylyricstelugu.blogspot.com. The owner of this blog has done a tremendous job at compiling songs from different timeframes. Most of the songs of the dataset are scraped using beautifulSoup python package from this blog. I will attach the code for scraping info from any blog.
Also, not sure if I will get another chance to thank my inspiration when it comes to art, who singlehandedly has kept me invested in this language despite any other challenges to art in Telugu - మహానులు సిరివెన్నెల సీతారామ శాస్త్రి గారు (Sirivennela Sitarama Sastry Sir)
Inspiration
There is limited interest in exploring Telugu as a language and culture. I really wish this serves as an inspiration for others to go nuts discovering nuances and hidden artistic or banal insights. I believe there really is a market for it. That said, on more immediate matters - hope a robust part of speech identifier, lemmatizer and other resources are developed"	1	51	1	vamsipriyathamreddy	telugu-lyrics
5612	5612	dirty_words		[]		0	7	0	chenghonghu	dirty-words
5613	5613	Premier League All Time Top 50 Goal Scorers 	Goal Scoring Warriors 	['football', 'sports']	Statistical analysis into the top 50 goal scorers of the past 3 decades since the inception of the Premier League in 1992. The main idea is to explore those goal-scoring frequencies with statistical analysis.	140	881	10	bazbabar	premier-league-all-time-top-50-goal-scorers
5614	5614	feedback torch models		[]		0	4	0	leolu1998	feedback-torch-models
5615	5615	dataset3		[]		0	41	0	asmaelabbassy	dataset3
5616	5616	OECD_data_FIN_NOR_SWE_2015-2019		['economics', 'beginner', 'tabular data']	"Content
Some economic data from OECD.org for Finland, Norway and Sweden between the years 2015 and 2019.
HHDI: Household disposable income (annual)
HHEXP: Household spending (annual)
GDP: Gross domestic product  (annual)
BCI: Business confidence index (monthly)
CCI: Consumer confidence index (monthly, no Norway)
CLI: Composite leading indicator (monthly)
CPI: Consumer price index (monthly, base year 2015)
Hopefully these would be useful in the TPS Jan 2022 competition."	6	21	0	siukeitin	oecd-data-fin-nor-swe-20152019
5617	5617	difference		[]		1	12	1	abrahamanderson	difference
5618	5618	Sorted EEG data		['earth and nature']		34	10	0	steamisteriststeam	sorted-eeg-data
5619	5619	Open Buildings	A dataset of building footprints to support social good applications.	[]	"Open Buildings
A dataset of building footprints to support social good applications.
https://arxiv.org/abs/2107.12283
Building footprints are useful for a range of important applications, from population estimation, urban planning and humanitarian response, to environmental and climate science. This large-scale open dataset contains the outlines of buildings derived from high-resolution satellite imagery in order to support these types of uses. The project being based in Ghana, the current focus is on the continent of Africa.
Dataset description
The dataset contains 516M building detections, across an area of 19.4M km2 (64% of the African continent).
For each building in this dataset we include the polygon describing its footprint on the ground, a confidence score indicating how sure we are that this is a building, and a Plus Code corresponding to the centre of the building. There is no information about the type of building, its street address, or any details other than its geometry.
More explanation is in the FAQ.
Data format
The dataset consists of 2 parts: building polygons and score thresholds.
Building polygons
Building polygons are stored in spatially sharded CSVs with one CSV per S2 cell level 4. Each row in the CSV represents one building polygon and has the following columns:
latitude: latitude of the building polygon centroid,
longitude: longitude of the building polygon centroid,
area_in_meters: area in square meters of the polygon,
confidence: confidence score [0.5;1.0] assigned by the model,
geometry: the building polygon in the WKT format (POLYGON or MULTIPOLYGON),
full_plus_code: the full Plus Code at the building polygon centroid,
Score thresholds
The estimated score thresholds are stored as one CSV. Each row in the CSV represents one S2 cell level 4 bucket and has the following columns:
s2_token: S2 cell token of the bucket,
geometry: geometry in the WKT format of the S2 cell bucket,
confidence_threshold_80%_precision, confidence_threshold_85%_precision, confidence_threshold_90%_precision: estimated confidence score threshold to get specific precision for building polygons in this S2 cell bucket,
building_count_80%_precision, building_count_85%_precision, building_count_90%_precision: number of building polygons in this S2 cell bucket with confidence score greater than or equal to the score threshold needed to get the specific precision,
building_count: number of building polygons in this S2 cell bucket.
Acknowledgements
Data from https://sites.research.google/open-buildings/#download
 - The data is shared under the Creative Commons Attribution (CC BY-4.0) license and the Open Data Commons Open Database License (ODbL) v1.0 license.
If this dataset is useful, please consider citing the technical report:
 - W. Sirko, S. Kashubin, M. Ritter, A. Annkah, Y.S.E. Bouchareb, Y. Dauphin, D. Keysers, M. Neumann, M. Cisse, J.A. Quinn. Continental-scale building detection from high resolution satellite imagery. arXiv:2107.12283, 2021."	7	206	3	paultimothymooney	open-buildings
5620	5620	defi_ia		[]		1	12	0	lyndaben	defi-ia
5621	5621	sberweights		[]		6	8	0	andreyraav	sberweights
5622	5622	CAPTCHA Collected from DarkNet Websites	All Images are Labeled	['image data']	"Introduction
These are the datasets used for my research when I was a master's student. It includes three real CAPTCHA datasets that are used in the dark web. Besides, there are also several synthesized data for different character lengths. For more information, check out the manuscripts: Paper 1 and [Paper 2] (https://arxiv.org/pdf/2012.07994.pdf).
Usage
Each image is labeled on its file name."	43	443	1	njznjz	captcha-collected-from-darknet-websites
5623	5623	Wordle Valid Words	All valid Wordle guesses and solutions	['games', 'puzzles']	"Context
Wordle is a daily word puzzle game. The object of the game is to guess the WORDLE in 6 tries.
After each guess, the color of the tiles will change to show how close your guess was to the word.
Content
By inspecting the page source, I found two dictionaries included in the game. One is a list of valid guesses. The other is a list of valid solutions.
Inspiration
It would be fun to see if optimal strategies can be derived from the dictionary of valid solutions."	323	2645	23	bcruise	wordle-valid-words
5624	5624	PortugalLegislativeElections	All Candidates for the 2022 Legislature Elections and those from 2019	['europe', 'politics', 'tabular data']		11	272	9	jorgegomeseu	portugallegislativeelections
5625	5625	Visual Modelling		[]		0	8	0	jumperkables	visual-modelling
5626	5626	UK COVID-19 Data	Official UK Government COVID-19 Data	['europe', 'public health', 'government', 'health', 'public safety', 'covid19']	"11th January 2020
Change to vaccination data made available by UK gov - now just cumulative number of vaccines delivered are available for both first and second doses. For the devolved nations the cumulative totals are available for the dates from when given, however for the UK as a whole the total doses given is just on the last date of the index, regardless of when those vaccines were given. 
4th January 2020
VACCINATION DATA ADDED - New and Cumulative First Dose Vaccination Data added to UK_National_Total_COVID_Dataset.csv and UK_Devolved_Nations_COVID_Dataset.csv
2nd December 2020:
NEW population, land area and population density data added in file NEW_Official_Population_Data_ONS_mid-2019.csv. This data is scraped from the Office for National Statistics and covers the UK, devolved UK nations, regions and local authorities (boroughs).
20th November 2020:
With European governments struggling with a 'second-wave' of rising cases, hospitalisations and deaths resulting from the SARS-CoV-2 virus (COVID-19), I wanted to make a comparative analysis between the data coming out of major European nations since the start of the pandemic.
I started by creating a Sweden COVID-19 dataset and now I'm looking at my own country, the United Kingdom.
The data comes from https://coronavirus.data.gov.uk/ and I used the Developer's Guide to scrape the data, so it was a fairly simple process. The notebook that scapes the data is public and can be found here. Further information about data collection methodologies and definitions can be found here.
The data includes the overall numbers for the UK as a whole, the numbers for each of the devolved UK nations (Eng, Sco, Wal & NI), English Regions and Upper Tier Local Authorities (UTLA) for all of the UK (what we call Boroughs). I have also included a small table with the populations of the 4 devolved UK nations, used to calculate the death rates per 100,000 population.
As I've said for before - I am not an Epidemiologist, Sociologist or even a Data Scientist. I am actually a Mechanical Engineer! The objective here is to improve my data science skills and maybe provide some useful data to the wider community.
Any questions, comments or suggestions are most welcome! I am open to requests and collaborations! Stay Safe!"	635	4958	16	vascodegama	uk-covid19-data
5627	5627	amesDataset2		[]		1	18	0	ljiljanagospic	amesdataset2
5628	5628	DeSSI Dataset for Structured Sensitive Information	A corpus containing 11k DB columns representing private & sensitive information	['categorical data', 'nlp', 'tabular data', 'text data', 'multiclass classification']	"Context
A corpus containing 11k DB columns representing private & sensitive information
Content
The
-gathering - faker, publicly available datasets
-split 60/20/20
-format
-multi-label
-types of labels
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	3	43	1	sensitivedetection	dessi-dataset-for-structured-sensitive-information
5629	5629	tokenizer2		[]		0	12	0	hawzen	tokenizer2
5630	5630	tokenizer		['earth and nature']		0	3	0	hawzen	tokenizer
5631	5631	Translated Wikipedia Biographies	A Dataset for Studying Gender Bias in Translation	['gender', 'linguistics', 'nlp', 'ml ethics']	"Translated Wikipedia Biographies
English -&gt; Spanish 
English-&gt; German 
Description
A research area for machine translation has been using context from surrounding sentences or passages to improve gender accuracy. Traditional NMT methods translate sentences individually, but gendered information is not always explicitly stated in each individual sentence. The challenge for the traditional methods that translate sentences in isolation appears when a choice in a translated sentence needs context present in earlier sentences. In other words, contextual information explicit in previous sentences in the source is needed to disambiguate gender that will be reflected explicitly in target sentences.
The Translated Wikipedia Biographies dataset has been designed to analyze common gender errors in machine translation like incorrect gender choices in pro-drop, possessives and gender agreement.
Each instance of the dataset represents a person (identified in the biographies as feminine or masculine), a rock band or a sport team (considered genderless).  Each entity is represented by a long text translation (8 to 15 connected sentences referring to that central entity).  Articles are written in native English and have been professionally translated to Spanish and German. For Spanish, translations were optimized for pronoun-drop, so the same set could be used to analyze pro-drop (Spanish → English) and gender agreement (English → Spanish).
Acknowledgements
Data source
License: CC-BY-SA 3.0"	19	361	12	paultimothymooney	translated-wikipedia-biographies
5632	5632	SE_ICPR		[]		0	4	0	tikoboss	se-icpr
5633	5633	Witcher Network	A social network dataset from the 7 books of the Witcher book series	['movies and tv shows', 'video games', 'data visualization', 'data analytics', 'social networks']	"Context
The Witcher is a fantasy novel series by Andrzej Sapkowski and is the inspiration behind multiple computer games and, of course, the popular Netflix show! The series consists of various plots and timelines and therefor I was really interested in seeing what a social network visualization of the book series would look like as well as analyzing how character centrality changes over the course of the series.
Content
The dataset contains the characters from The Witcher relationships between them. This dataset was compiled by webscraping a comprehensive list of characters from https://witcher.fandom.com/wiki/Category:Characters_in_the_stories and by performing text analysis on the digitized versions of each book posted on GitHub: https://github.com/dworschak/Witcher/tree/master/RESSOURCES/_books/text. To do this I wrote a function which searched each line in the book for the name of the characters appearing in the sentence and added them as the Source character. Then I defined a window size of 5 lines and made it so that if two characters were listed within 5 lined of eachother there would be an edge with a weight of one assigned to them OR if they were already in the dataset for that book +1 would be added to their weight. 
witcher_network.csv: contains 5 columns:
- Source &amp; Target: the two characters who have had an interaction
- Type: all connections for this dataset are undirected meanign they are bidirectional
- Weight: How many interactions has each set of characters had 
- Book: book # from which this character set is from --&gt; NOTE: two characters may appear multiple times in the dataset but in different books so you may want to do some dataset manipulation to combine rows
Tasks
visualize the network
analyze which character is the most important (degree centrality)
how does the network change over the course of the 7 books? Do the most important characters change?"	25	256	1	avasadasivan	witcher-network
5634	5634	additional_pos_data		[]		0	22	0	ghaithkhlifi	additional-pos-data
5635	5635	CSVRAF-TrainVald		[]		1	16	0	mohammedaaltaha	csvraftrainvald
5636	5636	ffffffffff		[]		2	6	0	zeeenb	ffffffffff
5637	5637	ner_model		[]		2	9	0	fabianomunizbelem	ner-model
5638	5638	Test_gammatone64_csv		[]		0	8	0	stealthobaga	test-gammatone64-csv
5639	5639	HCF5.3-6.0		[]		0	19	0	leinbnb	hcf5360
5640	5640	Babble_gammatone64_csv		[]		0	9	0	stealthobaga	babble-gammatone64-csv
5641	5641	CSVDS-RAFDS		[]		0	21	0	mohammedaaltaha	csvdsrafds
5642	5642	Weather in Athens		[]		1	24	0	panagiotisgia	weather-in-athens
5643	5643	turtle_data		[]		0	9	0	mikemollel	turtle-data
5644	5644	NLPSriv		[]		1	16	0	ganeshn88	nlpsriv
5645	5645	Lungs Disease Dataset (4 types)	Chest X-Rays of Viral Pneumonia, Bacterial Pneumonia, Covid and Tuberculosis	['medicine', 'neural networks', 'image data', 'transfer learning', 'health conditions']	"The Dataset contains chest x-rays images. This Dataset was prepared from various datasets like I combined the datasets accordingly (Yes I removed same images in dataset using VisiPics). It has 4 types of Lungs Diseases and a folder of Normal Lungs. I augmented the dataset with factor 6 so there are basically 10000 images.
References
https://www.kaggle.com/darshan1504/covid19-detection-xray-dataset
https://www.kaggle.com/muhammadrizkyperdana/lungs-dataset
https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia
https://www.kaggle.com/jtiptj/chest-xray-pneumoniacovid19tuberculosis?select=train
https://www.kaggle.com/iamsuyogjadhav/chest-x-ray-14-lungs-cropped
https://www.kaggle.com/tawsifurrahman/tuberculosis-tb-chest-xray-dataset"	33	196	3	omkarmanohardalvi	lungs-disease-dataset-4-types
5646	5646	FilmeIMDB		[]		0	11	0	coad07	filmeimdb
5647	5647	Car_Gammatone_64_csv		[]		0	10	0	stealthobaga	car-gammatone-64-csv
5648	5648	argentina-shp		['public safety']		0	18	0	fernandobordi	argentinashp
5649	5649	PDB_ndxed_dssp_phipsi_0		[]		0	5	0	edmorra	pdb-ndxed-dssp-phipsi-0
5650	5650	airline-passengers		[]		2	21	0	haneenhossam	airlinepassengers
5651	5651	tredence-mh		[]		1	12	0	shahidmandal	tredencemh
5652	5652	cots_model_exchane		[]		0	7	0	wangbo000	cots-model-exchane
5653	5653	Testset_AI		[]		0	5	0	giuseppevenezia	testset-ai
5654	5654	Light data (with void)		[]		0	5	0	synboost	light-data-with-void
5655	5655	CSVFer2013-TestVald		[]		1	7	0	mohammedaaltaha	csvfer2013testvald
5656	5656	Lwdb_CC		[]		0	21	0	sicpama	lwdb-cc
5657	5657	data_z		[]		0	1	0	mohammadzayd	data-z
5658	5658	support		[]		0	9	0	heyman1984	support
5659	5659	model_160		[]		0	2	0	malekbadreddine	model-160
5660	5660	CUHK CompCars Dataset	The Comprehensive Cars (CompCars) dataset	['automobiles and vehicles', 'classification', 'dnn', 'cnn', 'image data']	"Context
This dataset is presented in our CVPR 2015 paper,
Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang. A Large-Scale Car Dataset for Fine-Grained Categorization and Verification, In Computer Vision and Pattern Recognition (CVPR), 2015. PDF
Content
The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. Please refer to our paper for the details.
Acknowledgements
The CompCars database is available for non-commercial research purposes only.
All images of the CompCars database are obtained from the Internet which are not property of MMLAB, The Chinese University of Hong Kong. The MMLAB is not responsible for the content nor the meaning of these images.
You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the images and any portion of derived data.
You agree not to further copy, publish, or distribute any portion of the CompCars database. Except, for internal use at a single site within the same organization it is allowed to make copies of the database.
The MMLAB reserves the right to terminate your access to the database at any time.
All submitted papers or any publicly available text using the CompCars database must cite the following paper:
Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang. A Large-Scale Car Dataset for Fine-Grained Categorization and Verification, In Computer Vision and Pattern Recognition (CVPR), 2015.
Inspiration
The dataset is well prepared for the following computer vision tasks:
- Fine-grained classification
- Attribute prediction
- Car model verification
The train/test subsets of these tasks introduced in our paper are included in the dataset. Researchers are also welcomed to use it for any other tasks such as image ranking, multi-task learning, and 3D reconstruction."	5	275	3	renancostaalencar	compcars
5661	5661	CTA List of L Stops		[]		0	5	1	tomasaguilar	cta-list-of-l-stops
5662	5662	petfinder_swin_large_224_2		[]		0	4	0	atamazian	petfinder-swin-large-224-2
5663	5663	petfinder_swin_large_224_1		[]		0	8	0	atamazian	petfinder-swin-large-224-1
5664	5664	petfinder_swin_large_224_0		[]		0	6	0	atamazian	petfinder-swin-large-224-0
5665	5665	30 Rock Episode Data	Episodes and ratings for 30 Rock	['arts and entertainment', 'movies and tv shows', 'beginner', 'tabular data']	"Context
Series created by: Tina Fey
Number of seasons: 7
Number of episodes: 138
Original air dates: October 11, 2006 – January 31, 2013
Content
Data was acquired through downloading IMDb TV episodes datasets and scraping information from Wikipedia.
Acknowledgements
Thanks to IMDb, Wikipedia, and community curators.
Use
It should be easy to join these data files together on Title and Air Date fields to compare (for example) US viewers and IMDb ratings.
Motivation
I wanted to share a dataset about 30 Rock,  one of my favorite TV shows to binge watch."	3	22	1	bcruise	30-rock-episode-data
5666	5666	paw_learner_swin	A pre-trained swin leaner for pawpularity	[]		1	31	0	bengxue	paw-learner-swin
5667	5667	TF COTS EfficientDet-D2 		[]		0	11	0	swadeshjana	tf-cots-efficientdetd2
5668	5668	dataset		[]		1	141	0	kotashimomura	dataset
5669	5669	Chicago Community Areas Map		[]		0	7	1	tomasaguilar	chicago-community-areas-map
5670	5670	Healthcare Claim Data		[]		3	48	0	jai8004	healthcare-claim-data
5671	5671	CTA Stops (GTFS Format)		[]		0	6	1	tomasaguilar	cta-stops-gtfs-format
5672	5672	Shapes Dataset	A simple dataset for object detection	['computer science', 'beginner', 'cnn', 'image data', 'transfer learning', 'multilabel classification']		0	48	3	soumikrakshit	shapes-dataset
5673	5673	CTA Ridership - L Station Entries - Daily Totals		[]		0	15	1	tomasaguilar	cta-ridership-l-station-entries-daily-totals
5674	5674	Tamil Voice and Text	It's a google dataset has text and its respective audio file in tamil language .	['arts and entertainment']		1	14	0	manojkumars00	tamil-voice-and-text
5675	5675	Final exams		['standardized testing']		0	11	0	maliyanwar065119057	final-exams
5676	5676	Flooding image dataset	This dataset contains flooding images.	['united states', 'earth and nature', 'computer vision', 'image data']		6	85	3	hhrclemson	flooding-image-dataset
5677	5677	Dataset		[]		2	27	0	zakaria002	dataset
5678	5678	Brain CT Missing		['health']		1	21	0	josepc	brain-ct-missing
5679	5679	HourlyIntensities		[]		0	2	0	matthewmellanby	hourlyintensities
5680	5680	Hand Written Digit Recognition		['education']		1	14	1	sarangthemamarjit	hand-written-digit-recognition
5681	5681	petfinder-010		[]		0	25	0	kenkengoda	petfinder-010
5682	5682	part56789		[]		0	57	0	skloveyyp	part56789
5683	5683	transformer_learning_cv0_v0.5		[]		0	9	0	maxmingwang	transformer-learning-cv0-v05
5684	5684	Transformerlearn_cv0_v1		[]		0	1	0	maxdreams	transformerlearn-cv0-v1
5685	5685	Retinal Vessel Segmentation	a collection of retinal vessel images	['eyes and vision']		3	25	0	ipythonx	retinal-vessel-segmentation
5686	5686	The Covid-19 Conspiracy Theories Dataset		['arts and entertainment']		1	32	1	aviyosipof	the-covid19-conspiracy-theories-dataset
5687	5687	WalkorMopping		[]		0	15	1	mg2804	walkormopping
5688	5688	conv_next_models		[]		0	7	1	zloydanny	conv-next-models
5689	5689	binance btcusdt trade data 13985240-106731161		['finance']		0	3	0	hejianhua198711	binance-btcusdt-trade-data-13985240106731161
5690	5690	The Covid-19 Conspiracy Theories Dataset	The Covid-19 Conspiracy Theories Dataset	['arts and entertainment']		2	113	0	kerneler	the-covid19-conspiracy-theories-dataset
5691	5691	Flooding images dataset		[]		0	2	0	hhrclemson	flooding-images-dataset
5692	5692	car brands images		[]		0	13	0	mithranbala	car-brands-images
5693	5693	PetFinder-ZX4001-20220114144528		[]		0	4	0	hideyukizushi	petfinder-zx4001-20220114144528
5694	5694	PetFinder-ZX404-20220114143457		[]		0	12	0	hideyukizushi	petfinder-zx404-20220114143457
5695	5695	Exam Test Images		['standardized testing']		0	14	0	shakirbugti	exam-test-images
5696	5696	sanitext		[]		0	50	0	legendalive	sanitext
5697	5697	Named Entity Recognition (NER)  Corpus	Ready to use Named Entity Recognition Corpus	['nlp', 'deep learning', 'rnn', 'tabular data', 'text data']	"Task
Named Entity Recognition(NER) is a task of categorizing the entities in a text into categories like names of persons, locations, organizations, etc. 
Dataset
Each row in the CSV file  is a complete sentence, list of POS tags for each word in the sentence, and list of NER tags for each word in the sentence
You can use Pandas Dataframe to read and manipulate this dataset. 
Since each row in the CSV file contain lists, if we read the file with pandas.read_csv() and try to get tag lists by indexing the list will be a string.
&amp;gt;&amp;gt;&amp;gt; data['tag'][0] 
""['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']""
&amp;gt;&amp;gt;&amp;gt; type(data['tag'][0])
string
You can use the following to convert it back to list type: 
&amp;gt;&amp;gt;&amp;gt; from ast import literal_eval
&amp;gt;&amp;gt;&amp;gt; literal_eval(data['tag'][0] )
['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']
&amp;gt;&amp;gt;&amp;gt; type(literal_eval(data['tag'][0] ))
list
Acknowledgements
This dataset is taken  from Annotated Corpus for Named Entity Recognition by Abhinav Walia dataset and then processed. 
Annotated Corpus for Named Entity Recognition is annotated Corpus for Named Entity Recognition using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set.
Essential info about entities:
geo = Geographical Entity
org = Organization
per = Person
gpe = Geopolitical Entity
tim = Time indicator
art = Artifact
eve = Event
nat = Natural Phenomenon"	44	478	3	naseralqaydeh	named-entity-recognition-ner-corpus
5698	5698	fdlaerial		[]		1	7	0	thomasgak	fdlaerial
5699	5699	Global Warming		[]		6	57	0	bobbystefan	global-warming
5700	5700	cryptocurrency-prices	Top 15 Cryptocurrency Historical Prices	['currencies and foreign exchange']		6	45	0	himanshuvashisht878	cryptocurrencyprices
5701	5701	defacto-splicing	Splicings from the DEFACTO dataset	['image data', 'public safety']	"Context
Digital image forensic has gained a lot of attention as it is becoming easier for anyone to make forged images. Several areas are concerned by image manipulation: a doctored image can increase the credibility of fake news, impostors can use morphed images to pretend being someone else.
It became of critical importance to be able to recognize the manipulations suffered by the images. To do this, the first need is to be able to rely on reliable and controlled data sets representing the most characteristic cases encountered. The purpose of this work is to lay the foundations of a body of tests allowing both the qualification of automatic methods of authentication and detection of manipulations and the training of these methods.
Content
This dataset contains about 105000 splicing forgeries are available under the splicing directory. Each splicing is accompanied by two binary masks. One under the probe_mask subdirectory indicates the location of the forgery and one under the donor_mask indicates the location of the source. The external image can be found in the JSON file under the graph subdirectory.
Reference
If you use this dataset for your research, please refer to the original paper : 
@INPROCEEDINGS{DEFACTODataset, AUTHOR=”Gaël Mahfoudi and Badr Tajini and Florent Retraint and Fr{'e}d{'e}ric Morain-Nicolier and Jean Luc Dugelay and Marc Pic”, TITLE=”{DEFACTO:} Image and Face Manipulation Dataset”, BOOKTITLE=”27th European Signal Processing Conference (EUSIPCO 2019)”, ADDRESS=”A Coruña, Spain”, DAYS=1, MONTH=sep, YEAR=2019 }
and to the MSCOCO dataset
License
The DEFACTO Consortium does not own the copyright of those images. Please refer to the MSCOCO terms of use for all images based on their Dataset."	47	257	2	defactodataset	defactosplicing
5702	5702	nsk_image_search3_man4		[]		0	18	0	motono0223	nsk-image-search3-man4
5703	5703	Weekly store sales	Store sales week wise. Ideal for performing forecasting at store level.	['business', 'time series analysis', 'tabular data', 'retail and shopping']		11	129	0	akuchi123	weekly-store-sales
5704	5704	Happy and Sad faces	dataset of happy and sad faces	['online communities']		0	3	0	duaazehraalvi	lab-mid-2
5705	5705	CO2 emissions country wise with income group 	CO2 emissions countrywise (metric tons per capita) with income group (1960-2018)	['atmospheric science', 'environment', 'pollution']	"Context
Global warming is a burning issue of today's world. CO2 emission causes global warming and thus creating ecological imbalance. 
Content
The datasets contains country name, country code and their specific year wise CO2 emissions. The second dataset contains country name, code with their specific income group, like per head earning. Low per head income means low income group.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Is there any relation between higher income country and high rate CO2 emissions? Does higher rate CO2 emissions always done by developed countries?"	3	44	0	azizulhakim98	c02-emissions-country-wise-with-income-group
5706	5706	Boom Bikes - Linear Regression	This dataset contains weather and time series information and demand of bikes.	['cycling', 'linear regression']	"Context
the data set is of a bike renting company, the company wants us to predict the demand of b bikes in the future, looking at various factors like date, holidays, weather etc.."	119	780	12	kratos2597	boom-bikes-linear-regression
5707	5707	Complete Energy Profile of India [1965 - 2019]	Tabular Data of Production and Consumption of all Energy forms in India.	['india', 'energy', 'beginner', 'exploratory data analysis', 'tabular data']	"Context
This dataset contains all forms of energy production as well as consumption in India. Also, it contains data about different sources of energy from which how much amount of electricity is produced and the availability of electricity to % of the population.
Content
About attributes of the dataset: 
1. Year
2. Entity - India
3. Code - IND
4. Oil Consumption - EJ: Yearly consumption of oil in exajoule(EJ)
5. Gas Consumption - EJ: Yearly consumption of gas in exajoule(EJ)
6. Coal Consumption - EJ: Yearly consumption of coal in exajoule(EJ)
7. Solar Consumption - EJ: Yearly consumption of solar in exajoule(EJ)
8. Hydro Consumption - EJ: Yearly consumption of hydro in exajoule(EJ)
9. Nuclear Consumption - EJ: Yearly consumption of nuclear in exajoule(EJ)
10. Wind Consumption - EJ: Yearly consumption of wind in exajoule(EJ)
11. Geo Biomass Other - EJ: Yearly consumption of geo biomass and other energy resources in exajoule(EJ)
12. Biofuels (TWh): Yearly consumption of biofuels in TWh
13. Access to clean fuels and technologies for cooking  (% of the population)
14. Annual change primary energy consumption (%)
15. Annual CO2 emissions per unit energy (kg per kilowatt-hour)
16. Electricity Generation (TWh)
17. Electricity from coal (TWh)
18. Electricity from gas (TWh)
19. Electricity from hydro (TWh)
20. Electricity from other renewables (TWh)
21. Electricity from solar (TWh)
22. Electricity from oil (TWh)
23. Electricity from wind (TWh)
24. Electricity from nuclear (TWh)
25. Energy consumption per GDP (kWh per $)
26. Fossil fuels (% sub energy)
27. Low-carbon energy (% sub energy)
28. Nuclear (% sub energy)
29. Per capita electricity (kWh)
30. Energy consumption per capita (kWh)
31. Primary energy consumption (TWh)
32. Coal (% electricity)
33. Gas (% electricity)
34. Hydro (% electricity)
35. Solar (% electricity)
36. Wind (% electricity)
37. Oil (% electricity)
38. Nuclear (% electricity)
39. Other renewables (% electricity)
40. Fossil fuels (% electricity)
41. Low-carbon electricity (% electricity)
42. Nuclear (% electricity)
43. Renewables (% electricity)
44. Access to electricity (% of the population)
Note: Not all Columns contain data from 1965 to 2019.
Acknowledgements
Hannah Ritchie and Max Roser (2020) - ""Energy"". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/energy' [Online Resource]"	102	573	13	shubamsumbria	complete-energy-profile-of-india-1965-2019
5708	5708	Patient Analysis		[]		0	39	0	johndc1	patient-analysis
5709	5709	TurkishData		[]		0	14	0	rhythmnarula	turkishdata
5710	5710	bigbird processed dataset		[]		0	15	0	shreyasadhari123	bigbird-processed-dataset
5711	5711	World Of Bugs		[]		16	53	0	benedictwilkinsai	world-of-bugs
5712	5712	KoreanData		[]		0	11	0	rhythmnarula	koreandata
5713	5713	PetFinder-Model422		[]		0	5	0	lftuwujie	petfindermodel422
5714	5714	Predict Nba Player Position		['basketball']		0	21	0	fouadtaha	predict-nba-player-position
5715	5715	ConvNext oof 384		[]		2	12	0	mithilsalunkhe	convnext-oof-384
5716	5716	Top100_football_players(2022)	This dataset contains stats of top 100 football players in the world as of 2022.	['football']		6	35	1	shreyhenry	top100-football-players2022
5717	5717	SasOdevv		[]		0	2	0	muhammedtalhakara	sasodevv
5718	5718	Top100_football_players(2022)	This dataset contains stats of top 100 football players in the world as of 2022.	['football']		6	35	1	shreyhenry	top100-football-players2022
5719	5719	SasOdevv		[]		0	2	0	muhammedtalhakara	sasodevv
5720	5720	PivotTable		[]		1	5	0	ylmazdalkiran	pivottable
5721	5721	pet_model3		[]		3	132	0	yoshito	pet-model3
5722	5722	sample-superstore2		[]		2	7	0	wallacefqq	samplesuperstore2
5723	5723	EDA Airbnb Santiago, Chile	EDA Airbnb Santiago, Chile	['beginner', 'exploratory data analysis', 'data analytics', 'matplotlib', 'pandas']	EDA Airbnb Santiago, Chile	6	80	0	brunoarchetti	eda-airbnb-santiago-chile
5724	5724	PPP loans during the Covid-19 pandemic in USA	Covid-19 Loans in USA	['employment', 'business', 'finance', 'banking', 'covid19']	"The number of community banks in the United States has fallen from more than 13,000 in the mid-1980s to less than 5,000 today. These community-focused banks have consolidated mainly as a result of competitive pressures. Research shows that community banks are essential to maintaining economically fruitful communities, and losing these banks could be a significant blow to local infrastructure. 
One example of the importance of community banks was their role in distributing Paycheck Protection Program (PPP)1 loans during the Covid-19 pandemic. The PPP was designed to help small businesses keep their workers employed during the pandemic by providing funds through a short-term loan backed by the Small Business Administration (SBA). Preliminary research by CSBS shows that state-chartered banks were the primary distributor of PPP loans, and that community banks played an outsized role in the distribution of PPP funds.
CSBS is providing complete loan-level PPP data [available here (full file, 300MB), here (sample data) and here (data definitions)] that combines the publicly available files made available on sba.gov. To allow for analysis on depository institutions, CSBS will also be adding FDIC Certificate numbers to this file. When the institution is a bank, the FDIC Certificate number will allow participants to link the PPP data to the quarterly Call Report of Income and Condition, which can be accessed here. CSBS is also providing a sample dataset that can be updated and examined in Excel. Questions regarding the data can be sent to data@csbs.org. CERT number is based on originating lender, not servicing lender.
Field Name
Field Description
LoanNumber
Loan Number (unique identifier)
DateApproved
Loan Funded Date
SBAOfficeCode
SBA Origination Office Code
ProcessingMethod
Loan Delivery Method (PPP for first draw; PPS for second draw)
BorrowerName
Borrower Name
BorrowerAddress
Borrower Street Address
BorrowerCity
Borrower City
BorrowerState
Borrower State
BorrowerZip
Borrower Zip Code
LoanStatusDate
Loan Status Date - Loan Status Date is blank when the loan is disbursed but not Paid In Full or Charged Off
LoanStatus
Loan Status Description - Loan Status is replaced by 'Exemption 4' when the loan is disbursed but not Paid in Full or Charged Off
Term
Loan Maturity in Months
SBAGuarantyPercentage
SBA Guaranty Percentage
InitialApprovalAmount
Loan Approval Amount (at origination)
CurrentApprovalAmount
Loan Approval Amount (current)
UndisbursedAmount
Undisbursed Amount
FranchiseName
Franchise Name
ServicingLenderLocationID
Lender Location ID (unique identifier)
ServicingLenderName
Servicing Lender Name
ServicingLenderAddress
Servicing Lender Street Address
ServicingLenderCity
Servicing Lender City
ServicingLenderState
Servicing Lender State
ServicingLenderZip
Servicing Lender Zip Code
RuralUrbanIndicator
Rural or Urban Indicator (R/U)
HubzoneIndicator
Hubzone Indicator (Y/N)
LMIIndicator
LMI Indicator (Y/N)
BusinessAgeDescription
Business Age Description
ProjectCity
Project City
ProjectCountyName
Project County Name
ProjectState
Project State
ProjectZip
Project Zip Code
CD
Project Congressional District
JobsReported
Number of Employees
NAICSCode
NAICS 6 digit code
Race
Borrower Race Description
Ethnicity
Borrower Ethnicity Description
UTILITIES_PROCEED
Note: Proceed data is lender reported at origination. On the PPP application the proceeds fields were check boxes.
PAYROLL_PROCEED
MORTGAGE_INTEREST_PROCEED
RENT_PROCEED
REFINANCE_EIDL_PROCEED
HEALTH_CARE_PROCEED
DEBT_INTEREST_PROCEED
BusinessType
Business Type Description
OriginatingLenderLocationID
Originating Lender ID (unique identifier)
OriginatingLender
Originating Lender Name
OriginatingLenderCity
Originating Lender City
OriginatingLenderState
Originating Lender State
Gender
Gender Indicator
Veteran
Veteran Indicator
NonProfit
'Yes' if Business Type = Nonprofit Organization or Nonprofit Childcare Center or 501(c) Nonprofit
ForgivenessAmount
Forgiveness Amount
ForgivenessDate
Forgiveness Paid Date
CERT
Community Bank Flag
State vs. National Charter
Source - https://www.csbs.org/"	10	133	2	vaibhav2025	ppp-loans-during-the-covid19-pandemic-in-usa
5725	5725	SpanishData		[]		0	12	0	rhythmnarula	spanishdata
5726	5726	GermanData		[]		0	15	0	rhythmnarula	germandata
5727	5727	datatext		[]		0	11	0	kent19781978	input
5728	5728	PGCB Power demand data		['energy']		4	15	0	ashfakyeafi	pgcb-power-demand-data
5729	5729	superstore_sales_data	Sales and order data of a superstore	[]		4	27	1	shreyhenry	superstore-sales-data
5730	5730	Microsoft Malware Detection Precomputed		[]		27	286	0	sajilck	microsoft-malware-detection-precomputed
5731	5731	Visual Modelling Self Output Visualisations	Visual Modelling Self Output GIFs	[]		0	12	0	jumperkables	visual-modelling-self-output-visualisations
5732	5732	submitRR		[]		0	7	0	mkagglemm	submitrr
5733	5733	huggingface	download pretrained model files	['software', 'nlp', 'deep learning']		0	29	0	amulil	amulil-huggingface
5734	5734	longformer whitespace		['computer science']		4	15	0	shreyasadhari123	longformer-whitespace
5735	5735	embdswinexp		[]		0	3	0	gyanendradas	embdswinexp
5736	5736	Shipping from China to the USA		['transportation']	"Shipping from China to the USA - What do you need to know?
Introduction
If you are an American resistant and want to import certain goods from China because you sale them here in America, then you must have all the best possible information about the shipping from China to the USA. You need to know about the fares, product prices, how long would it take to deliver, weather conditions, special weeks or holidays in China and so on. All this information is required to avoid any type of confusion,
misconception, inconveniences, and problems in the shipment. There are three ways of shipment from China to the USA. One is air express that can deliver your products within 1 to 5 days. Another shipment way is China air freights. These Chinese air freights contain light-weighted products. It takes 1 to two weeks for your products to import from China and reach safely at their required destinations.
The third way is both convenient as well as complicated to get your shipments from China to the USA. This is China Sea freight that carries thousands of tons of valuable products to the USA within 15 to 25 days. This is one of the most important ways of transportation because it can carry a lot of varieties of different projects. Such as furniture goods, machinery products, large vehicles, sports cars, sports goods, hospital machinery, toys, food item sacks, and vise versa.
Chinese holidays:
Chinese people are very open to celebrate their holidays in a festival manner. These holidays delay or stop the transportation system from all three methods. After all, workers do have a family and they too need family time all together. Shipments can delay or stopped due to such festivities. The Chinese nation begins their year with the celebration of the new year, shipment delay for one day is common in this time. However, the Chinese celebrate their holidays of Chinese New year from end of the January to the middle of February. It takes 6 to 12 days of holidays and the shipment process is non-active during these holidays. Check the calendar before ordering something from China to the USA. The next holiday in the Chinese calendar happens on the 1st of April, this is called the Qingming festival that takes place for one day.
The month of May witnessed 3 days of holidays in the early 10s. this festival is celebrated in the honor of all the laborers, named labors day. At the end of June, the nation celebrates the world-famous dragon boat festival, which is another reason for the shipments to be suspended for 3 days. October is supposed as the month of festivals in China. Early October sees a Mid-Autumn festival for one day, a golden week festival that takes a whole week of holidays of October. Last but not the least is a most important one, China's national day that takes place for 3 days in October.
China to USA shipping; all information:
When you are willing to ship from China to the USA, you must consider all the possible conditions such as weather conditions, health conditions, festivals, and holidays of China. Beware of placing any shipment orders during the rush days of the year such as Christmas holidays and winter break. These holidays and breaks keep the air routes and sea routes busy all over the world especially in China where people celebrate every holiday wholeheartedly. You must know about the period it takes from China to USA shipping. For the possible time taken via seaways, we can say a maximum of 35 days will take to reach a ship throughout the way.
Cost of China to USA shipping
There are different prices of shipment of different methods from China to USA shipping. The prices mostly depend upon the weight, amount, and size of the product along with the shipment methods you choose. For China air express, it takes per kilo rate of the product from China to USA shipping. Precisely, it is $4 to $10 per kg. Similarly, when you use air freight from China to USA shipping, it will cost you $5 to $8 per kg. While working with the container, it will take a bit more price than usual because it is bigger, have more room as well as security when it is from China to USA shipping. The sea freight costs $2999 to $4000 on one container. The rise of the price of containers can affect the cost of products inside.
Every year, America imports a large amount of all types of goods via shipping from china to the USA. We have discussed the holidays and festivals in China, the methods that can be used in Shipping from China to the USA, as well as the cost and timings of the shipment procedures. We discusses the departure timings and costs from the Chinese port, lets shed some light on the landing and arrival costs of a shipment coming from China to the USA.
Landing costs of Shipping from China to the USA
Transportation through air and sea can cause some additional charges after landing at a seaport or airport. These Additional charges are known as "" landing charges"". Landing charges include all the charges that you might have to pay as customer clearance, delivery charges, paying to the shipping agent for the transportation to the warehouses, home delivery, or at any place. This is one of the most important things to be in your knowledge when you are getting some shipment from China to the USA. You need to see your pockets before making any orders and so.
Delay factors of shipments:
There could be many delay factors that you might take under consideration before you make yourself order any shipping from china to the USA. Say for supposing, except for the traditional holidays in China, there may be worst weather conditions on the seaport or airport that might get your shipment delayed. Currently, the global pandemic has made shipment from both airways as well as seaways quite challenging. The ban of movement or air flights, as well as sea shipping in many countries, have cast a massive impact on shipping from China to the USA."	4	67	1	sashagreg	shipping-from-china-to-the-usa
5737	5737	sample-Superstore		[]		0	11	0	wallacefqq	samplesuperstore
5738	5738	ConvNet384		[]		0	9	0	mithilsalunkhe	convnet384
5739	5739	pytorch-image-models-CDW		['software']		0	1	0	singularity1012	pytorchimagemodelscdw
5740	5740	timm_3monthsold		[]		3	126	7	nischaydnk	timm-3monthsold
5741	5741	my_swin_models		[]		0	6	0	hahahaha123ggg	my-swin-models
5742	5742	Diabetesky		[]		0	22	3	kderyldz	diabetesky
5743	5743	Test-Sqlite-Database	A SQlite3 Database to run on kaggle	['business', 'computer science', 'beginner', 'tabular data', 'sql']	"Context
This is an empty database that can be used for sq3lite module in Pandas to run Sql Queries on your data frames with ease."	1	23	0	zaikali	testsqlitedatabase
5744	5744	Heartbeat Sound		['health']		4	47	1	abdallahaboelkhair	heartbeat-sound
5745	5745	Biet Petfinder		[]		0	18	0	mithilsalunkhe	biet-petfinder
5746	5746	conv_models		[]		0	5	0	jianguotang	conv-models
5747	5747	111111111111		[]		0	5	0	rainfalllove	111111111111
5748	5748	weight_joke		[]		0	9	0	famert	weight-joke
5749	5749	gcp auth		[]		0	0	0	kishalmandal	gcp-auth
5750	5750	Models		['clothing and accessories']		0	13	0	filippoangelini	models
5751	5751	swin_pred_df		[]		0	3	0	deepkun1995	swin-pred-df
5752	5752	tgbr-train-video-2		['arts and entertainment']		0	11	0	prateekagnihotri	tgbr-train-video-2
5753	5753	Medicinet Diseases and Symptoms 		[]	"Context
Dataset created for Graph Analysis. 
Content
Dictionary containing disease, symptoms values. 
Acknowledgements
Dataset available due to the Medicinet community. Last scraped on November, 2021."	19	52	0	mirunaandreeagheata	medicinet-diseases-and-symptoms
5754	5754	tgbr-train-video-0		['arts and entertainment']		0	1	0	prateekagnihotri	tgbr-train-video-0
5755	5755	Harry Potter Movies Dataset	Chapters, Characters, Data_Dictionary, Dialogue, Movies, Places, Spells	['literature', 'movies and tv shows', 'music', 'exploratory data analysis', 'feature engineering']	"Context
Harry Potter Movies Dataset
Content
Contains different csv-files and LICENCE.
Acknowledgements
My acknowledgments are given to: Louis Chauvet
Inspiration
I love Harry Potter Movies. I found this dataset on the Internet (it was a part of a competition, that was posted on Linked In). 
License
MIT License
Copyright (c) 2021 Louis Chauvet
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."	751	5436	30	maricinnamon	harry-potter-movies-dataset
5756	5756	submitweight		[]		0	39	0	mmaymay	submitweight
5757	5757	convnextexp		[]		1	15	0	gyanendradas	convnextexp
5758	5758	GDP  of country using k-means dataset		[]		1	26	0	shasuatthackar	gdp-of-country-using-kmeans-dataset
5759	5759	KONTOL		[]		0	8	0	abdulslm	kontol
5760	5760	test_video		[]		0	0	0	amagassouba	test-video
5761	5761	timm (PyTorch Image Models)	PyTorch image models by Ross Wightman (v0.5.2)	['computer vision', 'deep learning', 'cnn', 'pytorch']	"Source: https://github.com/rwightman/pytorch-image-models
Version: v0.5.2
Installation: see this notebook for quick tutorial
import sys
sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')
import timm
License: Apache License 2.0"	3328	13458	183	kozodoi	timm-pytorch-image-models
5762	5762	Scitsr_data		[]		3	26	1	piyushlife	scitsr-data
5763	5763	PetFinder-Model552		[]		0	12	0	lftuwujie	petfindermodel552
5764	5764	starfish_packages		[]		0	24	0	kevin1742064161	starfish-packages
5765	5765	oof_cdw		[]		0	4	0	singularity1012	oof-cdw
5766	5766	vgg_rc		[]		0	19	0	ironluu	vgg-rc
5767	5767	GDP of each country and region(1960-2020)	GDP:Gross Domestic Product	['business']	"Countries
'United States', 'United Kingdom', 'France', nan, 'Japan', 'Canada', 'Italy', 'Brazil', 'Turkey', 'Mexico', 'Netherlands', 'Spain', 'Switzerland', 'South Africa', 'Austria', 'Denmark', 'New Zealand', 'Finland', 'Norway', 'Greece', 'Bangladesh', 'Nigeria', 'Chile', 'Colombia', 'South Korea', 'Pakistan', 'DRC', 'Thailand', 'Israel', 'Peru', 'Morocco', 'Malaysia', 'Puerto Rico', 'Iraq', 'Sri Lanka', 'Hong Kong', 'Sultan', 'Uruguay', 'Ghana', 'Zimbabwe', 'Guatemala', 'Ecuador', 'Syria', 'Senegal', 'Kenya', 'Zambia', 'Luxembourg', 'Jamaica', 'Madagascar', 'Dominica', 'Cambodia', 'Cameroon', 'Bolivia', ""C ô te d'Ivoire"", 'Afghanistan', 'Panama', 'Trinidad and Tobago', 'Nepal', 'Costa Rica', 'Niger', 'Uganda', 'Sierra Leone', 'Chad', 'Haiti', 'Papua New Guinea', 'Benin', 'Nicaragua', 'Burundi', 'Liberia', 'Somalia', 'Bahamas', 'Mawlawi', 'Gabon', 'Congo (Brazzaville)', 'Rwanda', 'Fiji Islands', 'Central Africa', 'Suriname', 'Mauritania', 'Bermuda', 'Eswatini', 'Lesotho', 'Belize', 'Saint Vincent and the Grenadines', 'Argentina', 'Equatorial Guinea', 'Egypt', 'Kuwait', 'Tunisia', 'Jordan', 'Paraguay', 'French Polynesia', 'New Caledonia', 'Brunei', 'Oman', 'Indonesia', 'Solomon Islands', 'Saudi Arabia', 'Germany', 'Cuba', 'Qatar', 'Monaco', 'Malta', 'Liechtenstein', 'Guinea Bissau', 'Greenland', 'Kiribati', 'UAE', 'Tonga', 'Saint Lucia', 'Antigua and Barbuda', 'Grenada', 'Vanuatu', 'Angola', 'Mozambique', 'Namibia', 'Cape Verde', 'Bhutan', 'Maldives', 'Ethiopia', 'Marshall Islands', 'Samoa', 'the Federated States of Micronesia', 'Albania', 'Laos', 'Vietnam', 'Lebanon', 'Poland', 'Czech Republic', 'Libya', 'Kazakhstan', 'Belarus', 'Uzbekistan', 'Slovakia', 'Azerbaijan', 'Georgia', 'Yemen', 'Macedonia', 'Moldova', 'Kyrgyzstan', 'Tajikistan', 'Armenia', 'Palau', 'Hungary', 'Palestine', 'Bosnia and Herzegovina', 'Croatia', 'Serbia', 'Lithuania', 'Latvia', 'Isle of man', 'Faroe Islands', 'Myanmar', 'San Marino', 'Timor Leste', 'Turks and Caicos Islands', 'Sao Tome and Principe', 'Guam', 'Northern Mariana Islands', 'American Samoa', 'South Sudan', 'cura ç Ao'
States
'America', 'Europe', 'Asia', 'Oceania', 'Africa'"	599	2960	21	holoong9291	gdp-of-all-countries19602020
5768	5768	csv_out		[]		1	14	0	mmaymay	csv-out
5769	5769	scRNA-seq for Ewing Sarcoma  CHLA9 CHLA10 TC71	GSE146221 10X Genomics single cell RNA-Seq of Ewing Sarcoma cell lines 	['genetics', 'biology', 'biotechnology', 'cancer']	"Content
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes.
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
There are several types of cells, denoted by CHLA9, CHLA10, and TC71
e.g. CHLA10: https://www.wikidata.org/wiki/Q54812148 
CHLA9: https://web.expasy.org/cellosaurus/CVCL_M150, https://www.wikidata.org/wiki/Q54812509
TC71: https://web.expasy.org/cellosaurus/CVCL_2213
Acknowledgements
Paper:
https://www.mdpi.com/2072-6694/12/4/948/htm
Cancers 2020, 12(4), 948; https://doi.org/10.3390/cancers12040948
Reconstruction of Ewing Sarcoma Developmental Context from Mass-Scale Transcriptomics Reveals Characteristics of EWSR1-FLI1 Permissibility
Henry E. Miller 1,2OrcID,Aparna Gorthi 1,2OrcID,Nicklas Bassani 2,Liesl A. Lawrence 1,2OrcID,Brian S. Iskra 1,2 andAlexander J. R. Bishop 
Data:
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE146221
Github:
https://github.com/Bishop-Laboratory/ewing_sarcoma_miller_cancers_2020
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6"	10	467	0	alexandervc	rna-seq-data
5770	5770	y = x + 10	Linear Regression Model DataSet for Beginners.	['linear regression']	"y = x + 10
This is a simple, but beginner-friendly dataset.
What to do with this DataSet
Split the data into three-set ( Train, Validation and Test) i.e, [70%, 15%, 15%]
Create a Machine learning model to predict the value of y for the respective x value.
Result can be seen with a machine learning model example which is deployed by me
Here: <a href=""https://apps.streamlitusercontent.com/adam-al-rahman/yx10/master/app.py/+/#number-array-dataset"" target=""_blank"">Machine Learning Model</a>"	3	23	0	adamalrahman	y-x-10
5771	5771	tomato_leaf_disease_augmented_train_test		[]		0	7	0	saroj12dangol	tomato-leaf-disease-augmented-train-test
5772	5772	handstyle		[]		1	16	0	zmjjiang	handstyle
5773	5773	Life Expectancy vs GDP, 1950-2018	Life expectancy vs GDP per capita from 1950 to 2018.	['people and society', 'economics', 'data analytics']	"Context
Life expectancy at birth is defined as the average number of years that a newborn could expect to live if he or she were to pass through life subject to the age-specific mortality rates of a given period. The years are from 1950 to 2018.
Content
For regional- and global-level data pre-1950, data from a study by Riley was used, which draws from over 700 sources to estimate life expectancy at birth from 1800 to 2001.
Riley estimated life expectancy before 1800, which he calls ""the pre-health transition period"". ""Health transitions began in different countries in different periods, as early as the 1770s in Denmark and as late as the 1970s in some countries of sub-Saharan Africa"". As such, for the sake of consistency, we have assigned the period before the health transition to the year 1770. ""The life expectancy values employed are averages of estimates for the period before the beginning of the transitions for countries within that region. ... This period has presumably the weakest basis, the largest margin of error, and the simplest method of deriving an estimate.""
For country-level data pre-1950,  Clio Infra's dataset was used, compiled by Zijdeman and Ribeira da Silva (2015).
For country-, regional- and global-level data post-1950, data published by the United Nations Population Division was used, since they are updated every year. This is possible because Riley writes that ""for 1950-2001, I have drawn life expectancy estimates chiefly from various sources provided by the United Nations, the World Bank’s World Development Indicators, and the Human Mortality Database"".
For the Americas from 1950-2015, the population-weighted average of Northern America and Latin America and the Caribbean was taken, using UN Population Division estimates of population size.
Acknowledgements
Life expectancy:
Data publisher's source: https://www.lifetable.de/RileyBib.pdf
Data published by: James C. Riley (2005) – Estimates of Regional and Global Life Expectancy, 1800–2001. Issue Population and Development Review. Population and Development Review. Volume 31, Issue 3, pages 537–543, September 2005., Zijdeman, Richard; Ribeira da Silva, Filipa, 2015, ""Life Expectancy at Birth (Total)"", http://hdl.handle.net/10622/LKYT53, IISH Dataverse, V1, and UN Population Division (2019)
Link: https://datasets.socialhistory.org/dataset.xhtml?persistentId=hdl:10622/LKYT53, http://onlinelibrary.wiley.com/doi/10.1111/j.1728-4457.2005.00083.x/epdf, https://population.un.org/wpp/Download/Standard/Population/
Dataset: https://ourworldindata.org/life-expectancy
GDP per capita:
Data publisher's source: The Maddison Project Database is based on the work of many researchers that have produced estimates of economic growth for individual countries. 
Data published by: Bolt, Jutta and Jan Luiten van Zanden (2020), “Maddison style estimates of the evolution of the world economy. A new 2020 update”.
Link: https://www.rug.nl/ggdc/historicaldevelopment/maddison/releases/maddison-project-database-2020
Dataset: https://ourworldindata.org/life-expectancy
Inspiration
The life expectancy vs GDP per capita analysis."	7	53	0	luxoloshilofunde	life-expectancy-vs-gdp-19502018
5774	5774	All of the Optiver Leaderboard files	(a collection of all the raw csv files)	['sports', 'finance']	"These are the Public Leaderboard score files from the kaggle Optiver Realized Volatility Prediction competition.
It also contains a unified merged_Optiver_scores.csv file, which was produced by the notebook ""Optiver: Interactive shakeup scatterplots""."	36	341	8	carlmcbrideellis	optiver-public-leaderboard-files
5775	5775	DATASET_TEST		[]		0	5	0	guoyan1	dataset-test
5776	5776	minejigsawX		[]		1	38	0	yus002	minejigsawx
5777	5777	Final concepts and data		[]		0	12	0	yiyangwang1105	final-concepts-and-data
5778	5778	vgg_rc		[]		10	9	0	mkagglemm	vgg-rc
5779	5779	COTS_YOLOv5_data		[]		0	6	0	georgeteo89	cots-yolov5-data
5780	5780	Face Mask Dataset(Yolov5 Format)		[]		1	28	0	yoloemperor	face-mask-datasetyolov5-format
5781	5781	Crop Recommendation	Maximize agricultural yield by recommending appropriate crops	['pandas', 'sklearn']		7	90	0	neerajpokala	crop-recommendation
5782	5782	stemmed_data_new		[]		0	1	0	regressionanalysisa	stemmed-data-new
5783	5783	faces happy sad		['online communities']		0	9	0	duaazehraalvi	faces-happy-sad
5784	5784	convnext_base_384		[]		0	1	0	xyzdivergence	convnext-base-384
5785	5785	COTS Fold4 Validation Dataset		[]		1	73	0	alexchwong	cots-fold4-validation-dataset
5786	5786	orig_swin_train(5)		[]		0	2	0	darkravager	orig-swin-train5
5787	5787	convnext_large_384		[]		0	2	0	xyzdivergence	convnext-large-384
5788	5788	spamdetection	email is spam or not	['tabular data', 'text data', 'email and messaging']		2	13	0	imranmmunshi	spamdetection
5789	5789	HappySadVEr2		[]		3	18	0	muhammaddin	happysadver2
5790	5790	Financial Table Data		['tabular data', 'image data', 'text data']		6	76	3	piyushlife	financial-table-data
5791	5791	Last_9_assessment		[]		0	2	0	prashanthsheri	last-9-assessment
5792	5792	HappySADVER1DATASET		[]		2	5	0	muhammaddin	happysadver1dataset
5793	5793	Emotion prediction（label）		['religion and belief systems']	"positive：label&gt;=0.7;
Neutral：0.4="	1	20	0	xiangdongyuan	emotion-predictionlabel
5794	5794	QuickDelivery startup		['business']		0	8	0	mukeshkumar95	quickdelivery-startup
5795	5795	happy and sad faces		['online communities']		2	22	0	duaazehraalvi	happy-and-sad-faces
5796	5796	model_weights_data		[]		0	12	0	shekharrastogi	model-weights-data
5797	5797	QuickDelivery startup company		['business']		1	1	0	mukeshkumar95	quickdelivery-startup-company
5798	5798	pet_data		[]		5	49	0	tkm2261	pet-data
5799	5799	centercrop-Image	centercrop and resize 224 	[]		3	28	0	sicpama	centercrop
5800	5800	Starbucks Store Loacation Dashboard		[]		11	73	0	azha123	starbucks-store-loacation-dashboard
5801	5801	best_baseline_model		[]		0	0	0	hustwoods	best-baseline-model
5802	5802	ready_models		[]		0	10	0	fidanmusazade	ready-models
5803	5803	Hollow Knight Combat Dataset	All Enemies health and Nail Damage values from Hollow Knight	['arts and entertainment', 'games', 'animals', 'beginner', 'data cleaning', 'tabular data']	Forge your own path in Hollow Knight! An epic action adventure through a vast ruined kingdom of insects and heroes. Explore twisting caverns, battle tainted creatures and befriend bizarre bugs, all in a classic, hand-drawn 2D style.	22	1125	10	achalmahale	hollow-knight-combat-dataset
5804	5804	Patent Match	A Dataset for Matching Patent Claims with Prior Art	['law']	"Copyright (c) 2021 HPI-Information-Systems
https://hpi.de/naumann/projects/web-science/paar-patent-analysis-and-retrieval/patentmatch.html
@article{risch2020match,
title={{PatentMatch}: A Dataset for Matching Patent Claims with Prior Art},
author={Risch, Julian and Alder, Nicolas and Hewel, Christoph and Krestel, Ralf},
year={2020},
journal = {ArXiv e-prints},
eprint={2012.13919},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
Downloaded
Training: https://owncloud.hpi.de/s/Uxqyl73NJhYPwoY
Test: https://owncloud.hpi.de/s/xOh2cXJXiJu9rzM
Training balanced: https://owncloud.hpi.de/s/E9AtoJou1kHF24Y
Test balanced: https://owncloud.hpi.de/s/iDk8DDWXRXkTPmm
Training ultra balanced: https://owncloud.hpi.de/s/4KoV9xlzYbgVMTT
Test ultra balanced: https://owncloud.hpi.de/s/nGNdjguuuQmmXZC
DPR training: https://owncloud.hpi.de/s/qtRYr2isCgIriIr
DPR test: https://owncloud.hpi.de/s/fn3Om0tnVwJnptk"	0	14	1	seshurajup	patent-match
5805	5805	Sarcasm Detection Data Set		[]		0	2	0	sid150794	sarcasm-detection-data-set
5806	5806	ant_bee_example		[]		0	15	0	haosenluo	ant-bee-example
5807	5807	dirty word		['arts and entertainment']		4	12	0	yochino	dirty-word
5808	5808	MMDetection with ConvNext		[]		0	9	0	mlneo07	mmdetection-with-convnext
5809	5809	libary		[]		0	0	0	shaoyanxia	libary
5810	5810	model_file		[]		0	0	0	shaoyanxia	model-file
5811	5811	COTS-temp		[]		0	72	0	ipythonx	cotstemp
5812	5812	keloid scars _vs _ hypertrophic scars	A dataset of images of Keloid scars and hypertrophic scars	['cancer']		1	15	0	quratulainislam	keloid-vs-scar
5813	5813	fbmapas		[]		0	15	1	fernandobordi	fbmapas
5814	5814	petfinder convnext		[]		0	10	0	sehwanjoo	petfinder-convnext
5815	5815	mis-mapas		[]		0	9	0	fernandobordi	mismapas
5816	5816	alldatatest		[]		0	1	0	dyqdyq1	alldatatest
5817	5817	Fold04 Clean Petfinder& fastai KF 10 Mixup - 5e-04		[]		0	5	0	iftiben10	fold04-clean-petfinder-fastai-kf-10-mixup-5e04
5818	5818	COVID-19CT 19K V1.0		[]		0	9	0	mustai	covid19ct-19k-v10
5819	5819	synthtext dbnet tf2 900		[]		5	6	0	ocrteamriad	synthtext-dbnet-tf2-900
5820	5820	synthtext dbnet tf2 600		[]		0	13	0	ocrteamriad	synthtext-dbnet-tf2-600
5821	5821	synthtext dbnet tf2 300		[]		0	21	0	ocrteamriad	synthtext-dbnet-tf2-300
5822	5822	convnextmodels		[]		0	6	0	greepex	convnextmodels
5823	5823	Electric Vehicle Charging Stations	Find electric vehicle charging stations in the United States and Canada. 	['cities and urban areas', 'environment', 'automobiles and vehicles', 'travel']	"Context
Find electric vehicle charging stations in the United States and Canada. 
Acknowledgements
Data source: https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/all/#request-url
Cover image credit: https://www.pexels.com/photo/sea-water-summer-industry-9799997/"	622	4512	28	prasertk	electric-vehicle-charging-stations-in-usa
5824	5824	Vision Transformers (VIT)		['health']		0	16	0	crained	vision-transformers-vit
5825	5825	Battery temperature analysis		['electronics']	"ts：timestamp；
tv：battery voltage；
soc：percentage of remaining battery capacity；
soe：percentage of battery residual energy；
soh：battery health；
ct：battery cycles；
afp：actual full battery capacity；
crc：remaining battery capacity；
act：average cell temperature；
ct：charging times；"	9	60	0	xiangdongyuan	battery-temperature-analysis
5826	5826	jigsaw-multilingual-toxic-comment-classification		[]		0	15	0	yochino	jigsawmultilingualtoxiccommentclassification
5827	5827	Toxic Comment Classification Challenge		[]		0	7	0	yochino	toxic-comment-classification-challenge
5828	5828	dirty_words		[]		0	10	0	yochino	dirty-words
5829	5829	swin_transformer_384_in1k	swin_transformer_384_in1k	[]		1	97	0	wanglinlei	swin-384-v1
5830	5830	swin384-clf-mixup	swin_transformer_384_no_fold4	[]		0	15	0	stuliuc	swin384clfmixup
5831	5831	NBA MVP candidates 1980 - 2022		['basketball']		11	52	0	dbtjdals	nba-mvp-candidates-1980-2022
5832	5832	sunspots		['astronomy']		0	3	0	cinthiamalena	sunspots
5833	5833	ft_mco_833_base_valid		[]		0	11	0	hangy132	ft-mco-833-base-valid
5834	5834	unetdicemodel1		[]		0	4	0	sunchch555	unetdicemodel1
5835	5835	nl_data_example		[]		1	29	0	saewoonam	nl-data-example
5836	5836	pawpularity end game siitaram		['games']		0	12	0	adityasrivastava7	pawpularity-end-game-siitaram
5837	5837	peteyo_subtaskmodel		[]		0	47	0	teyosan1229	peteyo-subtaskmodel
5838	5838	Emma Raducanu	Tweets about the winner of women US Open 2021	['celebrities', 'tennis', 'sports', 'nlp', 'news', 'online communities']	"Context
I collect recent tweets about Emma Raducanu, winner of women US Open 2021. The teen Brit with Romanian and Chinese parents, born in Canada and arrived in Great Britain at 2 years old, stormed the US event from qualifiers, playing 10 games without losing one single set (20 sets won in a row). She is the first British woman to win a Grand Slam since 1977 (Virginia Wade), the first women in US Open history to win the event from the qualifiers. She jumped more than 120 points in the ranking to land on 23rd position. She is also a very good student, landing A grades in mathematics and economy (her preferred domains) in her selective grammar school from south London.
<img src=""https://ichef.bbci.co.uk/news/976/cpsprodpb/7561/production/_120494003_gettyimages-1339395950.jpg"">
Data collection
The data is collected using tweepy Python package to access Twitter API. I use a relevant search term  for the topic (#EmmaRaducanu).
Data collection frequency
The data is collected continuously using a script that collects a small number of recent tweets (using Twitter API and tweepy). The dataset obtained at each sampling time step is merged with current (or previously collected) dataset and stored dataset in csv format is saved on disk.  Once or several times per day the currently accumulated dataset is uploaded on Kaggle as a new version of the tweets dataset.
Inspiration
You can perform multiple operations on the tweets about this British teen with meteoric ascension  at US Open 2021. Here are few possible suggestions:
Study the subjects of recent tweets about the new US Open champion or about tennis;
Perform various NLP tasks on this data source (topic modelling, sentiment analysis);
Can you identify tweets about tennis women, British athletes or US Open?
Follow the trends in the news about tennis, US Open or Emma;
Perform sentiment analysis on the tweets corpus but also split on topics, countries etc.
Study the hashtags (associated to the tweets) distribution."	48	1527	17	gpreda	emma-raducanu
5839	5839	baseline_jigsaw_hatim		[]		0	36	0	hatimbr	baseline-jigsaw-hatim
5840	5840	petfinder_exp82		[]		3	7	1	titericz	petfinder-exp82
5841	5841	Deepmind_Scraped_Jan2022		[]		0	2	0	anasputhawala	deepmind-scraped-jan2022
5842	5842	Deepmind_articles_webscraped_Jan2022	This data was web-scraped using BeautifulSoup and Selenium.	[]		0	4	1	anasputhawala	deepmind-articles-webscraped-jan2022
5843	5843	output		[]		0	3	1	ayahamad	output
5844	5844	Canada National & Provincial Per Capita Income	Source: Statistic Canada, 1976 to 2019, with 2019 constant dollars.	['business', 'economics', 'beginner', 'data cleaning', 'text data', 'canada']	"Filtered data which only remains 10 provinces and national capita income, median income, with 16 years old & over, both sexes, in 2019 constant dollar
The raw data had been already adjusted by 2019 constant dollar, from 1976-2019
Please be aware territories of Canada were not listed in the original dataset
For example, the 2018 Canada national average income is not equal to the average of 10 provinces income, since territories are not in the list. 
My practicing data exploration of this dataset:
Facts of Individuals Income in Canada, 1976 - 2019
Acknowledgements
Data source:
Income of individuals by age group, sex and income source, Canada, provinces and selected census metropolitan areas
Raw data version: 
Table: 11-10-0239-01 (formerly CANSIM 206-0052)
Release date:
2021-03-23 
Inspiration
I was really surprised when revealing these rows,  it seems like there isn't much growth since 1976 Canada average income is 40,800 dollars while 2019 is 49,000 dollars. (Please be noticed these are adjusted by 2019 constant dollar)
Please correct me if I was wrong. Thank you"	71	525	10	charlesluan	canada-national-provincial-capita-income-762019
5845	5845	Canadian Government Travel Disclosures	Proactive Travel Expense Disclosures Licensed Under Open Government Licence	['government', 'travel']	"Context
I downloaded this data from the Canadian government to explore travel expenditures.
Content
Contains expenses related to travel and information about the travel including: name, organisation, title, purpose and additional comments.
Acknowledgements
Thank you to the Canadian government for its efforts towards transparency and licensing the publication of this data. More license information here: https://open.canada.ca/en/open-government-licence-canada
Inspiration
I plan to clean this dataset and update it at some point. I'm curious to know more about how much we are spending on travel, who is spending it, where are they travelling and why. I'd also like to know how the pandemic has affected travel. The expenses are audited and approved prior to being disclosed in this dataset so I don't expect to find any absurd expenditures but I will look for them."	3	36	0	thomaskieffer	canadian-government-travel-disclosures
5846	5846	sartorius-third-place-models		[]	Models used by the third place team Not_experts in the Sartorius - Cell Instance Segmentation competition.	6	45	0	slawekbiel	sartoriusthirdplacemodels
5847	5847	SJ_Model+Interpretation		[]		0	13	0	sabinajafarova	sj-modelinterpretation
5848	5848	Home Sensordata	Sensordata of temperature, humidity and latency	['exercise', 'physical science', 'beginner', 'intermediate', 'tabular data']	"Context
Temperature, humidity and internet latency were recorded every minute over a period of +100 days.
Content
There are two files, which are identic (csv, json).
The columns are: 
- time_id: time of datapoint (format YYYY-MM-DD hh:mm:ss)
- ping_ms: latency of the internet in ms (float)
- temperature_c: temperature of the livingroom in °C (integer)
- humidity_p: humidity of the livingroom in % (integer
Inspiration
Can you find some special days with outliers?"	137	1829	13	johntrunix	home-sensordata
5849	5849	Garbage	Garbage items images hand labeld to 7 classes	['earth and nature', 'energy']	"Context
The data is being collected by Smart Trash bin powered with Raspberry Pi an ArduCam. The image quality is not the best because of the camera. This data is being used to train my home Trash classifier, so the data is constatly updating. 
My Goal is to create a dataset with at least of 10000 images. Be aware that the number of classes can be also changed. I am considering to separate compost on: fruits/veggies, compostable utensils, meat/fish, napkins/dirty paper. All the images have background with my trash bins and hands. 
Follow up:
1. Get rid of background (I might need to use image difference to get rid of background so my trimages will be less sensetive to different background)
2. Separate Compost and Trash
Content
The dataset consists of 2274 images of garbage splitted to 7 classes: cardboard, compost, glass, metal, paper, plastic,  trash. There's a part of images having a size of 224x224, however most of the images have the size of 320x320 and new incoming images will have a size of 320x320. 
Inspiration
This data is made by my Smart Trash camera project. You can use it if you need it. If you have any suggestions, please let me know."	7	47	0	fedorgrab	garbage
5850	5850	Traindataset		[]		0	10	0	hammadahmedkhan	traindataset
5851	5851	textex1		[]		0	2	0	nantiyafueangphuang	textex1
5852	5852	petfinder-colab-models-v1		[]		0	24	0	ks2019	petfinder-colab-models-v1
5853	5853	medt cell paspp global cache		[]		2	63	0	phcli2792000	medt-cell-paspp-global-cache
5854	5854	PetFinder-Model20		[]		0	4	0	lftuwujie	petfindermodel20
5855	5855	swin-tranformer-10kmixup1		[]		0	14	0	zhenyang8848	swintranformer10kmixup1
5856	5856	target		[]		0	3	0	abrahamanderson	target
5857	5857	Waste Classifier/		['earth and nature']		0	20	1	poudelsujan	waste-classifier
5858	5858	training		[]		0	2	0	abrahamanderson	training
5859	5859	yolov5	yolov5_for_great_barrier_reef	[]		0	57	0	zwehtetpaing123	yolov5
5860	5860	DATA-RUMAH-JAKSEL		['architecture']		0	13	0	gustiosamba	datarumahjaksel
5861	5861	WorkingTransformer		[]		0	26	0	darthvader4067	workingtransformer
5862	5862	Our Best Santa 2021 submissions	The absolutely super permutations 	['religion and belief systems', 'holidays and cultural events']		2	106	2	jbomitchell	best-current-santa-2021-submissions
5863	5863	petfinder-009		[]		0	3	0	kenkengoda	petfinder-009
5864	5864	petf_swin_large_508_model_metricfixed		[]		0	3	0	nhac43	petf-swin-large-508-model-metricfixed
5865	5865	Cyclistic 2021 data	Data about how annual members vs casual members use the bikes differently	['cycling', 'data visualization', 'data analytics', 'dplyr', 'tidyverse']	"Data is located in Amazon AWS server : https://divvy-tripdata.s3.amazonaws.com/index.html
Data are provide by  https://ride.divvybikes.com/data-license-agreement
the original data set was organized e organized in  monthly tables, however the data had been merged together to create an annual table for 2021."	65	600	5	adehghani	cyclistic-2021-data
5866	5866	divvyset		[]		0	8	0	peteandwood	divvyset
5867	5867	miniimagenet-tieredimagenet		[]		0	15	0	andrijdavid	fsl-imagenet
5868	5868	TerrainGAN-datasets		[]		1	12	0	jayinnn	terraingandatasets
5869	5869	test data		[]		0	4	0	shakirbugti	test-data
5870	5870	effmodels		[]		0	4	0	greepex	effmodels
5871	5871	0jghjgyj		[]		0	19	0	leonardlind	0jghjgyj
5872	5872	ig_res101_384_ext_x3		[]		0	1	0	xyzdivergence	ig-res101-384-ext-x3
5873	5873	Product Listing Walmart	This dataset includes product listing data from Walmart	['retail and shopping']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 439956  Domain Name : walmart.com  Date Range : 01st Apr 2021 - 30th Apr 2021   File Extension : csv
Available Fields : Uniq Id, Crawl Timestamp, Pageurl, Website, Title, Num Of Reviews, Average Rating, Number Of Ratings, Model Num, Sku, Upc, Manufacturer, Model Name, Price, Monthly Price, Stock, Carrier, Color Category, Internal Memory, Screen Size, Specifications, Five Star, Four Star, Three Star, Two Star, One Star, Discontinued, Broken Link, Joining Key    
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	43	386	6	promptcloud	product-listing-walmart
5874	5874	petf_cswin_l_224_mixup01_mfixed		[]		0	2	0	nhac43	petf-cswin-l-224-mixup01-mfixed
5875	5875	100 Sports Image Classification 	  13572 train, 500 test, 500 validate images 224,224,3 jpg format 	['sports', 'computer vision', 'classification', 'deep learning', 'image data']	"Context
Please upvote if you find this dataset of use. - Thank you
Images were gathered from internet searches. The images were scanned with  a duplicate image detector program I wrote. Any duplicate images were removed to prevent bleed through of images between the train, test and valid data sets. All images were then resized to 224 X224 X 3 and converted to jpg format.  A csv file is included that for each image file contains the relative path to the image file, the image file class label and the dataset (train, test or valid) that the image file resides in. This is a clean dataset. If you build a good model you should achieve at least 95% accuracy on the test set. If you build a very good model for example using transfer learning you should be able to achieve 98%+ on test set accuracy. If you find this data set useful please upvote. Thanks
Content
Collection of sports images covering 100 different sports.. Images are 224,224,3 jpg format. Data is separated into train, test and valid directories.  Additionallly a csv file is included for those that wish to use it to create there own train, test and validation datasets.
.
Inspiration
Wanted to build a high quality clean data set that was easy to use and had no bad images or duplication between the train, test and validation data sets. Provides a good data set to test your models on. Design for  straight forward application of keras preprocessing functions like ImageDataenerator.flow_from_directory or if you use the csv file ImageDataGenerator.flow_from_dataframe. This dataset was carefully created so that the region of interest (ROI) in this case the sport occupies approximately 50% of the pixels in the image. As a consequence even models of moderate complexity should achieve training and validation accuracies in the high 90's."	1107	8503	37	gpiosenka	sports-classification
5876	5876	GDPR Fines Repository	Updated : January 2022	['government', 'law', 'beginner', 'tabular data', 'public safety']	"Context
In May 2018, GDPR went into effect. Since then, the local european authorities (such as CNIL for France) issued fines to companies for non respect of the GDPR principles.
Content
A data referential of all known GDPR fines issued by european local authorities regarding GDPR reglementation."	8	16	0	valentindefour	gdpr-fines-referential
5877	5877	detoxify_wheel		[]		0	30	0	takeshiiijima	detoxify-wheel
5878	5878	mrcnn_origin		[]		1	5	0	ansuld	mrcnn-origin
5879	5879	models		['clothing and accessories']		5	101	0	marcocavaco	models
5880	5880	TutorialData		[]		0	12	0	namratakapoor1	tutorialdata
5881	5881	Bellabeat		[]		0	21	0	matthewmellanby	bellabeat
5882	5882	SasOdevTest		[]		0	23	0	cengizhankoal	sasodevtest
5883	5883	covid_47		[]		0	11	0	ananyaanu47	covid-47
5884	5884	Analysis of US accidents	A Countrywide Traffic Accident Dataset (2016 - 2019)	['law']	"Description
This is a countrywide traffic accident dataset, which covers 49 states of the United States. The data is continuously being collected from February 2016 to March 2019, using several data providers, including two APIs which provide streaming traffic event data. These APIs broadcast traffic events captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks."	9	128	1	sobhanmons	analysis-of-us-accidents
5885	5885	TB specific subset of NIH CXR dataset		['earth and nature']		0	12	0	szainabzaidi	tb-specific-subset-of-nih-cxr-dataset
5886	5886	health insurance bills		[]	"Context
This dataset contains 5110 pieces of data and 13 attributes. Find the most important feature for health_bills.
Content
To uncover the charge of health bills"	10	51	0	hanzhan	health-insurance-bills
5887	5887	Marital Status		[]		0	17	0	raihansikdar	marital-status
5888	5888	ta_library		[]		0	2	0	puppakcheera	ta-library
5889	5889	Hate Speech Dataset		[]		1	31	1	idealisticintj	hate-speech-dataset
5890	5890	noclean_cv_2	cv5- 2 - no clean data max length = 128 	['nlp', 'deep learning']		0	12	0	maxmingwang	noclean-cv-2
5891	5891	regression_sample_dataset	regression_sample_dataset	['beginner', 'regression', 'pandas']		52	521	5	rhythmcam	regression-sample-dataset
5892	5892	Movielens Rating Dataset	Non Personalised Recommendation System Application	['recommender systems']	"Context
MovieLens 100K dataset has been a standard dataset used for benchmarking recommender systems for more than 20 years now it can be used for recommender systems. For non commercial personalised recommendations for movies you can check out website:  https://movielens.org/
Data was collected through the MovieLens web site (movielens.umn.edu) during seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set."	5	29	0	mukeshmanral	movielens-rating-dataset
5893	5893	power comp		[]		1	16	0	killuazoldyck6066	power-comp
5894	5894	Super_Carros	More than 5 thousand vehicules.	['automobiles and vehicles', 'beginner', 'tabular data', 'dplyr', 'tidyverse']	"Context
I've been interested in web scraping recently and for practicing I decided to scrape useful data from a easy website to scrape.
Content
More than 5 thousand vehicules records with details and prices in US dollars and dominican pesos. The data is from a dominican car selling site Supercarros.com. Scripts at https://github.com/Lien3105/Scrape_Supercarros"	0	15	0	neilcarvajal	super-carros
5895	5895	Opensea collections		[]		7	51	0	vickylau32	opensea-collections
5896	5896	UTAR-YK 	Thermal Face Dataset	['earth and nature']		1	26	0	kirakiat	utaryk-thermal-face-dataset
5897	5897	adultdataset		[]		0	24	0	riyadsalih	adultdataset
5898	5898	cv1_epch2		[]		0	20	1	maxmingwang	cv1-epch2
5899	5899	denemetest		[]		0	10	1	cengizhankoal	denemetest
5900	5900	New Dataset Sad Faces		['online communities']		0	5	0	shakirbugti	new-dataset-sad-faces
5901	5901	Fruits and Vegetables Leaves Dataset - Balanced		['food']		1	67	3	neeleshj	fruits-and-vegetables-leaves-dataset-balanced
5902	5902	Online performance of UK		[]		3	36	1	jhonnyliver	online-performance-of-uk
5903	5903	Alzheimer Point Cloud Dataset		[]		3	68	0	mirsayeed	alzheimer-point-cloud-dataset
5904	5904	Vesseldataset		[]		0	5	0	shaoyanxia	vesseldataset
5905	5905	Food Recognition Benchmark 2022 - AICrowd Dataset		[]		0	13	0	dschettler8845	food-recognition-benchmark-2022-aicrowd-dataset
5906	5906	DailyActivity		[]		0	2	0	matthewmellanby	dailyactivity
5907	5907	petf_convnext_l_224_bs32_mixup10_mfixed		[]		0	1	0	nhac43	petf-convnext-l-224-bs32-mixup10-mfixed
5908	5908	baidu-text-similarity		['internet']		0	11	0	luanticbg	baidutextsimilarity
5909	5909	swin_large_patch4_window7_224_longaxis_bayes_2024		[]		0	3	0	ttagu99	swin-large-patch4-window7-224-longaxis-bayes-2024
5910	5910	SGCC-matlab.txt		[]		0	9	0	wizardwangjiang	sgccmatlabtxt
5911	5911	Proyecto 1: Exceso de Muertes por COVID19	Realiza una estimación de las muertes por COVID19 en Latinoamérica	['public health', 'mortality', 'beginner', 'public safety', 'covid19']	"Introducción
En este proyecto realizarás un análisis utilizando hojas de cálculo a partir de un conjunto de datos sobre las muertes de COVID-19 en 5 países de latinoamérica. Para realizar este análisis deberás entender los datos, pensar críticamente para determinar los indicadores relevantes y construir visualizaciones que permitan comunicar tus hallazgos. 
La situación
El fin de semana pasado te encontrabas con un grupo de amigos y amigas reflexionando acerca del impacto de la pandemia en nuestras vidas. En medio de la conversación, alguien preguntó cuál sería el país de Latinoamérica más afectado por la pandemia. Los medios han hablado de Perú como uno de los países más afectados en el mundo, pero en tu grupo de amistades hay personas de varios países de la región y cada una considera que la situación en su país ha sido tremendamente grave, por lo que no logran ponerse de acuerdo. Luego de un breve debate, deciden que lo mejor para responder la pregunta es ir a los datos y formar un juicio basado no en anécdotas y experiencias personales, sino en datos. 
La conversación entonces se volcó a determinar qué datos exactamente utilizar para el análisis. El COVID-19 ha causado contagios, hospitalizaciones, muertes, pérdidas económicas y laborales, daños psicológicos y un sin fin de efectos adicionales. Tu grupo coincide que tratar de abarcar todos los posibles impactos sería excesivo, y determinan que lo más lógico para un análisis inicial es centrarse en analizar los datos de muertes por consecuencia de la pandemia. 
Para esto, alguien sugiere simplemente calcular el total acumulado de muertes COVID-19 reportadas y hacer un ranking de los países, pero rápidamente se dan cuenta de que eso pondría a Brasil y México naturalmente en las primeras posiciones dado que tienen la mayor cantidad de habitantes y, por lo tanto, la mayor cantidad de muertes. Para comparar “manzanas con manzanas” alguien sugiere utilizar el indicador: muertes COVID-19 por cada 100 mil habitantes, y el grupo estuvo de acuerdo. 
Tú decides llevarte la tarea de realizar estos cálculos y regresar al grupo con algunas conclusiones. Sin embargo, justo cuando ya estabas por empezar a buscar los datos oficiales de muertes de COVID-19 de los gobiernos, una compañera mencionó que en algún hilo de Twitter había leído que las cifras oficiales de muertes por causa de COVID-19 podrían estar subestimando significativamente la cantidad real de muertes. El hilo hacía referencia a un artículo en un reconocido medio internacional que mencionaba varias razones por las que podría suceder esta subestimación:
&gt; “Primero, las estadísticas oficiales [de muertes COVID-19] en muchos países excluyen a las víctimas que no dieron positivo por coronavirus antes de morir, lo que puede ser una mayoría sustancial en lugares con poca capacidad de realizar pruebas. En segundo lugar, es posible que los hospitales y los registros civiles no procesen los certificados de defunción durante varios días, o incluso semanas, lo que genera retrasos en los datos. Y tercero, la pandemia ha dificultado que los médicos traten otras afecciones y ha disuadido a las personas de ir al hospital, lo que puede haber provocado indirectamente un aumento en las muertes por enfermedades distintas del Covid-19. ” 
&gt;
&gt; The Economist - “La verdaderas muertes de la pandemia”
Tu amiga propone entonces que un mejor indicador para analizar el impacto del COVID-19 en la mortalidad de los países es calculando el “exceso de muertes”. Te explica que esto se calcula tomando el número de personas que murieron por cualquier causa (no solo COVID-19) en un período de tiempo determinado y en un lugar determinado, y comparándolo con las muertes que hubiesen acontecido si COVID-19 no hubiese sucedido - lo cual puede estimarse con una línea base histórica de las muertes que sucedieron durante los últimos años pre-pandemia. Por ejemplo, si en México (un lugar determinado), durante la tercera semana de enero del 2021 (un período de tiempo determinado), se reportaron 44,667 muertes totales (por todas las causas, no solo COVID-19), y sabemos que en los 5 años previos a la pandemia, en esa misma semana, en México, en promedio, murieron 15,454 personas (línea base histórica), podríamos decir que la diferencia de 29,213 muertes (44,667 menos 15,454), pueden ser atribuidas a la pandemia. Es decir, el “exceso de muertes” (comparado con un estimado de los 5 años anteriores) es lo que se atribuye a la pandemia. 
Así, el reto que has asumido entonces es calcular los estimados del exceso de muertes por Covid-19 por cada 100 mil habitantes y comparar las cifras entre los países de la región
Entregable
Para considerar completado este proyecto deberás entregar tu copia de la hoja de cálculo (con tu nombre agregado al título) por medio de la plataforma de aprendizaje.
Tu hoja de cálculo deberá tener, como mínimo, lo siguiente:
Una tabla comparativa con el total de muertes reportadas por COVID-19 por cada 100 mil habitantes, el total de “exceso de muertes” por cada 100 mil habitantes y la diferencia entre ambas cifras, por país. 
Un gráfico que muestre los datos de la tabla anterior en forma de gráfico de barras. 
Un gráfico por cada país que muestre el comparativo de la evolución en el tiempo de las muertes reportadas COVID-19 vs. el cálculo de “exceso de muertes” semana a semana, ambos datos por cada 100 mil habitantes.
Un gráfico por cada país que muestre las mismas variables que el punto anterior, pero acumuladas en el tiempo.
Además deberás entregar un video de máximo 3 minutos explicando tus conclusiones. Para grabarte te recomendamos la plataforma Loom.
En la misma plataforma de aprendizaje deberás responder tres preguntas para validar que tus cálculos están correctos. Estas son:
1. Considerando cifras por cada 100,000 habitantes, ¿en qué país se observa mayor diferencia entre las muertes oficiales COVID reportadas y el estimado de exceso de muertes?
2. Considerando cifras por cada 100,000 habitantes, ¿en qué país se observa menor diferencia entre las muertes oficiales COVID reportadas y el estimado de exceso de muertes?
3. Considerando cifras por cada 100,000 habitantes, ¿cuál es el país con el mayor número de exceso de muertes?
&gt; 👩‍💻¿Te parece muy complejo? ¡No te preocupes! Hemos preparado una súper guía paso-a-paso para ayudarte a resolver el proyecto. Tenemos confianza que con tu esfuerzo, esta guía y el apoyo de tus compañeras por Slack podrás resolver el proyecto y aprender en el proceso. 
Objetivos de aprendizaje
Al resolver este proyecto serás capaz de:
Organizar datos en hojas de cálculo: conoces los distintos tipos de datos que acepta una celda y eres capaz de dar formato a monedas, fechas, números para mostrar de mejor forma la información. Adicionalmente ocupas filtros para organizar los datos y puedes ordenar columnas de mayor a menor (o viceversa) según su tipo de dato.
Manipular datos en hojas de cálculo: utilizas tablas dinámicas para calcular, resumir y analizar datos con el fin de ver comparaciones, patrones y tendencias en ellos. Además, logras conectar dos o más fuentes de datos utilizando la función BUSCAV (VLOOKUP).
Visualizar datos en hojas de cálculo: confeccionas gráficas de línea y barra para visualizar información con el fin de resumir hallazgos, encontrar patrones o comparar distintas series de datos.
Organizar y comunicar hallazgos: estructuras tu análisis en un reporte ordenado y claro. Comunicas efectivamente las conclusiones a las que llegaste en tu proyecto.
Consideraciones generales
Este proyecto debe ser realizado de forma individual. Sin embargo, como uno de los principios de este programa es la colaboración, te puedes apoyar en tus compañeras a través de la comunidad de Slack.
IMPORTANTE: el objetivo de este proyecto es que desarrolles las habilidades descritas en los objetivos de aprendizaje. Bajo ninguna circunstancia busca ser una análisis profundo y conclusivo del impacto de la pandemia en América Latina. Ni tampoco buscamos que los cálculos aquí realizados nos lleven a tomar conclusiones apresuradas. Solo buscamos que aprendas y te diviertas. Al mismo tiempo, a pesar de que este proyecto nos recuerda de las pérdidas de personas queridas que han vivido todas y cada una de las personas en el mundo a raíz de la pandemia, reconocemos que este tipo de análisis es el que utilizan líderes mundiales para afrontar la crisis sanitaria y, por ejemplo, definir qué países requieren más asistencia en términos de donaciones de vacunas, oxígeno u otros recursos. También, es un análisis que se hace desde la ciudadanía para exigir una mejor gestión y transparencia a sus gobernantes. Aquí compartimos un par de ejemplos de este análisis de la vida real (artículos en inglés): 
The Economist: Tracking covid-19 excess deaths across countries
Financial Times: Coronavirus tracker: the latest figures as countries fight the Covid-19 resurgence
¡Te deseamos mucho éxito!"	6	57	0	datacertlaboratoria	proyecto-1-exceso-de-muertes-por-covid19
5912	5912	vgg5weight		[]		0	16	0	ironluu	vgg5weight
5913	5913	winlinmac		[]		0	2	0	fernandobordi	winlinmac
5914	5914	dataset_speech		[]		0	17	0	nahlatarek	dataset-speech
5915	5915	balaji_2	personal_models for college project	['universities and colleges']		1	30	0	dhakshiin1601	balaji-2
5916	5916	Tsla Stock Data		[]		2	19	0	mahwiz	tsla-stock-data
5917	5917	balaji_1	personal_models for college project	['universities and colleges']		1	6	0	dhakshiin1601	balaji-1
5918	5918	lstm_12012021		[]		0	24	0	skibas	lstm-12012021
5919	5919	cots_yolox_epoch_40		[]		0	22	0	jiangjieke	cots-yolox-epoch-40
5920	5920	eca-nfnet-f30		[]		0	6	0	honihitak	eca-nfnet-f30
5921	5921	Cornell Movie-Dialog Corpus		['movies and tv shows']		1	80	4	sahrishaltafkhan	cornell-moviedialog-corpus
5922	5922	SGCC_matlab		[]		0	6	0	wizardwangjiang	sgcc-matlab
5923	5923	poornesh_full_dataset	custom_models used for personal data	[]		3	6	0	dhakshiin1601	poornesh-paws-full
5924	5924	HH.ru IT vacancies (IT вакансии Москва + Питер)	47330 unique IT vacancies from HH.ru (from 2021-10-25 to 2021-12-02)	['russia', 'employment', 'finance', 'tabular data', 'text data']	"Context
This data is parsed from hh.ru for Moscow and Saint Petersburg (from 2021-10-25 to 2021-12-02)
Content
The dataset consists of 15 columns and contains the following information: 
name of the company offering the job 
position name 
the fact of the presence of a salary in a vacancy 
minimum salary rate 
maximum salary rate 
experience 
schedule 
key skills 
detailed job description 
city 
professional job title 
specialization 
professional area 
date of publication of the vacancy 
Inspiration
What are the most wanted skills? specializations?
What is the average salary in Moscow and St. Petersburg? Average salary in popular specialties?
..."	23	251	6	vyacheslavpanteleev1	hhru-it-vacancies-from-20211025-to-20211202
5925	5925	BNATURE-Bengali Image Captioning Dataset	This is a dataset for Bengali caption generation from Image.	['lstm', 'cnn', 'image data', 'text data']		16	94	2	almominfaruk	bnaturebengali-image-captioning-dataset
5926	5926	Online store customer data	USA online store transcation. 	['exploratory data analysis', 'data visualization', 'data analytics', 'tabular data', 'retail and shopping']	"Context
This is a dummy dataset about USA online store transaction data. 
Content
There are 11 features. 
1. Transaction_date - Transaction date 
2.  Transaction_ID - This is a unique transaction id 
3. Gender - Customer Gender
4.  Age - Customer Age
5. Marital_status - Marital status about customer
6. State_names - Customer location of State.
7. Segment - Customer membership
8. Employees_status - Customer employment status
9. Payment_method - Payment method used by customer
10. Referal - Customer coming  from referral link or not
11. Amount_spent - Amount spent by customer per transaction
Acknowledgements
I am generating this dummy USA online store customer dataset with help of the Faker and Numpy python package. I would like to mention this article  - https://towardsdatascience.com/generating-fake-data-with-python-c7a32c631b2a. It helped me a lot."	178	1154	3	mountboy	online-store-customer-data
5927	5927	2021-22_EPL_PlayerStats	Premier league 2021-2022 player stats (till Matchday 20)	['football', 'europe', 'clustering', 'tabular data', 'sklearn']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
The dataset consists of advanced player metrics from the 2021-2022 English Premier League season. I'll perform PCA and clustering on the dataset to see what analysis I can arrive at.
Acknowledgements
Data is extracted from fbref.com
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	12	54	0	shashank069	202122-epl-playerstats
5928	5928	vgg5weight		[]		10	16	0	llixxin	vgg5weight
5929	5929	SOC anomaly index		[]		1	15	0	xiangdongyuan	soc-anomaly-index
5930	5930	Missing Migrants Dataset		['government', 'demographics', 'intermediate']		3	4	0	saomyasrishtihoro	missing-migrants-dataset
5931	5931	integration_test		[]		0	42	0	filippoangelini	integration-test
5932	5932	mask_rcnn_coco		[]		0	5	0	ansuld	mask-rcnn-coco
5933	5933	PetFinder - pretrain models 2		[]		0	4	0	nanguyen	petfinder-pretrain-models-2
5934	5934	ab_test_utils		[]		0	17	0	josebaxter	ab-test-utils
5935	5935	timm-pytorch-image-models-v0.5.2		['arts and entertainment']	License and other information: https://github.com/rwightman/pytorch-image-models	0	14	1	kami2suukyi	timm-pytorch-image-models
5936	5936	tempfile		['computer science', 'programming']		7	13	1	bbb6045	tempfile
5937	5937	Ceramic Tiles Defects (crack,spots,pinhole)	tiles surface defects	['artificial intelligence', 'computer vision', 'svm', 'image data', 'cv2']		4	102	3	riffatsiddiqui	ceramic-tiles-defects-crackspotspinhole
5938	5938	ucolor		[]		0	3	0	kaggelliumin	ucolor
5939	5939	Customer Segmentation (51k Records)	This is a Dummy Customer Segmentation Dataset consists of 51000 records.	['beginner', 'intermediate', 'exploratory data analysis', 'data analytics', 'tabular data']	"Context
Customer data is defined as the information your customers provide while interacting with your business via your website, mobile applications, surveys, social media, marketing campaigns, and other online and offline avenues. Customer data is a cornerstone to a successful business strategy. Data-driven organizations realize the importance of this and take action to ensure that they collect the necessary customer data points that would enable them to improve customer experience and fine-tune business strategy over time.
Content
This <b> Customer Segmentation </b> Dataset is a Dummy Dataset, which is generated mainly to practice various types of Data Science/Machine Learning activities. The Dataset is specially targeted towards beginners and newbies for getting their hands dirty with data. Though, even experienced professionals will also enjoy this Dataset. The Dataset consists of 51000 Customer records. <b> The Dataset contains information like: </b>
<b> First Name, Last Name, Title, Gender, E-mail, City, Country, Country Code, Latitude, Longitude, Phone, Street Address, Street Name, Street Number, Street Suffix, Time Zone, Company Name, Department, Job Title, Language, University, Linkedin Skill, IP Address. </b>
Acknowledgements
For more, please visit - https://www.mockaroo.com/"	220	2301	29	iamsouravbanerjee	customer-segmentation-51k-records
5940	5940	New Dataset Happy Faces	New Dataset Happy of only happy faces	[]		0	11	0	shakirbugti	new-dataset-happy-and-sad
5941	5941	earnings_surprises AY21/22 Sem2		[]		0	10	0	yuyan98	earnings-surprises-ay2122-sem2
5942	5942	Bangladesh Cricket Team Yearly Performance  	Yearly match results summary based on different types of grounds (Test,ODI,T-20)	['cricket']	"Context
Cricket is a very popular game in Bangladesh. As a fan of Bangladeshi cricket, i wanna have insights team performance based on  different types of venues.
Content
There are three datasets for test, ODI(One Day International) and T-20. The date started from the year Bangladesh started to play cricket,1986. All the data included here are of official matches. Each dataset contains yearly wins, loses and three types of venues, home ground, away ground and neutral ground total matches and wins,
.
Inspiration
To predict Bangladesh winning based on the ground they are playing."	17	157	7	azizulhakim98	bangladesh-cricket-team-yearly-performance
5943	5943	monthly AY21/22 Sem2		[]		1	7	0	yuyan98	monthly-ay2122-sem2
5944	5944	ig_res101_384_in22k_classification_external_0_1		[]		0	1	0	xyzdivergence	ig-res101-384-in22k-classification-external-0-1
5945	5945	w1_data AY21/22 Sem2		[]		1	11	0	yuyan98	w1-data-ay2122-sem2
5946	5946	ConvNeXt_large		[]		0	5	0	greepex	convnext-large
5947	5947	H andS		[]		0	8	0	shakirbugti	h-ands
5948	5948	Pretrained_weights		[]		0	25	0	xuhuizhan	pretrained-weights
5949	5949	pytorchcrf		[]		0	12	0	guanyuhang86	pytorchcrf
5950	5950	Dataset2.0 pixelart		[]		1	27	0	sarthak4u	dataset20-pixelart
5951	5951	swin fastai		[]		0	7	0	nilavanakilan	swin-fastai
5952	5952	Panda Image Dataset	Panda Panda Panda Panda	['animals', 'image data']	"Data Source
Chengdu Research Base of Gaint Panda Breeding
PNG File Name
0_: newborn panda body
1_: superstar
2_: overseas
3_: growth dairy
4_: sleepy
5_: mother and child
6_: cute
7_: play"	24	589	5	holoong9291	pandaimagedataset
5953	5953	fu_data		[]		0	12	0	leon1621	fu-data
5954	5954	Exploratory analysis of FIFA game datasets	Analysis of game edition data from 2015 to 2022	['games']		2	41	0	xaviersamper	players-data
5955	5955	H andSad	Happy and sad faces dataset	['online communities']		0	13	0	shakirbugti	h-andsad
5956	5956	train_data		[]		0	7	0	jonnyyang	train-data
5957	5957	effdet		[]		5	24	0	mrinath	effdet
5958	5958	catvsdog		[]		1	5	0	xinshiwang026	catvsdog
5959	5959	'21-'22 EPL Striker Pre-Contract Profiles		['football', 'europe', 'sports']		0	16	0	zclacken	2122-epl-striker-precontract-profiles
5960	5960	fluate_data		[]		2	31	0	bbb6045	fluate-data
5961	5961	Oswald TTF File for YOLOv5 Simplified: Infer		[]	Well, instead of Arial in ttf file, why don't we use Oswald? Just like us, in YOLOv5 Simplified: Infer!	0	29	0	dinowun	oswald-ttf-file-for-yolov5-simplified-infer
5962	5962	auto_2022		[]		0	10	0	stepankutkin	auto-2022
5963	5963	loan_data		[]		0	2	0	nadhirartrissani	loan-data
5964	5964	Human Detection data		['business']		6	21	0	aakashveera	human-detection-data
5965	5965	CSVExpw		[]		0	8	0	mohammedaaltaha	csvexpw
5966	5966	VOX_zh		[]		0	3	0	marinamaher	vox-zh
5967	5967	sentence-transformer		[]		1	12	0	sahib12	sentencetransformer
5968	5968	Swin-Transformer Large ImageNet-22K	Swin Transformer Pretrained Models	['transfer learning', 'transformers']	"Swin Transformer pretrained models using ImageNet-1K and ImageNet-22K datasets.
Reference: https://github.com/microsoft/Swin-Transformer"	1	30	0	cbentes	swintransformer-large-imagenet22k
5969	5969	Swin-Transformer Base ImageNet-22K	Swin Transformer Pretrained Models	['transfer learning', 'transformers']	"Swin Transformer pretrained models using ImageNet-1K and ImageNet-22K datasets.
Reference: https://github.com/microsoft/Swin-Transformer"	0	14	0	cbentes	swintransformer-base-imagenet22k
5970	5970	Extra Datasets for TPS Jan 2022| GDP-CPI-Holidays	Easy-to-use datasets for TPS train & test datasets	['europe', 'economics', 'data cleaning', 'news', 'datetime']	"I edited several datasets in Kaggle and make them more user-friendly.
Current Datasets are very convenient to add them to main dataset by single pandas method | merge(on=['date'|'year', 'country'])
Example notebook: https://www.kaggle.com/sardorabdirayimov/adding-extra-datasets-to-main
Good luck on competition!"	35	239	8	sardorabdirayimov	extra-datasets-for-tps-jan-2022-gdpcpiholidays
5971	5971	kfold_h3_4spiral		[]		3	22	0	rasterbunny	kfold-h3-4spiral
5972	5972	Consumer Price Index 2015-2019 in Nordic Countries	Consumer Price index for TPS Jan 2022	['banking', 'beginner', 'intermediate', 'advanced']	"Context
Consumer Prices Index In Finland, Sweden and Norway
Current csv file is provided by World Bank 
I hope that Consumer Index helps Kagglers to make their models more accurate.
Content
Current Dataset includes consumer prices between 2015 and 2019"	64	394	7	sardorabdirayimov	consumer-price-index-20152019-nordic-countries
5973	5973	299-inchi-split		[]		0	11	1	narminj	299inchisplit
5974	5974	Antispoofing2		[]		2	9	0	huyngqc	antispoofing2
5975	5975	Beer Scores	Beer scores prediction	['alcohol']		1	45	1	ssiddharth408	beer-scores
5976	5976	Student 		['universities and colleges']		5	43	3	meghagoriya	student
5977	5977	CNModelsLarge		[]		0	11	0	kashiwaba	cnmodelslarge
5978	5978	petf_convnext_l_224_bs32_cutmix10_mfixed		[]		0	4	0	nhac43	petf-convnext-l-224-bs32-cutmix10-mfixed
5979	5979	Pikachu Classification Dataset	Binary image classification dataset with pikachu	['games', 'anime and manga']	"Context
This is a very simple binary image classification dataset with pikachu. The dataset consist of test, train and validation dataset with images of pikachu and non pikachu.
The images was web scraping from the internet using Azure Bing Web Search API."	14	84	1	hal0samuel	pikachu-classification-dataset
5980	5980	NormalData		[]		0	8	0	chdaxx	normaldata
5981	5981	ft_no_leaky_sub_score_utr_7_128		[]		0	11	0	hangy132	ft-no-leaky-sub-score-utr-7-128
5982	5982	ridge_j_noclean_fasttext_multiseeds_oof		[]		0	42	0	shobhitupadhyaya	ridge-j-noclean-fasttext-multiseeds-oof
5983	5983	PetFinder-resize		['animals']		0	6	0	min7712	petfinderresize
5984	5984	Tortugas_20220112_V2		[]		1	7	0	miguelespinozac	tortugas-20220112-v2
5985	5985	PetFinder-ZX201-20220113040633		[]		0	9	0	hideyukizushi	petfinder-zx201-20220113040633
5986	5986	crwu_12k		[]		0	2	0	chdaxx	crwu-12k
5987	5987	A-Data		[]		0	17	0	kevinhys	adata
5988	5988	cricket match		['cricket']		2	10	3	muhammadammarjamshed	cricket-match
5989	5989	Anti_Spoofing-3		[]		2	19	0	tinguynch	anti-spoofing3
5990	5990	climate change indicators for Pakistan		['atmospheric science']		0	8	0	masiddiqui66	climate-change-indicators-for-pakistan
5991	5991	ImagePDF		[]		1	14	0	ignitedqneurons	imagepdf
5992	5992	Covid Vaccinations in United States🇺🇸💉	State-by-state data on United States COVID-19 vaccinations	['united states', 'beginner', 'intermediate', 'tabular data', 'covid19']	"Context
This dataset provides State-by-state data on United States COVID-19 vaccinations between 20 December of 2020 and 12 January of 2022. Data is taken daily by the United States Centers for Disease Control and Prevention
Columns
us_vaccinations File
- location: State name.
- date: date of the case.
- total_vaccinations: total number of doses administered. This is counted as a single dose, and may not equal the total number of people vaccinated, depending on the specific dose regime (e.g. people receive multiple doses). If a person receives one dose of the vaccine, this metric goes up by 1. If they receive a second dose, it goes up by 1 again.
- total_vaccinations_per_hundred: total_vaccinations per 100 people in the total population of the state.
- daily_vaccinations_raw: daily change in the total number of doses administered. It is only calculated for consecutive days. This is a raw measure provided for data checks and transparency, but we strongly recommend that any analysis on daily vaccination rates be conducted using daily_vaccinations instead.
- daily_vaccinations: new doses administered per day (7-day smoothed). For countries that don't report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found here.
- daily_vaccinations_per_million: daily_vaccinations per 1,000,000 people in the total population of the state.
- people_vaccinated: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.
- people_vaccinated_per_hundred: people_vaccinated per 100 people in the total population of the state.
- people_fully_vaccinated: total number of people who received all doses prescribed by the vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.
- people_fully_vaccinated_per_hundred: people_fully_vaccinated per 100 people in the total population of the state.
- total_distributed: cumulative counts of COVID-19 vaccine doses recorded as shipped in CDC's Vaccine Tracking System.
- total_distributed_per_hundred: cumulative counts of COVID-19 vaccine doses recorded as shipped in CDC's Vaccine Tracking System per 100 people in the total population of the state.
- share_doses_used: share of vaccination doses administered among those recorded as shipped in CDC's Vaccine Tracking System.
Data as of: May 18, 2021"	572	3206	34	berkayalan	vaccinations-in-united-states
5993	5993	qiyu_dataset		[]		0	3	0	ansuld	qiyu-dataset
5994	5994	cfdung5		[]		0	2	0	baanhle	cfdung5
5995	5995	abstract		['art']		0	4	0	dwiprastanto	abstract
5996	5996	ConvNet		[]		0	21	0	rainfalllove	convnet
5997	5997	Covid-19 Daily Vaccinations-Chicago Residents	Residents who received 1st dose only, plus those who received the 2nd dose	['public health', 'public safety']		2	18	0	venishal	chicago-residents-daily-vaccinations
5998	5998	Nic Cage Movies	Info about the movies of America's true National Treasure, Nicolas Cage.	['arts and entertainment', 'movies and tv shows']	"nic-cage.csv is a dataset I have curated containing information about Nic Cage movies. It has a list of all released Nic Cage acting roles, gathered from IMDb. I have included the movie, movie rating, year of release, character name, whether the role was live action or voice acting, and the Rotten Tomatoes score. Data was originally gathered in January, 2019 and was updated on August 7th, 2019. A new update has been made as of January 12th, 2022 to include films made through 2021. Rotten Tomatoes scores for all films were also updated in this version, if they had changed over time.
Movie, rating, and character are all strings with movie title, movie rating, and character name, respectively. Voice is binary: 0 indicates live action and 1 indicates a voice acting role. Year is numerical. Rotten Tomatoes Score is a combination of numerical and character data. If a score was not available, that space was filled with ""X""."	185	2425	8	eharlett	nic-cage-movies
5999	5999	7SegmentDisplay		[]		0	7	0	innamuzychenko	7segmentdisplay
6000	6000	ega_petfinder_fasiai		[]		0	6	0	kazuki1450	ega-petfinder-fasiai
6001	6001	DenseNet169_ICPR		[]		0	4	0	bachaboos	densenet169-icpr
6002	6002	202111_custom_models		[]		0	90	0	yshitara	202111-custom-models
6003	6003	convNeXt Models & Code		[]		67	331	7	seshurajup	convnext-models-code
6004	6004	Dataset		[]		0	526	1	bultyhbabah	dataset
6005	6005	VOX_AR		[]		1	13	0	marinamaher	vox-ar
6006	6006	Metacritics Best Video Games of All Time 2022	Dataframe that contains the list described on title, updated in Jan/2022	['video games']	"Metacritic's Best Video Games of All Time
This project was developed with the mission of creating a dataframe with the updated list from the beste videogames of all time by Metacritic website.
Reference
To collect the data, I created a python script that uses selenium and pandas. You can access it on my github 👇
- Github
The data was collected from the Metacritic website.
 - Metacritic
Author
@caiques121"	138	1111	17	caiquerezende	metacritics-best-video-games-of-all-time-2021
6007	6007	NewsMTSC: Sentiment in English News Articles	Target-dependent sentiment classification in English news articles	['news']	"NewsMTSC dataset
NewsMTSC is a high-quality dataset consisting of more than 11k manually labeled sentences sampled from English news articles. Each sentence was labeled by five human coders (the dataset contains only examples where the five coders assessed same or similar sentiment). The dataset is published as a full paper at EACL 2021: NewsMTSC: (Multi-)Target-dependent Sentiment Classification in News Articles.
Splits
The dataset consists of three splits (train, validation, and test). More information on the splits (and further subsets) can be found in our paper. Note that in contrast to the dataset published in our repository, we applied the following changes to the dataset as published on kaggle. First, to allow more convenient use of the dataset, we expanded the examples, e.g., each example having k targets was converted into k examples having 1 target. Second, we uploaded the rw subset on kaggle since it comes closest to the real-world distribution of sentiment.
Format
Each split is stored in a JSONL file. In JSONL, each line represents one JSON object. In our dataset, each JSON object consists of the following attributes. When using the dataset, you most likely will need (only) the attributes highlighted in bold.
1. mention: text of the mention within sentence
2. polarity: sentiment of the sentence concerning the target's mention (-1 = negative, 0 = neutral, 1 = positive)
3. from: character-based, 0-indexed position of the first character of the target's mention within sentence
4. to: last character of the target's mention
5. sentence: sentence
6. id: identifier that is unique within NewsMTSC
Notebook
Check out this example notebook on how to load NewsMTSC. (I don't know how to provide an exemplary notebook on kaggle, so decided to put a link instead)
Contact
If you find an issue with the dataset or our sentiment classification model or have a question concerning either, please open an issue in the repository.
* Repository: https://github.com/fhamborg/NewsMTSC
* Web: https://felix.hamborg.eu/
How to cite
If you use the dataset or parts of it, please cite our paper:
@InProceedings{Hamborg2021b,
  author    = {Hamborg, Felix and Donnay, Karsten},
  title     = {NewsMTSC: (Multi-)Target-dependent Sentiment Classification in News Articles},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2021)},
  year      = {2021},
  month     = {Apr.},
  location  = {Virtual Event},
}"	3	64	0	fhamborg	news-articles-sentiment
6008	6008	roupas_com_defeitos	Roupas com defeitos como rasgos, manchas, ...	[]		0	32	0	leandroipca	roupas-com-defeitos
6009	6009	Titanic		[]		4	11	0	hamzauras	titanic
6010	6010	Glove 300D Embeds SITARAM		[]		0	6	0	adityasrivastava7	glove-300d-embeds-sitaram
6011	6011	World Energy Consumption	Consumption of energy by different countries	['demographics', 'energy', 'data cleaning', 'data visualization', 'data analytics', 'electricity']	"Data on Energy by Our World in Data
This dataset is a collection of key metrics maintained by Our World in Data. It is updated regularly and includes data on energy consumption (primary energy, per capita, and growth rates), energy mix, electricity mix and other relevant metrics.
🗂️ Download the complete Energy dataset : CSV | XLSX | JSON
Data sources
Energy consumption (primary energy, energy mix and energy intensity): this data is sourced from a combination of two sources—the BP Statistical Review of World Energy and SHIFT Data Portal.
Electricity consumption (electricity consumption, and electricity mix): this data is sourced from a combination of two sources—the BP Statistical Review of World Energy and EMBER – Global Electricity Dashboard.
Other variables: this data is collected from a variety of sources (United Nations, World Bank, Gapminder, Maddison Project Database, etc.). More information is available in our codebook.
The complete Our World in Data Energy dataset
Our complete Energy dataset is available in CSV, XLSX, and JSON formats.
The CSV and XLSX files follow a format of 1 row per location and year. The JSON version is split by country, with an array of yearly records.
The variables represent all of our main data related to energy consumption, energy mix, electricity mix as well as other variables of potential interest.
Changelog
On March 31, 2021, we updated 2020 electricity mix data.
On September 9, 2020, the first version of this dataset was made available.
Data alterations
standardize names of countries and regions. Since the names of countries and regions are different in different data sources, standardize all names to the Our World in Data standard entity names.
recalculate primary energy in terawatt-hours. The primary data sources on energy—the BP Statistical Review of World Energy, for example—typically report consumption in terms of exajoules. recalculated these figures as terawatt-hours using a conversion factor of 277.8.
calculate per capita figures. All of our per capita figures are calculated from our metric Population, which is included in the complete dataset. These population figures are sourced from Gapminder and the UN World Population Prospects (UNWPP).
License
All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.
The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our database, and you should always check the license of any such third-party data before use.
Authors
This data has been collected, aggregated, and documented by Hannah Ritchie, Max Roser and Edouard Mathieu.
The mission of Our World in Data is to make data and research on the world’s largest problems understandable and accessible. Read more about our mission.
Columns and their description
| column                                  | description                                                                                                                                             |
|-----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| iso_code                                | ISO 3166-1 alpha-3 three-letter country codes                                                                                                           |
| country                                 | Geographic location                                                                                                                                     |
| year                                    | Year of observation                                                                                                                                     |
| coal_prod_change_pct                    | Annual percentage change in coal production                                                                                                             |
| coal_prod_change_twh                    | Annual change in coal production, measured in terawatt-hours                                                                                            |
| gas_prod_change_pct                     | Annual percentage change in gas production                                                                                                              |
| gas_prod_change_twh                     | Annual change in gas production, measured in terawatt-hours                                                                                             |
| oil_prod_change_pct                     | Annual percentage change in oil production                                                                                                              |
| oil_prod_change_twh                     | Annual change in oil production, measured in terawatt-hours                                                                                             |
| energy_cons_change_pct                  | Annual percentage change in primary energy consumption                                                                                                  |
| energy_cons_change_twh                  | Annual change in primary energy consumption, measured in terawatt-hours                                                                                 |
| biofuel_share_elec                      | Share of electricity consumption that comes from biofuels                                                                                               |
| biofuel_cons_change_pct                 | Annual percentage change in biofuel consumption                                                                                                         |
| biofuel_share_energy                    | Share of primary energy consumption that comes from biofuels                                                                                            |
| biofuel_cons_change_twh                 | Annual change in biofuel consumption, measured in terawatt-hours                                                                                        |
| biofuel_consumption                     | Primary energy consumption from biofuels, measured in terawatt-hours                                                                                    |
| biofuel_elec_per_capita                 | Per capita electricity consumption from biofuels, measured in kilowatt-hours                                                                            |
| biofuel_cons_per_capita                 | Per capita primary energy consumption from biofuels, measured in kilowatt-hours                                                                         |
| carbon_intensity_elec                   | Carbon intensity of electricity production, measured in grams of carbon dioxide emitted per kilowatt-hour                                               |
| coal_share_elec                         | Share of electricity consumption that comes from coal                                                                                                   |
| coal_cons_change_pct                    | Annual percentage change in coal consumption                                                                                                            |
| coal_share_energy                       | Share of primary energy consumption that comes from coal                                                                                                |
| coal_cons_change_twh                    | Annual change in coal consumption, measured in terawatt-hours                                                                                           |
| coal_consumption                        | Primary energy consumption from coal, measured in terawatt-hours                                                                                        |
| coal_elec_per_capita                    | Per capita electricity consumption from coal, measured in kilowatt-hours                                                                                |
| coal_cons_per_capita                    | Per capita primary energy consumption from coal, measured in kilowatt-hours                                                                             |
| coal_production                         | Coal production, measured in terawatt-hours                                                                                                             |
| coal_prod_per_capita                    | Per capita coal production, measured in kilowatt-hours                                                                                                  |
| electricity_generation                  | Electricity generation, measured in terawatt-hours                                                                                                      |
| biofuel_electricity                     | Electricity generation from biofuels, measured in terawatt-hours                                                                                        |
| coal_electricity                        | Electricity generation from coal, measured in terawatt-hours                                                                                            |
| fossil_electricity                      | Electricity generation from fossil fuels, measured in terawatt-hours. This is the sum of electricity generation from coal, oil and gas.                 |
| gas_electricity                         | Electricity generation from gas, measured in terawatt-hours                                                                                             |
| hydro_electricity                       | Electricity generation from hydropower, measured in terawatt-hours                                                                                      |
| nuclear_electricity                     | Electricity generation from nuclear power, measured in terawatt-hours                                                                                   |
| oil_electricity                         | Electricity generation from oil, measured in terawatt-hours                                                                                             |
| other_renewable_electricity             | Electricity generation from other renewable sources, measured in terawatt-hours                                                                         |
| other_renewable_exc_biofuel_electricity | Electricity generation from other renewable sources excluding biofuels, measured in terawatt-hours                                                      |
| renewables_electricity                  | Electricity generation from renewables, measured in terawatt-hours                                                                                      |
| solar_electricity                       | Electricity generation from solar, measured in terawatt-hours                                                                                           |
| wind_electricity                        | Electricity generation from wind, measured in terawatt-hours                                                                                            |
| energy_per_gdp                          | Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.                                                        |
| energy_per_capita                       | Primary energy consumption per capita, measured in kilowatt-hours per year                                                                              |
| fossil_cons_change_pct                  | Annual percentage change in fossil fuel consumption                                                                                                     |
| fossil_share_energy                     | Share of primary energy consumption that comes from fossil fuels                                                                                        |
| fossil_cons_change_twh                  | Annual change in fossil fuel consumption, measured in terawatt-hours                                                                                    |
| fossil_fuel_consumption                 | Fossil fuel consumption, measured in terawatt-hours. This is the sum of primary energy from coal, oil and gas.                                          |
| fossil_energy_per_capita                | Per capita fossil fuel consumption, measured in kilowatt-hours. This is the sum of primary energy from coal, oil and gas.                               |
| fossil_cons_per_capita                  | Per capita fossil fuel consumption, measured in kilowatt-hours. This is the sum of primary energy from coal, oil and gas.                               |
| fossil_share_elec                       | Share of electricity consumption that comes from fossil fuels (coal, oil and gas combined)                                                              |
| gas_share_elec                          | Share of electricity consumption that comes from gas                                                                                                    |
| gas_cons_change_pct                     | Annual percentage change in gas consumption                                                                                                             |
| gas_share_energy                        | Share of primary energy consumption that comes from gas                                                                                                 |
| gas_cons_change_twh                     | Annual change in gas consumption, measured in terawatt-hours                                                                                            |
| gas_consumption                         | Primary energy consumption from gas, measured in terawatt-hours                                                                                         |
| gas_elec_per_capita                     | Per capita electricity consumption from gas, measured in kilowatt-hours                                                                                 |
| gas_energy_per_capita                   | Per capita primary energy consumption from gas, measured in kilowatt-hours                                                                              |
| gas_production                          | Gas production, measured in terawatt-hours                                                                                                              |
| gas_prod_per_capita                     | Per capita gas production, measured in kilowatt-hours                                                                                                   |
| hydro_share_elec                        | Share of electricity consumption that comes from hydropower                                                                                             |
| hydro_cons_change_pct                   | Annual percentage change in hydropower consumption                                                                                                      |
| hydro_share_energy                      | Share of primary energy consumption that comes from hydropower                                                                                          |
| hydro_cons_change_twh                   | Annual change in hydropower consumption, measured in terawatt-hours                                                                                     |
| hydro_consumption                       | Primary energy consumption from hydropower, measured in terawatt-hours                                                                                  |
| hydro_elec_per_capita                   | Per capita electricity consumption from hydropower, measured in kilowatt-hours                                                                          |
| hydro_energy_per_capita                 | Per capita primary energy consumption from hydropower, measured in kilowatt-hours                                                                       |
| low_carbon_share_elec                   | Share of electricity consumption that comes from low-carbon sources. This is the sum of electricity from renewables and nuclear                         |
| low_carbon_electricity                  | Electricity generation from low-carbon sources, measured in terawatt-hours. This is the sum of electricity generation from renewables and nuclear power |
| low_carbon_elec_per_capita              | Per capita electricity consumption from low-carbon sources, measured in kilowatt-hours                                                                  |
| low_carbon_cons_change_pct              | Annual percentage change in low-carbon energy consumption                                                                                               |
| low_carbon_share_energy                 | Share of primary energy consumption that comes from low-carbon sources. This is the sum of primary energy from renewables and nuclear                   |
| low_carbon_cons_change_twh              | Annual change in low-carbon energy consumption, measured in terawatt-hours                                                                              |
| low_carbon_consumption                  | Primary energy consumption from low-carbon sources, measured in terawatt-hours                                                                          |
| low_carbon_energy_per_capita            | Per capita primary energy consumption from low-carbon sources, measured in kilowatt-hours                                                               |
| nuclear_share_elec                      | Share of electricity consumption that comes from nuclear power                                                                                          |
| nuclear_cons_change_pct                 | Annual percentage change in nuclear consumption                                                                                                         |
| nuclear_share_energy                    | Share of primary energy consumption that comes from nuclear power                                                                                       |
| nuclear_cons_change_twh                 | Annual change in nuclear consumption, measured in terawatt-hours                                                                                        |
| nuclear_consumption                     | Primary energy consumption from nuclear power, measured in terawatt-hours                                                                               |
| nuclear_elec_per_capita                 | Per capita electricity consumption from nuclear power, measured in kilowatt-hours                                                                       |
| nuclear_energy_per_capita               | Per capita primary energy consumption from nuclear, measured in kilowatt-hours                                                                          |
| oil_share_elec                          | Share of electricity consumption that comes from oil                                                                                                    |
| oil_cons_change_pct                     | Annual percentage change in oil consumption                                                                                                             |
| oil_share_energy                        | Share of primary energy consumption that comes from oil                                                                                                 |
| oil_cons_change_twh                     | Annual change in oil consumption, measured in terawatt-hours                                                                                            |
| oil_consumption                         | Primary energy consumption from oil, measured in terawatt-hours                                                                                         |
| oil_elec_per_capita                     | Primary energy consumption from oil, measured in terawatt-hours                                                                                         |
| oil_energy_per_capita                   | Per capita primary energy consumption from oil, measured in kilowatt-hours                                                                              |
| oil_production                          | Oil production, measured in terawatt-hours                                                                                                              |
| oil_prod_per_capita                     | Per capita oil production, measured in kilowatt-hours                                                                                                   |
| other_renewables_elec_per_capita        | Per capita electricity consumption from other renewables, measured in kilowatt-hours                                                                    |
| other_renewables_share_elec             | Share of electricity consumption that comes from other renewables                                                                                       |
| other_renewables_cons_change_pct        | Annual percentage change in energy consumption from other renewables                                                                                    |
| other_renewables_share_energy           | Share of primary energy consumption that comes from other renewables                                                                                    |
| other_renewables_cons_change_twh        | Annual change in other renewable consumption, measured in terawatt-hours                                                                                |
| other_renewable_consumption             | Primary energy consumption from other renewables, measured in terawatt-hours                                                                            |
| other_renewables_energy_per_capita      | Per capita primary energy consumption from other renewables, measured in kilowatt-hours                                                                 |
| per_capita_electricity                  | Electricity consumption per capita, measured in kilowatt-hours                                                                                          |
| population                              | Total population                                                                                                                                        |
| primary_energy_consumption              | Primary energy consumption, measured in terawatt-hours                                                                                                  |
| renewables_elec_per_capita              | Per capita primary energy consumption from renewables, measured in kilowatt-hours                                                                       |
| renewables_share_elec                   | Share of electricity consumption that comes from renewables                                                                                             |
| renewables_cons_change_pct              | Annual percentage change in renewable energy consumption                                                                                                |
| renewables_share_energy                 | Share of primary energy consumption that comes from renewables                                                                                          |
| renewables_cons_change_twh              | Annual change in renewable energy consumption, measured in terawatt-hours                                                                               |
| renewables_consumption                  | Primary energy consumption from renewables, measured in terawatt-hours                                                                                  |
| renewables_energy_per_capita            | Per capita electricity consumption from renewables, measured in kilowatt-hours                                                                          |
| solar_share_elec                        | Share of electricity consumption that comes from solar                                                                                                  |
| solar_cons_change_pct                   | Annual percentage change in solar consumption                                                                                                           |
| solar_share_energy                      | Share of primary energy consumption that comes from solar                                                                                               |
| solar_cons_change_twh                   | Annual change in solar consumption, measured in terawatt-hours                                                                                          |
| solar_consumption                       | Primary energy consumption from solar, measured in terawatt-hours                                                                                       |
| solar_elec_per_capita                   | Per capita electricity consumption from solar, measured in kilowatt-hours                                                                               |
| solar_energy_per_capita                 | Per capita primary energy consumption from solar, measured in kilowatt-hours                                                                            |
| gdp                                     | Total real gross domestic product, inflation-adjusted                                                                                                   |
| wind_share_elec                         | Share of electricity consumption that comes from wind                                                                                                   |
| wind_cons_change_pct                    | Annual percentage change in wind consumption                                                                                                            |
| wind_share_energy                       | Share of primary energy consumption that comes from wind                                                                                                |
| wind_cons_change_twh                    | Annual change in wind consumption, measured in terawatt-hours                                                                                           |
| wind_consumption                        | Primary energy consumption from wind, measured in terawatt-hours                                                                                        |
| wind_elec_per_capita                    | Per capita electricity consumption from wind, measured in kilowatt-hours                                                                                |
| wind_energy_per_capita                  | Per capita primary energy consumption from wind, measured in kilowatt-hours                                                                             |"	885	4933	13	pralabhpoudel	world-energy-consumption
6012	6012	DenseNet121_ICPR		[]		0	4	0	tikoboss	densenet121-icpr
6013	6013	EFV2_ICPR		[]		0	3	0	bachaboos	efv2-icpr
6014	6014	DataAugmentation_Tortugas_01		[]		1	14	0	miguelespinozac	dataaugmentation-tortugas-01
6015	6015	petfinder-008		[]		0	4	0	kenkengoda	petfinder-008
6016	6016	Liver Transplant	Liver Transplants in US by gender	['medicine', 'tabular data']	"Context
Transplants in the U.S. by Recipient Gender Page 1 of 1
U.S. Transplants Performed : January 1, 1988 - December 31, 2021
For Organ = Liver, Format = Portrait
https://optn.transplant.hrsa.gov/data/view-data-reports/national-data/#
Content
Transplants in the U.S. by Recipient Gender
Acknowledgements
organdonor.gov
https://optn.transplant.hrsa.gov/data/view-data-reports/national-data/#
Inspiration
Transplants and organ donors."	29	384	17	mpwolke	cusersmarildownloadslivercsv
6017	6017	test.csv		[]		0	3	0	gehadmagdy	testcsv
6018	6018	siwn30exp		[]		0	28	0	gyanendradas	swin30exp
6019	6019	CyclisticTripData		[]		0	13	0	sjohnson8675309	cyclistictripdata
6020	6020	mobile_com_hwdata		[]		0	3	0	alarayildiz	mobile-com-hwdata
6021	6021	inbreast2		[]		0	8	0	michaeljeremy	inbreast2
6022	6022	models_3		[]		1	33	0	awaptk	models-3
6023	6023	Dataset de Favelas em São Paulo	Dataset + Shapefile de favelas na cidade de São Paulo.	['cities and urban areas', 'brazil', 'demographics', 'beginner', 'geospatial analysis']	"Hi guys!
Folks, this dataset is about slums(Favelas) in São Paulo City. We have columns about area, coordinates(X,Y), Favela's name, Street/Avenue, 
 name on government agency and jurisdiction of area ('Municipal' is jurisdiction of city). Guys i've extract this data from 'Dados Abertos SP', and this data was updated in 2015, in ""Dados Abertos SP"" they have got this favelas area from ""Sabesp"" (The Basic Sanitation Company of the State of São Paulo) and SEHAB (Municipal Housing Secretariat). If you notice in data explorer, have  two archives, a CSV file and a ZIP file. In ZIP file you can plot favelas areas in some geographic software like Google Earth or Arcgis. 
(Em português):
 Rapaziada, só uma observação: o arquivo em ZIP contém layers (camadas), que podem ser plotadas em algum software de geoprocessamento como o Google Earth ou Arcgis. Sendo assim da para visualizar a área da favela com essas layers. Outra observação, só é possível plotar esse dataset com os poligonos (áreas demarcadas) no excel, se você tiver o complemento do Arcgis para Office. 
I will show you a example:
The next example will show Favela's area, with Ctrl+[+]:
Guys any thing call me up on Email: marcus.rodrigues4003@gmail.com
Rapaziada, vou gravar um video em breve explicando como eu peguei esses dados, como fazer para plotar no Google Earth e como manipular dados geográficos no python.
Fonte:http://dados.prefeitura.sp.gov.br/pt_PT/dataset/favelas"	12	216	2	markfinn1	dataset-de-favelas-em-so-paulo
6024	6024	jigsaw-toxic-inference-summary		[]		1	29	0	briannnlee	jigsawtoxicinferencesummary
6025	6025	Housing Prices Dataset	Housing Prices Prediction - Regression Problem	['real estate']	"Description:
A simple yet challenging project, to predict the housing price based on certain factors like house area, bedrooms, furnished, nearness to mainroad, etc. The dataset is small yet, it's complexity arises due to the fact that it has strong multicollinearity. Can you overcome these obstacles & build a decent predictive model?
Acknowledgement:
Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.
Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the sales w.r.t a single & multiple feature.
Also evaluate the models & compare thier respective scores like R2, RMSE, etc."	875	5202	26	yasserh	housing-prices-dataset
6026	6026	datasetsaj		[]		0	5	0	ashishjha040994	datasetsaj
6027	6027	COTS_tfrecord_dataset_5fold		[]		36	59	0	kyawlin	cots-tfrecord-dataset-5fold
6028	6028	pretrained_models		[]		54	214	1	serengil	pretrained-models
6029	6029	njknkj		[]		0	4	0	nipsidas	njknkj
6030	6030	Henley Passport Index(2006-2022)	Dataset of strongest passport from 2006 -2022	['people and society', 'government', 'economics', 'aviation', 'travel']	The Henley Passport Index (HPI) is a global ranking of countries according to the travel freedom of holders of their ordinary – as opposed to diplomatic – passport holders. It started in 2006 as Henley & Partners Visa Restrictions Index (HVRI) and was modified and renamed in January 2018. The site provides a ranking for 199 passports of the world according to the number of countries their holders can travel to visa-free. The number of countries that a specific passport can access becomes its visa-free 'score'. In collaboration with the International Air Transport Association (IATA), and based on official data from their global database Henley & Partners has analysed  the visa regulations of the vast majority of the countries and territories in the world since 2006.	46	511	9	ramjasmaurya	henley-passport-index20062022
6031	6031	MH : uHack Sentiments	MH : uHack Sentiments 2.0: Decode Code Works	['intermediate', 'nlp', 'tabular data', 'multiclass classification', 'multilabel classification']	"The data set provided to you has a mix of customer reviews for products across categories and retailers. We would like you to model on the data 
to bucket the future reviews in their respective topics (Note: A review can talk about multiple topics)
Overall polarity (positive/negative sentiment)
Train: 6136 rows x 14 columns
Test: 2631 rows x 14 columns
Topics (Components, Delivery and Customer Support, Design and Aesthetics, Dimensions, Features, Functionality, Installation, Material, Price, Quality and Usability)
Polarity (Positive/Negative)
Note: The target variables are all encoded in the train dataset for convenience. Please submit the test results in the similar encoded fashion for us to evaluate your results.
| Field Name            Data Type   Purpose                         Variable type
Id              Integer     Unique identifier for each review             Input
Review              String      Review written by customers 
                                                                          on a retail website                 Input
Components          String      1: aspects related to components 
                                        0: None                          Target
Delivery and Customer 
Support                         String      1: some aspects related to delivery, return, 
                                        exchange and customer support 
                                        0: None                         Target
Design and 
Aesthetics              String      1: some aspects related to components
                                        0: None                         Target
Dimensions          String      1: related to product dimension and size
                                        0: None                         Target
Features                    String      1: related to product features
                                        0: None                             Target
Functionality           String      1: related to working of a product
                                        0: None                         Target
Installation            String      1: related to installation of the product
                                        0: None                         Target
Material                    String      1: related to material of the product
                                        0: None                         Target
Price               String      1: related to pricing details of a product
                                        0: None                         Target
Quality             String      1: related to quality aspects of a product
                                        0: None                         Target
Usability                   String      1: related to usability of a product
                                        0: None                         Target
Polarity                    Integer     1: Positive sentiment; 
                                        0: Negative Sentiment               Target|
Note: This data is collected from then below machinehack site for the purpose to be used by fellow kagglers to code in kaggle kernels and submit to the competition. ( https://machinehack.com/hackathon/uhack_sentiments_20_decode_code_words/data )"	3	63	2	mohamedziauddin	mh-uhack-sentiments
6032	6032	QSAR Biodegradation Data Set 🔬	 QSAR biodegradation Data Set provided by UCI.	['earth and nature', 'biology', 'chemistry']	"QSAR Biodegradation Data Set (With Column Descriptions) 🔬
Abstract:
Data set containing values for 41 attributes (molecular descriptors) used to classify 1055 chemicals into 2 classes (ready and not ready biodegradable).
Source:
Kamel Mansouri, Tine Ringsted, Davide Ballabio (davide.ballabio '@' unimib.it), Roberto Todeschini, Viviana Consonni, Milano Chemometrics and QSAR Research Group (http://michem.disat.unimib.it/chm/), UniversitÃ degli Studi Milano â€“ Bicocca, Milano (Italy)
Data Set Information:
The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (UniversitÃ degli Studi Milano â€“ Bicocca, Milano, Italy). The research leading to these results has received funding from the European Communityâ€™s Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.
The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.
Attribute Information:
41 molecular descriptors and 1 experimental class:
1) SpMaxL: Leading eigenvalue from Laplace matrix 2) JDz(e): Balaban-like index from Barysz matrix weighted by Sanderson electronegativity
3) nHM: Number of heavy atoms
4) F01[N-N]: Frequency of N-N at topological distance 1
5) F04[C-N]: Frequency of C-N at topological distance 4
6) NssssC: Number of atoms of type ssssC
7) nCb-: Number of substituted benzene C(sp2)
8) C%: Percentage of C atoms
9) nCp: Number of terminal primary C(sp3)
10) nO: Number of oxygen atoms
11) F03[C-N]: Frequency of C-N at topological distance 3
12) SdssC: Sum of dssC E-states
13) HyWiB(m): Hyper-Wiener-like index (log function) from Burden matrix weighted by mass 14) LOC: Lopping centric index 15) SM6L: Spectral moment of order 6 from Laplace matrix
16) F03[C-O]: Frequency of C - O at topological distance 3
17) Me: Mean atomic Sanderson electronegativity (scaled on Carbon atom)
18) Mi: Mean first ionization potential (scaled on Carbon atom)
19) nN-N: Number of N hydrazines
20) nArNO2: Number of nitro groups (aromatic)
21) nCRX3: Number of CRX3
22) SpPosAB(p): Normalized spectral positive sum from Burden matrix weighted by polarizability 23) nCIR: Number of circuits 24) B01[C-Br]: Presence/absence of C - Br at topological distance 1 25) B03[C-Cl]: Presence/absence of C - Cl at topological distance 3 26) N-073: Ar2NH / Ar3N / Ar2N-Al / R..N..R 27) SpMaxA: Leading eigenvalue from adjacency matrix (Lovasz-Pelikan index)
28) Psii1d: Intrinsic state pseudoconnectivity index - type 1d
29) B04[C-Br]: Presence/absence of C - Br at topological distance 4
30) SdO: Sum of dO E-states
31) TI2L: Second Mohar index from Laplace matrix 32) nCrt: Number of ring tertiary C(sp3) 33) C-026: R--CX--R 34) F02[C-N]: Frequency of C - N at topological distance 2 35) nHDon: Number of donor atoms for H-bonds (N and O) 36) SpMaxB(m): Leading eigenvalue from Burden matrix weighted by mass
37) PsiiA: Intrinsic state pseudoconnectivity index - type S average
38) nN: Number of Nitrogen atoms
39) SM6_B(m): Spectral moment of order 6 from Burden matrix weighted by mass
40) nArCOOR: Number of esters (aromatic)
41) nX: Number of halogen atoms
42) experimental class: ready biodegradable (RB) and not ready biodegradable (NRB)"	9	115	2	nadernarcisse	qsar-biodegradation-data-set
6033	6033	Face mask image classification 	Classification between with and without face mask images	['classification', 'image data']	"Context
This data is for creating an image classification model for classifying the images with and without face mask.
Content
The data contains 2 folders : with_mask and without_mask. First one contains all the images with face mask and second one without face masks.
Acknowledgements
The data is gathered from different sources and assembled in one place so that different varieties of images can be obtained in one place."	10	153	0	ayku09	face-mask-image-classification-data
6034	6034	LTU ACTor Gesture Training Images	Dataset for Gesture Image Classification	['movies and tv shows', 'artificial intelligence', 'computer vision', 'deep learning', 'cnn', 'image data']	"Context
Dataset of gesture images consisting of frames containing author J. Schulte for use in the human-vehicle Leader-Follower Autonomy (LFA) project. In this project, we have enabled a human leader to activate and deactivate the following function of an autonomous vehicle (Lawrence Technological University's ACTor1) using gesture recognition. We demonstrate a reliable and practical application of gesture recognition in a real-world scenario. For more information, please see our paper here: [PUT PAPER HERE]. Also please see our GitHub repository here: https://github.com/jschulte-ltu/ACTor_Person_Following.
Cropped pictures are generated by using YOLO to locate people in a frame and cut out an image of our target size (192x192 pixels) to feed into a TensorFlow Posenet to perform pose estimation. Pose data is then fed into a DNN classifier to return the most likely gesture.
Content
The images are split into three folders, each with one gesture. ""Start"" is the starting gesture; a user that gives a ""start"" command will be the pose target, and will be closely followed by the vehicle. ""Halt"" is the stopping gesture; this commands the vehicle to stop tracking the current target. ""None"" is a series of poses that are neither ""stop"" nor ""start""; the vehicle will not change behaviour if not given a specific command.
Acknowledgements
Co-authors: Chan-Jin Chung, Nicholas Paul, Mitchel Pleune
Other Acknowledgements: Joseph Redmon (YOLO creator), Google Engineers (Tensorflow)"	0	18	0	dashlambda	ltu-actor-gesture-training-images
6035	6035	stackingw		[]		0	5	0	ironluu	stackingw
6036	6036	Extended CMU MoCap dataset for BeatGAN		['computer science', 'software']	"This is a CSV raw version of the CMU MoCap dataset subset used in [Zhou et al., 2019]. There is no windowing, striding nor normalisation applied to the data.
For more information concerning the structure of the data please see Beatgan Repo.
The dataset is also extended by a new class dancing. For the unextended dataset please see here. The additional data can be found here here. For a list of the data actually processed please see processed_files.txt
Structure
All of the data was concatenated into a single CSV data.csv. The provided labels.csv provides the labels for each data sample.
Labels
Zhou et al. use three classes for their dataset. The first class walking (labelled as 0) is considered the normal class and the jogging (labelled as 1), jumping (labelled as 2) and dancing (labelled as 3) are considered abnormal classes.
License Notes from the original dataset authors:
Original Authors
This data is free for use in research projects.
You may include this data in commercially-sold products,
but you may not resell this data directly, even in converted form.
If you publish results obtained using this data, we would appreciate it
if you would send the citation to your published paper to jkh+mocap@cs.cmu.edu,
and also would add this text to your acknowledgments section:
The data used in this project was obtained from mocap.cs.cmu.edu.
The database was created with funding from NSF EIA-0196217."	4	6	0	maximdolg	extended-cmu-mocap-dataset-for-beatgan
6037	6037	Ballon d'or nominees	Map of where the best footballers of 2021 are from.	['football', 'sports', 'beginner', 'data visualization', 'data analytics', 'image data']	"Context
I'm a big soccer/football fan and have been my whole life (it practically comes with the territory when you are Portuguese). I'm just beginning my data analysis journey, and have decided to use soccer as an inspiration for some of my early projects.
Content
This is a map that shows the concentration of Ballon d'Or nominees from each country.
The Ballon d'Or is the award given to the best footballer of the year, and every year there are thirty nominees.
Acknowledgements
This was originally made in tableau. To see the original dashboard, click here.
image provided by Travel Nomades 
Inspiration
The love of the game"	249	1844	13	dcgonk	ballon-dor-nominees
6038	6038	efficientnet_b3a		[]		0	5	0	greepex	efficientnet-b3a
6039	6039	CKPTdata2		[]		0	66	0	fanglizhao	ckptdata2
6040	6040	resnet50		[]		0	5	0	greepex	resnet50
6041	6041	trainedmodel		[]		0	1	0	hayden234	trainedmodel
6042	6042	Spotify Top 200 Daily Global 2017 - 2021	Spotify Top 200 Songs Daily, Global from 2017 to 2021, over 350000 tracks	['music', 'json']		10	123	0	c0lydxmas	spotify-top-200-daily-global-2017-2021
6043	6043	Truth Detection/Deception Detection/Lie Detection	14k political statements with 6 degrees of truthfulness from Politifact.com	['united states', 'government', 'law', 'politics', 'tabular data']	"Truth Detection / Lie Detection / Deception Detection / Fact Checking
Context
The following dataset was obtained by parsing statements and their veracity verdict from Politifact.com. Contains 14k affirmations up till late 2020.
The statements obtained are of 6 categories: True, Mostly True, Half-True, Mostly False, False, Pants on Fire!
This dataset can be used for multiple purposes: attempting to detect truthfulness based on statement language (or conversely, detecting lies), fact-checking integration or just EDA for political purposes.
Content
There are 4 columns in politifact.csv: statement, source, link, veracity.
statement - statement made by celebrity or politician.
source - can be a person, but not necessarily.
link - URL of affirmation.
veracity - degree of truthfulness given by the Politifact.com team.
Other variants have certain classes removed and are binarized (into truths and lies).
Have a quick look over this notebook for more details: https://www.kaggle.com/thesergiu/part-1-quick-eda-on-politifact-csv
Don't forget to upvote the dataset if you find it useful!
Acknowledgements
Initial Source: www.politifact.com
Creator GitHub Link: https://github.com/the-sergiu
GitHub Repo Link for more context: https://github.com/the-sergiu/TruthDetection"	100	1369	7	thesergiu	truth-detectiondeception-detectionlie-detection
6044	6044	athlete_dataset		[]		0	4	0	prasadsourav	athlete-dataset
6045	6045	petfinder_exp55		[]		3	41	1	titericz	petfinder-exp55
6046	6046	COCO TFRecords Valid		[]		0	7	0	mmelahi	coco-tfrecords-valid
6047	6047	petfinder_exp77		[]		4	12	1	titericz	petfinder-exp77
6048	6048	outfile_x		[]		1	6	0	ohyoonah	outfile-x
6049	6049	models_weights_data		[]		0	12	0	arkhamking	models-weights-data
6050	6050	Election Polls DataSets	Election Polls DataSets	['politics', 'beginner', 'intermediate', 'data analytics', 'classification']	"Current polls files contain data since the most recent election. Historical files contain data prior to the most recent election.
Presidential Primary Polls: current, historical
Presidential General Election Polls: current, historical
Senate Polls: current, historical
House Polls: current, historical
Governor Polls: current, historical
Presidential Approval Polls: current, historical
Vice Presidential Approval Polls: current
Generic Ballot Polls: current, historical
Favorability Polls: current
Historical approval polls prior to the Trump presidency are not available, however, The Roper Center maintains a collection of historical approval polls.
Historical vice presidential approval polls prior to the Biden presidency are not available.
**At this time, we are tracking only post-2020 Trump favorability. Favorability polls prior to this time are not available."	4	57	2	gmkeshav	election-polls-datasets
6051	6051	billboard		[]		3	9	0	fernandobordi	billboard
6052	6052	US HOMELESSNESS	In recent years, homelessness increased nationally by almost one percent. 	['public health', 'data cleaning', 'data visualization', 'data analytics', 'text data']		18	108	1	beelaboo	us-homelessness
6053	6053	shandong_posweight2		[]		0	5	0	iwillfindthatgirl	shandong-posweight2
6054	6054	Birds vs Drone Dataset	Identify whether there is a Bird Or Drone in the sky	['classification', 'cnn', 'image data', 'binary classification', 'keras']	"Context
Identifying If there is a Bird Or Drone present in the sky can be a good solution to identify drones in the sky.
Content
This dataset contain 2 Folders of Birds and Drones respectively which contain images of Birds and Drones present in the sky.
The Bird images are Scrapped  and Drones images are taken from another dataset.
Inspiration
This dataset can help someone to classify between drones or Birds.
If you like the dataset please upvote it"	20	240	4	harshwalia	birds-vs-drone-dataset
6055	6055	Data_validation		[]		0	11	0	vitomanuguerra	data-validation
6056	6056	Temperature by city	Weather data of different countries by cities 	['weather and climate', 'data cleaning', 'tabular data', 'text data', 'regression']		9	57	1	joyee19	temperature-by-city
6057	6057	15ya_gunwoo		[]		6	5	0	gunw00	15ya-gunwoo
6058	6058	firstDataSet		[]		0	1	0	nan2021	firstdataset
6059	6059	analisis-tw		[]		0	13	0	fernandobordi	analisistw
6060	6060	Human dataset latest		['earth and nature']		0	14	1	aakashveera	human-dataset-latest
6061	6061	Email marketing analysis 		[]		4	42	0	vickylau32	email-marketing-analysis
6062	6062	CNModels		[]		0	3	0	kashiwaba	cnmodels
6063	6063	Owen test		[]		0	8	0	geoanorak	owen-test
6064	6064	DETR - Weights and Supplies	DETR - Fine-tuned weights and libraries for the Great Barrier Reef Competition	['exercise']	"An adaption of End to End Object Detection with Transformers:DETR to the Great Barrier Reef Competition
Training notebook: 🐠 Reef - DETR - Detection Transformer - Train
Inference notebook: 🐠 Reef - DETR - Detection Transformer - Infer
This is the output of the training notebook."	16	326	8	julian3833	detr-weights-and-supplies
6065	6065	exuntask2		[]		1	29	0	ayaanmustafa	exuntask2
6066	6066	Electronic component		[]		1	14	0	akeshm	electronic-component
6067	6067	PetFinder-ZX140-20220112130313		[]		0	8	0	hideyukizushi	petfinder-zx140-20220112130313
6068	6068	Human handcreated		[]		0	4	0	aakashveera	human-handcreated
6069	6069	Power Stations in India	Number Of Power Stations In India	['india', 'beginner', 'intermediate', 'data analytics', 'python']	"Context
This Data file is Shows the Detail of Number of PowerStation's in India.
Content
This data set shows' The type of power stations in India and what fuel type is using and other details. 
Acknowledgements
thanks to Wikipedia and other government sites to making this database."	43	283	6	sandipdevre	power-stations-in-india
6070	6070	Customer Churn Dataset	Customer Churn Dataset	[]		5	32	1	hamidmehdi	customer-churn-dataset
6071	6071	ConvNext 2022		[]		0	15	0	mlneo07	convnext-2022
6072	6072	linear3		[]		0	7	0	abrahamanderson	linear3
6073	6073	minimize		[]		0	10	0	abrahamanderson	minimize
6074	6074	Wikipedia Talk Corpus		['nlp', 'text data']	"Context
https://figshare.com/articles/dataset/Wikipedia_Talk_Corpus/4264973
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	2	45	2	tomooinubushi	wikipedia-talk-corpus
6075	6075	TOP 100 E-SPORTS GAMES BY PRIZE POOL	TOP 100 E-SPORTS GAMES 	['games', 'video games', 'beginner', 'exploratory data analysis', 'simulations']	"Context
THIS DATABASE SHOWES THE TOP 100 GAMES BY WINNING PRIZE POOL
Content
FILE DESCRIBE THE GAME RANK AND WINING PRIZE POOL AND OTHER DETAILS
Acknowledgements
THIS DATA IS BASED ON WEB DATA.
Inspiration
ITS SHOWES THAT HOW E-SPORTS IS GROWING FOR GAMERS COMMUNITY."	29	241	4	sandipdevre	top-100-esports-games-by-winning-money-prize
6076	6076	linear2		[]		0	1	0	abrahamanderson	linear2
6077	6077	PetFinder Competition AdoptionSpeed		[]		0	37	0	vineethakkinapalli	petfinder-competition-adoptionspeed
6078	6078	swin224cropexp14ft		[]		0	7	1	nischaydnk	swin224cropexp14ft
6079	6079	vaccineCityLevel		[]		2	11	0	richard0218	vaccinecitylevel
6080	6080	NT3000_vouvhres		[]		2	6	0	richard0218	nt3000-vouvhres
6081	6081	NT3000_vouvhres		[]		0	3	0	jambochen	nt3000-vouvhres
6082	6082	3000dataexam		[]		0	16	0	tonysmallfish	3000dataexam
6083	6083	vaccineCityLevel		[]		3	11	0	kevinwo	vaccinecitylevel
6084	6084	pytonsfds		[]		1	28	0	s410711217	pytonsfds
6085	6085	110-2 python 期末考		[]		0	12	0	a41073019	1102-python
6086	6086	Binary1201		[]		6	4	0	daominhkhanh	binary1201
6087	6087	dbnet dataset		['earth and nature']		1	59	0	riadhossainapsis	dbnet-dataset
6088	6088	mymeleset		[]		0	1	0	moonthy	mymeleset
6089	6089	pf-161-train-4-data		[]		0	2	0	makotoikeda	pf-161-train-4-data
6090	6090	reef_baseline_fold12		[]		557	1217	22	steamedsheep	reef-baseline-fold12
6091	6091	Game Of Thrones TV Series script data	Script of famous HBO TV Series with dialogues and speaker data	['movies and tv shows', 'north america', 'text mining', 'tabular data', 'text data']	"Game of Thrones TV show
Game of Thrones is the most watched and most famous TV Show in world. It was a HBO orignal TV show which is still considered as the best TV series ever made. 
This Dataset
This dataset contains the scripts of the Game of Thrones TV show with dialogues and their respective speakers along with the episode name and the season at which the dialogue was spoken.
Acknowledgements
This dataset was originally scapped from various websites for a Neural search project."	52	690	8	gopinath15	gameofthrones
6092	6092	loan Prediction	used to analysis and deal with missing values	[]		5	32	0	deepjani	ipl-matches
6093	6093	data2.csv		['social networks']		1	18	0	ashishkumarsharma	data2csv
6094	6094	DEL hard of pawpularityscorefront-tfrecord-fold		[]		0	5	0	suyinchen1024	del-hard-of-pawpularityscorefronttfrecordfold
6095	6095	MyPylon		['asia', 'engineering', 'image data', 'electricity']		1	12	0	ramzuredha	mypylon
6096	6096	ConvNeXt		[]		55	109	6	mithilsalunkhe	convnext
6097	6097	Combine test data		['earth and nature']		0	3	0	aamir2000	combine-test-data
6098	6098	Transformer Ensemble		['music']		1	37	1	sakshamdwivedi10	transformer-ensemble
6099	6099	SepsisDataXLS		[]		3	20	0	namratakapoor1	sepsisdataxls
6100	6100	precision rate 2		['business']		0	7	0	qiujiaqi	precision-rate-2
6101	6101	precision rate		['business']		0	1	0	qiujiaqi	precision-rate
6102	6102	harga-rumah		['ratings and reviews']		1	26	0	gustiosamba	hargarumah
6103	6103	lovely1		[]		0	5	1	zhengyuntao123	lovely1
6104	6104	Mood , Emotions and Sentiments		[]		2	31	0	asiasamreen	mood-emotions-and-sentiments
6105	6105	cotscoco		[]		27	52	1	ferlockx	cotscoco
6106	6106	Emoticon Unicode Dataset Translated to Indonesian		[]		0	13	0	dwiahmad	emoticon-unicode-dataset-translated-to-indonesian
6107	6107	DETR Broken Barrier Reef Weights		[]		0	7	0	julian3833	detr-broken-barrier-reef-weights
6108	6108	Pytorch2Keras VGG Weights	pytorch2keras VGG weights	['exercise']		6	51	0	tom99763	vgg16-weights
6109	6109	Demo Dataset		[]		0	23	0	ayesha111	demo-dataset
6110	6110	Nutrition datasets	This dataset consists of the levels of proteins present in different foods	['nutrition', 'computer science', 'programming']	"Context
This dataset consists of the levels of various proteins, fats and other things that are present in a variety of foods (8789 to be precise). The aim of this dataset is to help beginners learn pandas, numpy and eda.
Content
There are 8789 rows. Each row is a unique food item made up of different items. There are 77 columns. Each column represents the level of a vitamin or a protein or fat content that is present in the corresponding food. There are null values in this dataset.
Acknowledgements
This dataset was collected from a pandas course that I attended in Udemy. The course's author is Andy Bek.
Inspiration
The inspiration behind this dataset is to provide a dataset in which any beginner can apply their pandas, numpy and visualization skills."	14	139	1	sathyakrishnan12	nutrition-datasets
6111	6111	indices optimization sample data		[]		0	17	0	ravendrakr	indices-optimization-sample-data
6112	6112	swin_large_224_longaxis_bayes_2022_re		[]		0	9	0	ttagu99	swin-large-224-longaxis-bayes-2022-re
6113	6113	trivia_filted_by_ans		[]		2	13	0	vivonnn	trivia-filted-by-ans
6114	6114	pawpularityscorefront-tfrecord-pre-score-50		[]		0	8	0	suyinchen1024	pawpularityscorefronttfrecordprescore50
6115	6115	new_qa_filted_by_ans		[]		3	17	0	vivonnn	new-qa-filted-by-ans
6116	6116	Pawpularity with EfficientNetB2 FineTuning output		[]		0	24	1	lonnieqin	pawpularity-with-efficientnetb2-finetuning-output
6117	6117	Abnormal speed judgment		['health conditions']	"speed1:speed of time
speed2:speed of time+1"	0	20	0	xiangdongyuan	abnormal-speed-judgment
6118	6118	PetFinder-ZX200-20220112023312		[]		0	13	0	hideyukizushi	petfinder-zx200-20220112023312
6119	6119	NYC Homeless Service Requests (2020)	311 Service Requests Recieved By The Department of Homeless Services In 2020	['united states', 'public health', 'housing', 'government', 'social issues and advocacy']	311 is a non-emergency number that provides access to non-emergency municipal services. The following dataset was collected from New York City's public dataset via Google's marketplace.	1	75	2	donnetew	nyc-homeless-complaints-2020
6120	6120	w1_data_2022_spring		[]		1	33	0	ytlee8	w1-data-2022-spring
6121	6121	jigsaw2021-wat067-data		[]		0	8	0	wataoka	jigsaw2021-wat067-data
6122	6122	DHL Courier Facilities Dataset	DHL Courier locations and metadata	['cities and urban areas', 'geography', 'geospatial analysis', 'travel', 'urban planning']	"DHL Facilities
DHL is an international Umbrella brand and trademark for the courier, package delivery a and express mail service, which is a division of the German logistics firm Deutsche Post. The company group delivers over 1.6 billion parcels per year.
The company DHL itself was founded in San Francisco, USA, in 1969 and expanded its service throughout the world by the late 1970s. In 1979, under the name of DHL Air Cargo, the company entered the Hawaiian islands with an inter-island cargo service using two DC-3 and four DC-6 aircraft. Adrian Dalsey and Larry Hillblom personally oversaw the daily operations until its eventual bankruptcy closed the doors in 1983. At its peak, DHL Air Cargo employed just over 100 workers, management, and pilots.
Content
This dataset contains metadata of DHL locations along with the address and contact details 
Acknowledgements
Homeland Infrastructure Foundation-Level Data (HIFLD)"	131	1119	12	shivamb	dhl-courier-facilities-dataset
6123	6123	AUC driver 2		['education']		0	5	0	aamir2000	auc-driver-2
6124	6124	Prisons and Secure Detention Facilities in USA 	Secure Detention Facilities - Homeland Infrastructure Foundation Level Data	['crime', 'government', 'military', 'politics', 'public safety']	"USA Prisons and Secure Detention Facilities
Source: Homeland Infrastructure Foundation Level Data (HIFLD) database. (https://gii.dhs.gov/HIFLD)
These facilities range in the jurisdiction from federal (excluding military) to local governments. Polygon geometry is used to describe the extent of where the incarcerated population is located (fence lines or building footprints). This feature class’s attribution describes many physical and social characteristics of detention facilities in the United States and some of its territories. The attribution for this feature class was populated by open-source search methodologies of authoritative sources."	109	769	14	shivamb	prisons-and-secure-detention-facilities-in-usa
6125	6125	DataCleansing		[]		2	44	0	kakamana	datacleansing
6126	6126	OOF_SAMPLE_DATASET(Regression)	oof test dataset (out of fold)	[]		4	26	1	rhythmcam	oof-test-dataset
6127	6127	Mileage prediction		['automobiles and vehicles']	"SC:Remaining battery capacity；
EC:Remaining battery energy；
DR:Estimated remaining mileage；
RMC:Single ride mileage;
TMC:Total accumulated mileage；
SET:Ride end time；
SST:Ride start time；"	3	28	0	xiangdongyuan	mileage-prediction
6128	6128	Randomized MNIST	MNIST Dataset Randomly Split	['computer science']		3	16	0	ernestobh	randomized-mnist
6129	6129	my-petfinder-pawpularity-score		[]		0	19	0	ragiko	mypetfinderpawpularityscore
6130	6130	magnetic_tile_defect_datasets		[]		1	83	0	jaypume	magnetic-tile-defect-datasets
6131	6131	AI_SPARK		[]		4	23	0	junghokim	ai-spark
6132	6132	slarge_384_1019_0112		[]		0	12	0	wuyhbb	slarge-384-1019-0112
6133	6133	crypto_subreddit_user_count_data	Total & active user subreddit counts for 100+ crypto coins : 6/1/2021 - 1/6/2022	['finance', 'economics', 'internet', 'beginner', 'investing', 'currencies and foreign exchange']	"I thought it would be interesting to see the relationship between the price movement of the top 100 crypto coins and the user activity of their respective subreddits. 
The dataset is made up of the daily total subreddit user count and current active subreddit users from 6/1/2021 - 1/6/2022."	2	97	0	calenmcnickles	crypto-subreddit-user-count-data
6134	6134	E-Commerce Dataset	Auto-generated E-commerce dataset 	['business']		15	78	0	onigbenga	ecommerce-dataset
6135	6135	Zindi User Behaviour Birthday Challenge		['holidays and cultural events']		0	25	0	harshbansal27	zindi-user-behaviour-birthday-challenge
6136	6136	Swiggy Restaurants Dataset of Metro Cities	Restaurant details of major metropoliton cities with population more than 4 Mn	['data analytics', 'tabular data', 'food', 'restaurants', 'IPython']	"Context
This dataset contains swiggy registered restaurants details of major metropoliton cities of India. I have considered only metropoliton cities with population 4.5 million. As per the Census of India 2011 definition of more than 4 million population, some of the major Metropolitan Cities in India are:
Mumbai (more than 18 Million)
Delhi (more than 16 Million)
Kolkata (more than 14 Million)
Chennai (more than 8.6 million)
Bangalore (around 8.5 million)
Hyderabad (around 7.6 million)
Ahmedabad (around 6.3 million)
Pune (around 5.05 million)
Surat (around 4.5 million)
Content
I have scrapped the data using python. 
It may not have all the restaurants of a particular city because if during webscrapping any restaurant has not enabled swiggy as their delivery partner, that restaurant's details will not be scrapped. Though I have scrapped same cities multiple times, to include maximum restaurant details. The data is collected on 12th Jan 2022.
Acknowledgements
Thank you swiggy for the dataset.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	25	101	2	aniruddhapa	swiggy-restaurants-dataset-of-metro-cities
6137	6137	databinli		[]		2	4	0	nurmelike	databinli
6138	6138	Single Family Loan-Level Dataset	Loan-level credit performance data and credit risk modeling	['united states', 'real estate', 'intermediate', 'tabular data']	"Context
As part of a larger effort to increase transparency, Freddie Mac is making available loan-level credit performance data on all mortgages that the company purchased or guaranteed from 1999 to 2021.
The availability of this data will help investors build more accurate credit performance models in support of ongoing risk sharing initiatives highlighted by company regulator, the Federal Housing Finance Agency in the 2021 conservatorship scorecard .
Content
The original standard  datasets cover approximately 48.9 million mortgages (including HARP loans) originated between January 1, 1999 and June 30, 2021. Monthly loan performance data (This dataset originated last Quarter of 2021), including credit performance information up to and including property disposition, is being disclosed through September 30, 2021. Specific credit performance information in the dataset includes voluntary prepayments and loans that were Foreclosure Alternatives and REOs. Specific actual loss data in the dataset includes net sales proceeds, MI recoveries, non-MI recoveries, expenses, current deferred UPB, and due date of last paid installment.
Data Dictionary:  http://www.freddiemac.com/fmac-resources/research/pdf/user_guide.pdf
Full data download( All Quarters) : https://freddiemac.embs.com/FLoan/Data/downloadQ.php
Full data download( Annual): https://freddiemac.embs.com/FLoan/Data/downloadA.php
Acknowledgements
Freddie Mac  was chartered by Congress in 1970 to support the U.S. housing finance system and to help ensure a reliable and affordable supply of mortgage funds across the country. Rather than lending directly to borrowers, Freddie Mac operates in the U.S. secondary mortgage market, buying loans that meet our standards from approved lenders. Those lenders are then, in turn, able to provide more loans to qualified borrowers and keep capital flowing into the housing market. Freddie Mac then pools the mortgages it buys into securities, which they sell to investors around the world.
Inspiration
Credit performance and credit risk modeling"	39	303	2	arashnic	single-family-loanlevel-dataset
6139	6139	lgbm_models_11_01_22_v2		[]		0	3	0	malekbadreddine	lgbm-models-11-01-22-v2
6140	6140	Jigsaw Rate Severity Additional data	Jigsaw Rate Severity of Toxic Comments 2021	['nlp', 'regression']		21	365	10	chryzal	jigsaw-rate-severity-additional-data
6141	6141	imagesbinli		[]		0	14	0	nurmelike	imagesbinli
6142	6142	data emas		['business']		3	15	0	vialistianggraeny	data-emas
6143	6143	IMDB Movie Review Sentiment Original Data Set		['movies and tv shows']		0	28	0	aryamanbabber	imdb-movie-review-sentiment-original-data-set
6144	6144	fb-longformer-large-1536		['social networks']		196	446	13	abhishek	fblongformerlarge1536
6145	6145	ptnt_model		[]		0	6	1	zloydanny	ptnt-model
6146	6146	resnetv2w		[]		0	22	0	ironluu	resnetv2w
6147	6147	Online Retail Dataset		['business']		7	72	0	madhuradeshpande	online-retail-dataset
6148	6148	FiveThirtyEight Comic Characters Dataset		['comics and animation']		0	18	0	ibrahimoduwole	fivethirtyeight-comic-characters-dataset
6149	6149	Churn_Data		[]		0	9	0	starlitlolith	churn-data
6150	6150	covid_190		[]		0	11	0	ananyaanu47	covid-190
6151	6151	YawDD driver		[]		2	11	0	aamir2000	yawdd-driver
6152	6152	petfinder_models_2		[]		0	13	0	sinpcw	petfinder-models-2
6153	6153	models-data		[]		0	5	0	sai1881	modelsdata
6154	6154	COCO TFRecords Train 0		['travel']		0	9	0	mmelahi	coco-tfrecords-train-0
6155	6155	fog_smog	A dataset of 1000 smog and 1000 fog images	['automobiles and vehicles']		1	28	0	qbasit	fog-smog
6156	6156	Gas-Turbine CO and NOx Emission Data	Industrial gas turbines for power generation 	['energy']	"A powerplant  engine( gas-turbine) is mainly used to generate electricity. Since the engine can different types of fuels, the engine can have different levels of CO2 and NO emission gases. 
This dataset is generated from a gas turbine in Turkey. More info:
https://journals.tubitak.gov.tr/elektrik/issues/elk-19-27-6/elk-27-6-54-1807-87.pdf
Column name descriptions:
Variable (Abbr.) Unit Min Max Mean
Ambient temperature (AT) C â€“6.23 37.10 17.71
Ambient pressure (AP) mbar 985.85 1036.56 1013.07
Ambient humidity (AH) (%) 24.08 100.20 77.87
Air filter difference pressure (AFDP) mbar 2.09 7.61 3.93
Gas turbine exhaust pressure (GTEP) mbar 17.70 40.72 25.56
Turbine inlet temperature (TIT) C 1000.85 1100.89 1081.43
Turbine after temperature (TAT) C 511.04 550.61 546.16
Compressor discharge pressure (CDP) mbar 9.85 15.16 12.06
Turbine energy yield (TEY) MWH 100.02 179.50 133.51
Carbon monoxide (CO) mg/m3 0.00 44.10 2.37
Nitrogen oxides (NOx) mg/m3 25.90 119.91 65.29
What you can do with this dataset:
You can create a regression model to generate the level of Carbon monoxide (CO) and Nitrogen oxides (NOx). 
You can also find out which features are most correlated to CO2 and NO."	12	113	1	muniryadi	gasturbine-co-and-nox-emission-data
6157	6157	Staples2		[]		0	9	0	chrisgrove	staples2
6158	6158	reeeef_yolov5_baseline_yolov5s		[]		121	311	2	steamedsheep	reeeef-yolov5-baseline-yolov5s
6159	6159	India Agriculture Crop Production	India District-wise agriculture crop area and production 1997-2020 (upd 01.2022)	['india', 'agriculture', 'statistical analysis', 'tabular data']	"India District-wise agriculture crop area and production statistics 1997-2020
Latest update 01.2022.
Next update of dataset is scheduled on summer 2022.
Data collected from Ministry of Agriculture and Farmers Welfare of India 
Kudos to them!
Due to limitations at source data is only partially available in latest years
- 2018-19 season - data available on 34 of 35 states and territories
- 2019-20 season - data available on 26 of 35 states and territories
Thank you!"	342	1819	13	pyatakov	india-agriculture-crop-production
6160	6160	UK Monster Job Listing Dataset	This dataset includes job data from Monster UK	['jobs and career']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 305602  Domain Name : monster.uk  Date Range : 01st Apr 2021 - 30th Jun 2021   File Extension : ldjson
Available Fields : uniq_id, crawl_timestamp, url, job_title, category, company_name, city, state, country, post_date, job_description, inferred_salary_from, inferred_salary_to, inferred_salary_time_unit, job_board, geo, valid_through, html_job_description, is_remote, test_contact_email, contact_email, test1_cities, test1_states, test1_countries, site_name, domain, postdate_yyyymmdd, predicted_language, inferred_iso3_lang_code, test1_inferred_city, test1_inferred_state, test1_inferred_country, inferred_country, inferred_salary_currency, has_expired, last_expiry_check_date, latest_expiry_check_date, dataset, postdate_in_indexname_format, segment_name, duplicate_status, fitness_score  
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud, DataStock and live job data from JobsPikr.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	3	66	1	promptcloud	uk-monster-job-listing-dataset
6161	6161	reef_999		[]		2	19	0	pavel900	reef-999
6162	6162	GGGGGG		[]		0	36	0	wujunwww	gggggg
6163	6163	COCO TFRecords Train 3		['arts and entertainment']		0	9	0	mmelahi	coco-tfrecords-train-3
6164	6164	COCO TFRecords Train 2		['arts and entertainment']		0	2	0	mmelahi	coco-tfrecords-train-2
6165	6165	COCO TFRecords Train 1		['travel']		0	1	0	mmelahi	coco-tfrecords-train-1
6166	6166	resnetv2w		[]		5	10	0	nicholasleee	resnetv2w
6167	6167	resnetv2w		[]		5	12	0	livennn	resnetv2w
6168	6168	samil-test		[]		0	2	0	amoghj8	samiltest
6169	6169	denseNet121		[]		0	11	0	khaqan	densenet121
6170	6170	trainmmmmm		[]		0	17	0	massimomarcelletti	trainmmmmm
6171	6171	breast cancer data		['cancer']		0	21	0	srinivasan29	breast-cancer-data
6172	6172	2aaaaaaaaa		[]		0	2	0	wolaicarry	2aaaaaaaaa
6173	6173	yolov5l		[]		0	11	0	phoenix9032	yolov5l
6174	6174	defacto-inpainting	Object-Removals from the DEFACTO dataset	['image data', 'public safety']	"Context
Digital image forensic has gained a lot of attention as it is becoming easier for anyone to make forged images. Several areas are concerned by image manipulation: a doctored image can increase the credibility of fake news, impostors can use morphed images to pretend being someone else.
It became of critical importance to be able to recognize the manipulations suffered by the images. To do this, the first need is to be able to rely on reliable and controlled data sets representing the most characteristic cases encountered. The purpose of this work is to lay the foundations of a body of tests allowing both the qualification of automatic methods of authentication and detection of manipulations and the training of these methods.
Content
This dataset contains about 25000 object-removal forgeries are available under the inpainting directory. Each object-removal is accompanied by two binary masks. One under the probe_mask subdirectory indicates the location of the forgery and one under the inpaint_mask which is the mask use for the inpainting algorithm.
Reference
If you use this dataset for your research, please refer to the original paper : 
@INPROCEEDINGS{DEFACTODataset, AUTHOR=”Gaël Mahfoudi and Badr Tajini and Florent Retraint and Fr{'e}d{'e}ric Morain-Nicolier and Jean Luc Dugelay and Marc Pic”, TITLE=”{DEFACTO:} Image and Face Manipulation Dataset”, BOOKTITLE=”27th European Signal Processing Conference (EUSIPCO 2019)”, ADDRESS=”A Coruña, Spain”, DAYS=1, MONTH=sep, YEAR=2019 }
and to the MSCOCO dataset
License
The DEFACTO Consortium does not own the copyright of those images. Please refer to the MSCOCO terms of use for all images based on their Dataset."	18	124	0	defactodataset	defactoinpainting
6175	6175	titanic		[]		0	20	0	uncatchcat	titanic
6176	6176	all_courses		[]		3	38	0	grannysmithapples	all-courses
6177	6177	fma_mel		[]		0	11	0	giuseppemagazz	fma-mel
6178	6178	sartorius cell instance segmentation		[]		0	10	0	kagglelyy	sartorius-cell-instance-segmentation
6179	6179	COCO Mask		[]		0	2	0	mmelahi	coco-mask
6180	6180	SIIM COVID 19 detection stage 1：training		[]		0	5	0	kagglelyy	siim-covid-19-detection-stage-1training
6181	6181	FORZA HORIZON 5 CARS DATASET	Car database from the game Forza Horizon 5 	['auto racing', 'automobiles and vehicles', 'beginner', 'exploratory data analysis', 'tabular data']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
Forza Horizon 5 is a 2021 racing video game developed by Playground Games and published by Xbox Game Studios. The twelfth main instalment of the Forza series, the game is set in a fictionalized representation of Mexico. This dataset is a collection of cars that we can find and buy in the game. The dataset contains various features such as car speed, ratings, power etc.
Acknowledgements
Image Source (stream)"	316	3151	35	deepcontractor	froza-horizon-5-cars-dataset
6182	6182	pytorch gan collection		[]		0	9	0	kagglelyy	pytorch-gan-collection
6183	6183	UseEmbeddingsV4		[]		0	14	0	aqeeljanjua	useembeddingsv4
6184	6184	pytorch example of model		['earth and nature']		0	3	0	kagglelyy	pytorch-example-of-model
6185	6185	global wheat detection 2020 stage 1：training		[]		0	9	0	kagglelyy	global-wheat-detection-2020-stage-1training
6186	6186	az_num		[]		0	15	0	runxiaudreyzhao	az-num
6187	6187	Vehicle Detection for Yolov5	Set of images for training yolov5 for vehicle detection	['automobiles and vehicles']		2	31	0	therealshihab	vehicle-detection-for-yolov5
6188	6188	Model10/1		[]		20	15	0	khanhdmdragon	model101
6189	6189	doggggg		[]		0	15	0	chenhaochen	doggggg
6190	6190	Dataset		[]		1	14	1	karlandriansalvado	dataset
6191	6191	224tinymodels		[]		0	8	0	greepex	224tinymodels
6192	6192	Arabic Summarization Benchmark		[]		0	14	0	bavlygeorge	arabic-summarization-benchmark
6193	6193	50Startups		[]		1	7	0	fernandobordi	50startups
6194	6194	snli_1.0		[]		0	4	0	tengfeidong	snli-10
6195	6195	pf-161-train-3-data		[]		0	4	0	makotoikeda	pf-161-train-3-data
6196	6196	comic data		[]		0	7	0	michaeljeyasuriyaa	comic-data
6197	6197	Tagging Documents		['business']		0	9	0	anuvagoyal	tagging-documents
6198	6198	Why you should use an InventHelp service		['law']	"Anyone can get invention help if they have an idea. It's a great way learn about the market and the industry. When you're just starting out and don't know where to turn for assistance, an expert can guide you. Here are a few reasons to hire an invention help service. 1. It's free! InventHelp is a service that provides free information to new inventors about how to bring their ideas to market.
Visit here https://www.thelakewoodscoop.com/news/2022/01/the-benefits-of-turning-to-inventhelp-for-patent-assistance.html for invent help.
An invention needs to have an edge. It must solve a problem or fill a need. InventHelp can help if you don't know where to begin. Their patent services include patent searches, patent lawyers, and follow-ups with leads. Invention help is vital to your success. However, you should be wary of companies that claim to be experts in their field. Invention help should not be used blindly.
A patent search is essential for determining whether your idea has a chance of achieving patent protection. A trusted IP attorney will conduct a search for patents on your behalf and not try to charge you thousands of dollars. This will help determine whether your idea is unique and what the next steps are. If you've got a patent, your invention can move from concept to reality. Just be sure that you're not trying to steal another person's idea.
Investing in an IP lawyer is a smart move. A trusted patent attorney will conduct a patent search on your behalf. They will not try and sell you monthly subscriptions or scam you out of thousands. An IP attorney will not only check for the novelty of your idea, but can also help you decide what steps to take in order to pursue it. An IP attorney is your best option if you don't want a costly patent search.
If your idea has a patent, you can use it to file a patent for it. A patent is a legal document that describes the invention and the process for protecting it. InventHelp is a valuable resource for inventors. Their attorneys can help you quickly get your idea approved. This process can be overwhelming, so it's important to hire an experienced invention lawyer. They can help you through the patent application and ensure that your invention gets to market.
Invention help services are available to help you with your idea. You can hire an IP attorney to run a patent search on your behalf. You should not be tempted to pay a monthly fee or to con you. Patent protection may be necessary if your idea is original. Patents are free. Inventions can be patented if their creators have the right tools. This is one of the most crucial steps to protecting your invention."	0	15	0	sashagreg	why-you-should-use-an-inventhelp-service
6199	6199	petfinder-004		[]		0	8	0	kenkengoda	petfinder-004
6200	6200	efficientnetb0v2.h5		[]		0	12	0	alyonaogloblina	efficientnetb0v2h5
6201	6201	CT Data		[]		1	16	0	ajaynandam	ct-data
6202	6202	textstat		[]		0	21	0	yaremamishchenko	textstat
6203	6203	EfficientNetB0		[]		0	7	0	alyonaogloblina	efficientnetb0
6204	6204	GDP per capita: Finland, Norway, Sweden (2015-19)	Relevant for TPS Jan 2022 competition	['economics', 'tabular data']	"Context
Relevant for the Tabular Playground Series January 2022 competition. 
Content
A csv file containing GDP per capita of Nordic countries between 2015-2019. 
Sources
Website: https://www.macrotrends.net/. In particular:
Finland: GDP per capita.
Norway: GDP per capita.
Sweden: GDP per capita.
Inspiration
Carl McBride Ellis has a similar dataset for GDP of each country."	196	1116	17	samuelcortinhas	gdp-per-capita-finland-norway-sweden-201519
6205	6205	Tennis Players Ranks Prediction Using ATP Elo	Rank Prediction using different Court's Elo	['sports', 'data visualization', 'data analytics', 'tabular data', 'text data']	"Content
Given a Dataset has Elo ratings of worldwide players, with their respective Elos on different types of courts.
Your Primary Objective is to Create A Rank Prdiction Model. Which can predict the Rank of Player Based on their Elo Rank."	21	195	2	anupangadi	tennis-players-ranks-prediction-using-atp-elo
6206	6206	French nuclear reactors availability (2015-2021)	Capacity available for each of the 58 French reactors with a 30 minutes timestep	['europe', 'science and technology', 'energy', 'tabular data', 'electricity']	"Context
As power prices skyrocketed in December 2021, the low availability of the French nuclear fleet was pointed out as one as one of the factors causing tensions on European electricity markets.
Questions about the reliability of the ageing French nuclear power plant fleet are not new, they have been aggravated in the last 2 years by the Covid-19 epidemic. This dataset provide an exhaustive view of the availability of all commercial nuclear reactors in France over the last 7 years, based on regulatory disclosures by the operator, EDF. 
Content
The dataset contains 3 files:
French_nuclear_reactors_availability.csv provide the maximum capacity in megawatts available from each of the 58 French nuclear reactors from Jan, 1 2015 to Dec, 31 2021 with a 30 minutes timestep.
French_nuclear_reactors_availability_full.csv contains the same data as the previous file with additional details:
- Power_available: maximum capacity available in megawatt
- Derating: difference between the nominal capacity of the reactor and the maximum capacity available in megawatt
- Unavailability_type: each event is reported by the operator either as ""planifié""/planned (e.g.: maintenance, refueling...) or ""fortuit""/forced (e.g.: failure, strike...). This classification is not always accurate.
- Unavailability_cause: cause of the unavailability as reported by the operator: ""Défaillance""/failure, ""Maintenance prévisionnelle""/planned maintenance, ""Arrêt / Fermeture""/permanent shutdown or ""Informations complémentaires""/other (see Additional_info)
- Additional_info: any additional information provided by the operator, in French, English or both
French_nuclear_reactors_list.csv provides various informations on the 58 French nuclear reactors:
- Plant and reactor name
- Position (latitude, longitude)
- Model: Reactor model (more details)
- Rated_power: Reactor nominal capacity in megawatt
- Grid_connection: Date of first grid connection
- Cooling: type of cooling system (direct or indirect, more details)
- Cooling_source: type of water body used for cooling (river or sea)
Two reactors were permanently shut down during the period covered by these data: Fessenheim 1 and 2, respectively on Feb, 22 2020 and Jun, 29 2020. 
Sources and acknowledgements
Since 2014, the EU's Regulation on Wholesale Energy Market Integrity and Transparency (REMIT) require power plant operators to disclose the availability of their assets. Reactors availability is derived from these disclosures, available on EDF's transparency platform.
Example of an unavailability report:
Informations about nuclear reactors are based on the World Nuclear Association's reactors database.
Cover picture: Spiritrespect, CC BY-SA 3.0, via Wikimedia Commons
Inspiration
The unavailability of a nuclear reactor can have numerous and complex causes. For example, it can be linked to maintenance operations exceeding the planned schedule, to a deliberate choice by the operator to save fuel, to environmental conditions... As a result, it is difficult for electricity market stakeholders to forecast them accurately.
The analysis of these data and their combination with other data sets can perhaps help to better understand, anticipate and, possibly, improve the availability of nuclear reactors in France or elsewhere.
Nuclear energy provides more than two thirds of the electricity in France and a quarter in the EU, so the economic and environmental stakes are high."	82	1029	12	thrasy	french-nuclear-reactors-availability-20152021
6207	6207	embeddeddata		[]		0	8	0	ayushsingh488	embeddeddata
6208	6208	P_Pr_k=5swin244_selu		[]		0	12	0	durgammohanpranay99	p-pr-k5swin244-selu
6209	6209	ozayokumus		[]		0	4	0	konya4242	ozayokumus
6210	6210	classes		['education']		1	15	1	fujianhua	classes
6211	6211	r2gen-base-data		[]		1	17	0	hemakahansika	r2genbasedata
6212	6212	Electronics_Sales_Data_2019		['data cleaning', 'data visualization', 'data analytics', 'matplotlib', 'pandas']	In this project, I've tried to analyze and answer business questions about 12 months' worth of sales data. The data contains hundreds of thousands of electronics store purchases broken down by month, product type, cost, purchase address, etc.	13	78	1	ibrahimothman	electronics-sales-data-2019
6213	6213	Ask Reddit	Questions and Answers from r/AskReddit subreddit	['nlp', 'text data', 'online communities', 'social networks']	"Context
AskReddit ... (r/AskReddit), is one of the largest Reddit communities. Posts and comments are about questions and answers on a variety of random topics. The collection is a rich source of all kind of topics in English language.
The data is not filtered.
Collection
Reddit posts from subreddit r/AskReddit, downloaded from https://www.reddit.com/r/AskReddit using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
* title - relevant for posts
 score - relevant for posts - based on impact, number of comments
 id - unique id for posts/comments
 url - relevant for posts - url of post thread
 commns_num - relevant for post - number of comments to this post
 created - date of creation
 body - relevant for posts/comments - text of the post or comment
* timestamp - timestamp
Acknowledgements
All merit goes to the contributors to the posts of subreddit r/AskReddit. I only collects them daily.
Inspiration
You can use the data to:
* Perform sentiment analysis;
* Identify discussion topics;"	37	2158	9	gpreda	ask-reddit
6214	6214	Reddit Book Suggestions	Posts and comments from r/booksuggestions subreddit	['nlp', 'text data', 'online communities', 'social networks']	"Context
Book Suggestions (r/booksuggestions), is a subreddit where people discuss about books.
Collection
Reddit posts from subreddit booksuggestions , downloaded from https://www.reddit.com/r/booksuggestions/ using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
title - relevant for posts
score - relevant for posts - based on impact, number of comments
id - unique id for posts/comments
url - relevant for posts - url of post thread
commns_num - relevant for post - number of comments to this post
created - date of creation 
body - relevant for posts/comments - text of the post or comment
timestamp - timestamp"	11	219	1	gpreda	reddit-book-suggestions
6215	6215	Models		['clothing and accessories']		0	9	0	ayushsingh488	models
6216	6216	Fingleprint-recognition-data		[]		0	18	0	twistww	fingleprint-recognition-data
6217	6217	jigsawexp101		[]		1	32	0	gyanendradas	jigsawexp101
6218	6218	DataArabicHandwritten		[]		1	62	0	nurulaayunnisa	dataarabichandwritten
6219	6219	unet code		[]		0	2	0	snchen	unet-code
6220	6220	Data Fusion Contest 19		[]		0	14	0	snchen	data-fusion-contest-19
6221	6221	Daily Exchange Rates per Euro 1999-2022	time series of foreign exchange rates per 1 Euro	['europe', 'business', 'finance', 'banking', 'currencies and foreign exchange']	"21+ years of the euro € 💶
&gt; 04 Jan 1999 - 10 Jan 2022
It wasn't until 1999 that the euro really began its journey, when 11 countries (Austria, Belgium, Finland, France, Germany, Ireland, Italy, Luxembourg, the Netherlands, Portugal and Spain) fixed their exchange rates and created a new currency with monetary policy passed to the European Central Bank. Today euro is 20+ years old.
Content
Reference rates are euro foreign exchange rates observed on major foreign exchange trading venues at a certain point in time = they are the price of one currency in terms of another currency. The rates are usually updated around 16:00 CET on every working day, except on TARGET closing days. 
Dataset contains date and Euro rate corresponding to Argentine peso, Australian dollar, Bulgarian lev, Brazilian real, Canadian dollar, Swiss franc, Chinese yuan renminbi, Cypriot pound, Czech koruna, Danish krone, Algerian dinar, Estonian kroon, UK pound sterling, Greek drachma, Hong Kong dollar, Croatian kuna, Hungarian forint, Indonesian rupiah, Israeli shekel, Indian rupee, Iceland krona, Japanese yen, Korean won, Lithuanian litas, Latvian lats, Moroccan dirham, Maltese lira, Mexican peso, Malaysian ringgit, Norwegian krone, New Zealand dollar, Philippine peso, Polish zloty, Romanian leu, Russian rouble, Swedish krona, Singapore dollar, Slovenian tolar, Slovak koruna, Thai baht, Turkish lira, New Taiwan dollar, US dollar, South African rand.
Some currency in the list doesn't exist anymore; it was replaced by the Euro €: Cypriot pound (2007), Estonian kroon (2011), Greek drachma (2002), Lithuanian litas (2015), Latvian lats (2014), Maltese lira (2008), Slovenian tolar (2007), Slovak koruna (2009). 
Bulgarian lev since 2002 is pegged to the Euro: 1 € = 1.9558 leva.
Acknowledgements
All data provided by European Central Bank Statistical Data WareHouse, EXR - Exchange Rates.
Dataset is versioned and stays on weekly update."	1754	12370	35	lsind18	euro-exchange-daily-rates-19992020
6222	6222	silgiler		[]		0	2	0	konya4242	silgiler
6223	6223	ISUIdata		[]		1	16	0	kaggelliumin	isuidata
6224	6224	MBCNN_model		[]		0	3	0	testtraining	mbcnn-model
6225	6225	using catboost		[]		0	13	0	toshikiharaguchi	using-catboost
6226	6226	submission		['business']		0	30	0	iiglace	submission
6227	6227	word count		[]		0	11	0	prabhakarbonela	word-count
6228	6228	MLB team batting data		['baseball']		1	12	0	sanghyunkim123	mlb-team-batting-data
6229	6229	pedestrians		[]		0	7	0	riccijaneabao	pedestrians
6230	6230	swin_large_patch4_window7_224_longaxis_bayes_2022		[]		1	107	0	nyeongmin	swin-large-patch4-window7-224-longaxis-bayes-2022
6231	6231	stratified shuffle split oof py	stratifiedShuffleSplit OOF ensemble function 	['music', 'beginner', 'ensembling', 'optimization']		0	16	4	rhythmcam	stratified-shuffle-split-oof-py
6232	6232	Probolinggo_City	civilization datasets from Statistics Indonesia	['social science', 'demographics', 'beginner', 'tabular data']	"Demographic is one of the important sectors in government's planning to become wealthy by deciding a good policy.
So, there's a story behind every dataset and here's your opportunity to share yours."	3	34	2	yusufnurfananiikhsan	probolinggo-city
6233	6233	petfinder-003		[]		0	4	0	kenkengoda	petfinder-003
6234	6234	Sign Language Dataset	Indonesia Jakarta Central	[]		0	30	0	creatifdevindonesia	sign-language-datset
6235	6235	Indian Gallantry Awards	A exhaustive dataset of all the gallantry award winners of India.	['india', 'beginner', 'tabular data']	"Context
A dataset that covers the achievements of Bravehearts of Indian Forces.
Content
The dataset contains 6 Files -&gt; pvc , mvc, vc, ac, kc, sc.
They are Param Vir Chakra, Mahavir Chakra, Vir Chakra for war time awards
And Ashoka Chakra,, Kirti Chakra and Shaurya Chakra for Peacetime."	28	331	11	anshulmehtakaggl	indian-gallantry-awards
6236	6236	Credit EDA Case Study Data		['universities and colleges', 'beginner', 'exploratory data analysis']		28	213	2	adityamishra0708	credit-eda-case-study-data
6237	6237	Cyclistic dataset | Capstone  | Case Study 1	Bike-Sharing company dataset Jan 21 - Dec 21	[]		0	51	0	asfakkaggle	case-study-cyclistic
6238	6238	data_for_maphia		[]		0	17	0	easyhardhoon	data-for-maphia
6239	6239	feedbackexp103		[]		0	34	0	gyanendradas	feedbackexp103
6240	6240	Jobs by Industry, Top 35 US Metro Areas	A breakdown by metro area of how many jobs fall into 20 industry categories	['united states', 'north america', 'employment']	"Context
I gathered this data in order to compare industry composition between metro areas. 
Content
This is 2019 data from OnTheMap. On Sheet1 you'll find 35 US Metro areas and the industry composition for each. Numbers reflect ""All Jobs"" in a CBSA. Job totals are given as raw number, not percentage.
Acknowledgements
See OnTheMap for source data.
Inspiration
There's plenty of good stuff to be done with this data. I build a similarity index to compare the metros to one other, but you may find interesting applications by grouping by region or adding other variables to what's given."	9	85	2	walterogozaly	jobs-by-industry-top-35-us-metro-areas
6241	6241	datset2		[]		0	12	0	niteshsinghr	dataset2
6242	6242	DeepDream		[]		1	13	0	ravinash218	deepdream
6243	6243	Udacity Courses Dataset 2021	Data collected on Courses available on Udacity	['websites', 'education', 'recommender systems', 'text data', 'online communities']	"Context
2021 has seen a boom in the MOOCs due to the Covid-19 Pandemic. With the availability of numerous paid and free resources on the internet, it becomes overwhelming for students to learn new skills. Therefore, this dataset can be used to create Recommender Systems and recommend courses to students based on the Skills and Difficulty Level entered by the student. The Course Link is also provided, which can be offered by the Recommender System for easy access.
Content
This dataset was scraped off the publicly available information on the Udacity website in September 2021 and manually entered in the case where the data was improperly scraped. It can be used in Recommender Systems to promote Udacity courses based on the Difficulty Level and the Skills needed. 
Acknowledgements
The dataset was obtained from the publicly available information on the Udacity website. I do not own any information."	59	554	15	khusheekapoor	udacity-courses-dataset-2021
6244	6244	annots_reel		[]		0	11	1	nischaydnk	annots-reel
6245	6245	Hemorrhage Abnormal Densenet121 fold4		[]		1	18	0	hamedghavami	hemorrhage-abnormal-densenet121-fold4
6246	6246	Tennis ATP rankings based on ELO scores	Predict and analyize tennis ranking based on various aspects of  ELO  	['tennis', 'sports', 'classification', 'neural networks', 'regression']	"The Association of Tennis Professionals (ATP) is the governing body of the men's professional tennis circuits – the ATP Tour, the ATP Challenger Tour and the ATP Champions Tour.
It was formed in September 1972 by Donald Dell, Jack Kramer, and Cliff Drysdale to protect the interests of professional tennis players, and Drysdale became the first President. Since 1990, the association has organized the ATP Tour, the worldwide tennis tour for men and linked the title of the tour with the organization's name. It is the governing body of men's professional tennis. In 1990 the organization was called the ATP Tour, which was renamed in 2001 as just ATP and the tour being called ATP Tour. In 2009 the name of the tour was changed again and was known as the ATP World Tour, but changed again to the ATP Tour by 2019.1 It is an evolution of the tour competitions previously known as Grand Prix tennis tournaments and World Championship Tennis (WCT).The ATP's global headquarters are in London. ATP Americas is based in Ponte Vedra Beach, Florida; ATP Europe is headquartered in Monaco; and ATP International, which covers Africa, Asia and Australasia, is based in Sydney, Australia.
Elo rating considers opponent rating as well, meaning wins over quality opponents are worth more than wins over lesser rated players. Ultimate Tennis Statistics uses sophisticated tennis-customized Elo rating formula."	138	1086	18	ramjasmaurya	tennis-atp-rankings-based-on-elo-scores
6247	6247	URack Store Sales Prediction		['business']		1	19	0	rahulnaher	urack-store-sales-prediction
6248	6248	train_dumpdata_tod_schema		[]		0	12	0	daoduonggugobert2	train-dumpdata-tod-schema
6249	6249	Hospital Tools Dataset 		['hospitals and treatment centers']		2	23	0	creatifdevindonesia	hospital-tools-dataset
6250	6250	Jigsaw Toxic Comment - Clean Data		[]		7	33	0	harits	jigsaw-toxic-comment-clean-data
6251	6251	2021_CCF中文命名实体识别算法鲁棒性评测		[]		0	34	0	niubi666	2021-ccf
6252	6252	PetFinder: TF with TPU 512x512		[]		0	123	0	lhagiimn	petfinder-tf-with-tpu-512x512
6253	6253	Temperature change	Global Warming, Temperature Change, Climate Change	['global', 'earth and nature', 'atmospheric science', 'environment', 'weather and climate', 'health']	"Context
Data description
The FAOSTAT Temperature Change domain disseminates statistics of mean surface temperature change by country, with annual updates. The current dissemination covers the period 1961–2019. Statistics are available for monthly, seasonal and annual mean temperature anomalies, i.e., temperature change with respect to a baseline climatology, corresponding to the period 1951–1980. The standard deviation of the temperature change of the baseline methodology is also available. Data are based on the publicly available GISTEMP data, the Global Surface Temperature Change data distributed by the National Aeronautics and Space Administration Goddard Institute for Space Studies (NASA-GISS).
Content
Statistical concepts and definitions
Statistical standards: Data in the Temperature Change domain are not an explicit SEEA variable. Nonetheless, country and regional calculations employ a definition of “Land area” consistent with SEEA Land Use definitions, specifically SEEA CF Table 5.11 “Land Use Classification” and SEEA AFF Table 4.8, “Physical asset account for land use.” The Temperature Change domain of the FAOSTAT Agri-Environmental Indicators section is compliant with the Framework for the Development of Environmental Statistics (FDES 2013), contributing to FDES Component 1: Environmental Conditions and Quality, Sub-component 1.1: Physical Conditions, Topic 1.1.1: Atmosphere, climate and weather, Core set/ Tier 1 statistics a.1.
Statistical unit: Countries and Territories.
Statistical population: Countries and Territories.
Reference area: Area of all the Countries and Territories of the world. In 2019: 190 countries and 37 other territorial entities.
Code - reference area: FAOSTAT, M49, ISO2 and ISO3 (http://www.fao.org/faostat/en/#definitions). FAO Global Administrative Unit Layer (GAUL National level – reference year 2014. FAO Geospatial data repository GeoNetwork. Permanent address: http://www.fao.org:80/geonetwork?uuid=f7e7adb0-88fd-11da-a88f-000d939bc5d8.
Code - Number of countries/areas covered: In 2019: 190 countries and 37 other territorial entities.
Time coverage: 1961-2020
Periodicity: Monthly, Seasonal, Yearly
Base period: 1951-1980
Unit of Measure: Celsius degrees °C
Reference period: Months, Seasons, Meteorological year
Acknowledgements
Documentation on methodology: Details on the methodology can be accessed at the Related Documents section of the Temperature Change (ET) domain in the Agri-Environmental Indicators section of FAOSTAT.
Quality documentation: For more information on the methods, coverage, accuracy and limitations of the Temperature Change dataset please refer to the NASA GISTEMP website: https://data.giss.nasa.gov/gistemp/
                                                                                                                                                    Source: http://www.fao.org/faostat/en/#data/ET/metadata
Inspiration
Climate change is one of the important issues that face the world in this technological era. The best proof of this situation is the historical temperature change.  You can investigate if any hope there is for stopping global warming  :)
Can you find any correlation between temperature change and any other variable? 
(Using ISO3 codes for merging any other countries' data sets possible.)
Prediction of temperature change: there is also an overall world temperature change in the country list as 'World'."	8192	55748	177	sevgisarac	temperature-change
6254	6254	DatosSalario		[]		0	18	0	fernandobordi	datossalario
6255	6255	SPY Intraday OHLC	Intraday SPY ETF trading data as date, time, open, high, low, close and volume.	['investing']		4	40	0	abidou	spy-intraday-ohlc
6256	6256	resultall		[]		0	11	0	yutotakaki	resultall
6257	6257	Chart_2 Data		[]		0	21	0	anthonyoswald	chart-2-data
6258	6258	noise_dataset		[]		17	22	0	xiaojin712	noise-dataset
6259	6259	vitp16w		[]		0	16	0	ironluu	vitp16w
6260	6260	time_series_oof_function_py	time seriese off function	['beginner', 'time series analysis', 'optimization']		0	23	1	rhythmcam	time-series-oof-function-py
6261	6261	selected paw model		[]		0	7	0	darksouls4	selected-paw-model
6262	6262	/kaggle/input/g-research-crypto-forecasting/model		['computer science']		0	24	0	hayden234	kaggleinputgresearchcryptoforecastingmodel
6263	6263	New-Year-Address-Comments-YouTube	Russian comments by youtube users on the videos by RT and Ekaterina Shulman	['russia', 'nlp', 'text data', 'social issues and advocacy', 'social networks']	"Контекст (Context)
Сбор комментариев под новогодними обращениями осуществлялся с целью обучения Seq2Seq модели для генерации политически-ангажированных комментариев пользователей из России. Кроме того, данные могут использоваться для обучения алгоритмов классификации текста по его эмоциональной окраске (однако в данном случае нужно будет провести дополнительную работу по разметке данных или использовать другие подходы, такие как word2vec).
English:
The collection of comments under the New Year Addresses was carried out in order to train the Seq2Seq model to generate politically-biased comments from russian YouTube users. In addition, the data can be used to train algorithms for classifying text according to type of emotions it expresses (however, in this case, additional work will need to be done to mark up the data or use other approaches, such as word2vec).
Содержимое (Content)
Внутри есть два файла: Putin.txt и Schulman.txt - оба содержат комментарии под видео с новогодними обращениями (текст имеет кодировку utf-8, то есть в нём присутствуют такие символы как, например, Emoji).
English:
There are two files inside: Putin.txt and Schulman.txt - both contain comments for videos with New Year Adresses (the text is utf-8 encoded, so it contains special characters such as, for example, Emoji).
Благодарности (Acknowledgements)
В данном разделе я хотел упомянуть, что данные были собраны, благодаря материалу, опубликованному здесь. 
Выражаю благодарность за данный материал François St-Amant
English:
I wanted to mention that the data was collected thanks to the material posted [here] (https://towardsdatascience.com/how-to-scrape-youtube-comments-with-python-61ff197115d4).
Thanks for this material to [François St-Amant] (https://francoisstamant.medium.com/membership)
Источники вдохновения (Inspiration)
Проект, в рамках которого проводился сбор данных, был вдохновлен статьей Nikitius_Ivanov Сравнительный анализ тональности комментариев в YouTube.
English:
The project, which data was collected for, was inspired by the Nikitius_Ivanov article ""Comparative analysis of YouTube comment sentiment"" on habr.com."	0	56	0	igorlukhnev	newyearaddresscommentsyoutube
6264	6264	cots_exp003_yolov5_fold0		[]		3	22	0	tiandaye	cots-exp003-yolov5-fold0
6265	6265	Art and Artists from the Museum of Modern Art	Over 100,000 artworks	['universities and colleges', 'museums', 'education', 'science and technology']	"Content
In this dataset, you will find information regarding over 10,000 artists whose artwork is displayed in Chicago's Museum of Modern Art.
Acknowledgements
This dataset comes from https://data.world/moma/collection."	364	2738	25	rishidamarla	art-and-artists-from-the-museum-of-modern-art
6266	6266	A1 Data	Please refer to the assignment description for more information	[]		3	13	0	mcgillcomp599w22	a1-data
6267	6267	valid_dataset		[]		0	21	0	navidmoradi	valid-dataset
6268	6268	Notas Taquigráficas da CPI da Pandemia	Falas dos senadores durante as sessões da CPI	['brazil', 'politics', 'text data']	"Contexto
Este dataset tem como sua origem a popularização da CPI da Pandemia nas redes sociais e uma menção do Senador Randolfe às notas taquigráficas. Considerando-se a importância desta comissão para a discussão de temas tão importantes para a população brasileira, resolvi dar uma pequena contribuição para análises políticas e fiscalização dos representantes a partir da disponibilização destes dados em um dataset acessível para todas e todos.
Espero que estes dados sirvam para podermos acompanhar o posicionamento dos parlamentares neste espaço e entender as ""narrativas"" articuladas pelos senadores e depoentes. Vai vendo, Brasil! 
Conteúdo
Estes dados foram coletados a partir de uma raspagem de dados das notas taquigráficas na seção da CPI da Pandemia. Utilizou-se o rvest e arrumei os dados complementares (partido, região), já presentes nas notas, com a ajuda do stringr. Para ter acesso ao código de raspagem de dados, criei um repositório em meu github e teve-se como base da coleta de dados a seção da comissão no site do Senado.
O dataset está organizando pelas falas dos senadores, sendo estas representadas em cada linha. Outros dados estão dispostos nas outras colunas, inspirando-nos na organização dos dados propostos pelos organizadores do tidyr.
No presente momento, coletei os dados do que consideraria a ""primeira temporada"" da CPI, antes do depoimento dos irmãos Miranda (26/06/21). Após o fim da comissão, atualizarei os dados com as sessões restantes.
Agradecimentos
Este dataset só foi possível graças aos servidores do Senado Federal, que trabalham diuturnamente para que possamos acompanhar as atividades na CPI. Fica aqui meus agradecimentos a todos os servidores de carreira que nos garantem o acesso aos nossos direitos e resistem contra a tergiversação dos bens públicos.
Problemas de pesquisa
Esta base de dados é adequada para quem quiser realizar análises textuais. Pode-se perguntar, por exemplo, quais são os principais temas debatidos na CPI, como esses senadores se posicionam sobre diferentes temas, etc."	4	126	0	gabrielpistelli	notas-taquigrficas-da-cpi-da-covid
6269	6269	cats_and_dogs_breeds_model		[]		0	6	0	jquinteiro	cats-and-dogs-breeds-model
6270	6270	Cyclistics Bike Share Data	Nov. 1, 2020 - Nov. 4, 2021 -- unified, cleaned, and expanded from originals	['retail and shopping']		0	17	0	alexanderhwhite	cyclistics-bike-share-data-nov-2020-nov-2021
6271	6271	permsMat		[]		0	8	0	singularity1012	permsmat
6272	6272	COVID-19 data		[]		4	22	0	mostalotfy	covid19-data
6273	6273	yolo_format		[]		5	31	1	ferlockx	yolo-format
6274	6274	FDL_UAV_flooded areas		[]		0	12	0	a1996tomousyang	fdl-uav-flooded-areas
6275	6275	petf_swin_l_508_model_384_cutmix10_mfixed		[]		0	4	0	nhac43	petf-swin-l-508-model-384-cutmix10-mfixed
6276	6276	Shenzhen_and_MontgomeryDataset		[]		0	11	0	abdullahbaloch	shenzhen-and-montgomerydataset
6277	6277	Optiver public leaderboard		[]		19	214	5	slawekbiel	optiver-public-leaderboard
6278	6278	Bairros de São Paulo	Coordenadas dos distritos de São Paulo	['brazil', 'geography', 'beginner', 'tabular data']	"Bairros de São Paulo
Fala rapaziadinha, segue o dataset contendo as coordenadas do perímetro de cada distrito de São Paulo - Capital. Junto com o dataset, tem os arquivos de plot, para você explorar no google earth, arcgis, QGIS ou outro software de geoprocessamento.
Passo a Passo:
-Primeiramente baixei os shapefiles no site de dados abertos da Prefeitura de São Paulo;
-Depois eu importei o arquivo no QGIS e fiz uma conversão para GEOJSON (é melhor para manipular no Python);
-Depois salvei no google drive e fiz as manipulações no google colaboratory com Python;
Utilidade:
-Bem eu estava melhorando um dataset de favelas na cidade de São Paulo, e estava querendo colocar uma coluna de Distrito. Essa já foi uma utilidade;
-Se você trabalha com logística como eu, é importante ter os shapefiles para fazer estudo de região. O arquivo CSV ajuda bastante e é o maior problema, pois converter os pontos de coordenadas em de um shapefile em um CSV, é bem chatinho e provavelmente você só vai conseguir usando alguma linguagem de programação;
-Desenvolvedores. Se você não usar nenhum tipo de API, bem provavelmente você vai precisar de um database com as coordenadas dos perímetros das regiões para colocar o nome do Distrito nos endereços.
Qualquer dúvida é só me chamar!
(11) 94937-0306 |Whatsapp|
marcus.rodrigues4003@gmail.com |Email|"	4	146	5	markfinn1	bairros-de-so-paulo
6279	6279	movielens20M		[]		1	24	0	superrock	movielens20m
6280	6280	CommonLit Various		[]		2	350	1	markwijkhuizen	commonlit-various
6281	6281	ruddit_pairs_dataset		[]		0	3	0	panser	ruddit-pairs-dataset
6282	6282	Calculating Flow Rate from Pressure Measurements	We want to predict the flow using the pressure-in and pressure-out	['earth and nature', 'intermediate', 'text data', 'regression', 'matplotlib', 'pandas']		0	124	1	mohsenvahabpour	calculating-flow-rate-from-pressure-measurements
6283	6283	UrbanSound_8732		[]		30	57	0	xiaojin712	urbansound-8732
6284	6284	UpsClassifierRobert		[]		0	18	0	alinflorin	upsclassifierrobert
6285	6285	flair_	Install Flair Offline	[]	"USAGE:
&gt; import sys
&gt; pathtoflair = '../input/flair/offline_files/'
&gt;sys.path.append(pathtoflair)
Then import the functions you require
&gt;from flair.data import Sentence
&gt;from flair.models.text_regression_model import TextRegressor"	0	38	0	danielbruintjies	flair
6286	6286	Train_data_upscaled		[]		0	9	0	yannistsiotas	train-data-upscaled
6287	6287	sccl11		[]		2	16	0	urospetricevic	sccl11
6288	6288	tf-gbr-yoloweights		['sports']		0	33	0	iankabra	tfgbryoloweights
6289	6289	swin_large_224_my_strategy		[]		0	3	0	darkravager	swin-large-224-my-strategy
6290	6290	Dogs50		[]		0	13	0	wastingtime	dogs50
6291	6291	Efficientnet tf utilities		['business']		0	2	0	ks2019	efficientnet-tf-utilities
6292	6292	Lovely pet		['arts and entertainment']		1	24	0	nizhen	lovely-pet
6293	6293	glovedata		[]		2	3	1	liuyunqing	glovedata
6294	6294	Bellabeat - Capstone	Case Study 2 based on FitBit Fitness Tracker Data	['health and fitness', 'exercise', 'data analytics', 'dplyr', 'ggplot2', 'r']	"Business task
Provide a high-level recommendation to help guide Bellabeat’s marketing strategy to unlock new growth opportunities.
Key stakeholders
Urška Sršen, cofounder and Chief Creative Officer of Bellabeat
Sando Mur, Mathematician and Bellabeat’s cofounder
Data sources used
https://www.kaggle.com/arashnic/fitbit 
FitBit Fitness Tracker Data (CC0: Public Domain, dataset made available through Mobius)
Documentation of any cleaning or manipulation of data
RStudio Cloud is the best tool for this project due to data size.
Packages used
install.packages(""lubridate"")
library(lubridate)
library(ggplot2)
install.packages(""dplyr"")
 library(dplyr)
Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	5	89	0	vmb2021	bellabeat-capstone
6295	6295	Comcast Telecom Complaints Dataset	Comcast Telecom Complaints Dataset	['mobile and wireless', 'beginner', 'nlp', 'tabular data', 'nltk']	"Context
Estimation of Comcast Customer Top Complaints.
Acknowledgements
Kaggle Datasets"	87	688	5	yasserh	comcast-telecom-complaints
6296	6296	muertes_chile_2016-2021	Deaths in Chile between 2016 and 2021 released by chilean DEIS	[]		8	45	0	felipeea	muertes-chile-20162021
6297	6297	cal205		[]		0	7	0	avik18	cal205
6298	6298	covid_analysis	the data se contains covid/non-covid data from 2018 to 2020	['statistical analysis', 'tabular data', 'insurance', 'covid19']	"Context
Life insurance claims for past 3 years
Content
Analyze the data  to compare the covid and non covid records,also needs to check if there has been increment in other cause of death during covid times.
&gt; checkall the causeofdth,if neccessry group the similar ones together
5&gt; check the count of covid and non covid insid
6&gt; check the sex of covid and non covid deaths
7&gt; check which states were badly hit by covid and non covid deaths
8&gt; check the % of retro vs assumed
9&gt; monthwise analysis of covid death in 2020
10&gt;compare the covid y ear 2020 with previous years 2018 and 2019
column description:
COUNT(C.CLAIMID): number of claims 
SEX: sex of the insured, U for unidentified
Birthdate
DOD: Date of Death
Issage : policy  issue age with the insurance  company 
ASMDRETRO: A: we have to pay the client,R: we need to get payment from the other companies
Staters: State of Death
CAUSEOFDTH:  cause of death,COV is for Covid
setup_date: date when Death notification was received
REQAMT: Amount to be paid or received"	1	59	0	mamta1004	covid-analysis
6299	6299	Face_Forensics_croped_C40(100Videos)		[]		0	8	0	amr1996	face-forensics-croped-c40100videos
6300	6300	Face_Forensics_croped_raw(100Videos)		[]		7	18	0	amr1996	face-forensics-croped-raw100videos
6301	6301	KeyCars	Photos of car keys with LabelImg	['beginner', 'image data']		1	21	2	asisheriberto	keycars
6302	6302	LJSpeech		[]		0	22	0	fag9897	ljspeech
6303	6303	Reddit r/AITA post and comments	This data contains posts from the subreddit r/AITA with their comments.	['video games']		0	4	0	oliverposewitz	reddit-raita-post-and-comments
6304	6304	LCBO data	Webscraped data for Canadian Alcohol	[]		15	614	2	deanpb	lcbo-data
6305	6305	test_data_first		[]		0	6	0	mittamaria	test-data-first
6306	6306	datasciencestages		[]		0	11	0	abrahamanderson	datasciencestages
6307	6307	train data		[]		0	18	0	christopherchaves	train-data
6308	6308	us cities dataset		[]		3	16	0	abrahamanderson	us-cities-dataset
6309	6309	ADP_KR_p4	Advanced Data Analytics Professional (KOREA)	[]	"ADP 시험대비
문제 링크
(python) : https://www.kaggle.com/kukuroo3/problem4-python  
(R)          : https://www.kaggle.com/kukuroo3/problem4-r   
English translation version will be prepared soon"	12	352	3	kukuroo3	adp-kr-p4
6310	6310	used thermodynamic in engineer works		['physics']		5	40	0	fadzleejaslan	used-thermodynamic-in-engineer-works
6311	6311	Practica3ML		[]		1	28	0	mletsisi2122grupo16	practica3ml
6312	6312	MISSING MIGRANTS (2014-2021)	Missing Migrants - 2014 to 2021 (Tracks Death & Missing Of Migrants)	['education', 'artificial intelligence', 'intermediate', 'advanced', 'public safety']	"Context
Missing Migrants Project tracks deaths of migrants, including refugees and asylum-seekers, who have died or gone missing in the process of migration towards an international destination. Please note that these data represent minimum estimates, as many deaths during migration go unrecorded
What is included in Missing Migrants Project data?
Missing Migrants Project counts migrants  who have died at the external borders of states, or in the process of migration towards an international destination, regardless of their legal status. The Project records only those migrants who die during their journey to a country different from their country of residence.
Missing Migrants Project data include the deaths of migrants who die in transportation accidents, shipwrecks, violent attacks, or due to medical complications during their journeys. It also includes the number of corpses found at border crossings that are categorized as the bodies of migrants, on the basis of belongings and/or the characteristics of the death. For instance, a death of an unidentified person might be included if the decedent is found without any identifying documentation in an area known to be on a migration route.  Deaths during migration may also be identified based on the cause of death, especially if is related to trafficking, smuggling, or means of travel such as on top of a train, in the back of a cargo truck, as a stowaway on a plane, in unseaworthy boats, or crossing a border fence.  While the location and cause of death can provide strong evidence that an unidentified decedent should be included in Missing Migrants Project data, this should always be evaluated in conjunction with migration history and trends.
What is excluded?
The count excludes deaths that occur in immigration detention facilities or after deportation to a migrant’s homeland, as well as deaths more loosely connected with migrants´ irregular status, such as those resulting from labour exploitation. Migrants who die or go missing after they are established in a new home are also not included in the data, so deaths in refugee camps or housing are excluded.  The deaths of internally displaced persons who die within their country of origin are also excluded. There remains a significant gap in knowledge and data on such deaths. Data and knowledge of the risks and vulnerabilities faced by migrants in destination countries, including death, should not be neglected, but rather tracked as a distinct category.
What sources of information are used in the Missing Migrants Project database?
The Missing Migrants Project currently gathers information from diverse sources such as official records – including from coast guards and medical examiners – and other sources such as media reports, NGOs, and surveys and interviews of migrants. In the Mediterranean region, data are relayed from relevant national authorities to IOM field missions, who then share it with the Missing Migrants Project team. Data are also obtained by IOM and other organizations that receive survivors at landing points in Italy and Greece. IOM and UNHCR also regularly coordinate to validate data on missing migrants in the Mediterranean.
Data on the United States/Mexico border are compiled based on data from U.S. county medical examiners, coroners, and sheriff’s offices, as well as media reports for deaths occurring on the Mexican side of the border. In Africa, data are obtained from media and NGOs, including the Regional Mixed Migration Secretariat and the International Red Cross/Red Crescent. The quality of the data source(s) for each incident is assessed through the ‘Source quality’ variable, which can be viewed in the data.
Across the world, the Missing Migrants Project uses social and traditional media reports to find data, which are then verified by local IOM staff whenever possible. In all cases, new entries are checked against existing records to ensure that no deaths are double-counted. In all regions, Missing Migrants Project data represent a minimum estimate of the number of migrant deaths.
To learn more about data sources, visit the thematic page on migrant deaths and disappearances in the Global Migration Data Portal. 
Content
What are the variables used in the Missing Migrants Project database?
This section presents the list of variables that constitute the Missing Migrants Project database.  While ideally, all incidents recorded would include entries for each of these variables, the challenges described above mean that this is not always possible.  The minimum information necessary to register an incident is the date of the incident, the number of dead and/or the number of missing, and the location of death. If the information is unavailable, the cell is left blank or “unknown” is recorded, as indicated in below. 
1. Web ID - An automatically generated number used to identify each unique entry in the dataset.
2. Region - Region in which an incident took place. For more about regional classifications used in the dataset, click here.
3. Incident Date - Estimated date of death. In cases where the exact date of death is not known, this variable indicates the date in which the body or bodies were found. In cases where data are drawn from surviving migrants, witnesses or other interviews, this variable is entered as the date of the death as reported by the interviewee.  At a minimum, the month and the year of death is recorded. In some cases, official statistics are not disaggregated by the incident, meaning that data is reported as a total number of deaths occurring during a certain time period. In such cases the entry is marked as a “cumulative total,” and the latest date of the range is recorded, with the full dates recorded in the comments.
4. Year - The year in which the incident occurred.
5. Reported month - The month in which the incident occurred.
6. Number dead - The total number of people confirmed dead in one incident, i.e. the number of bodies recovered.  If migrants are missing and presumed dead, such as in cases of shipwrecks, leave blank.
7. Number missing - The total number of those who are missing and are thus assumed to be dead.  This variable is generally recorded in incidents involving shipwrecks.  The number of missing is calculated by subtracting the number of bodies recovered from a shipwreck and the number of survivors from the total number of migrants reported to have been on the boat.  This number may be reported by surviving migrants or witnesses.  If no missing persons are reported, it is left blank.
8. Total dead & missing - The sum of the ‘number dead’ and ‘number missing’ variables.
9. Number of survivors - The number of migrants that survived the incident, if known. The age, gender, and country of origin of survivors are recorded in the ‘Comments’ variable if known. If unknown, it is left blank.
10. Number of females - Indicates the number of females found dead or missing. If unknown, it is left blank. This gender identification is based on a third-party interpretation of the victim's gender from information available in official documents, autopsy reports, witness testimonies, and/or media reports.
11. Number of males - Indicates the number of males found dead or missing. If unknown, it is left blank. This gender identification is based on a third-party interpretation of the victim's gender from information available in official documents, autopsy reports, witness testimonies, and/or media reports.
12. Number of children - Indicates the number of individuals under the age of 18 found dead or missing. If unknown, it is left blank.
13. Age - The age of the decedent(s). Occasionally, an estimated age range is recorded. If unknown, it is left blank.
14. Country of origin - Country of birth of the decedent. If unknown, the entry will be marked “unknown”.
15. Region of origin - Region of origin of the decedent(s). In some incidents, region of origin may be marked as “Presumed” or “(P)” if migrants travelling through that location are known to hail from a certain region. If unknown, the entry will be marked “unknown”.
16. Cause of death - The determination of conditions resulting in the migrant's death i.e. the circumstances of the event that produced the fatal injury. If unknown, the reason why is included where possible.  For example, “Unknown – skeletal remains only”, is used in cases in which only the skeleton of the decedent was found.
17. Location description - Place where the death(s) occurred or where the body or bodies were found. Nearby towns or cities or borders are included where possible. When incidents are reported in an unspecified location, this will be noted.
18. Location coordinates - Place where the death(s) occurred or where the body or bodies were found. In many regions, most notably the Mediterranean, geographic coordinates are estimated as precise locations are not often known. The location description should always be checked against the location coordinates.
19. Migration route - Name of the migrant route on which incident occurred, if known. If unknown, it is left blank.
20. UNSD geographical grouping - Geographical region in which the incident took place, as designated by the United Nations Statistics Division (UNSD) geoscheme. For more about regional classifications used in the dataset, click here.
21. Information source - Name of source of information for each incident. Multiple sources may be listed.
22. Link - Links to original reports of migrant deaths / disappearances if available. Multiple links may be listed.
23. Source quality - Incidents are ranked on a scale from 1-5 based on the source(s) of information available. Incidents ranked as level 1 are based on information from only one media source. Incidents ranked as level 2 are based on information from uncorroborated eyewitness accounts or data from survey respondents. Incidents ranked as level 3 are based on information from multiple media reports, while level 4 incidents are based on information from at least one NGO, IGO, or another humanitarian actor with direct knowledge of the incident. Incidents ranked at level 5 are based on information from official sources such as coroners, medical examiners, or government officials OR from multiple humanitarian actors.
24. Comments - Brief description narrating additional facts about the death.  If no extra information is available, this is left blank.
Acknowledgements
The downloads above are licensed under a Creative Commons Attribution 4.0 International License. This means that Missing Migrants Data free to share and adapt, as long as the appropriate attribution is given. This includes stating that the source is 'IOM's Missing Migrants Project', and indicating if changes were made to the data. Ideally, a link to this website should also be included.
For more information visit: https://missingmigrants.iom.int
Inspiration
Number of identified and unidentified Dead and Missing Migrants ?
What more information ?
Best practice implementation (model) for the benefit of Humankind ?
and . . . ."	25	141	4	methoomirza	missing-migrants-20142021
6313	6313	109populaton_numbers		[]		1	5	0	kevinwo	109populaton-numbers
6314	6314	NYC_Rolling_Sales	All Sales From December 2020 - November 2021	['retail and shopping']		1	15	0	zhengyiling	rollingsales
6315	6315	TFrecords Cartoon Faces	Tfrecords of Google's cartoon faces dataset	['art', 'people', 'people and society', 'gan', 'tensorflow']		0	21	0	brendanartley	tfrecords-cartoon-faces
6316	6316	Normalized Swin 384 Petfinder		[]		0	13	0	mithilsalunkhe	normalized-swin-384-petfinder
6317	6317	churn_svm		[]		0	23	1	kapilbhatt11	churn-svm
6318	6318	out_version_6		[]		1	38	0	colabmab	out-version-6
6319	6319	Dataset(Toys)		[]		1	21	0	kaustavroy11	datasettoys
6320	6320	3090_kqi_ex027_1_rice_bin_365		[]		0	6	0	anonamename	3090-kqi-ex027-1-rice-bin-365
6321	6321	petfinder-tfrecords-ext		[]		0	15	0	ks2019	petfinder-tfrecords-ext
6322	6322	Name Entity with Transformer output		['software']		1	12	2	lonnieqin	name-entity-with-transformer-output
6323	6323	houssam		[]		5	28	0	houssammoussa	houssam
6324	6324	Architectural Heritage Elements Image64 Dataset	Architectural Heritage Elements Dataset	['earth and nature', 'architecture']	Architectural Heritage Elements Dataset (AHE) is an image dataset for developing deep learning algorithms and specific techniques in the classification of architectural heritage images. This dataset consists of 10235 images classified in 10 categories: Altar, Apse, Bell tower, Column, Dome (inner), Dome (outer), Flying buttress, Gargoyle, Stained glass, Vault. It is inspired by the CIFAR-10 dataset but with the objective in mind of developing tools that facilitate the tasks of classifying images in the field of cultural heritage documentation. Most of the images have been obtained from Flickr and Wikimedia Commons (all of them under creative commons license).	4	29	1	ikobzev	architectural-heritage-elements-image64-dataset
6325	6325	COTS_tf		[]		0	13	0	georgeteo89	cots-tf
6326	6326	UEFA Champions League 2021/22	Statistical Information on the current state of the UCL teams	['football', 'intermediate', 'exploratory data analysis', 'data analytics', 'tabular data']	"Context
So I started out with a team to work on tasks pertaining data science and machine learning for the year. The very first task on our list was web scraping; The goal was to think of a problem that data could solve and create a dataset to address this problem. 
Thou the intuition of the kind of task the problem space should address is explained in my article on __steps to creating a proper dataset. I kind of went for an intuitive dataset, one that answers question and I wanted to answer questions on the current state of the UEFA Champions League. Question like does possession matter to winning teams??? Well that and the fact that I just love football analysis, so why not!
Content
The data was generate from the current group stage result of the UEFA Champions League and should be progressive in nature; I hope to be able to compile statistical result of every season of teams from the current year to preceding years. Unfortunately, i wasn't able to recover data on teams stats performance in previous years -- information like their overall ball possession in the season before 2021/22.
But progressively acquiring this data won't be a problem and the intuitions on what matters in champions league football would be easy to estimate.
Acknowledgements
Well I believe the energy from working with the team was of great significance to me and uefa.com where the data was gathered from using the python scrapy tool.
Inspiration
My love for tactical analysis! Although the current state of the dataset might never capture everything, being able to start from somewhere is very essential."	17	168	0	ganiyuolalekan	uefa-champions-league-202122
6327	6327	Vancouver_Airbnb_EDA		[]		0	16	0	marcelorguerra	vancouver-airbnb-eda
6328	6328	Estonia offences detected traffic supervision		['law']		1	6	0	hypnoseal	estonia-offences-detected-traffic-supervision
6329	6329	Estonia offenses against public order and property		[]		1	5	0	hypnoseal	estonia-offenses-against-public-order-and-property
6330	6330	Estonia offenses against property		['real estate']		1	9	0	hypnoseal	estonia-offenses-against-property
6331	6331	SMCO_Training		[]		0	2	0	rhishabmukherjee	smco-training
6332	6332	NHL play-by-play data	NHL play-by-play data from 2007	['hockey']	"All the data found in this directory was retrieved using the hockey_scraper python package available on pip. It should hopefully contain all NHL pbp and shift data from the 2007 season until today. 
The source code and licensing information for the hockey_scraper project can be found here - https://github.com/HarryShomer/Hockey-Scraper."	92	859	4	s903124	nhl-playbyplay-data-from-2007
6333	6333	dadosdocurso2		[]		0	21	0	vicentetoledo	dadosdocurso2
6334	6334	heartdisease		[]		1	19	0	avinav49	heartdisease
6335	6335	minist		[]		1	16	0	fengyang1988	minist
6336	6336	Gen Rocket, Inc.		['business']		0	15	0	genrocketinc	gen-rocket-inc
6337	6337	imagenet_petfinder_tf		[]		0	15	0	germanjke	imagenet-petfinder-tf
6338	6338	KION Dataset		['earth and nature']		3	60	1	asenin	kion-dataset
6339	6339	personal loan		[]		0	18	0	ahmedmansourdev1	personal-loan
6340	6340	Heat Stroke		[]		4	32	0	tahiatazin1510997643	heat-stroke
6341	6341	egohands		[]		0	5	0	chaturvediabhay24	egohands
6342	6342	pf-161-train-1-data		[]		0	9	0	makotoikeda	pf-161-train-1-data
6343	6343	Africad		[]		2	16	0	princengema	africad
6344	6344	trivia_full_filted		[]		1	21	0	vivonnn	trivia-full-filted
6345	6345	Bitcoin Mining	Evolution of country share	['research', 'global', 'beginner', 'tabular data', 'currencies and foreign exchange', 'plotly']	"Description
This dataset is provided for public use by Cambridge Centre for Alternative Finance (University of Cambridge, Judge Business School) and can be found by the link: https://ccaf.io/cbeci/mining_map
A detailed description of the methods and approaches used to calculate the indicators specified in this dataset is also available at the link.
Inspiration
The data may be of interest from the point of view of applying various data visualization techniques. Let's try to find the most informative visualization of the presented data!"	31	543	7	alekseyromanovich	evolution-of-country-share
6346	6346	1milCars		[]		0	3	0	ketelsen	1milcars
6347	6347	swin trans [Mix up] Kfold5		['arts and entertainment']		0	15	0	min7712	swin-trans-mix-up-kfold5
6348	6348	Gestures and Actions		['online communities']		0	16	0	ravinash218	gestures-and-actions
6349	6349	swin_large_224_longaxis_bayes_2023		[]		0	10	0	ttagu99	swin-large-224-longaxis-bayes-2023
6350	6350	PFM PSE		['arts and entertainment']		0	37	0	ssinyu	pfm-pse
6351	6351	petf_swin_l_508_model_384_mixup10_mfixed		[]		0	2	0	nhac43	petf-swin-l-508-model-384-mixup10-mfixed
6352	6352	train-data		['transportation']		1	10	0	dasarisreenivas	traindata
6353	6353	Cyber security training	Cyber security training | Ethical hacking course online	['education']		17	157	0	cysecon	cyber-security-training
6354	6354	test_dataset_kaggle		[]		28	1041	0	maxmar	test-dataset-kaggle
6355	6355	Swin-tiny-224		[]		0	17	0	peixiangong	swintiny224
6356	6356	VISKOMbaru		[]		0	10	0	ryanadeputrasutopo	viskombaru
6357	6357	tf_dataset		[]		0	11	0	danielemarzetti	tf-dataset
6358	6358	word couunt		[]		0	12	0	debashishghosh12	word-couunt
6359	6359	Brain-Tumor-Progression_masktumoronly		['cancer']	"This is a mask tumor only MRI image from https://www.kaggle.com/andrewmvd/brain-tumor-progression
How to Cite this Dataset:
If you use this dataset in your research, please credit the authors and the original platform that hosted this dataset:
Schmainda KM, Prah M (2018). Data from Brain-Tumor-Progression. The Cancer Imaging Archive. http://doi.org/10.7937/K9/TCIA.2018.15quzvnb
Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045-1057."	2	14	0	sosonger	braintumorprogression-masktumoronly
6360	6360	Solar Car dataset	Datset for Digital Control System 	['renewable energy']		0	69	0	danielaltanwing	solar-car-dataset
6361	6361	HousePrice1		[]		0	6	0	abhishek12121999	houseprice1
6362	6362	the effect of study duration		['education']		1	48	0	faryzen	the-effect-of-study-duration
6363	6363	CSGO Body and head Detect 	Body and head Detect 	['games', 'computer vision', 'image data', 'tensorflow', 'pytorch']		4	47	1	kaln27	csgo-body-and-head-detect
6364	6364	ODI TEst		['cricket']		0	3	0	rashedsabbir	odi-test
6365	6365	remote sensing dataset		['earth and nature']		2	43	0	essea0	remote-sensing-dataset
6366	6366	maestro_h5_files		[]		0	15	0	kunfang98927	maestro-h5-files
6367	6367	Photo_livrable3		[]		2	12	0	alexandrevivier	photo-livrable3
6368	6368	face_database		[]		0	11	0	kundurutharun	face-database
6369	6369	timm_pytorch		[]		0	5	0	nizhen	timm-pytorch
6370	6370	longformer dataset processed		[]		0	5	0	mdman123	longformer-dataset-processed
6371	6371	Health Care Analytics	Predicting Patient Outcome	['healthcare', 'health', 'data cleaning', 'ensembling', 'regression']	"Context
Part of Janatahack Hackathon in Analytics Vidhya
Content
The healthcare sector has long been an early adopter of and benefited greatly from technological advances. These days, machine learning plays a key role in many health-related realms, including the development of new medical procedures, the handling of patient data, health camps and records, and the treatment of chronic diseases. 
MedCamp organizes health camps in several cities with low work life balance. They reach out to working people and ask them to register for these health camps. For those who attend, MedCamp provides them facility to undergo health checks or increase awareness by visiting various stalls (depending on the format of camp). 
MedCamp has conducted 65 such events over a period of 4 years and they see a high drop off between “Registration” and number of people taking tests at the Camps. In last 4 years, they have stored data of ~110,000 registrations they have done.
One of the huge costs in arranging these camps is the amount of inventory you need to carry. If you carry more than required inventory, you incur unnecessarily high costs. On the other hand, if you carry less than required inventory for conducting these medical checks, people end up having bad experience.
The Process:
MedCamp employees / volunteers reach out to people and drive registrations.
During the camp, People who “ShowUp” either undergo the medical tests or visit stalls depending on the format of health camp.
Other things to note:
Since this is a completely voluntary activity for the working professionals, MedCamp usually has little profile information about these people.
For a few camps, there was hardware failure, so some information about date and time of registration is lost.
MedCamp runs 3 formats of these camps. The first and second format provides people with an instantaneous health score. The third format provides 
information about several health issues through various awareness stalls.
Favorable outcome:
For the first 2 formats, a favourable outcome is defined as getting a health_score, while in the third format it is defined as visiting at least a stall.
You need to predict the chances (probability) of having a favourable outcome.
Train / Test split:
Camps started on or before 31st March 2006 are considered in Train
Test data is for all camps conducted on or after 1st April 2006.
Acknowledgements
Credits to AV
Inspiration
To share with the data science community to jump start their journey in Healthcare Analytics"	1466	15078	43	abisheksudarshan	health-care-analytics
6372	6372	Smartphone Sensor Dataset	Android sensor data collected while performing several daily human activities	[]	"Smartphone Sensor Dataset
Data Set Information:
The experiments have been carried out with a group of 5 volunteers within an age bracket of 19-28 years.
Each person performed 11 activities ( Walking on stairs, Walking in the corridor, Walking nearby road, 
Walking in the playground, Sitting in classroom/room, Sitting in the car, Sitting in auto-rickshaw, Sitting 
in the bus, Standing in the classroom, Standing in bus, Standing nearby road, Sleeping on the bed and Running) 
having a smartphone(Realme 3i) in their pant's front right/left(randomly) pocket. Using its embedded 
accelerometer, gyroscope, etc, we captured 3-axial acceleration, 3-axial linear acceleration, etc sensor
signals at a constant rate of 25Hz. The experiments were collected and labeled by an android app named AndoSensor( available on 
Play Store as well). The obtained data files were saved as different CSV files.
For each record in the dataset, it is provided:
1. Triaxial acceleration(total)
2. Triaxial gravitational acceleration
3. Triaxial linear acceleration
4. Triaxial angular velocity
5. Triaxial Magnetic field intensity
6. Triaxial orientation
7. Sound intensity
8. Longitude, Latitude, and Altitude"	0	33	0	image69of69pie	smartphone-sensor-dataset
6373	6373	checkpoints		['law']		4	22	0	alexandrevivier	checkpoints
6374	6374	pf-161-train-2-data		[]		0	3	0	makotoikeda	pf-161-train-2-data
6375	6375	Leyenda_a		[]		0	25	0	karimsalhi	leyenda-a
6376	6376	leaf disease dataset (collection)		['earth and nature']		0	21	0	hewarathnaashen	leaf-disease-dataset-collection
6377	6377	jigsaw		['puzzles']		1	13	0	regressionanalysisa	jigsaw
6378	6378	Visitors to ASEAN countries	Number of visitors to ASEAN countries from year 2015 to 2020.	['government', 'social science', 'travel']	"Content
The dataset contains number of visitors (in person) to every ASEAN country, organized by origin country, from year 2015 to year 2020. In this dataset, The Association of Southeast Asian Nations (ASEAN) consist of 10 members, which are Brunei Darussalam, Cambodia, Indonesia, Myanmar, Lao PDR, Malaysia, Philippines, Singapore, Thailand and Viet Nam.
Note: 
- Total EU-27 refers to 27 European Union countries without UK. Total EU-28 is no longer used in year 2020. 
- Data for 2020 is preliminary.
- Brunei Darussalam's data for 2013 and 2014 covers arrivals by air and land while other years covers arrivals by air only.
Acknowledgements
The raw data for this dataset is made publicly available in https://data.aseanstats.org/visitors.
Photo by <a href=""https://unsplash.com/@anniespratt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Annie Spratt</a> on <a href=""https://unsplash.com/s/photos/travel?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	28	261	1	ongengkheng	visitors-to-asean-countries-by-origin-countries
6379	6379	mycoco		[]		0	17	0	thanhtungphannguyen	mycoco
6380	6380	neuspell		[]		2	20	0	yaremamishchenko	neuspell
6381	6381	General Table Detection Dataset	General Table Detection Dataset (ICDAR 19 + Marmot + Github)	['business']		13	107	3	rhtsingh	general-table-recognition-dataset
6382	6382	Final Fantasy Dialogue Scripts	Dialogue from Final Fantasy cutscenes by game, character, and wordcount	['popular culture', 'literature', 'video games', 'nlp', 'text data', 'anime and manga']	"These are cutscene dialogue scripts for the Final Fantasy game franchise. They exclude most repeatable dialogue, such as battle text, on-screen cues, and generic NPC interactions. Scripts are from the collections on the Final Fantasy Frandom.com wiki: https://finalfantasy.fandom.com/wiki/Category:Scripts
I cleaned the scraped text with a VBA script and my own nerdy knowledge of the series. I intend to upload script files for all games as they become available  on the source site. As of Jan 9, 2022, there are 9 titles available on the site.
Please let me know if you find errors in the data!"	14	253	2	tylerhuxtable	final-fantasy-dialogue-scripts
6383	6383	LG_time_series_day02_dataset		[]		0	24	0	jiyoon52	lg-time-series-day02-dataset
6384	6384	myjigsaw		[]		1	24	0	yunyuze	myjigsaw
6385	6385	Charleston Sea Level daily dataset (1921 - 2014)	Sea level condition in Charleston, South Carolina	['earth and nature', 'time series analysis', 'tabular data']	"Source
This dataset was taken from ERDDAP at the Asia Pacific Data Research Center which is public data.
Content
The main items in this dataset are datetime, and sea level observation. Other items do not really necessary since the location is the same (Charleston) and there is no information in depth and sensor type code."	4	32	0	kevinwibowo	charleston-sea-level-daily-dataset-1921-2014
6386	6386	Removed_Background_Dataset		[]		0	10	0	stefanojp	removed-background-dataset
6387	6387	VOC2007_label		[]		0	7	0	guowei920322	voc2007-label
6388	6388	How to See Private Account Photos in Instagram		['internet']	"There are two ways to view private Instagram account photos. The first is to ask the person you'd like to follow if you want to see their posts. This is done via the following request process. It's similar to Facebook. However, the second way is to get the user's permission. To let the user know you want to see their posts, send them an email or a status message on Whatsapp.
How to see private account photos on Instagram, once you have the username of the person you'd like to see the photos, you can use a web search. All you need is the IG user name and the URL of the post you'd like to see. Then, you can type the username in Google and search for the account. If you're lucky, the images will come up. If they're not, you'll have to contact the person and ask them if you can see the posts.
You can also find the photos on the account by going to its profile and clicking the ""Follow"" button. This option is only available to people who follow the person. If you don't know the person, you can ask a follower to show you their posts. If the person replies, you'll be able to view their private account photos. Then, you can find other posts by searching for them on Instagram.
Once you've found a person you want to view private account photos with, you need to send a follow request. Once you have received a response, you should wait for a few minutes. If the person doesn't respond, you can also try a Google search to see what private accounts look like. Once you have a list of followers you can ask them for their posts.
You can also view private Instagram account photos by creating a fake account with a different email address. Using a fake account can make it difficult to access private accounts, so you need to create a new account. It's important to use the correct username and email address when you're trying to view private account photos on Instagram. Then, enter the user's password and click ""View profile"". After you've done this, you'll have the ability to browse other people's profile pictures.
There are several other ways to view private Instagram account photos. Follow someone's profile. If you're trying to spy on a specific person's private account, you should follow the person. You can then check the uploaded photos and videos. Then, you'll know if you can trust the person and what to expect. You can follow an account if you are interested."	1	18	0	sashagreg	how-to-see-private-account-photos-in-instagram
6389	6389	1920x1280_L_53_pth		[]		2	13	0	dragonzhang	1920x1280-l-53-pth
6390	6390	TSP_Jan2022_GDP_Per_Quarter	Quarterly GDP Sweden, Finland, Norway for TSP-2022-Jan	['cities and urban areas', 'europe', 'tabular data']	"Context
There are certain features on quarterly level which might be helpful for the TSP-2022-Jan contest. This dataset contains GDP of the countries from 2014 till 2019 on a quarterly level. It also provides last quarter results on the same rows with Year over year and Quarter over quarter changes.
Content
Countries included are Finland, Sweden and Norway.
Year and Quarter from 2014 Q1 till 2019 Q4
Last_Quarter_GDP is quarter to quarter
Last_Year_Same_Quarter_GDP is same quarter of last year's GDP value
All values are in USD Mn
Acknowledgements
Source : https://countryeconomy.com/gdp/"	23	85	2	aliasgherman	tsp-jan2022-gdp-per-quarter
6391	6391	Consecutive users dataset		[]		0	15	0	lapriya1	consecutive-users-dataset
6392	6392	VOC2007_GCL		[]		0	15	0	weiguo920219	voc2007-gcl
6393	6393	CSWs' intention to CCa Screening		['health']		1	27	1	habitubirhaneshetu	csws-intention-to-cca-screening
6394	6394	PetFinder-ZX130-20220110062957		[]		0	14	0	hideyukizushi	petfinder-zx130-20220110062957
6395	6395	Worldwide Omicron Dataset	World wide omicron dataset	['covid19']	Omicron is the latest variationof Coronavirus which is more dangerous than Delta . This database is the country wise list of omicron since december 2021.	368	2606	20	mdjafrilalamshihab	worldwide-omicron-dataset
6396	6396	style_wh_yp_ft1_out		[]		0	6	0	oldbirdaz	style-wh-yp-ft1-out
6397	6397	PetFinder-ZX1301-20220110061613		[]		1	31	0	hideyukizushi	petfinder-zx1301-20220110061613
6398	6398	yolov5mbestpt		[]		0	12	0	niioniio	yolov5mbestpt
6399	6399	Cyclistic Trip Data	Cyclistic’s historical trip data from January 2021 to December 2021	['business']	"Context
The data sets have a different name because Cyclistic is a fictional company. For the purposes of Google Data Analytics Professional Certificate case study, the data sets are appropriate and will enable you to answer the business questions.
Content
The original data sets can be found in here.
The data included the following attributes:
ride_id : a unique ID per ride
rideable_type: the type of bicycle
started_at: the date and time that the bicycle was started
ended_at: the date and time that the bicycle was ended
start_station_name: the name of start station
start_station_id: a unique ID for the start station
end_station_name: the name of end station
end_station_id : a unique ID for the end station
start_lat: the latitude of the start station
start_lng: the longitude of the start station
end_lat: the latitude of the end station
end_lng: the longitude of the end station
member_casual: the type of membership
Acknowledgements
Acknowledgements to Google and Coursera for introducing this data set.
Inspiration
how annual members and casual riders differ?
why casual riders would buy a membership?
how digital media could affect their marketing tactics?
License
The data has been made available by Motivate International Inc. under this license."	1	46	0	isrilcaniago	cyclistic-trip-data
6400	6400	organdfintuepetfinder		[]		1	8	0	vanle73	organdfintuepetfinder
6401	6401	vit16data		[]		3	10	0	nicholasleee	vit16data
6402	6402	Neckline Classification for Flatlay Images		['cities and urban areas', 'image data', 'e-commerce services']	"Context
Most modern retailers use AI systems to tag their skus for enriching their product and making it more discoverable. Along those lines, the task here is to build a single-label classification model to tag each one of these images into one category.
Content
The dataset contains flatlay images (images without human models) of upper/full body clothes. We are required to classify if the cloth's region around the neck falls under one of these three categories i.e. round, scoop or v-neck.
The following image gives an understanding of the three different types of necklines mentioned above."	2	33	0	vinayaknyk	neckline-classification-for-flatlay-images
6403	6403	SVR exp51		[]		0	3	0	kurokurob	svr-exp51
6404	6404	Tft Advisor		[]		0	35	0	hamzaachehboune	tft-advisor
6405	6405	Weather Time Dataset in 2020	Max Planck Institution 	[]		0	26	0	phanttan	weather-time-dataset-in-2020
6406	6406	justflipall		[]		0	9	0	vanle73	justflipaall
6407	6407	YOLOX mmdet pretrained		[]		20	18	0	duykhanh99	yolox-mmdet-pretrained
6408	6408	sample		[]		0	0	0	junji64	sample
6409	6409	LG_time_series_day01_dataset		[]		5	44	0	insungbaek	lg-time-series-day01-dataset
6410	6410	eb2_val1		[]		0	14	0	calaway84	eb2-val1
6411	6411	official_SQuAD_data_v1.1_and_v2.0		[]	"Context
The script created this dataset:
https://www.kaggle.com/tusonggao/get-official-squad-data/notebook"	0	4	0	tusonggao	official-squad-data-v11-and-v20
6412	6412	Load_cifar_data		[]		0	19	1	ding314	load-cifar-data
6413	6413	Cartoon Faces (Google's Cartoon Set)	Cartoon Set is a collection of random, 2D cartoon avatar images.	['people', 'education', 'image data', 'anime and manga']	"Context
All credit for this dataset goes to the Google Team who built this dataset.
See the official page for the dataset here --&gt; Cartoon Set
''
Cartoon Set is a collection of random, 2D cartoon avatar images. The cartoons vary in 10 artwork categories, 4 color categories, and 4 proportion categories, with a total of ~1013 possible combinations. We provide sets of 10k and 100k randomly chosen cartoons and labeled attributes.
''
Content
The original images are in a PNG format, but I converted them to a JPG format to limit the amount of memory used to store the images. There are 10 subfolders that each contain 10k of the 100k images. Each image is 500x500 pixels. 
Inspiration
I would love to see some GAN models built with this dataset.
That being said, I would love to see anything creative that comes from this dataset. Experiment away!"	56	276	4	brendanartley	cartoon-faces-googles-cartoon-set
6414	6414	Fortune Global 2000 Companies (2021)	Fortune Global 2000 Companies with Sales, Profits, Assets, Market Value	['global', 'business', 'finance']	"Fortune 2000 Companies (2021)
List of top 2000 companies with their total sales, total profits, total assets, and total market value
Since 2003, Forbes’ Global 2000 list has measured the world’s largest public companies in terms of four equally weighted metrics: assets, market value, sales and profits. Last year’s edition offered a glimpse into the early economic implications of the Covid-19 pandemic. Now, we see the results over 12 months of market turmoil and unfathomable human loss."	643	4109	29	shivamb	fortune-global-2000-companies-till-2021
6415	6415	TianChidata		[]		0	5	1	ccshenzhen	tianchidata
6416	6416	yolov5_layout		[]		1	9	0	onchutrng	yolov5-layout
6417	6417	dataset		[]		0	16	0	kmustnrp	dataset
6418	6418	beit_large_16_224		[]		0	7	0	akiyoshisutou	beit-large-16-224
6419	6419	window12_384		[]		0	17	0	akiyoshisutou	window12-384
6420	6420	BestParameter		[]		0	15	0	axzhang	bestparameter
6421	6421	TF_COTS_CLAHE		[]		0	8	0	magdbayoumi	tf-cots-clahe
6422	6422	petfinder_exp66		[]		5	17	1	titericz	petfinder-exp66
6423	6423	Modelo_Gravedad		[]		0	8	0	fernandoboviedo	modelo-gravedad
6424	6424	Iris_data-of-som		[]		1	18	0	sayedalisayed	iris-dataofsom
6425	6425	Valorant Pro Matches - Full Data		['games']	"Context
Valorant is still relatively new to the eSports scene, so the data analysis for pro or semi-pro games is still in its infancy stage. One of the biggest issues is sourcing the data. Vlr.gg is similar to CSGO's hltv.org that provides some great information on matches, but extracting its data isn't very accessible. Luckily, they (for now) allow scraping the website as much as you want.
I had a lot of issues because even though the HTML/CSS format is generally the same, there's a bunch of edge cases to account for and even times where the formatting completely breaks my parser. I didn't upload my code because it's honestly super messy, but I might in the future when I clean it up. The data set currently get most matches up to Jan 1, 2022, and I think there's like 400 out of ~11.5k that got errors and I couldn't add to the database. Probably about 200 are from the very first matches that got posted on vlr.gg.
Content
There is four tables. The top level is Matches that will tell you teams playing and match (map) score. Game is the next level that breaks down the specific maps played. Then Game_Rounds gives a round by round breakdown which shows who won, economy of each team, win type, and buy type, whenever the info is available. The game rounds are packaged in one string that you should be able to cast as a json. Lastly there is Game_Scoreboard which gives you the player performance, as well as things like number of first kills, first deaths, 2Ks, 3Ks, One v Ones, One v Twos, ect.
Inspiration
https://www.kaggle.com/hidious/valorant-vlrgg-results-and-stats
This dataset scrapes the results pages based on map score, but will only get match score, or map score if they only played 1 map. My data set tries to scrape everything available."	34	594	4	visualize25	valorant-pro-matches-full-data
6426	6426	Bert from-reddit-twitter		['social networks']		0	11	0	panser	bert-fromreddittwitter
6427	6427	Data Extraction from Photos	Images with Many Objects	['image data']	"$$\color{#9911ff}{\mathscr{CONTEXT}}$$
Data preparation for the main datasets.
$$\color{#9911ff}{\mathscr{CONTENT}}$$
Images with many visual objects (handwritten math notes, supermarket prices, etc.).
$$\color{#9911ff}{\mathscr{ACKNOWLEDGMENTS}}$$
Thanks for all comments and improvements in forked notebooks.
$$\color{#9911ff}{\mathscr{ INSPIRATION}}$$
What is the best way to extract information from photos with hundreds of objects?
Is it possible to check decisions by ML algorithms?"	6	327	5	olgabelitskaya	data-extraction-from-photos
6428	6428	Iris_data		[]		0	13	0	mustiztemiz	iris-data
6429	6429	Auto price prediction Moscow 2020		['automobiles and vehicles']		1	15	1	pankratozzi	auto-price-prediction-moscow-2020
6430	6430	YKS Six Pack	"All crowdfunding projects mentioned in ""YKS"" podcast"	['finance']	"List of all crowdfunding projects from yourkickstartersucks podcast (https://shows.pippa.io/yourkickstartersucks/) that are discussed in segment ""The Six Pack""
Data for Kickstarter projects is taken automatically via script, for other platforms, data (some of it) is entered manually
To sort in more meaningful way goal, pledge and average pledge is given in unified currency - USD. Conversion is done by using https://exchangerate.host/. This data is refreshed every time dataset is updated with new entries"	147	9799	4	officerbribe	yks-six-pack
6431	6431	Exonerations 1989-2021	Exonerations between 1989 and 2021 	[]		1	7	0	vchitepu	exonerations-19892021
6432	6432	COVID 19 Panel	A COVID panel with 107 columns updated four times per day 	['health', 'medicine', 'covid19']	"As COVID is evolving, it's useful to have convenient access to a wide range of data to help understand the current situation. This is a wide panel updated four times per day. It includes measures like deaths and cases, vaccinations rates, proportion of sequences that are variants of interest (e.g. Delta), Google mobility data and much more. This allows for very rich analysis to be done easily. And the dataset updates four times per day, meaning the analysis can be kept up-to-date. 
Acknowledgements
This data draws heavily on the work done by Our World in Data. The panel starts with their data but then adds on additional metrics (such as Google mobility data, % of sequences that are Delta, Vaccine type by country)."	195	5076	14	antgoldbloom	covid19panels
6433	6433	ismagiltal		[]		0	9	0	vitkishloh228	ismagiltal
6434	6434	Bachelorette	900 collected samples, technical information	['beginner']	"About this dataset
&gt; <p>Bachelorette / Bachelor<br>
The raw data behind the stories:</p>
<p>How To Spot A Front-Runner On The ‘Bachelor’ Or ‘Bachelorette’<br>
Rachel’s Season Is Fitting Neatly Into ‘Bachelorette’ History<br>
Rachel Lindsay’s ‘Bachelorette’ Season, In Three Charts<br>
Data was scraped from the Bachelor Nation Wiki and then missing seasons were filled in by ABC and FiveThirtyEight staffers.</p>
<p>Header   Description<br>
SHOW    Bachelor or Bachelorette<br>
SEASON  Which season<br>
CONTESTANT  An identifier for the contestant in a given season<br>
ELIMINATION-1   Who was eliminated in week 1<br>
ELIMINATION-2   Who was eliminated in week 2<br>
ELIMINATION-3   Who was eliminated in week 3<br>
ELIMINATION-4   Who was eliminated in week 4<br>
ELIMINATION-5   Who was eliminated in week 5<br>
ELIMINATION-6   Who was eliminated in week 6<br>
ELIMINATION-7   Who was eliminated in week 7<br>
ELIMINATION-8   Who was eliminated in week 8<br>
ELIMINATION-9   Who was eliminated in week 9<br>
ELIMINATION-10  Who was eliminated in week 10<br>
DATES-1 Who was on which date in week 1<br>
DATES-2 Who was on which date in week 2<br>
DATES-3 Who was on which date in week 3<br>
DATES-4 Who was on which date in week 4<br>
DATES-5 Who was on which date in week 5<br>
DATES-6 Who was on which date in week 6<br>
DATES-7 Who was on which date in week 7<br>
DATES-8 Who was on which date in week 8<br>
DATES-9 Who was on which date in week 9<br>
DATES-10    Who was on which date in week 10<br>
Eliminates connote either an elimination (starts with ""E"") or a rose (starts with ""R"").<br>
Eliminations supercede roses.<br>
""E"" connotes a standard elimination, typically at a rose ceremony. ""EQ"" means the contestant quits. ""EF"" means the contestant was fired by production. ""ED"" connotes a date elimination. ""EU"" connotes an unscheduled elimination, one that takes place at a time outside of a date or rose ceremony.<br>
""R"" means the contestant received a rose. ""R1"" means the contestant got a first impression rose.<br>
""D1"" means a one-on-one date, ""D2"" means a 2-on-1, ""D3"" means a 3-on-1 group date, and so on.<br>
Weeks of the show are deliminated by rose ceremonies, and may not line up exactly with episodes.<br>
About this file<br>
Edit<br>
LAST UPDATED<br>
2 years ago<br>
OWNER<br>
<a href=""/fivethirtyeight"" target=""_blank"" rel=""nofollow"">@fivethirtyeight</a><br>
CREATED<br>
2 years ago<br>
SIZE<br>
2.54 KB<br>
LABELS<br>
Add labels<br>
DESCRIPTION</p>
<p>This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-05-22</code> <a href=""https://fivethirtyeight.com/features/the-bachelorette/"" target=""_blank"" rel=""nofollow"">How To Spot A Front-Runner On The ‘Bachelor’ Or ‘Bachelorette’</a></li>
<li><code>2017-07-17</code> <a href=""https://fivethirtyeight.com/features/rachels-season-is-fitting-neatly-into-bachelorette-history/"" target=""_blank"" rel=""nofollow"">Rachel’s Season Is Fitting Neatly Into ‘Bachelorette’ History</a></li>
<li><code>2017-08-07</code> <a href=""https://fivethirtyeight.com/features/rachel-lindsays-bachelorette-season-in-three-charts/"" target=""_blank"" rel=""nofollow"">Rachel Lindsay’s ‘Bachelorette’ Season, In Three Charts</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 900 samples along with Elimination 6, Dates 1, technical information and other features such as:
- Dates 7
- Elimination 8
- and more.
How to use this dataset
&gt; - Analyze Elimination 1 in relation to Elimination 5
- Study the influence of Elimination 9 on Dates 3
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	24	481	7	yamqwe	bachelorettee
6435	6435	Machine Learning users on Github	Data Ingestion from Github API	['africa', 'business', 'education', 'software', 'beginner', 'text mining']	"Data was scraped from Github's API.
Columns
LOGIN: shows the user's Github login
ID: user's id
URL: API link to the user's profile
NAME: fullname of the user
COMPANY: organization the user's affiliated with
BLOG: link to the user's blog site
LOCATION: location where the user resides
EMAIL: user's email address
BIO: about the user
This dataset contains over 600 users from Lagos, Nigeria and Rwanda
Source: https://github.com/ProsperChuks/Github-Data-Ingestion/tree/main/data"	42	396	1	prosperchuks	machine-learning-users-on-github
6436	6436	onlineRetail		[]		2	14	0	akgulhilal	onlineretail
6437	6437	parte 1		[]		2	24	0	alefrance	parte-1
6438	6438	Covid19C7064_3		[]		0	14	0	mustai	covid19c7064-3
6439	6439	CXR dataframes		[]		0	26	1	mhkoosheshi	cxr-dataframes
6440	6440	GBR-yolox-l		['sports']		1	69	0	trongminhle	gbryoloxl
6441	6441	mmcv_full_142_cp37		[]		0	6	0	rkstgr	mmcv-full-142-cp37
6442	6442	KU Fall Courses		['universities and colleges']		1	23	0	grannysmithapples	ku-fall-courses
6443	6443	🌡️ French department temperature since 2018	Evolution of daily temperatures in French departments	['europe', 'geography', 'energy', 'time series analysis', 'news']	"Context
Find the daily temperatures of the French departments.
Content
For each department you will find the minimum, average and maximum temperatures since 2018. 
Acknowledgements
Thanks to the data.gouv team for putting together this dataset
Inspiration
Beautiful visualizations in perspective with many ideas of time series forecasting. To be able to show the evolution of the temperatures in order to alarm the population on the climatic deregulation."	72	561	12	pierrelouisdanieau	french-department-temperature-since-2018
6444	6444	SuperStore Sales Dataset		['business']	"Content
Sales records from a superstore in the united states. The dates range from January 2011 to December 2014"	7	66	2	raphaelabayomi	superstore-sales-dataset
6445	6445	P_Pr_trail_swin_overfit_0.1		[]		0	12	0	durgampranay	p-pr-trail-swin-overfit-01
6446	6446	andylabels		[]		0	27	0	chengnanli643	andylabels
6447	6447	pycaret		[]		2	30	1	sytuannguyen	pycaret
6448	6448	Huge Dataset - COVID 19 worldwide case	COVID 19 dataset from Jan 28, 2020  to Jan 8, 2022	['public health', 'earth and nature', 'mortality', 'time series analysis']	"Context
The dataset contains COVID 19 details till Jan 8 2 2022.
Content
It contains 67 columns and 153172 rows
Column details :
iso_code:    ISO 3166-1 alpha-3 – three-letter country codes
continent:  Continent of the geographical location
location:   Geographical location
date:   Date of observation
total_cases:  Total confirmed cases of COVID-19
new_cases:  New confirmed cases of COVID-19
new_cases_smoothed: New confirmed cases of COVID-19 (7-day smoothed)
total_cases_per_million:    Total confirmed cases of COVID-19 per 1,000,000 people
new_cases_per_million:  New confirmed cases of COVID-19 per 1,000,000 people
new_cases_smoothed_per_million: New confirmed cases of COVID-19 (7-day smoothed) per 1,000,000 people
total_deaths:   Total deaths attributed to COVID-19
new_deaths: New deaths attributed to COVID-19
new_deaths_smoothed:    New deaths attributed to COVID-19 (7-day smoothed)
total_deaths_per_million:   Total deaths attributed to COVID-19 per 1,000,000 people
new_deaths_per_million: New deaths attributed to COVID-19 per 1,000,000 people
new_deaths_smoothed_per_million:    New deaths attributed to COVID-19 (7-day smoothed) per 1,000,000 people
excess_mortality:    Percentage difference between the reported number of weekly or monthly deaths in 2020–2021 and the projected number of deaths for the same period based on previous years.
excess_mortality_cumulative:    Percentage difference between the cumulative number of deaths since 1 January 2020 and the cumulative projected deaths for the same period based on previous years.
excess_mortality_cumulative_absolute:   Cumulative difference between the reported number of deaths since 1 January 2020 and the projected number of deaths for the same period based on previous years.
excess_mortality_cumulative_per_million:  Cumulative difference between the reported number of deaths since 1 January 2020 and the projected number of deaths for the same period based on previous years, per million people. 
icu_patients:   Number of COVID-19 patients in intensive care units (ICUs) on a given day
icu_patients_per_million:   Number of COVID-19 patients in intensive care units (ICUs) on a given day per 1,000,000 people
hosp_patients:  Number of COVID-19 patients in the hospital on a given day
hosp_patients_per_million:  Number of COVID-19 patients in hospital on a given day per 1,000,000 people
weekly_icu_admissions:   Number of COVID-19 patients newly admitted to intensive care units (ICUs) in a given week
weekly_icu_admissions_per_million:  Number of COVID-19 patients newly admitted to intensive care units (ICUs) in a given week per 1,000,000 people
weekly_hosp_admissions: Number of COVID-19 patients newly admitted to hospitals in a given week
weekly_hosp_admissions_per_million: Number of COVID-19 patients newly admitted to hospitals in a given week per 1,000,000 people
stringency_index:    Government Response Stringency Index: composite measure based on 9 response indicators including school closures, workplace closures, and travel bans, rescaled to a value from 0 to 100 (100 = strictest response)
reproduction_rate:  Real-time estimate of the effective reproduction rate (R) of COVID-19. 
total_tests:    Total tests for COVID-19
new_tests:  New tests for COVID-19 (only calculated for consecutive days)
total_tests_per_thousand:   Total tests for COVID-19 per 1,000 people
new_tests_per_thousand: New tests for COVID-19 per 1,000 people
new_tests_smoothed: New tests for COVID-19 (7-day smoothed). For countries that don't report testing data on a daily basis, we assume that testing changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window
new_tests_smoothed_per_thousand: New tests for COVID-19 (7-day smoothed) per 1,000 people
positive_rate:  The share of COVID-19 tests that are positive, given as a rolling 7-day average (this is the inverse of tests_per_case)
tests_per_case: Tests conducted per new confirmed case of COVID-19, given as a rolling 7-day average (this is the inverse of positive_rate)
tests_units:    Units used by the location to report its testing data
total_vaccinations: Total number of COVID-19 vaccination doses administered
people_vaccinated:  Total number of people who received at least one vaccine dose
people_fully_vaccinated:    Total number of people who received all doses prescribed by the vaccination protocol
total_boosters: Total number of COVID-19 vaccination booster doses administered (doses administered beyond the number prescribed by the vaccination protocol)
new_vaccinations:   New COVID-19 vaccination doses administered (only calculated for consecutive days)
new_vaccinations_smoothed:  New COVID-19 vaccination doses administered (7-day smoothed). For countries that don't report vaccination data on a daily basis, we assume that vaccination changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window
total_vaccinations_per_hundred: Total number of COVID-19 vaccination doses administered per 100 people in the total population
people_vaccinated_per_hundred:  Total number of people who received at least one vaccine dose per 100 people in the total population
people_fully_vaccinated_per_hundred:    Total number of people who received all doses prescribed by the vaccination protocol per 100 people in the total population
total_boosters_per_hundred: Total number of COVID-19 vaccination booster doses administered per 100 people in the total population
new_vaccinations_smoothed_per_million:  New COVID-19 vaccination doses administered (7-day smoothed) per 1,000,000 people in the total population
new_people_vaccinated_smoothed: Daily number of people receiving their first vaccine dose (7-day smoothed)
new_people_vaccinated_smoothed_per_hundred: Daily number of people receiving their first vaccine dose (7-day smoothed) per 100 people in the total population
population: latest available values of population
population_density: Number of people divided by land area, measured in square kilometers, most recent year available
median_age: Median age of the population, UN projection for 2020
aged_65_older:  Share of the population that is 65 years and older, most recent year available
aged_70_older:  Share of the population that is 70 years and older in 2015
gdp_per_capita: Gross domestic product at purchasing power parity (constant 2011 international dollars), most recent year available
extreme_poverty:    Share of the population living in extreme poverty, most recent year available since 2010
cardiovasc_death_rate: Death rate from cardiovascular disease in 2017 (annual number of deaths per 100,000 people)
diabetes_prevalence:    Diabetes prevalence (% of population aged 20 to 79) in 2017
female_smokers: Share of women who smoke, most recent year available
male_smokers:   Share of men who smoke, most recent year available
handwashing_facilities: Share of the population with basic handwashing facilities on-premises, most recent year available
hospital_beds_per_thousand: Hospital beds per 1,000 people, most recent year available since 2010
life_expectancy:    Life expectancy at birth in 2019
human_development_index:  A composite index measuring average achievement in three basic dimensions of human development—a long and healthy life, knowledge, and a decent standard of living. 
Acknowledgements
Hasell, J., Mathieu, E., Beltekian, D. et al. A cross-country database of COVID-19 testing. Sci Data 7, 345 (2020)
License
Creative Commons BY license. 
Inspiration
visualize the data with the dataset for insights.**"	64	466	23	sandhyakrishnan02	latest-covid-19-dataset-worldwide
6449	6449	Blackout Poetry Dataset	Dataset for generating blackout poetry	['linguistics', 'nlp', 'deep learning', 'text data', 'transformers']	"Blackout Poetry Dataset
A blackout poetry dataset constructed from publicly available short stories and large poems. The dataset consists of two variants: 8K and 16K examples of passages along with a poem generated from the passage and the indices of the words in the passage from which words in the poem have been selected. The dataset also contains perplexity scores for each of the poems indicating the language quality of the poems.
The dataset was constructed synthetically, and hence contains multiple poor poems and frequent grammatical errors. However, it is a great starting point for the task of applying machine learning to blackout poetry generation.
The dataset was first introduced in MAPLE – MAsking words to generate blackout Poetry using sequence-to-sequence LEarning.
Content
The dataset has two variants:
- 8K (sampled poems from the 16K dataset with the lowest perplexity scores)
- 16K
Both variants contain data in the following format:
| passage                                           | poem                                  | indices           |
|---------------------------------------------------|---------------------------------------|-------------------|
| Did the CIA tell the FBI that it knows the wor... | cia fbi the biggest weapon            | [2, 5, 9, 24, 25] |
| A vigilante lacking of heroic qualities that\n... | lacking qualities that damn criminals | [2, 5, 6, 11, 12] |
The passage is the text from which the poem is generated. The poem is the generated poem. The indices are the indices of the words in the text that are chosen for the poem. 
Acknowledgements
This dataset was generated synthetically using Liza Daly's pattern matching based blackout poetry generation."	34	530	9	aditeyabaral	blackout-poetry-dataset
6450	6450	feebackexp102		[]		0	15	0	gyanendradas	feedbackexp102
6451	6451	Companies Gone Public in 2021	Companies, Stock Symbol, Listing type	[]		1	58	0	idomon	companies-gone-public-in2021
6452	6452	Wind Power in Germany	Time series of actual wind power infeed	['renewable energy', 'beginner', 'time series analysis', 'tabular data', 'electricity']	"Context
Wind power in Germany is leading the way in the field of renewable electricity generation. The installed capacity of all wind turbine generators (WTG’s) in the country amounted of more than 83,000 megawatts (MW) at the end of 2020.
Wind turbines are the most important form of renewable energy exploitation, generating electricity both on land (onshore) and in the open sea (offshore). Very simply put, they work as wind mills combined with a dynamo: the wind makes the rotor turn and this movement is transformed into electrical energy by a generator.
The first wind turbines were already used for electricity generation in the early 20th century. Nevertheless, the worldwide interest in electricity generation by means of wind turbines only began to increase significantly after the oil crises of the 1970s. The development of the large renewable installations that are now common, began in the 1990s and has shown a strongly rising trend that continues to this day. This is due to requirements coming into force within the German Renewable Energy Sources Act (EEG) in the year 2000.
Unlike conventional power plants or renewable biomass installations, the electricity generation from wind energy fluctuates and strongly depending on the weather conditions. Unfortunately, the generational capacity from wind turbines rarely matches the electricity demand at the time. Depending on the wind speed, the volume of electricity generated by wind turbines varies and due to requiring large open spaces for efficient generation, turbines are often found in sparsely populated areas with a correspondingly low electricity demand."	15	158	0	l3llff	wind-power
6453	6453	Amazon Surge for TPS JAN 2022		['retail and shopping']	"Context
It basically contains values from 0 - 100, where 0 is least popular & 100 is an extreme surge in searches. Since its amazon, it directly reflects the surge in sales. You can use this data for TPS JAN 2022"	4	53	3	anirudhyadav9784	amazon-surge-for-tps-jan-2022
6454	6454	 FakesStorage	This is a spanish dataset for fake news detection. 	['computer science', 'intermediate', 'text data', 'news']	"GITHUB: https://github.com/alcorpas10/FakesStorage
Context
This project aims to build a Spanish fake news dataset with news from the Internet. The objective of this dataset is to contribute to the development of systems capable of identifying online fake news. The dataset is still a work in progress.
Este proyecto está dirigido a la construcción de un dataset de noticias falsas (o ""fake news"") en internet que estén en español. Con este dataset buscamos contribuir a la construcción de sistemas para identificar noticias falsas en internet. El dataset sigue en construcción.
Content
This repository was created using Python and can be used and adapted to download articles about fake news from fact-checkers' websites.
The version of the latest dataset provided in this repository includes the following files (WebScraping folder):
fakenewsMaldita.json: Fake news collected from Maldita.es
fakenewsNewtral.json: Fake news collected from Newtral.es
fakenewsFactCheck.json: Fake news collected from FactCheck.org
Each JSON file contains the following data for each article:
| Parameter | Data type | Description |
| --- | --- |
|id | int |Unique identifier for each news article in the dataset|
| titulo | string   | Title of the news article |
| link | string| Article's link from the fact-checker |
| words_count | dictionary | The number of occurrences for each word in the body text |
Authors:
David Bugoi: 
Daniela Alejandra Córdova
Alejandro Corpas Calvo
Javier Gómez Moraleda
Daniel Hernández Martínez
Erik Karlgren Domercq
Adri Turiel Charro"	0	70	0	danielacordovaporta	fakesstorage
6455	6455	TPS - Jan22 | Google Trends Kaggle search DataSet	TPS - Jan22 | Google Trends Kaggle search DataSet	['computer science', 'beginner', 'data visualization', 'time series analysis', 'python']	"""Kaggle"" keyword web search (WorldWide) dataset from the google trends. I am not sure if this is going to help. I searched for Finland, Sweden, and Norway but the data is not enough."	7	105	5	ankitkalauni	tps-jan22-google-trends-kaggle-search-dataset
6456	6456	Tetuan City power consumption	city power consumtion	['electricity']	"Source:
Adarsh Pal Singh (IIIT Hyderabad, India): adarshpal.singh '@' alumni.iiit.ac.in
Dr. Sachin Chaudhari (IIIT Hyderabad, India): sachin.c '@' iiit.ac.in
Data Set Information:
The experimental testbed for occupancy estimation was deployed in a 6m Ã— 4.6m room. The setup consisted of 7 sensor nodes and one edge node in a star configuration with the sensor nodes transmitting data to the edge every 30s using wireless transceivers. No HVAC systems were in use while the dataset was being collected.
Five different types of non-intrusive sensors were used in this experiment: temperature, light, sound, CO2 and digital passive infrared (PIR). The CO2, sound and PIR sensors needed manual calibration. For the CO2 sensor, zero-point calibration was manually done before its first use by keeping it in a clean environment for over 20 minutes and then pulling the calibration pin (HD pin) low for over 7s. The sound sensor is essentially a microphone with a variable-gain analog amplifier attached to it. Therefore, the output of this sensor is analog which is read by the microcontrollerâ€™s ADC in volts. The potentiometer tied to the gain of the amplifier was adjusted to ensure the highest sensitivity. The PIR sensor has two trimpots: one to tweak the sensitivity and the other to tweak the time for which the output stays high after detecting motion. Both of these were adjusted to the highest values. Sensor nodes S1-S4 consisted of temperature, light and sound sensors, S5 had a CO2 sensor and S6 and S7 had one PIR sensor each that were deployed on the ceiling ledges at an angle that maximized the sensorâ€™s field of view for motion detection.
The data was collected for a period of 4 days in a controlled manner with the occupancy in the room varying between 0 and 3 people. The ground truth of the occupancy count in the room was noted manually.
Please refer to our publications for more details.
Attribute Information:
Date: YYYY/MM/DD
Time: HH:MM:SS
Temperature: In degree Celsius
Light: In Lux
Sound: In Volts (amplifier output read by ADC)
CO2: In PPM
CO2 Slope: Slope of CO2 values taken in a sliding window
PIR: Binary value conveying motion detection
Room_Occupancy_Count: Ground Truth
Relevant Papers:
Adarsh Pal Singh, Vivek Jain, Sachin Chaudhari, Frank Alexander Kraemer, Stefan Werner and Vishal Garg, â€œMachine Learning-Based Occupancy Estimation Using Multivariate Sensor Nodes,â€ in 2018 IEEE Globecom Workshops (GC Wkshps), 2018.
Adarsh Pal Singh, 'Machine Learning for IoT Applications: Sensor Data Analytics and Data Reduction Techniques', Masters Thesis, [Web Link], 2020.
Citation Request:
If you use this dataset in your research, please cite the following paper:
Adarsh Pal Singh, Vivek Jain, Sachin Chaudhari, Frank Alexander Kraemer, Stefan Werner and Vishal Garg, â€œMachine Learning-Based Occupancy Estimation Using Multivariate Sensor Nodes,â€ in 2018 IEEE Globecom Workshops (GC Wkshps), 2018."	58	554	7	gmkeshav	tetuan-city-power-consumption
6457	6457	petf_swin_large_508_model_cutmix10_metricfixed		[]		0	3	0	nhac43	petf-swin-large-508-model-cutmix10-metricfixed
6458	6458	Land Mobile Broadcast Towers	 communications,  opportunity,  homeland security,  mobile,  broadcast,  towers	['geography', 'internet', 'mobile and wireless', 'electronics', 'engineering']	This dataset represents Land Mobile Broadcast tower locations as recorded by the Federal Communications Commission, extracted from the FCC Licensing Database. It is known that there are some errors in the licensing information - latitude, longitude and ground elevation data as well as frequency assignment data from which these files were generated.	35	291	6	gmkeshav	land-mobile-broadcast-towers
6459	6459	stemmed_data		[]		0	11	0	regressionanalysisa	stemmed-data
6460	6460	Keras Yolo Weights	YoloV3 Weights in Keras	[]		0	20	1	ravishah1	keras-yolo-weights
6461	6461	FoodConsumption		[]		0	16	0	aybukepolat	foodconsumption
6462	6462	petfinder-tfrecords-v4		[]		0	8	0	ks2019	petfinder-tfrecords-v4
6463	6463	plainText		['earth and nature']		1	8	0	grannysmithapples	plaintext
6464	6464	homework 2		['education']		0	4	0	dildobuster	homework-2
6465	6465	Macro-economic composite: Finland, Norway, Sweden	Composite index for TPS Jan 2022	['economics']		3	32	1	lucamassaron	macroeconomic-composite-finland-norway-sweden
6466	6466	labelme		[]		0	9	0	shubhamsindal0098	labelme
6467	6467	skip_embeded_captions		[]		0	13	0	bouchrazeitane	skip-embeded-captions
6468	6468	homework 1		['education']		0	12	0	dildobuster	homework-1
6469	6469	densenet201		[]		0	12	0	clare9	densenet201
6470	6470	Mall_customers_txt		[]		0	11	0	sharaddeshmukh	mall-customers-txt
6471	6471	Medium 2021 data science articles dataset	Data about 45k+ data science articles published on medium in 2021.	['business', 'computer science']	"Context
As a beginner data science writer, I found guides about how to optimize my Text,  but none of those
was specific about data science. So let's build it!
Content
The data was collected using this notebook
The dataset contains information about almost all Medium articles published in 2021 that contains one of the following tags:
Data Science
Machine Learning
Artificial Inteligence
Deep Learning
Data
Big Data
Analytics"	58	806	10	viniciuslambert	medium-2021-data-science-articles-dataset
6472	6472	Room Occupancy Prediction (IoT)		['classification', 'text data']		39	216	1	seymagoksel	room-occupancy-prediction-iot
6473	6473	show Shap Values py Dataset(Regression)	show SHAP Values Funciton(Regression Model)	['model explainability']	"usage : 
TRAIN_PATH = ""../input/house-prices-advanced-regression-techniques/train.csv""
TARGET = ""SalePrice""
RARE_COLUMN = 'Id'
SEED = 2022
showShapValues(TRAIN_PATH,TARGET,RARE_COLUMN,SEED)"	3	100	2	rhythmcam	show-shap-values-py-dataset
6474	6474	Flower_data_102class		[]		0	11	0	duykhanguyn	flower-data-102class
6475	6475	captions_embedded		[]		0	12	0	bouchrazeitane	captions-embedded
6476	6476	sartorius-train-vix2		[]		0	10	0	prateekagnihotri	sartorius-train-vix2
6477	6477	sartorius-train-vix		['arts and entertainment']		0	1	0	prateekagnihotri	sartorius-train-vix
6478	6478	pet2_age_model		[]		0	33	0	ktakita	pet2-age-model
6479	6479	swin_ting_model_embed_fastai		[]		2	57	0	hellozq	swin-ting-model-embed-fastai
6480	6480	binance btcusdt trade data 106731162-224286059		['business']		0	22	0	hejianhua198711	binance-btcusdt-trade-data-106731162224286059
6481	6481	petfinder-tfrecords-v3		[]		0	18	0	ks2019	petfinder-tfrecords-v3
6482	6482	LovelyDoggoTemp		[]		1	24	0	nizhen	lovelydoggotemp
6483	6483	Dice Images	Images of Dice for Image Classification.	['computer science', 'classification', 'deep learning', 'image data']	"Context
There is no story behind this dataset, I just felt that I should also have a dataset  😬 .
About the Dataset.
The dataset contains top view of dice digits which can be used as an alternative to the MNIST dataset for digit recognition, a benchmark dataset for classification.
The images currently are only 120 and attempts to augment the data have already been made through the Tensorflow  data augmentation pipeline, which further increased the dataset to about 1600 images(with random rotations, crops amongst other operations)
Image Type and Nomenclature
For the small dataset that we have here, the images were made from just two dice. The images of the dice are resized to be similar to that of the MNIST dataset for testing results on the already present models.
The images currently in the dataset are named as follows:
{number}_{color of the dice*}{transform angle}{transformation direction} 
Expectation
My aim is that the dataset should be big enough so as to not cause overfitting. The dataset should also be diverse enough so that the model for which it is used is accurate. 
Albeit augmentation of the dataset is a way to increase the dataset size, original images are preferred for their variability amongst many variables that I might have neglected in my analysis.
if the direction is necessary, it is mentioned
* Although the images are converted to grayscale, the color of the dice might be feature that is required for some other analysis. 
Acknowledgements
There is no one particularly that comes to mind, because each and every picture in this small dataset was manually edited by me, although I would like to help
Inspiration
The question that I have is whether this dataset can be used for Image Classification ? My take on this problem : 
GitHub Implementation"	1	56	0	yashsrivastava51213	dice-images
6484	6484	learning		['education']		1	28	0	clare9	learning
6485	6485	captions		['arts and entertainment']		0	14	0	bouchrazeitane	captions
6486	6486	Leaf Disease Dataset (combination)	256x256 color images of leaves healthy and affected	['agriculture', 'classification', 'image data', 'cv2', 'keras']	"Context
while working on the leaf disease classification model created this collected dataset based on the plantvillage, rice leaf disease dataset and cassava dataset already publically available in Kaggle.
Content
all images are well organized in train, test, and validation directories within their relevant class name folder. all images are in 256x256 size and RGB colour scheme. cassava images do not contain all images that are in the original dataset. they are preprocessed to 256x256 and also croped u using tensorflow.
Acknowledgements
plant village dataset
rice leaf disease dataset
cassava leaf dataset"	15	337	5	asheniranga	leaf-disease-dataset-combination
6487	6487	Employment ratio before/after giving birth	Employment ratio of mothers before/after giving birth(South Korea)	[]	"Data Background
Hello, this is dataset about the ratio of mothers getting employed before/after birth.
It is a small dataset but I wanted to check what kind of reality mother faces when having a child. It would be nice if I can compare the data with other countries, but I only got South Korea data :(. If anyone has similar dataset that I can compare with, it would be interesting to work on together!
Being a mother is definitely a bless and something to be celebrated, but it is also evident that there are lots of things to give up for child. I believe biggest risk that mother sacrifices most is their 'career.' 
South Korea is well known for extremely low annual birth rate. According to recent statistics, 0.92 child are born per woman (2019). Low annual birth rate implies how hostile social environment is to have child and make family. Politicians are coming up with new support policies to improve low birth rate issue, but still it's not working well. 
Hope South Korea become a  bright society where mothers can pursue their career!
Data Description
Minus days : before birth
Plus days : after birth
Birth : Birth date
Source
Source: dataset from Korea Statistical Information Service(https://kosis.kr/index/index.do)"	6	72	0	bell2psy	employment-ratio-beforeafter-giving-birth
6488	6488	Sales_Data		[]		2	21	0	shubhamverse	sales-data
6489	6489	COTS_train_test_split		[]		1	1	0	georgeteo89	cots-train-test-split
6490	6490	Shakespeare 		[]		0	14	0	ahmedmohameddawoud	shakespeare
6491	6491	fastai-models		[]		0	24	0	malekbadreddine	fastaimodels
6492	6492	image-299x299-data	diseas x-ray images 17 class	['online communities']		2	17	1	mrtacr2534	image299x299data
6493	6493	flowers		[]		0	10	0	bouchrazeitane	flowers
6494	6494	COTS-YoloXs-e40-AP-25.34-StratifiedK	Training Weights of YoloX	['exercise']	"Context
Trained on StratifiedK 5 Folds (4919 images) using
- yolox-s
- e=40
- input_size=(960x960)
- batch_size=16
- NMSthreshold=0.65
Dataset link - https://www.kaggle.com/nyanswanaung/cotscocostratifiedk5folds-4919-imgs
Training Code link(v5) - https://www.kaggle.com/nyanswanaung/yolox-training-with-5-folds-best-ap-25-34-v5/edit"	2	47	2	nyanswanaung	cotsyoloxse40ap2534stratifiedk
6495	6495	UAE Dataset		['earth and nature']		3	16	0	morashidy	uae-dataset
6496	6496	3090_kqi_ex027_1		[]		0	3	0	anonamename	3090-kqi-ex027-1
6497	6497	Retail		[]		1	17	0	nishthakapoor17	retail
6498	6498	5G smartphones available in India 	Latest data about the 5g phones available in India (2022)	['india', 'people and society', 'business', 'electronics', 'data analytics']	Smartphones have become an integral part of our lifestyle. With the advancement in technology, smartphones are becoming smarter, faster, and more powerful with every new generation. There was a time when mobile phones were restricted to just receive or make phone calls. Fast forward it today and you will see that you can do virtually anything on a smartphone. You can book a ride, respond to emails, navigate to different places, click amazing photos, or just binge watch some favorite shows on different streaming applications. Smartphones are now available at every price point in the country whether it be the entry-level segment or the premium segment. So, if you are looking for the latest smartphones that are currently available in India, then this is the page you should follow.	122	719	9	ramjasmaurya	5g-smartphones-available-in-india
6499	6499	Sequence_model_weights		[]		0	7	0	jisnava	sequence-model-weights
6500	6500	harry_potter		[]		3	19	0	deadpool69250	harry-potter
6501	6501	dataset2		[]		0	11	0	milonsheikh	dataset2
6502	6502	feedback_estimators		[]		0	35	0	delaram	feedback-estimators
6503	6503	C01 connectors 		[]		3	10	0	rahulramesh07	c01-connectors
6504	6504	tested by me		[]		0	13	0	milonsheikh	tested-by-me
6505	6505	keras_efficientnet_v2		[]		0	21	0	nizhen	keras-efficientnet-v2
6506	6506	longformer base 4096 processed data		[]		2	24	0	errordividebyzero	longformer-base-4096-processed-data
6507	6507	Sports data analysis		['sports']		2	22	0	nehasingh12	sports-data-analysis
6508	6508	Bank_Daily_Cash_Position_18_series	Bank_Daily_Cash_Position_18_series	[]		1	14	0	tedtad	bank-daily-cash-position-18-series
6509	6509	Bitcoin Five Years Prices 2017-2021		['currencies and foreign exchange']		14	77	0	anaghaclement	bitcoin-five-years-prices-20172021
6510	6510	INFY.NS Infosys 5 years Stock Prices		['business', 'investing']		0	7	0	anaghaclement	infyns-infosys-5-years-stock-prices
6511	6511	pet2teyomodel1		[]		1	76	0	teyosan1229	pet2teyomodel1
6512	6512	Cars Data		[]		9	70	6	hetvigandhi03	cars-data
6513	6513	SETI RADIO SIGNALS CSV DATASET		[]		3	14	0	abrahamanderson	seti-radio-signals-csv-dataset
6514	6514	Uniswap_rates_preceeding_cyclic_arbitrages_raw		[]		19	30	1	ogst68	uniswap-rates-preceeding-cyclic-arbitrages-raw
6515	6515	Coffee leaf disease trained models	trained models for the coffee leaf disease dataset	['health']		1	47	0	jtaglione	coffee-disease-trained-models
6516	6516	allentelescope		[]		0	9	0	abrahamanderson	allentelescope
6517	6517	Best movies of 2021	Ranked by tomatometer on Rottentomato	['popular culture', 'movies and tv shows', 'tabular data']	"Context
Rotten Tomatoes is an American review-aggregation website for film and television. The company was launched in August 1998. Although the name ""Rotten Tomatoes"" connects to the practice of audiences throwing rotten tomatoes when disapproving  of a poor stage performance, the original inspiration comes from a scene featuring tomatoes in the Canadian film Léolo (1992). Rotten Tomatoes is collecting every new Certified Fresh movie into one list, creating our guide to the best movies of 2021.  Among them you’ll find blockbusters (Shang-Chi), documentaries (Lily Topples the World), awards contenders (The Green Knight), the cutting-edge in horror (The Night House).
Content
The data was obtained using webscraping. The Python language with the ""BeautifulSoup"", ""requests"", ""re"", ""pandas"" and ""numpy"" packages  was used for this process and ""SelectorGadet"" add-on, which made the work with the site easier. Each line in the database refers to one movie going from the end of the ranking to the bottom (from a worse rating to a better one). The data source is the official RottenTomatoes website with the ranking of movies from 2021.
Photo by <a href=""https://unsplash.com/@geoffreymoffett?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Geoffrey Moffett</a> on <a href=""https://unsplash.com/s/photos/movies?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	433	2624	17	michau96	best-movies-of-2021
6518	6518	tf_efficientnetv2		[]		0	16	0	akiyoshisutou	tf-efficientnetv2
6519	6519	Yahoo Finance Nifty 50 Dataset for 5 years 		['business', 'investing']		0	29	0	anaghaclement	yahoo-finance-nifty-50-dataset-for-5-years
6520	6520	wawawawawawa		[]		0	11	0	zhangyiifan	wawawawawawa
6521	6521	yolov5_without_sperate		[]		0	37	0	gfchang	yolov5-without-sperate
6522	6522	jrstc-synthetic-data		['business']		40	131	0	simonmeoni	jrstcsyntheticdata
6523	6523	Victorian COVID-19 Cases By Postcode	An expanded version of the publicly available dataset	[]	"Context
This dataset is derived from the list of cases that is published at:
https://discover.data.vic.gov.au/dataset/all-victorian-sars-cov-2-cases-by-local-government-area-postcode-and-acquired-source
It is joined with the following two sources to get postcode and population data.
https://datapacks.censusdata.abs.gov.au/datapacks/
https://www.matthewproctor.com/australian_postcodes
Content
The dataset is expanded to include 0 days for every suburb (hence the size). I have attempted to replicate some relevant statistics, such as estimated replication number using formulas found here:
https://www.hbs.edu/ris/Publication%20Files/20-112_4278525d-ccf2-4f8a-b564-2e95d0e7ca5b.pdf
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	3	16	0	dgokeeffe	victorian-australia-covid19-cases-by-postcode
6524	6524	ZUM_dataset		[]		0	17	0	twmprojekt	zum-dataset
6525	6525	Ski jumping results database	Results of (almost) all important ski jumping competitions (2009-now)	['europe', 'sports', 'categorical data', 'exploratory data analysis', 'japan']	"Context
Hello. As a big ski jumping fan, I would like to invite everybody to something like a project called ""Ski Jumping Data Center"". Primary goal is as below:
Collect as many data about ski-jumping as possible and create as many useful insights based on them as possible
In the mid-September last year (12.09.20) I thought ""Hmm, I don't know any statistical analyses of ski jumping"". In fact, the only easily found public data analysis about SJ I know is https://rstudio-pubs-static.s3.amazonaws.com/153728_02db88490f314b8db409a2ce25551b82.html
Question is: why? This discipline is in fact overloaded with data, but almost nobody took this topic seriously. Therefore I decided to start collecting data and analyzing them. However, the amount of work needed to capture various data (i.e. jumps and results of competitions) was so big and there is so many ways to use these informations, that make it public was obvious. In fact, I have a plan to expand my database to be as big as possible, but it requires more time and (I wish) more help.
Content
Data below is (in a broad sense) created by merging a lot of (&gt;6000) PDFs with the results of almost 4000 ski jumping competitions organized between (roughly) 2009 and 2021. Creation of this dataset costed me about 150 hours of coding and parsing data and over 4 months of hard work. My current algorithm can parse in a quasi-instant way results of the consecutive events, so this dataset can be easily extended. For details see the Github page:
https://github.com/wrotki8778/Ski_jumping_data_center
The observations contain standard information about every jump - style points, distance, take-off speed, wind etc. Main advantage of this dataset is the number of jumps - it's quite high (by the time of uploading it's almost 250 000 rows), so we can analyze this data in various ways, although the number of columns is not so insane.
Acknowledgements
Big ""thank you"" should go to the creators of tika package, because without theirs contribution I probably wouldn't create this dataset at all.
Inspiration
I plan to make at least a few insights from this data:
1) Are the wind/gate factor well adjusted? 
2) How strong is the correlation between the distance and the style marks? Is the judgement always fair?
3) (advanced) Can we create a model that predicts the performance/distance of an athlete in a given competition? Maybe some deep learning model?
4) Which characteristics of athletes are important in achieving the best jumps - height/weight etc.?"	160	2564	16	wrotki8778	ski-jumping-results-database-2009now
6526	6526	bigbird_large_pytorch		[]		2	31	0	shujun717	bigbird-large-pytorch
6527	6527	Chocolate Ratings	Reviews on more than 2400 chocolate bars!	['global', 'tabular data', 'cooking and recipes', 'food', 'retail and shopping']	"About this dataset :chocolate_bar:
&gt; Chocolate is paramount in our society, it is a food product made from roasted and ground cacao pods, that is available as a liquid, solid or paste, on its own or as a flavoring agent in other foods. Cacao has been consumed in some form since at least the Olmec civilization (19th-11th century BCE) and the majority of Mesoamerican people - including the Maya and Aztecs - made chocolate beverages.
&gt; The first solid chocolate bar put into production was made by J. S. Fry & Sons of Bristol, England in 1847 and today it is estimated to be an USD 130.56 billion dollar industry.
&gt; In light of all of this, this dataset contains reviews on more than 2400 different chocolate bars along with metadata and information on US and Canadian based producers. A rating scale is provided and is defined as follows:
- 4.0 - 5.0 = Outstanding
- 3.5 - 3.9 = Highly Recommended
- 3.0 - 3.4 = Recommended
- 2.0 - 2.9 = Disappointing
- 1.0 - 1.9 = Unpleasant
How to use this dataset
&gt; - Create a regression model to predict chocolate bar rating;
- Explore the most memorable features/ingredients associated with chocolate bars.
Highlighted Notebooks
&gt; - Your kernel can be featured here!
- More datasets
Acknowledgements
If you use this dataset in your research, please credit the authors.
&gt; ### Citation
&gt; Manhattan Chocolate Society, Flavors of Cacao [Internet]. Available from: http://flavorsofcacao.com/
&gt; ### License
Public Domain
&gt; ### Splash banner
Icon by Freepik.
Photo by Universal Eye available on Unsplash."	765	6106	37	andrewmvd	chocolate-ratings
6528	6528	Opioids in the US: CDC Nonfatal Overdoses	Can you use data science to show trends by region and drug over time?	['united states', 'earth and nature', 'data cleaning', 'data visualization', 'tabular data', 'drugs and medications']	"Following on from my datasets on Drug Overdose deaths in the United States, https://www.kaggle.com/craigchilvers/opioids-vssr-provisional-drug-overdose-statistics and https://www.kaggle.com/craigchilvers/opioids-in-the-us-cdc-drug-overdose-deaths, here is a dataset on non-fatal overdoses. It is broken down by age and gender, and also by State. There are also breakdowns into overall drug overdoses, heroin overdoses, opioid overdoses and stimulant overdoses.
This data set is good for tracking progress or deterioration in states over time, especially through choropleth graphs."	11	87	0	craigchilvers	opioids-in-the-us-cdc-nonfatal-overdoses
6529	6529	UPI transactions in terms of value	UPI transaction in terms of value from April'21 to Dec'21	['india', 'business', 'finance', 'banking', 'news']	"We all know that the volume of UPI transactions is skyrocketing in India. I wanted to know the market share of UPI apps in terms of their value. I have taken this from the NPCI website and planning to update it as and when it gets updated on their site.
The data is from April'21 to Dec'21. It has a collection of all the apps which use UPI and the value of monthly transactions.
There was a post on LinkedIn which showed the top apps using UPI in terms of volume and wanted to dive into the value part of things, you will be surprised to see the top player in terms of value is.
Your data will be in front of the world's largest data science community. What questions do you want to see answered? WhatsApp being a dominant player in the social media space should have been at least in the top 15 places but, the regulatory stuff and slow rollout have pushed them very far. I would like to monitor their progress, and also the rule of 30% market share (in terms of volume) is also going to affect the system one way or another I would love to see if this affects the value of the transactions and the monetisation part going forward."	5	89	0	rahulharinath	upi-transactions-in-terms-of-value
6530	6530	Base_finale		[]		2	26	1	emmanueltalba	base-finale
6531	6531	Conv_YXL_L3		[]		0	7	0	ding314	conv-yxl-l3
6532	6532	bzjstrainsets		[]		0	22	0	ujninusadit	bzjstrainsets
6533	6533	Taiwan Lottery 539		[]	"Context
Taiwan Lottery 539 data from 2007
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	4	33	0	johnnytw	taiwan-lottery-539
6534	6534	ADP_KR_p3	Advanced Data Analytics Professional (KOREA)	[]	"ADP 시험대비
문제 링크
(python) : https://www.kaggle.com/kukuroo3/problem3-python  
(R)          : https://www.kaggle.com/kukuroo3/problem3-r   
English translation version will be prepared soon"	11	143	3	kukuroo3	adp-kr-p3
6535	6535	valset1		[]		0	9	0	haianhjobs	valset1
6536	6536	BoardGameGeek Top100	Top 100 boardgames ranking	['games', 'board games', 'beginner', 'advanced', 'data cleaning']	"Context
This is the Top 100 BoardGameGeek ranking in Jan 2022.
Content
The data contains basic game information. It is suitable for beginners and advanced people since various techniques could be practiced:
- Data cleaning and preprocessing. Certain rows are intentionally left to be processed for improving the results.
- EDA
- prediction of a score of a new game
- sentiment analysis (Subtitle)
- Comparison of subcategories
- Choosing optimal parameters of a game in order to achieve maximum fun (score).
Acknowledgements
I would like to thank the BGG community and every board game player. Cheers!
Inspiration
Could you predict the interest in a new game based on this dataset?"	62	487	6	medaxone	boardgamegeek-top100
6537	6537	Acidentes no Espaço Aéreo Brasileiro (Cenipa)	Dataset e os relatórios dos acidentes no espaço Aéreo Brasileiro.	['brazil', 'aviation', 'intermediate', 'data cleaning', 'tabular data']	"Dataset of Aircraft Accidents on Brazilian Airspace
Hey Fellas!
Guys, this dataset is about Aircraft Accidents on Brazilian airspace. The agency responsible to investigate, relate, do preventions and publish reports about aircraft accidents is CENIPA (Centro de Investigaçãoe e Prevenção de Acidentes Aeronauticos). CENIPA investigate every aircraft accidents in Brazil, since flight recorder until wreckage of aircrafts.
Follow the website with dataset and reports:
datasets: http://dados.gov.br/dataset/ocorrencias-aeronauticas-da-aviacao-civil-brasileira
Reports: http://sistema.cenipa.aer.mil.br/cenipa/paginas/relatorios/relatorios.php
Cenipa Website:
https://www2.fab.mil.br/cenipa/index.php/o-cenipa"	3	74	3	markfinn1	acidentes-areos
6538	6538	paw_timm_best_model		[]		0	10	0	dwchen	paw-timm-best-model
6539	6539	Movie Datasets		[]		7	40	0	theomartinsantoso	movie-datasets
6540	6540	python-box		['computer science']		0	10	0	min7712	pythonbox
6541	6541	PetFinder-Z211-20220109081106		[]		0	17	0	hideyukizushi	petfinder-z211-20220109081106
6542	6542	Flower Classification_TPU		[]		3	26	4	andrej0marinchenko	flower-classification-tpu
6543	6543	petfinder-tfrecords-v2		[]		0	29	0	ks2019	petfinder-tfrecords-v2
6544	6544	NIFTY50 20 years Daily Trend	NIFTY50 Index Historical Trend from 1 Jan 1999 to 31-Dec-2021	['finance', 'intermediate', 'tabular data', 'regression', 'news']		5	26	2	haridattmishra	nifty50-20-years-daily-trend
6545	6545	India PMFBY statistics	Coverage and enrollment data on PMFBY/WBCIS insurance schemes of India	['india', 'agriculture', 'statistical analysis', 'tabular data', 'insurance']	"Coverage and farmers enrollment data on PMFBY (Pradhan Mantri Fasal Bima Yojana) and WBCIS (Weather based Crop Insurance Scheme) agriculture insurance schemes of India.
Data were collected from publicly available PMFBY dashboard as of 01.2022. 
Datasets include all data since the introduction of PMFBY scheme in 2018 up to Rabi 2021-22.
PMFBY/WBCIS enrollment statistics data
District-wise data on farmers actual enrollment in PMFBY/WBCIS schemes including:
Farmers demographics
Area insured
Insurance premiums
State and central government subsidies
Data were collected from https://pmfby.gov.in/adminStatistics/dashboard
PMFBY/WBCIS insurance coverage data
District-wise data on coverage terms of insurance schemes including:
Crop-wise insurance sums, premium rates and indemnity levels
Start and end dates for farmers enrollment
Selected insurance companies name and contacts
Data were collected from Insurance Premium Calculator on https://pmfby.gov.in/
Limitations
Tried to rename columns, so that theшк name would be self-explanatory. If any details are still needed feel free to request desciption update.
Some states are not represented on PMFBY enrollment statistics dashboard
States and districts names were not synchronizedbetween datasets
Acknowledgements
Kudos to PMFBY administration for making the source data publicly available online.
Thank you
Data last updated on: January 9, 2022
Updates scheduled:
- Coverage data on Kharif-2022 - June, 2022
- Enrollment data on Kharif-2022 - August, 2022"	23	172	2	pyatakov	india-pmfby-statistics
6546	6546	Pen images		[]		1	49	1	takshpanchal	pen-images
6547	6547	PetFinder-Z210-20220109073446		[]		0	5	0	hideyukizushi	petfinder-z210-20220109073446
6548	6548	Conv_YXL_L2		[]		0	6	0	ding314	conv-yxl-l2
6549	6549	OCTnipa		[]		0	12	0	niparoy1123	octnipa
6550	6550	Lioneel_messi		[]		0	6	0	katzurasharma	lioneel-messi
6551	6551	Aviation Accident Database & Synopses	The NTSB aviation accident dataset up to Feb 2021	['aviation', 'beginner', 'exploratory data analysis', 'data visualization', 'text data']	"Content
The NTSB aviation accident database contains information from 1962 and later about civil aviation accidents and selected incidents within the United States, its territories and possessions, and in international waters.
Acknowledgements
Generally, a preliminary report is available online within a few days of an accident. Factual information is added when available, and when the investigation is completed, the preliminary report is replaced with a final description of the accident and its probable cause. Full narrative descriptions may not be available for dates before 1993, cases under revision, or where NTSB did not have primary investigative responsibility.
Inspiration
Hope it will teach us how to improve the quality and safety of traveling by Airplane."	10286	83577	169	khsamaha	aviation-accident-database-synopses
6552	6552	Adversarial Attack on Imagenet Dataset	Adversarial attack dataset using FGSM on Imagenet Subset	['computer science', 'intermediate', 'advanced', 'gan', 'adversarial learning']	"Context
This is an adversarial Dataset made using an FGSM attack. You can find more about it here https://github.com/anirudh9784/Adversarial-Attacks-and-Defences
Content
There are 1000 Images that are a subset of the Imagenet Dataset."	3	171	5	anirudhyadav9784	adversarial-attack-on-imagenet-dataset
6553	6553	mail data	This dataset contains around 5000 mail data.	['email and messaging']		15	54	3	suraj452	mail-data
6554	6554	restaurant score competition		['restaurants']		0	22	0	cod666	restaurant-score-competition
6555	6555	Sanskrit (Present Tense - 3rd Person - Singular) 	(वर्थमानकालः - प्रथमपुरुषः - एकवचनम्) few examples.	[]		0	12	1	jithendrajk	sanskrit-present-tense-3rd-person-singular
6556	6556	Loan Pred		[]		0	16	0	shreyaatri	loan-pred
6557	6557	pawpularityscorefront-tfrecord-fold-0-4		[]		0	42	0	suyinchen1024	pawpularityscorefronttfrecordfold04
6558	6558	Google Data Analytics Capstone 1	12 months of Cyclistic Trip Data	['cycling', 'travel']	"Context
This is public data that you can use to explore how different customer types are using Cyclistic bikes.
Content
The data can be downloaded in here.
Each columns of feature details:
ride_id: Ride ID
rideable_type: Bike Type
started_at: Start of Ride
ended_at: End of Ride
start_station_name: Start Station Name
start_station_id: Start Station ID
end_station_name: End Station Name
end_station_id: End Station ID
start_lat: Start Latitude
start_lng: Start Longitude
end_lat: End Latitude
end_lng: End Longitude
member_casual: Type of Membership
Acknowledgements
Acknowledgements to Google and Coursera for introducing this datasets. You can download the Cyclistic’s historical trip data here. The data has been made available by
Motivate International Inc. under this license.
Inspiration
How different customer types are using Cyclistic bikes?"	1	47	0	rayyuda	google-data-analytics-capstone-1
6559	6559	Superstore data		['business']	"Context
This is a data set that is sales data from a retail store. It is often used with Tableau.
Content
The data set was downloaded from the Tableau data set website. It represents one calendar year.
Acknowledgements
Thank you to the Tableau Public Resources
Inspiration
What are the projected sales and profit for the coming year?"	5	43	0	jeromejohn	superstore-data
6560	6560	YOLOv5 Simplified: GBR Train Output	That dataset is known for our YOLOv5 Simplified Series.	['business']		0	26	1	dinowun	yolov5-simplified-gbr-train-output
6561	6561	NIPA3k		[]		0	16	0	niparoy1123	nipa3k
6562	6562	autogluon_new		[]		0	4	0	sihead	autogluon-new
6563	6563	Covid-19 Bangladesh Dataset (Daily cases)	Day wise number of total tests, confirmed cases and death cases of Bangladesh.	['asia', 'covid19']	"Context
The covid-19 pandemic has taken over all world. In this dataset, i want to dive in the daily cases of my country, Bangladesh to get more insights about this pandemic.
Content
This dataset consists of 646 rows and 6 columns. The columns contains dates, lab tests of the specific dates, confirmed cases(positive cases), positivity rate, total number of deaths and death  rate. The data starts from 3rd April,2020 and 8th January,2022.
Inspiration
I wanna know which upcoming months are risky compared to previous dataset. And which months should be in total lock-down in respect to this dataset."	5	54	0	azizulhakim98	covid19-bangladesh-dataset-daily-cases
6564	6564	stylegan_weights		[]		40	53	1	edwardyangiscool	stylegan-weights
6565	6565	Facial Emotion Dataset	Facial Emotion Dataset for image generation using gan and image recognition	['online communities']		4	75	0	subramanyasubbu	images-of-different-emotions-combined
6566	6566	NIPA_OCT		[]		2	20	0	niparoy1123	nipa-oct
6567	6567	Vahan Website CAPTCHA dataset	Labelled CAPTCHA dataset scraped from vahan.nic.in know your vehicle page	[]		0	35	1	sohamrakhunde	vahan-website-captcha-dataset
6568	6568	pawpularity-score-front		[]		0	71	0	nizhen	pawpularityscorefront
6569	6569	yolov5+L		[]		0	13	0	getcoin	yolov5l
6570	6570	CQ500 YoloV5 Model2		[]		1	12	0	fereshtej	cq500-yolov5-model2
6571	6571	AreasinCaliforniaWithHighAQI	California Air Quality 	[]		1	7	0	dapoayanbode	areasincaliforniawithhighaqi
6572	6572	Population by Country (1960 - 2020)	Provided by The World Bank	['people', 'social science', 'demographics', 'tabular data']	"Introduction
This dataset consists of countries population from 1960 to 2020.
Acknowledgements
Source: https://data.worldbank.org/indicator/SP.POP.TOTL"	11	88	2	aliaamiri	historical-worldwide-countries-population
6573	6573	All Stocks Data of Indian Stock Market(1 Year)	Stock Market, NSE, 1 Year, 2435 Stocks	['business']	"After some rigorous SQL queries and coding on python. I made this dataset. In this dataset, all stocks of the Indian Stock Market are present a total of 2435 stocks. The data is of 1-year rows represent stock name and column represent date and I have filled the table with closing price.
Enjoy and do some stock price predictions."	102	658	6	gmkeshav	all-stocks-data-of-indian-stock-market1-year
6574	6574	comment_text_data		[]		0	3	0	kalinayan	comment-text-data
6575	6575	mp2q2data17		[]		14	15	0	miladaf	mp2q2data17
6576	6576	bikedata		[]		0	11	0	saurim	bikedata
6577	6577	Amazon Mobile Phone Reviews Dataset	Amazon Dataset : Reviews on smart phones 	['nlp', 'naive bayes', 'ratings and reviews']		9	110	0	rajatagg	amazon-mobile-phone-reviews-dataset
6578	6578	IMDB_data		[]		0	13	0	kalinayan	imdb-data
6579	6579	dataset		[]		0	10	0	sumankumar2508	dataset
6580	6580	COTS-COCO-StratifiedK-5-folds (4919 imgs)		[]		2	32	1	nyanswanaung	cotscocostratifiedk5folds-4919-imgs
6581	6581	Laptop	Dữ liệu laptop thu thập được từ Fptshop, PhongVu, Dienmayxanh	[]	"Nguồn dữ liệu về laptop thu thập được từ 3 trang web:
https://fptshop.com.vn/
https://www.dienmayxanh.com/
https://phongvu.vn/
Có 17 trường dữ liệu:
id: ID laptop
brand: Thương hiệu của laptop
cpu: Bộ xử lý trung tâm. 
&gt;CPU đóng vai trò như não bộ của một chiếc Laptop, tại đó mọi thông tin, thao tác, dữ liệu sẽ được tính toán kỹ lưỡng và đưa ra lệnh điều khiển mọi hoạt động của Laptop.
cpu_GHz: Tốc độ xử lý CPU hay tần số tính toán và làm việc của nó được đo bằng đơn vị GHz hoặc MHz
&gt; Nếu cùng một dòng chip ví dụ như Core i3 thì xung nhịp cao hơn đồng nghĩa với tốc độ xử lý nhanh hơn, khả năng làm việc tốt hơn. Tuy nhiên, nếu giữa 2 dòng chip khác nhau như Core i3 hai nhân xung nhịp 2.2GHz và Intel Pentium Dual core 2.3GHz thì không thể so sánh ngay được bởi vì tốc độ xử lý của Laptop còn phụ thuộc rất nhiều yếu tố khác
cpu_brand: Hãng sản xuất CPU
ram: Bộ nhớ tạm (đơn vị GB)
&gt; RAM là bộ nhớ tạm của máy giúp lưu trữ thông tin hiện hành để CPU có thể truy xuất và xử lý.
scrsize: Kích thước màn hình laptop (đơn vị inch)
gpu: đơn vị xử lý đồ họa chuyên dụng, nhiệm vụ chính là tăng tốc, xử lý đồ họa cho bộ xử lý của CPU
&gt; Ngoài tác vụ đồ họa, GPU còn xử lý thông tin đa luồng, song song và bộ nhớ ở tốc độ cao.
gpu_brand: Hãng sản xuất GPU
memory: Dung lượng lưu trữ (đơn vị GB)
drive_type: Loại ổ đĩa
opersystem: Hệ điều hành
since: Năm sản xuất
shop: Cửa hàng phân phối
price: Giá sản phẩm
url: Đường dẫn đến sản phẩm"	7	54	0	sthnhcng	laptop
6582	6582	DirtyDataSet		[]		4	19	0	sumankumar2508	dirtydataset
6583	6583	petfinder-always-split-data		[]		1	91	0	hakase1	petfinderalwayssplitdata
6584	6584	Glassdoor 50 best jobs in USA from 2015-2021	see how top jobs change over time	['employment', 'income', 'jobs and career']	"Context
All jobs are not created equal. Take a look at the dynamic nature of high demand job over time. Make your career next move worthwhile.
Acknowledgements
cover photo credit: https://www.pexels.com/photo/person-in-in-red-sweater-holding-silver-pen-7108465/
Inspiration
changes in high demand jobs over time
salary vs satisfaction"	138	1140	10	prasertk	glassdoor-best-jobs-in-usa-from-20152021
6585	6585	COVID-19 Deaths Data		[]		3	30	0	youssefmaher	covid19-deaths-data
6586	6586	kqi_ex024_1		[]		0	5	0	anonamename	kqi-ex024-1
6587	6587	Public Charging Stations in Hawaii	Dataset of Public Charging Stations in Hawaii	['business', 'energy', 'automobiles and vehicles', 'intermediate', 'electricity']	"Context
Public Charging Stations in Hawaii
Source
https://data.hawaii.gov/d/95x5-qrxh
Last updated
https://data.hawaii.gov/data.json : 2014-10-30"	22	147	8	meetnagadia	public-charging-stations-in-hawaii
6588	6588	vit_large_patch16_224_3multitask_nodataleak		[]		0	22	0	shawndong98	vit-large-patch16-224-3multitask-nodataleak
6589	6589	jabcare0109		[]		0	15	0	lxinha	jabcare0109
6590	6590	Football Coaches - Stats and Trophies	Coaches -info & trophies - Top 5 Leagues in European football	['football', 'europe', 'sports', 'beginner', 'exploratory data analysis']	"all_coaches.csv
Contains basic information about football coaches who work in the top 5 European leagues.
all_tropheys.csv
Contains trophies won by these managers."	8	55	0	vaske93	football-coaches-stats-and-tropheys
6591	6591	swin_large_patch4_win7_224_3multitask_nodataleak		[]		0	12	0	shawndong98	swin-large-patch4-win7-224-3multitask-nodataleak
6592	6592	scRNA-seq Human Ewing sarcoma EW-8 GSE171205	Human Ewing sarcoma cell line EW-8 with different levels of EWSR/FLI1	['genetics', 'biology', 'biotechnology', 'cancer']	"Data and Context
Data - results of single cell RNA sequencing, i.e. rows - correspond to cells, columns to genes (or vice versa).
value of the matrix shows how strong is ""expression"" of the corresponding gene in the corresponding cell.
https://en.wikipedia.org/wiki/Single-cell_transcriptomics
Particular data:
GEO: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE171205
Paper: 
https://www.biorxiv.org/content/10.1101/750539v1
https://link.springer.com/article/10.1007/s13402-021-00640-x
Human Ewing sarcoma cell line EW-8 with different levels of EWSR/FLI1 using siRNA-EWSR1 were collected and profiled using single-cell RNA-seq. The single-cell RNA-sequencing data identified diverse reponses leading to the prediction of two rare cell identities in an unperturbed population (si-Control) with stem-like and dormant-like characterisitics. These newly discovered cell states may play important roles in undrstanding cancer heterogeneity, drug resistance and recurrence in Ewing sarcoma .
Overall design  Three population of EW-8 cells were studied for their gene expression information in a single cell level (Unperturbed EW-8 or si-Control, Dormant 1 population or si-EWSR1-48h, Dormant 2 population or si-EWSR1-120h)
Contributor(s)  Houghton P, Khoogar R, Chen Y, Lai Z
Public on Dec 02, 2021
See also tutorials:
Course at Sanger's institute
https://scrnaseq-course.cog.sanger.ac.uk/website/tabula-muris.html
Course at CZ-hub:
https://chanzuckerberg.github.io/scRNA-python-workshop/intro/about
On kaggle - copies of the notebooks and data from the course above
https://www.kaggle.com/aayush9753/singlecell-rnaseq-data-from-mouse-brain
Inspiration
Single cell RNA sequencing is important technology in modern biology,
see e.g.
""Eleven grand challenges in single-cell data science""
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1926-6
Also see review :
Nature. P. Kharchenko: ""The triumphs and limitations of computational methods for scRNA-seq""
https://www.nature.com/articles/s41592-021-01171-x"	0	25	0	alexandervc	scrnaseq-human-ewing-sarcoma-ew8-gse171205
6593	6593	No Show Appointments		[]		0	13	0	youssefmaher	no-show-appointments
6594	6594	Butterfly Image Classification 75 species	9285 train, 375 test, 375 validation images 224 X 224 X 3 jpg format 	['biology', 'computer vision', 'classification', 'cnn', 'image data', 'tensorflow']	"Train, Test. Validation data set for 75 butterfly species. All images are 224 X 224 X 3 in jpg format .
Train set consists of 9285 images partitioned into 75 sub directories one for each species.
Test set consists of 375 images partitioned into 75 sub directories with 5 test images per species.
Valid set consists of 375 images partitioned into 750 sub directories with 5 validation images per species.
3 CSV files are included. The butterflies.csv file consists of 3 columns with 10,035 rows, one row for each image
in the dataset. The columns are filepaths, labels and data set. The filepaths column contains the relative path to the image.
The labels column contains the species label associated with the image file. The data set column specifies which
dataset (train, test or valid) the associated image belongs to.  The class_dict.csv file is provided to enable use of the trained model.
It consists of 6 columns, class_index, class, SCIENTIFIC NAME, height, width, and scale by and 75 rows one row for each species.
class_index is the  integer index of the class. class is the species name which is the common name for the butterfly species.
SCIENTIFIC NAME is the official scientific name of the species. Height and width are the values used by the model  when trained.
scale by is the value to scale the image pixels by. For EfficientNet model the scale is 1 since these models expect pixels in the range 0 to 255.
The CLASS NAMES.csv file has 2 columns by 75 rows. The COMMON NAME column contains the common name of the butterfly and the
SCIENTIFIC NAME column contains the associated scientific name of the species. Finally the 6 images file contains 6 butterfly images. 
This file is used to demonstrate use of the prediction function within the kernel."	1029	9552	31	gpiosenka	butterfly-images40-species
6595	6595	Feline Pregnancy Dataset	Estradiol & Progesterone Data for Different Felines	[]	"<b>DATA SOURCE</b>
https://data.mendeley.com/datasets/h8mcfz95bf/2
<b>LIST OF FEATURES</b>
0-4 : Plasma or serum concentrations
0-1 : Estradiol
0 : Anoestrus or interestrus Basal [Ciruclating estradiol (pg/ml)] 
1 : Estrus (Peak) [Blood E2pg/ml]
2-4 : Progesterone
2 : Basal (i.e. not diestrus) [Blood p4 ng/ml]
3 : Diestrus/luteal phase (Peak) | Non-pregnant luteal phase NPLP [Blood p4ng/ml]
4 : Diestrus/luteal phase (Peak) | Pregnant luteal phase PLP [Blood p4ng/ml]
5-9 : Fecal metabolites
5-6 : Fecal estradiol metabolites (FEM) 
5 : Anoestrus or interestrus Basal
6 : Estrus (Peak)
7-9 : Fecal progesterone metabolites (FPM)
7 : Basal (i.e. not diestrus)
8 : Diestrus/luteal phase (Peak) | Non-pregnant luteal phase NPLP
9 : Diestrus/luteal phase (Peak) | Pregnant luteal phase PLP"	3	141	7	shtrausslearning	feline-pregnancy
6596	6596	brittlenes		[]		0	7	0	selmayaman	brittlenes
6597	6597	Jigsaw Toxic Datasets		[]		0	15	1	coldfir3	jigsaw-toxic-datasets
6598	6598	Unitary Toxic Models	Models downloaded from https://huggingface.co/unitary	['internet']		0	29	2	coldfir3	unitary-toxic-models
6599	6599	American Chess Is Great Again	900 Chess Transfers, technical information	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-08-08</code> <a href=""https://fivethirtyeight.com/features/american-chess-is-great-again/"" target=""_blank"" rel=""nofollow"">American Chess Is Great Again</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 900 samples along with Transfer Date, Id, technical information and other features such as:
- Federation
- Url
- and more.
How to use this dataset
&gt; - Analyze Form. Fed in relation to Transfer Date
- Study the influence of Id on Federation
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	23	285	3	yamqwe	chess-transferse
6600	6600	Mayweather Marketing Tactics	How to turn undefeated record into a marketing behemoth	['beginner', 'news']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-08-18</code> <a href=""https://fivethirtyeight.com/features/mayweather-is-defined-by-the-zero-next-to-his-name/"" target=""_blank"">Mayweather Is Defined By The Zero Next To His Name</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 2000 samples along with Date, Url, technical information and other features such as:
- Name
- Wins
- and more.
How to use this dataset
&gt; - Analyze Date in relation to Url
- Study the influence of Name on Wins
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	13	198	4	yamqwe	undefeated-boxerse
6601	6601	Tokyo Weather Data 	Over 40 years of weather data covering numerous meteorological phenomena	['asia', 'weather and climate', 'time series analysis', 'tabular data', 'japan']	"Content
The main Tokyo weather dataset covers over 40 years of recordings and includes 18 different meterogical variables.
Acknowledgements
All information measured by the Japanese Meterological Agency.
Inspiration
I created this dataset to practice time series forecasting."	18	119	1	tom111989	tokyo-weather-data
6602	6602	Live Tweets During Mayweather vs Mcgregor Fight	The Mayweather-McGregor Fight, Told Through Emojis	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-08-30</code> <a href=""https://fivethirtyeight.com/features/the-mayweather-mcgregor-fight-as-told-through-emojis/"" target=""_blank"">The Mayweather-McGregor Fight As Told Through Emojis</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 10000 samples along with Text, Retweeted, technical information and other features such as:
- Emojis
- Created At
- and more.
How to use this dataset
&gt; - Analyze Screen Name in relation to Text
- Study the influence of Retweeted on Emojis
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	10	102	1	yamqwe	mayweather-vs-mcgregore
6603	6603	zone2json		[]		0	5	0	sparksim	zone2json
6604	6604	Aquarium Dataset	Dataset of 6 types of Aquatic Animals, from Roboflow	['fish and aquaria']	"Aquarium Dataset
This dataset is an object detection dataset. It has 7 classes listed down below
fish
stingray
jellyfish
penguin
shark
puffin
starfish
These are common animals that are seen at an aquarium and are more of a toy dataset. Check out the faster-rcnn training notebook to get started. 
Data obtained from roboflow: https://public.roboflow.com/object-detection/aquarium"	21	153	2	sharansmenon	aquarium-dataset
6605	6605	petfinder_exp53		[]		4	24	0	titericz	petfinder-exp53
6606	6606	Pickles		['neural networks']	"Context
There are various ways to identify network traffic. This dataset is created with idea of using and improving Deep Learning methods for this cause.
Content
This dataset contains pickled, processed data, filtered using nfstream (nDPI), taken from local network traffic, obtained using mirroring port."	0	14	0	nedjon	pickles
6607	6607	classgrades		[]		0	21	0	selmayaman	classgrades
6608	6608	brittleness		[]		0	6	0	selmayaman	brittleness
6609	6609	gbr_efficientdet_d0		[]		0	23	0	cyrusmanuel	gbr-efficientdet-d0
6610	6610	BEiT_large_patch16_offline	beit_large_patch16 resolution 224 and 512 to be used offline	[]		2	43	0	bengxue	beit-large-patch16-512
6611	6611	Capital and Small Letters		[]		4	8	1	suryatejamenta	capital-and-small-letters
6612	6612	torchvision 0.11.1 cp37		[]		0	18	0	rkstgr	torchvision-0111-cp37
6613	6613	swin_large_384_embeddings		[]		0	5	0	darkravager	swin-large-384-embeddings
6614	6614	GitHub Data		[]		0	23	0	mohammadkumail	github-data
6615	6615	GD Data for Jigsaw Toxic Severity		[]		2	20	0	kryval	gd-data-for-jigsaw-toxic-severity
6616	6616	Braindlseg		[]		2	26	1	anupriyashetty	braindlseg
6617	6617	List and Cause of Banned Games 	The list of games that have been banned and cause of prohibition.	['games', 'board games', 'card games', 'video games', 'nlp']	"Introduction
The dataset provides information on games that have been prohibited by various countries for a variety of reasons, resulting in governments making political decisions and establishing strict regulations against games that involve violence or violate religious or cultural feelings. This is a list of video games that have been prohibited or outlawed by various countries throughout the world. Governments that have outlawed video games have been condemned for increasing digital piracy, reducing commercial prospects, and infringing on people's rights.
Content
Country:  This feature contains the names of countries that have banned a range of games in their respective countries.
Banned Games:  The attribute contains a list of games that are or were prohibited.
Reason: The characteristic offers insight into the reasons why some games were banned in certain nations.
Inspiration
With Meta's latest revelations about the Metaverse, which includes virtual reality. For humans, endless possibilities for developing virtual reality games, narratives, and many other things would constitute a new layer of reality. This dataset, on the other hand, was designed with that goal in mind: to create and encourage them to play games without the intrusion of laws and regulations that may someday restrict our favorite games. This dataset contains the responses to the following questions, which will boost user engagement with games and illustrate useful methods for various stakeholders and businesses.
What is the justification for governments banning games?
What should we do if we want to distribute games in particular countries but don't want to be banned?
What is the primary reason for the games' prohibition?
and many more...
We might be able to deduce additional underlying information by applying machine learning algorithms and models. With difficulties like these, our detective skills could come in helpful. It's exactly as Arthur Conan Doyle described it:
                            “The world is full of obvious things which nobody by any chance ever observes.”
Acknowledgements
References : 
1. Wikipedia :"	24	429	1	sarveshtalele	list-and-cause-of-banned-games
6618	6618	nba_dataset		[]		1	23	0	rochitjain	nba-dataset
6619	6619	swin_large_fold(5-10)		[]		0	4	0	darkravager	swin-large-fold510
6620	6620	swin_large_224		[]		0	4	0	darkravager	swin-large-224
6621	6621	lfastexp3		[]		0	4	1	nischaydnk	lfastexp3
6622	6622	roformer_tpu		[]		5	23	0	zaakciiru	roformer-tpu
6623	6623	PetFinder-Model27		[]		0	3	0	lftuwujie	petfindermodel27
6624	6624	Total Government Expenditure on Education	Total general government expenditure on education 1970 to 2019	['education', 'finance', 'government']	"Context
Total general government expenditure on education (all levels of government and all levels of education), given as a
share of GDP.  The last two decades have seen a small but general increase in the share of income that countries devote to education.
Content
Although the data is highly irregular due to missing observations for many countries, we can still observe a broad upward trend for the majority of countries. Specifically, it can be checked that of the 88 countries with available data for 2000/2010, three-fourths increased education spending as a share of GDP within this decade. As incomes – measured by GDP per capita – are generally increasing around the world, this means that the total amount of global resources spent on education is also increasing in absolute terms.
The reference years reflect the school year for which the data are presented. In some countries the school year spans two calendar years (for example, from September 2010 to June 2011); in these cases the reference year refers to the year in which the school year ended (2011 in the example).
Acknowledgements
Data publisher's source: UNESCO Institute for Statistics.
Published by: World Development Indicators - World Bank (2021.07.30).
Link: http://data.worldbank.org/data-catalog/world-development-indicators
Dataset: https://ourworldindata.org/global-rise-of-education
Inspiration
Which countries have the biggest expenditures when it comes to education?"	3	32	0	luxoloshilofunde	total-government-expenditure-on-education
6625	6625	sparql-csv		[]		0	6	0	efikopsacheili	sparqlcsv
6626	6626	IMDB_dataset		[]		0	10	0	kalinayan	imdb-dataset
6627	6627	gbr_my_models		[]		5	159	0	pedrocouto39	gbr-my-models
6628	6628	beit_large_patch16_224_3multitask_nodataleak		[]		0	18	0	shawndong98	beit-large-patch16-224-3multitask-nodataleak
6629	6629	sensorss		[]		0	0	0	monagalal	sensorss
6630	6630	Drivers_analysis_foundation		[]		1	2	0	sparksim	drivers-analysis-foundation
6631	6631	Object Detection	Image Processing for Data Building	['arts and entertainment', 'computer science', 'image data']	"Context
Data preparation for the main datasets.
For example, Classification of Handwritten Letters
Content
Images with many visual objects.
Acknowledgments
Thanks for all comments and improvements in forked notebooks.
Inspiration
What is the best way to detect hundreds of objects in the images?"	50	3218	13	olgabelitskaya	object-detection
6632	6632	EfficientNet-Swin-Weights-C		['exercise']		0	6	0	kittenraidrua	efficientnet-swin-weights-c
6633	6633	Ee nagaraniki emaindi script data 	Character dialogue script CSV for the movie ee nagaraniki emaindi	['arts and entertainment']		0	17	0	rayehaarika	ee-nagaraniki-emaindi-script-data
6634	6634	cosine_similarity_csv		[]		0	13	0	mihikagaonkar	cosine-similarity-csv
6635	6635	High-resolution GeoTIFF images of climatic data	GeoTIFF images of Monthly Maximum Temperature  from 2010 to 2018	['weather and climate', 'intermediate', 'computer vision', 'image data']	"Content
Monthly maximum temperature GeoTIFF images for 2010-2018. These images are generated from observed data, with Global Climate Models (also know as General Circulation Models)
The monthly average maximum temperature (°C) is the climatic variable represented in the images (That is, if it is January, we compute the average of all the daily maximum temperatures for the 31 days...)
The spatial resolution is 2.5 minutes (~21 km2). The Folder cotains 109 GeoTiff (.tif) files, for each month of the year (January is 1; December is 12), for the period 2010-2018.
Here, a notebook of A Quick Look at GeoTiff Temperature Images
Acknowledgements
Thanks to the Climatic Research Unit, University of East Anglia (United Kingdom) and the  International Journal of Climatology.
Inspiration
These GeoTIFF images have the potential to be used in computer vision applications for the climate and beyond.
For a more extensive and multivariable analysis, consult the Dataset Monthly precipitation totals GeoTIFF pictures for 2010-2018"	46	713	11	abireltaief	highresolution-geotiff-images-of-climatic-data
6636	6636	Superstore sales		[]		8	30	0	anmolprakash121	superstore-sales
6637	6637	Resume pdf		[]		4	35	0	anuvagoyal	resume-pdf
6638	6638	Italian Food Recipes 		['cooking and recipes']		2	24	0	edoardoscarpaci	italian-food-recipes
6639	6639	Vision Transformer		['eyes and vision']		0	13	1	sakshamdwivedi10	vision-transformer
6640	6640	Historical data on the trading of cryptocurrencies	Archive data on cryptocurrencies trading	['business', 'finance', 'currencies and foreign exchange']	"Context
This is historical data on cryptocurrency tradings for the period from 2016-01-01 to 2022-01-08.
If you enjoy this dataset please upvote so I can see it is popular and I need to update it.
Thank you!
Content
This dataset will be good for data analysis in predicting the price for digital cryptocurrencies."	937	8410	55	georgezakharov	historical-data-on-the-trading-of-cryptocurrencies
6641	6641	SQuAD v1 data preprocessed with Spacy new		[]	"Context
The script created this dataset:
https://www.kaggle.com/tusonggao/squad-v1-data-preprocess-with-spacy-new/notebook"	2	12	0	tusonggao	squad-v1-data-preprocessed-with-spacy-new
6642	6642	instagram		[]		8	38	0	lukasneubauer	instagram
6643	6643	Solar-car weights yolov5n	Weights for Digital Control System project	['exercise', 'renewable energy']		0	28	0	danielaltanwing	solarcar-weights-yolov5n
6644	6644	Solar-car weights yolov5s	Weights for Digital Control System project	[]		2	9	0	danielaltanwing	solarcar-weights-yolov5s
6645	6645	simpsons-dataset	dataset with the Simpsons characters	[]		0	23	0	anacomo	simpsonsdataset
6646	6646	auto mpg in R Module		[]		1	17	0	avilashsen	auto-mpg-in-r-module
6647	6647	French_Banks_Grades_Appstores_CSV	It indicates the monthly grades that two French banks obtain, on the AppStore.	[]		0	11	0	nspaniol	french-banks-grades-appstores-csv
6648	6648	Baseball_Dataset_2018	Baseball statistics from 1871 to 2015.	['baseball']		0	35	0	nspaniol	baseball-dataset-2018
6649	6649	images		[]		0	11	0	pranavtushar	images
6650	6650	modellll		[]		0	10	0	luatvytran	modellll
6651	6651	groceries		[]		3	26	0	norhazifaharum	groceries
6652	6652	Finance		[]		1	19	0	pranavtushar	finance
6653	6653	cait-1758		['arts and entertainment']		0	8	0	honihitak	cait-1758
6654	6654	fastai-tuneaugs-large-nowarp-656463	5-fold models for pawpularity contest	[]		0	9	0	bachan	fastai-tuneaugs-large-nowarp-656463
6655	6655	Retail Sales: Clothing & Clothing Accessory Stores	https://fred.stlouisfed.org/series/MRTSSM448USN	['social science']	"Source: U.S. Census Bureau
Release: Monthly Retail Trade and Food Services
Units:  Millions of Dollars, Not Seasonally Adjusted
Frequency:  Monthly
The most recent month's value of the advance estimate based on data from a subsample of firms from the larger Monthly Retail Trade Survey is available at https://fred.stlouisfed.org/series/RSCCASN
Information about the Monthly Retail Trade Survey can be found on the Census website at https://www.census.gov/retail/mrts/about_the_surveys.html
Suggested Citation:
U.S. Census Bureau, Retail Sales: Clothing and Clothing Accessory Stores [MRTSSM448USN], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/MRTSSM448USN, January 8, 2022."	3	68	0	ahmedmohameddawoud	retail-sales-clothing-clothing-accessory-stores
6656	6656	CAM_images		[]		0	20	0	lnhtrang	cam-images
6657	6657	TED Talk Transcripts (2006 - 2021)	Data on 4000+ TED talks scraped from the Official TED website on 7/4/2021	['education', 'social science', 'text data']	"Context
This dataset had been created as a part of a Quantitative Text Analysis project I had to complete during my Masters' degree. I wanted to explore the content of TED talks (being a big TED fan myself). There were datasets available at the time, but most of them were outdated. Also, I wanted to try my hand at web scraping too! So, this data took shape 😄 
Content
The complete details on the data acquisition process is available here: https://deepnote.com/@ramshankar-yadhunath/Scraping-TED-fRqC4ebhTRaNrtcOSrIXMQ. I would highly recommend having a read if you are interested in using web scraping as a data acquisition technique.
Acknowledgements
A big shoutout to the work by @rounakbanik with https://www.kaggle.com/rounakbanik/ted-talks. That was my starting point. Also, Vishal Gupta's https://github.com/The-Gupta/TED-Scraper is a useful resource."	4	51	0	thedatabeast	ted-talk-transcripts-2006-2021
6658	6658	Unet_weights		[]		0	4	0	jisnava	unet-weights
6659	6659	aritificialneural_network		[]		1	18	0	abrahamanderson	aritificialneural-network
6660	6660	tps-jan-2022		[]		1	36	4	andrej0marinchenko	tpsjan2022
6661	6661	tfimm tensorflow image models		['arts and entertainment']		0	28	0	rizkykiky	tfimm-tensorflow-image-models
6662	6662	resnet_models		[]		0	4	0	riadalmadani	resnet-models
6663	6663	Company IPO's		['business']		0	3	0	pranoypatra	company-ipos
6664	6664	Hong Kong MarkSix 2008- 2021	Hong Kong MarkSix history from 2008 - 2021	['history']		4	34	1	jonathan888lam	hong-kong-marksix
6665	6665	COTS-COCO-5-folds (4919 imgs)	Dataset is splitted using GroupK 	['earth and nature']		1	86	1	nyanswanaung	cotscoco5folds-4919-imgs
6666	6666	Used car price dataset	Use this data for your machine learning model to predict used car price	['beginner', 'exploratory data analysis', 'linear regression', 'tabular data']		6	78	4	rounak02	dataset
6667	6667	BTC_CNN_2021_imgs		[]		0	14	0	karolguzikowski	btc-cnn-2021-imgs
6668	6668	Yolo v5		[]		0	11	0	udaykumarkb	yolo-v5
6669	6669	Black Mask		[]		0	20	1	kushv16	black-mask
6670	6670	Images_CLAHE		[]		0	21	1	kushv16	images-clahe
6671	6671	COVID-19 Variants Worldwide Evolution	Virus Variants Evolution (including Delta and Omicron) from Our World in Data	['healthcare', 'public health', 'covid19']	"<img src=""https://images.newscientist.com/wp-content/uploads/2020/02/11165812/c0481846-wuhan_novel_coronavirus_illustration-spl.jpg"">
Context
The data is about COVID-19 variants (see more details about this here).
Data is collected daily from Our World in Data GitHub repository for covid-19, merged and uploaded. 
Content
The data (COVID-19 Variants) contains the following information:
* location- this is the  country for which the variants information is provided; 
 date - date for the data entry; 
* variant - this is the variant corresponding to this data entry;
 num_sequences - the number of sequences processed (for the country, variant and date);
 perc_sequences - percentage of sequences from the total number of sequences (for the country, variant and date);
 num_sequences_total - total number of sequences (for the country, variant and date);
Acknowledgements
I would like to specify that I am only making available Our World in Data collected data about COVID-19 variants to Kagglers. My contribution is very small, just daily collection, merge and upload of the updated version, as maintained by Our World in Data in their GitHub repository.
Inspiration
Track COVID-19 variants in the World, answer instantly to your questions: 
- What variants are sequenced and in which country? 
- How the variants evolved and where? 
- Where the diversity of the variants is large and where only one variants are proliferating?
Combine this information with the vaccination and daily COVID cases datasets to follow the complex dynamic of virus spreading, morphing in variants, and the effect of vaccination on limiting the virus spread."	401	5155	12	gpreda	covid19-variants
6672	6672	The Big Flaw In Online Movie Ratings	Al Gore’s New Movie Exposes The Big Flaw In Online Movie Ratings	['arts and entertainment', 'movies and tv shows', 'beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-09-07</code> <a href=""https://fivethirtyeight.com/features/al-gores-new-movie-exposes-the-big-flaw-in-online-movie-ratings/"" target=""_blank"" rel=""nofollow"">Al Gore’s New Movie Exposes The Big Flaw In Online Movie Ratings</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 80000 samples along with Category, Mean, technical information and other features such as:
- 8 Pct
- 9 Pct
- and more.
How to use this dataset
&gt; - Analyze 5 Pct in relation to Average
- Study the influence of 2 Votes on Respondents
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	32	387	2	yamqwe	inconvenient-sequele
6673	6673	Handwritting		[]		2	23	0	udaykumarkb	handwritting
6674	6674	Predicting the NFL!	Can You Beat FiveThirtyEight's NFL Predictions?	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-09-29</code> <a href=""https://projects.fivethirtyeight.com/nfl-predictions-game/"" target=""_blank"" rel=""nofollow"">Can You Beat FiveThirtyEight's NFL Predictions?</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/nfl-elo-game"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/nfl-elo-game</a></p>
This dataset was created by FiveThirtyEight and contains around 20000 samples along with Team2, Playoff, technical information and other features such as:
- Score2
- Elo Prob1
- and more.
How to use this dataset
&gt; - Analyze Season in relation to Result1
- Study the influence of Team1 on Elo1
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	8	205	2	yamqwe	nfl-elo-gamee
6675	6675	What The World Thinks Of Trump?	What The World Thinks Of Trump	['politics', 'beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-09-18</code> <a href=""https://fivethirtyeight.com/features/what-the-world-thinks-of-trump/"" target=""_blank"" rel=""nofollow"">What The World Thinks Of Trump</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 0 samples along with Hungary, South Africa, technical information and other features such as:
- Brazil
- Kenya
- and more.
How to use this dataset
&gt; - Analyze Russia in relation to Japan
- Study the influence of Uk on Spain
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	15	226	1	yamqwe	trump-world-truste
6676	6676	USA Key Economic Indicators	Key economic data for United States in monthly and quarterly timeframes.	['business', 'economics', 'time series analysis']	"Context
Domino’s Pizza, like many other restaurant chains, is getting pinched by higher food costs. The company’s chief executive, Richard Allison, anticipates “unprecedented increases” in the company’s food costs, which could jump by 8-10%. He said that is three to four times what the pizza chain would normally expect in a year.
This leads to the paramount issue of inflation which affects every aspects of the economy, from consumer spending, business investment and employment rates to government programs, tax policies, and interest rates. The recent release of consumer inflation data showed prices rose at the fastest pace since 1982. Inflation forecasting is key in the conduct of monetary policy and can be used in many other ways such as preserving asset values.  This dataset is a consolidated macroeconomic official statistics from 1981 to 2021, containing data available in month and quarterly format.
Content
The Core Consumer Price Index (ccpi) measures the changes in the price of goods and services, excluding food and energy due to their volatility. It measures price change from the perspective of the consumer. It is a often used to measure changes in purchasing trends and inflation.
Do note there are some null values in the dataset.
Acknowledgements
All data belongs to the U.S. Bureau of Economic Analysis official release, and are retrieved from FRED, Federal Reserve Bank of St. Louis.
Inspiration
What are some noticeable patterns or seasonality of the economy? What are the current trends of the economy?
Which indicators has an effect on Core CPI or vice-versa based on predictive power or influence?
Quarterly data and monthly data can be merged with forward-fill or interpolation methods.
What is the forecast of Core CPI in 2022?"	48	421	7	calven22	usa-key-macroeconomic-indicators
6677	6677	The (Very) Long Tail Of Hurricane Recovery	The (Very) Long Tail Of Hurricane Recovery	['beginner']	"About this dataset
&gt; <p><strong>See Readme for more details.</strong><br>
This repository contains a selection of the data -- and the data-processing scripts -- behind the articles, graphics and interactives at FiveThirtyEight.</p>
<ul>
<li><code>2017-09-19</code> <a href=""https://projects.fivethirtyeight.com/sandy-311/"" target=""_blank"" rel=""nofollow"">The (Very) Long Tail Of Hurricane Recovery</a></li>
</ul>
<p>We hope you'll use it to check our work and to create stories and visualizations of your own. The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 2000 samples along with Mose, Acs, technical information and other features such as:
- Nysemergencymg
- Hpd
- and more.
How to use this dataset
&gt; - Analyze Chall in relation to Sbs
- Study the influence of Dob on Nycservice
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	7	89	2	yamqwe	sandy-311-callse
6678	6678	kqi_ex012_1		[]		0	16	0	anonamename	kqi-ex012-1
6679	6679	MaskNet Dataset	1,058 images of littered face masks in different environments	['arts and entertainment', 'cities and urban areas', 'environment', 'computer vision', 'image data', 'covid19']	"Table of Contents
The MaskNet Dataset
Paper
Citation
Alternative Download Links
The MaskNet Dataset
MaskNet dataset consist of 1,058 images of littered face masks in different environments, such as streets, buildings, and parks
Can be utilized for face mask detection in a variety of scenarios
For more information, please refer to the paper
The photos come in an original size of 3024 x 4032 pixels, but resized to 540 x 720 pixels
Originally posted on GitHub
File Format: jpg
Size: ~200MB
Paper
Location-aware hazardous litter management for smart emergency governance in urban eco-cyber-physical systems
You can find the paper (Open Access) HERE.**  
DOI: 10.1007/s11042-021-11654-w
Citation
If you find the dataset useful in your research, please consider citing:
@article{Peyvandi2022,
  author={Peyvandi, Amirhossein
  and Majidi, Babak
  and Peyvandi, Soodeh
  and Patra, Jagdish C.
  and Moshiri, Behzad},
  title={Location-aware hazardous litter management for smart emergency governance in urban eco-cyber-physical systems},
  journal={Multimedia Tools and Applications},
  year={2022},
  month={Jan},
  day={03},
  issn={1573-7721},
  doi={10.1007/s11042-021-11654-w},
  url={https://doi.org/10.1007/s11042-021-11654-w}
}"	12	400	5	tenebris97	masknet
6680	6680	feedbackbaseline101		[]		0	28	0	gyanendradas	feedbackbaseline101
6681	6681	reviews		['ratings and reviews']		0	10	0	parivarshitha	reviews
6682	6682	Statlog (Heart) Data Set	This is clean and ready to use Statlog (Heart) dataset	['healthcare', 'health', 'beginner', 'exploratory data analysis', 'classification', 'tabular data']	"About Dataset:
This dataset is a heart disease database similar to a database already present in the repository (Heart Disease databases) but in a slightly different form.
Cite at:
Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."	155	1320	8	shubamsumbria	statlog-heart-data-set
6683	6683	kqi_ex011_1		[]		0	6	0	anonamename	kqi-ex011-1
6684	6684	Egohands-Segmentation		[]		1	20	0	chaturvediabhay24	egohandssegmentation
6685	6685	r9d2crop		[]		0	32	0	ironluu	r9d2crop
6686	6686	Wild-animal-dataset		['animals', 'computer vision', 'image data', 'cv2', 'tensorflow']	Wild Animal Dataset	4	56	4	chandrug	wildanimaldataset
6687	6687	bank_fraud		[]		1	21	0	akashsanjaywagh	bank-fraud
6688	6688	Walgreens pharmacy products dataset	Walgreens pharmacy products list	[]	"Context
Walgreens products list dataset
Content
Walgreens is second largest pharmacy store chain in the united states. Crawl feeds team extracted records for research and analysis purposes.
Fields:
url, title, brand, weight, price, currency, availability, description, ingredients, warnings, breadcrumbs, images, uniq_id, scraped_at
Acknowledgements
Get complete dataset from the crawl feeds.
Inspiration
Crawl feeds in house  team extracted records for research and analysis purposes."	11	65	0	crawlfeeds	walgreens-pharmacy-products-dataset
6689	6689	Top Rated TMDB Movies Dataset	Trending Top 10 TMDB Movies 2021-2022	['arts and entertainment']		0	21	0	muhammadmuzammil196	top-rated-tmdb-movies-dataset
6690	6690	wine_data		[]		0	5	0	sreejithneo	wine-data
6691	6691	bert_baseline_weight_2		[]		0	11	0	alisherianvar	bert-baseline-weight-2
6692	6692	HousePrices10.csv		[]		0	13	0	abhishekkumarscse242	houseprices10csv
6693	6693	total_mem		[]		0	1	0	udi123	total-mem
6694	6694	mlub2021-session10		[]		0	7	0	clararivadulladur	mlub2021session10
6695	6695	for_Ehud		[]		0	0	0	yonatanvarssano	for-ehud
6696	6696	lfastexp2f7		[]		0	23	2	nischaydnk	lfastexp2f7
6697	6697	Cyclisic Bikeshare Data	Consisting data for period Oct20-Nov21	['health']		1	16	0	trishlabhardwaj	bikeshareoct20nov21
6698	6698	original seg spade synthesis		[]		1	6	0	shashil	original-seg-spade-synthesis
6699	6699	Jamumas	Postion IOT+ML receipt AI determines Mineral Location	['asia']	"Context
Starting from within the earth and being held by humans, it is processed in the hamlet facilities and becomes appropriate technology. With this summary, we hope that the public can be helped to determine and work to apply their knowledge to continue to take part in beautifying our earth.
About The Project Jamumas
As a source of public data in relation to Determination of Public Data for Visualization and Mapping with Data in determining the content of the earth's crust affecting plant growth and the environment
Contents
From time to time data from one data to another is developed by thinking about improving Internet machine learning (IOT) to be processed into output that is no longer predictions but Artificial Intelligence (AI) Perfect the contest
Thank-you note
We wouldn't be here without the help of others. If you owe an attribution or thanks, include it here along with a quote from previous research.
Inspiration
Our data is widely exposed, the sequence of our research spreads so fast because what we need is this supporting tool and technology is able to find gold and diamonds."	1	197	0	mayakarya	awaland
6700	6700	google_electra_large_discriminator		[]		1	3	0	qinyukun	google-electra-large-discriminator
6701	6701	Covid-19 India Statewise Data	Covid-19 India Statewise Data as on January 02, 2022	[]		1	26	0	deepakwadge	covid19-india-statewise-data
6702	6702	model_4	swin_transformer_384_fold_4	[]		0	9	0	stuliuc	model-4
6703	6703	Gender_dataset		[]		0	21	0	karthikvenkatasamy	gender-dataset
6704	6704	albert_xxlarge_v2		[]		1	4	0	qinyukun	albert-xxlarge-v2
6705	6705	kaggle2022_1		[]		0	7	0	sonbomme	kaggle2022-1
6706	6706	HousePrices5.csv		[]		1	22	0	abhishekkumarscse242	houseprices5csv
6707	6707	HousePrices2.csv		[]		0	3	0	abhishekkumarscse242	houseprices2csv
6708	6708	HousePrices2		[]		0	6	0	abhishekkumarscse242	houseprices2
6709	6709	members		[]		1	2	0	udi123	members
6710	6710	deberta_v3_large		[]		1	8	0	qinyukun	deberta-v3-large
6711	6711	GroNLP_hateBERT		[]		1	6	0	qinyukun	gronlp-hatebert
6712	6712	NLPP_text2code		[]		0	45	0	aminabz	nlpp-text2code
6713	6713	PetFinder-Z153-20220108074748		[]		0	9	0	hideyukizushi	petfinder-z153-20220108074748
6714	6714	DRL_for_JSP		[]		0	12	0	aihongsun	drl-for-jsp
6715	6715	unetdrivendata		[]		0	5	0	gyanendradas	unetdrivendata
6716	6716	compiled		[]		0	15	0	mohammadabdulbasit	compiled
6717	6717	Brain_Tumor_Classification		[]		0	10	0	saskue	brain-tumor-classification
6718	6718	lfastexp1f7		[]		1	26	1	nischaydnk	lfastexp1f7
6719	6719	Beijing pm2.5 2010.1.1-2014.12.31		['social issues and advocacy']		1	12	0	greencow	beijing-pm25-20101120141231
6720	6720	keras_cv_attention_models		[]		0	11	0	harshbansal27	keras-cv-attention-models
6721	6721	shandong_6_fold_3		[]		0	11	0	iwillfindthatgirl	shandong-6-fold-3
6722	6722	model_roberta_base_two_stage		[]		0	4	0	qinyukun	model-roberta-base-two-stage
6723	6723	r9swinRR		[]		3	6	0	nicholasleee	r9swinrr
6724	6724	INDONESIAN FOOD		['cooking and recipes']		1	43	0	theresalusiana	indonesian-food
6725	6725	r9swinRR		[]		5	22	0	kevinccee	r9swinrr
6726	6726	r9swinRR		[]		4	14	0	kevincesar	r9swinrr
6727	6727	Gold And Nifty50 (2000-2020)	Gold price in rupees per gram and NIFTY50 Price (Monthly data since 2000 - 2021)	['finance', 'economics', 'beginner', 'tabular data', 'text data']	"Context
I don't know about you but I have always been concerned about the relationship between different assets. And gold and stocks never left my mind no matter what!!. As I started searching for data It was really hard for me to find it. So after a lots of search I processed the data and made it easy to be analyzed.
Content
I have kept things simple. Just monthly data. At the end of the month, mostly(this part is not reliable but it works). For gold the unit of data is Price in rupees per gram (since we in India are mostly familiar with this units). And NIFTY50 data is also have monthly data mostly at the end of the month. When you try to find the price of Stocks they provide high, low, open and closing prices. But here I have took only one column of price for the sake of simplicity. 
Acknowledgements
Thanks to my friends for the help."	24	141	3	dewashyadubey	gold-and-nifty50-20002020
6728	6728	diabates		[]		3	17	0	minhajulislamakib	diabates
6729	6729	experiment101		[]		0	3	0	gyanendradas	experiment101
6730	6730	TF COTS MRCNN Model		[]		0	12	0	swadeshjana	tf-cots-mrcnn-model
6731	6731	review		['ratings and reviews']		0	5	0	krishnagiri	review
6732	6732	track_id_for_tensorflow_great_barrier_reef_dataset		[]		18	243	8	hengck23	track-id-for-tensorflow-great-barrier-reef-dataset
6733	6733	F.R.I.E.N.D.S Netflix script data	Script data of famous netfilx TV show FRIENDS with dialouges and speaker data	['movies and tv shows', 'united states', 'text mining', 'tabular data', 'text data']	"Friends TV show
Friends is a netflix original american Sitcom TV show famous all over the world.
This dataset
This is a dataset of the scripts of the F.R.I.E.N.D.S with dialogues and their respective speakers along with the episode name and the season at which the dialogue was spoken.
This data is originally scrapped form websites and subtitle data of the TV show for a neural search project involving searching of the speaker for a given dialogue."	78	616	5	gopinath15	friends-netflix-script-data
6734	6734	RaC Dataset		[]		1	8	0	aiznea	rac-dataset
6735	6735	subaru ds train 0106 quarter		['automobiles and vehicles']		0	10	0	wuliaokaola	subaru-ds-train-0106-quarter
6736	6736	gbr_models		[]		2	162	0	mybirth0407	gbrmodels
6737	6737	House Price Data		['real estate']		3	37	0	waleednegm	house-price-data
6738	6738	Basketball data		['basketball']		0	19	0	jaydenb0	basketball-data
6739	6739	pawpularity_train_manipulated01		[]		4	32	0	toshihikok	pawpularity-train-manipulated01
6740	6740	0108_weights		[]		0	32	0	godliang2	0108-weights
6741	6741	Accurate helicopter shapes/segmentation	Natural variance and original shapes	['aviation', 'image data']	"The data set was produced by MetaVision team using original content.
Several segmentation techniques were used to generate accurate masks from the original video streams, preserving natural variance and original shapes.
This set is just a ""building block"". Feel free to augment it or combine it with other sets."	1	32	0	metavision	accurate-helicopter-shapessegmentation
6742	6742	SQuAD v1 data preprocessed with Spacy		[]	"Context
The script created this dataset:
https://www.kaggle.com/tusonggao/squad-v1-data-preprocess-with-spacy"	3	5	0	tusonggao	squad-v1-data-preprocessed-with-spacy
6743	6743	lgbm_datas		[]		0	10	0	soraozawa	lgbm-datas
6744	6744	Btc_binance_chart		[]		1	14	0	metformin	btc-binance-chart
6745	6745	baoat_durerized_2		[]		5	23	1	scalpah	baoat-durerized-2
6746	6746	Congressional Resignations	600 collected samples, technical information	['beginner', 'news']	"About this dataset
&gt; <h1>Congressional Resignations</h1>
<p>Data behind the story <a href=""https://fivethirtyeight.com/features/more-people-are-resigning-from-congress-than-at-any-time-in-recent-history/"" target=""_blank"" rel=""nofollow"">We’ve Never Seen Congressional Resignations Like This Before</a>.</p>
<p><code>congressional_resignations.csv</code> contains information about the 615 members of Congress who resigned or were removed from office from March 4, 1901 (the first day of the 57th Congress) through January 15, 2018, including the resigning member’s party and district, the date they resigned, the reason for their resignation and the source of the information about their resignation.</p>
<p>The reasons are categorized as follows:</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Code</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>X</code></td>
<td>Unwanted sexual contact</td>
</tr>
<tr>
<td><code>A</code></td>
<td>Consensual sex scandals</td>
</tr>
<tr>
<td><code>B</code></td>
<td>Other scandals</td>
</tr>
<tr>
<td><code>C</code></td>
<td>Other office</td>
</tr>
<tr>
<td><code>D</code></td>
<td>Private sector</td>
</tr>
<tr>
<td><code>E</code></td>
<td>Health/family</td>
</tr>
<tr>
<td><code>F</code></td>
<td>Other</td>
</tr>
<tr>
<td><code>G</code></td>
<td>Left early</td>
</tr>
<tr>
<td><code>H</code></td>
<td>Military service</td>
</tr>
<tr>
<td><code>I</code></td>
<td>Election overturned</td>
</tr>
</tbody>
</table></div>
This dataset was created by FiveThirtyEight and contains around 600 samples along with Congress, Member, technical information and other features such as:
- Party
- Resignation Date
- and more.
How to use this dataset
&gt; - Analyze Source in relation to Category
- Study the influence of Reason on District
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	23	353	7	yamqwe	congressional-resignationse
6747	6747	March Madness 2018	600 collected samples, technical information	['basketball', 'beginner']	"About this dataset
&gt; <p>This file contains links to the data behind our <a href=""https://projects.fivethirtyeight.com/2018-march-madness-predictions/"" target=""_blank"" rel=""nofollow"">2018 March Madness Predictions</a>.</p>
<p>fivethirtyeight_ncaa_forecasts.csv contains power ratings for each team and the chance of each team reaching every round of the tournament. It includes men's and women's forecasts, with one forecast for each day of the tournament.</p>
<p>Source: <a href=""https://github.com/fivethirtyeight/data/tree/master/march-madness-predictions-2018"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/march-madness-predictions-2018</a></p>
This dataset was created by FiveThirtyEight and contains around 600 samples along with Rd1 Win, Rd7 Win, technical information and other features such as:
- Team Id
- Playin Flag
- and more.
How to use this dataset
&gt; - Analyze Team Region in relation to Team Name
- Study the influence of Gender on Rd5 Win
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	20	210	5	yamqwe	march-madness-2018e
6748	6748	Predicting Candy Win Rate In Halloween 	100 collected samples, technical information	['beginner', 'food', 'holidays and cultural events']	"About this dataset
&gt; <p>Data behind <a href=""http://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking/"" target=""_blank"">The Ultimate Halloween Candy Power Ranking</a></p>
<p><code>candy-data.csv</code> includes attributes for each candy along with its ranking. For binary variables, 1 means yes, 0 means no. The data contains the following fields:</p>
<div><table><thead>
<tr>
<th>Header</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>chocolate</td>
<td>Does it contain chocolate?</td>
</tr>
<tr>
<td>fruity</td>
<td>Is it fruit flavored?</td>
</tr>
<tr>
<td>caramel</td>
<td>Is there caramel in the candy?</td>
</tr>
<tr>
<td>peanutalmondy</td>
<td>Does it contain peanuts, peanut butter or almonds?</td>
</tr>
<tr>
<td>nougat</td>
<td>Does it contain nougat?</td>
</tr>
<tr>
<td>crispedricewafer</td>
<td>Does it contain crisped rice, wafers, or a cookie component?</td>
</tr>
<tr>
<td>hard</td>
<td>Is it a hard candy?</td>
</tr>
<tr>
<td>bar</td>
<td>Is it a candy bar?</td>
</tr>
<tr>
<td>pluribus</td>
<td>Is it one of many candies in a bag or box?</td>
</tr>
<tr>
<td>sugarpercent</td>
<td>The percentile of sugar it falls under within the data set.</td>
</tr>
<tr>
<td>pricepercent</td>
<td>The unit price percentile compared to the rest of the set.</td>
</tr>
<tr>
<td>winpercent</td>
<td>The overall win percentage according to 269,000 matchups.</td>
</tr>
</tbody>
</table></div>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 100 samples along with Sugarpercent, Nougat, technical information and other features such as:
- Crispedricewafer
- Hard
- and more.
How to use this dataset
&gt; - Analyze Chocolate in relation to Pluribus
- Study the influence of Pricepercent on Bar
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	57	600	7	yamqwe	candy-power-rankinge
6749	6749	JHU Global Covid-19 Cases	Contains a composite of global case data collected by Johns Hopkins University	['education', 'covid19']	"Content
This is a composition of the files found in 
 https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports
folder maintained by Johns Hopkins University.  It starts on 22 Jan 2020 and is current up to 30 October 2020.   New fields were added as necessary to accommodate the additions to the file structure on successive dates, but these new fields remain blank for earlier dates.  Aside from this, this data remains unaltered from its original format.  Any user should be aware there may be more than one report for any given region on any given date as this is raw data, which can interfere with aggregation functions, and that regions appear to have been labelled differently as time has progressed, which will require identification and cleaning.
Acknowledgements
Thank-you to Johns Hopkins University Center for Systems Science and Engineering for compiling and storing this data that has been a great help in tracking this large event of historical significance."	25	927	0	squinkiii	pull-datacsv
6750	6750	The Media Really Has Neglected Puerto Rico in 2017	Median mentions of puerto rico, technical information	['natural disasters', 'politics', 'beginner', 'news']	"About this dataset
&gt; <p>Data behind the stories:</p>
<ul>
<li><a href=""https://fivethirtyeight.com/features/the-media-really-has-neglected-puerto-rico/"" target=""_blank"" rel=""nofollow"">The Media Really Has Neglected Puerto Rico</a></li>
<li><a href=""https://fivethirtyeight.com/features/the-media-really-started-paying-attention-to-puerto-rico-when-trump-did/"" target=""_blank"" rel=""nofollow"">The Media Really Started Paying Attention To Puerto Rico When Trump Did</a></li>
</ul>
<p><strong>Online News Data</strong></p>
<p>Data about Online News was collected using the <a href=""https://mediacloud.org/"" target=""_blank"" rel=""nofollow"">Media Cloud</a> dashboard, an open source suite of tools for analyzing a database of online news.</p>
<ul>
<li><code>mediacloud_hurricanes.csv</code> contains the number of sentences per day that mention Hurricanes Harvey, Irma, Jose, and Maria in online news.</li>
<li><code>mediacloud_states.csv</code> <strong>(Updated through 10/10/2017)</strong> contains the number of sentences per day that mention Puerto Rico, Texas, and Florida in online news.</li>
<li><code>mediacloud_trump.csv</code> <strong>(Updated through 10/10/2017)</strong> contains the number of headlines that mention Puerto Rico, Texas, and Florida, as well as headlines that mention those three locations along with 'President' or 'Trump'.</li>
<li><code>mediacloud_top_online_news.csv</code> contains a list of sources included in Media Cloud's ""U.S. Top Online News"" collection.</li>
</ul>
<p><strong>TV News Data</strong></p>
<p>TV News Data was collected from the <a href=""https://archive.org/details/tv"" target=""_blank"" rel=""nofollow"">Internet TV News Archive</a> using the <a href=""https://television.gdeltproject.org/cgi-bin/iatv_ftxtsearch/iatv_ftxtsearch"" target=""_blank"" rel=""nofollow"">Television Explorer</a> tool.</p>
<ul>
<li><code>tv_hurricanes.csv</code> - contains the percent of sentences per day in TV News that mention Hurricanes Harvey, Irma, Jose, and Maria.</li>
<li><code>tv_hurricanes_by_network.csv</code> - contains the percent of sentences per day in TV News per network that mention Hurricanes Harvey, Irma, Jose, and Maria.</li>
<li><code>tv_states.csv</code> <strong>(Updated through 10/10/2017)</strong> - contains the percent of sentences per day in TV News that mention Puerto Rico, Texas, and Florida.</li>
</ul>
<p><strong>Google Search Queries</strong></p>
<p>Google search data was collected using the <a href=""https://trends.google.com/trends/"" target=""_blank"" rel=""nofollow"">Google Trends</a> dashboard.</p>
<ul>
<li><code>google_trends.csv</code> - Contains data on google trend searches for Hurricanes Harvey, Irma, Jose, and Maria.</li>
</ul>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 100 samples along with Title:"" Puerto Rico"", Title: Florida, technical information and other features such as:
- Title: Texas And (title: Trump Or Title: President)
- Title: Texas
- and more.
How to use this dataset
&gt; - Analyze Title:"" Puerto Rico"" And (title: Trump Or Title: President) in relation to Title: Florida And (title: Trump Or Title: President)
- Study the influence of Title:"" Puerto Rico"" on Title: Florida
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	6	101	1	yamqwe	puerto-rico-mediae
6751	6751	5000 transcripts of YouTube AI related videos 	english transcriptions	['artificial intelligence', 'nlp', 'text mining', 'tabular data', 'text data']	"Context
Collected transcripts from AI related videos of YouTube for nlp purposes
Content
Raw english transcriptions 
Acknowledgements
YouTube for sharing transcriptions
Inspiration
topic finding
clustering
other nlp techniques"	13	152	6	jfcaro	5000-transcripts-of-youtube-ai-related-videos
6752	6752	eca-nfnet-l2-1755		[]		0	23	0	honihitak	eca-nfnet-l2-175
6753	6753	pytorch_longformer_large		[]		7	21	0	shujun717	pytorch-longformer-large
6754	6754	2021 U.S. Hass Avocado Market Demand & Elasticity 		['business', 'food']	"A dataset displaying Hass avocado unit sales, average sales price, revenues, and price elasticity according to region and season in the year 2021.
I created this data set through the mining and cleaning of data provided by the Hass Avocado Board at https://hassavocadoboard.com/category-data/.
I appended the Season and Price Elasticity columns using R functions."	10	72	0	zclacken	2021-us-hass-avocado-market-demand-elasticity
6755	6755	songsdata	Dataset contains audio features of rock and hip hop songs	['music', 'intermediate', 'data analytics', 'feature engineering', 'audio data']	Dataset contains audio features of rock and hip hop songs	3	28	0	nushkaa	songsdata
6756	6756	pytorch_longformer_base		[]		0	4	0	shujun717	pytorch-longformer-base
6757	6757	yolov5x		[]		0	6	0	getcoin	yolov5x
6758	6758	yolov5s-1280-train		[]		1	24	0	getcoin	yolov5s1280train
6759	6759	data_am		[]		0	11	0	sai1881	data-am
6760	6760	C02 Raw Data by Country(1960 - 2011)		['atmospheric science', 'beginner', 'exploratory data analysis', 'tabular data']		1	23	0	wahaba	c02-raw-data-by-country1960-2011
6761	6761	Sdcnet Seg output		[]		1	6	0	shashil	sdcnet-seg-output
6762	6762	Movie Revenue Data		['movies and tv shows']		13	68	1	wahaba	movie-revenue-data
6763	6763	world-gdp-by-country-2000-2020	world-gdp-by-country-2000-2020	['business', 'economics']		5	18	0	vladlee	worldgdpbycountry20002020
6764	6764	The Great Gatsby script.csv		['literature']		3	28	0	angelo2386	the-great-gatsby-scriptcsv
6765	6765	ggggjhbb		[]		0	4	0	uniquemart	ggggjhbb
6766	6766	covid19arg		[]		1	19	0	fernandobordi	covid19arg
6767	6767	PT Weights ViT SITARAM	Pre Trained Vision Transformer Weights (loaded from timm library) SITARAM	['health']		0	8	0	adityasrivastava7	pt-weights-vit-sitaram
6768	6768	wjw_i80		[]		0	9	0	anyisheng	wjw-i80
6769	6769	wjw_us101		[]		0	9	0	anyisheng	wjw-us101
6770	6770	Germen Credit Loan default		[]		7	46	0	rahulsinghchowhan	germen-credit-loan-default
6771	6771	Wireless Sensor Network Data		[]		15	174	15	halimedogan	wireless-sensor-network-data
6772	6772	featuresales01		[]		1	27	1	sytuannguyen	featuresales01
6773	6773	200k images of birds (iNaturalist)	200K images of birds over 1486 classes.	['biology', 'artificial intelligence', 'advanced', 'classification', 'image data']	"iNaturalist 2021 Birds Data
This is a subset of the iNaturalist 2021 birds data. There is 1486 species, and 100k images. Each species has around 140 images. 
I created this dataset because the previous version of this dataset was not large enough. That dataset had 50 images per class. This dataset adds an extra 100 images to each class."	9	147	4	sharansmenon	inatbirds100k
6774	6774	GDAC_Cyclistic_Bikeshare_dec2020_nov2021	Capstone for google data analytics certification	['education']	"Context
This dataset was provided and when taking the google data analytic course under an open data license. This data can be used in your own analysis or by following the guided analysis provided in the course work.
Content
The contents of this dataset is limited to the ride_id, rideable_type, date_start, date_end, and the station they got on and off. No identifiable information can be found in this dataset.
Acknowledgements
This dataset would not be possible without googles help.
Inspiration
The questions answered in this dataset are part of the certification coursework."	1	14	0	harnestroysantiago	gdac-cyclistic-bikeshare-dec2020-nov2021
6775	6775	train.sample		['business']		0	17	0	yujingjjy	trainsample
6776	6776	tpufc01		[]		2	15	1	sytuannguyen	tpufc01
6777	6777	Pubmed Knowledge Graph Dataset	PubMed raw data are not included into CSV files	['diseases', 'categorical data', 'health', 'biotechnology', 'medicine', 'drugs and medications']	"Context
PubMed Knowledge Graph Datasets
http://er.tacc.utexas.edu/datasets/ped
Content
Dataset Name : PKG2020S4 (1781-Dec. 2020), Version 4
The new version PKG, PKG2020S4 (1781-Dec. 2020), updated the previous PKG version with PubMed 2021 baseline files, PubMed daily updates files (up to Jan. 4th 2021), and extracted bio-entities, author disambiguation results, extended author information, Scimago that containing journal information, and WOS citations which contains reference relations between PMID and reference PMID and extracted from WOS.
Database Features: 1-PKG2020S4 (1781-Dec. 2020) Features.pdf (https://web.corral.tacc.utexas.edu/dive_datasets/PKG2020S4/PKG2020S4_MySQL/1-PKG2020S4%20(1781-Dec.%202020)%20Features.pdf)
Database Description: 2-PKG2020S4 (1781-Dec. 2020) Database Description.pdf (https://web.corral.tacc.utexas.edu/dive_datasets/PKG2020S4/PKG2020S4_MySQL/2-PKG2020S4%20(1781-Dec.%202020)%20Database%20Description.pdf)
Acknowledgements
http://er.tacc.utexas.edu/datasets/ped
Inspiration
http://er.tacc.utexas.edu/datasets/ped"	2	43	0	krishnakumarkk	pubmed-knowledge-graph-dataset
6778	6778	Mall_Customers_Data		[]		1	38	1	ahmedmokbel	mall-customers-data
6779	6779	model_256_by_256		[]		0	16	1	narminj	model-256-by-256
6780	6780	Hanshin Univ. ABC Camp (AI) - South Korea		['universities and colleges']		0	16	0	teddylee777	abc-camp-ai
6781	6781	projects movies		['arts and entertainment']		0	3	0	udi123	projects-movies
6782	6782	Physician Data		[]		2	178	0	prathamesh0902	physician-data
6783	6783	German news headlines (politics and economics)	Teasers and headlines scraped from several news pages and magazines	['literature', 'politics']	"Context
The project was started as a web-scraping exercise to get more experience particularly with the scrapy framework. Since I daily check news from several sources I decided to get a webscraper do the work for me and look for the interesting headlines from politics and economics.
The news sources have been anonymised and the licence limited to non-commercial use since this is the prerequisite to scrape the data from those homepages.
Content
In the csv file you find around 8400 records of news headlines from 7 different sources. For each record a teaser (or sub-headline) and a headline is provided.
Acknowledgements
My thanks go to Upendra who has a great Youtube channel on webscraping (https://www.youtube.com/user/eupendras).
Inspiration
All data enthusiasts are highly welcome to use the data and make something out of it. I will try and practise topic modelling as well as translation tasks with transformer models. Any inspiration for this or comments on my notebooks (which I will publish shortly) are highly appreciated!"	5	101	1	matthiasse	german-news-headlines-politics-and-economics
6784	6784	world-population-growth-by-country-2000-2020	world-population-growth-by-country-2000-2020	['social science']		2	43	0	vladlee	worldpopulationgrowthbycountry20002020
6785	6785	world-gdp-growth-by-country-2000-2020		['business']		3	44	0	vladlee	worldgdpgrowthbycountry20002020
6786	6786	Restaurant		['restaurants']		0	10	0	parivarshitha	restaurant
6787	6787	BackupDataset		[]		1	10	0	seanosborne	backupdataset
6788	6788	Latest Dataset for Omicron variant of COVID-19	Omicron variant details till Jan 5th 2022	['biology', 'data visualization', 'data analytics']	"Context
The Omicron variant, variant B.1.1.529, was first reported to WHO on 24 November 2021 and was classified as a variant of concern by WHO on 26 November 2021. The classification was made on the advice of the Technical Advisory Group on Virus Evolution, based primarily on information from South Africa that the variant has a large number of mutations and has caused a detrimental change in COVID-19 epidemiology.
The data set is downloaded from https://ourworldindata.org/
Content
The dataset contains:
Entity: Country Details 
Code : Country: Country Code
Day: Day of Report 
Omicron_Percentage: Percentage of Omnicron Impact on the day mentioned in Day column
Acknowledgements
To all the authors and laboratories responsible for producing this data and sharing it via the GISAID initiative.
Khare, S., et al (2021) GISAID’s Role in Pandemic Response. China CDC Weekly, 3(49): 1049-1051. doi: 10.46234/ccdcw2021.255 PMCID: 8668406
Elbe, S. and Buckland-Merrett, G. (2017) Data, disease, and diplomacy: GISAID’s innovative contribution to global health. Global Challenges, 1:33-46. doi:10.1002/gch2.1018 PMCID: 31565258
Shu, Y. and McCauley, J. (2017) GISAID: from vision to reality. EuroSurveillance, 22(13) doi:10.2807/1560-7917.ES.2017.22.13.30494 PMCID: PMC5388101
Also  CoVariants.org and https://ourworldindata.org/
Inspiration
Visualize the Omicron variant, variant B.1.1.529 spread."	456	3823	46	sandhyakrishnan02	omicron-variant-of-covid19
6789	6789	WSN-DS	WSN-DS: A dataset for intrusion detection systems in wireless sensor networks	['research', 'science and technology', 'computer science', 'mobile and wireless', 'classification']	"For more details:  https://www.hindawi.com/journals/js/2016/4731953/
Denial of Service (DoS) attacks in wireless sensor network 
Blackhole, Grayhole, Flooding, and Scheduling attacks.
Internet of things 
security"	36	408	4	bassamkasasbeh1	wsnds
6790	6790	Noksha Lite Raw	Noksha Lite Raw People Real vs Sketch	['food']		0	34	0	abuhasnayenzillanee	noksha-lite
6791	6791	Accurate plane shapes/segmentation	Natural variance and original shapes	['image data']	"The data set was produced by MetaVision team using original content.
Several segmentation techniques were used to generate accurate masks from the original video streams, preserving natural variance and original shapes.
This set is just a ""building block"". Feel free to augment it or combine it with other sets."	3	63	0	metavision	accurate-plane-shapessegmentation
6792	6792	ADP_KR_p2	Advanced Data Analytics Professional (KOREA)	[]	"ADP 시험대비
문제 링크
(python) : https://www.kaggle.com/kukuroo3/problem2-python     
(R)          : https://www.kaggle.com/kukuroo3/problem2-r   
English translation version will be prepared soon"	23	226	4	kukuroo3	adp-kr-p2
6793	6793	ovdatatest		[]		1	3	0	mrparamatma	ovdatatest
6794	6794	nbdatatest		[]		0	19	0	mrparamatma	nbdatatest
6795	6795	bdatatest		[]		0	17	0	mrparamatma	bdatatest
6796	6796	MI-EEG Left-Hand vs Right-Hand		['health']		1	53	0	abenna	mieeg-lefthand-vs-righthand
6797	6797	covid-arg		[]		1	9	0	fernandobordi	covidarg
6798	6798	CSVDS-ExpW		[]		1	21	0	mohammedaaltaha	csvdsexpw
6799	6799	arg-covid-centro		[]		0	8	0	fernandobordi	argcovidcentro
6800	6800	randomforest		[]		0	13	0	nadireus	randomforest
6801	6801	SvyazPhone		[]		0	0	0	alexzlat	svyazphone
6802	6802	Clean Petfinder& fastai KF 10 Mixup model data		[]		0	13	0	kohiya	clean-petfinder-fastai-kf-10-mixup-model-data
6803	6803	Weighti		[]		1	33	0	xuhuizhan	weighti
6804	6804	COTS-Filtered	3 fold dataset. train: video 0-1, valid/test: video 2	[]		1	18	0	caoyaoyuan	cotsfiltered
6805	6805	covid-centro-arg		[]		0	7	0	fernandobordi	covidcentroarg
6806	6806	Teacher class information		['education']		1	11	0	mdnomanshakh	teacher-class-information
6807	6807	stock_reliance		[]		0	12	0	vikrantkumar007	stock-reliance
6808	6808	horse2zebra_blur		[]		0	30	0	e94076039	horse2zebra-blur
6809	6809	Scraped job descriptions	Job descriptions scraped for NLP	['employment', 'nlp', 'text data', 'jobs and career']	"This dataset has a total of 4413 job descriptions, scraped from one single website. The job descriptions might be no longer available, so this dataset is mainly useful for practising NLP and research proposes.
To see more about the scraping process, feel free to check this blog post:
https://peakd.com/hive-163521/@macrodrigues/scraping-job-descriptions-for-nlp-project"	32	315	1	marcocavaco	scraped-job-descriptions
6810	6810	Bike Underground Detection	Based on Smartphone Accelerometer Data	['sports', 'cycling', 'classification', 'tabular data']	"Context
In the seminar ""Analysis of Sensor Data"" at the Leuphana University of Lüneburg, we conducted a project on ""Underground Detection during Cycling"" at the beginning of the Corona Pandemic. At that time, I and my teammates had a lot of time to ride our bikes and collect data for our project work. The acceleration data was recorded with the smartphone through the Phyphox app and saved as CSV files. More information about the data can be found in the following section.
Content
The dataset consists of training and test data, which can be found in the corresponding subfolders. All CSV files contain an index, timestamps and the acceleration of the X, Y and Z axes. Information about each file can be found in train.csv as well as test.csv: The surface of the recording, the recording frequency of the smartphone as well as with which bike was ridden. The latest information is relevant because all bicycles had different suspension and thus the amplitudes in the acceleration are different. As far as possible, a consistent recording setup was used to ensure that the data is comparable.
Further information
The mentioned project work can be found on the LG4ML platform (Data Science Platform in Lüneburg), including information about the approach and processing of the data. The project can be found at the following link: https://lg4ml.org/grounddetection/"	3	63	1	nilshmeier	bike-underground-detection
6811	6811	Game dice	https://app.roboflow.com/dicedataset/dice-txtg5/	['games']		0	19	0	epanemu	dice-3
6812	6812	results_tr		[]		0	8	0	muhsinertrk	results-tr
6813	6813	All Disney Movies Scraped from Wiki and OMDB API	This is a part of my Data Visualization project	['comics and animation']		6	48	1	thesocialcow	all-disney-movies-scraped-from-wiki-and-omdb-api
6814	6814	usebzeroshotresults		[]		12	59	0	muennighoff	usebzeroshotresults
6815	6815	PepsiCo Lab Potato Chips Quality Control	Image dataset for the binary classification of potato chips for quality control	['intermediate', 'binary classification', 'transfer learning', 'tensorflow']	"Starter code at
Kindly start this repository to encourage me to contribute more to the open source.
https://github.com/concaption/PepsiCo-Lab-Potato-Quality-Control
Context
A lot of research is being done to improve the quality of the food. Frito-Lay is the subsidiary of Pepsico that makes globally famous potato chips. The chips get burnt due to during the process. There is a minimum requirement for the amount of the burned or damaged part that can be on a chip.
As a PepsiCo intern, you are advised to build a deep learning-based approach towards solving the quality issues of potato chips. You are advised to find the area of the chips that is damaged. 
Also research about other chips quality control methods that are being researched in academia and industry.
* https://doi.org/10.1016/B978-0-12-802232-0.00022-0
* 
Content
The directory contains a balanced dataset of binary classification; the two labels being defective and non-defective. The data set contains almost a thousand image.
Acknowledgements
http://UsamaNavid.com/
http://BilalMujahid.me/
Inspiration
The inspiration of the project is to solve the problems in the industry using cutting-edge technology and bring academia and industry together. Industry 4.0 is here."	55	1109	13	concaption	pepsico-lab-potato-quality-control
6816	6816	Model for web		['clothing and accessories']		1	17	0	ninadekbote	model-for-web
6817	6817	gkf530f		[]		2	3	0	phoenix9032	gkf530f
6818	6818	pet-finder-duplicates		['animals']		2	55	0	awaptk	petfinderduplicates
6819	6819	Feelings prediction dataset 88,000		[]		6	186	1	talhaaljunaid170221	feelings-prediction-dataset-88000
6820	6820	Feelings prediction dataset 88,000		[]		6	186	1	talhaaljunaid170221	feelings-prediction-dataset-88000
6821	6821	Not the price of books	Datasets that used in the workshop of Not ONLY the price of books	['literature']		1	27	2	dsxavier	not-the-price-of-books
6822	6822	python-box		['computer science']		0	10	0	kangchand	pythonbox
6823	6823	catmeta		[]		0	15	0	ernnnn4u	catmeta
6824	6824	Dataset for Classification of Citrus Diseases 		['biology']		90	752	3	jonathansilva2020	dataset-for-classification-of-citrus-diseases
6825	6825	Song Popularity Dataset	Song Popularity Prediction - Regression Problem	['arts and entertainment', 'music', 'beginner', 'linear regression', 'tabular data', 'regression']	"Description:
Humans have greatly associated themselves with Songs & Music. It can improve mood, decrease pain and anxiety, and facilitate opportunities for emotional expression. Research suggests that music can benefit our physical and mental health in numerous ways. 
Lately, multiple studies have been carried out to understand songs & it's popularity based on certain factors. Such song samples are broken down & their parameters are recorded to tabulate. Predicting the Song Popularity is the main aim.
The project is simple yet challenging, to predict the song popularity based on energy, acoustics, instumentalness, liveness, dancibility, etc. The dataset is large & it's complexity arises due to the fact that it has strong multicollinearity. Can you overcome these obstacles & build a decent predictive model?
Acknowledgement:
The dataset is referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the song popularity.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	1775	11341	45	yasserh	song-popularity-dataset
6826	6826	ADP_KR_p1	Advanced Data Analytics Professional (KOREA)	[]	"ADP 시험대비
문제 링크
(python) : https://www.kaggle.com/kukuroo3/problem-python  
(R)          : https://www.kaggle.com/kukuroo3/problem-r
English translation version will be prepared soon"	73	744	5	kukuroo3	adp-kr-p1
6827	6827	Four Wheeler Number / License Plate Detection	Detect Number Plate for Four Wheeler	['law', 'automobiles and vehicles', 'computer vision']	"Context
Detect Number plates for two wheeler.
Content
Using Computer Vision to detect vehicle number plates..
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Data Collected from below sources with my Phone:
1) My apartment parking area
2) My company parking slot
3) Google"	6	62	0	sureshmecad	four-wheeler-number-license-plate-detection
6828	6828	w2v-model		['clothing and accessories']		0	19	0	a24998667	w2v-model
6829	6829	InpaintingDataset	Images for inpainting studies using deep image prior. This is a toy problem	['arts and entertainment', 'beginner', 'image data']		3	52	0	sebastiendelprat	inpaintingdataset
6830	6830	CapstoneDataset		[]		2	5	0	imdineshgrewal	capstonedataset
6831	6831	Great Barrier Reef COTS - Object Detection API	Transformed Great Barrier Reef dataset into a TFRecord for Object Detection API	['earth and nature', 'earth science']		3	24	0	r4cube	greatbarrierreef-training-setup
6832	6832	ridge_j_noclean_glove_multiseeds_oof		[]		0	11	0	shobhitupadhyaya	ridge-j-noclean-glove-multiseeds-oof
6833	6833	News Classification and Emotion Detection Dataset 		[]		0	29	0	razamukhtar007	news-classification-and-emotion-detection-dataset
6834	6834	trial1test-join		[]		0	26	0	mennagalal	trial1testjoin
6835	6835	test1ForNST		[]		0	90	0	ceciliawyc	test1fornst
6836	6836	Bank Model Prediction		['banking']		3	37	0	abdullahfazili	bank-model-prediction
6837	6837	Properati Digital House 2021		[]		43	6	0	joaquin96	properati-digital-house-2021
6838	6838	cots-tfrecords-5-folds-groupkfold		[]		0	29	0	lazcoder	cotstfrecords5foldsgroupkfold
6839	6839	PetFinder-Z152-20220107111844		[]		0	10	0	hideyukizushi	petfinder-z152-20220107111844
6840	6840	COCO ResNet152 Features Val		[]		0	16	0	mariofrcrce	coco-resnet152-features-val
6841	6841	vittool		[]		0	12	0	ironluu	vittool
6842	6842	Job Performance		[]	"Context
This dataset covers 1000 randomly selected Northern California workers in office jobs.
Content
Data were collected in 2020 through questionnaires directly to selected individuals and analysis of their history.
Inspiration
The purpose of the dataset is purely academic, useful as an exercise in the first approaches with structural equation models.
Would you be able to define the performance of workers, for example, through characteristics such as social / motivation skills or intellectual skills?"	6	42	0	michealronn	job-performance
6843	6843	Sample_pdf		[]		0	24	0	amogh0810	sample-pdf
6844	6844	jigsaw2021-wat059-data		[]		0	5	0	wataoka	jigsaw2021-wat059-data
6845	6845	players2016_21		[]		2	16	0	agarwalkshitij	players2016-21
6846	6846	jigsaw2021-wat060-data		[]		0	1	0	wataoka	jigsaw2021-wat060-data
6847	6847	Image of different graphics cards (GPU)	818 images of different video cards from the top.	['arts and entertainment', 'computer science', 'exploratory data analysis', 'computer vision', 'gan', 'image data', 'retail and shopping']	"Context
The variety of video cards with different visual solutions interested me in compiling this dataset.
Content
The folder contains 818 images of video cards received and manually cleaned from the site https://catalog.onliner.by"	35	371	5	pavelbiz	image-of-different-graphics-cards-gpu
6848	6848	titanic01		[]		0	10	1	sytuannguyen	titanic01
6849	6849	jigsaw2021-wat058-data		[]		0	1	0	wataoka	jigsaw2021-wat058-data
6850	6850	digitrecognizer01		[]		0	10	1	sytuannguyen	digitrecognizer01
6851	6851	Two Wheeler Number / License Plate Detection	Detect Number Plate for Two Wheeler	['law', 'computer vision', 'neural networks', 'cnn']	"Context
Detect Number plates for two wheeler.
Content
Using Computer Vision to detect vehicle number plates..
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Data Collected from below sources with my Phone:
1) My apartment parking area
2) My company parking slot
3) Google"	23	425	5	sureshmecad	number-plate-two-wheeler
6852	6852	swintool		[]		1	516	0	ironluu	swintool
6853	6853	jigsaw2021-wat056-data		[]		0	5	0	wataoka	jigsaw2021-wat056-data
6854	6854	datapca		[]		2	44	1	jakubdbrowski	datapca
6855	6855	ninapro2		[]		1	50	0	gianmarcoguarnier	ninapro2
6856	6856	Traffic and Drugs Related Violations Dataset	Come, Let’s Explore Traffic Violation Data	['law', 'intermediate', 'data cleaning', 'data visualization', 'tabular data']	"About Dataset:
This dataset contains around 65k+ traffic-related violation records.
Attribute Information:
1. stop_date - Date of violation
2. stop_time - Time of violation
3. driver_gender - Gender of violators (Male-M, Female-F)
4. driver_age - Age of violators
5. driver_race - Race of violators
6. violation - Category of violation :
- Speeding
- Moving Violation (Reckless Driving, Hit and run, Assaulting another driver, pedestrian, improper turns and lane changes, etc)
- Equipment (Window tint violations, Headlight/taillights out, Loud exhaust, Cracked windshield, etc.)
- Registration/Plates
- Seat Belt
- other (Call for Service, Violation of City/Town Ordinance, Suspicious Person, Motorist Assist/Courtesy, etc.)
7. search_conducted - Whether search is conducted in True and False form
8. stop_outcome - Result of violation
9. is_arrested - Whether a person was arrested in True and False form
10. stop_duration - Detained time for violators approx (in minutes)
11. drugs_related_stop - Whether a person was involved in drugs crime (True, False)"	420	2522	14	shubamsumbria	traffic-violations-dataset
6857	6857	rain dataset	This dataset contains the details that can predict the opportunity of rain	['earth and nature']	"Context
This dataset contains the features required for predicting the Opportunity of Rain .Predict using the Machine Learning Algorithms.
Content
This Dataset Contains every Attributes Related to the Rain.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Can you able to predict the Weather ??? Try using ML or Deep Learning??"	30	312	4	aiswaryasivakumar	rain-dataset
6858	6858	Dataset Harga Rumah Bandung		[]		8	43	0	rafliaping	dataset-harga-rumah-bandung
6859	6859	Conv_YXL_L8-2		[]		0	11	0	ding314	conv-yxl-l82
6860	6860	Parkinson Multiple sound recordings	Multiple Types of Sound Recordings Data Set	['diseases', 'advanced', 'data analytics', 'text data', 'pandas']		2	69	5	jeevanantham006	parkinson-multiple-sound-recordings
6861	6861	tpsjan02		[]		1	14	1	sytuannguyen	tpsjan02
6862	6862	tpsjan01		[]		1	13	1	sytuannguyen	tpsjan01
6863	6863	Week9Dataset		[]		10	535	0	just4jcgeorge	week9dataset
6864	6864	timm_whl		[]		0	8	0	djagatiya	timm-whl
6865	6865	vitweight		[]		0	13	0	ironluu	vitweight
6866	6866	 GPTJ-6B Slim Autoregressive Language Model	A 6 billion parameter, autoregressive text generation model trained on The Pile.	['intermediate', 'nlp', 'gpu', 'pytorch', 'transformers']	"Original Creators:
Ben Wang (kingoflolz)
EleutherAI
Citations:
1. @misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
2. @misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
GPT-J-6B
A 6 billion parameter, autoregressive text generation model trained on The Pile.
Acknowledgments
This project would not have been possible without compute generously provided by the TPU Research Cloud with assistance from EleutherAI.
Thanks to the Cloud TPU team at Google for providing early access to the Cloud TPU VM alpha (now publicly available!)
Thanks to everyone who have helped out one way or another (listed alphabetically):
Aran Komatsuzaki for advice with experiment design and writing the blog posts.
James Bradbury for valuable assistance with debugging JAX issues.
Janko Prester for creating the web demo frontend.
Laurence Golding for adding some features to the web demo.
Leo Gao for running zero shot evaluations for the baseline models for the table.
License
The weights of GPT-J-6B are licensed under version 2.0 of the Apache License.
Model Details
Hyperparameter  Value
n_parameters    6,053,381,344
n_layers    28*
d_model 4,096
d_ff    16,384
n_heads 16
d_head  256
n_ctx   2,048
n_vocab 50,257 (same tokenizer as GPT-2/3)
position encoding   Rotary position encodings (RoPE)
RoPE dimensions 64
* each layer consists of one feedforward block and one self attention block
The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3."	1	31	1	aryashah2k	gptj6b-slim-autoregressive-language-model
6867	6867	AP Handbooks		[]		3	25	0	kt1987	ap-handbooks
6868	6868	Sdcnet Cityscapes Segmentation		['art']		0	16	0	shashil	sdcnet-cityscapes-segmentation
6869	6869	test123		[]		1	9	0	onchutrng	test123
6870	6870	SZIN_224_W		[]		0	15	0	bachaboos	szin-224-w
6871	6871	coco_weight	this is the weights of yolov3 pretrain in coco	[]		0	2	0	lweize	coco-weight
6872	6872	Data of Energy Production, Trade & Consumption	Data from UN about energy production, trade and consumption of 236 countries	['global', 'environment', 'energy', 'renewable energy', 'beginner']		113	799	4	manomayjamble	data-of-energy-production-trade-consumption
6873	6873	landmark-448-train-6		[]		0	15	0	kittenraidrua	landmark-448-train-6
6874	6874	landmark-448-train-5		[]		0	9	0	kittenraidrua	landmark-448-train-5
6875	6875	landmark-448-train-7		[]		0	8	0	kittenraidrua	landmark-448-train-7
6876	6876	landmark-448-val-3		[]		0	15	0	kittenraidrua	landmark-448-val-3
6877	6877	RNSA_Subset_PNGs_Uploaded_by_Aftab_70K+_7_1_2022		[]		0	21	0	aftabhussaincui	rnsa-subset-pngs-uploaded-by-aftab-70k-7-1-2022
6878	6878	ccks2020		[]		0	16	0	haowu11	ccks2020
6879	6879	multimodal		[]		5	16	0	anunnikrishnan	multimodal
6880	6880	PyTorch Geometric	PyTorch Geometric wheels for torch 1.9.1 and cuda 11.0	['python']		5	173	1	lyomega	torch-geometric
6881	6881	TIANCHI_二手车		[]		0	35	0	niubi666	tianchi
6882	6882	tfrecords_256_by_256_autoencoded		[]		0	9	1	narminj	tfrecords-256-by-256-autoencoded
6883	6883	mydataset		[]		0	12	0	navidmoradi	mydataset
6884	6884	Conv_YXL_L8		[]		0	8	0	ding314	conv-yxl-l8
6885	6885	srssss		[]		0	11	0	wangmaos	srssss
6886	6886	furniture		[]		2	28	0	leon1621	furniture
6887	6887	Supermarket Sales		[]		9	70	0	richyirad	supermarket-sales
6888	6888	KDDTrain		[]		0	1	0	balasubramaniankn	kddtrain
6889	6889	efftool		[]		0	25	0	ironluu	efftool
6890	6890	mmcv-1.4.1		[]		0	18	0	xiaohaige	mmcv141
6891	6891	Training_1.csv		[]		0	28	0	williamsmedina	training-1csv
6892	6892	UniverseNet		[]		0	14	0	xiaohaige	universenet
6893	6893	Turkish Folk Song Lyrics	Turkish Folk Song Lyrics from Different Regions of Turkey	['culture and humanities', 'music', 'europe', 'nlp', 'text data']	"Context
This dataset includes lyrics from popular Turkish Folk Songs including region of songs. All the data is scraped from lyrics pages. ( Bu veri seti popüler Türkçe türkülerin sözlerinden oluşmaktadır. Sözler internet sitelerinden toplanmıştır.)
Disclaimer : The file might contain missing songs, lyrics in a different language, or incorrect lyrics.
Content
Folk songs are a crucial part of a culture. It represents the language of emotions which makes this dataset interesting for answers of many questions. For example, wording might be different from region to region. Or, which words are more popular in which regions?
Inspiration
This dataset is a great resource for applying NLP techniques. Many different methods from basic analysis like wordcloud to lyrics generation with LSTM neural networks can be applied."	3	92	0	emreokcular	turkish-folk-song-lyrics
6894	6894	xception		[]		0	9	0	jiepengxu	xception
6895	6895	Hand Cricket Dataset	Hand signs for numbers 0 to 6	['cricket']	"Context
This dataset contains images of hand signs for numbers 0 to 6. The dataset contains a total of 1848 images. To ensure generality (i.e prevent overfitting to one type of hand in one type of environment) images were taken with 4 persons, in 6 different lighting conditions, in 3 different background.
Context
This dataset contains images of hand signs for numbers 0 to 6. The dataset contains a total of 1848 images. To ensure generality (i.e prevent overfitting to one type of hand in one type of environment) images were taken with 4 persons, in 6 different lighting conditions, in 3 different background.
Sample images after augmentation are shown below:
Application
The dataset was created to build the hand-cricket/odd or even game. The game can be found at Hand-Cricket"	1	62	0	abhinavnayak	hand-cricket-dataset
6896	6896	landmark-448-val-4		[]		0	16	0	kittenraidrua	landmark-448-val-4
6897	6897	384models		[]		0	35	0	greepex	384models
6898	6898	Bellabeat Case Study	Guiding Marketing Strategy with Average Fitness Trends	['business', 'marketing', 'beginner', 'data cleaning', 'data analytics']	"Context
Bellabeat is a tech-driven company that creates fashionable wellness products for women. It was founded by Urška Sršen, who has a background in art and is the Chief Creative Officer, and Sando Mur, who has a background in mathematics and is a key member of the Bellabeat executive team.
Data Source:
Data comes from the dataset FitBit Fitness Tracker Data, a CCO Public Domain Licensed dataset made available on Kaggle by the user Mobius. It contains data from thirty individuals who consented and submitted the data for their personal fitness trackers.
Business Task:
Analyze data from Fitbit and find trends that can be applied to Bellabeat fitness tracking products to help guide a new marketing strategy."	60	408	4	dcgonk	bellabeat-case-study
6899	6899	HRNet_Mscale		[]		0	28	0	crackcode466	hrnet-mscale
6900	6900	b5ns_1810		[]		1	13	0	honihitak	b5ns-1810
6901	6901	Head Orientation Digested	A mapping from the eyes poistion to the rotation of the head	['health']		0	7	0	ziadloo	head-orientation-digested
6902	6902	Kuwait COVID-19 Dataset	Data from Kuwait COVID-19 Dashboard and MOH	['asia', 'public health', 'tabular data', 'covid19']	"Kuwait COVID-19 Dataset
Description
Dataset is scrapped from the public Kuwait COVID-19 Dashboard which is maintained by Central Agency for Information Technology (Cait) and data from the Ministry of Health - Kuwait all credits to both of them.
Vaccine Data scrapped from Ministry of Health - Kuwait website.
This dataset just provides their public data into a CSV format for Data Science / Machine Learning purposes.
Acknowledgements
Kuwait Ministry of Health
Central Agency for Information Technology
Links to all sources provided above."	24	589	4	aalshamali	kuwait-covid19-dataset
6903	6903	crimen-boston		[]		0	18	0	fernandobordi	crimenboston
6904	6904	reddit pro-ana dataset	dataset on a banned subreddit r/ProED (pro-eating disorder)	['online communities']		1	36	0	matakahas	reddit-proana-dataset
6905	6905	UCF-QNRF_TEST		['universities and colleges']		1	15	0	marathonking	ucfqnrf-test
6906	6906	MNIST_M	Added handwritten digital dataset with various backgrounds	['image data', 'online communities']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	0	23	1	washkaiw	mnist-m
6907	6907	pengunjung mall		['retail and shopping']		0	9	0	sanialuthfia	pengunjung-mall
6908	6908	cokedataset		[]		2	13	0	luatvytran	cokedataset
6909	6909	bpf_tfrec1014		[]		0	30	0	yusukekita	bpf-tfrec1014
6910	6910	bpf_tfrec59		[]		0	124	0	yusukekita	bpf-tfrec59
6911	6911	bpf_tfrec04		[]		0	132	0	yusukekita	bpf-tfrec04
6912	6912	jigsaw_toxic_more_language(from Pavel Ostyakov)	use tta to get augmented data	['arts and entertainment']	"data created by Pavel Ostyakov
I didn't find an online version so I downloaded them from following link and uploaded it
link : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038"	2	19	0	yezyouo	jigsaw-toxic-more-language
6913	6913	trial1-training_join		[]		0	19	0	mennagalal	trial1training-join
6914	6914	Big Data Bowl 2022 Supplementals		[]		0	10	0	vingkan	big-data-bowl-2022-supplementals
6915	6915	nfl_kickoffs		[]		0	5	0	sertalpbilal	nfl-kickoffs
6916	6916	tuned swin model for paw	tuned swin model for paw	[]		0	19	0	igorjovanovic	tuned-swin-model-for-paw
6917	6917	submissions2		[]		0	8	0	jason03yingling	submissions2
6918	6918	Big Data Bowl 2022 JM US		[]	Supplemental data for creating the main notebook.	9	24	0	jpmiller	big-data-bowl-2022-jm-us
6919	6919	images		[]		0	5	0	marcotilli	images
6920	6920	hitters3csv		[]		0	8	0	serdargurler	hitters3csv
6921	6921	Current Health Expenditure Per Capita		['categorical data', 'matplotlib', 'numpy', 'pandas', 'seaborn']		4	24	1	yarkork	current-health-expenditure-per-capita
6922	6922	Payments for Utility Services	Trends in Real Estate Payments	['russia', 'business', 'real estate', 'image data']	"Context
I consider it necessary to develop automated planning and tracking of utility bills. As well as analysis of the trend in changes in prices and rates of use.
Content
The data are collected in the form of image files and distributed across payment areas. Metering devices are presented separately.
Acknowledgements
Thanks for any coding works and ideas.
Inspiration
What is the most effective mechanism for the economy in utility payments?"	0	44	0	olgabelitskaya	payments-for-utility-services
6923	6923	CarPrice_Prediction.csv		[]		3	23	0	mdwasimakhtar03	carprice-predictioncsv
6924	6924	timm-j		['arts and entertainment']		1	14	0	jasonpv	timmj
6925	6925	gifsForNFL		[]		0	8	0	sohumshah	gifsfornfl
6926	6926	result_tr		[]		1	16	0	muhsinertrk	result-tr
6927	6927	BDB_Final_Data2		[]		1	10	0	mattymo18	bdb-final-data2
6928	6928	datasetWine		[]		2	24	0	rodriguekalash	datasetwine
6929	6929	petfinder_swin_dog_ext		[]		0	9	0	atamazian	petfinder-swin-dog-ext
6930	6930	FinancialNewsHeadline		[]		1	19	1	keitazoumana	financialnewsheadline
6931	6931	TPU Food 101		['business']	"Context
Your favorite food dataset is back! This time with 30s (hopefully) training with transfer learning
Content
This holds the tfrecords for TPU utilization. They were crafted from the train and test splits from TF Datasets (TFDS).
The format is image, class. Class is in sparse mode -- you can one hot with a decoder.
Acknowledgements
Also available:
here @ Kaggle 1
here @ Kaggle 2"	0	23	0	alexanderlav	tpu-food-101
6932	6932	Nordic Holidays		['travel']		4	17	0	jason03yingling	nordic-holidays
6933	6933	sccl-10		[]		1	23	0	urospetricevic	sccl10
6934	6934	S1-E1-signal		[]		0	10	0	marymalaaeldeen	s1e1signal
6935	6935	Schmoedown Scores	Scores before Sudden Death	[]		0	24	1	dandolinski	schmoedown-scores
6936	6936	Upload Plot Gifs		['online communities']		0	29	0	mauromauro	upload-plot-gifs
6937	6937	Finland_Norway_Swedish_holidays		['computer science']	"Context
It may be helpful for Jan 2022 TPS Kaggle Competition.
Content
Holiday table for Finland/Norway/Swedish. Days are based on 2022 public holidays from internet.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	1	41	3	ifashion	finland-norway-swedish-holidays
6938	6938	danila	danila-miner 2.3.1 d	['video games']		13	33	0	antonchernetsov	danila
6939	6939	"RUNNING""calorie:heartrate"	"Fitness~""Bellbeat""~Tracker"	['exercise', 'computer science', 'programming']	"title: 'BellaBeat Fitbit'
author: 'C Romero'
date: 'r Sys.Date()'
output:
  html_document:
    number_sections: true
    toc: true
{r}
Installation of the base package for data analysis tool
install.packages(""base"")
{r}
Installation of the ggplot2 package for data analysis tool
install.packages(""ggplot2"")
{r}
install Lubridate is an R package that makes it easier to work with dates and times.
install.packages(""lubridate""){r}
Installation of the tidyverse package for data analysis tool
install.packages(""tidyverse"")
{r}
Installation of the tidyr package for data analysis tool
install.packages(""dplyr"")
{r}
Installation of the readr package for data analysis tool
install.packages(""readr"")
{r}
Installation of the tidyr package for data analysis tool
install.packages(""tidyr"")
Importing packages
metapackage of all tidyverse packages
library(base)
library(lubridate)# make dealing with dates a little easier
library(ggplot2)# create elegant data visialtions using the grammar of  graphics
library(dplyr)# a grammar of data manpulation
library(readr)# read rectangular data text
library(tidyr)
Running code
In a notebook, you can run a single code cell by clicking in the cell and then hitting 
the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, 
you can run code by highlighting the code you want to run and then clicking the blue arrow
at the bottom of this window.
Reading in files
{r}
list.files(path = ""../input"")
load the activity and sleep data set{r}
dailyActivity &lt;- read_csv(""../input/wellness/dailyActivity_merge.csv"")
sleepDay &lt;- read_csv(""../input/wellness/sleepDay_merged.csv"")
```
check for duplicates and na
sum(duplicated(dailyActivity))
sum(duplicated(sleepDay))
sum(is.na(dailyActivity))
sum(is.na(sleepDay))
now we will remove duplicate from sleep & create new dataframe
sleepy &lt;- sleepDay %&gt;% distinct() 
head(sleepy)
head(dailyActivity)
count number of id's total sleepy & dailyActivity frames
n_distinct(dailyActivity$Id)
n_distinct(sleepy$Id)
get total sum steps for each member id
dailyActivity %&gt;% group_by(Id) %&gt;% summarise(freq = sum(TotalSteps))  %&gt;% arrange(-freq)
Tot_dist &lt;- dailyActivity %&gt;% 
  mutate(Id = as.character(dailyActivity$Id)) %&gt;%
group_by(Id) %&gt;% 
  summarise(dizzy = sum(TotalDistance))  %&gt;% 
  arrange(-dizzy)
now get total min sleep & lie in bed
sleepy %&gt;% group_by(Id) %&gt;% summarise(Msleep = sum(TotalMinutesAsleep)) %&gt;% arrange(Msleep)
sleepy %&gt;% group_by(Id) %&gt;% summarise(inBed = sum(TotalTimeInBed)) %&gt;% arrange(inBed)
plot graph for ""inbed and sleep data"" & ""total steps and distance""
{r}
ggplot(Tot_dist) + 
  geom_count(mapping = aes(y= dizzy, x= Id, color = Id, fill = Id, size = 2)) +
  labs(x = ""member id's"", title  = ""distance miles"" ) +
  theme(axis.text.x = element_text(angle = 90))"	9	92	0	romechris34	wellness
6940	6940	Food Delivery		[]		9	122	0	rafaelgfelippe	food-delivery
6941	6941	fontchinese		[]		0	5	0	tonysmallfish	fontchinese
6942	6942	Snake Dataset- India	The image dataset of Indian snakes with class as venomus and non-venomous.	['india', 'animals', 'cnn', 'image data', 'pytorch']	"This is the image dataset of various snake species found in India. The dataset is classified into train and test previously and then to Non-Venomous and Venomous. It contains images of cobras to vipers to rat snake and green tree vine.
The size of every image is 400×400. 
If you need a dataset with snake classes, you can comment in this dataset. I will provide that also.
Hope you will like my effort."	55	813	17	adityasharma01	snake-dataset-india
6943	6943	PREMIERLEAGUESTATS BEGINNER 	BEGINNER SQL STATEMENTS	['business']		3	54	0	adibkazi	premierleaguestats-beginner
6944	6944	model7_Extended		[]		0	18	0	mhashoorii	model7-extended
6945	6945	Cancer_datasetgg		[]		0	17	0	gayathri26	cancer-datasetgg
6946	6946	FiveThirtyEight Comic Characters Dataset-gg		['comics and animation']		0	18	0	gayathri26	fivethirtyeight-comic-characters-datasetgg
6947	6947	pawpularity_cnn		[]		0	19	0	sourabhwarrier	pawpularity-cnn
6948	6948	heat_disease_prediction_dataset		[]		0	31	0	mdwasimakhtar03	heat-disease-prediction-dataset
6949	6949	FiveThirtyEight Comic Characters Dataset.		['comics and animation']		0	8	0	gayathri26	fivethirtyeight-comic-characters-dataset
6950	6950	Cyclistic Bike Share Case Study	Comparing Members to Casual Riders	['sports']	"Context
In this case study in Google's Data Analytics course, Cyclistic is a fictional bike share company with hundreds of stations across Chicago; bikes can be unlocked from one station and returned to any other station. There are two groups of customers, ""members"" (who purchase annual memberships) and ""casual riders"" (who purchase single-ride or full-day passes). Because members are much more profitable than casual riders, the company is interested in a marketing campaign to convert casual riders to members. They have tasked the data analyst with looking into the differences between these two groups. In this case study, I walk through how I use Excel, SQL and Tableau to discover and illustrate differences between these two groups.
Data Cleaning and Preparation
First, as instructed in the case study, I added two columns to each spreadsheet:
1. ride_length (by subtracting the started_at column from the ended_at column and formatting to Time&gt;37:30:55)
2. day_of_ week (using function: =weekday(started_at,1) and formatting the cell to General)
Then, I inspected and cleaned each of the twelve monthly CSV files in this dataset, doing the following in each monthly file:
Checking for duplicates in the ride_id column. There were none.
Added filter dropdowns to spreadsheet to quickly check all entries in the member/casual column were either ""member"" or ""casual."" There were no issues.
Sorted table by the started_at column to quickly ensure all rides have a start date in the month to which the monthly file pertains. There were no issues. 
Sorted table by the ride_length column to quickly see if there are many rides of a length of zero or less. There were many of these. I decided to remove all rides of less than thirty seconds, the idea being it would be difficult to get anywhere in 30 seconds and these may have been cases of people having trouble unlocking the bikes or changing their minds, or some kind of data error. There was no clear delineation; I made a judgement call, and I could just as well have picked the one minute mark or the zero second mark. In situations like these, I also like to do a separate analysis where I remove rows at the zero second mark and rows at the one minute mark to see if they produce substantially different results in terms of comparing ride length and number of trips between members and casual riders. It did not seem to produce a meaningful difference, and I kept my base case at removing all rides less than 30 seconds.
I ran a pivot table with start_station_name and then start_station_id as rows and Count of ride_id as values as a convenient way to check that there was not more than one id assigned to one station. I then flipped the two rows in the pivot to check that there was not more than one station assigned to one id. I repeated the steps using end_station_name and end_station_id. I did not see any multiple IDs for a given station name or vice versa. However, I did see station IDs with ""Test"" in the ID. Whether these were cases of a customer or an employee bringing a bike to or from a test facility was unclear (all of these were tagged to either ""member"" or ""casual"" in the member_casual column, which suggests they are customers or that employees are misleadingly represented as customers in the data). I made a judgement call to sort the table by start_station_ID and end_station_ID and removed any rides with ""Test"" in either ID. An analysis comparing members and casual riders with and without this filter, and seeing if this decision makes a substantial difference would be useful here, time permitting.
I also noticed from the pivot that many rows had blank station names and IDs. I made a judgement call to include these rows when analyzing ride length and number of trips. These rows are likely to be legitimate rides, and we do not need to know the station name or ID to compare ride length or number of rides. Moreover, removing these legitimate rides could skew the data if station names/IDs are more likely not to be reported in casual rides, for instance. An analysis comparing members and casual riders with and without this filter would be useful here.
However, for comparing which stations were more popular starting points among members vs casual riders, rows with blank station names/IDs would need to be removed. Therefore, I created 12 excel files for each month with these rows removed specifically for the start station analysis to be explained later. I also ran a pivot table in one of the monthly tables with start\station_id and then start_lat as rows and noticed that many stations had slightly different latitude and longitude coordinates depending on the ride entry; this would be dealt with in a SQL query as explained later.
Analysis and Sharing - Number of Trips
To compare members with casual riders based on number of trips, we can use Excel. In each monthly file, I ran a pivot table with day_of_week as a row, member_casual as a column and count of ride_ID as a value. From this pivot table, we can get the total number of rides for members and for casual riders for each day of the week in a given month, and the total for each monthly file gives the number of rides for each month. I created a summary excel file with the number of rides for both members and casual riders for each month over the last 12 months, and for each day of the week across the prior 12 months. I then loaded the summary excel table into Tableau and produced the following illustrations:
Rides Per Day of Week
Rides Per Month
As the first illustration shows, casual riders clearly use this service more on the weekend, whereas use among members is spread more evenly across the week, likely due to a greater share of commutes to work among members.
The second illustration shows that both members and casual riders use this service much more when weather is warmer, and there is very little use during the cold Chicago winter. Casual riders do show a steeper dropoff immediately at the end of the summer, suggesting a substantial share of casual riders may be summer tourists. It should be noted that when looking at trips per month, growth in the business over the course of the last twelve months may skew results intended to analyze seasonality.
Analysis and Sharing - Average Trip Length
To get an average trip length across the 12 monthly files, I thought it would be easier to use SQL than Excel, so I used Google BigQuery for my SQL queries. The first step would be to consolidate the 12 monthly files into one table. Under the project ""class-project-1-332418"", I created a dataset called ""bike_length"" and in this dataset I uploaded 12 tables corresponding to the 12 monthly files. I then ran the following query to consolidate the twelve monthly files and saved the output as a table called ""BikeLength"" in the same dataset:
SELECT
FROM class-project-1-332418.bike_length.AprilLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.AugLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.DecLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.FebLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.JanLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.JulyLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.JuneLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.MarchLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.MayLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.NovLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.OctoberLength
UNION ALL 
SELECT
FROM class-project-1-332418.bike_length.SepLength
AS BikeLength
Next, I ran the following query to get the average ride length across the previous 12 months for both members and casual riders:
SELECT 
     AVG(date_diff(ended_at,started_at,MINUTE))
FROM class-project-1-332418.bike_length.BikeLength
GROUP BY member_casual
Results from the query were approximately 27 minutes for casual riders and 14 minutes for members. Part of this difference is due to the fact that casual riders have multi-day trips to a much greater extent than members. To compare trip length by day of week, I ran the query below:
SELECT AVG(date_diff(ended_at,started_at,MINUTE))
FROM class-project-1-332418.bike_length.BikeLength
WHERE member_casual = 'member'
GROUP BY day_of_week
I then ran the query again only replacing member with casual under WHERE. The results of both queries are shown in the following illustration:
Average Ride Length by Day
For each day of the week, casual riders have a substantially longer average ride length than members. Both members and casual riders have a higher average ride length on the weekend. 
Analysis and Sharing - Starting Locations
Next, I wanted to see if there was a difference between members and casual riders in terms of their starting locations. To do this, I start with the 12 excel files I created specifically for this part of the analysis, as noted previously. Unlike the files used for the other parts of the analysis, these files exclude entries where station ID is blank. I used SQL to analyze the start locations across the twelve monthly files. Under the project ""class-project-1-332418"", I created a dataset called ""Bicycle"" and in this dataset I uploaded 12 tables corresponding to the 12 monthly files. I then ran the following query to consolidate the twelve monthly files and saved the output as a table called ""Year_Start"" in the same dataset:
SELECT
FROM class-project-1-332418.Bicycle.April_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.Aug_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.Dec_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.Feb_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.Jan_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.July_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.June_Start
UNION ALL
SELECT
FROM class-project-1-332418.Bicycle.March_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.May_Start
UNION ALL
SELECT
FROM class-project-1-332418.Bicycle.Nov_Start
UNION ALL 
SELECT
FROM class-project-1-332418.Bicycle.Oct_Start
UNION ALL
SELECT
FROM class-project-1-332418.Bicycle.Sep_Start
Next, I ran the following query to get the number of rides for each starting station across the previous 12 months for both members and casual riders. It also returns the average latitude and longitude coordinates for each station. An average is used to account for the problem noted previously that in the files, stations have slightly different coordinates across entries. 
SELECT
    COUNT(ride_id) number_of_rides, ROUND(AVG(start_lat),4) avg_st_lat, ROUND(AVG(start_lng),4) avg_st_lng, start_station_name
FROM class-project-1-332418.Bicycle.Year_Start
WHERE member_casual = 'member'
GROUP BY start_station_name
ORDER BY number_of_rides DESC 
In then ran the query again, replacing ""member"" with ""casual"" under WHERE. I uploaded the results to Tableau, The following link shows two maps, one for casual members and one for the member category (notice the two tabs to toggle between). The maps illustrate that start locations are more concentrated toward the touristy city center among the casual riders, while start locations for members are more spread out.
Maps of Start Locations
Conclusion
Casual riders generally have longer rides than members, prefer summers and especially weekends to a greater extent than members, and are more concentrated toward the city center in terms of their starting locations. My recommendations to the company based on this analysis are:
Given the steep drop off in activity at the end of the summer, and the congregation of starting locations in more touristy areas, further analysis may be warranted to determine what share of casual riders are tourists. If most casual riders are tourists, an annual membership may not be attractive to them.
After considering the previous point, the company still believes it should move forward with a marketing campaign targeting casual riders, I would time the campaign to best leverage the strong share of activity over the summer.
Given that casual riders prefer weekends to a much greater extent than members prefer weekends, weekend-only annual membership with a lower price than current annual memberships may be attractive to many casual riders.
Acknowledgements
Thanks to the Google Data Analytics course on Coursera for providing this case study."	2	58	0	tonygregoire	bicycle-rental-start-locations
6951	6951	Covid-19 Global Dataset	Up-to-date numbers of daily Confirmed, Death and Active cases for 218 countries	['global', 'health', 'time series analysis', 'deep learning', 'covid19']	"For the latest analysis and visualizations of the COVID-19 pandemic, check out my constantly updated EDA notebook here 📈.
Context
&gt; Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the strain of coronavirus that causes coronavirus disease 2019 (COVID-19), the respiratory illness responsible for the COVID-19 pandemic.
Since its first identification in December 2019 in Wuhan, China, this virus has taken the world by storm. Some people prefer to look at the positive side of things and how this pandemic has brought forward several positive changes. However, the collateral damages produced by this pandemic cannot be overlooked. From the Economic impact to Mental Health impacts, this pandemic period will arguably be one of the hardest periods we'll encounter in our lives.
That being said, we always have to arm ourselves with hope. With the new advancements in the vaccine studies, let's hope to wake up from this nightmare as soon as possible.
&gt; “Hope is being able to see that there is light despite all of the darkness.” – Desmond Tutu
As for the reason for me building this dataset, it's because I couldn't get my hands on an easily digestible and up-to-date dataset of Covid-19, so, I decided to build my own using Python and web scraping techniques.
I will also update this dataset as frequently as possible!
Content
This data was scraped from woldometers.info on 2022-01-05 by Joseph Assaker.
218 countries are represented in this data.
All of countries have records dating from 2020-2-15 until 2022-01-05 (691 days per country).
That's with the exception of China, which has records dating from 2020-1-22 until 2022-01-05 (715 days per country).
Summary Data Columns Description:
country: designates the Country in which the the row's data was observed.
continent: designates the Continent of the observed country.
total_confirmed: designates the total number of confirmed cases in the observed country.
total_deaths: designates the total number of confirmed deaths in the observed country.
total_recovered: designates the total number of confirmed recoveries in the observed country.
active_cases: designates the number of active cases in the observed country.
serious_or_critical: designates the estimated number of cases in serious or critical conditions in the observed country.
total_cases_per_1m_population: designates the number of total cases per 1 million population in the observed country.
total_deaths_per_1m_population: designates the number of total deaths per 1 million population in the observed country.
total_tests: designates the number of total tests done in the observed country.
total_tests_per_1m_population: designates the number of total test done per 1 million population in the observed country.
population: designates the population count in the observed country.
Daily Data Columns Description:
date: designates the date of observation of the row's data in YYYY-MM-DD format.
country: designates the Country in which the the row's data was observed.
cumulative_total_cases: designates the cumulative number of confirmed cases as of the row's date, for the row's country.
daily_new_cases: designates the daily new number of confirmed cases on the row's date, for the row's country.
active_cases: designates the number of active cases (i.e., confirmed cases that still didn't recover nor die) on the row's date, for the row's country.
cumulative_total_deaths: designates the cumulative number of confirmed deaths as of the row's date, for the row's country.
daily_new_deaths: designates the daily new number of confirmed deaths on the row's date, for the row's country.
Acknowledgements
As previously mentioned, all the data present in this dataset is scraped from worldometers.info.
Inspiration
Going through this data, Kagglers can visualize various trends in their own country, or compare several countries.
One can also combine this dataset with other news and key points in time (lockdowns, new UK mutation, Holidays, etc.) in order to study the effects of these events on the progression of Covid-19 in a multitude of countries.
Implementing time series analysis on this dataset would also be an amazing idea! Getting a deep learning algorithm to learn from this sea of data and try to predict the future turn of events could be quite interesting!"	5994	27516	93	josephassaker	covid19-global-dataset
6952	6952	dedushki		[]		1	15	0	skvayzer	dedushki
6953	6953	Bikes Data Set		['cycling']		1	16	0	avilashsen	bikes-data-set
6954	6954	cotscocov2		[]		2	25	0	phoenix9032	cotscocov2
6955	6955	FieldValue2020		[]		0	37	0	rxsims	fieldvalue2020
6956	6956	La Liga Top Three Teams with Points since 1929	All the data provided here is Team name and points.	['football']	"Context
I love football. As I am a fan of Barcelona, one of the teams of La Liga, I created this dataset in order to get the insights of  the title race in the margin of points earned throughout the entire season.
Content
The table consists of 94 rows and 7 columns. Each row contains specific season, top three names of the table and their respective points from 1929 to 2021.
Inspiration
How many points are required to win La liga in the upcoming years? What is the possible range of points in order to get a position in the top three?"	114	715	7	azizulhakim98	la-liga-top-three-teams-with-points-since-1929
6957	6957	IMDb_movies.csv		['internet']		10	30	0	valeriaroberti24	imdb-moviescsv
6958	6958	VITON plus		['business']		0	5	0	rkuo2000	viton-plus
6959	6959	pikachuexperiment90		[]		0	16	0	gyanendradas	pikachuexperiment90
6960	6960	Projet_leyenda		[]		4	39	3	karimsalhi	projet-leyenda
6961	6961	advertising2csv		[]		0	4	0	serdargurler	advertising2csv
6962	6962	tiny_model		[]		0	26	0	fightforever1	tiny-model
6963	6963	hitters2csv		[]		0	18	0	serdargurler	hitters2csv
6964	6964	2D Data Matrix Code 	Dataset of 2D data matrix code on pharma products + relative label in yolo for	[]		1	27	0	coccodev	2d-data-matrix-code
6965	6965	Hitters.csv		['baseball']		1	28	0	serdargurler	hitterscsv
6966	6966	lyrics		[]		0	15	0	aliizadi1	lyrics
6967	6967	224models	111111111111111111111111111111	[]		3	52	0	greepex	224models
6968	6968	Saved_Data		[]		0	12	0	sandeepmohanmuktha	saved-data
6969	6969	netflix_data		[]		1	41	0	housseinihadia	netflix-data
6970	6970	fpl2016_21		[]		0	25	0	agarwalkshitij	fpl2016-21
6971	6971	cheval		[]		1	3	0	emnadaknou	cheval
6972	6972	COVID-CT_Dataset		[]		0	7	0	ragibmehedi	covidct-dataset
6973	6973	Windstorm_data		[]		0	0	0	pk1618122	windstorm-data
6974	6974	Artstation Landscape Thumbnails		['art']		5	34	0	victororlov	artstation-landscape-thumbnails
6975	6975	fdl-network		['news']		0	32	0	thomasgak	fdlnetwork
6976	6976	Feedback_analysis_clf	Classifier trained on the feedback analysis model	['computer science', 'text data']		0	17	1	icarus96	feedback-analysis-clf
6977	6977	AirNow Bishkek	PM2.5 pollution data of US Embassy, BIshkek, Kyrgyzstan	['environment', 'pollution']	Source: https://www.airnow.gov/international/us-embassies-and-consulates/#Kyrgyzstan$Bishkek	6	638	1	pavelisayenko	airnow-bishkek
6978	6978	ICH_Train_Val_Test		[]		0	18	0	aftabhussaincui	ich-train-val-test
6979	6979	pet2_breed_model		[]		0	14	0	ktakita	pet2-breed-model
6980	6980	subaru ds train 0106		['automobiles and vehicles']		0	23	0	wuliaokaola	subaru-ds-train-0106
6981	6981	Lyme (Clean and Dirty)		[]		7	171	0	yoctoman	lyme-clean-and-dirty
6982	6982	COVID Data on Infection, Death and Vaccination		[]	This is a dataset on global COVID infection, death and vaccination record, which is made available by Our World in Data through this link.	4	44	0	nathanieldiu	covid-data-on-infection-death-and-vaccination
6983	6983	traindata		[]		2	41	0	frezneal	traindata
6984	6984	pet-swin224-1753		[]		0	10	0	honihitak	pet-swin224-1753
6985	6985	O/L Results		['standardized testing']		0	33	0	dulininadisha	ol-results
6986	6986	Advertising		[]		0	7	0	saurabhktiwari	advertising
6987	6987	metric		[]		1	27	1	swatiwasal	metric
6988	6988	tfEmbeddings		[]		1	20	0	aqeeljanjua	tfembeddings
6989	6989	OAI-MRI-3DDESS		[]	"Context
The dataset was downloaded from the OAI https://nda.nih.gov/oai/ website. There are 3D DESS KNEE MRI images for 3K patients at baselines aquisition time. 
The image slices are labeled as positive / negative using the KL grades extracted from the Xray images from the OAI Xray dataset."	0	5	0	mohamedberrimi	oaimri3ddess
6990	6990	textdata		[]		18	275	0	rkuo2000	textdata
6991	6991	KDDTest		[]		0	12	0	balasubramaniankn	kddtest
6992	6992	Cat_BREED		[]		1	25	0	noixiahpla	cat-breed
6993	6993	Deepcov_list		[]		0	18	0	jisnava	deepcov-list
6994	6994	petfinder_scripts		[]		0	70	0	ktakita	petfinder-scripts
6995	6995	One Time Password	All OTP messages used for signing in using temporary phone number	['internet', 'nlp', 'text mining']	"Data of all OTP messages used to signing in different applications using temporary phone number. This data was scraped from sms24
All the credits for this data goes to sms24 website."	6	144	1	muhammedabdulazeem	one-time-password
6996	6996	Temp21/12		[]		5	40	0	daominhkhanh	temp2112
6997	6997	homeworksr		[]		2	8	0	dildobuster	homeworksr
6998	6998	BrainTumorDataset		[]		2	9	0	marufadewole	braintumordataset
6999	6999	Retails Dashboard	Retail Financials Power BI Report	['business', 'finance', 'data visualization', 'data analytics']		61	400	1	leratomotsamai	retails-dashboard
7000	7000	3090_ex017_2		[]		1	17	0	anonamename	3090-ex017-2
7001	7001	unsampled		[]		1	30	1	tanvir507	unsampled
7002	7002	subaru ds train 0106 half		['automobiles and vehicles']		0	12	0	wuliaokaola	subaru-ds-train-0106-half
7003	7003	Furniture2		[]		1	23	0	ningkang218	furniture2
7004	7004	Furniture1		[]		0	25	0	ningkang218	furniture1
7005	7005	retail		[]		1	26	0	aravinthbosem	retail
7006	7006	36000 Proxies	36000 Working Proxies	['international relations', 'internet', 'online communities', 'social networks']	"Context
36000 Working Proxies
Content
36000 Working Proxies
Acknowledgements
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	8	186	7	mathurinache	36000-proxies
7007	7007	Do You Know Where America Stands On Guns?	Poll quiz about guns	['beginner']	"About this dataset
&gt; <p>This folder contains the data behind the quiz <a href=""https://projects.fivethirtyeight.com/guns-parkland-polling-quiz/"" target=""_blank"" rel=""nofollow"">Do You Know Where America Stands On Guns?</a></p>
<p><code>guns-polls.csv</code> contains the list of polls about guns that we used in our quiz. All polls have been taken after February 14, 2018, the date of the school shooting in Parkland, Florida.</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 100 samples along with End, Republican Support, technical information and other features such as:
- Start
- Support
- and more.
How to use this dataset
&gt; - Analyze Question in relation to Url
- Study the influence of Population on Pollster
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	49	521	3	yamqwe	poll-quiz-gunse
7008	7008	dataset120		[]		0	14	0	gerymaligne	dataset120
7009	7009	Bike Sharing Dataset	BoomBikes Bike Sharing Prediction - Regression Problem	['transportation', 'automobiles and vehicles', 'beginner', 'linear regression', 'regression']	"Description:
A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.
In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.
They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:
Which variables are significant in predicting the demand for shared bikes.
How well those variables describe the bike demand
Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.
Bussiness Goal:
We are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.
Acknowledgement:
This dataset is referred from Kaggle & UCI Repository.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the share of bikes.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	350	2501	14	yasserh	bike-sharing-dataset
7010	7010	Club Soccer Predictions	Global Club Soccer Rankings	['football', 'sports', 'people', 'people and society', 'beginner']	"About this dataset
&gt; <p>Data behind <a href=""https://projects.fivethirtyeight.com/soccer-predictions/"" target=""_blank"" rel=""nofollow"">Club Soccer Predictions</a> and <a href=""https://projects.fivethirtyeight.com/global-club-soccer-rankings/"" target=""_blank"" rel=""nofollow"">Global Club Soccer Rankings</a>.</p>
<p><code>spi_matches.csv</code> contains match-by-match SPI ratings and forecasts back to 2016.</p>
<p><code>spi_matches_latest.csv</code> contains match-by-match SPI ratings and forecasts for each league's latest season.</p>
<p><code>spi_global_rankings.csv</code> contains current SPI ratings and rankings for men's club teams.</p>
<p><code>spi_global_rankings_intl.csv</code> contains current SPI ratings and rankings for men's international teams.</p>
<p><em><strong>License:</strong></em> The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
<p><em><strong>Updated:</strong></em> synced from source weekly</p>
This dataset was created by FiveThirtyEight and contains around 10000 samples along with League Id, Nsxg1, technical information and other features such as:
- Season
- Proj Score2
- and more.
How to use this dataset
&gt; - Analyze Importance1 in relation to Spi2
- Study the influence of Probtie on Team2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	51	630	1	yamqwe	club-soccer-predictionse
7011	7011	map_images		[]		0	10	2	mariofrei	map-images
7012	7012	pos_img		[]		1	25	0	vincentwang25	pos-img
7013	7013	OWMmod_2		[]		0	3	0	ding314	owmmod-2
7014	7014	homeworks		[]		0	12	0	dildobuster	homeworks
7015	7015	homework		['education']		0	20	0	dildobuster	homework
7016	7016	extract		['nutrition']		2	52	0	jinitsan	extract
7017	7017	RLEASE AND REMOVE RIGHT NOW		[]		0	23	0	alexisterrell	rlease-and-remove-right-now
7018	7018	busi_trained_model		[]		0	27	0	kongqiangzhu	busi-trained-model
7019	7019	glioma absent		['cancer']		0	8	0	arunsai125	glioma-absent
7020	7020	cybers		[]		0	20	0	chahattandon	cybers
7021	7021	livecell_8_class_dataset		[]		0	31	0	adc28827810	livecell-8-class-dataset
7022	7022	lgbm_models_06_01_22_goss		[]		0	9	0	malekbadreddine	lgbm-models-06-01-22-goss
7023	7023	longformer-baseline-pb-cfg-f4-cv-0.6207		[]		0	3	0	atharvaingle	longformer-baseline-pb-cfg-f4
7024	7024	OWMmod_0045		[]		0	7	0	ding314	owmmod-0045
7025	7025	sartuoris_c_dataset		[]		0	20	0	adc28827810	sartuoris-c-dataset
7026	7026	greatbarrierreef		[]		1	20	0	saraswatitiwari	greatbarrierreef
7027	7027	fake_covid		[]		2	12	0	deepakbhandare	fake-covid
7028	7028	Fake.csv		[]		2	16	0	deepakbhandare	fakecsv
7029	7029	Conv_5t-02		[]		0	4	0	ding314	conv-5t02
7030	7030	exampleoptloc		[]		0	5	0	rxsims	exampleoptloc
7031	7031	tez_fb_large		[]		171	312	5	abhishek	tez-fb-large
7032	7032	CaseStudy_Bellabeat		['health and fitness']		0	27	0	mickeydlee11	casestudy-bellabeat
7033	7033	SlowDeepFood	GCC-30: types of Middle Eastern food and Pizza-Styles: 14 types Italian Pizza.	['deep learning', 'cooking and recipes']	"Context
We tested the pipeline for fast collection of food image data sets by creating two custom data sets: Pizza-Styles focuses on variations of the popular Italian dishes whereas GCC-30 focuses on a particularly interesting cuisine(namely, Middle Eastern cuisine) that has not yet been addressed by the food computing community.
Content
SlowDeepFood datasets contain two different datasets i.e., GCC-30 contains 30 Middle Eastern Cuisine, and Pizza-Styles contains 14 different styles of Italian Pizza, Also refer to our list of publicly available, geo-referenced food data sets at https://slowdeepfood.github.io/datasets/
Both of the datasets were used in the conference Paper ""SlowDeepFood: a food computing framework for regional gastronomy"".
Here is the link to the paper https://diglib.eg.org/handle/10.2312/stag20211476
Acknowledgements"	3	52	0	naumanullah	slowdeepfood
7034	7034	Skills		[]		2	22	0	medvedevlev	skills
7035	7035	labeled_test_data		[]		0	22	0	medvedevlev	labeled-test-data
7036	7036	lgbm_models_06_01_22		[]		0	12	0	malekbadreddine	lgbm-models-06-01-22
7037	7037	Collections of Audio Files	Sound Processing and Word Recognition	['music', 'russia', 'people and society', 'linguistics', 'programming', 'audio data']	"Context
The home collection of audio recordings can be useful for training in processing sound files and speech recognition.
Content
Audio files with different formats.
Acknowledgements
Thanks to those who post examples of handling audio files.
Inspiration
Speech in Russian is more difficult to recognize in comparison with others or not?"	1	35	0	olgabelitskaya	collections-of-audio-files
7038	7038	BIKESHARE DATA	DATA ANALYSIS USING R	[]		1	25	0	trishlabhardwaj	bikeshare-data
7039	7039	stage2(&gt;=55) of Lovely (fastai & timm)		[]		0	19	0	nizhen	stage255-of-lovely-fastai-timm
7040	7040	stage2(&lt;16) of Lovely (fastai & timm)		[]		0	1	0	nizhen	stage216-of-lovely-fastai-timm
7041	7041	200 hour yoga teacher training rishikesh		['exercise']		0	10	0	yoganagri	200-hour-yoga-teacher-training-rishikesh
7042	7042	300 Hour Yoga Teacher Training Rishikesh		['exercise']		0	30	0	yoganagri	300-hour-yoga-teacher-training-rishikesh
7043	7043	Netflix2022		[]		2	25	0	heysuresh	netflix2022
7044	7044	images		[]		0	20	0	cyberpuks	images
7045	7045	agriculture product export from India_2014_2015	products exported from India	['agriculture']		5	56	1	harshavarthinib	agriculture-product-export-from-india-2014-2015
7046	7046	Affnist_data		[]		0	58	0	njnj123	affnist-data
7047	7047	CoNIC Challenge Dataset	Patch-level LIZARD dataset for CoNIC Challenge	['healthcare', 'earth and nature', 'biology', 'health', 'medicine', 'image data']	"The dataset consists of Haematoxylin and Eosin stained histology images at 20x objective magnification (~0.5 microns/pixel) from 6 different data sources. For each image, an instance segmentation and a classification mask is provided. Within the dataset, each nucleus is assigned to one of the following categories:
Epithelial
Lymphocyte
Plasma 
Eosinophil
Neutrophil
Connective tissue
For more information on the dataset and the associated categories, we encourage participants to read the original dataset paper.
Data Format
Our provided patch-level dataset contains 4,981 non-overlapping images of size 256x256 provided in the following format:
- RGB images
- Segmentation & classification maps
- Nuclei counts
The RGB images and segmentation/classification maps are each stored as a single NumPy array. The RGB image array has dimensions 4981x256x256x3, whereas the segmentation & classification map array has dimensions 4981x256x256x2. Here, the first channel is the instance segmentation map and the second channel is the classification map. For the nuclei counts, we provide a single csv file, where each row corresponds to a given patch and the columns determine the counts for each type of nucleus. The row ordering is in line with the order of patches within the numpy files.
A given nucleus is considered present in the image if any part of it is within the central 224x224 region within the patch. This ensures that a nucleus is only considered for counting if it lies completely within the original 256x256 image.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
This dataset was provided by the Organizers of the CoNIC Challenge:
- Simon Graham (TIA, PathLAKE)
- Mostafa Jahanifar (TIA, PathLAKE)
- Dang Vu (TIA)
- Giorgos Hadjigeorghiou (TIA, PathLAKE)
- Thomas Leech (TIA, PathLAKE)
- David Snead (UHCW, PathLAKE)
- Shan Raza (TIA, PathLAKE)
- Fayyaz Minhas (TIA, PathLAKE)
- Nasir Rajpoot (TIA, PathLAKE)
TIA: Tissue Image Analytics Centre, Department of Computer Science, University of Warwick, United Kingdom
UHCW: Department of Pathology, University Hospitals Coventry and Warwickshire, United Kingdom
PathLAKE: Pathology Image Data Lake for Analytics Knowledge & Education, University Hospitals Coventry and Warwickshire, United Kingdom"	6	210	5	aadimator	conic-challenge-dataset
7048	7048	yolox_X_1920x1280_2nd		[]		0	2	0	dragonzhang	yolox-x-1920x1280-2nd
7049	7049	all_hatss		[]		0	9	0	firozk	all-hatss
7050	7050	Conv_dis_1task		[]		0	21	0	ding314	conv-dis-1task
7051	7051	Beverage brands in Vietnam		[]		1	33	0	phamtrongliem	beverage-brands-in-vietnam
7052	7052	Conv_5task-03		[]		0	2	0	ding314	conv-5task03
7053	7053	python in kaggle		['computer science']		0	32	0	ziangzhou188	python-in-kaggle
7054	7054	cryptopunk_hats		[]		0	13	0	geezus6pro	cryptopunk-hats
7055	7055	stereData		[]		0	47	0	yeguoxing	steredata
7056	7056	ridge_j_cleanexp2_fasttext_multiseeds_oof		[]		0	9	0	shobhitupadhyaya	ridge-j-cleanexp2-fasttext-multiseeds-oof
7057	7057	Conv_dis5task02		[]		0	14	0	ding314	conv-dis5task02
7058	7058	people_i	people image for matting	[]		2	33	0	moneyhongrain	people-i
7059	7059	Metaverse Crypto Tokens Historical data 📊 📓	Top leading and buzzing metaverse tokens	['games', 'finance', 'exploratory data analysis', 'data visualization', 'time series analysis']	"Context
The metaverse, a living and breathing space that blends physical and digital, is quickly evolving from a science fiction dream into a reality with endless possibilities. A world where people can interact virtually, create and exchange digital assets for real-world value, own digital land, engage with digitized real-world products and services, and much more.
Major tech giants are beginning to recognize the viability and potential of metaverses, following Facebook’s groundbreaking Meta rebrand announcement. In addition to tech companies, entertainment brands like Disney have also announced plans to take the leap into virtual reality. 
While the media hype is deafening, your average netizen isn’t fully aware of what a metaverse is, how it operates and, most importantly—what benefits and opportunities it can offer them as a user. 
What Is The Metaverse?
In its digital iteration, a metaverse is a virtual world based on blockchain technology. This all-encompassing space allows users to work and play in a virtual reflection of real-life and fantasy scenarios, an online reality, ranging from sci-fi and dragons to more practical and familiar settings like shopping centers, offices, and even homes. 
Users can access metaverses via computer, handheld device, or complete immersion with a VR headset. Those entering the metaverse get to experience living in a digital realm, where they will be able to work, play, shop, exercise, and socialize. Users will be able to create their own avatars based on face recognition, set up their own businesses of any kind, buy real estate, create in-world content and asset,s and attend concerts from real-world superstars—all in one virtual environment,
With that said, a metaverse is a virtual world with a virtual economy. In most cases, it is an online reality powered by decentralized finance (DeFi), where users exchange value and assets via cryptocurrencies and Non-Fungible Tokens.
What Are Metaverse Tokens?
Metaverse tokens are a unit of virtual currency used to make digital transactions within the metaverse. Since metaverses are built on the blockchain, transactions on underlying networks are near-instant. Blockchains are designed to ensure trust and security, making the metaverse the perfect environment for an economy free of corruption and financial fraud.
Holders of metaverse tokens can access multiple services and applications inside the virtual space. Some tokens give special in-game abilities. Other tokens represent unique items, like clothing for virtual avatars or membership for a community. If you’ve played MMO games like World of Warcraft, the concept of in-game items and currencies are very familiar. However, unlike your traditional virtual world games, metaverse tokens have value inside and outside the virtual worlds. Metaverse tokens in the form of cryptocurrency can be exchanged for fiat currencies. Or if they’re an NFT, they can be used to authenticate ownership to tethered real-world assets like collectibles, works or art, or even cups of coffee.
Some examples of metaverse tokens include SAND of the immensely popular Sandbox metaverse. In The Sandbox, users can create a virtual world driven by NFTs. Another token is MANA of the Decentraland project, where users can use MANA to purchase plots of digital real estate called “LAND”. It is even possible to monetize the plots of LAND purchased by renting them to other users for fixed fees. The ENJ token of the Enjin metaverse is the native asset of an ecosystem with the world’s largest game/app NFT networks.
Dataset Information
The dataset brings some of the most dominant cryptos among the other metaverse cryptos. Pls refer to the file Metaverse coins.csv to find the list of metaverse crypto coins.
The dataset will be updates on a weekly basis with more and more additional metaverse tokens, Stay tuned ⏳"	240	2046	15	kaushiksuresh147	metaverse-cryptos-historical-data
7060	7060	sartorius-train		['arts and entertainment']		0	13	0	prateekagnihotri	sartorius-train
7061	7061	Urbanization	Investigation of urbanization indicators based on geographical area.	['social science']	"Urbanization
Urbanization is one of the indicators that is growing due to industrial advances and technology, the study of this indicator can affect the living conditions, employment, transportation, etc. of communities around the world.
Urbanization is one of the indicators that is growing due to industrial advances and technology, the study of this indicator can affect the living conditions, employment, transportation, etc. of communities around the world.
Gratitude
globaldatalab.org
Photo by <a href=""https://unsplash.com/@levimeirclancy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Levi Meir Clancy</a> on <a href=""https://unsplash.com/s/photos/urbanization?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	5	57	2	mahanmir1383	urbanization
7062	7062	resnet50	cccccccccccccccccccccccccccccccc	[]		0	4	0	aftabhussaincui	resenet152v2
7063	7063	Cyclistic-data sets.		[]		1	43	1	abhaymanjunathnaik	cyclisticdata-sets
7064	7064	ridge_j_cleanexp1_fasttext_multiseeds_oof		[]		0	4	0	shobhitupadhyaya	ridge-j-cleanexp1-fasttext-multiseeds-oof
7065	7065	TestDataset		[]		1	16	0	ananyaangadi	testdataset
7066	7066	Hate-speech-CNERG/dehatebert-mono-english		[]		0	43	0	tunminhhunh	hatespeechcnergdehatebertmonoenglish
7067	7067	TIANCHI_learn_二手车交易价格预测		[]		1	80	1	niubi666	tianchi-learn
7068	7068	apsis multi tests		[]		9	206	0	riadhossainapsis	apsis-multi-tests
7069	7069	module1		[]		0	9	0	sandeepmohanmuktha	module1
7070	7070	Dogssss		[]		0	15	1	muhammadammarjamshed	dogssss
7071	7071	model_299_by_299		[]		0	61	1	narminj	model-299-by-299
7072	7072	mask images		['arts and entertainment']		0	17	0	milangautammg	mask-images
7073	7073	PAW-cait-m48-448-seed5		['arts and entertainment']		0	2	0	a24998667	paw-cait-m48-448-seed5
7074	7074	skin_disease_3disease_11		[]		2	31	0	hardikkher28	skin-disease-3disease-11
7075	7075	Decimal and thousand separators in every locale	properly format number based on locale code	['data cleaning', 'python']	"Context
A collection of decimal and thousand separators used in each locale/culture.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
cover photo credit: https://www.pexels.com/photo/collection-of-banknotes-with-dollar-bill-on-top-4025825/ (Photo by Pratikxox from Pexels)
Inspiration
How to properly parse and format number properly in every locale/culture."	69	454	6	prasertk	decimal-and-thousand-separators-in-every-locale
7076	7076	NFT Dataset from DefiLlama	Dataset on Nifty collections, daily volume and chain level info	['arts and entertainment', 'finance', 'tabular data', 'investing']	"Context
NFTs or Nifties are making monumental changes in the art market and the way in which these assets are stored and traded. 
The idea behind this dataset is to identify the NFT collections are making waves in the community currently.
Content
NFT Daily Volume:
Daily trading volumes of the NFT market is captured in this file
NFT Collections:
Different popular NFT collections and their daily trading volume is present in this file
NFT Chainlevel:
Chain level NFT trading information is present in this file
Acknowledgements
Thanks to DefiLlama for making this NFTs available in the first place
Photo by <a href=""https://unsplash.com/@dylancalluy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Dylan Calluy</a> on <a href=""https://unsplash.com/s/photos/nft?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>
Inspiration
Some questions that can be answered are
* How the NFT Volume has changed over time?
* What are the trending NFT collections?
* What are the top sold NFT collections?"	109	1310	10	sudalairajkumar	nft-dataset-from-defillama
7077	7077	Moutinho28	VGG-16 weights for all epochs in TensorFlow	['earth and nature']		2	25	1	miguelcalado	moutinho28
7078	7078	tps notes 		[]		0	13	0	saraswatitiwari	tps-notes
7079	7079	results		['standardized testing']		0	15	0	vvennhao	results
7080	7080	PetFinder - pretrained models		[]		0	45	0	nanguyen	petfinder-pretrained-models
7081	7081	r9weight0		[]		15	65	0	mmaymay	r9weight0
7082	7082	Star Wars Survey	Star Wars Survey Analysis Dataset	['arts and entertainment', 'movies and tv shows']	"Star Wars Survey
This dataset contains data behind the story [America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters)].
Content
This dataset contains 38 columns with about more than 1100 rows to put your data analysis skills to test.
More specifically, there are 38 columns and 1187 unique rows. There are some unnamed columns but those are just the extension of some named columns spanned over rows. You can choose to modify them according to your needs. This dataset will boost your ""Cleaning the Data"" skills.
Acknowledgements
This dataset is from:
(https://fivethirtyeight.com/)"	4	56	0	arnavr10880	star-wars-survey
7083	7083	conv_5task_1		[]		0	16	0	ding314	conv-5task-1
7084	7084	petfinder 190		[]		0	4	0	hiteshkumars	petfinder-190
7085	7085	diabetes1		[]		4	22	0	mohmoualla	diabetes1
7086	7086	wine quality		['alcohol']		2	44	0	thotasivateja57	wine-quality
7087	7087	ExampleEnsemble		[]		0	20	0	rxsims	exampleensemble
7088	7088	Hard skills and soft skills dataset		[]		7	24	0	balamuruga	hard-skills-and-soft-skills-dataset
7089	7089	path_deviations_by_carrier		[]		1	35	0	robynritchie	path-deviations-by-carrier
7090	7090	r9weight		[]		14	52	1	llibinn	r9weight
7091	7091	V1 model		[]		0	21	0	intjerlu	v1-model
7092	7092	perfomance evaluation		[]		0	209	0	dakorep	perfomance-evaluation
7093	7093	biodesign_butterfly_2		[]		2	4	0	zmjjiang	biodesign-butterfly-2
7094	7094	Covid19CT83353		[]		0	17	0	mustai	covid19ct83353
7095	7095	Biodesign_Butterfly		[]		1	5	0	zmjjiang	biodesign-butterfly
7096	7096	csphdis1-assignment-1		[]		0	4	0	shixiangui	csphdis1assignment1
7097	7097	json_Reef		[]		2	3	1	zhuhaoqi	json-reef
7098	7098	NFLBDB		[]		0	130	0	benjenkins96	nflbdb
7099	7099	deberta_base		[]		0	15	0	masatakaaoki	deberta-base
7100	7100	fastai_models_vit_large_r50_s32_224		[]		7	35	0	aosanori	fastai-models-vit-large-r50-s32-224
7101	7101	time-serise-19-covid-combined_csv.csv		[]	"COVID-19 virus data from datahub.
https://datahub.io/core/covid-19#resource-time-series-19-covid-combined"	0	38	0	woohanyoung	timeserise19covidcombined-csvcsv
7102	7102	Clash Royale Battles- Upper Ladder, December 2021	A large collections of Clash Royale Battles	['games', 'video games', 'categorical data', 'intermediate', 'binary classification']	"Context
Clash Royale is a wildly popular game that revolves around fast 1v1 battles. The game is unique in that, along with skill, a player's ""deck"" (exactly 8 deployable troops the player picks) has also been widely thought to affect one's chances in the game. It is intriguing to be able to see how the endless world of deck configurations match up.
Content
The dataset has two files:
- data_ord.csv: This is juicy part. A collection of 3/4ths of 1 million 1v1 battles recorded from the game.
    - Each row is 1 battle
    - The first 8 columns describe Player 1's deck, representing each card with a number
    - The next 8 columns describe Player 2's deck, representing each card with a number
    - The next 2 columns describe the respective trophy counts of Players 1 and 2
    - The last column is the outcome of the battle; 1 if Player 1 wins, and 0 if Player 2 wins
    - IMPORTANT: Player decks HAVE NO ORDER (In data_ord.csv, they are sorted by ascending card ID, but they can be in any arrangement in real life), and card ID numbers are all categorical
- cardlist.csv: You will notice that rather than the names of a card, they are instead described as integers in data_ord.csv. This file translates each card ID from data_ord.csv into its corresponding card name.
Acknowledgements
Data obtained from the Clash Royale API
Inspiration
I collected this data in hopes to see if the aforementioned ""deck configuration"" and ""trophy counts"" could, for the most part, fully predict the outcome of a game."	142	1620	17	nonrice	clash-royale-battles-upper-ladder-december-2021
7103	7103	Usage of Parking lots in Lisbon Portugal	Historical data about usage of available parking lots in Lisbon Portugal	['engineering', 'transportation', 'tabular data']	"Context
Urban mobility is an interesting subject for a city. Parking lots play an important role in the movement of vehicles across the city. This data set can be used to perform analysis regarding the usage rate, maximum capacity a and spatial position of parking places in the city of Lisbon-Portugal.
Content
The dataset is provided by EMEL, which is the responsible company that manages the parking places in Lisbon. It contains the geographic position of the parking places (WGS84), maximum capacity, and usage of each parking place during the year os 2020.
There are 3 files, respectively for the 1st, 2nd, and 4th quarter. Unfortunately t there is no data for the 3rd quarter.
Inspiration
The main goal is to answer the following questions:
1. How well-distributed is the parking places in Lisbon?
2. Given that Portugal had lockdowns during 2020, is it possible to notice its effects in the dataset?
3. Is it possible to notice a movement during the day, from parking places outside to inside the city?"	2	86	0	aryvini	usage-of-parking-lots-in-lisbon-portugal
7104	7104	inpainting_models		[]		2	78	0	erknee	inpainting-models
7105	7105	img_cls_cs_py		[]		0	39	0	lonelvino	img-cls-cs-py
7106	7106	base-comparateur-de-territoires		[]		1	14	1	ziadfellahidrissi	basecomparateurdeterritoires
7107	7107	Carvana Image Masking (PNG)		[]	Original: https://www.kaggle.com/c/carvana-image-masking-challenge	11	36	0	ipythonx	carvana-image-masking-png
7108	7108	SemEval 2014 Task 4: AspectBasedSentimentAnalysis	SemEval-2014 ABSA Task is based on laptop and restaurant reviews	['nlp', 'text mining', 'text data', 'transfer learning', 'transformers']	"Copied from https://alt.qcri.org/semeval2014/task4/#, all credits to respective authors. 
SemEval-2014 Task 4
Task Description: Aspect Based Sentiment Analysis (ABSA)
Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided.
In particular, the task consists of the following subtasks:
Subtask 1: Aspect term extraction
Given a set of sentences with pre-identified entities (e.g., restaurants), identify the aspect terms present in the sentence and return a list containing all the distinct aspect terms. An aspect term names a particular aspect of the target entity.
For example, ""I liked the service and the staff, but not the food”, “The food was nothing much, but I loved the staff”. Multi-word aspect terms (e.g., “hard disk”) should be treated as single terms (e.g., in “The hard disk is very noisy” the only aspect term is “hard disk”).
Subtask 2: Aspect term polarity
For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative).
For example:
“I loved their fajitas” → {fajitas: positive}
“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive}
“The fajitas are their first plate” → {fajitas: neutral}
“The fajitas were great to taste, but not to see” → {fajitas: conflict}
Subtask 3: Aspect category detection
Given a predefined set of aspect categories (e.g., price, food), identify the aspect categories discussed in a given sentence. Aspect categories are typically coarser than the aspect terms of Subtask 1, and they do not necessarily occur as terms in the given sentence.
For example, given the set of aspect categories {food, service, price, ambience, anecdotes/miscellaneous}:
“The restaurant was too expensive”  → {price}
“The restaurant was expensive, but the menu was great” → {price, food}
Subtask 4: Aspect category polarity
Given a set of pre-identified aspect categories (e.g., {food, price}), determine the polarity (positive, negative, neutral or conflict) of each aspect category.
For example:
“The restaurant was too expensive” → {price: negative}
“The restaurant was expensive, but the menu was great” → {price: negative, food: positive}
Datasets:
Two domain-specific datasets for laptops and restaurants, consisting of over 6K sentences with fine-grained aspect-level human annotations have been provided for training.
Restaurant reviews:
This dataset consists of over 3K English sentences from the restaurant reviews of Ganu et al. (2009). The original dataset of Ganu et al. included annotations for coarse aspect categories (Subtask 3) and overall sentence polarities; we modified the dataset to include annotations for aspect terms occurring in the sentences (Subtask 1), aspect term polarities (Subtask 2), and aspect category-specific polarities (Subtask 4). We also corrected some errors (e.g., sentence splitting errors) of the original dataset. Experienced human annotators identified the aspect terms of the sentences and their polarities (Subtasks 1 and 2). Additional restaurant reviews, not in the original dataset of Ganu et al. (2009), are being annotated in the same manner, and they will be used as test data. 
Laptop reviews:
This dataset consists of over 3K English sentences extracted from customer reviews of laptops. Experienced human annotators tagged the aspect terms of the sentences (Subtask 1) and their polarities (Subtask 2). This dataset will be used only for Subtasks 1 and 2. Part of this dataset will be reserved as test data. 
Dataset format:
The sentences in the datasets are annotated using XML tags.
The following example illustrates the format of the annotated sentences of the restaurants dataset.
xml
The possible values of the polarity field are: “positive”, “negative”, “conflict”, “neutral”. The possible values of the category field are: “food”, “service”, “price”, “ambience”, “anecdotes/miscellaneous”.
The following example illustrates the format of the annotated sentences of the laptops dataset. The format is the same as in the restaurant datasets, with the only exception that there are no annotations for aspect categories. Notice that we annotate only aspect terms naming particular aspects (e.g., “everything about it” does not name a particular aspect).
xml
In the sentences of both datasets, there is an"	38	436	2	charitarth	semeval-2014-task-4-aspectbasedsentimentanalysis
7109	7109	PWP SVR		[]		0	11	0	aishikai	pwp-svr
7110	7110	Dude Perfect Battles Stats	Statistics for Solo Battle Videos posted on Dude Perfect's YouTube channel	['popular culture', 'arts and entertainment', 'celebrities', 'movies and tv shows', 'sports']	"Context
Dude Perfect (DP) is one of the most recognized content creator groups on YouTube, popularized mainly by their trick shot videos. Within the last couple of years, they have started producing a significant number of 'Battle' videos, where the five main DP members (Tyler, Garrett, Cody, Cory, and Coby), along with the occasional special guest, compete against each other in a series of sport-related challenges. While the challenges and the style of each of the Battles is different each time, each challenge will have a winner (or winners, as will be addressed later), and will have a 'podium', that is, others who finish within the top 2 or top 3 of the competition.
There are two types of Battles, which we will refer to as Solo and Team Battles . Solo Battles are those where the DP members compete against each other individually, while Team Battles are those where DP members are in separate teams and the winner includes all members from the winning team. In this dataset, only Solo Battles are considered.
Content
This dataset records who wins and who places on the podium in each challenge, as well as overall statistics about the performance of each DP member historically in Battles. 
As the style of each Battle is not consistent, there are some Battles which have a format that necessitates a difference in how the standings are recorded in this dataset. These situations are as follows:
- For each Battle, there is either a distinct 'podium' which consist of the members who make it to the 'Finals', or there is a objective measurement of who places first, second, and third, such as the number of points that each member obtains or the time in which they complete a race. These two methods are used to designate most of the standings.
- For some Battles , there are only two members in the 'Finals'. In this case, there is not a member recorded in third place.
- In some Battles , such as the Blindfolded Archery Battle, there is not a clear and objective way to decide the second and third place, due to how the 'Finals' for that particular Battle are structured. Thus, the players that would be in second and third are both given second place in that Battle.
- For the Drone Hunting Battle, there was not one but two winners. In this case, both winners were given first place.
- For some Battles,  such as the Go Kart Battle, either the first, second or third place member is not one of the main 5 DP members. In this case, this spot is not claimed. For example, in Go Kart Battle Panda won, Cody finished second, and Cory finished third. In this case, Cody and Cory would keep second and third, and no one would claim first place.
It should be noted that while there is a Battle with two winners (Drone Hunting Battle) and a Battle with a winner who is not one of the main 5 DP members (Go Kart Battle), these are the only two Battles of their kind. Thus, out of the 60 Solo Battles, there are still 60 winners out of the main 5 DP members.
Sources
All data is sourced from Dude Perfect's YouTube channel."	31	329	4	isaacwen	dude-perfect-battles-stats
7111	7111	ASdjqldnqk		[]		0	26	0	trillertj	qwerty
7112	7112	ImportingDataInPython		[]		0	70	0	kakamana	importingdatainpython
7113	7113	Spending		['news']		4	10	0	catluong	spending
7114	7114	Medical Appointment		[]		2	40	0	awosikuvivian	medical-appointment
7115	7115	last_038_pth		[]		1	19	0	dragonzhang	last-038-pth
7116	7116	tfgbr-masks		[]		0	30	0	ks2019	tfgbr-masks
7117	7117	Punta Cana Hotels Review - Spanish		['hotels and accommodations']		4	27	1	beltrewilton	punta-cana-hotels-review-spanish
7118	7118	Human Action2		[]		1	49	0	prantosarkar	human-action2
7119	7119	Covid-19 in Campos dos Goytacazes.		['health']		3	276	0	lucas12j	covid19-in-campos-dos-goytacazes
7120	7120	master dictionary		[]		3	7	1	prateekiet	master-dictionary
7121	7121	fghjkfrtyui		[]		0	19	0	andriy12	fghjkfrtyui
7122	7122	h5 model		[]		1	342	0	kittlein	h5-model
7123	7123	duo_dataset		[]		0	13	0	phillipsi	duo-dataset
7124	7124	Apple Stock Price in Last 180 days	APPL Stock data in last 6 months	['business']	"Context✔️
this dataset contains stock prices of famous apple company
Content💯
Timestamp, High, Low, close and Open values
Acknowledgements👋
thanks to alphavantage to its amazing api, I put the link below:
https://www.alphavantage.co/"	0	14	3	cyruskouhyar	apple-stock-price-in-last-180-days
7125	7125	bathsize1		[]		7	25	1	kaggleqrdl	bathsize1
7126	7126	adjusted_299_by_299_tfrecords		[]		0	1	1	narminj	adjusted-299-by-299-tfrecords
7127	7127	aoa_colwise_softmax		[]		0	38	0	enkrish259	aoa-colwise-softmax
7128	7128	172 articles urls		['internet']		0	5	1	prateekiet	172-articles-urls
7129	7129	Book Assignment		['literature']		0	14	0	nikhilnaganur	book-assignment
7130	7130	Books Assignment		['literature']		0	29	0	nikhilnaganur	books-assignment
7131	7131	Kinematics Motion Data	Kinematics Motion Data from IOS Device	['earth and nature', 'physics', 'mobile and wireless', 'classification', 'binary classification']	"Description:
Smartphones are getting intelligent day by day to assist Human's to aid in their day to day activites. A new feature has emerged popular in the fitness comunity that keeps an account of one's daily footsteps. 
More advanced versions include differentiating between detecting the difference between walking & run. This is achieved with the help of Sensors. Several such Sensorory data is recorded with IOS device & labelled as walking or running as 0 or 1. 
Currently, the dataset contains a single file which represents 88588 sensor data samples collected from accelerometer and gyroscope from iPhone 5c in 10 seconds interval and ~5.4/second frequency. This data is represented by following columns (each column contains sensor data for one of the sensor's axes):
acceleration_x
acceleration_y
acceleration_z
gyro_x
gyro_y
gyro_z
There is an activity type represented by ""activity"" column which acts as label and reflects following activities:
* ""0"": walking
* ""1"": running
Apart of that, the dataset contains ""wrist"" column which represents the wrist where the device was placed to collect a sample on:
* ""0"": left wrist
* ""1"": right wrist
Additionally, the dataset contains ""date"", ""time"" and ""username"" columns which provide information about the exact date, time and user which collected these measurements.
Can you build a strong classifier model to detect these so that it can be incorporated in the IOS device?
Acknowledgements:
This dataset has been referred from Kaggle.
It complements https://github.com/vmalyi/run-or-walk project which aims to detect whether the person is running or walking based on deep neural network and sensor data collected from iOS device.
This dataset has been accumulated with help of ""Data Collection"" iOS app specially developed for this purpose: https://github.com/vmalyi/run-or-walk/tree/master/ios_app_data_collection.
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict the type of Kinematic Motion.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	212	2367	25	yasserh	kinematics-motion-data
7132	7132	Customer Personality Analysis using clustering		['business']		5	57	0	israelwakayo	customer-personality-analysis-using-clustering
7133	7133	TITANIC_KNN		[]		0	21	1	kapilbhatt11	titanic-knn
7134	7134	Image Segmentation For work		['arts and entertainment']		0	19	0	milangautammg	image-segmentation-for-work
7135	7135	Marvel Characters		[]		0	41	0	whoslenny	marvel-characters
7136	7136	cascades		[]		0	2	0	rjouba	cascades
7137	7137	trrsssssssssss		[]		0	1	0	prajwalgotmare	trrsssssssssss
7138	7138	Fashin_bag		[]		0	24	0	trillertj	fashin-bag
7139	7139	jwehrkjqwejl		[]		0	16	0	trillertj	jwehrkjqwejl
7140	7140	Social_Network_Ads.csv		[]		13	37	0	mdwasimakhtar03	social-network-adscsv
7141	7141	plexp13oof1765f7		[]		1	18	1	nischaydnk	plexp13oof1765f7
7142	7142	Public_and_unofficial_holidays_Nor_Fin_Swe_2015-19	Holidays in Norway, Finland and Sweden for TPS - Jan 2022 competition	['beginner', 'time series analysis', 'tabular data', 'regression', 'holidays and cultural events']		108	208	11	vpallares	public-and-unofficial-holidays-nor-fin-swe-201519
7143	7143	face mask dataset		[]		1	41	0	rjouba	face-mask-dataset
7144	7144	movie_dataset	movie dataset for recommendation system	[]		8	25	0	yakrrao	movie-dataset
7145	7145	Extended2		[]		0	4	0	mhashoorii	extended2
7146	7146	Extended3		[]		0	11	0	mhashoorii	extended3
7147	7147	Extended1		[]		0	5	0	mhashoorii	extended1
7148	7148	competi		[]		0	23	0	ramidregos	competi
7149	7149	Humanoid - RICO - Android apps	Prepared data from RICO dataset	[]		1	55	0	wervertonfg	humanoiddataset
7150	7150	Walters microbiome dataset		['biology']		0	14	0	msandrk	walters-microbiome-dataset
7151	7151	TurkishLira x USD 2005-2022 Historical Daily Data	turkishlira and usd 17 years of daily data 	['finance', 'currencies and foreign exchange']		0	5	0	goktugcetin	turkishlira-x-usd-20052022-historical-daily-data
7152	7152	Config		[]		0	27	0	benbangbang	config
7153	7153	Kaggle colors	easy to use Kaggle brand colors	['computer science', 'exploratory data analysis', 'data visualization', 'matplotlib', 'plotly', 'seaborn']	"TL;DR
This is set of colors related to 'Kaggle brand book' which might be helpful when creating visualizations"	4	97	2	fergusfindley	kaggle-colors
7154	7154	stage1 (fastai & timm)		['earth and nature']		0	14	0	nizhen	stage1-fastai-timm
7155	7155	crnn_pretrained		[]		0	12	0	timofeev25	crnn-pretrained
7156	7156	pet2-tfrecords2-384-014		[]		0	14	0	bamps53	pet2-tfrecords2-384-014
7157	7157	brain-gan-weights		['biology']		3	31	0	maryamsaeedi	brainganweights
7158	7158	petfinder_swin_cat		[]		0	12	0	atamazian	petfinder-swin-cat
7159	7159	covid19_10000		[]		0	11	0	mustai	covid19-10000
7160	7160	petfinder_swin_dog		[]		0	7	0	atamazian	petfinder-swin-dog
7161	7161	Consumer Price Index (CPI) United States	All Items for the United States	['business', 'finance', 'economics', 'retail and shopping', 'currencies and foreign exchange']	"Context - Consumer price index
A consumer price index is a price index, the price of a weighted average market basket of consumer goods and services purchased by households. Changes in measured CPI track changes in prices over time.
Content
USACPIALLMINMEI, Consumer Price Index: All Items for the United States, Index 2015=100, Monthly, Not Seasonally Adjusted
Acknowledgements
https://fred.stlouisfed.org/series/USACPIALLMINMEI
FRED Graph Observations
Federal Reserve Economic Data
Link: https://fred.stlouisfed.org
Help: https://fredhelp.stlouisfed.org
Economic Research Division
Federal Reserve Bank of St. Louis
Organization for Economic Co-operation and Development, Consumer Price Index: All Items for the United States [USACPIALLMINMEI], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/USACPIALLMINMEI, January 4, 2022."	126	1558	10	sfktrkl	consumer-price-index-cpi-united-states
7162	7162	tensorflow		[]		0	11	0	saraswatitiwari	tensorflow
7163	7163	ResNet152		[]		0	20	0	xuhuizhan	resnet152
7164	7164	InceptionV3_mxnet		[]		0	11	0	xuhuizhan	inceptionv3-mxnet
7165	7165	number		[]		1	6	0	lexton2	number
7166	7166	Train data		[]		1	25	0	shalhamucha18	train-data
7167	7167	onetofivemodel		[]		0	8	0	gyanendradas	onetofivemodel
7168	7168	yueguoqing2		[]		0	17	0	yuefei2021	yueguoqing2
7169	7169	sixtotenmodel		[]		0	36	0	gyanendradas	sixtotenmodel
7170	7170	FS-Mol	FS-Mol: A Few-Shot Learning Dataset of Molecules	['earth and nature', 'chemistry']	"FS-Mol: A Few-Shot Learning Dataset of Molecules
Dataset from Microsoft's ""FS-Mol: A Few-Shot Learning Dataset of Molecules"" [link]
Summary
FS-Mol is A Few-Shot Learning Dataset of Molecules, containing molecular compounds with measurements of activity against a variety of protein targets. The dataset is presented with a model evaluation benchmark which aims to drive few-shot learning research in the domain of molecules and graph-structured data."	0	12	0	andyjwilkinson	fsmol
7171	7171	Test Videos 		['arts and entertainment']		0	21	0	aychul	test-videos
7172	7172	Order Sequence		[]		1	15	0	fyener	order-sequence
7173	7173	tps jan22		['news']		0	18	0	saraswatitiwari	tps-jan22
7174	7174	License_Plate_Demo	License plate pictures taken by mobile phone in underground garage	[]		0	41	1	wangreview	license-plate
7175	7175	Raisin Dataset UCI		['earth and nature']	Images of Kecimen and Besni raisin varieties grown in Turkey were obtained with CVS. A total of 900 raisin grains were used, including 450 pieces from both varieties. These images were subjected to various stages of pre-processing and 7 morphological features were extracted. These features have been classified using three different artificial intelligence techniques.	1	17	0	amirhosseinzabbah	raisin-dataset-uci
7176	7176	Pharma_IT_Share_Price_BSE.xlsx	Pharma IT company share prices	[]		3	14	0	amangala	bse-it-4cos-pharma-4cos-share-prices-2006-2021
7177	7177	Al_144		[]		0	15	0	jiayangxie	al-144
7178	7178	Heart Failure Prediction Dataset	Heart Failure Dataset with Classificatioo	['heart conditions']	"Attribute Information
Age: age of the patient [years]
Sex: sex of the patient [M: Male, F: Female]
ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
RestingBP: resting blood pressure [mm Hg]
Cholesterol: serum cholesterol [mm/dl]
FastingBS: fasting blood sugar [1: if FastingBS &amp;gt; 120 mg/dl, 0: otherwise]
RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or 
    depression o &amp;gt; 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]
ExerciseAngina: exercise-induced angina [Y: Yes, N: No]
Oldpeak: oldpeak = ST [Numeric value measured in depression]
ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
HeartDisease: output class [1: heart disease, 0: Normal]"	13	81	0	abdallahwagih	heart-failure-prediction-dataset
7179	7179	Dynatec Laboratories, Inc.		['healthcare', 'public health', 'business', 'health', 'health conditions']		0	7	1	dynateclabs	dynatec-laboratories-inc
7180	7180	hrnetparas		[]		1	83	0	tttts1	hrnetparas
7181	7181	nobell		[]		0	9	0	lucasbarbosalucao	nobell
7182	7182	Social Distancing		[]		0	26	0	ankit87	social-distancing
7183	7183	Movie Lens Dataset		['arts and entertainment']		1	28	3	aerospacer	movie-lens-dataset
7184	7184	beit_base_patch16_384		[]		0	14	0	akiyoshisutou	beit-base-patch16-384
7185	7185	Bike Ridership Data	Bike Ridership Data from Dec, 2020 to Nov, 2021	['transportation', 'beginner', 'data analytics', 'tidyverse']	"Context
I collected Cyclistic’s historical trip data from Dec2020- Nov2021 to analyze and identify trends.For the purposes of this case study, IDownloaded the previous 12 months of Cyclistic trip data here. The data has been made available by Motivate International Inc. under this license.
Content
The Bike Ridership Data contains bike riding data from December, 2020 to November, 2021. It has 13 columns and more than 5M rows. Columns are ride_id(char), rideable_type(char), start_station_id and name(char), end_station_id and name(char), member_casual(char), started_at and ended_at (datetime), start_lat and lng, end_lat and lng(dbl) type.
References
Divvy trip data from Dec2020-Nov2021. Retrieved from https://divvy-tripdata.s3.amazonaws.com/index.html"	1	21	0	humaperveen	bike-ridership-data
7186	7186	lstm_vocab		[]		0	12	0	enkrish259	lstm-vocab
7187	7187	Data for Player involvement as a result...		['education']		0	27	0	pawestrojny	data-for-player-involvement-as-a-result
7188	7188	images		[]		0	10	0	pierrealexandre78	images
7189	7189	iteration_vs_epoch		[]		0	8	0	lucaschmidtsantiago	iteration-vs-epoch
7190	7190	TMDB movies dataset	This contain the dataset of TMDB movies 	['arts and entertainment']		12	69	0	bhanushekhawat	tmdb-movies-dataset
7191	7191	Minidataset480		[]		0	14	0	gerymaligne	minidataset480
7192	7192	talib_download_files		[]		1	12	0	noi031	talib-download-files
7193	7193	BALF-table-data		[]		0	21	0	lz1997	balftabledata
7194	7194	Bilist Kaggle		['computer science']		0	38	0	sajilck	bilist-kaggle
7195	7195	Speeches from Slovak national parliament (NRSR)	In Slovak language. Scraped from http://tv.nrsr.sk	['languages', 'government', 'politics', 'linguistics', 'internet', 'text data']	"Context
This dataset contains speeches from meetings of National Parliament of Slovak Republic (Narodná rada Slovenskej republiky - NRSR). It was created by scraping official NRSR web archive http://tv.nrsr.sk. Code of the scraper is available at https://gitlab.com/msvana/nrsr-downloader.
Content
The dataset contains 58828 speeches from 4 last election terms including the current one. Oldest speech included is from May 26th 2010, the newest one is from March 13th 2018.
The data is provided in JSON format as an array of objects with following attributes:
speaker string: full name of the speaker
timestamp string: time of the speech in ISO format
speech string: text of the speech.
More fields might be added in the future.
Records are ordered by timestamp from newest to the oldest. The text is UTF-8 encoded. Very basic preprocessing has been performed on the data:
speaker name and speech text have been trimmed (white-space characters have been removed from the beginning and end of the string)
in speech text frequencies of white-space characters have been replaced by single space character."	42	1249	2	msvana	parliament-of-slovak-republic-nrsr-speeches
7196	7196	dataset2		[]		0	22	0	okkahermawany	dataset2
7197	7197	Microsoft Malware Bilist Kaggle	BoW Titles of Full Training Data of Microsoft Malware Detection Dataset	[]		2	31	0	sajilck	bilisttotal
7198	7198	plex10noisyoof1741		[]		0	16	1	nischaydnk	plex10noisyoof1741
7199	7199	fruitbat12		[]		7	26	0	akoluacik	fruitbat12
7200	7200	Public Holidays of Nordic Countries | TPS - Jan 22	Public Holidays in Finland, Sweden and Norway between 2015 and 2018	['europe', 'beginner', 'holidays and cultural events', 'travel']	"Context
Public Holidays in Nordic countries such as Finland, Sweden and Norway between 2016 and 2018.
Very helpful for TPS - Jan 2022 competition. 
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	8	61	4	sardorabdirayimov	public-holidays-of-nordic-countries-tps-jan-22
7201	7201	lstm_vocab_file_aoa	vocabulary of my train and test data	[]		0	4	0	enkrish259	lstm-vocab-file-aoa
7202	7202	Ugam Sentiment analysis		[]		0	14	0	ashokkumarpalivela	ugam-sentiment-analysis
7203	7203	mnist-128-40000-full-3x		['computer science']		0	11	0	liuyouwei	mnist12840000full3x
7204	7204	Informal economy in the regions of Ukraine		[]		4	29	0	vitaliiveremiienko	informal-economy-in-the-regions-of-ukraine
7205	7205	washingmachine		[]		0	14	0	suoyu233	washingmachine
7206	7206	exampleaddress		[]		0	21	0	gyanendradas	exampleaddress
7207	7207	dataset		[]		0	20	0	okkahermawany	dataset
7208	7208	fruitbat11		[]		3	21	0	akoluacik	fruitbat11
7209	7209	amerika		[]		0	30	0	aellatif	amerika
7210	7210	GDP 2015-2019: Finland, Norway, and Sweden	(in billions of US$)	['economics']	"Description
This is a csv file of the GDP data was obtained from The World Bank website.
Hopefully it will be of use in the Tabular Playground Series - Jan 2022 competition."	566	1983	38	carlmcbrideellis	gdp-20152019-finland-norway-and-sweden
7211	7211	2021_shandong_dongying		[]		0	4	0	niubi666	2021-shandong-dongying
7212	7212	atis_intent_classification		[]		2	24	0	tayfuntuna	atis-intent-classification
7213	7213	Public Transport Traffic in Minsk	1 minute interval data about vehicle coordinates	['transportation', 'automobiles and vehicles', 'beginner', 'geospatial analysis', 'tabular data']	"Where data came from?!
Data was collected from real vehicles with period in one minute. 
Content
This dataset represents information about public transport movement during time."	16	673	2	l3llff	traffic-data-in-minsk
7214	7214	Titanic,XGBoost	XGBoost algorithm on the famous Titanic dataset	['earth and nature', 'xgboost']		0	22	0	mahanmir1383	titanic
7215	7215	resnet50		[]		0	1	0	anyisheng	resnet50
7216	7216	pretrain8		[]		0	25	0	zhouhaolong	pretrain8
7217	7217	Singapore toursits historical data		['history']		1	36	0	irishilary	singapore-toursits-historical-data
7218	7218	split fold [COTS]		[]		1	10	0	hiuptm	split-fold-cots
7219	7219	Great-Barrier-reef	This is tfrecords dataset	[]		0	24	0	khukir	greatbarrierreef
7220	7220	bankfull		[]		0	4	0	revannathjondhale	bankfull
7221	7221	Houses Dataset		[]		4	35	0	sirakr	houses-dataset
7222	7222	HoChiMinhUS_Embass_PM2.5_Concentration	PM2.5 Concentration from Ho Chi Minh city US Embassador	['atmospheric science']		0	27	0	phnamnc	hochiminhus-embass-pm25-concentration
7223	7223	Car scratch dataset		['automobiles and vehicles']		10	71	0	samyukthamobile	car-scratch-dataset
7224	7224	YoloX Trained Great Barrier Dataset 40 Epochs		['earth and nature']		0	28	0	owaiskhan9654	yolox-trained-great-barrier-dataset-40-epochs
7225	7225	cy-dcl-3		[]		0	6	1	chenyan2021	cydcl3
7226	7226	outpic-data		[]		0	21	0	chenyang525	outpicdata
7227	7227	GoogleandHuman voice		[]		1	28	0	secondname2	googleandhuman-voice
7228	7228	Telecom Churn Data	Telecom Churn Case Study	['business']	"In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.
To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.
In this Telecom churn case study we are using Pareto principle which says that approximately 80% of revenue comes from the top 20% customers called high-value customers.
Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc.
Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.
The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.
The business objective is to predict the churn in the last (i.e. the ninth) month using the data from the first three months. In churn prediction, we assume that there are three phases of customer lifecycle.
The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.
The ‘action’ phase: The customer experience starts to sore in this phase.
The ‘churn’ phase: In this phase, the customer is said to have churned.
We are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase
We have to build a model to predict the churn for our high value customers in future and by knowing this, the company can take action steps such as providing special plans, discounts on recharge etc. It will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks."	11	143	0	hritikesinghrawat	telecom-churn-data
7229	7229	SKF_TFRecords_20_bins_dogs_cats_labels		[]		0	1	0	saiyan1202	skf-tfrecords-20-bins-dogs-cats-labels
7230	7230	public-r-textbook		['education']		0	36	0	ssd23331	publicrtextbook
7231	7231	Bike_sharing_company_3796	Summary of a fictional bike sharing company capstone project 2021	['automobiles and vehicles', 'cycling', 'beginner', 'data cleaning', 'data analytics', 'tabular data']	"Context
Summary of attributes of a fictional Bike sharing company for the year 2021.
Content
The spreadsheet contains data about different types of rides such as electric ,classic and docked preferred by customers(casual/annual) .It also gives detailed information about average ride time for customers and most active day of the week throughout the entire year of 2021"	0	25	0	siddheshgothankar	bike-sharing-company-3796
7232	7232	Reddit Dad Jokes	r/dadjokes subreddit posts	['nlp', 'text data']	"Context
Dad Jokes (r/dadjokes), is a subreddit where people share jokes.
Content
The dataset contains four columns which are;
author: The username of the person who made the joke
url: The Post/Joke's URL
joke: The joke
score: As a scoring result of Reddit's number of upvotes minus the number of downvotes
date: The date of the post
Inspiration
Can you find which joke is good or bad?"	31	452	6	oktayozturk010	reddit-dad-jokes
7233	7233	Perks of Being a Habitant of a Metropolis		[]		0	65	0	waqaasahmad	perks-of-being-a-habitant-of-a-metropolis
7234	7234	Houses You Can Buy in Karachi in a Budget		[]		0	52	0	waqaasahmad	houses-you-can-buy-in-karachi-in-a-budget
7235	7235	6 Reasons to Consult Estate Agent		[]		0	62	0	waqaasahmad	6-reasons-to-consult-estate-agent
7236	7236	6 Easy Steps for the Shifting of Your Home		[]		0	58	0	waqaasahmad	6-easy-steps-for-the-shifting-of-your-home
7237	7237	output_file		[]		0	9	0	tylersanders	output-file
7238	7238	checkpoint		['law']		2	77	0	anhthemduocngu	checkpoint
7239	7239	starfish_dataset		[]		12	62	0	kevin1742064161	starfish-dataset
7240	7240	Home Credit Team8 Dataset		['lending']		0	59	0	chnhgr	home-credit-team8-dataset
7241	7241	img_test111		[]		0	15	0	zhufuchunshui	img-test111
7242	7242	PWP ViT		[]		0	4	0	aishikai	pwp-vit
7243	7243	UKDALE-VAE		[]		0	41	0	benbangbang	ukdalevae
7244	7244	NFL_Special_Play_Legal_Plays_Only		[]		0	21	0	kushtrivedi14728	nfl-special-play-legal-plays-only
7245	7245	PFM-LTM		['music']		0	27	0	ssinyu	pfmltm
7246	7246	yolov5.txt		[]		0	19	0	jagadeeshmarusani	yolov5txt
7247	7247	transformers-2022-01		['movies and tv shows']		0	33	0	a24998667	transformers-2022-01
7248	7248	final_matrix_300		[]		0	21	0	vickyzhen	final-matrix-300
7249	7249	BaseAugTortugas01		[]		0	9	0	miguelespinozac	baseaugtortugas01
7250	7250	Extract text from image		['software']		0	10	0	shiwenyu	extract-text-from-image
7251	7251	outpic-file		[]		0	11	0	chenyang525	outpicfile
7252	7252	coral_yolox_fold5		[]		2	34	0	garvitgarg	coral-yolox-fold5
7253	7253	P_Pr_k=10Swin224		[]		0	26	0	durgammohanpranay	p-pr-k10swin224
7254	7254	picture_invoice		[]		0	13	0	chrisgrove	picture-invoice
7255	7255	pwscore		[]		2	74	2	sakshamdwivedi10	pwscore
7256	7256	COBRAFixation	COBRA reduce  respiratory movement of the lung or liver tumor.	['health conditions']	"Context
Respiratory movement is a tough matter for the radiation therapy of the lung and liver.  Abdominal compression is one of the procedures to reduce the movement. We used COBRA to compress the abdomen. COBRA has two hemispherical heads, these show like fangs of snake cobra. 
   Is it useful or not ? 
Content
Five volunteers (['Person']; H, K, S, T,W) joined the study in COBRAw study (DATAC include Six volunteers ,'W', 'O', 'Fu', 'w', 'f' and 's'.). COBRA can be used under magnetic field, so measurements were performed using MRI without exposure to radiation. 
 Five edges of the liver (['Part']; C-M, C-R, S-A, S-M, S-P ) were selected to measure the length of movement along body axis. Uppermost minus undermost length [mm] was stored in column ['Value'].
 Status in COBRAw were categorized as F (free; no compression as control), C (compressed by COBRA), M ( with music for beating the time of respiration) .
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	3	278	1	hisatonagano	cobrafixation
7257	7257	Real estate prices in Tashkent, Uzbekistan	Data scraped from uybor.uz	['cities and urban areas', 'real estate', 'data visualization', 'data analytics', 'tabular data']	"Context
The dataset containt the prices for real estate in Tashkent, Uzbekistan. Data was scraped from uybor.uz, real-estate advertisement website. Data was scraped back in 2019.
Content
The dataset contains following columns:
address - approximate address of the real-estate
district - the district the real-estate located in
rooms - number of rooms
size - total size of the unit in square meters
level - which level the unit located at
max_levels - maximum levels of the building
price - price in USD
lat - latitude
lng - longitude"	27	186	1	anvarnarz	tashkent-real-estate-2019
7258	7258	petfinder_exp108		[]		0	9	0	ktakita	petfinder-exp108
7259	7259	new4_pretrain		[]		0	13	0	lsyluestc	new4-pretrain
7260	7260	model_roberta_large_pretrain		[]		0	35	0	qinyukun	model-roberta-large-pretrain
7261	7261	students performance on 2019 SARESP		['education']		18	60	0	andrmiller	students-performance-on-2019-saresp
7262	7262	picture-file		[]		0	38	0	chenyang525	picturefile
7263	7263	mlm-corpus		[]		0	2	0	a24998667	mlm-corpus
7264	7264	LXXX01		[]		0	22	0	ding314	lxxx01
7265	7265	LXXX01		[]		0	1	0	sufeng314	lxxx01
7266	7266	PAW-swin-base-patch4-window12-384-xf-seed5		[]		0	2	0	a24998667	swin-base-patch4-window12-384-xf-seed5
7267	7267	Sonar data		[]		0	2	0	nareshkoppisetti	sonar-data
7268	7268	OWMmod	Lian xu xue xi moxing，OWM  moxing	['arts and entertainment']		0	27	0	ding314	owmmod
7269	7269	picture file		['arts and entertainment']		0	3	0	chenyang525	picture-file
7270	7270	Suwonuniversity_dataset		[]		0	101	0	woohanyoung	suwonuniversity-dataset
7271	7271	Image graying processing-a bird	using the  weighted mean method to do graying process of a bird	['biology']		0	38	0	shiwenyu	image-edge-processinga-bird
7272	7272	model_deberta_v3_base_1		[]		0	12	0	qinyukun	model-deberta-v3-base-1
7273	7273	NFL BDB Model Data		[]		17	124	0	rykermoreau	nfl-bdb-model-data
7274	7274	hydropower-cost		['renewable energy']		0	4	0	johnnyt001	hydropowercost
7275	7275	PetFinder-Z1411-20220105001414		[]		0	18	0	hideyukizushi	petfinder-z1411-20220105001414
7276	7276	NFL2022_Input_Data		[]		0	19	0	solenodon	nfl2022-input-data
7277	7277	xgb-model		['clothing and accessories']		0	8	0	honihitak	xgbmodel
7278	7278	tackle_model	preds from tackle model	['sports']		1	23	0	sportsstatseli	tackle-model
7279	7279	pf_st_latest		[]		0	7	0	pablolarrosa	pf-st-latest
7280	7280	Cinc2020Bandpassf		[]		0	4	0	evanwen	cinc2020bandpassf
7281	7281	datasets_for_ethans_tableau_course		[]		2	18	1	madhuryadav	datasets-for-ethans-tableau-course
7282	7282	Dataset for ecigarette sentiments 2021		[]		0	19	1	monisnazar	dataset-for-ecigarette-sentiments-2021
7283	7283	Divvy-dataset-12_2020-11_2021		['earth and nature']		1	24	0	lilithayrapetyan	divvydataset12-202011-2021
7284	7284	EEG_Data		[]		0	32	0	ahmedmohamed1234277	eeg-data
7285	7285	UAE CAR Used Dataset	This is Used Car Dataset which Has been Fetched using Web Scraping Technique	['research', 'categorical data', 'intermediate', 'python', 'middle east']	"The United Arab Emirates Used Car Dataset
This dataset gives information about used cars in the United Arab Emirates. This Data Set has been Collected from web Scraping Techniques Using Beautiful Soup and Python Selenium.
I would like to thank my superiors and especially Mohd Naif who helped me a lot to collect this Dataset🤝 🤝 
If you would have any questions please raise them in Discussion Section. I would be delighted to Help.🙌 🙌 🙌"	9	76	0	owaiskhan9654	uae-car-used-dataset
7286	7286	california_housing		[]		1	3	0	saraferguswps	california-housing
7287	7287	PWP CAiT		[]		0	7	0	aishikai	pwp-cait
7288	7288	cornell_movie_dialogs.zip'		[]		0	15	0	sahrishaltafkhan	cornell-movie-dialogszip
7289	7289	airbnb_listings		[]		2	13	0	christosmitsianis	airbnb-listings
7290	7290	aoa_bert_5epochs_cdm_2e6_models		[]		0	9	0	enkrish259	aoa-bert-5epochs-cdm-2e6-models
7291	7291	picture1		[]		0	17	1	muhammadammarjamshed	picture1
7292	7292	picture		[]		0	20	1	muhammadammarjamshed	picture
7293	7293	PWP XCiT		[]		0	3	0	aishikai	pwp-xcit
7294	7294	North calling center project		['internet']		0	37	0	chideraodumegwu	north-calling-center-project
7295	7295	Chemicals in cosmetics	how safe are chemicals in our cosmetics?	[]		4	52	0	chideraodumegwu	chemicals-in-cosmetics
7296	7296	testcorr2_up		[]		1	12	0	mrparamatma	testcorr2-up
7297	7297	Cat & Dog images for Classification	Cat and Dogs images with proper labeling for classification problems.	['animals', 'beginner', 'classification', 'cnn', 'image data']		98	585	13	ashfakyeafi	cat-dog-images-for-classification
7298	7298	Pawpularity-swin-new-exp1		[]		0	5	0	shigeria	pawpularity-swin-new-exp1
7299	7299	plexp4oof1757f5		[]		0	4	1	nischaydnk	plexp4oof1757f5
7300	7300	MIND-large	Dataset for MIND's News Recommendation Competition	[]		15	107	0	hieunm21	mindlarge
7301	7301	New Year’s Resolutions in 2021 and 2022	Responses from a public survey	['survey analysis', 'holidays and cultural events']	"Context
This data was gathered with a public survey asking respondents questions about their New Year's resolutions (if any) in 2021 and 2022.
Content
Each row represents one response to the survey. Each column represents one question, with more details given in the column description on Kaggle.
Acknowledgements
A special thanks to the following people:
- Every single person who has responded to the survey with valid answers for making this entire dataset possible"	271	2111	14	bsoyka3	new-years-resolutions-in-2021-and-2022
7302	7302	Base_Tortugas_20220104		[]		0	12	0	miguelespinozac	base-tortugas-20220104
7303	7303	COVID-19 vaccines list countrywise	List of vaccines administered in the country up to the current date.	['healthcare', 'public health', 'health', 'science and technology', 'medicine', 'drugs and medications', 'public safety']	"Context
Dataset from Johns Hopkins University which maintains various datasets related to COVID 19 pandemic
Content
Data set  column details
location: name of the country (or region within a country).
iso_code: ISO 3166-1 alpha-3 – three-letter country codes.
vaccines: list of vaccines administered in the country up to the current date.
last_observation_date: date of the last observation in our data.
source_name: name of our source for data collection.
source_website: web location of our source. It can be a standard URL if numbers are consistently reported on a given page; otherwise, it will be the source for the last data point.
Acknowledgements
Mathieu, E., Ritchie, H., Ortiz-Ospina, E. et al. A global database of COVID-19 vaccinations. Nat Hum Behav (2021). https://doi.org/10.1038/s41562-021-01122-8
Inspiration
Visualize and See which all vaccines are available in which all countries. You can start with this notebook.
Licence
Creative Commons BY license"	205	1847	28	sandhyakrishnan02	covid19-vaccines-list-countrywise
7304	7304	aoa_bert_10_epochs_cdw_2e6_nobatch_models		[]		0	17	0	enkrish259	aoa-bert-10-epochs-cdw-2e6-nobatch-models
7305	7305	boston geojson		[]		0	11	0	xujiang1993	boston-geojson
7306	7306	GoEmotions	GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit	['exploratory data analysis', 'nlp', 'text mining', 'statistical analysis', 'text data']	"Context
GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral.
Content
Number of examples: 58,009.
Number of labels: 27 + Neutral.
Maximum sequence length in training and evaluation datasets: 30.
On top of the raw data, we also include a version filtered based on reter-agreement, which contains a train/test/validation split:
Size of training dataset: 43,410.
Size of test dataset: 5,427.
Size of validation dataset: 5,426.
The emotion categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise.
For more details on the design and content of the dataset, please see our paper.
.
Acknowledgements
Inspiration
Multi Classification of emotions"	88	1318	12	mathurinache	goemotions
7307	7307	BrazilianCars		[]		8	84	0	alejandrash	braziliancars
7308	7308	mp2-q2-data21		[]		0	2	0	miladaf	mp2q2data21
7309	7309	traindata		[]		1	7	0	stevenzhou90	traindata
7310	7310	testdata		['business']		0	16	0	stevenzhou90	testdata
7311	7311	word2vec-glove.6B.300d		['retail and shopping']		4	21	4	serquet	word2vecglove6b300d
7312	7312	NJ TGES Summary 2000-2018		['literature']		2	6	0	timvitale	nj-tges-summary-20002018
7313	7313	lhamatest		[]		2	35	0	mauriciominguini	lhamatest
7314	7314	224lcrp		[]		0	18	1	datafan07	224lcrp
7315	7315	ICPC World Final Ranking of Last 10 Years!	ICPC World Final Ranking of 50/year of Last 10 Years!	['science and technology', 'computer science', 'programming']	"The Dataset has information of ICPC world final's top team of last 10 years. 
Total Year - 10
Total year consists - Top 50 team
Total Team = 500
What is ICPC?
The International Collegiate Programming Contest is an algorithmic programming contest for college students. Teams of three, representing their university, work to solve the most real-world problems, fostering collaboration, creativity, innovation, and the ability to perform under pressure. Through training and competition, teams challenge each other to raise the bar on the possible. Quite simply, it is the oldest, largest, and most prestigious programming contest in the world.
[Source - https://icpc.global/ ]
What can we do using this Dataset?
~We will able to predict the next champion and top most team
~By analysis , we can predict success rate , rate of problem solving etc."	63	508	11	mdjafrilalamshihab	icpc-world-final-ranking-of-last-10-years
7316	7316	Cyclistic_bike_share_Case_Study_Dec2020_Nov2021	Google DA Professional Certificate's public data	['intermediate', 'tabular data']		1	28	0	shivasinghgogreen	cyclistic-bike-share-case-study-dec2020-nov2021
7317	7317	solar_radiation	dataset showing radiation caused by sun	['earth and nature', 'environment', 'intermediate', 'regression']		214	1525	18	ishaanthareja007	solar-radiation
7318	7318	Pawpularity-swin-new-exp0		[]		1	5	0	shigeria	pawpularity-swin-new-exp0
7319	7319	Latest Covid-19 India Status		[]		0	31	0	abhishekraj2002	latest-covid19-india-status
7320	7320	Military Aircraft Detection Dataset	military aircraft images with aircraft type and bounding box annotations	['military', 'engineering', 'computer vision', 'image data']	"bounding box in PASCAL VOC format (xmin, ymin, xmax, ymax)
36 aircraft types
(A10, A400M, AG600, B1, B2, B52 Be200, C130, C17, C5, E2, EF2000, F117, F14, F15, F16, F18, F22, F35, F4, J20, JAS39, MQ9, Mig31, Mirage2000, RQ4, Rafale, SR71(may contain A12), Su57, Tu160, Tu95(may contain Tu142), U2, US2, V22, XB70, YF23)"	1116	11702	56	a2015003713	militaryaircraftdetectiondataset
7321	7321	yolov5_swin_transformer_trained		[]		4	23	0	yamqwe	yolov5-swin-transformer-trained
7322	7322	class2		[]		0	2	0	aybukepolat	class2
7323	7323	Brain Tumor Classification(MRI)		[]		2	35	0	yashuyaswanth	brain-tumor-classificationmri
7324	7324	Web Analytics		[]		1	48	0	malvikachauhan	web-analytics
7325	7325	Alzheimer MRI 4 classes dataset	"Copy of ""https://www.kaggle.com/tourist55/alzheimers-dataset-4-class-of-images"""	[]	"Context
This dataset is a copy of the images in the dataset at the link: Alzheimer's Dataset (4 class of Images).
Content
The original dataset contained MRI images of 32 horizontal slices of the brain divided into 4 classes:
- Mild Dementia
- Moderate Dementia
- Non Dementia
- Very Mild Dementia
For each classes there were a different number of subjects:
- 28 subjects for the Mild Dementia Class
- 2 subjects for the Moderate Dementia Class
 - 100 subjects for the Non Dementia Class
-  70 subjects for the Very Mild Dementia Class
The problem of the original dataset was that the train and the test sets contained different slices of the brain because the images of the dataset were ordered by the position of the slice and the train/test set division was performed by putting the first percentage of images in the train set and the last ones in the test set.
In this dataset the original train and test set have been united and the images have been divided between train, test and validation set randomly."	18	154	1	marcopinamonti	alzheimer-mri-4-classes-dataset
7326	7326	tedcleaned.en.test.transformer.step50000.en2nl		[]		0	8	0	alisonpeten	tedcleanedentesttransformerstep50000en2nl
7327	7327	tedcleaned.nobpe.en.test.BPE.step50000.en2nl		[]		0	15	0	alisonpeten	tedcleanednobpeentestbpestep50000en2nl
7328	7328	Accurate drone shapes/segmentation	Natural variance and original shapes	['image data']	"The data set was produced by MetaVision team using original content.
Several segmentation techniques were used to generate accurate masks from the original video streams, preserving natural variance and original shapes.
This set is just a ""building block"". Feel free to augment it or combine it with other&nbsp;sets."	52	822	9	metavision	accurate-drone-shapessegmentation
7329	7329	iris-dataset-LR		[]		0	5	0	kapilbhatt11	irisdatasetlr
7330	7330	teamlogos		[]		1	4	0	trevorhughes	teamlogos
7331	7331	Beverage (Drinking) in Vietnam		[]		0	27	0	phamtrongliem	beverage-drinking-in-vietnam
7332	7332	The Ultimate Bulgarian NLP Dataset	928,757,769 Charecters of Bulgarian	['literature', 'psychology', 'nlp', 'text mining', 'lstm', 'text data']	"Context
I wanted to build the best Bulgarian text completion model, so I made the dataset.
Content
The dataset contains 13 files with a total size of 1.67 GB that is filtered on this alphabet, which consists of 103 characters:
АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЬЮЯабвгдежзийклмнопрстуфхцчшщъьюя1234567890!@#$%^&amp;*()-_=+`~[]{}|;':""\,./&amp;lt;&amp;gt;
Chitanka.info works
All the works in my chitanka.info dataset are line-by-line filtered and put into 8 separate chunks (stored in chitanka_info chunks) with lengths of 100,000,000 characters except for the last one with a length of 27,843,817 characters for a total length of 727,843,817 characters or 1.22 GB.
Forumnauka
Scraped paragraphs off the forum ""forumnauka"" above 50 characters. Stored in forumnauka_sentences.txt.
* 80,953 Paragraphs
* 18,378,783 Characters or 33.14 MB
Kaldata
Scraped paragraphs off the forum ""Kaldata"" above 50 characters. Stored in kaldata_sentences.txt.
* 8,606 Paragraphs
* 1,346,927 Characters or 2.41 MB
News sentences
Filtered sentences off the Leipzig Corpora collection. Stored in news_sentences.txt.
* 905,700 Sentences
* 83,644,905 Characters or 151.73 MB
Wikipedia sentences
Filtered sentences off the Leipzig Corpora collection. Stored in wikipedia_sentences.txt.
* 685,793 Sentences
* 66,903,292 Characters or 121.57 MB
Wikipedia paragraphs
Filtered scraped paragraphs from Wikipedia. Stored in wikipedia_paragraphs.txt.
* 77,762 Paragraphs
* 30,640,045 Characters or 55.24 MB
Acknowledgements
Chitanka for the files in the folder  chitanka_info chunks
Forumnauka for the paragraphs in forumnauka_sentences.txt
Kaldata for the paragraphs in kaldata_sentences.txt
Leipzig Corpora collection for the sentences in news_sentences.txt and wikipedia_sentences.txt
Wikipedia for the sentences in wikipedia_paragraphs.txt
Used Beautiful Soup 4 and Requests for scraping.
Licences
Leipzig Corpora collection - https://wortschatz.uni-leipzig.de/en/usage
Wikipedia - (Creative commons (CC BY-SA 3.0))[https://creativecommons.org/licenses/by-sa/3.0/deed.bg]
<hr>

Контекст
Исках да създам най-добрия български искуствен интелект за завършване на текст, затова изградих набора от данни.
Съдържание
Наборът от данни съдържа 13 файла с общ размер от 1,67 GB, които се филтрират по тази азбука, която се състои от 103 букви:
АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЬЮЯабвгдежзийклмнопрстуфхцчшщъьюя1234567890!@#$%^&amp;*()-_=+`~[]{}|;':""\,./&amp;lt;&amp;gt;
Chitanka.info произведения
Всички произведения в моя набор от данни за chitanka.info се филтрират ред по ред и се поставят в 8 отделни парчета (съхранени в chitanka_info chunks) с дължини от 100,000,000 букви с изключение на последния с дължина 27,843,817 букви за обща дължина от 727,843,817 букви или 1,22 GB.
Форумнаука
Абзаци от форум ""forumnauka"" над 50 букви. Съхранява се във forumnauka_sentences.txt.
* 80,953 параграфа
* 18,378,783 букви или 33,14 MB
Калдата
Абзаци от форума „Kaldata“ над 50 букви. Съхранява се в kaldata_sentences.txt.
* 8,606 параграфа
* 1,346,927 букви или 2,41 MB
Изречения за новини
Филтрирани изречения от колекция Leipzig Corpora. Съхранява се в news_sentences.txt.
* 905,700 изречения
* 83,644,905 букви или 151,73 MB
Уикипедия изречения
Филтрирани изречения от колекция Leipzig Corpora. Съхранява се в wikipedia_sentences.txt.
* 685,793 изречения
* 66,903,292 букви или 121,57 MB
Абзаци от Уикипедия
Филтрирани абзаци от Уикипедия. Съхранява се в wikipedia_paragraphs.txt.
* 77,762 параграфа
* 30,640,045 букви или 55,24 MB
Източници
Chitanka за файловете в папката chitanka_info chunks
Форумнаука за абзаците в forumnauka_sentences.txt
Kaldata за абзаците в kaldata_sentences.txt
Колекция Leipzig Corpora за изреченията в news_sentences.txt и wikipedia_sentences.txt
Уикипедия за изреченията в wikipedia_paragraphs.txt
Използвани Beautiful Soup 4 и Requests за вземане на информация онлайн.
Лицензи
Колекция Leipzig Corpora - https://wortschatz.uni-leipzig.de/en/usage
Уикипедия - Creative Commons (CC BY-SA 3.0)"	7	94	1	nikitricky	the-ultimate-bulgarian-nlp-dataset
7333	7333	Trabalho_Visao_Leucemia		[]		33	3	0	lcscunha	trabalho-visao-leucemia
7334	7334	Complete Works of Rabindranath Tagore	Collection of 3438 literary items from all genres	['languages', 'arts and entertainment', 'literature', 'nlp', 'text data']	"Context
Rabindranath Thakur, (born May 7, 1861, Calcutta [now Kolkata], India—died August 7, 1941, Calcutta), Bengali poet, short-story writer, song composer, playwright, essayist, and painter who introduced new prose and verse forms and the use of colloquial language into Bengali literature, thereby freeing it from traditional models based on classical Sanskrit. He was highly influential in introducing Indian culture to the West and vice versa, and he is generally regarded as the outstanding creative artist of early 20th-century India. In 1913 he became the first non-European to receive the Nobel Prize for Literature. 
[Source: https://www.britannica.com/biography/Rabindranath-Tagore]
Content
This dataset includes 8 csv files under /csv directory and 7 txt files under /txt directory containing the complete works of Rabindranath Tagore. Each csv file includes all the literary works found in that genre in csv format, except all_collection.csv (which combines works from all genres). Each txt file includes all the literary works found in that genre in aggregated txt format. The content in both formats are passed through a basic preprocessing step to clear out empty spaces, in-page titles and page numbers. The txt formats would be suitable for training various sequential models, whereas csv formats would be useful for literary analyses and comparative studies.
Files:
1. ""all_collection.csv"": Combination of all other files. This contains 3438 individual items from all genres.
2. ""drama.csv"": Contains all the works in the ""Drama"" genre.
3. ""essay.csv"": Contains all the works in the ""Essay"" genre.
4. ""misc.csv"": Contains all the miscellaneous works.
5. ""novel.csv"": Contains all the works in the ""Novel"" genre.
6. ""poem.csv"": Contains all the works in the ""Poetry"" genre.
7. ""song.csv"": Contains all the works in the ""Song"" genre.
8. ""story.csv"": Contains all the works in the ""Story"" genre.
9. ""drama.txt"": Contains aggregated works in the ""Drama"" genre.
10. ""essay.txt"": Contains aggregated works in the ""Essay"" genre.
11. ""misc.txt"": Contains aggregated miscellaneous works.
12. ""novel.txt"": Contains aggregated works in the ""Novel"" genre.
13. ""poem.txt"": Contains aggregated works in the ""Poetry"" genre.
14. ""song.txt"": Contains aggregated works in the ""Song"" genre.
15. ""story.txt"": Contains aggregated works in the ""Story"" genre.
Acknowledgements
I scraped the entire body of works of Rabindranath Tagore from the amazing source published by the ""Department of Information Technology & Electronics, Government of West Bengal, India"", https://rabindra-rachanabali.nltr.org
All credit goes to them for their meticulous attempt in preparing this quality collection of works and publishing them online for others to benefit from.
Inspiration
While trying to work on a project where I wanted to generate text in the style of Rabindranath Tagore, a great luminary of Bengali literature and conduct statistical analysis on various themes found in the works of Rabindranath, I could not find a comprehensive dataset that provided granular access to individual items in every genre, e.g poem, story, essay, novel or drama. Hence, I decided to compile this dataset to do just that.
I hope this will provide aspirations to machine learning enthusiasts and general literary experts alike to delve deep into the bottomless ocean of Bengali literature and explore the hidden gems unlocked by the advancement of multitude of techniques in the field of machine learning.
Banner image source: https://upload.wikimedia.org/wikipedia/commons/d/d1/Rabindranath_Tagore.jpg"	16	328	9	aagalib	complete-works-of-rabindranath-tagore
7335	7335	GPR1200 Dataset	GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval 	['arts and entertainment', 'computer vision', 'classification', 'clustering', 'image data']	"Context
GPR1200: A Benchmark for General-Purpose Content-Based Image Retrieval (ArXiv)
Content
Similar to most vision related tasks, deep learning models have taken over in the field of content-based image retrieval (CBIR) over the course of the last decade. However, most publications that aim to optimise neural networks for CBIR, train and test their models on domain specific datasets. It is therefore unclear, if those networks can be used as a general-purpose image feature extractor. After analyzing popular image retrieval test sets we decided to manually curate GPR1200, an easy to use and accessible but challenging benchmark dataset with 1200 categories and 10 class examples. Classes and images were manually selected from six publicly available datasets of different image areas, ensuring high class diversity and clean class boundaries.
Acknowledgements
Inspiration
Benchmark your Image Retrieval Models on It"	34	614	13	mathurinache	gpr1200-dataset
7336	7336	retinanetweights		[]		1	56	0	bangliliu	retinanetweights
7337	7337	face_mask_detector-with_without_incorrect_mask	face mask detection dataset	['computer vision', 'deep learning', 'neural networks', 'image data', 'multiclass classification']	"this dataset used for face mask detection and contains more than 10k images.
it split into 3 directories (train, validation and test)
each containes 3 subdirectories (incorrect_mask, with_mask, without_mask)."	72	312	1	rjouba	dataset
7338	7338	results		['standardized testing']		0	5	0	pngwenhan	results
7339	7339	Hull Data		[]		1	19	0	bkumagai	hull-data
7340	7340	longformer-baseline-pb-cfg-f3-cv-0.6361		['video games']		0	5	0	atharvaingle	longformer-baseline-pb-cfg-f3
7341	7341	head-ct		[]		0	21	0	lucaschmidtsantiago	headct
7342	7342	tedcleaned.en.test.step30000.en2nl		[]		0	10	0	alisonpeten	tedcleanedenteststep30000en2nl
7343	7343	tedcleaned.en.test.step35000.en2nl		[]		0	8	0	alisonpeten	tedcleanedenteststep35000en2nl
7344	7344	PAW-vit-large-patch16-384-seed2		[]		0	9	0	a24998667	paw-vit-large-patch16-384-seed2
7345	7345	Swin Transformer FasterRCNN Saved Model		[]		1	36	0	mlneo07	swin-transformer-fasterrcnn-saved-model
7346	7346	taipei traffic		[]		0	23	0	richard0218	taipei-traffic
7347	7347	Human Life Expectancy Around the World	country wise life expectancy of humans (historical data)	['arts and entertainment', 'social science', 'demographics', 'beginner', 'exploratory data analysis', 'tabular data']	"Context
The human lifespan is the maximum number of years an individual from the human species can live based on observed examples. 
Fact
The longest verified lifespan for any human is that of Frenchwoman Jeanne Calment, who is verified as having lived to age 122 years.
Acknowledgements
Dataset source"	1146	7410	40	deepcontractor	human-life-expectancy-around-the-world
7348	7348	DrinkingImageDemo		[]		0	25	0	phamtrongliem	drinkingimagedemo
7349	7349	scores400		[]		1	5	1	jeongtw	scores400
7350	7350	Turkish Banknote Dataset		['currencies and foreign exchange']		0	28	0	volkanonder	turkish-banknote-dataset
7351	7351	train_qa.txt		[]		0	2	0	shivamchaudhary11	train-qatxt
7352	7352	Apple product price list 2022 in 26 countries	How much a buyer pay for Apple product in each country	['business', 'software', 'intermediate', 'exploratory data analysis', 'currencies and foreign exchange']	"Context
Wanna know how much person in each country pay for Apple products.
Acknowledgements
Data is scraped from Apple gifts page (https://www.apple.com/shop/gifts) on 2-Jan-2022.
Inspiration
Learn how to clean price data.
Learn how to pivot data to easily compare prices across countries"	882	5518	35	prasertk	apple-product-price-list-from-26-countries-2022
7353	7353	new_1pretrain		[]		0	18	0	lsyluestc	new-1pretrain
7354	7354	thesaurus files		[]		0	14	0	prashitajain	thesaurus-files
7355	7355	deep_sharev1		[]		0	27	0	liandanshisunfine	deep-sharev1
7356	7356	Numbers Classification Dataset	Self-generated numbers with different backgrounds and positions.	['beginner', 'classification', 'cnn', 'image data']	"Context
Collection of Numbers Images generated as Base64 image combination. Used as a captcha to find 2 of the same numbers.
Content
The dataset contains the specific numbers in a specific folder. The size is 110x110 pixels."	7	108	1	ox0000dead	numbers-classification-dataset
7357	7357	taiwancancerdata		[]		0	4	0	szuchiali2020	taiwancancerdata
7358	7358	PAW-beit-base-patch16-384-seed5		[]		0	16	0	a24998667	paw-beit-base-patch16-384-seed5
7359	7359	Innovation Analysis In Textile Companies 		['research', 'education']		8	43	1	alizulfi	innovation-analysis-in-textile-companies
7360	7360	fruitbatdataset		[]		69	24	0	akoluacik	fruitbatdataset
7361	7361	datasets for natural language processing	6 processed and in different context public datasets for nlp tasks	['sampling', 'earth and nature', 'social science', 'nlp', 'classification', 'text data']	"Introduction
Datasets below are already published as public.  projectSOTS brought some of these datasets together as team to use for nlp optimization, text augmentation and so on. Moreover, they are mostly changed for purpose. Datasets have quite different context from each other in terms of text structure and labeling. 
Content
Datasets have binary notation except ctweet dataset (it has 3 labels). Food, sarcasm and stweet are pre-processed by making lower case and removing punctuations, hashtags, usernames and html tags. Reddit, ctweet and toxic datasets are still require pre-processing, so that is up to you. Their sentimental perspective and data structure are quite different. All binary datasets are labelled as 0 and 1, and ctweet has 0,1 and 2 under 'Y' column.
""ctweet, stweet, food"" datasets are positive or negative analysis (sentiment) -&gt; 0 negative -&gt; 1 positive  (ctweet has neutral 0, 1, 2) 
In ""reddit and sarcasm"" we ask for is there sarcasm or irony in the sentence?  (sarcasm) -&gt; 0 no -&gt; 1 yes
In ""toxic"" dataset we have sentences to detect is there insulting? -&gt; 0 no -&gt; 1 yes
stweet and reddit are a million+ sentences datasets.
food and toxic are medium size datasets.
sarcasm and ctweet small datasets.
Inspiration
These different contextual datasets can be used for sentiment analysis, text classification, text optimization and so on."	24	537	6	toygarr	datasets-for-natural-language-processing
7362	7362	model_weights		[]		0	14	0	jiantengfei	model-weights
7363	7363	AutoGluon Sentimental MODEL	AutoGluon NLP Prediction 	['arts and entertainment', 'nlp', 'text data']		0	133	1	rhythmcam	autogluon-sentimental-model
7364	7364	imported data		['intermediate', 'exploratory data analysis', 'nlp', 'classification']		0	44	4	rounak02	imported-data
7365	7365	lyon housing	price of real estate in the french city of Lyon	['europe', 'real estate', 'economics', 'tabular data', 'regression']	"Context
I built this dataset during my researches to buy my first home. It contains the price of real-estate transactions between 2017 and 2021 in Lyon and Villeubanne. It was put together using the ""Demande de Valeur Foncière géolocalisé"", and the ""Document de Filiation Informatisé"" datasets. Both are open datasets provided by the french government. The DVF dataset is an history of geolocalised real-estate sales which I have cleaned and reformatted here. Evictions and trade were filtered out as well as transactions of multiple real-estate goods at once. The dataset was then subsequently cleaned of extreme data. The DFI dataset is a creation history of land plots, which I used to infer the construction date of each good (assuming it is the date of the land plot creation + 2 years). Land plots that are too old are not present in the DFI dataset, and the threshold date of 1950-01-01 was used as a filling value. The intended use is for predicting the price of real-estate goods. I also included the GPS coordinates of each tram and metro station in a json file for additional feature construction.
This dataset is provided under MIT licence
Licence
Copyright 2021 Benoit Favier
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."	18	146	3	benoitfavier	lyon-housing
7366	7366	newdata_6726_3dataset(812)		[]		0	15	0	mustai	newdata-6726-3dataset812
7367	7367	mobilenet		[]		0	8	0	jenachung	mobilenet
7368	7368	 NYU Acceptance Rate and Statistics		['education']		1	17	0	david90johnson	nyu-acceptance-rate-and-statistics
7369	7369	itwillwork		[]		0	3	0	fcbfcb1998	itwillwork
7370	7370	Football players faces dataset	A cropped dataset for face recognition(machine learning) models 	['football']	"Context
This dataset was specifically collected for an ai project where we opted for face recognition model as our project.
Content
This is a small dataset for experimenting machine learning techniques. It has a training directory containing around 300 photos for each of the 8 football players
Code
https://github.com/mohamedaleya/data-analytics-project/blob/main/FIFA_Face_Recognition.ipynb
Acknowledgements
Contributors: 
https://www.kaggle.com/alaariahi
https://www.kaggle.com/mohamedaleya
https://www.kaggle.com/azizbali"	18	149	0	azizbali	football-players-faces-dataset
7371	7371	pet_models		[]		0	147	0	yoshito	pet-models
7372	7372	COVID-19 (coronavirus) dataset by OWID	Comprehensive Dataset from Our World in Data	['healthcare', 'public health', 'health', 'health conditions', 'covid19']	"This dataset is a set of select datasets pertaining to COVID-19 including COVID-19 statistics, vaccination, variation and testing 
This is a Work in Progress"	441	2676	23	kalilurrahman	covid19-coronavirus-dataset-by-owid
7373	7373	Imported Data		[]		0	22	3	hetvigandhi03	imported-data
7374	7374	EZINNE PYTHON PROJECT 	python project carried out in Chun analysis and Machine learning	['computer science', 'programming']		2	17	0	ejehuezinne	ezinne-python-project
7375	7375	Medical Entity Recognition		['health']		0	20	2	yasserh	medical-entity-recognition
7376	7376	tata_data		[]		0	7	0	awaisfarooq1519	tata-data
7377	7377	model_upload		[]		0	12	0	jenachung	model-upload
7378	7378	TF_LONGFORMER_V0		[]		0	43	0	svenspa	tf-longformer-v0
7379	7379	Google Data Analyst Certificate Case Study 1	Data Bike-Share from 122020 to 022021	['education', 'cycling']		1	28	0	mscjuliocesar	data-bikeshare-from-122020-to-022021
7380	7380	Weights_U_Net_new_fold_1		[]		0	30	0	jisnava	weights-u-net-new-fold-1
7381	7381	Ingredients Image from Food label		['business']		1	33	0	shensivam	ingredients-image-from-food-label
7382	7382	Nutritional Facts from Food label		['nutrition']		1	99	1	shensivam	nutritional-facts-from-food-label
7383	7383	Roberta-validfit-attention-768-1		['arts and entertainment']		3	17	0	sqianghan	robertavalidfitattention7681
7384	7384	mydata1414	1234567890123211431423	[]		0	40	0	fcbfcb1998	mydata
7385	7385	23 Pet Breeds Image Classification	The dataset contains images of dogs and cats with about 170 images per breed.	['animals', 'classification', 'deep learning', 'image data', 'transfer learning']	"Content
This Dataset contains images of dogs and cats.
That is 23 of the most common breeds; 15 breeds of dogs and 8 breeds of cats.
Each of the classes has 170 images (except the 'mumbai cat' class) and is separated by folders.
All of the images are in .jpg / .jpeg format.
Most of the images have been cleaned, but further cleaning may be needed to fit to your individual models.
Inspiration
The dataset is highly inspired by the Oxford Dataset that has 37 breeds of cats and dogs.
This one contains the most common breeds of cats and dogs around the world.
I really wanted to create my very own dataset, for my custom deep learning model."	169	1418	9	aseemdandgaval	23-pet-breeds-image-classification
7386	7386	Kamboja & Mawar		[]		0	10	0	safrizarizqi	kamboja-mawar
7387	7387	jigsaw-3epo-196		['puzzles']		0	72	0	yuzhoudiyishuai	jigsaw3epo196
7388	7388	Tips to Become a Successful Investor		['investing']		0	85	0	waqaasahmad	tips-to-become-a-successful-investor
7389	7389	Land Hot Spots in Bahara Kahu		[]		0	77	0	waqaasahmad	land-hot-spots-in-bahara-kahu
7390	7390	monodepth_pretrain30		[]		0	25	0	lsyluestc	monodepth-pretrain30
7391	7391	6 Reasons to Consult a Real Estate Agent		[]		0	50	0	waqaasahmad	6-reasons-to-consult-a-real-estate-agent
7392	7392	Biwi Kinect Head Pose Database	Realtime head pose evaluation using RGBD data	['video games', 'people', 'image data']	"Context
Because cheap consumer devices (e.g., Kinect) acquire row-resolution, noisy depth data, we could not train our algorithm on clean, synthetic images as was done in our previous CVPR work. Instead, we recorded several people sitting in front of a Kinect (at about one meter distance). The subjects were asked to freely turn their head around, trying to span all possible yaw/pitch angles they could perform.
Content
To be able to evaluate our real-time head pose estimation system, the sequences were annotated using the automatic system of www.faceshift.com, i.e., each frame is annotated with the center of the head in 3D and the head rotation angles.
The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation.
Acknowledgements
Even though the algorithms work on depth images alone, we provide the RGB images as well. Please note that this is a database acquired with frame-by-frame estimation in mind, not tracking. For this reason, some frames are missing.
The database is made available for research purposes only. You are required to cite our work whenever publishing anything directly or indirectly using the data:
@article{fanelli_IJCV,
  author = {Fanelli, Gabriele and Dantone, Matthias and Gall, Juergen and Fossati, Andrea and Van Gool, Luc},
  title = {Random Forests for Real Time 3D Face Analysis},
  journal = {Int. J. Comput. Vision},
  year = {2013},
  month = {February},
  volume = {101}, 
  number = {3},
  pages = {437--458}
}
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	1899	225468	44	kmader	biwi-kinect-head-pose-database
7393	7393	Things that Lead to Increase in Price 		[]		0	53	0	waqaasahmad	things-that-lead-to-increase-in-price
7394	7394	tf_efficientnet_l2_ns-df73bb44.pth 		[]		0	23	0	akiyoshisutou	tf-efficientnet-l2-nsdf73bb44pth
7395	7395	Plotly Dashboard Healthcare	HEALTHCARE : PLOTLY DASHBOARD	['healthcare', 'business', 'health', 'exploratory data analysis', 'plotly']	"Context
Data Visualization
Content
a. Scatter plot
    i. The webapp should allow the user to select genes from datasets and plot 2D scatter plots between 2 variables(expression/copy_number/chronos) for 
       any pair of genes.
ii. The user should be able to filter and color data points using metadata information available in the file “metadata.csv”.
iii. The visualization could be interactive - It would be great if the user can hover over the data-points on the plot and get the relevant information (hint - 
        visit https://plotly.com/r/, https://plotly.com/python)
iv. Here is a quick reference for you. The scatter plot is between chronos score for TTBK2 gene and expression for MORC2 gene with coloring defined by
        Gender/Sex column from the metadata file.
b. Boxplot/violin plot
   i. User should be able to select a gene and a variable (expression / chronos / copy_number) and generate a boxplot to display its distribution across 
      multiple categories as defined by user selected variable (a column from the metadata file)
ii. Here is an example for your reference where violin plot for CHRONOS score for gene CCL22 is plotted and grouped by ‘Lineage’
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	10	125	4	sureshmecad	plotly-dashboard-healthcare
7396	7396	Student Marks Dataset	Student Marks Prediction - Regression Problem	['education', 'beginner', 'linear regression', 'regression', 'primary and secondary schools']	"Description:
The data consists of Marks of students including their study time & number of courses. The dataset is downloaded from UCI Machine Learning Repository.
Properties of the Dataset: \
Number of Instances: 100\
Number of Attributes: 3 including the target variable.
The project is simple yet challenging as it is has very limited features & samples. Can you build regression model to capture all the patterns in the dataset, also maitaining the generalisability of the model?
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the student marks wrt multiple features.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	1507	8854	27	yasserh	student-marks-dataset
7397	7397	anscombe		['beginner', 'linear regression', 'korea', 'numpy', 'pandas']		1	14	2	jeongtw	anscombe
7398	7398	finalfinal		[]		0	1	0	maideyildiz	finalfinal
7399	7399	Garbage Dataset		['earth and nature']		2	27	1	enesbayturk	garbage-dataset
7400	7400	jajaja	hats, custom selected	[]		2	5	0	firozk	jajaja
7401	7401	Training		[]		0	15	1	hamdanzakirin	training
7402	7402	starsubseq		[]		2	15	0	phoenix9032	starsubseq
7403	7403	Few-NERD: Dataset		['arts and entertainment']		1	9	0	sparknlp	fewnerd-dataset
7404	7404	longformer-baseline-pb-cfg-f2-cv-0.6282		['games']		0	5	0	atharvaingle	longformer-baseline-pb-cfg-f2
7405	7405	OWMmod0	moxing lianxu xuexi OWM	[]		0	3	0	sufeng314	owmmod0
7406	7406	Data12345		[]		1	30	1	shriteshaher	data12345
7407	7407	kneeOA_2		[]		0	24	0	schiebieni	kneeoa-2
7408	7408	longformer-baseline-pb-cfg-f1-cv-0.6201		['video games']		0	4	0	atharvaingle	longformer-baseline-pb-cfg-f1
7409	7409	dbpedia_csv_file	a baseline for detecting out-dated knowledge graph	[]		0	17	0	eldestinofeng	dbpedia-csv-file
7410	7410	Tabular Playground Series - Jan 2022	Practice your ML skills on this approachable dataset!	['earth and nature', 'business', 'banking', 'beginner', 'time series analysis', 'datetime']	"This file is exact copy of the latest Event [ https://www.kaggle.com/c/tabular-playground-series-jan-2022/overview] & reason why it's created with below reasons:
- Anyone can experiment without any judgement.
- To make more mistakes to learn better. 
For this challenge, you will be predicting a full year's worth of sales for three items at two stores located in three different countries. This dataset is completely fictional but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches.
Good luck!
Files
- train.csv - the training set, which includes the sales data for each date-country-store-item combination.
- test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.
- sample_submission.csv - a sample submission file in the correct format"	22	347	4	gauravduttakiit	tabular-playground-series-jan-2022
7411	7411	InfluenceCharacteristicCrudeOilPredictionData		[]	"Context
This is a multi factor data set about crude oil futures
Including the rise and fall of the US dollar index and the US dollar index, and the rise and fall of gold price and gold price
The time length is from 2020 to 2021, and the time granularity is days"	3	51	0	renxb666	influencecharacteristiccrudeoilpredictiondata
7412	7412	modelparams		[]		0	18	0	haowu11	modelparams
7413	7413	chinese rest holiday dataset 2020 to 2022	Only include rest holiday	['travel']		34	814	27	holoong9291	chinese-rest-holiday-dataset-2020-to-2022
7414	7414	df_train_cars		[]		0	12	0	laysanmart	df-train-cars
7415	7415	BRFSS 2020 Survey Data	Behavioral Risk Factor Surveillance System 2020 Survey Data	['united states', 'healthcare', 'health', 'survey analysis']	"The Behavioral Risk Factor Surveillance System (BRFSS) is a collaborative project between all of the states in the United States and participating US territories and the Centers for Disease Control and Prevention (CDC). 
BRFSS’s objective is to collect uniform state-specific data on health risk behaviors, chronic diseases and conditions, access to health care, and use of preventive health services related to the leading causes of death and disability in the United States. BRFSS conducts both landline and mobile phone-based surveys with individuals over the age of 18. General factors assessed by the BRFSS in 2020 included health status and healthy days, exercise, insufficient sleep, chronic health conditions, oral health, tobacco use, cancer screenings, and access to healthcare.
Section Names:
Record Identification -&gt; Columns 0 to 8
Land Line Introduction -&gt; Columns 9 to 20
Cell Phone Introduction -&gt; Columns 21 to 30
Respondent Sex -&gt; Column 31
Health Status -&gt; Column 32
    a. Healthy Days -&gt; Columns 33 to 35
    b. Health Care Access -&gt; Columns 36 to 39
    c. Exercise -&gt; Column 40
    d. Inadequate Sleep -&gt; Column 41
    e. Chronic Health Conditions -&gt; Columns 42 to 54
    f. Oral Health -&gt; Columns 55 and 56
Demographics -&gt; Columns 57 to 69
Disability -&gt; Columns 70 to 75
Tobacco Use -&gt; Columns 71 to 75
Alcohol Consumption -&gt; Columns 76 to 79
Immunization -&gt; Columns 80 to 83
Falls -&gt; Columns 84 and 85
Seatbelt Use and Drinking and Driving -&gt; Columns 86 and 87
Breast and Cervical Cancer Screening -&gt; Columns 88 to 94
Prostate Cancer Screening -&gt; Columns 95 to 100
Colorectal Cancer Screening -&gt; Columns 101 to 110
HIV/AIDS -&gt; Columns 111 to 113
Diabetes -&gt; Columns 114 to 124
ME/CFS -&gt; Columns 125 to 127
Hepatitis Treatment -&gt; Columns 128 to 133
Health Care Access -&gt; Column 134
Cognitive Decline -&gt; Columns 135 to 140
Caregiver -&gt; Columns 141 to 149
E-Cigarettes -&gt; Columns 150 and 151
Marijuana Use -&gt; Columns 152 to 154
Lung Cancer Screening -&gt; Columns 155 to 158
Cancer Survivorship: 
    a. Type of Cancer -&gt; Columns 159 to 162
    b. Course of Treatment -&gt; Columns 163 to 170
    c. Pain Management -&gt; Columns 171 and 172
Prostate Cancer Screening Decision Making -&gt; Columns 173 and 174
HPV Vaccination -&gt; Columns 175 and 176
Tetanus Diphtheria (Tdap) (Adults) -&gt; Column 177
Place of Flu Vaccination -&gt; Column 178
Sex at Birth -&gt; Column 179
Sexual Orientation and Gender Identity (SOGI) -&gt; Columns 180 to 182
Adverse Childhood Experience -&gt; Columns 183 to 193
Random Child Selection -&gt; Columns 194 and 195
Childhood Asthma Prevalence -&gt; Columns 196 and 197
Questionnaire Version -&gt; Column 198
Questionnaire Language -&gt; Column 199
Urban Rural -&gt; Columns 200 and 201
Weighting Variables -&gt; Columns 202 to 207, 212 to 215
Child Demographic Variables -&gt; Columns 208 to 210
Child Weighting Variables -&gt; Column 211
Calculated Variables  -&gt; Columns 216 to 228, 236 to 278
Calculated Race Variables -&gt; Columns 229 to 235
Acknowledgements
This dataset has been published annually by the CDC since 1984. You can find the original dataset as a ASCII format and past years data from here
Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System Survey Data. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, [2020]."	13	132	0	aemreusta	brfss-2020-survey-data
7416	7416	testing		['electronics']		0	0	0	twm222	testing
7417	7417	samples		[]		0	19	1	tsechunwang	samples
7418	7418	Lichess: Chess Games Statistics	online chess games with ratings, number of blunders, mistakes, inaccuracies.	['global', 'data cleaning', 'statistical analysis', 'tabular data', 'pandas']	"Context
Thanks to the Netflix original series, The Queen's Gambit, many people have been intrigued to start their chess journey. As a person who likes to find patterns in clutters, I decided to gather chess games played online (on lichess.org) and produce an expandable dataset from an existing dataset on Kaggle..
Content
The above dataset was my starting point since I used the usernames in it to scrape more games and usernames. Then, I structured the column and rows in a different way than the original source.
Game ID:
Every game on lichess.org has an ID. Adding the game ID after lichess.org/ would lead you to the actual game for better visualization of the game
White & Black Ratings:
Lichess rates its player with integer values; The higher the rating, the more skillful a player is.
PS. All the games in the dataset are rated because players would be more serious while playing when their ratings were at risk.
Opening ECO:
The first moves in a chess game are called ""the opening phase"" which typically shapes the game (aggressive, positional)
Opening examples: Ruy Lopez, Sicilian Defence, The English, The Queen's Gambit, Bongcloud Attack, Grob's Attack, Caro Kann Defence, etc...
Since the names of openings tends to be lengthy, the Encyclopaedia of Chess Openings (ECO) has dedicated codes for each opening. e.g. The Queen's Gambit's code is D06.
Opening Ply:
In chess, opening moves that have been explored and played by the masters are called ""theory"". Opening ply is the number of plies that follows masters' chess theory before deviating into the not-so-well-explored moves.
A ply is 1/2 of a move. e.g. (1. d4 d5 is a move and has 2 plies, but 1.d4 is only 1 ply)
Average Centipawn Loss:
Centipawn is the advantage measurement unit in chess; 100 centipawn equals 1 pawn. 
Chess is about minimizing your losses and maximizing your advantage, and more skilled players know how to do that better than beginners. 
The centipawn of the is calculated by the open-source chess engine Stockfish, and the average is basically the total of centipawn loss divided by the number of plies played by a certain player.
Blunder:
Blunders are moves that are considered unforgivable errors that can change the game drastically if exploited by the opponents correctly
Mistake:
Mistakes are moves that are considered less drastic errors than blunders, that still loses the players advantage over to their opponent.
Inaccuracies:
Inaccuracies are not horrible moves, but a move is said to be inaccurate if there are better moves that improve the chess position.
Acknowledgements
I would like to thank the great chess community at lichess who is enriching the chess world with all the free and open-source features to everyone. I also want to thank the developers of Berserk, the API I used, for making a Python API and maintaining it regularly. Finally, I would like to thank Mitchell J who provided the starting point for my dataset.
Inspiration
I was inspired to come back to chess by International Master Levy Rozman AKA. GothamChess on YouTube. His YouTube series called ""Guess the Elo"" inspired me to collect data that would be relevant to determine a chess player level."	361	3628	59	ahmedalghafri	lichess-chess-games-statistics
7419	7419	test_dataset		[]		0	13	1	datapythonic	test-dataset
7420	7420	dataset1		[]		0	32	0	qqpp86	dataset1
7421	7421	nersci		[]		0	20	0	ahmetsuerdem	nersci
7422	7422	NSE-Nifty50	Stocks in Nifty50 index	['business', 'investing']	"Context
Being an Active investor, Nifty is a staple for the day.
Naturally, I was inclined to look deep into the Nifty50 to understand the stocks' performance better.
This is a rich dataset that can be used to answer a lot of Nifty50 questions
Content
The dataset contains the stocks that comprise the Nifty50 index. 
It has the data on the each stock's OHLC, Volume and Turnovers.
It also possess the data on each stock's 52W Highs and lows.
Acknowledgements
A big thanks to NSE India for sharing the data on their website. (https://www.nseindia.com/)
Inspiration
This dataset will be a great asset to do your analysis to answer some of the questions of Nifty 50 stocks.
For example: What stocks are doing exceptionally well in the last one month ?
I'm sure you can spring up many more questions that can be answered with the analysis of this dataset."	4	48	0	naveenkaranth	nsenifty50
7423	7423	inputsstream		[]		0	24	0	zhaoyizhongzhao	inputsstream
7424	7424	transformer-xl		['electronics']		0	12	0	yuanpq	transformerxl
7425	7425	swin_trained		[]		5	41	0	yamqwe	swin-trained
7426	7426	SuperStore Sales		[]		3	76	1	nazarmahadialseied	superstore-sales
7427	7427	transformer-xl		['electronics']		0	12	0	yuanpq	transformerxl
7428	7428	swin_trained		[]		5	41	0	yamqwe	swin-trained
7429	7429	SuperStore Sales		[]		3	76	1	nazarmahadialseied	superstore-sales
7430	7430	widerperson kitti		[]		15	40	0	imneonizer	widerperson-kitti
7431	7431	Turkey Covid-19 Lockdown Days		['europe', 'asia', 'covid19']	"Covid-19 Lockdown Dates in Turkey
In data, there are 3 columns.
Date: Dates of lockdown
City: The lockdown has applied on how many different provinces
URL: Source of lockdown news
If you trying to calculate covid-19 or lockdown affect in your project (like sales data or etc.) this data can help for your purpose.
This data may be superficial but you can find more information like the lockdowns has applied which provinces or what was the time when lockdown has started in URL sources."	4	52	1	recepyilkici	turkey-covid19-lockdown-days
7432	7432	Alexnet		[]		0	41	0	yingyingkan	alexnet
7433	7433	mydata		['business']		0	0	0	sambapadhai	mydata
7434	7434	Pre-trained Models		[]		0	21	0	diveintoai	pretrained-models
7435	7435	model_deberta_v3_base		[]		0	4	0	qinyukun	model-deberta-v3-base
7436	7436	ExpW-DS		[]		0	31	0	mohammedaaltaha	expwds
7437	7437	pawpularityscorefront-tfrecord-fold-5-9		[]		0	39	0	suyinchen1024	pawpularityscorefronttfrecordfold59
7438	7438	brat-datasets		[]		0	25	0	shinnyayoshida	bratdatasets
7439	7439	semmelweis		[]		0	8	0	churros247	semmelweis
7440	7440	coco_json_Detectron2		[]		0	51	0	laminkoko	coco-json-detectron2
7441	7441	"Gasoline data from Adonis Yatchew website	"		[]		3	68	0	virasemenova1	gasoline-data-from-adonis-yatchew-website
7442	7442	Reddit - Machine Learning and Data Science	Different Machine Learning and Data Science subreddits posts.	['education', 'computer science', 'beginner', 'nlp', 'online communities', 'social networks']	"Please, If you enjoyed this dataset, don't forget to upvote it.
Content
This dataset contains a couple of fields with the information based on Reddit post submission, such:
title
id
redditor
num_upvotes
subreddit
url
num_comments
created_on
body
upvote_ratio
over_18
link_flair_text
edited
Method
The data was extracted using the PRAW:The Python Reddit API Wrapper.
Credits
Cover Image: Photo by <a href=""https://unsplash.com/@marius?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Marius Masalar</a> on <a href=""https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	5	109	0	themlphdstudent	reddit-machine-learning-and-data-science
7443	7443	Animal -5 Mammal	This dataset contain 5 different mammals(Cat, Dog , Elephant , Horse , Lion)	['animals', 'classification', 'neural networks', 'image data', 'tensorflow']	"Hello everyone!
About This Data
This is the dataset I have used for my matriculation thesis.
It contains about 15K medium quality animal images belonging to 10 categories: dog, cat, horse, elephant ,lion.
All the images have been collected from ""google images"" and have been checked by human. There is some erroneous data to simulate real conditions (eg. images taken by users of your app).
How to Cite this Dataset
If you use this dataset in your research, please credit the authors"	42	547	17	shiv28	animal-5-mammal
7444	7444	GANfolk	Portraits from WikiArt.org and Open Images to be used from training GANS	['art', 'people', 'gan', 'image data']	"This is a collection of portraits from WikiArt.org and Open Images to be used for training GANS.
You can see my article on Medium and check out the results here, opensea.io/collection/ganfolk
The source code and generated images are released under the CC BY-SA license.
If you use these source images to create new images, please give attribution like this: This image was created with GANfolk by Robert A. Gonsalves."	4	115	2	robgonsalves	ganfolk
7445	7445	Salaries by College Major		['jobs and career']		11	59	0	churros247	salaries-by-college-major
7446	7446	PreComputed		[]		0	36	0	sajilck	precomputed
7447	7447	Iklan Sosmed		[]		0	23	0	daffaaryaguna	iklan-sosmed
7448	7448	classify2	resnet50_clf_feature_224	[]		0	18	0	stuliuc	classify2
7449	7449	LANL - Đầu vào đã chuẩn hóa		['tabular data']	"Context
Khi làm contest của LANL về dự đoán động đất, nhận thấy việc xử lý dữ liệu đầu vào tốn rất nhiều thời gian và nặng. Do đó, việc lưu lại các file sẽ giúp việc huấn luyện nhanh hơn.
Content
3 file, 2 file input cho training và testing, 1 file cho nhãn.
Acknowledgements
Trích xuất dữ liệu từ contest của LANL"	1	24	0	ngodang	lanl-u-vo-chun-ha
7450	7450	performer_1129	123123123123123123123123123123123123	[]		1	24	0	henrywu1011	performer-1129
7451	7451	Assinment1		[]		0	2	0	unhejunlu	assinment1
7452	7452	Daftar Gaji		[]		0	6	0	daffaaryaguna	daftar-gaji
7453	7453	jigsaw_20220104_multi		[]		0	3	0	qinyukun	jigsaw-20220104-multi
7454	7454	NBA Player stats 2000-2020 seasons	scraped from basketballreference	['basketball']		11	71	0	bme3412	nba-player-stats-20002020-seasons
7455	7455	Model 2 Version 3	Public score 17.87145	['news']		0	13	0	odyssey119	model-2-version-3
7456	7456	petmodel		[]		0	46	0	usagichoko	petmodel
7457	7457	PetFinder-Model53		[]		0	27	0	lftuwujie	petfindermodel53
7458	7458	Feature Importance		['computer science']		0	11	0	trevorhughes	feature-importance
7459	7459	global		['business']		0	21	0	jrstats	global
7460	7460	big_streetview_pics_zoomed		[]		0	33	0	andreimuresanu	big-streetview-pics-zoomed
7461	7461	JRSTC_linear_model		[]		0	4	0	learnitanyway	jrstc-linear-model
7462	7462	Top 10 Features		[]		0	3	0	trevorhughes	top-10-features
7463	7463	Water Footprint Recommender System Data		['computer science']	"Context
This dataset contains all the data and embeddings used for the production of this project: A Food Recommendation System for reducing the Water Footprint (https://github.com/TurconiAndrea/water-footprint-reducer-rs). 
It contains data from two different realities: Food.com, a well-known American recipe site, and Planeat, an Italian site that allows you to plan recipes to save food waste. The dataset is divided into two parts: embeddings, which can be used directly to execute the work and receive suggestions, and raw data, which must first be processed into embeddings.
Content
This dataset contains: 
Embedding
food.com
ingredients.pkl
model.pkl
recipes.pkl
reviews.pkl
planeat
ingredients.pkl
model.pkl
recipes.pkl
reviews.pkl
Input data
food.com
orders.csv
recipes.csv
planeat
orders.csv
recipes.csv"	1	68	0	turconiandrea	water-footprint-recommender-system-data
7464	7464	tedcleaned.google.en2nl		['internet']		0	0	0	alisonpeten	tedcleanedgoogleen2nl
7465	7465	watson		[]		0	2	0	mathurinache	watson
7466	7466	Movie Trivia Schmoedown		['movies and tv shows']		1	21	1	dandolinski	movie-trivia-schmoedown
7467	7467	its jokeboy		[]		4	18	0	overgrowth	its-jokeboy
7468	7468	U.S. Gun Violence Records 2014-2021	Archive of Gun Violence incidents in United States	['united states', 'crime', 'social science', 'beginner', 'news']	"Aggregated information of over three thousand incidents of gun violence within the United States from Jan 2014 through Dec 2021. 
Address of the incident is mentioned along with the number of causalities or injuries.
To see further details of the incident including participants, incident details and news sources use: 
https://www.gunviolencearchive.org/incident/{incident_id}. 
This data along with the latest events available at the Gun Violence Archive."	1054	6407	26	konivat	us-gun-violence-archive-2014
7469	7469	005DataSetMc		[]		0	15	0	maksymkorolov	005datasetmc
7470	7470	GalaxyTest		[]		4	5	0	mauriciominguini	galaxytest
7471	7471	CT_15_each_class		[]		1	14	0	aftabhussaincui	ct-15-each-class
7472	7472	House Price Prediction	House Price Prediction	['cities and urban areas', 'beginner', 'data cleaning', 'data visualization', 'tabular data']		1	104	1	akarshsharma	house-price-prediction
7473	7473	ortopedik		[]		0	30	0	esraokka	ortopedik
7474	7474	U.S. CO2 Emissions and Ann Arbor Weather	U.S. CO2 emissions and temperature change in Ann Arbor between 1960-2020	['united states', 'earth and nature', 'atmospheric science', 'weather and climate']	"This dataset is created for the ""Becoming an independent data scientist"" assignment for the Coursera course ""Applied Plotting, Charting & Data Representation in Python.""
This dataset contains:
1. Climate data for Ann Arbor, Michigan for the years 1960-2020.
2. CO2 emission and population data for the U.S.
The data is used to compare and analyze yearly changes in U.S. CO2 emissions and average temperatures in Ann Arbor."	6	71	0	bkermen	independent-data-scientist
7475	7475	BPM32kicks4389		['business', 'tabular data']	Beam Position Monitor (BPM) data	0	83	0	kilean	bpm32kicks4389
7476	7476	MOLES SELECTOS DON MARIO 		[]		0	30	0	molesdonmario	moles-selectos-don-mario
7477	7477	Medicare B Drug Spending 2015 to 2019	Annual Medicare B fee for service drug spend 	['healthcare', 'drugs and medications']		0	36	0	koan14	medicare-b-drug-spending-2015-to-2019
7478	7478	brackish yolov5		[]		3	24	0	ks2019	brackish-yolov5
7479	7479	flowers		[]		0	5	0	mathyslioson	flowers
7480	7480	pay120models		[]		0	16	0	olegefimov	pay120models
7481	7481	longformer-baseline-pb-cfg-f0-cv-0.6169		[]		0	6	0	atharvaingle	longformer-baseline-pb-cfg-f0
7482	7482	ignaz_picture		[]		0	1	0	abrahamanderson	ignaz-picture
7483	7483	big_streetview_pics		[]		1	4	0	andreimuresanu	big-streetview-pics
7484	7484	pt longformer base		[]		13	48	3	kishalmandal	pt-longformer-base
7485	7485	Star Trek Scripts	TOS, TNG, DS9, VOY, ENT all episode scripts	['movies and tv shows']		0	24	0	tamarafingerlin	star-trek-scripts
7486	7486	pikachuswindataset		[]		0	17	0	gyanendradas	pikachuswindataset
7487	7487	PREVENTION		['public safety']		3	12	0	kailliang	prevention
7488	7488	RSMIND		[]		0	42	0	anhthemduocngu	rsmind
7489	7489	Natural Diamonds (Prices + Images)	A dataset for natural diamonds	['earth and nature', 'clothing and accessories', 'finance', 'computer vision', 'data analytics', 'investing']	"Natural Diamonds
Natural diamonds are one of the precious stones bought to wear as jewellery or as investment as well. Diamonds are not that glittery and beautiful in their raw form. The rough diamond stone is normal looking stone as others are. The miners filters the mined soil to find the rough diamonds and sell them to the manufactures. The manufactures do the creative work on those rough stones. There are many shapes of polished diamonds available in the market. Manufactures plan and polish the rough diamonds based on the maximum financial gain from the polished product. Here, the diamond's price depends upon hundreds of parameters but mainly on 4 C's (Carat, Cut, Clarity, Color). 
Dataset
Some of the main features of the diamonds are present in this dataset. I have managed to gather data on shapes like round, cushion, radiant, emerald, heart, oval, etc. You can use it in any way to create a machine learning project for your self. In some cases, the diamond image is not available (there is a .png type alternative image available) for those diamonds.
Features
id -&gt; unique id of the diamond
shape -&gt; shape of the polished diamond
Weight -&gt; weight of the diamond in Carats (the bigger the weight the expensive it is)
Clarity -&gt; clarity of the diamonds (FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, SI3, I1, I2, I3)
Colour -&gt; colour shade of the diamond (D, E, F, ... Z)
Cut -&gt; cutting level of the polished stone (Poor, Fair, Good, very Good, Excellent)
Polish -&gt; polish level of the stone
Symmetry -&gt; over all symmetry of the stone's shape 
Fluorescence -&gt; Fluorescence is the ability of certain chemicals to give off visible light after absorbing radiation which is not normally visible, such as ultraviolet light
Latest Version
Diamonds2
Acknowledgements
I have scraped the data from
capital wholesale diamonds
The purpose for scraping this data is to learn more on diamonds using Data Science
Thank You for the Data!
Projects for You
EDA on the different diamonds and there parameters
Impact  of different parameters on price of the diamond
Color classification of the diamond based on the image given
Shape classification of the diamond based on the image given
Inclusion detection using images
Future Updates
The data is not complete, still it's enough to start with it. I will update with more data and more shape.
Please upvote, if you like this dataset"	292	3024	23	harshitlakhani	natural-diamonds-prices-images
7490	7490	qqqqqq		[]		0	14	0	migiii	qqqqqq
7491	7491	PWP Swin-Transformer		[]		0	4	0	aishikai	pwp-swintransformer
7492	7492	Mini Turkish Banknote Dataset		['currencies and foreign exchange']		0	12	0	volkanonder	mini-turkish-banknote-dataset
7493	7493	Armed-dataset		[]		1	43	0	victoryofoegbu	armeddataset
7494	7494	Covid-Xray-Balanced-Dataset		[]		0	39	0	jarvisai7	covidxraybalanceddataset
7495	7495	Cyclistic_trip_data - A public dataset		[]		1	13	0	yeasirabehayet	cyclistic-trip-data-a-public-dataset
7496	7496	15歲以上吸菸者每天平均吸菸支數		[]		0	29	0	a41073019	15smoke
7497	7497	archive		[]		0	21	0	ads071	archive
7498	7498	all_data_file		[]		0	14	0	tusarthbihani	all-data-file
7499	7499	output		[]		0	22	0	zhangyiifan	output
7500	7500	Troop Deployments USA	 U.S. troop deployments to Europe	['europe', 'history', 'military', 'beginner', 'classification']	"Context
Dataframe containing country-year observations of US military deployments to Europe countries from 2006 through 2015.
Content
This dataset consists of 279 rows and nine columns, which contain the following information:
country: name of the country. 
code:  three-digit numerical code defined by the United Nations. 
iso3c: alpha-3 - Three-character alpha code defined by the International Organization for Standardization (ISO).
year: the year of the observation.
troops: number of US military personnel deployed to the host country. 
army: army personnel deployed to the host country.
navy: navy personnel deployed to the host country.
air: air force personnel deployed to the host country.
marine: marine corps personnel deployed to the host country.
Acknowledgements
Allen, Michael A., Michael E. Flynn, and Carla Martinez Machain. 2021. “Global U.S. military deployment data: 1950-2020.” Conflict Management and Peace Science. TBD."	16	162	1	iskk97	usa-troop-data-in-europe
7501	7501	model_file		[]		0	6	0	ads071	model-file
7502	7502	Acute Lymphoblastic Leukemia image dataset		['cancer']		23	187	1	asraf047	acute-lymphoblastic-leukemia-image-dataset
7503	7503	NFL_Transformed_Data		[]		0	20	0	kamaljp	nfl-transformed-data
7504	7504	Amazon		['retail and shopping']		0	11	0	naqiraza	amazon
7505	7505	pet2-tfrecords2-448-001		[]		0	4	0	bamps53	pet2-tfrecords2-448-001
7506	7506	Covid Pneumonia dataset	 COVID-19 chest x-ray images 	['health', 'computer science', 'deep learning', 'covid19']	"Publicly available COVID-19 chest x-ray images from the Italian Society of Medical, Radiopaedia, and NIH Clinical Center were used here.
In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 500,000 scholarly articles, including over 200,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up."	7	105	2	redwan1010	covid-pneumonia-dataset
7507	7507	youtube videos titles 2	youtube videos title along with category and videoId	[]		2	41	0	pushpaksaraf	youtube-videos-title-and-description
7508	7508	MissUniverseandWorld	Miss Universe and Miss World Dataset	['research', 'arts and entertainment', 'global', 'people and society', 'text data']	"Context
In recent past decades, India has produced promising beauty's with brains in world platform. This data set surely confirms it esspecially after 1994.
Content
The dataset contains Year, Country, name of holder, Age, Hometown, National Title, Date, Entrants.
Inspiration
When India's Harnaz Sandhu lift the title of 'Miss Universe 2021'."	4	68	0	vidyeshsakpal	missuniverseandworld
7509	7509	IndianCurrencyDetectionAndClassification		[]		2	22	0	najiaboo	indiancurrencydetectionandclassification
7510	7510	چگونه در خانه پولدار شویم؟	پولدار شدن را با ما فرا بگیرید	[]		0	35	0	kolmok	whatis
7511	7511	GPTJ-6B Large Autoregressive Language Model	A 6 billion parameter, autoregressive text generation model trained on The Pile.	['intermediate', 'nlp', 'tpu', 'pytorch', 'transformers']	"Original Creators:
Ben Wang (kingoflolz)
EleutherAI
Citations:
1. @misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
2. @misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
GPT-J-6B
A 6 billion parameter, autoregressive text generation model trained on The Pile.
Acknowledgments
This project would not have been possible without compute generously provided by the TPU Research Cloud with assistance from EleutherAI.
Thanks to the Cloud TPU team at Google for providing early access to the Cloud TPU VM alpha (now publicly available!)
Thanks to everyone who have helped out one way or another (listed alphabetically):
Aran Komatsuzaki for advice with experiment design and writing the blog posts.
James Bradbury for valuable assistance with debugging JAX issues.
Janko Prester for creating the web demo frontend.
Laurence Golding for adding some features to the web demo.
Leo Gao for running zero shot evaluations for the baseline models for the table.
License
The weights of GPT-J-6B are licensed under version 2.0 of the Apache License.
Model Details
Hyperparameter  Value
n_parameters    6,053,381,344
n_layers    28*
d_model 4,096
d_ff    16,384
n_heads 16
d_head  256
n_ctx   2,048
n_vocab 50,257 (same tokenizer as GPT-2/3)
position encoding   Rotary position encodings (RoPE)
RoPE dimensions 64
* each layer consists of one feedforward block and one self attention block
The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model dimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3."	32	445	11	aryashah2k	gptj6b-large-autoregressive-language-model
7512	7512	datasetlivrable2		[]		8	26	0	alexandrevivier	datasetlivrable2
7513	7513	pet2-tfrecords2-384-013		[]		0	1	0	bamps53	pet2-tfrecords2-384-013
7514	7514	Yoga-Poses	The dataset contains- 900 + diverse images  for four different Yoga Asanas.	['exercise']		2	48	1	guhankesav	yogaposes
7515	7515	toxic public dataframes		['health']		40	141	4	readoc	toxic-public-dataframes
7516	7516	Timeseries Transformer Trained 		[]		4	41	0	yamqwe	timeseries-transformer-trained
7517	7517	mp2-q2		[]		0	3	0	miladaf	mp2q2
7518	7518	cropsss		[]		0	0	0	khalilurrehman	cropsss
7519	7519	Top 10 Highest-Paid Athletes 2011 - 2021	Tennis | NBA | Soccer	['sports', 'business']	"Context
Forbes is not just one of the most popular business magazines!! It contains countless articles on numerous subjects (e.g., business, investing, technology, entrepreneurship, etc.), reporting valuable data and insights. 
For instance, Forbes publishes annual lists of wealthy people reporting their worth such as ""Forbes 400"" and ""Forbes World's Billionaires list"". 
Athletes are not an exception, and every year lists are published for the top highest paid individuals.
Content
The data are scrapped manually from Forbes articles listing the top 10 highest-paid athletes in tennis, NBA, and soccer.
Athletes can have multiple sources of income. 
•  Team sports athletes earn a salary paid by their team whereas individual sports athletes compete in tournaments for prize money (such as tennis players).
•  Most of the time, brands are paying athletes to promote their products (on and off the court) as a marketing promotional strategy to reach a wider target audience and boost their sales / profit.
The dataset contains 11 years of data starting from 2011.
Source
Forbes official website: https://www.forbes.com/ (dataset last updated on 3rd of January 2022)
Inspiration
•  Which sport rewarded its athletes the most in each year?
•  Is there a trend across years for the total earnings of the top 10 highest-paid athletes of each sport? 
•  Does this trend change when looking into salaries (or prize money) and endorsements separately?
•  Which country 'earns' the most out of those three sports each year?
These are examples of interesting questions that could be answered by analysing this dataset.
If you are interested, please have a look at the Tableau dashboard that I have created to help answer the above questions, and report some of my insights.
Tableau dashboard: https://public.tableau.com/views/AthelesSaleries/SportsEarningsAnalysis?:language=en-US&publish=yes&:display_count=n&:origin=viz_share_link"	62	371	2	dimitrisangelide	top-10-highestpaid-athletes-tennis-nba-soccer
7520	7520	tomato leaves		['food']		1	19	0	madhumithapericharla	tomato-leaves
7521	7521	Text Genre Identification Corpus GINCO 	https://www.clarin.si/repository/xmlui/handle/11356/1467	['nlp', 'text mining', 'deep learning', 'text data', 'multilabel classification']	"This is the train:test (80:20) split with texts and labels from the Slovene Web genre identification corpus GINCO 1.0. The corpus contains web texts, manually annotated with genre, from two Slovene web corpora.
The corpus allows for automated genre identification and genre analyses as well as other web corpora research, and comprises 1002 texts (478,969 words), manually annotated with 21 genre categories: News/Reporting, Announcement, Research Article, Instruction, Recipe, Call (such as a Call for Papers), Legal/Regulation, Information/Explanation, Opinionated News, Review, Opinion/Argumentation, Promotion of a Product, Promotion of Services, Invitation, Promotion, Interview, Forum, Correspondence, Prose, List of Summaries/Excerpts, and Other.
For more information and access to freely available corpus in JSON format with additional parameters, which can be used for multitarget classification as well, see: https://www.clarin.si/repository/xmlui/handle/11356/1467
For more information on the construction of the corpus, annotation procedure and initial experiments, see the article The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild (https://arxiv.org/abs/2201.03857)"	3	32	0	tajakuz	gincodataframededuptraindevtest
7522	7522	Winter Olympics (1924 - 2018)	This dataset contains the data of all the past Winter Olympic events	['sports', 'beginner', 'intermediate', 'advanced', 'tabular data']	"Context
As Winter Olympic 2022 is approaching, I aim to better understand the different disciplines and sports that are competed at the event, so that I can better appreciate the event overall. Thus, I have scraped the following data
Content
There are 3 CSV. The first one olympic_events.csv, include the information about the different Olympic events on a high level, with information such as name of the event, where and when it took place, number of overall participants, etc. 
events_medals.csv dive deeper into  which are the countries that have won a medal, how many have they won, etc.
The last one, discipline_details.csv, dive deeper into all the sport competed at each individual event
Acknowledgements
This data has been collected through https://olympedia.org/
Inspiration
Which countries have improved the most in recent year?
Does the location of the event, whether it is in Asia, Europe, etc. affect individual countries performance?"	162	818	5	leminhnguyen	winter-olympics-1924-2018
7523	7523	Simulated HVAC Energy Consumption 	EnergyPlus Simulation Results for a Building with and Active Chilled Beam HVAC. 	[]		5	80	0	alirezaetemad	simulated-hvac-energy-consumption
7524	7524	Discount Builders Supply & Hardware Store	Discount&nbsp;Builders&nbsp;Supply&nbsp;&&nbsp;Hardware 	[]		3	56	0	discountbuilder	discountbuilderssupply
7525	7525	beer_quality		[]		0	10	0	thomasmargnac	beer-quality
7526	7526	CMU MoCap Dataset as used in BeatGAN	A CSV version of the CMU MoCap dataset subset used in BeatGAN	['computer science', 'software']	"This is a CSV raw version of the CMU MoCap dataset subset used in [Zhou et al., 2019]. There is no windowing, striding nor normalisation applied to the data.
For more information concerning the structure of the data please see Beatgan Repo.
Structure
All of the data was concatenated into a single CSV data.csv. The provided labels.csv provides the labels for each data sample.
Labels
Zhou et al. use three classes for their dataset. The first class walking (labelled as 0) is considered the normal class and the jogging (labelled as 1) and jumping (labelled as 2) are considered abnormal classes.
License Notes from the original dataset authors:
Original Authors
This data is free for use in research projects.
You may include this data in commercially-sold products,
but you may not resell this data directly, even in converted form.
If you publish results obtained using this data, we would appreciate it
if you would send the citation to your published paper to jkh+mocap@cs.cmu.edu,
and also would add this text to your acknowledgments section:
The data used in this project was obtained from mocap.cs.cmu.edu.
The database was created with funding from NSF EIA-0196217."	6	23	0	maximdolg	cmu-mocap-dataset-as-used-in-beatgan
7527	7527	 Lending Club Loan Data - Most Accurate	LendingClub is a US peer-to-peer lending company, headquartered in San Francisco	[]	"LendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.
There are many LendingClub data sets on Kaggle. Here is the information on this particular data set:
<table>
  <thead>
    <tr style=""text-align: right"">
      <th></th>
      <th>LoanStatNew</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>loan_amnt</td>
      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>term</td>
      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>int_rate</td>
      <td>Interest Rate on the loan</td>
    </tr>
    <tr>
      <th>3</th>
      <td>installment</td>
      <td>The monthly payment owed by the borrower if the loan originates.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>grade</td>
      <td>LC assigned loan grade</td>
    </tr>
    <tr>
      <th>5</th>
      <td>sub_grade</td>
      <td>LC assigned loan subgrade</td>
    </tr>
    <tr>
      <th>6</th>
      <td>emp_title</td>
      <td>The job title supplied by the Borrower when applying for the loan.*</td>
    </tr>
    <tr>
      <th>7</th>
      <td>emp_length</td>
      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>home_ownership</td>
      <td>The home ownership status provided by the borrower during registration&nbsp;or obtained from the credit report.&nbsp;Our values are: RENT, OWN, MORTGAGE, OTHER</td>
    </tr>
    <tr>
      <th>9</th>
      <td>annual_inc</td>
      <td>The self-reported annual income provided by the borrower during registration.</td>
    </tr>
    <tr>
      <th>10</th>
      <td>verification_status</td>
      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>
    </tr>
    <tr>
      <th>11</th>
      <td>issue_d</td>
      <td>The month which the loan was funded</td>
    </tr>
    <tr>
      <th>12</th>
      <td>loan_status</td>
      <td>Current status of the loan</td>
    </tr>
    <tr>
      <th>13</th>
      <td>purpose</td>
      <td>A category provided by the borrower for the loan request.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>title</td>
      <td>The loan title provided by the borrower</td>
    </tr>
    <tr>
      <th>15</th>
      <td>zip_code</td>
      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>
    </tr>
    <tr>
      <th>16</th>
      <td>addr_state</td>
      <td>The state provided by the borrower in the loan application</td>
    </tr>
    <tr>
      <th>17</th>
      <td>dti</td>
      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>
    </tr>
    <tr>
      <th>18</th>
      <td>earliest_cr_line</td>
      <td>The month the borrower's earliest reported credit line was opened</td>
    </tr>
    <tr>
      <th>19</th>
      <td>open_acc</td>
      <td>The number of open credit lines in the borrower's credit file.</td>
    </tr>
    <tr>
      <th>20</th>
      <td>pub_rec</td>
      <td>Number of derogatory public records</td>
    </tr>
    <tr>
      <th>21</th>
      <td>revol_bal</td>
      <td>Total credit revolving balance</td>
    </tr>
    <tr>
      <th>22</th>
      <td>revol_util</td>
      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>
    </tr>
    <tr>
      <th>23</th>
      <td>total_acc</td>
      <td>The total number of credit lines currently in the borrower's credit file</td>
    </tr>
    <tr>
      <th>24</th>
      <td>initial_list_status</td>
      <td>The initial listing status of the loan. Possible values are – W, F</td>
    </tr>
    <tr>
      <th>25</th>
      <td>application_type</td>
      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>
    </tr>
    <tr>
      <th>26</th>
      <td>mort_acc</td>
      <td>Number of mortgage accounts.</td>
    </tr>
    <tr>
      <th>27</th>
      <td>pub_rec_bankruptcies</td>
      <td>Number of public record bankruptcies</td>
    </tr>
  </tbody>
</table>"	25	298	0	ahmedmohameddawoud	lending-club-loan-data-most-accurate
7528	7528	Pakistan dataset 1960 to 2020	Pakistan dataset 1444 features	['education', 'finance', 'health', 'beginner', 'tabular data']	"Pakistan Dataset
SOURCE_ORGANIZATION
International Monetary Fund, Balance of Payments Statistics Yearbook and data files.
World Bank staff estimates based data from International Monetary Fund's Direction of Trade database.
International Monetary Fund, Balance of Payments Statistics Yearbook and data files.
World Bank staff estimates through the WITS platform from the Comtrade database maintained by the United Nations Statistics Division.
World Bank staff estimates using the World Integrated Trade Solution system, based on data from United Nations Conference on Trade and Development's Trade Analysis and Information System (TRAINS) database and the World Trade Organization’s (WTO) Integrated Data Base (IDB) and Consolidated Tariff Schedules (CTS) database.
United Nations Conference on Trade and Development, Handbook of Statistics and data files, and International Monetary Fund, International Financial Statistics.
World Bank staff estimates based on the United Nations Population Division's World Urbanization Prospects: 2018 Revision.
Derived from total population. Population source: (1) United Nations Population Division. World Population Prospects: 2019 Revision, (2) Census reports and other statistical publications from national statistical offices, (3) Eurostat: Demographic Statistics, (4) United Nations Statistical Division. Population and Vital Statistics Reprot (various years), (5) U.S. Census Bureau: International Database, and (6) Secretariat of the Pacific Community: Statistics and Demography Programme.
World Bank staff estimates using the World Bank's total population and age/sex distributions of the United Nations Population Division's World Population Prospects: 2019 Revision.
World Bank staff estimates based on age/sex distributions of United Nations Population Division's World Population Prospects: 2019 Revision."	38	336	5	yasirarfat	pakistan-dataset-1960-to-2020
7529	7529	Football Tackles	Is your model smart enough to spot a foul	['football', 'deep learning', 'cnn', 'image data', 'tensorflow']	"Context
The dataset is a collection of images for tackles and clean fouls based on the principle of first contact. The collection have directories with names starting from VAR followed by the approximate number of images present in each class. All other directories is a subset if var600 directory.
Inspiration
Can you build a CNN that can find if an event in football(soccer) is a clean foul or a tackle ? Reference can be found here: https://github.com/aamir09/VarCnn"	3	44	0	zaikali	football-tackles
7530	7530	Salary Variations Dataset		[]		0	12	0	edigasathishgoud	salary-variations-dataset
7531	7531	responden		[]		0	17	1	satriomartawicaksono	responden
7532	7532	densenet121		[]	"Context
hehehehehe"	0	31	1	pangzi233	densenet121
7533	7533	Covid data worldwide		[]	"Task
Put in practice your pandas and visualizations skills with this dataset about the covid pandemic"	2	160	10	maxdiazbattan	covid-data-worldwide
7534	7534	Dataset Kaggle Santa-2021 | Base solution 2440	Solution Improvement Dataset for Kaggle Santa Competition	['movies and tv shows', 'computer science', 'tabular data', 'text data', 'optimization']	"Context
This dataset contains the basic solution for the Santa 2021 - Funny Movie Editing competition.
The dataset consists of three combination blocks, marked with groups. The first and the last block contain permutations starting with 🎅🤶 (must be in all lines of the group), the middle block with the name ""body"" contains the shortest permutations within which there is one more obligatory permutation
Content
This dataset contains the following:
- combinations.csv: the tabular dataset containing data for three groups (column 'line')
- splitted_combs.txt: the text data containing splitted combinations for search wildcards
- show_indents.txt: the text data containing visualization splitted combinations (indents and intersections)
symbols = {
    0: '🌟',  # wildcard
    1: '🎅',  # first start combination
    2: '🤶',  # second start combination
    3: '🦌',
    4: '🧝',
    5: '🎄',
    6: '🎁',
    7: '🎀',
}
Source
Link : https://www.kaggle.com/c/santa-2021"	1	65	0	renokan	dataset-kaggle-santa-2021
7535	7535	Alzheimer_MRI_ds		[]	"Context
This dataset is a copy of the images in the dataset at the link: Alzheimer's Dataset (4 class of Images).
Content
The original dataset contained MRI images of 32 horizontal slices of the brain divided into 4 classes:
- Non Dementia
- Very Mild Dementia
- Mild Dementia
- Moderate Dementia
For each classes there were a different number of subjects:
- 100 subjects for the Non Dementia Class
-  70 subjects for the Very Mild Dementia Class
- 28 subjects for the Mild Dementia Class
- 2 subjects for the Moderate Dementia Class
The problem of the original dataset was that the train and the test sets contained different slices of the brain because the images of the dataset were ordered by the position of the slice and the train/test set division was performed by putting the first percentage of images in the train set and the last ones in the test set.
In this dataset, for each of the 4 classes, the images of the previous division between train and test sets have been grouped and divided in new added directories, one for each subject. Then the subjects of each class have been divided between train and test set with approximately a 80/20 % division. The train set was subsequently divided into 5 folders for each class (labelled from A to E).
For the Moderate Class, as it only consisted in 2 different subjects, one was used in the test set and the other in the train set."	1	48	0	marcopinamonti	alzheimer-mri-ds
7536	7536	Leather Defect detection and Classification	Machine Learning and Deep learning	['education']		9	86	1	praveen2084	leather-defect-classification
7537	7537	Cardiomegaly Disease Prediction Using CNN		['cnn', 'image data', 'binary classification', 'health conditions', 'news', 'tensorflow']	"Context
I want to predict images of Cardiomegaly disease, I did not find specific dataset so create this from original NIH Chest X-ray Dataset.
This data is processed data of original data.
Content
This dataset is processed and taken form Original Dataset.
Using CSV information I separate images of Cardiomegaly and process with CLAHE and resize it into 128*128 height and width.
The training and testing images are in equal portion in ratio of 1:1.
Acknowledgements
I would like to thanks to NIH Clinical Center, provides one of the largest publicly available chest x-ray datasets to scientific community
Inspiration
Anyone can access this data and make Notebook over How to Calculate Cardiomegaly Ratio Measurement (CTR) using following information,
chest X-ray images, radiologists employ CTR as one of the most important indicators of cardiomegaly due to the simplicity of the calculation. CTR of a chest X-ray image is calculated as cardiac diameter (the diameter of the heart) divided by the thoracic diameter (the diameter of the chest). Speciﬁcally, CTR can be calculated from three measurements, MRD, the midline-to-right heart diameter, MLD, the midline-to-left heart diameter, and ID, the internal diameter of chest as
                                                                                                     CTR = (MRD + MLD)/ID
Where, MRD and MLD is measured from the greatest perpendicular diameter from midline to right and left heart border, respectively. Figure 1 visualizes the details of CTR calculation. A CTR value of 0.5 is generally considered to indicate the upper limit of normal."	32	344	7	rahimanshu	cardiomegaly-disease-prediction-using-cnn
7538	7538	Mall Customer Segmentation	Customer segmentation using k-means	['exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics', 'python']	"Context
This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis . I will demonstrate this by using unsupervised ML technique (KMeans Clustering Algorithm) in the simplest form.
Content
You are owing a supermarket mall and through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score.
Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.
Problem Statement
You own the mall and want to understand the customers like who can be easily converge [Target Customers] so that the sense can be given to marketing team and plan the strategy accordingly.
Acknowledgements
Have a view on complete implementation of customer segmentation from the below provided link
https://github.com/NelakurthiSudheer/Mall-Customers-Segmentation 
Inspiration
By the end of this case study , you would be able to answer below questions.
1- How to achieve customer segmentation using machine learning algorithm (KMeans Clustering) in Python in simplest way.
2- Who are your target customers with whom you can start marketing strategy [easy to converse]
3- How the marketing strategy works in real world"	30	327	1	nelakurthisudheer	mall-customer-segmentation
7539	7539	translate		[]		1	29	0	haowu11	translate
7540	7540	tabular-playground-series-jan-2022		[]		0	15	0	mathurinache	tabularplaygroundseriesjan2022
7541	7541	debug-longformer-baseline-colab-cm-f0-cv-0.2155		[]		0	5	0	atharvaingle	debug-longformer-baseline-colab-cm-f0
7542	7542	🇮🇳 Indian Startup Funding Of Year 2021 (Dataset)	Indian Startup Funding 2021 ( Jan to Dec )	['india', 'business', 'finance', 'investing']	"Indian Startup Funding 2021
Information of Indian Startup Funding 2021 ( From Jan to Dec )
Data will update for December month in beginning of year 2022
Note (added on Jan 3 2022): December month data also included with latest version
Source: Startup Talky ( startuptalky . com )
Note: We found some data in amount column show unuseful information (i.e information as some data with string), which seems some issue from data source (i.e above mentioned source website)"	76	520	7	kdsharmaai	indian-startup-funding-2021
7543	7543	Swinlarge224Fastai10kf		[]		0	11	0	mithilsalunkhe	swinlarge224fastai10kf
7544	7544	new_dataset		[]		0	10	0	adiiind	new-dataset
7545	7545	Indian Premier League		['football']		2	12	1	miteshchaudhary	indian-premier-league
7546	7546	toxic_train_data		[]		1	35	0	toru59er	toxic-train-data
7547	7547	weights		['exercise']		0	25	0	ahmadyasser01	weights
7548	7548	jigsaw_20220103_multi		[]		0	6	0	qinyukun	jigsaw-20220103-multi
7549	7549	Best Cities and Countries for Startups	Dataset is about the best cities and countries for startups in the world (2021)	['geography', 'business', 'finance', 'data analytics', 'investing']	A startup or start-up is a company or project undertaken by an entrepreneur to seek, develop, and validate a scalable business model.1 While entrepreneurship refers to all new businesses, including self-employment and businesses that never intend to become registered, startups refer to new businesses that intend to grow large beyond the solo founder.3 At the beginning, startups face high uncertainty4 and have high rates of failure, but a minority of them do go on to be successful and influential.5 Some startups become unicorns; that is privately held startup companies valued at over US$1 billion.	1164	8296	50	ramjasmaurya	best-cities-and-countries-for-startups-in-2021
7550	7550	pretrained		[]		0	14	0	michaelmelnikov	pretrained
7551	7551	test yi xia 		['arts and entertainment']		0	23	0	kohkaixuan	test-yi-xia
7552	7552	Top 1000 Kaggle Datasets	Kaggle's most popular datasets 	['websites', 'global', 'computer science', 'data visualization', 'pandas']	"From wiki
Kaggle, a subsidiary of Google LLC, is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish data sets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.
Kaggle got its start in 2010 by offering machine learning competitions and now also offers a public data platform, a cloud-based workbench for data science, and Artificial Intelligence education. Its key personnel were Anthony Goldbloom and Jeremy Howard. Nicholas Gruen was founding chair succeeded by Max Levchin. Equity was raised in 2011 valuing the company at $25 million. On 8 March 2017, Google announced that they were acquiring Kaggle.1
Source:
Kaggle"	216	2755	13	notkrishna	top-1000-kaggle-datasets
7553	7553	Maddison Project Dataset 2020 Population by Region	Maddison Project Database 2020	['history', 'demographics']	"Context
The Maddison Project Database provides information on comparative economic growth and income levels over the very long run. The 2020 version of this database covers 169 countries and the period up to 2018. For questions not covered in the documentation, please contact maddison@rug.nl.
Content
We now offer a new 2020 update of the Maddison Project database, which uses a different methodology compared to the 2018 update. The approach of the 2018 update is identical to that of Penn World Tables, and consistent with recent economic and statistical research in this field. However, applying this approach systematically results in historical outcomes that are not consistent with current insights by economic historians, as explained in Bolt and Van Zanden (2020).
The 2020 update has to some extent gone back to the original Maddison approach to remedy for this (see documentation). Both the 2018 and the 2020 datasets incorporate the available recent work by economic historians on long term economic growth, the 2020 is most complete in this respect.
Acknowledgements
Attribution requirement -
All original papers must be cited when:
the data is shown in any graphical form
subsets of the full dataset that include less than a dozen (12) countries are used for statistical analysis or any other purposes
A list of original papers can be found in the source sheet of the database. When neither a) or b) apply, then the MPD as a whole should be cited.
Maddison Project Database, version 2020. Bolt, Jutta and Jan Luiten van Zanden (2020), “Maddison style estimates of the evolution of the world economy. A new 2020 update ”.
Inspiration
You can find some inspiration here : https://ourworldindata.org/global-economic-inequality-introduction"	9	113	6	mathurinache	maddison-project-dataset-2020-population-by-region
7554	7554	Maddison Population by Country	Maddison Project Database 2020	['history', 'demographics', 'economics']	"Context
The Maddison Project Database provides information on comparative economic growth and income levels over the very long run. The 2020 version of this database covers 169 countries and the period up to 2018. For questions not covered in the documentation, please contact maddison@rug.nl.
Content
We now offer a new 2020 update of the Maddison Project database, which uses a different methodology compared to the 2018 update. The approach of the 2018 update is identical to that of Penn World Tables, and consistent with recent economic and statistical research in this field. However, applying this approach systematically results in historical outcomes that are not consistent with current insights by economic historians, as explained in Bolt and Van Zanden (2020).
The 2020 update has to some extent gone back to the original Maddison approach to remedy for this (see documentation). Both the 2018 and the 2020 datasets incorporate the available recent work by economic historians on long term economic growth, the 2020 is most complete in this respect.
Acknowledgements
Attribution requirement -
All original papers must be cited when:
the data is shown in any graphical form
subsets of the full dataset that include less than a dozen (12) countries are used for statistical analysis or any other purposes
A list of original papers can be found in the source sheet of the database. When neither a) or b) apply, then the MPD as a whole should be cited.
Maddison Project Database, version 2020. Bolt, Jutta and Jan Luiten van Zanden (2020), “Maddison style estimates of the evolution of the world economy. A new 2020 update ”.
Inspiration
You can find some inspiration here : https://ourworldindata.org/global-economic-inequality-introduction"	10	95	7	mathurinache	maddison-population-by-country
7555	7555	SeaShip		[]		8	36	0	tangwenyang	seaship
7556	7556	Maddison Project Dataset 2020 GDP by Country	Maddison Project Database 2020	['history', 'business', 'demographics', 'economics']	"Context
The Maddison Project Database provides information on comparative economic growth and income levels over the very long run. The 2020 version of this database covers 169 countries and the period up to 2018. For questions not covered in the documentation, please contact maddison@rug.nl.
Content
We now offer a new 2020 update of the Maddison Project database, which uses a different methodology compared to the 2018 update. The approach of the 2018 update is identical to that of Penn World Tables, and consistent with recent economic and statistical research in this field. However, applying this approach systematically results in historical outcomes that are not consistent with current insights by economic historians, as explained in Bolt and Van Zanden (2020).
The 2020 update has to some extent gone back to the original Maddison approach to remedy for this (see documentation). Both the 2018 and the 2020 datasets incorporate the available recent work by economic historians on long term economic growth, the 2020 is most complete in this respect.
Acknowledgements
Attribution requirement -
All original papers must be cited when:
the data is shown in any graphical form
subsets of the full dataset that include less than a dozen (12) countries are used for statistical analysis or any other purposes
A list of original papers can be found in the source sheet of the database. When neither a) or b) apply, then the MPD as a whole should be cited.
Maddison Project Database, version 2020. Bolt, Jutta and Jan Luiten van Zanden (2020), “Maddison style estimates of the evolution of the world economy. A new 2020 update ”.
Inspiration
You can find some inspiration here : https://ourworldindata.org/global-economic-inequality-introduction"	12	80	6	mathurinache	maddison-project-dataset-2020-gdp-by-country
7557	7557	Maddison Project Dataset 2020 Raw	Maddison Project Database 2020	['history', 'business', 'demographics', 'economics']	"Context
The Maddison Project Database provides information on comparative economic growth and income levels over the very long run. The 2020 version of this database covers 169 countries and the period up to 2018. For questions not covered in the documentation, please contact maddison@rug.nl.
Content
We now offer a new 2020 update of the Maddison Project database, which uses a different methodology compared to the 2018 update. The approach of the 2018 update is identical to that of Penn World Tables, and consistent with recent economic and statistical research in this field. However, applying this approach systematically results in historical outcomes that are not consistent with current insights by economic historians, as explained in Bolt and Van Zanden (2020).
The 2020 update has to some extent gone back to the original Maddison approach to remedy for this (see documentation). Both the 2018 and the 2020 datasets incorporate the available recent work by economic historians on long term economic growth, the 2020 is most complete in this respect.
Acknowledgements
Attribution requirement -
All original papers must be cited when:
the data is shown in any graphical form
subsets of the full dataset that include less than a dozen (12) countries are used for statistical analysis or any other purposes
A list of original papers can be found in the source sheet of the database. When neither a) or b) apply, then the MPD as a whole should be cited.
Maddison Project Database, version 2020. Bolt, Jutta and Jan Luiten van Zanden (2020), “Maddison style estimates of the evolution of the world economy. A new 2020 update ”.
Inspiration
You can find some inspiration here : https://ourworldindata.org/global-economic-inequality-introduction"	75	600	16	mathurinache	maddison-project-dataset-2020-raw
7558	7558	ver2_vn_handwriting_data		[]		0	29	0	acousticmusic	ver2-vn-handwriting-data
7559	7559	Eye Movement Data Set for Desktop Activities	An eye-tracking dataset for desktop activity classification task.	['eyes and vision']	"Abstract
The data set consists of raw gaze coordinates (x-y) of 24 participants while doing 8 desktop activities. 
Dataset Information
The dataset consists of raw gaze coordinates of 24 participants while they were performing 8 desktop activities – Read, Browse, Play, Search, Watch, Write, Debug, and Interpret. All the activities except Watch were 5 minutes long. The eye movements were recorded using a desktop mounted Tobii X2-30 eye tracker and Tobii Pro Studio software.
Citation Request
Please cite the below paper if you are using this dataset.
Srivastava, N., Newn, J., & Velloso, E. (2018). Combining Low and Mid-Level Gaze Features for Desktop Activity Recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2(4), 189."	10	114	0	namratasri01	eye-movement-data-set-for-desktop-activities
7560	7560	House_data_prediction		[]	"About this file
What are the things that a potential home buyer considers before purchasing a house? The location, the size of the property, vicinity to offices, schools, parks, restaurants, hospitals or the stereotypical white picket fence? What about the most important factor — the price?
Now with the lingering impact of demonetization, the enforcement of the Real Estate (Regulation and Development) Act (RERA), and the lack of trust in property developers in the city, housing units sold across India in 2017 dropped by 7 percent. In fact, the property prices in Bengaluru fell by almost 5 percent in the second half of 2017, said a study published by property consultancy Knight Frank.
For example, for a potential homeowner, over 9,000 apartment projects and flats for sale are available in the range of ₹42-52 lakh, followed by over 7,100 apartments that are in the ₹52-62 lakh budget segment, says a report by property website Makaan. According to the study, there are over 5,000 projects in the ₹15-25 lakh budget segment followed by those in the ₹34-43 lakh budget category.
Buying a home, especially in a city like Bengaluru, is a tricky choice. While the major factors are usually the same for all metros, there are others to be considered for the Silicon Valley of India. With its help millennial crowd, vibrant culture, great climate and a slew of job opportunities, it is difficult to ascertain the price of a house in Bengaluru."	2	63	0	avnishsharma24	house-data-prediction
7561	7561	 Combined Cycle Power Plant Data Set		[]		0	24	1	shashank52ez	combined-cycle-power-plant-data-set
7562	7562	Starbucks Locations Worldwide 2021 version	Name, ownership type, and location of every Starbucks store in operation	['exercise', 'beginner', 'classification', 'tabular data']	"Context
Starbucks started as a roaster and retailer of whole bean and ground coffee, tea and spices with a single store in Seattle’s Pike Place Market in 1971. The company now operates more than 28,289 retail stores in 49 countries.
Content
This dataset includes a record for every Starbucks or subsidiary store location currently in operation as of Nov 2021
Acknowledgements
This data was scraped from the Starbucks store locator webpage (https://www.starbucks.com/store-locator/store/)
and it's updated data of https://www.kaggle.com/starbucks/store-locations
Inspiration
What city or country has the highest number of Starbucks stores per capita? What two Starbucks locations are the closest in proximity to one another? What location on Earth is farthest from a Starbucks? How has Starbucks expanded overseas?"	889	6574	55	kukuroo3	starbucks-locations-worldwide-2021-version
7563	7563	PetFinder-Z111-20220103065520		[]		3	34	1	hideyukizushi	petfinder-z111-20220103065520
7564	7564	Cryptocurrencies Historical Data	Cryptocurrencies Historical Data	['finance', 'currencies and foreign exchange']	"Context
Cryptocurrencies like bitcoin, ethereum are very  extremely volatile, many have gained a staggering amount of money through it, again many have lost doing so. So I started collecting the data of some cryptocurrencies in a usable form (.csv) and made it public so that anyone can use it to further analyze their risk of investment through code.
Content
The dataset has one csv file for each currency.
Date : Date of observation
Open : Opening price on the given day
High : Highest price on the given day
Low : Lowest price on the given day
Close : Closing price on the given day
Volume : Volume of transactions on the given day
Market Cap : Market capitalization in USD
Circulating Supply: The amount of coins that are circulating in the market and are in public hands.
The Circulating Supply is not directly taken from coinmarketcap, but it is calculated using the formula,
Circulating Supply = Market Cap / Closing price of the respective day
Acknowledgements
This data is taken from coinmarketcap and it is free to use the data.
The cover photo is taken from pixabay, which is free for commercial use and no attribution required
Inspiration
Some of the questions which could be inferred from this dataset are:
Predicting the future price of the currencies
How does the price fluctuations of currencies correlate with each other?
Seasonal trend in the price fluctuations"	106	594	2	mushfirat	106-cryptocurrency-historical-data
7565	7565	fer2013_with_mask		[]		1	14	0	yepuwang	fer2013-with-mask
7566	7566	loguru lib ds		[]		33	101	0	awsaf49	loguru-lib-ds
7567	7567	BDSL thesis datasets		[]		0	29	0	kaziimranahmed	bdsl-thesis-datasets
7568	7568	Great-Barrier-Reef: YOLOv5 [train] ds		[]		180	703	6	awsaf49	greatbarrierreef-yolov5-train-ds
7569	7569	Darknet Run Environment		[]		0	179	0	eshaandeshpande	darknet-run-environment
7570	7570	bbox lib ds		[]		47	152	1	awsaf49	bbox-lib-ds
7571	7571	images		[]		0	23	0	sandeepmohanmuktha	images
7572	7572	Car Evaluation Database	Car Evaluation Database	['beginner', 'intermediate', 'text data']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	15	59	0	shashwat1008	car-evaluation-database
7573	7573	JRSTC_Linear_comp		[]		0	6	0	learnitanyway	jrstc-linear-comp
7574	7574	PetFinder-Z110-20220103052117		[]		0	11	0	hideyukizushi	petfinder-z110-20220103052117
7575	7575	Pokemon go fast moves		['science and technology']		0	30	1	jjjjjjjjjjason	pokemon-go-fast-moves
7576	7576	r9weight		[]		5	36	0	llixxin	r9weight
7577	7577	feedback-ner		[]		2	24	0	guanghan	feedbackner
7578	7578	clearwei		[]		13	56	0	mkagglemm	clearwei
7579	7579	clearwei		[]		13	56	0	mkagglemm	clearwei
7580	7580	AAPL Tick Data 		['business', 'investing']	"About
Data is tick-by-tick trade data for AAPL for November 2021. Courtesy of FirstRateData Tick Data
Format
Data is in csv format as below:
Trade Data : 
{timestamp yyyy-MM-dd HH:mm:ss.ssssss},{price},{volume},{exchange code (see below)},{trade conditions (see below)}
Exchange Codes
1 - Nasdaq
2 - NYSE 
3 - NYSE American
4 - FINRA
5 - Nasdaq OMX
6 - NYSE National
7 - Cboe
8 - NYSE Arca
9 - Investors Exchange
10 - International Securities Exchange
11 - Cboe BATS
13 - Nasdaq Philadelphia
Trade Codes
A - Acquisition (Trade executed as a result of an Exchange acquisition) 
B - Odd Lot Trade (Trade volume not in a defined trading lot for the security)
C - Official Close (Trade identified as the exchange offical close trade)
D - Derivatively Priced (Trade not based on market quoatations).
E - Extended Hours (Trade executed ourside of regualar exchange trading hours - Form T)
I - Distribution (Trade made as part of a larger block trade)
L - Seller's Option (Trade executed with flexible devilery of  the seller's choice, with a maximum of T+60)
M - Trade Thru Exempt (Trade has no condition attached that it is executed at the most favorable price across multiple exchanges)
N - Next Day (Trade settling on the next trading day T+1)
O - Official Open (Trade identified as the exchange offical open trade)
P - Prior Reference Price Trade (Trade executed at a price prior to the contemporarily traded prices)
S - Intermarket Sweep (Trade was the execution due to an Intermarket Sweep Order.)
T - Cash Sale (Trade requiring delivery of securities and payment on the same day the trade - T+0)"	2	22	0	johnkd	aapl-tick-data
7581	7581	Classification of Ripeness Stage of Mango Fruit	Biochemical, physical, and electrical properties of Nam Dok Mai Si Tong	['food']	"Context
Mangoes of the variety Nam Dok Mai Si Tong at a commercial orchard in Nakorn Ratchasima province, Thailand, were tagged in November 2018, when they were approximately 5 mm in diameter. The commercial maturation stage for Num Dok Mai Si Tong for export has been determined to be 85-95 day after fruit set (DAFS). From January to February 2019, the mangoes were harvested four times: 80, 90, 100, and 110 DAFS.  Mango samples were brought to a laboratory at Kasetsart University in Bangkok on the same day they were harvested. At each harvesting, we divided mango samples into two groups: one which contained twenty-five samples for evaluation of biochemical, physical, and electrical properties; and the other, which consisted of five samples for measurement of physical and electrical qualities. The mango samples in both groups were measured on days 1, 3, 5, 7, and 9 after harvest. The biochemical examination of mango samples in one group was not carried out so that the same mangoes were tested for 9 days. A total of 120 mango fruits were used in this research.
Content
Train_Data.csv contains biochemical, physical, and electrical properties of 100 mango fruits.
Validate_Data.csv contains physical, and electrical properties of 20 mango fruits.
There are 8 parameters as follows.
W is weight of mango fruit in grams measured by an electronic balance (DH-2000, China) accurate to 0.01 g. 
C and V are the capacitance of mango in pF and the voltage across the plates of the parallel-plate capacitor sensor in mV, respectively, measured using methods as described in previous works 1 with a hand-held LCR Meter (Keysight, U1733C) set to a frequency of 100 kHz (Figure. 1).
R, G, and B are the average of RGB color values of the skin colors at the top (near stem), center, and bottom of each fruit on both sides of the mangoes measured using ColorMax sensors (EMX Industries, CM1000-7-25, USA).
TA and TSS are titratable acidity and total soluble solids determined from the extracted mango juice using a digital refractometer (ATAGO, PAL-1, Japan) and a pH-meter (HANNA, HI98127, USA). The extracted mango juice was manually titrated to pH 8.1 with 0.1 mol/L NaOH, for TA.
&gt; 1 Juansah, J.; Budiastra, I.W.; Dahlan, K.; K. B. Seminar Electrical properties of garut citrus fruits at low alternating current signal and its correlation with physicochemical properties during maturation. Int. J. Food Prop. 2014, 17, 1498–1517."	8	146	0	denchai	classification-of-ripeness-stage-of-mango-fruit
7582	7582	S&P100stocks		[]		9	297	0	takahiro1127	sp100stocks
7583	7583	exp044-044-beit-large-chaug-mixup-chbs		[]		0	3	0	kurokurob	exp044-044-beit-large-chaug-mixup-chbs
7584	7584	AWSAF_Best		[]		0	17	0	r26104039	awsaf-best
7585	7585	Android Phones	Dataset of various Android phones	['mobile and wireless', 'os']	"Context
Android is the most used operating systems in the mobile phones field, it would be interesting to explore the different manufacturers and devices that uses it and which versions of Android operating system are widely used
Content
The data has about 1300 rows including 4 attributes described as following:
Name: Mobile phone name
Brand: Manufacturer brand name
Release: Release date of the mobile
Version: Android version of the mobile
Questions to be answered
How many phones use Android 11 ?
Which phones were released the latest ?
Which brand has the most phones released ?
How many brands are there"	176	1418	25	khaiid	android-phones
7586	7586	Borsa Istanbul: Turkish Stock Exchange Dataset	Historical Equity Prices of Various Companies In Sole Exchange Entity of Turkey	['business', 'finance', 'time series analysis', 'tabular data', 'investing']	"Introduction
The Borsa Istanbul (abbreviated as BIST) is the sole exchange entity of Turkey combining the former Istanbul Stock Exchange (ISE), the Istanbul Gold Exchange, and the Derivatives Exchange of Turkey under one umbrella. It was established as an incorporated company on April 3, 2013, and began to operate on April 5, 2013.
BIST is the only corporation in Turkey for securities exchange established to provide trading in equities, bonds and bills, revenue-sharing certificates, private sector bonds, foreign securities, and real estate certificates as well as international securities. BIST is home to 320 national companies. Trading hours are 09:30–12:30 for the first session and 14:00–17:30 for the second session, on workdays. All BIST members are incorporated banks and brokerage houses.
BIST price indices are computed and published throughout the trading session while the return indices are calculated and published at the close of the session only. The most common indices are BIST-100, BIST-50, BIST-30.
For detailed information: Borsa Istanbul
Dataset
This dataset currently contains the aggregated daily numerical indicators (i.e.: min-max price, trade volume, and, etc..) for each equity of a certain company and is saved as CSV file. It contains data between 2015-11-30 and 2021-12-21.
Updates
Depending on the interest shown in the data, updates will be made in the future. Additional suggestions and recommendations are welcomed.
Version 1: Initial publishing.
References
Wikipedia
[Borsa Istanbul's Official Website] (https://www.borsaistanbul.com/en)"	6	51	0	gokhankesler	borsa-istanbul-turkish-stock-exchange-dataset
7587	7587	yolox_darknet		[]		0	24	0	getcoin	yolox-darknet
7588	7588	checkpoint		['law']		0	55	0	minhhust	checkpoint
7589	7589	yolox_x		[]		0	3	0	getcoin	yolox-x
7590	7590	2021-ohio-hospitalizations		[]		1	7	0	trishharness	2021ohiohospitalizations
7591	7591	jigsaw_20220103		[]		0	7	0	qinyukun	jigsaw-20220103
7592	7592	SDA using python		['earth and nature']		1	29	0	nobody0101	sda-using-python
7593	7593	AAPL Historical Intraday Dataset		['business', 'investing']	"About
The is a complete dataset of 1-minute bars from 2010 to 2019. All prices and volumes are adjusted for dividends and splits. Data sourced from FirstRate Intraday Stock Data
Format
Data format is csv with the below structure:
{timestamp},{open},{high},{low},{close},{volume}"	4	27	0	johnkd	aapl-historical-intraday-dataset
7594	7594	swin_large_patch4_window7_224_7_folds_pusz7m0i		[]		0	23	0	reighns	swin-large-patch4-window7-224-7-folds-pusz7m0i
7595	7595	BIRD_FOR_PMG		[]		1	12	0	jiantengfei	bird-for-pmg
7596	7596	Model 2 Version 2		[]		0	50	0	hanszhou615	model-2-version-2
7597	7597	NSW postcode to LGA	Postcode to LGA (Local Government Area) table in New South Wales (NSW)	['australia', 'geography']	"Context
In the course of analysing COVID-19 data in NSW, I prepared this data table to help convert postcodes to LGAs.
Content
NSW postcodes and the LGAs they belong to."	3	36	1	henrylin03	nsw-postcode-to-lga
7598	7598	oofvtf		[]		1	14	1	kaggleqrdl	oofvtf
7599	7599	FilledTrainData		[]		2	10	0	axzhang	filledtraindata
7600	7600	Galaxies-Test		['astronomy']		1	9	0	mauriciominguini	galaxiestest
7601	7601	Titanic		[]		0	19	0	mustiztemiz	titanic
7602	7602	Pawpularity with EfficientNetB2 output		[]		2	12	1	lonnieqin	pawpularity-with-efficientnetb2-output
7603	7603	OH_2021_vax		[]		2	1	0	trishharness	oh-2021-vax
7604	7604	Predicting Who Gets Vax or Dies of COVID		[]		0	19	0	trishharness	predicting-who-gets-vax-or-dies-of-covid
7605	7605	Top 250 Movies IMDB	scraped from https://www.imdb.com/chart/top/?ref_=nv_mv_250	['movies and tv shows']		5	78	0	serdarcaglar	top-250-movies-imdb
7606	7606	iris_v2		[]		1	3	3	aerospacer	iris-v2
7607	7607	Prices of top cryptocurrencies	Today's Cryptocurrency Prices by Market Cap	['bayesian statistics', 'finance', 'text mining', 'time series analysis', 'currencies and foreign exchange']	"Context
Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and this post helped me get started. Once the basics are done, the data scientist inside me started raising questions like:
How many cryptocurrencies are there and what are their prices and valuations?
Why is there a sudden surge in the interest in recent days?
So what next?
Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to Blockchain Info, I was able to get quite a few parameters on once in two day basis.
This will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.
Content
The dataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013. This dataset has the historical price information of some of the top crypto currencies by market capitalization.
Date : date of observation (1st jan 2014 to 1st jan 2022)
Open : Opening price on the given day
High : Highest price on the given day
Low : Lowest price on the given day
Close : Closing price on the given day
Volume : Volume of transactions on the given day
Market cap-The Capital of this coin"	24	179	3	kuntalmaity	prices-of-top-cryptocurrencies
7608	7608	iris.csv		['internet']		6	9	3	aerospacer	iriscsv
7609	7609	Cyclistic Bike-Sharing Analysis Case Study		[]		0	19	0	cherylhan	cyclistic-bikesharing-analysis-case-study
7610	7610	boxplot		['software']		0	10	0	abrahamanderson	boxplot
7611	7611	Cypriot Tourism prospects rmd	*Volume and spending habits.    *  Tendencies.    *  Corona's impact.	['travel']		7	60	1	antonioskokiantonis	cypriot-tourism-prospects-rmd
7612	7612	dataCrime20062008		[]		1	21	1	dataqest	datacrime20062008
7613	7613	jigsaw-toxic-comment-classification-challenge		[]		2	6	0	panser	jigsawtoxiccommentclassificationchallenge
7614	7614	shoppingsXpopulation_BRAZIL		[]		0	53	1	victoraugustocavalli	shoppingsxpopulation-brazil
7615	7615	Doge Coin: An explosion	see the doge coin price explosion in this last year.	['history', 'finance', 'economics', 'investing', 'currencies and foreign exchange']	"Context🔥
this Dataset contains the doge coin prices in 2019-Now
Content💯
Doge Coin prices with details, open, close, low and high prices. open and close and all related dates.
API that I got the results of is CoinAPI:
with free plan you can access rest api i put the link below so you can also use it.
https://www.coinapi.io/
Acknowledgements✔️
thanks to CoinAPI  for this amazing service.
I will be happy if you vote up it and follow my kaggle profile.😃 
I did the same thing for bitcoin: https://www.kaggle.com/cyruskouhyar/btcprices2015now"	10	123	6	cyruskouhyar	doge-coin-an-explosion
7616	7616	MyHateBertPretrained		[]		0	8	0	panser	myhatebertpretrained
7617	7617	rapids svr	swin transform and NN and SVR	['games']		1	45	0	hanszhou615	rapids-svr
7618	7618	White_Fly_Detection_Complete		[]		0	15	0	aqeeljajjacui	white-fly-detection-complete
7619	7619	Mumbai Vaccination		[]		0	40	0	afrozin	mumbai-vaccination
7620	7620	CSE497 Datasets		[]		0	16	0	kaziimran21	cse497-datasets
7621	7621	Astronomical spectral 2		['astronomy']		0	34	0	mohamedishag	astronomical-spectral-2
7622	7622	play_gifs		[]		1	14	0	liamdao	play-gifs
7623	7623	weights		['exercise']		0	45	0	timmy106340103	weights
7624	7624	Medical no-show-2016		[]		0	19	0	essamsabbah	medical-noshow2016
7625	7625	La Liga Match Data 	La Liga Match Data from 2014-2020	['football', 'beginner', 'intermediate', 'exploratory data analysis', 'data visualization']	"La Liga is by far one of the world’s most entertaining leagues. They have some of the best managers, players and fans! But, what makes it truly entertaining is the sheer unpredictability. There are 6 equally amazing teams with a different team lifting the trophy every season. Not only that, the league has also witnessed victories from teams outside of the top 6. So, let us analyze some of these instances.
So far, the implementation of statistics into soccer has been positive. Teams can easily gather information about their opponents and their tactics. This convenience allows managers to create well-thought out game plans that suit their team, maximize opponents' weaknesses, and increase their chances of winning.
A goal is scored when the whole of the ball passes over the goal line, between the goalposts and under the crossbar, provided that no offence has been committed by the team scoring the goal. If the goalkeeper throws the ball directly into the opponents' goal, a goal kick is awarded.
THE TIME OF SEASON/MOTIVATION: While a club battling for a league title is going to be hungry for a win, as is a side that is fighting to stay up, a club that has already won the title or has already been relegated is unlikely to work as hard, and often rest players as well.
THE REFEREE: Of course, when referee's send players off it make a massive impact on a match, but even if he is just awarding a yellow card then it can affect the outcome of the game as the player booked is less likely to go in as hard for the rest of the match.
SUBSTITUTES: The whole point of substitutes is for them to be able to come on and impact a match. Subs not only bring on a fresh pair of legs that are less tired than starters and more likely to track back and push forward, but can also play crucial roles in the formation of a team.
MIND GAMES/MANAGERS: Playing mind games has almost become a regular routine for top level managers, and rightly so. Just a simple mind game can do so much to impact a match, a good example coming from Sir Alex Ferguson.
Per his autobiography, when Manchester United were losing late on in a match at a certain point he would tap his watch and make sure to let the opposition know he is signalling this to his players. United's opposition already know that United have a tendency to come back from behind, and upon seeing this gesture they will think that United are going to come back. And because scientific studies prove that living creatures are more likely to accept things that have happened before than not - horses are more likely to lose to a horse they have already lost to in a race even if they are on an even playing field - they often succumb to a loss.
FORM/INJURIES/FIXTURES: A team on better form is more likely to win a match than if they have been on a poor run of form, while a team in the middle of a condensed run of fixtures is less likely to win than a well rested team.
These are just some of the things that affect matches - if you have any other just mention them in the comment section below and I'll try to add them in!"	130	917	14	sanjeetsinghnaik	la-liga-match-data
7626	7626	Zomato_data		[]		5	34	1	virajoak	zomato-data
7627	7627	covid 19 data 		[]		1	26	0	saba1999	covid-19-data
7628	7628	fullpreds		[]		1	5	0	trevorhughes	fullpreds
7629	7629	GBRfold0		[]		1	5	0	phoenix9032	gbrfold0
7630	7630	How I Met Your Mother Episodes Data	HIMYM Episode Ratings and Descriptions	['arts and entertainment', 'movies and tv shows', 'beginner', 'tabular data']	"Context
Series created by: Carter Bays and Craig Thomas
Number of seasons: 9
Number of episodes: 208
Original air dates: September 19, 2005 – March 31, 2014
Content
Data was acquired through downloading IMDb TV episodes datasets and scraping information from Wikipedia.
Acknowledgements
Thanks to IMDb, Wikipedia, and community curators.
Use
It should be easy to join these data files together on Title and Air Date fields to compare (for example) US viewers and IMDb ratings.
Motivation
I wanted to share a dataset about How I Met Your Mother,  one of my favorite TV shows to binge watch."	59	500	7	bcruise	how-i-met-your-mother-episodes-data
7631	7631	Images_Data		[]		0	10	0	karthikkusuma	images-data
7632	7632	python		['computer science', 'programming']		0	29	0	timmy106340103	python
7633	7633	BANKMARKETING		[]		5	32	1	suleakay	bankmarketing
7634	7634	CokeTest		[]		0	7	0	luatvytran	coketest
7635	7635	timmmmmmmmmmm		[]		0	14	0	hanszhou615	timmmmmmmmmmm
7636	7636	Istanbul October 2020 Airbnb Dataset		[]		4	28	0	ezgitukel	istanbul-october-2020-airbnb-dataset
7637	7637	Most Popular Programming Languages Since 2004	Popularity of Programming Languages from 2004 to 2022	['computer science', 'programming']	"Context
Well, I was looking for a Most Popular Programming Languages dataset for my YouTube channel video and couldn't find anything decent. So, I collect it for my use and share it.
Content
This dataset contains data about the Most Popular Programming Languages from 2004 to 2022. All Programming Languages values are in percentage form out of 100 %
Acknowledgements
The data was pulled from https://pypl.github.io
If this dataset is useful for you then don't forget to upvote."	1572	9736	37	muhammadkhalid	most-popular-programming-languages-since-2004
7638	7638	Chess (King-Rook vs. King-Pawn) Data Set	 King+Rook versus King+Pawn on a7 	['games', 'categorical data']		45	660	13	ananthr1	chess-kingrook-vs-kingpawn-data-set
7639	7639	List of Smartphones Released in India in 2021 	Excel file consisting of smartphone specifications 	['data analytics', 'matplotlib', 'pandas', 'seaborn', 'python']	It is an Excel file which consist of smartphone specifications.	12	61	2	sachinsurve	list-smartphones-released-in-india-in-2021
7640	7640	jigsaw_20220102_1		[]		0	3	0	qinyukun	jigsaw-20220102-1
7641	7641	Most Popular IDE Since 2004	Most Popular Integrated Development Environment from 2004 to 2022	['earth and nature', 'software']	"Context
Well, I was looking for a Most Popular IDE dataset for my YouTube channel video and couldn't find anything decent. So, I collect it for my use and share it.
Content
This dataset contains data about the Most Popular IDE from 2004 to 2022. All IDE values are in percentage form out of 100 %
Acknowledgements
The data was pulled from https://pypl.github.io
If this dataset is useful for you then don't forget to upvote."	64	981	4	muhammadkhalid	most-popular-ide-since-2004
7642	7642	dataset 30_7 nside 64		[]		0	3	0	lino08	dataset-30-7
7643	7643	matches		[]		0	3	0	krishnasingh14	matches
7644	7644	RepoRec2		[]		1	42	0	sciannameaandrea	reporec2
7645	7645	iris.csv		['internet']		2	10	0	chowdeswarreddy	iriscsv
7646	7646	matches.cv		['jobs and career']		1	2	0	krishnasingh14	matchescv
7647	7647	cars.csv	cars csv dataset file	['automobiles and vehicles']		6	115	0	chowdeswarreddy	carscsv
7648	7648	cvdataset1		[]		0	37	0	hongxiawang	cvdataset1
7649	7649	bit-m-r50x1		[]		0	8	0	redhunter20	bitmr50x1
7650	7650	World Population Country - Year		[]		4	52	0	juanperossa	world-population-country-year
7651	7651	Human Emotions Dataset(HES)		['earth and nature', 'classification', 'image data', 'keras', 'tensorflow', 'pytorch']		22	159	0	muhammadhananasghar	human-emotions-datasethes
7652	7652	mfcc_small		[]		1	34	0	giuseppemagazz	mfcc-small
7653	7653	PowerPred		[]		0	29	0	prabakaranchandran	powerpred
7654	7654	yuandankuaile		[]		1	29	0	yygqwjl	yuandankuaile
7655	7655	Sazka_history	Eurojackpot and Sportka history	['history']		0	19	0	jakubslavicek	s-sorte
7656	7656	Dutch Proverbs	Popular sayings and phrases from the Netherlands	['languages', 'literature', 'nlp', 'text mining', 'text data']		5	42	2	levrex	dutch-proverbs
7657	7657	World of Warcraft PvP Ladder		[]		1	31	0	mikkkttim	world-of-warcraft-pvp-ladder
7658	7658	P_Pr_k=5_swin		[]		0	10	0	durgammohanpranay99	p-pr-k5-swin
7659	7659	yhxwfx_hongloumeng		[]		0	18	0	huchenjiang	yhxwfx-hongloumeng
7660	7660	123456		[]		0	5	0	richard0218	123456
7661	7661	toutiao		[]		1	47	1	ioslide	toutiao
7662	7662	taipei		[]		0	18	0	richard0218	taipei
7663	7663	gt-files		['online communities']		0	26	0	shuhuizi	gtfiles
7664	7664	IDRID hard exudates		['health']		0	13	0	kushv16	idrid-hard-exudates
7665	7665	CYCLISTIC Bike_Share_Analysis	Analyzing historical bike trip data to identify trends and gain insight	['cycling', 'data cleaning', 'data visualization', 'data analytics', 'r']	"Context
Exploring historical trip data to discover trends and insight on how the various consumer segments use bike differently.  These insights and trends will inform the marketing team to develop data driven strategy to convert the casual riders to the more profitable annual membership consumer base. 
Content
The historical trip data was provided to us in csv files. It was in quarterly report. I all 4 2019 quarter trip data was imported and combined to a single data frame.
The original dataset can be found from : 
https://divvy-tripdata.s3.amazonaws.com/index.html"	4	71	0	evanskoffi	cyclistic-bike-share-analysis
7666	7666	MBBL Elo Database		[]		0	149	0	clairefredriksson	mbbl-elo-database
7667	7667	Customer Segmentation Dataset	Unsupervised Learning Online Retail Customer Segmentation	['business', 'computer science', 'beginner', 'clustering', 'retail and shopping']	"Description:
A company that sells some of the product, and you want to know how well does the selling performance of the product. You have the data that can we analyze, but what kind of analysis that we can do? Well, we can segment customers based on their buying behavior on the market.
Keep in mind that the data is really huge, and we can not analyze it using our bare eyes. We will use machine learning algorithms and the power of computing for it.
This project will show you how to cluster customers on segments based on their behavior using the K-Means algorithm in Python.
I hope that this project will help you on how to do customer segmentation step-by-step from preparing the data to cluster it.
Acknowledgements:
This dataset has been referred from UCI ML Repository:
https://archive.ics.uci.edu/ml/datasets/online+retail
Objective:
Understand the Dataset & cleanup (if required).
Build a clustering model to segment the customer-based similarity.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	727	4814	30	yasserh	customer-segmentation-dataset
7668	7668	Gan OPC dataset		[]		2	10	0	malk1510	gan-opc-dataset
7669	7669	oldcar		[]		0	14	0	dingdong18k	oldcar
7670	7670	roberta-02d-768-256-Lnorm-0.2d-relu-1-10fold-vafit		[]		0	8	0	qhshao	roberta02d768256lnorm02drelu110foldvafit
7671	7671	dbpedia_kg_dataset		[]		2	25	0	eldestinofeng	dbpedia-kg-dataset
7672	7672	Katonic_unbalanced_classification_dataset		[]		0	13	0	prashanthsheri	katonic-unbalanced-classification-dataset
7673	7673	1.9M+ COVID-19 Tweets	#covid-19 tweets in different languages	['biology', 'text data', 'online communities', 'covid19']	"Context
The dataset contains tweets that are used #covid-19 as a hashtag.
Content
The data contains the following information:
Username: Username of the person who tweeted
User Display Name: Name of the person who tweeted
User Description: Description of the person who tweeted
User Location: Location of the person who tweeted
User Verified: Whether the person who tweeted is verified
User Protected: Whether the person who tweeted is protected
User Followers Count: Number of followers of the person who tweeted
User Friends Count: Number of friends of the person who tweeted
User Statuses Count: Number of tweets of the person who tweeted
User Favourites Count: Number of favourites of the person who tweeted
Tweet Content: Content of the tweet
Tweet Language: Language of the tweet
Tweet Retweet Count: Number of retweets of the tweet
Tweet Quote Count: Number of quotes of the tweet
Tweet Reply Count: Number of replies of the tweet
Tweet Like Count: Number of likes of the tweet
Tweet ID: ID of the tweet
Tweet URL: URL of the tweet
Tweet Date: Date of the tweet"	19	186	3	oktayozturk010	19m-covid19-tweets
7674	7674	Best in League of Legend history	Best Players and Teams	['games', 'video games', 'sports', 'history']	"Context
League of Legends is one of the most popular computer games in the world. The entertainment created by Riot Games has become one of the most popular esports disciplines and is very popular all over the world (especially in Asia and Europe). During the 10-year history of the game, many great players and teams that this database is about have participated in the competition.
Content
The database contains 2 tables: the best players and the best teams looking at all the Legue of Legend games. The criteria for rankings were money earned on tournaments. The data was taken from Liquipedia which is synonymous with wikipedia for esports competitions. The tables were obtained using the ""rvest"" package in R 4.0 and the ""SelectorGadet"" add-on, which made the work with the site easier. Downloaded pages are not listed as disallow in ""robots.txt"". Each line from the database corresponds to the team/player accordingly.
Inspiration
This data can be combined with other League of Legend data to enhance your notebook or add to other esports related analytics. Anyway, how you use the data is entirely up to you!"	127	1635	22	michau96	the-best-in-league-of-legend-history
7675	7675	World Population Data	World Population Growth	['africa', 'social science', 'numpy', 'pandas', 'plotly', 'python']	Population in the world is currently (2020) growing at a rate of around 1.05% per year (down from 1.08% in 2019, 1.10% in 2018, and 1.12% in 2017). The current average population increase is estimated at 81 million people per year. Annual growth rate reached its peak in the late 1960s, when it was at around 2%.	60	350	3	kuntalmaity	world-population-data
7676	7676	boxing		[]		0	24	0	lowellsingson	boxing
7677	7677	Literature search on TB-HIV infection in SA		[]	"Context
The electronic database was searched to estimate the prevalence of HIV in TB patients in sub-Saharan Africa.
Content
The data contains a total of 68 published studies on tuberculosis and HIV from 1990 to 2017 in sub_shaan Africa countries. Authors_name, publication_year, country, Africa_georegion, tuberculosis_case(cases), PLWHA(total), population_type(pop), mean_age, types_TB, population_HIV_prop(pop_HIV1) and population_TB_prop(pop_TB1) were the columns included. The details are available elsewhere https://doi.org/10.1007/s10461-018-02386-4."	0	14	0	yalemgelaw	literature-search-on-tbhiv-infection-in-sa
7678	7678	Big-5-OCEAN-model-for-personality-detection		[]		1539	1132	0	eddiebrock	big5oceanmodelforpersonalitydetection
7679	7679	Airports	Information of Airports from around the world and their geographical data	['aviation', 'intermediate', 'tabular data', 'travel']	"I built a CO2 calculator for flights for my client and here is one interesting dataset I came across. Information regarding AIrports and its location are provided from this dataset and also you can find the distance between them using the latitude and the longitude.
Column Info:
Airport_ID: Specific code for airport given by OpenFlights
Name: Name of the Airport
City: City where the airport is located
Country: Country where the airport is located
IATA: Specific code given by IATA(3 letters)
ICAO: Specific code given by ICAO(4 letters)
Latitude: Geographical Information in 6 digits
Longitude: Geographical Information in 6 digits
Altitude: In feet
Timezone: Hours offset from UTC
DST: Daylight savings time
TZ: Timezone region"	352	2317	17	thoudamyoihenba	airports
7680	7680	sentinel		['news']		0	22	0	kyriakitychola	sentinel
7681	7681	Fastai 384 Pred		[]		0	28	0	mithilsalunkhe	fastai-384-pred
7682	7682	efficientnet2		[]		0	12	0	ohmikawa	efficientnet2
7683	7683	ntpu_tfrec		[]		0	14	0	aa19990416	ntpu-tfrec
7684	7684	test22		[]		1	11	1	testsib	test22
7685	7685	encsat		[]		1	83	0	unfriendlyai	encsat
7686	7686	Bitcoin Trust Weighted Signed Networks (SNAP)	Graphs from the Stanford Network Analysis Platform	['banking', 'investing', 'currencies and foreign exchange', 'online communities']	"Bitcoin Alpha trust weighted signed network
https://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html               
Dataset information                                                      
This is who-trusts-whom network of people who trade using Bitcoin on a 
platform called Bitcoin Alpha (http://www.btcalpha.com/). Since Bitcoin
users are anonymous, there is a need to maintain a record of users'    
reputation to prevent transactions with fraudulent and risky users. Members
of Bitcoin Alpha rate other members in a scale of -10 (total distrust) to
+10 (total trust) in steps of 1. This is the first explicit weighted signed
directed network available for research.                                 
Dataset statistics                                                     
Nodes   3,783                                                          
Edges   24,186                                                         
Range of edge weight    -10 to +10                                     
Percentage of positive edges    93%                                      
Similar network from another Bitcoin platform, Bitcoin OTC, is available at
https://snap.stanford.edu/data/soc-sign-bitcoinotc.html (and as        
SNAP/bitcoin-otc in the SuiteSparse Matrix Collection).                  
Source (citation) Please cite the following paper if you use this dataset: 
S. Kumar, F. Spezzano, V.S. Subrahmanian, C. Faloutsos. Edge Weight    
Prediction in Weighted Signed Networks. IEEE International Conference on 
Data Mining (ICDM), 2016.                                              
http://cs.stanford.edu/~srijan/pubs/wsn-icdm16.pdf                       
The following BibTeX citation can be used:                             
@inproceedings{kumar2016edge,                                          
  title={Edge weight prediction in weighted signed networks},          
  author={Kumar, Srijan and Spezzano, Francesca and                    
     Subrahmanian, VS and Faloutsos, Christos},                        
  booktitle={Data Mining (ICDM), 2016 IEEE 16th Intl. Conf. on},       
  pages={221--230},                                                    
  year={2016},                                                         
  organization={IEEE}                                                  
}                                                                        
The project webpage for this paper, along with its code to calculate two 
signed network metrics---fairness and goodness---is available at       
http://cs.umd.edu/~srijan/wsn/                                           
Files                                                                  
File    Description                                                    
soc-sign-bitcoinalpha.csv.gz                                           
    Weighted Signed Directed Bitcoin Alpha web of trust network          
Data format                                                            
Each line has one rating with the following format:                      
SOURCE, TARGET, RATING, TIME
where                                                                    
SOURCE: node id of source, i.e., rater                               
TARGET: node id of target, i.e., ratee                               
RATING: the source's rating for the target,                          
        ranging from -10 to +10 in steps of 1                        
TIME: the time of the rating, measured as seconds since Epoch.
Notes on inclusion into the SuiteSparse Matrix Collection, July 2018:
The SNAP data set is 1-based, with nodes numbered 1 to 7,604.            
In the SuiteSparse Matrix Collection, Problem.A is the directed rating 
graph, a matrix of size n-by-n with n=3,783, which is the number of    
unique node id's appearing in the SOURCE or TARGET of any edge.          
Problem.aux.nodeid is an array of size 3783 that gives the node id's   
corresponding to each row and column of the matrix.  nodeid(i)=id if the 
id in the temporal edge appears as the ith row and column of the A matrix. 
A(i,j) is the rating that member nodeid(i) gave member nodeid(j).        
Since there are no duplicate ratings, the Problem.A matrix can hold all
the edges without losing any information.  The ratings are in the range
-10 to 10, but are never zero, so the MATLAB Problem.A sparse matrix   
contains all the edges.                                                  
The timestamps appear in the Problem.aux.Time matrix.  It has the same 
nonzero pattern as Problem.A.
Bitcoin OTC trust weighted signed network
https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html                 
Dataset information                                                      
This is who-trusts-whom network of people who trade using Bitcoin on a 
platform called Bitcoin OTC (http://www.bitcoin-otc.com/). Since Bitcoin 
users are anonymous, there is a need to maintain a record of users'    
reputation to prevent transactions with fraudulent and risky users. Members
of Bitcoin OTC rate other members in a scale of -10 (total distrust) to +10
(total trust) in steps of 1.  This is the first explicit weighted signed 
directed network available for research.                                 
Dataset statistics                                                     
Nodes   5,881                                                          
Edges   35,592                                                         
Range of edge weight    -10 to +10                                     
Percentage of positive edges    89%                                      
Similar network from another Bitcoin platform, Bitcoin Alpha, is available 
at https://snap.stanford.edu/data/soc-sign-bitcoinalpha.html (and as   
SNAP/bitcoin-alpha in the SuiteSparse Matrix Collection).                
Source (citation) Please cite the following paper if you use this dataset: 
S. Kumar, F. Spezzano, V.S. Subrahmanian, C. Faloutsos. Edge Weight    
Prediction in Weighted Signed Networks. IEEE International Conference on 
Data Mining (ICDM), 2016.                                                
The following BibTeX citation can be used:                             
@inproceedings{kumar2016edge,                                          
  title={Edge weight prediction in weighted signed networks},          
  author={Kumar, Srijan and Spezzano, Francesca and Subrahmanian, VS   
     and Faloutsos, Christos},                                         
  booktitle={Data Mining (ICDM), 2016 IEEE 16th Intl. Conf. on},       
  pages={221--230},                                                    
  year={2016},                                                         
  organization={IEEE}                                                  
}                                                                        
The project webpage for this paper, along with its code to calculate two 
signed network metrics---fairness and goodness---is available at       
http://cs.umd.edu/~srijan/wsn/                                           
Files                                                                  
File    Description                                                    
soc-sign-bitcoinotc.csv.gz                                             
    Weighted Signed Directed Bitcoin OTC web of trust network            
Data format                                                            
Each line has one rating, sorted by time, with the following format:     
SOURCE, TARGET, RATING, TIME
where                                                                    
SOURCE: node id of source, i.e., rater                               
TARGET: node id of target, i.e., ratee                               
RATING: the source's rating for the target,                          
        ranging from -10 to +10 in steps of 1                        
TIME: the time of the rating, measured as seconds since Epoch.
Notes on inclusion into the SuiteSparse Matrix Collection, July 2018:
The SNAP data set is 1-based, with nodes numbered 1 to 35,592.           
In the SuiteSparse Matrix Collection, Problem.A is the directed rating 
graph, a matrix of size n-by-n with n=5,881, which is the number of    
unique node id's appearing in the SOURCE or TARGET of any edge.          
Problem.aux.nodeid is an array of size 5,881 that gives the node id's  
corresponding to each row and column of the matrix.  nodeid(i)=id if the 
id in the temporal edge appears as the ith row and column of the A matrix. 
A(i,j) is the rating that member nodeid(i) gave member nodeid(j).        
Since there are no duplicate ratings, the Problem.A matrix can hold all
the edges without losing any information.  The ratings are in the range
-10 to 10, but are never zero, so the MATLAB Problem.A sparse matrix   
contains all the edges.                                                  
The timestamps appear in the Problem.aux.Time matrix.  It has the same 
nonzero pattern as Problem.A."	2	44	0	wolfram77	graphs-snap-soc-sign-bitcoin
7687	7687	COVID-19 Rapid Response Virtual Agent Dataset	Covid-19 ChatBot ...	['nlp', 'covid19', 'ml ethics']		5	81	0	nelakurthisudheer	covid19-rapid-response-agent-chatbot-dataset
7688	7688	cup dataset new		['football']		0	36	0	yijiangfan	cup-dataset-new
7689	7689	soc-redditHyperlinks Graphs (SNAP)		[]		0	33	0	wolfram77	graphs-soc-reddithyperlinks
7690	7690	movies..csv		['arts and entertainment']		0	10	0	alokkumar3155	moviescsv
7691	7691	Mydata		['business']		0	85	0	dtrnngc	mydata
7692	7692	lists_all		[]		0	24	0	lsyluestc	lists-all
7693	7693	Melanoma with Augmented Images	Melanoma with Augmented Images	[]		0	52	0	saschamet	melanoma-augmented
7694	7694	wine equality dataset	white wine quality detection	['alcohol']		1	26	1	harshavarthinib	wine-equality-dataset
7695	7695	openimages		[]		7	7	0	umeshsati54	openimages
7696	7696	Crypto-data-part1	Prices of top cryptocurrencies	['bayesian statistics', 'finance', 'text mining', 'time series analysis', 'currencies and foreign exchange']	"Context
Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etc are constantly coming in the news articles I read. So I wanted to understand more about it and this post helped me get started. Once the basics are done, the data scientist inside me started raising questions like:
How many cryptocurrencies are there and what are their prices and valuations?
Why is there a sudden surge in the interest in recent days?
So what next?
Now that we have the price data, I wanted to dig a little more about the factors affecting the price of coins. I started of with Bitcoin and there are quite a few parameters which affect the price of Bitcoin. Thanks to Blockchain Info, I was able to get quite a few parameters on once in two day basis.
This will help understand the other factors related to Bitcoin price and also help one make future predictions in a better way than just using the historical price.
Content
The dataset has one csv file for each currency. Price history is available on a daily basis from April 28, 2013. This dataset has the historical price information of some of the top crypto currencies by market capitalization.
Date : date of observation
Open : Opening price on the given day
High : Highest price on the given day
Low : Lowest price on the given day
Close : Closing price on the given day
Volume : Volume of transactions on the given day"	127	1075	16	tusharsarkar	cryptodatapart1
7697	7697	cars data		[]		6	51	0	tojintpanicker	cars-data
7698	7698	Abalone Age Prediction	Training using Logistic Regression and other Algorithms 	['computer science', 'software']		2	49	0	ramyasudhatg	abalone-age-prediction
7699	7699	Higgs Twitter Graphs (SNAP)	Graphs from the Stanford Network Analysis Platform	['internet', 'email and messaging', 'online communities']	"Higgs Twitter Dataset
https://snap.stanford.edu/data/higgs-twitter.html                        
Dataset information                                                      
The Higgs dataset has been built after monitoring the spreading processes
on Twitter before, during and after the announcement of the discovery of a 
new particle with the features of the elusive Higgs boson on 4th July 2012.
The messages posted in Twitter about this discovery between 1st and 7th
July 2012 are considered.                                                
The four directional networks made available here have been extracted from 
user activities in Twitter as:                                           
1. re-tweeting (retweet network)                                     
2. replying (reply network) to existing tweets                       
3. mentioning (mention network) other users                          
4. friends/followers social relationships among user involved        
   in the above activities                                           
5. information about activity on Twitter during the discovery of     
   Higgs boson
It is worth remarking that the user IDs have been anonimized, and the same 
user ID is used for all networks. This choice allows to use the Higgs  
dataset in studies about large-scale interdependent/interconnected     
multiplex/multilayer networks, where one layer accounts for the social 
structure and three layers encode different types of user dynamics.      
For more information about data collection, please refer to our paper.   
Dataset statistics are calculated for the graph with the highest number of 
nodes and edges:                                                         
Social Network statistics                                              
Nodes   456,626                                                        
Edges   14,855,842                                                     
Nodes in largest WCC    456290 (0.999)                                 
Edges in largest WCC    14855466 (1.000)                               
Nodes in largest SCC    360210 (0.789)                                 
Edges in largest SCC    14102605 (0.949)                               
Average clustering coefficient  0.1887                                 
Number of triangles 83023401                                           
Fraction of closed triangles    0.002901                               
Diameter (longest shortest path)    9                                  
90-percentile effective diameter    3.7                                  
Retweet Network statistics                                             
Nodes   256,491                                                        
Edges   328,132                                                        
Nodes in largest WCC    223833 (0.873)                                 
Edges in largest WCC    308596 (0.940)                                 
Nodes in largest SCC    984 (0.004)                                    
Edges in largest SCC    3850 (0.012)                                   
Average clustering coefficient  0.0156                                 
Number of triangles 21172                                              
Fraction of closed triangles    0.0001085                              
Diameter (longest shortest path)    19                                 
90-percentile effective diameter    6.8                                  
Reply Network statistics                                               
Nodes   38,918                                                         
Edges   32,523                                                         
Nodes in largest WCC    12839 (0.330)                                  
Edges in largest WCC    14944 (0.459)                                  
Nodes in largest SCC    322 (0.008)                                    
Edges in largest SCC    708 (0.022)                                    
Average clustering coefficient  0.0058                                 
Number of triangles 244                                                
Fraction of closed triangles    0.0001561                              
Diameter (longest shortest path)    29                                 
90-percentile effective diameter    10                                   
Mention Network statistics                                             
Nodes   116,408                                                        
Edges   150,818                                                        
Nodes in largest WCC    91606 (0.787)                                  
Edges in largest WCC    132068 (0.876)                                 
Nodes in largest SCC    1801 (0.015)                                   
Edges in largest SCC    7069 (0.047)                                   
Average clustering coefficient  0.0825                                 
Number of triangles 23068                                              
Fraction of closed triangles    0.0002417                              
Diameter (longest shortest path)    18                                 
90-percentile effective diameter    6.5                                  
Data format - higgs-activity_time.txt                                    
userA userB timestamp interaction
Interaction can be RT (retweet), MT (mention) or RE (reply). Each link 
is directed. The user IDs in this dataset corresponds to the ones    
adopted to anonymize the social structure, thus the datasets (1) - (5) 
can be used together for complex analysis involving structure and    
dynamics.
Note 1: the direction of links depends on the application, in general. 
For instance, if one is interested in building a network of how      
information flows, then the direction of RT should be reversed when  
used in the analysis. Nevertheless, the choice is left to the        
researcher and his/her own interpretation of the data, whereas we just 
provide the observed actions, i.e., who                              
retweets/mentions/replies/follows whom.
Note 2: users mentioned in retweeted tweets are considered as mentions.
For instance, if @A retweets the tweet “hello @C @D"" sent by @B, then
the following links are created: @A @B timeX RT, @A @C timeX MT, @A @D 
timeX MT, because @C and @D can be notified that they have been      
mentioned in a retweet. Similarly in the case of a reply. If for some
reason the researcher does not agree with this choice, he/she can    
easily identify this type of links and remove the mentions, for      
instance.
Source (citation)                                                      
M. De Domenico, A. Lima, P. Mougel and M. Musolesi. The Anatomy of a   
Scientific Rumor. (Nature Open Access) Scientific Reports 3, 2980 (2013).
http://www.nature.com/srep/2013/131018/srep02980/full/srep02980.html     
Files                                                                  
File    Description                                                    
social_network.edgelist.gz  Friends/follower graph (directed)          
retweet_network.edgelist.gz                                            
    Graph of who retweets whom (directed and weighted)                 
reply_network.edgelist.gz                                              
    Graph of who replies to who (directed and weighted)                
mention_network.edgelist.gz                                            
    Graph of who mentions whom (directed and weighted)                 
higgs-activity_time.txt.gz                                             
    The dataset provides information about activity on                 
    Twitter during the discovery of Higgs boson                            
Notes on inclusion into the SuiteSparse Matrix Collection, July 2018:
The SNAP data set is 1-based, with all nodes in all graphs numbered 1  
to n=456,626.                                                            
In the SuiteSparse Matrix Collection, each matrix is the same size, n-by-n 
where n=456,626, so that row/column i in each matrix refers to the same
person i across all matrices.  This means that some rows and columns of
the Retweet, Mention, and Reply matrices are empty, but these are left in
so all four matrices can be compared with each other.                    
Problem.A is the primary social network, and is a directed graph       
with no edge weights (an unsymmetric binary matrix).  A(i,j)=1 if      
person i follows person j.  It is not a multigraph.                      
Retweet = Problem.aux.retweet is the Retweet network, where Retweet(i,j) 
is the number of times that person i retweets a tweet of person j.       
Mention = Problem.aux.mention is the Mention network, where Mention(i,j) 
is the number of times that person i mentions person j.                  
Reply = Problem.aux.reply is the Reply network, where Reply(i,j)       
is the number of times that person i replies to person j.                
The Retweet, Mention, and Reply matrices represent multigraphs since each
(i,j,t) with the same i and j but different timestamp t is considered a
separate edge.  The timestamps do not appear in these matrices, however. 
The higgs-activity_time.txt is a set of labeled temporal edges.  Each edge 
in the SNAP data set has the form (i,j,time,interaction) where interaction 
is string (RT, MT, or RE).  In the SuiteSparse Matrix collection, these
edges are stored as a dense matrix, Problem.aux.temporal_edges, where the
kth row of the matrix holds the kth line of the higgs-activity_time.txt
file as the temporal edge [i j interaction time].  The interaction is  
converted to an integer, where 1=RT (retweet), 2=MT (mention), and 3=RE
(reply)."	3	60	0	wolfram77	graphs-snap-higgs-twitter
7700	7700	scores_em		[]		2	8	2	jeongtw	scores-em
7701	7701	nlp_modeling_baseline		[]		0	25	0	shekharrastogi	nlp-modeling-baseline
7702	7702	positive		[]		0	4	0	ir0nick	positive
7703	7703	PoolFormer		[]		0	4	0	aishikai	poolformer
7704	7704	Weights		['exercise']		0	23	0	rukonuddin	weights
7705	7705	swin-large-224-tl-2		[]		0	19	0	watanabetakahiro	swinlarge224tl2
7706	7706	Pytorch TIMM		[]		1	3	0	aishikai	pytorch-timm
7707	7707	Pakistan Intellectual data		[]		0	13	0	ehteshamalikhan	pakistan-intellectual-data
7708	7708	cifar100-python		['computer science', 'programming']		0	8	0	heydarsoudani	cifar100python
7709	7709	Pet Pawpularity Poolformer		[]		0	11	0	aishikai	pet-pawpularity-poolformer
7710	7710	COTS Positives TFRecords		[]		0	20	0	shreyasagarwal	cots-positives-tfrecords
7711	7711	LSTF_Dataset	LSTF_Dataset, LSTF_Dataset	[]		5	18	1	forestsking	lstf-dataset
7712	7712	jigsaw		['puzzles']		2	12	1	tensorchoko	jigsaw
7713	7713	e621-faces-dataset		['earth and nature']	"Context
Taken from https://github.com/arfafax/E621-Face-Dataset
Inspiration
GAN"	0	25	0	longng0611	e621facesdataset
7714	7714	TB_Special_Play_Video		[]		0	16	0	kushtrivedi14728	tb-special-play-video
7715	7715	drd-v2		['automobiles and vehicles']		0	24	0	gowrav143	drdv2
7716	7716	wheat5model		[]		0	22	0	tranluatvy	wheat5model
7717	7717	P_Pr_k=5Swin384		[]		0	4	0	durgammohanpranay	p-pr-k5swin384
7718	7718	Movie Bechdel Test Scores	Female Representation in Fiction Quantified	['arts and entertainment', 'movies and tv shows', 'data visualization', 'data analytics', 'tabular data']	"Context
We are working on a project to compare the Bechdel test and other female representation metrics. I came across this awesome website that contains the Bechdel Test score of more than 9,300 movies. I called the website API and got two versions of the Bechdel data. It took me about 7 hours. 
Python code to use the API is in the Code section of this dataset: Bechdel_api.ipynb. You can use the code to get the most updated data.
My analysis is also in the Code section: Bechdel_vis.ipynb.
The project GitHub repo is here: https://github.com/AlisonYao/HCDS-Bechdel-Test-Final-Project
Content
Bechdel.csv is what you can get from the API call directly. But I noticed that it is missing 4 columns of information (visible, submitterid, dubious, date) compared to the data you can get from fetching the data by IMDb id.  I think the dubious column is really important so I used the IMDb id from this dataset to get all information on a movie by fetching the data of each movie one by one. I put the more detailed information in Bechdel_detailed.csv.
Acknowledgements
All credits go to https://bechdeltest.com/. I am merely calling the API and organizing the data. But I would like to share it here to save you guys some time. And also meaningless and repetitive use of the API only brings unnecessary burden and risk crashing the website. So here is the data.
Inspiration
I used this dataset to see if more movies are passing the Bechdel Test these years. I plan to join this dataset with data on cast and crew female proportion to find relationships, check correlations and compare different female representation measurements."	90	1443	10	alisonyao	movie-bechdel-test-scores
7719	7719	Binance Coin Data	BinanceCoin USD (BNB-USD) | Daily data | 2017 - 2021	['finance', 'time series analysis', 'investing', 'currencies and foreign exchange', 'datetime']	"What is Binance?
Binance is a cryptocurrency exchange that provides a platform for trading various cryptocurrencies. It was founded in 2017 and is domiciled in the Cayman Islands. Binance is currently the largest exchange in the world in terms of daily trading volume. Binance was founded by Changpeng Zhao, a developer who had previously created high frequency trading software. Binance was initially based in China, but later moved its headquarters out of China due to China's increasing regulation of cryptocurrency. Binance is currently under investigation by both the United States Department of Justice and Internal Revenue Service on allegations of money laundering and tax offenses. The UK's Financial Conduct Authority ordered Binance to stop all regulated activity in the United Kingdom in June 2021.
Data Description
This dataset provides the history of daily prices of Binance Coin. The data starts from 25-Jul-2017. All the column descriptions are provided. Currency is USD."	319	2935	13	varpit94	binancecoin-data
7720	7720	exp039-039-swin-l-384-fastai-chaug-mixup		[]		0	2	0	kurokurob	exp039-039-swin-l-384-fastai-chaug-mixup
7721	7721	GB_Special_Play_Video		[]		0	1	0	kushtrivedi14728	gb-special-play-video
7722	7722	models		['clothing and accessories']		1	44	0	awaptk	models
7723	7723	m2h2_img_audio		[]		0	17	1	nischaydnk	m2h2-img-audio
7724	7724	SQLCaseStudy_1	Dannys Diner SQL Case Study	[]		1	33	0	jonathanlouisrogers	sqlcasestudy-1-dannysdiner
7725	7725	pvpoke rankings and stats Jan 1 2022		[]		1	11	1	jjjjjjjjjjason	pvpoke-rankings-and-stats-jan-1-2022
7726	7726	EDA on Top 5 Cryptocurrencies	Top 5 Cryptocurrencies : Bitcoin, Ethereum, Binance Coin, Tether,  Ripple	['beginner', 'investing', 'currencies and foreign exchange']		6	62	0	nshreya788	eda-on-top-5-cryptocurrencies
7727	7727	COVID-19 Chest X-ray dataset		[]		0	37	1	emanuelreyes012	covid19-chest-xray-dataset
7728	7728	yolox-x-f4		[]		1	26	0	watanabetakahiro	yoloxxf4
7729	7729	GitHub Programming Languages Data	Statistics for Programming Languages used on GitHub	['computer science', 'programming', 'software', 'bigquery', 'python']	"Context
A common question for those new and familiar to computer science and software engineering is what is the most best and/or most popular programming language. It is very difficult to give a definitive answer, as there are a seemingly indefinite number of metrics that can define the 'best' or 'most popular' programming language.
One such metric that can be used to define a 'popular' programming language is the number of projects and files that are made using that programming language. As GitHub is the most popular public collaboration and file-sharing platform, analyzing the languages that are used for repositories, PRs, and issues on GitHub and be a good indicator for the popularity of a language.
Content
This dataset contains statistics about the programming languages used for repositories, PRs, and issues on GitHub. The data is from 2011 to 2021.
Source
This data was queried and aggregated from BigQuery's public github_repos and githubarchive datasets.
Limitations
Only data for public GitHub repositories, and their corresponding PRs/issues, have their data available publicly. Thus, this dataset is only based on public repositories, which may not be fully representative of all repositories on GitHub."	758	5399	28	isaacwen	github-programming-languages-data
7730	7730	Aluminium-copper to make product	The amount of aluminium, copper amount to make product either sheet,plate.	[]		4	33	0	shiveshraj	aluminiumcopper-to-make-product
7731	7731	Stack Overflow Tags Data	Statistics for Stack Overflow's 1000 Most Popular Tags	['computer science', 'programming', 'software', 'bigquery', 'python']	"Context
A common question for those new and familiar to computer science and software engineering is what is the most best and/or most popular programming language. It is very difficult to give a definitive answer, as there are a seemingly indefinite number of metrics that can define the 'best' or 'most popular' programming language.
One such metric that can be used to define a 'popular' programming language is the number of posts relating to that language on public forums. With Stack Overflow being perhaps the most commonly used forum for questions related to programming languages, analyzing the number of posts and other metrics for specific programming languages on Stack Overflow can be a good indicator for the popularity of a language.
Content
This dataset contains statistics about posts, views, answers, comments, and favorites relating to the 1000 most popular tags on Stack Overflow, including those designated for questions relating to specific programming languages such as 'python' and 'javascript'. The data is from 2008 to 2021, and is sorted into rows for each tag, for each year.
Source
This data was queried and aggregated from BigQuery's public stackoverflow dataset."	41	382	2	isaacwen	stack-overflow-tags-data
7732	7732	jigsaw-unintended-bias-in-toxicity-classification		['health']		2	24	0	chenghonghu	jigsawunintendedbiasintoxicityclassification
7733	7733	jigsaw-multilingual-toxic-comment-classification		[]		0	45	0	chenghonghu	jigsawmultilingualtoxiccommentclassification
7734	7734	Cyclistic Dataset	Google Data Analysis Certificate Capstone Project 	['business', 'cycling']	Divvy bike share dataset from the google data analytics certification.	3	50	0	edwinhfuentes	cyclistic-dataset
7735	7735	reddit eating disorder dataset	dataset on a subreddit r/EDAnomymous	['video games']		6	85	0	matakahas	reddit-redanomymous-dataset
7736	7736	FDIC - Banks In Kentucky	Bank institutions established in Kentucky state	['banking']	A dataset of FDIC-insured institutions established in Kentucky between 1869 and 2020. Records include demographic information related to the institution such as locational detail (name, city, state, etc) and operating status. The dataset also includes a url to the FDIC’s public records for up to date info for each institution.	3	40	2	donnetew	fdic-banks-in-kentucky
7737	7737	Cat and Dog Images	1000 cat images and 1000 dog images	['animals', 'computer vision', 'classification', 'deep learning', 'image data']	"Introduction
This dataset contains 1000 cat images and 1000 dogs images collected from Internet. You can use them for machine learning tasks, such as image classification."	14	126	1	googolples	cat-dog-images
7738	7738	frozen_model_weights		[]		0	20	0	ryancl	frozen-model-weights
7739	7739	new_lyrics		[]		0	16	0	miladaf	new-lyrics
7740	7740	QS World University Rankings 2021 - World		['universities and colleges']		11	181	1	zerghamwarraich1	qs-world-university-rankings-2021-world
7741	7741	Education Rankings by Country 2021		['education']		8	58	1	zerghamwarraich1	education-rankings-by-country-2021
7742	7742	my_lyrics		[]		0	20	0	miladaf	my-lyrics
7743	7743	World Economic Outlook Database: October 2021		['business']		2	44	0	zerghamwarraich1	world-economic-outlook-database-october-2021
7744	7744	COTS_dataset_videos		[]		0	11	0	viniciocastillo	cots-dataset-videos
7745	7745	Menu Data 2		[]		0	5	0	danasmith1	menu-data-2
7746	7746	GE_Aviation_kpis_dataset		[]		0	18	0	fetchii	ge-aviation-kpis-dataset
7747	7747	UNet Lanes v2		[]		0	11	0	jasonyuan000	unet-lanes-v2
7748	7748	Toxic Comment Classification Challenge		[]		2	32	0	chenghonghu	toxic-comment-classification-challenge
7749	7749	flatten		['arts and entertainment']		0	15	0	sireesh	flatten
7750	7750	books_read_in_2021	Books I read in 2021	['literature']		4	33	0	laurenbaur	books-read-in-2021
7751	7751	data mining project	Shanghai housing rental forecast	['china ', 'categorical data', 'business']		1	35	0	krystaljung	data-mining-project
7752	7752	Tumor_Classification		[]		0	4	0	eberlaurentelliuyacc	tumor-classification
7753	7753	MY SPOTIFY WRAPPED EDA	Spotify wrapped 2021	['music', 'exploratory data analysis', 'data cleaning', 'data analytics', 'audio data']		3	40	0	imraanvirani	my-spotify-wrapped-eda
7754	7754	swin transformer		[]		0	3	0	hanszhou615	swin-transformer
7755	7755	tips_seaborn		[]		0	27	0	najielkotob	tips-seaborn
7756	7756	swin224centercrop 		[]		0	20	1	datafan07	swin224centercrop
7757	7757	FinanceData		[]		19	39	0	suehuynh	financedata
7758	7758	Bangla Sign Languase Dataset (CSE 497)		[]		0	24	1	kazimahadihasan	bangla-sign-languase-dataset-cse-497
7759	7759	BTC-Prices-2014-Now	This Dataset contains Bitcoin prices in 2014-2021. Vote up if you liked	['history', 'business', 'finance', 'investing', 'currencies and foreign exchange']	"Context🔥
This is a Dataset that represent bitcoin prices in 2015-2021
Content✔️
we have open, close, low and high prices in usd.
the dates(open, close) also exists in this csv file.
API that I got the results of is CoinAPI:
with free plan you can access rest api i put the link below so you can also use it.
https://www.coinapi.io/
Acknowledgements
I just want to thank CoinAPI  this amazing service.
if you liked it I will be happy if you vote up it and follow my kaggle profile.😃"	20	167	6	cyruskouhyar	btcprices2015now
7760	7760	Food Lovers: Global Cuisine Preferences Survey	What Food is eaten Where?	['global', 'categorical data', 'cooking and recipes']	"Context
What cuisine do you prefer? Maybe you like eating local food, McDonalds or Sushi.
This dataset was compiled from responses to a short survey investigating where in the world different cuisines are popular.
Content
Participants of our Jotform Survey were asked 3 questions:
What kind of food do you like? (You can pick several) 
What kind of food can be found where you live? (You can pick several)
Where do you live?$
&gt; $ If you are concerned about your privacy, you don't have to mark your precise address. Please place the coordinate marker on anywhere CLOSE to where you live.
Food_Preferences.csv and Food_Availability.csv correspond to the first and second question, respectively. Each datapoint is represented by the participant's ""provided"" geolocation, which while geographically accurate should not be precise. This is given as the longitude and latitude on two columns. The 10 default cuisine options we provided can be found in Default_Options.csv:
American
Chinese
French
Greek
Indian
Italian
Japanese
Mexican
Spanish
Thai
User-added options are listed in User_Entered_Options.csv. Note that default option columns are marked as True or False, and user-added options are True or n/a, depending on the participant's answer.
Acknowledgements
Original survey conducted by MSc Data Science students from ITU Copenhagen for Advanced Applied Statistics and Multivariate Calculus (Autumn 2021).
Cover Photo by <a href=""https://unsplash.com/@gabrielerimini?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Gabriele bartoletti stella</a> on <a href=""https://unsplash.com/s/photos/food-basket?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>
Inspiration
Datasets on particular cuisines, such as common ingredients and nutritional values are available, however, it does not seem that the geographic extent of world cuisines is well documented. Cuisine is often extraterritorial, extending beyond national, even cultural, borders. And of course, since food is marked with [""social labels""] (https://hiyamaya.files.wordpress.com/2009/12/social_labels.pdf) [(Zadek et al., 1998) ] (https://philpapers.org/rec/ZADSLT) there is incentive to market certain foods as part of a particular cuisine. So let us hope that this humble dataset might offer some new insight on food and geography."	50	322	1	themadprogramer	food-lovers-global-cuisine-preferences-survey
7761	7761	company sale	Dataset about companies spending the money on advertising,fm radio,newspaper	['retail and shopping']		4	23	1	sourabh2001	company-sale
7762	7762	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_37		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-37
7763	7763	Hacker News: All Posts, Stories, Ask/Show HN, etc	All posts since 2006 (+3.3M), without including comments.	['internet', 'news', 'python']	"I used Algolia's API to download the posts. I wrote a thin Python wrapper around it that made it very simple to download them (and keep them updated).
Check the full library if you're interested in your own downloads: https://github.com/santiagobasulto/python-hacker-news."	185	4114	19	santiagobasulto	all-hacker-news-posts-stories-askshow-hn-polls
7764	7764	small dataset		[]		71	18	0	zarthanz7n	small-dataset
7765	7765	Ecommerce1		[]		1	7	1	jhuluk	ecommerce1
7766	7766	PetFinder-Model52		[]		0	4	0	lftuwujie	petfindermodel52
7767	7767	 Wine Data Set	 Using chemical analysis determine the origin of wines	['alcohol']		53	897	17	ananthr1	wine-data-set
7768	7768	TPS-01-22 with AutoKeras output v1		[]		4	12	1	lonnieqin	tps0122-with-autokeras-output-v1
7769	7769	pf-142-train-5-data		[]		0	15	0	makotoikeda	pf-142-train-5-data
7770	7770	Loan dataset		[]		2	19	0	suryaabd	loan-dataset
7771	7771	PyCaret Regression Prediction Model Util		[]		0	33	1	rhythmcam	pycaret-regression-prediction-model-util
7772	7772	Room Occupancy Estimation Data Set	Data set for estimating the precise number of occupants in a room	['beginner', 'tabular data']	"Data Set Information:
The experimental testbed for occupancy estimation was deployed in a 6m Ã— 4.6m room. The setup consisted of 7 sensor nodes and one edge node in a star configuration with the sensor nodes transmitting data to the edge every 30s using wireless transceivers. No HVAC systems were in use while the dataset was being collected.
Attribute Information:
Date: YYYY/MM/DD
Time: HH:MM:SS
Temperature: In degree Celsius
Light: In Lux
Sound: In Volts (amplifier output read by ADC)
CO2: In PPM
CO2 Slope: Slope of CO2 values taken in a sliding window
PIR: Binary value conveying motion detection
Room_Occupancy_Count: Ground Truth"	91	1133	28	ananthr1	room-occupancy-estimation-data-set
7773	7773	Heartattack prediction using SVM(classifer)		[]		10	69	0	suryaabd	heartattack-prediction-using-svmclassifer
7774	7774	Germany states Geo json		[]		0	21	0	karlpetz	germany-states-geo-json
7775	7775	Topic Modeling for Research Articles	NLP Dataset for Beginners	['earth and nature', 'education', 'psychology', 'nlp']	"Context
Part of Guided Hackathon in Analytics Vidhya
Content
Topic Modeling for Research Articles 2.0
Researchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more and more difficult. Tagging or topic modelling provides a way to give clear token of identification to research articles which facilitates recommendation and search process. 
Earlier on the Independence Day we conducted a Hackathon to predict the topics for each article included in the test set. Continuing with the same problem, In this Live Hackathon we will take one more step ahead and predict the tags associated with the articles.
Given the abstracts for a set of research articles, predict the tags for each article included in the test set. 
Note that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: 
Computer Science
Mathematics
Physics
Statistics
List of possible tags are as follows:
[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]
Acknowledgements
Credits to AV
Inspiration
To share with the data science community to jump start their journey in NLP"	305	3469	16	abisheksudarshan	topic-modeling-for-research-articles
7776	7776	Datacamp_Seaborn		[]		3	7	0	kakamana	datacamp-seaborn
7777	7777	Letterboxd Films		['movies and tv shows']		7	129	0	whistlebug23	letterboxd-films
7778	7778	MNAD-bigbigdata		[]		0	4	0	mubarizkhan	mnadbigbigdata
7779	7779	xlm roberta 		['arts and entertainment', 'nlp', 'pytorch', 'transformers']	"Context
XLM ROBERTA pretrained on NER for 25 epochs for competition  Feedback Prize
Data can be found here"	1	40	0	roeilevy	xlm-roberta
7780	7780	car_details	Details of second hand cars	['business', 'intermediate', 'linear regression', 'tabular data', 'sklearn']		136	747	13	ishaanthareja007	car-details
7781	7781	testcsv		[]		0	1	0	mahaboobjanishaik	testcsv
7782	7782	Holidays_Finland_Norway_Sweden_2015-2019		[]		93	375	24	drcapa	holidays-finland-norway-sweden-20152019
7783	7783	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_39		[]		0	19	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-39
7784	7784	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_38		[]		0	11	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-38
7785	7785	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_36		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-36
7786	7786	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_35		[]		0	9	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-35
7787	7787	test_void(101)		[]		0	15	0	osama13044	test-void101
7788	7788	mind4rec		[]		0	5	0	anhthemduocngu	mind4rec
7789	7789	Top 250s in IMDB	TOP RATED MOVIES,SERIES AND VIDEO GAMES on IMDB	['popular culture', 'arts and entertainment', 'movies and tv shows', 'video games', 'people and society']	"IMDb (an abbreviation of Internet Movie Database)2 is an online database of information related to films, television series, home videos, video games, and streaming content online – including cast, production crew and personal biographies, plot summaries, trivia, ratings, and fan and critical reviews. IMDb began as a fan-operated movie database on the Usenet group ""rec.arts.movies"" in 1990, and moved to the web in 1993. It is now owned and operated by IMDb.com, Inc., a subsidiary of Amazon.
As of June 2021, the database contained some 8 million titles (including television episodes) and 10.4 million person records.3 Additionally, the site had 83 million registered users. The site's well-used message boards were disabled in February 2017."	690	3474	27	ramjasmaurya	top-250s-in-imdb
7790	7790	Uber Fares Dataset	Can you predict the fare for Uber Rides - Regression Problem	['business', 'automobiles and vehicles', 'linear regression', 'regression', 'travel']	"Description:
The project is about on world's largest taxi company Uber inc. In this project, we're looking to predict the fare for their future transactional cases. Uber delivers service to lakhs of customers daily. Now it becomes really important to manage their data properly to come up with new business ideas to get best results. Eventually, it becomes really important to estimate the fare prices accurately.
The datset contains the following fields:
key - a unique identifier for each trip
fare_amount - the cost of each trip in usd
pickup_datetime - date and time when the meter was engaged
passenger_count - the number of passengers in the vehicle (driver entered value)
pickup_longitude - the longitude where the meter was engaged
pickup_latitude - the latitude where the meter was engaged
dropoff_longitude - the longitude where the meter was disengaged
dropoff_latitude - the latitude where the meter was disengaged
Acknowledgement:
The dataset is referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the fare price of uber ride.
Also evaluate the models & compare thier respective scores like R2, RMSE, etc."	1271	8777	36	yasserh	uber-fares-dataset
7791	7791	Covid-19_Tweets_mapped_data		[]		0	27	0	ahmedrasidunbaridip	covid19-tweets-mapped-data
7792	7792	mind-small		[]	mind small dataset. not include valid file	0	20	0	longpt	mindsmall
7793	7793	houseprice		[]		0	27	0	guotong211755	houseprice
7794	7794	paw_models		[]		17	70	2	yamqwe	paw-models
7795	7795	NFS_single		[]		0	14	0	revanthraj	nfs-single
7796	7796	Laboratorio TIA		[]		2	33	0	albertodepablos	laboratorio-tia
7797	7797	Xiaomi E-Scooter Pro 2 Marrakech ride	Xiaomi Pro 2 e-Scooter daily rides dataset, Moroccan roads	['automobiles and vehicles', 'time series analysis', 'random forest', 'xgboost', 'tabular data']	"Context
Impact of different factors on the Xiaomi Pro 2 e-Scooter battery life, real daily rides dataset (Moroccan roads).
Content
This data is gathered in order to help understand the impact of various factors such as Cells temperature, SoC, Speed, Total mileage... on the Lithium-ion battery's health used for this type of e-Scooters.
The name of the csv files indicate the day when the data was recorded."	37	449	2	draoumohcine	xiaomi-escooter-pro-2-marrakech-ride
7798	7798	hh_data		[]		2	27	0	imgremlin	hh-data
7799	7799	2018 Election Results		['politics']		0	15	0	ehteshamalikhan	2018-election-results
7800	7800	Iemocap-full-release		[]	"IEMOCAP DATABASE
The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database is an acted, multimodal and multispeaker database, recently collected at SAIL lab at USC. It contains approximately 12 hours of audiovisual data, including video, speech, motion capture of face, text transcriptions. It consists of dyadic sessions where actors perform improvisations or scripted scenarios, specifically selected to elicit emotional expressions. IEMOCAP database is annotated by multiple annotators into categorical labels, such as anger, happiness, sadness, neutrality, as well as dimensional labels such as valence, activation and dominance. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication."	28	88	3	dejolilandry	iemocapfullrelease
7801	7801	forged characters detection on passports		[]		1	36	1	turabbajeer	forged-characters-detection-on-passports
7802	7802	forged characters detection on driving licence		['law']		0	20	1	turabbajeer	forged-characters-detection-on-driving-licence
7803	7803	diabetes-dataset	Structured data for diabetes prediction	['diabetes']		2	31	1	omkar110401	diabetesdataset
7804	7804	kitti2015		[]		0	17	0	lsyluestc	kitti2015
7805	7805	posetrack_FOD		[]		3	17	0	antocad	posetrack-fod
7806	7806	large_384_in22k_3407	large swin 384 22k seed 3407 10 folds	[]		0	12	0	kingkong153	large-224-in22k-3407
7807	7807	EURUSD		['investing']		0	4	0	cenaav	eurusd
7808	7808	titanic		[]		1	5	0	joonyong	titanic
7809	7809	Delhi Nurseries Data	This Dataset about Plant Nurseries in Delhi NCR Region	['education']		0	32	1	souryadipstan	delhi-nurseries-data
7810	7810	diverh		[]		0	14	0	liangwh	diverh
7811	7811	Marketing Data	Sample of users who purchased product	[]		1	22	1	upendra860	marketing-data
7812	7812	trained_sparse_matrix		[]		1	17	0	cngvng	trained-sparse-matrix
7813	7813	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_34		[]		0	13	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-34
7814	7814	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_33		[]		0	12	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-33
7815	7815	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_32		[]		0	10	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-32
7816	7816	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_31		[]		0	9	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-31
7817	7817	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_30		[]		0	0	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-30
7818	7818	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_29		[]		0	12	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-29
7819	7819	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_28		[]		0	19	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-28
7820	7820	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_25		[]		0	0	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-25
7821	7821	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_24		[]		0	21	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-24
7822	7822	Text_extraction		[]		0	21	0	yuanshup	text-extraction
7823	7823	anime face getchu 32x32	Anime face from Brian Chao but resized to 32x32. All credits goes to Mckinsey666	['anime and manga']		2	16	0	sebastiendelprat	anime-face-getchu-32x32
7824	7824	Number of Married Pairs		[]		0	4	0	jambochen	number-of-married-pairs
7825	7825	Home Credit team8 Model Tuning		['automobiles and vehicles']		0	45	0	chnhgr	home-credit-team8-model-tuning
7826	7826	Tweet Act		['arts and entertainment']	"Context
Speech acts are a way to conceptualize speech as action. This holds true for communication on any platform, including social media platforms such as Twitter. 
Content
The dataset contains 1 csv file which consists of two columns. One containing the tweets from various topics and the other containing the tweet acts. There are 7 types of tweet acts altogether."	1	46	0	shreyangshubera	tweet-act
7827	7827	jigsaw2021-wat055-data		[]		0	7	0	wataoka	jigsaw2021-wat055-data
7828	7828	my_anime_faces		[]		0	33	0	lijiabin2021	my-anime-faces
7829	7829	trainedModel4GussionEstimate		[]		0	17	0	darksouls4	trainedmodel4gussionestimate
7830	7830	Assets Under Management Monthly Extract	Customer Investment Holdings for one month	['business']		2	51	0	nithinfelix	assets-under-management-monthly-extract
7831	7831	Tweets Política España	Tweets polític@s de los partidos PSOE, PP, VOX, Unidas Podemos y Ciudadanos	['politics', 'nlp', 'email and messaging']	"Contexto
El Procesamiento del Lenguaje Natural es una de las áreas de la inteligencia artificial muy estudiada hoy en día que tiene entre otros objetivos el entendimiento del lenguaje natural. El NLP esta avanzando cada día más pero se centra mucho en la lengua inglesa. Con este dataset se pretende aportar a la comunidad un pequeño corpus en Español con tweets escritos por políticos/as Españoles/as de los partidos PSOE, PP, VOX, Unidas Podemos y Ciudadanos.
Contenido
El dataset (copus) esta formado por:
cuenta: Nombre de la cuenta de twitter (haseada) que escribe el tweet.
partido: Partido político al que pertenece el usuario
timestamp: Instante en el que se publicó el tweet.
tweet: Contenido textual del tweet.
Agradecimientos
Twitter
Inspiración
Con este dataset espero que los usuarios de Kaggle de habla hispana se animen a compartir conocimiento de Procesamiento de Lenguaje Natural por medio de Notebooks y que podamos aprender sobre NLP en Español."	31	420	1	ricardomoya	tweets-poltica-espaa
7832	7832	anime_faces		[]		0	17	0	lijiabin2021	anime-faces
7833	7833	anime_faces/faces		[]		0	7	0	lijiabin2021	anime-facesfaces
7834	7834	Facebook Ad Library	Copy of the Ads from Facebook Ad Library API	['politics', 'social networks']	"Facebook Ad Library Copy (Github)
This Project aims to provide a searchable and complete copy of the political ads on the Facebook Ad Library
Facebook already provides all this data via their web interface. The Problem with this is that it's hardly searchable and therefore no real use for Analysis of political advertisement. In Addition to this the API is hard to access and limited in many ways.
This data is already meant to be public so this dataset is just helping to provide the transparency ~~Facebook~~ Meta wants to provide.
About the Data
The data is loaded directly from their official API.
The data is downloaded by country and page_id obtained from the Ad-reports. These should only include pages related to politics or issues of political importance but not all are clearly flagged.
Cause these reports contain user generated page_names and disclaimers some names or disclaimers might be corrupted by strange characters.
Another thing I realized is that some ads (or whole pages?) are temporarily (or permanently?) not searchable by the page_id which published them. You can identify them by checking the specified amount of Ads from the report vs. the actual loaded amount of ads in the dataset. Often this also results in pages seemingly having 0 ads. You can identify them by the msg field being empty (msg=""""). 
One more problem is that advertisements from page_id=0 pages are not searchable. These often just refer to a ""Instagram User of some id"" or other Facebook-Platform users. Have a look at the reports I used for more information.
In the end this resulted in the ads in this dataset being less than it should be according to the reports.
There are two json files per country:
todo.json: based on the Ad-reports and contains all pages crawled from with the timestamp of the last crawl and the paging cursor (after) in the ""msg"" field
ads.json: Contains the actual ads with the following fields:
Fields
id(""_id"" in the table)
ad_creation_time
ad_creative_bodies
ad_creative_link_captions
ad_creative_link_descriptions
ad_creative_link_titles
ad_delivery_start_time
ad_delivery_stop_time
bylines
currency
delivery_by_region
demographic_distribution
estimated_audience_size
impressions
languages
page_id
page_name
publisher_platforms
spend
The field ad_snapshot_url is not crawled as it's just a combination of the id and your access token:
`https://www.facebook.com/ads/archive/render_ad/?id="	13	235	1	lejo11	facebook-ad-library
7835	7835	jigsaw_20220101		[]		0	10	0	qinyukun	jigsaw-20220101
7836	7836	swinT_pretrained		[]		0	4	0	kakarroto	swint-pretrained
7837	7837	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_27		[]		0	10	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-27
7838	7838	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_26		[]		0	12	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-26
7839	7839	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_23		[]		0	16	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-23
7840	7840	Reddit - Finance Posts (r/wallstreetbets, r/gm...)	r/wallstreetbets, r/gme, r/stocks, r/pennystocks, r/robinhoodpennystocks, r/i...	['finance', 'exploratory data analysis', 'time series analysis', 'investing', 'online communities']	"Description
Reddit submissions from finance/investment/stock related posts:
- r/wallstreetbets: #775326 (2021-01-01 00:02:06 - 2021-12-31 23:55:51)
- r/gme: #273327 (2021-01-01 04:08:51 - 2021-12-31 23:59:44)
- r/personalfinance: #131181 (2021-01-24 19:30:31 - 2021-12-31 23:55:49)
- r/stocks: #75857 (2021-01-01 00:05:17 - 2021-12-31 22:34:41)
- r/pennystocks: #54785 (2021-01-01 00:13:41 - 2021-12-31 23:30:46)
- r/stockmarket: #43809 (2021-01-01 02:42:42 - 2021-12-31 23:48:27)
- r/investing: #41912 (2021-01-01 00:18:40 - 2021-12-31 23:37:54)
- r/options: #28782 (2021-01-01 01:39:43 - 2021-12-31 23:38:00)
- r/robinhoodpennystocks: #23304 (2021-01-01 00:27:36 - 2021-12-31 22:00:14)
- r/robinhood: #18893 (2021-01-01 00:22:48 - 2021-12-31 23:12:52)
- r/forex: #14643 (2021-01-01 00:07:45 - 2021-12-31 23:21:17)
- r/financialindependence: #10338 (2021-01-01 00:26:15 - 2021-12-31 21:45:26)
- r/finance: #7130 (2021-01-01 02:33:23 - 2021-12-31 23:35:09)
- r/securityanalysis: #1510 (2021-01-01 12:21:09 - 2021-12-30 12:56:24)
Data
See collection methodology, all times in UTC:
- id (string): The id of the submission.
- author (string): The redditors username.
- created (datetime): Time the submission was created.
- retrieved (datetime): Time the submission was retrieved.
- edited (datetime): Time the submission was modified.
- pinned (integer): Whether or not the submission is pinned.
- archived (integer): Whether or not the submission is archived.
- locked (integer): Whether or not the submission is locked.
- removed (integer): Whether or not the submission is mod removed.
- deleted (integer): Whether or not the submission is user deleted.
- is_self (integer): Whether or not the submission is a text.
- is_video (integer): Whether or not the submission is a video.
- is_original_content (integer): Whether or not the submission has been set as original content.
- title (string): The title of the submission.
- link_flair_text (string): The submission link flairs text content.
- upvote_ratio (number): The percentage of upvotes from all votes on the submission.
- score (integer): The number of upvotes for the submission.
- gilded (integer): The number of gilded awards on the submission.
- total_awards_received (integer): The number of awards on the submission.
- num_comments (integer): The number of comments on the submission.
- num_crossposts (integer): The number of crossposts on the submission.
- selftext (string): The submission selftext on text posts.
- thumbnail (string): The submission thumbnail on image posts.
- shortlink (string): The submission short url.
Usage
See getting started, data available as csv and hdf:
- submissions_reddit.csv: Load file using pandas or any other framework.
- submissions_reddit.h5: Load file using pandas &gt;= 1.2.1 and python &gt;= 3.8.5.
Legal
Provided ""as is"" without guarantee of completeness.
Photo by Lorenzo from Pexels.
Last update: 2022-01-01 10:02:00 UTC."	844	20096	38	leukipp	reddit-finance-data
7841	7841	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_22		[]		0	10	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-22
7842	7842	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_21		[]		0	0	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-21
7843	7843	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_20		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-20
7844	7844	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_19		[]		0	10	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-19
7845	7845	Emotion Recognition1		[]		1	36	0	ravinash218	emotion-recognition1
7846	7846	800k+ COVID-19 Vaccine Tweets	Tweets about the COVID-19 vaccines	['text data', 'covid19']	"Context
The dataset contains tweets who is used vaccine as a keyword.
Content
The data contains the following information:
Username: Username of the person who tweeted
User Display Name: Name of the person who tweeted
User Description: Description of the person who tweeted
User Location: Location of the person who tweeted
User Verified: Whether the person who tweeted is verified
User Protected: Whether the person who tweeted is protected
User Followers Count: Number of followers of the person who tweeted
User Friends Count: Number of friends of the person who tweeted
User Statuses Count: Number of tweets of the person who tweeted
User Favourites Count: Number of favourites of the person who tweeted
Tweet Content: Content of the tweet
Tweet Language: Language of the tweet
Tweet Retweet Count: Number of retweets of the tweet
Tweet Quote Count: Number of quotes of the tweet
Tweet Reply Count: Number of replies of the tweet
Tweet Like Count: Number of likes of the tweet
Tweet ID: ID of the tweet
Tweet URL: URL of the tweet
Tweet Date: Date of the tweet"	17	179	2	oktayozturk010	800k-covid19-vaccine-tweets
7847	7847	Handwritten1		[]		1	43	0	chinmaymharidas	handwritten1
7848	7848	Datos COVID-19 Chile	Data by Ministerio de Salud	['health', 'covid19']		299	4055	12	dataobservatory	datoscovid19chile
7849	7849	Tweets of Joe Biden (2007 - 2021)	Joe Biden's tweets from official Twitter accounts @JoeBiden and @POTUS	['politics']	"Content
Joe Biden's (@JoeBiden and @POTUS accounts) tweets from 24th October 2007 to 31st December 2021.
Username: Username of the person who tweeted
Tweet ID: ID of the tweet
Tweet URL: URL of the tweet
Tweet Date: Date of the tweet
Tweet Content: Content of the tweet
Tweet Language: Language of the tweet
Tweet Retweet Count: Number of retweets of the tweet
Tweet Quote Count: Number of quotes of the tweet
Tweet Reply Count: Number of replies of the tweet
Tweet Like Count: Number of likes of the tweet"	8	103	2	oktayozturk010	tweets-of-joe-biden-2007-2021
7850	7850	dataset_ocr_Vn	Dataset Vietnamese (Địa chỉ)	['cnn']		0	94	1	acousticmusic	dataset-ocr-vn
7851	7851	handwritten 		[]		0	6	0	chinmaymharidas	handwritten
7852	7852	wjw_data_1		[]		0	8	0	pangzi233	wjw-data-1
7853	7853	OpenMMLab Essential Repositories	https://github.com/MaxVanDijck/openmmlab-kaggle	['computer vision', 'classification', 'gan', 'transfer learning']	"OpenMMLab offline repositories
Compressed OpenMMLab repositories that are commonly used in computer vision applications. Created by the shell script available on Github.
Please remember to upvote ⬆️ if you find this dataset helpful! 
Content Checklist
✅ Working, ❌ Not Working
✅ MMCV - 1.4.2
✅ MMClassification - 0.19.0
✅ MMDetection - 2.20.0
❌ MMSegmentation
❌ MMPose
❌ MMOCR
❌ MMGeneration
❌ MMFewShot
✅ MMTracking - 0.8.0
Suggested Usage
See this notebook for suggested usage
Acknowledgements
https://github.com/open-mmlab"	12	484	12	maxvandijck	openmmlab-essential-repositories
7854	7854	jigsaw_20211231		[]		0	9	0	qinyukun	jigsaw-20211231
7855	7855	cit-Patents Graph (SNAP)	Temporal US Patent citation network 1975-1999	['law']		2	34	0	wolfram77	graph-snap-cit-patents
7856	7856	MyAnimeList scraping decades of anime	Dataset from a project to learn web scraping and data manipulation with MAL	['beginner', 'tabular data', 'anime and manga']	"Context
Fairly new to programming, I was trying to get my first project down by scraping the season pages from MyAnimeList.
Content
Using Python with BeautifulSoup as scraping modules, I collect information from the anime in a range of seasons I specified at https://myanimelist.net/anime/season
I only use information available on this page so you will not find the extensive information some can give you in other Kaggle DataSet.
You can find the code on GitHub here
Plots result from my MyAnalizer (see code on Git)"	34	878	4	crazygump	myanimelist-scrappind-a-decade-of-anime
7857	7857	Greater Reef YOLOR		['earth science']		0	11	0	morizin	greater-reef-yolor
7858	7858	Competition2		[]		0	11	0	robertwolak	competition2
7859	7859	Top 100 Science Fiction Books and their Reviews	Top 100 sci fi books of all time with their reviews	['literature', 'education', 'nlp', 'data visualization', 'pandas']	"Context
&gt; Science fiction (sometimes shortened to sci-fi or SF) is a genre of speculative fiction which typically deals with imaginative and futuristic concepts such as advanced science and technology, space exploration, time travel, parallel universes, and extraterrestrial life. It has been called the ""literature of ideas"", and it often explores the potential consequences of scientific, social, and technological innovations.
Science fiction can trace its roots back to ancient mythology, and is related to fantasy, horror, and superhero fiction, and contains many subgenres. Its exact definition has long been disputed among authors, critics, scholars, and readers.
Science fiction, in literature, film, television, and other media, has become popular and influential over much of the world. Besides providing entertainment, it can also criticize present-day society, and it is often said to inspire a ""sense of wonder"".
Content
&gt; The dataset contains one file with the overall summary of the dataset and the rest contain reviews. Each file contains 1800 reviews about 20 books (according to their rank). The dataset will receive frequent updates with new reviews and titles.
Acknowledgements
&gt; The data has been obtained by scraping Goodreads list of top 100 science fiction books of all time. Used python libraries such as BeautifulSoup, selenium, etc.
Source: Goodreads"	62	642	4	notkrishna	top-100-science-fiction-books-and-their-reviews
7860	7860	TPS_JAN_PREDS		[]		0	6	0	yamqwe	tps-jan-preds
7861	7861	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_18		[]		0	0	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-18
7862	7862	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_17		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-17
7863	7863	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_16		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-16
7864	7864	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_13		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-13
7865	7865	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_12		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-12
7866	7866	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_11		[]		0	11	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-11
7867	7867	Reddit_comment_data	comment data of subreddit Bitcoin	['finance', 'svm', 'text data', 'investing', 'currencies and foreign exchange', 'python']	"Context
To check social reaction about bitcoin price I collected this data set
Content
All comments from subreddit Bitcoin in 2020-01-01~2021-08-01"	2	10	1	thinkjin99	reddit-comment-data
7868	7868	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_08		[]		0	11	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-08
7869	7869	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_05		[]		0	2	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-05
7870	7870	New Years 2021 Tweets	"100K Tweets that mention ""New Years"" on December 31 2021"	['nlp', 'text data', 'online communities', 'social networks']	"Context
I wrote a quick script to scrape 100k tweets that mentioned the keywords ""New Year"". I pulled these tweets from the Twitter API over the span of a couple of hours so there wouldn't be a clustering of tweets from a single timezone/country.
Content
These tweets were all scraped in the evening to the night of December 31st, 2021 from the Twitter API. I ignored all tweets that just retweeted or quote tweets from other users. 
Column 1
This column is just to keep track of the tweet number in this dataset. Since the id column tracks the tweet id from Twitter and those numbers are quite large. I wanted something smaller to keep track of ids in this scope.
author_id
This column is the unique id of the author of the tweet. 
id
This column is the tweet id provided by Twitter.
text
The text of the tweet. Some tweets contain emojis, links, and mentions. 
username
The username of the author of the tweet. 
Acknowledgements
This dataset would not exist without the Twitter API. 
Inspiration
One of my main ideas of something that could be done with this data would be a sentiment analysis on how people were feeling about the new year starting."	44	474	4	thomaslazarus	new-years-2021-tweets
7871	7871	ImageNet64	ImageNet dataset of size 64x64	['computer science']		4	17	0	thimac	imagenet64
7872	7872	Mutton Price changes in 8 years	Average Mutton Price changes from 3 jan 2013 to 30 dec 2021 in Pakistan	['asia', 'categorical data']		1	18	0	ehteshamalikhan	mutton-price-changes-in-8-years
7873	7873	distil-more		[]		0	39	0	weisihao	distilmore
7874	7874	ooo9999	220102_0000_Wine_Quality	[]		0	14	0	aigotoaimasakimm	ooo9999
7875	7875	Zomato reviews 		['india', 'classification', 'clustering', 'text data', 'restaurants']	"Context
This was initially planned for simple sentimental analysis but with the rise in text based deep learning techniques, there are so much that can be done on this dataset
Content
There are 3 columns, Index, reviews and ratings from top restaurants in Delhi ncr region, India.
Inspiration
I was using this for abstract based sentimental analysis, please go ahead if you want to solve this task."	14	133	1	taaresh7	zomato-reviews
7876	7876	horseimage		[]		2	32	4	ishaanthareja007	horseimage
7877	7877	pf-142-train-4-data		[]		0	2	0	makotoikeda	pf-142-train-4-data
7878	7878	Kibor Rates - 1 year	1 year Kibor Rates in Pakistan 	['finance']		0	3	0	ehteshamalikhan	kibor-rates-1-year
7879	7879	Computer Vision		[]		0	22	0	ravinash218	computer-vision
7880	7880	Best models pet finder		[]		0	10	0	mithilsalunkhe	best-models-pet-finder
7881	7881	Clean Data for Jigsaw Rate Severity		[]		0	42	0	harits	clean-data-for-jigsaw-rate-severity
7882	7882	Propnight	General information about the video game Propnight	['games', 'video games']	"Propnight Data
This dataset contains the basic info of the characters and maps in the video game Propnight.
Data Validity
Data is updated up through the 1.0.15 patch."	1	54	0	parrotypoisson	propnight
7883	7883	Clean Data for Jigsaw Rate Severity		[]		0	42	0	harits	clean-data-for-jigsaw-rate-severity
7884	7884	Propnight	General information about the video game Propnight	['games', 'video games']	"Propnight Data
This dataset contains the basic info of the characters and maps in the video game Propnight.
Data Validity
Data is updated up through the 1.0.15 patch."	1	54	0	parrotypoisson	propnight
7885	7885	pawpularity-triplet-base		['earth and nature']		1	16	0	bhaskardey09	pawpularitytripletbase
7886	7886	predict_data		[]		0	18	0	akiyoshisutou	predict-data
7887	7887	BaseTortugas_Version01		[]		6	30	0	miguelespinozac	basetortugas-version01
7888	7888	Jobactive Employment Fund	Jobactive Employment Fund for Transaction Analysis and Classification	['australia', 'employment', 'finance', 'government']	"The document and the dataset are licensed under Creative Commons Attribution 2.5 Australia.
The dataset contains the applications of getting employment funds to cover the applicants' costs related to job searching, job training, relocation assistance, medical and mental expenses and et cetera that can help the applicants to acquire a job and become independent. Therefore, it consists of the demographic details, such as education level, age, race, and Jobactive Stream of the applicants, and the reason for getting the employment fund. There are 3 streams in Jobactive Stream: A (highly trained applicants that require minimum support), B (mediumly trained applicants that require additional support for them to be job ready) and C (applicants that require support to gain vocational or professional skills to be job ready).
The dataset requires data cleaning and data transformation before data analysis and statistical modelling.
The dataset is good for descriptive analysis, time series analysis, and classification. 
The dataset is originally from: https://data.gov.au/data/dataset/6fd24a3b-6028-473e-985a-b863bac25521 .
The Jobactive program details can be found at: https://nesa.com.au/jobactive/ ."	17	236	1	ariosliew92	jobactive-employment-fund
7889	7889	Dallas_Cowboys_Special_Play_2018_to_2020		[]		1	6	0	kushtrivedi14728	dallas-cowboys-special-play-2018-to-2020
7890	7890	Tennessee_Titans_Special_Play_2018_to_2019		[]		1	17	0	kushtrivedi14728	tennessee-titans-special-play-2018-to-2019
7891	7891	Tampa_Bay_Buccaneers_Special_Play_2018_to_2020		[]		0	14	0	kushtrivedi14728	tampa-bay-buccaneers-special-play-2018-to-2020
7892	7892	Los_Angeles_Rams_Special_Play_2018_to_2020		[]		1	3	0	kushtrivedi14728	los-angeles-rams-special-play-2018-to-2020
7893	7893	Green_Bay_Packers_Special_Play_2018_to_2020		[]		1	22	0	kushtrivedi14728	green-bay-packers-special-play-2018-to-2020
7894	7894	Soccer Player Data from fbref.com	Soccer player performance data for the Big 5 European leagues	['football', 'sports']	"Context
This dataset is the byproduct of a web scraper I wrote as a side project. If you're interested, you can check it out here: https://github.com/biniyamYohannes/fbref-scraper.
Content
The dataset contains player performance data for ~3000 players currently playing in the Big 5 European soccer leagues. 
The dataset structure closely follows the format of the tables from fbref (for example https://fbref.com/en/players/d70ce98e/Lionel-Messi). General information about a player such as name, position, etc., can be found in the info.csv file. The info.csv file has a primary key (id) which is unique for every player.
The remaining files contain performance data. Every row of a file is identified by an (id, season, squad) primary key. These tables can be joined with the info table on the (id) column. 
Keep in mind that the data for season 2021-2022 only contains numbers for about half the season.
Acknowledgements
All the data belongs to fbref.com and Sports Reference LLC.
Image credit: Photo by <a href=""https://unsplash.com/@viennachanges?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Vienna Reyes</a> on <a href=""https://unsplash.com/wallpapers/sports/soccer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>"	125	771	6	biniyamyohannes	soccer-player-data-from-fbrefcom
7895	7895	Causes of Death in Indonesia	Based on Official Published Reports (Annual Data)	['diseases', 'public health', 'health', 'public safety', 'covid19']	"Context
Dataset Penyebab Kematian di Indonesia dikompilasi dalam bentuk file CSV untuk memudahkan analisis.
Content
Data dikompilasi berdasarkan data eksplisit yang disebutkan di dalam laporan. Tidak mengikutsertakan data estimasi dan data implisit (yang hanya menyebutkan ""banyak"", ""sedikit"", dicantumkan relatif terhadap populasi, multi-intepretasi, dll). Dataset ini terdiri dari 2 file, yaitu sbb:
1. ""Penyebab Kematian di Indonesia yang Dilaporkan - Raw.csv"" : berisi data yang dikompilasi dari Profil Kesehatan Indonesia Tahun 2004 s.d Tahun 2019 dan data COVID-19. URL sumber data tercantum pada kolom ""Source URL"". Kolom ""Type"" (Jenis Penyebab Kematian) diisi oleh saya sendiri, tidak berasal dari sumber yang telah disebutkan, namun terinspirasi dari Profil Kesehatan Indonesia Tahun 2019 yang membagi Krisis Kesehatan menurut Jenis Bencana, yaitu Bencana Sosial, Bencana Alam, dan Bencana Non Alam. Dalam konteks dataset ini, saya menggunakan 3 jenis seperti itu tapi sedikit dimodifikasi, yaitu menjadi: ""Bencana Sosial"", ""Bencana Alam"", dan ""Bencana Non Alam dan Penyakit""
1. ""Penyebab Kematian di Indonesia yang Dilaporkan - Clean.csv"" : berisi data yang telah dibersihkan dari file ""Penyebab Kematian di Indonesia yang Dilaporkan - Raw.csv"", melalui metode: a) Untuk row data yang memiliki data redudancy yang lebih dari 1, dipilih data dari laporan tahun terakhir, dengan asumsi bahwa laporan tahun terakhir merupakan update/perbaikan terbaru dari data tahun yang lalu; b) Untuk row data yang memiliki data redudancy yang lebih dari 1 di tahun yang sama, dipilih data yang memiliki jumlah total paling besar. Dalam konteks validitas data, saya menyarankan Anda untuk menggunakan file ""Clean"" ini daripada file ""Raw"". File Raw tetap saya publikasikan untuk transparansi dan koreksi apabila ada informasi lebih lanjut dari Anda atau visitor lain.
Saya telah membuat video visualisasi berdasarkan dataset ini (data COVID-19 per tanggal 28/11/2020), dapat dilihat pada Youtube (klik di sini).
Acknowledgements
Terima kasih kepada instansi yang telah mempublikasikan data-data ini, seperti: Kementerian Kesehatan (data Profil Kesehatan Indonesia) melalui website pusdatin.kemkes.go.id dan Satuan Tugas Penanganan COVID-19 melalui website covid19.go.id"	1537	7873	27	hendratno	cause-of-death-in-indonesia
7896	7896	image-model-structures		['arts and entertainment']		0	33	0	a24998667	image-model-structures
7897	7897	Kansas_CIty_Chiefs_Special_Plays_2018_to_2020		[]		0	4	0	kushtrivedi14728	kansas-city-chiefs-special-plays-2018-to-2020
7898	7898	Dollar to LBP (Parallel Market) Exchange Rate	Dollar to LBP (Parallel Market) Exchange Rate 2018 - 2021	['currencies and foreign exchange']		0	41	1	najielkotob	dollar-to-lbp-parallel-market-exchange-rate
7899	7899	pf-142-train-3-data		[]		0	7	0	makotoikeda	pf-142-train-3-data
7900	7900	PetFinder-Model49		[]		0	12	0	lftuwujie	petfindermodel49
7901	7901	startup		[]		0	27	0	t2ravage	startup
7902	7902	Polars fast DataFrame library		['data analytics', 'tabular data', 'pandas']	"Context
Polars is an alternative for Pandas that is significantly faster. See H2O's benchmark here.
Content
This dataset includes the wheel for polars 0.12.7 and it's requirement typing_entension 4.0.1.
They can be installed offline in Kaggle notebooks.
Acknowledgements
https://github.com/pola-rs/polars
https://www.pola.rs/"	3	38	0	rluethy	polars-fast-dataframe-library
7903	7903	LifeExpectancyData	Life Expectancy Data	['social science']	"Life Expectancy Prediction Using Artificial Intelligence:
Research Paper: https://docs.google.com/document/d/1Abwx7C97sMjsfow5Xk8GOCDblNaQr8T8WXh7SIJAbVo/edit?usp=sharing
Introduction:
According to the survey from PwC (PricewaterhouseCoopers) report in 2016, data have shown that nearly half (47%) of 18-34 age group surveyed had changed their eating habits towards a healthier diet and further data has shown that 53% of the age 18-34 claimed that they have planned to change their eating habits to be healthier over the next year. According to research done by LiveScience, eating healthy and doing physical activity can in fact increase our life expectancy, also in one of the articles from BBC (British Broadcasting Corporation) “Do we really live longer than our ancestors? ” have stated that in 1841, a baby girl and boy was expected to live just about 40 years of age, but in 2016 a baby girl or boy was expected to live till 80 years of age. Controllable factors like eating healthy and doing exercise regularly can in fact increase our life expectancy. But can non-controllable factors like Country’s status, mortality rates, GDP, schooling, average income, government’s expenditure on health and the rate of child deaths possibly affect our life expectancy? To answer those concerns, we will input data from a Dataset called “Life Expectancy(WHO)” provided by Kumar Rajarshi in Kaggle and with the help of machine learning to process a considerable amount of data to train and analyze and make a prediction of life expectancy based on the value we feed to the algorithm.
Project Details:
For this project, I have used the Dataset called “Life Expectancy(WHO)” provided by Kumar Rajarshi from Kaggle, to try to predict the total life expectancy by inputting non-controllable factors according to the data set like Country’s status, mortality rates, GDP, schooling, average income, government’s expenditure on health and the rate of child deaths to answer will non-controllable factor affect our life expectancy."	41	220	0	just249	lifeexpectancydatacsv
7904	7904	LOBSTER_DATA_SET		[]		9	110	2	tikoboss	lobster-data-set
7905	7905	photo for opencv		[]		0	9	0	bhavinmoriya	photo-for-opencv
7906	7906	nyuv2_FOD		[]		2	14	0	antocad	nyuv2-fod
7907	7907	Pakistan Railways Shapefiles		[]		0	24	0	zerghamwarraich1	pakistan-railways-shapefiles
7908	7908	World Zip 		[]		1	12	0	zerghamwarraich1	world-zip
7909	7909	Pakistan Places Shapefiles and Datasets		[]		0	22	0	zerghamwarraich1	pakistan-places-shapefiles-and-datasets
7910	7910	corrected-dataset-41-greater		[]		0	14	0	akshatgupta2810	correcteddataset41greater
7911	7911	COVID vaccination vs. mortality 	you can find COVID vaccination percentage and progress beside death counts	['global', 'public health', 'health', 'medicine', 'public safety', 'covid19', 'python']	"Context
The COVID-19 outbreak has brought the whole planet to its knees.More over 4.5 million people have died since the writing of this notebook, and the only acceptable way out of the disaster is to vaccinate all parts of society. Despite the fact that the benefits of vaccination have been proved to the world many times, anti-vaccine groups are springing up all over the world. This data set was generated to investigate the impact of coronavirus vaccinations on coronavirus mortality. 
Content
| country | iso_code | date | total_vaccinations | people_vaccinated | people_fully_vaccinated | New_deaths | population | ratio |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| country name  | iso code for each country | date that this data belong | number of all doses of COVID vaccine usage in that country | number of people who got at least one shot of COVID vaccine | number of people who got full vaccine shots  | number of daily new deaths | 2021 country population | % of vaccinations in that country at that date = people_vaccinated/population * 100 |
Data Collection
This dataset is a combination of the following three datasets:
1.https://www.kaggle.com/gpreda/covid-world-vaccination-progress 
2.https://covid19.who.int/WHO-COVID-19-global-data.csv 
3.https://www.kaggle.com/rsrishav/world-population 
you can find more detail about this dataset by reading this notebook: 
https://www.kaggle.com/sinakaraji/simple-linear-regression-covid-vaccination
Countries in this dataset:
||||||
| --- | --- | --- | --- | --- |
| Afghanistan | Albania | Algeria | Andorra | Angola |
| Anguilla | Antigua and Barbuda | Argentina | Armenia | Aruba |
| Australia | Austria | Azerbaijan | Bahamas | Bahrain |
| Bangladesh | Barbados | Belarus | Belgium | Belize |
| Benin | Bermuda | Bhutan | Bolivia (Plurinational State of) | Brazil |
| Bosnia and Herzegovina | Botswana | Brunei Darussalam | Bulgaria | Burkina Faso |
| Cambodia | Cameroon | Canada | Cabo Verde | Cayman Islands |
| Central African Republic | Chad | Chile | China | Colombia |
| Comoros | Cook Islands | Costa Rica | Croatia | Cuba |
| Curaçao | Cyprus | Denmark | Djibouti | Dominica |
| Dominican Republic | Ecuador | Egypt | El Salvador | Equatorial Guinea |
| Estonia | Ethiopia | Falkland Islands (Malvinas) | Fiji | Finland | 
| France | French Polynesia | Gabon | Gambia | Georgia | 
| Germany | Ghana | Gibraltar | Greece | Greenland |
| Grenada | Guatemala | Guinea | Guinea-Bissau | Guyana | 
| Haiti | Honduras | Hungary | Iceland | India |
| Indonesia | Iran (Islamic Republic of) | Iraq | Ireland | Isle of Man |
| Israel | Italy | Jamaica | Japan | Jordan |
| Kazakhstan | Kenya | Kiribati | Kuwait | Kyrgyzstan |
| Lao People's Democratic Republic | Latvia | Lebanon | Lesotho | Liberia |
| Libya | Liechtenstein | Lithuania | Luxembourg | Madagascar |
| Malawi | Malaysia | Maldives | Mali | Malta |
| Mauritania | Mauritius | Mexico | Republic of Moldova | Monaco |
| Mongolia | Montenegro | Montserrat | Morocco | Mozambique |
| Myanmar | Namibia | Nauru | Nepal | Netherlands |
| New Caledonia | New Zealand | Nicaragua | Niger | Nigeria |
| Niue | North Macedonia | Norway | Oman | Pakistan |
| occupied Palestinian territory, including east Jerusalem |||||
| Panama | Papua New Guinea | Paraguay | Peru | Philippines |
| Poland | Portugal | Qatar | Romania | Russian Federation |
| Rwanda | Saint Kitts and Nevis | Saint Lucia |
| Saint Vincent and the Grenadines | Samoa | San Marino | Sao Tome and Principe | Saudi Arabia |
| Senegal | Serbia | Seychelles | Sierra Leone | Singapore |
| Slovakia | Slovenia | Solomon Islands | Somalia | South Africa |
| Republic of Korea | South Sudan | Spain | Sri Lanka | Sudan |
| Suriname | Sweden | Switzerland | Syrian Arab Republic | Tajikistan |
| United Republic of Tanzania | Thailand | Togo | Tonga | Trinidad and Tobago |
| Tunisia | Turkey | Turkmenistan | Turks and Caicos Islands | Tuvalu |
| Uganda | Ukraine | United Arab Emirates | The United Kingdom | United States of America |
| Uruguay | Uzbekistan | Vanuatu | Venezuela (Bolivarian Republic of) | Viet Nam |
| Wallis and Futuna | Yemen | Zambia | Zimbabwe ||"	2804	16104	47	sinakaraji	covid-vaccination-vs-death
7912	7912	City of Houston Employment by Occupation & by Sex	City of Houston 2019 Employment by Occupation by Sex Population 16 years & Over	[]		1	50	0	cordenuru	city-of-houston-employment-by-occupation
7913	7913	800k+ COVID-19 Vaccine Tweets	Tweets about the COVID-19 vaccines	['text data', 'covid19']	"Context
The dataset contains tweets who is used vaccine as a keyword.
Content
The data contains the following information:
Username: Username of the person who tweeted
User Display Name: Name of the person who tweeted
User Description: Description of the person who tweeted
User Location: Location of the person who tweeted
User Verified: Whether the person who tweeted is verified
User Protected: Whether the person who tweeted is protected
User Followers Count: Number of followers of the person who tweeted
User Friends Count: Number of friends of the person who tweeted
User Statuses Count: Number of tweets of the person who tweeted
User Favourites Count: Number of favourites of the person who tweeted
Tweet Content: Content of the tweet
Tweet Language: Language of the tweet
Tweet Retweet Count: Number of retweets of the tweet
Tweet Quote Count: Number of quotes of the tweet
Tweet Reply Count: Number of replies of the tweet
Tweet Like Count: Number of likes of the tweet
Tweet ID: ID of the tweet
Tweet URL: URL of the tweet
Tweet Date: Date of the tweet"	33	379	6	kerneler	15-million-covid19-vaccine-tweets
7914	7914	face_classification		[]		0	25	0	kleber0	face-classification
7915	7915	PetFinder raw data for TPU 6-11		[]		0	14	0	muneaki	petfinder-raw-data-for-tpu-611
7916	7916	tfrec_0_40		[]		0	20	0	akshatgupta2810	tfrec-0-40
7917	7917	PC Games Sales	Dataset of the best selling pc games	['games', 'board games', 'card games', 'video games']	"Context
Gaming industry is an interesting field to explore, it would be fun knowing who is the most popular publishers and developers and which games are the most popular.
Content
Name: Name of the game
Sales: Sales of the game in Millions
Series: Series of the game
Release: Release date of the game
Genre: Genre of the game
Developer: Developer of the game
Publisher: Publisher of the game
Questions to be answered
Which genre is the most popular ?
Which publisher published most of the games ?
Which developer developed most of the games ?
Which series is the most popular ?"	986	6108	34	khaiid	most-selling-pc-games
7918	7918	FDIC - Banks In Washington State 	Bank institutions established in Washington state	['united states', 'history', 'finance', 'government', 'banking']	A dataset of FDIC-insured institutions established in Washington state between 1869 and 2020. Records include demographic information related to the institution such as locational detail (name, city, state, etc) and operating status. The dataset also includes a url to the FDIC’s public records for up to date info for each institution.	17	137	2	donnetew	fdic-banks-in-washington-state-17822020
7919	7919	FTIR honey dataset	Honey Data Set for classification	['earth and nature', 'biology', 'chemistry', 'classification']	I need your opinion on my data file. The file contains FTIR measurements of 1 sugar and 16 types of honey samples.FTIR measurements were made again by adding 10%, 20%, and 40% sugar to 16 honey samples. FTIR measurements are in the wavelength range of 4000-650 nm. When the FTIR measurement of unknown honey is taken, I want to find out what type of honey it is and how much sugar has been added. What machine learning method would you suggest?	2	48	0	alfaturk	ftir-honey-dataset
7920	7920	HMBD v1	HMBD v1: Arabic Handwritten Characters Dataset	['artificial intelligence', 'computer science', 'computer vision', 'deep learning', 'image data']	"HMBD Dataset v1
HMBD v1 is an Arabic Handwritten Characters Dataset.
(1) Introduction:
The HMBD dataset captures the different positions of the Arabic handwritten characters; isolated, beginning, middle, and end; besides, the numbers. 
(2) Published Paper:
The HMBD dataset is published in ""A new Arabic handwritten character recognition deep learning system (AHCR-DLS)"" where the construction, pre-processing, and compilation phases are discussed.
Link: https://link.springer.com/article/10.1007/s00521-020-05397-2
DOI: https://doi.org/10.1007/s00521-020-05397-2
(3) Dataset Specifications:
Version: 1.0.
The number of classes (categories) is 115. 
The number of unique images is 54,115. 
The number of volunteers is 125.
Each image dimension is 300 x 300 (i.e. width = 300 and height = 300).
Background color: White.
Character color: Black.
(4) Dataset Template:
The seven-page dataset template file used in collecting the dataset is stored in ""Dataset Template v1.pdf"".
(5) Directory Hierarchy:
The hierarchy of the folder is stored in ""tree.txt"" and ""folders.txt"". The first contains the folders' and files' names while the latter one contains only the folders' names.
(6) Citation:
Balaha, H.M., Ali, H.A., Saraya, M. et al. A new Arabic handwritten character recognition deep learning system (AHCR-DLS). Neural Comput & Applic (2020). https://doi.org/10.1007/s00521-020-05397-2
@article{balaha2020new,
  title={A new Arabic handwritten character recognition deep learning system (AHCR-DLS)},
  author={Balaha, Hossam Magdy and Ali, Hesham Arafat and Saraya, Mohamed and Badawy, Mahmoud},
  journal={Neural Computing and Applications},
  pages={1--43},
  year={2020},
  publisher={Springer}
}
(7) Licence:
The HMBD dataset is licensed by ""CC BY-NC-SA 4.0"".
&gt;The ""CC BY-NC-SA 4.0"" is one of the Creative Commons (CC) licenses and allows the different users to share the dataset only if they (1) give the credits to the copyright holders, (2) do not use the dataset for any commercial purposes, and (3) distribute any additions, transformations or changes to the dataset under this same license.
Full Description: https://creativecommons.org/licenses/by-nc-sa/4.0/"	35	1360	7	hossammbalaha	hmbd-v1-arabic-handwritten-characters-dataset
7921	7921	Iris_dataset		[]		1	33	0	siddheshkadam	iris-dataset
7922	7922	techvsoil	closing share prices of a selection of technology and oil companies over 1 year	[]		0	18	0	georgetyler1	techvsoil
7923	7923	IMF Zinc Price Forecast Dataset	The Dataset to forecast the future prices of zinc.	['business', 'manufacturing', 'beginner', 'time series analysis', 'tabular data']	"Description:
A simple yet challenging project, to forecast the IMF commodity price of Zinc, based on monthly totals zinc price recorded from 1980 to 2016.
Can you overcome these obstacles & Forecast its future prices?
This data frame contains the following columns:
Month : The month of observation
Price : Average Prices of zinc in that particular month
Acknowledgement
This dataset is referred from Kaggle.
Objectives:
Understand the Dataset & cleanup (if required).
Perform the necessary checks like stationarity & DF on the Dataset.
Build a forcasting model to forecast zinc prices."	35	190	4	yasserh	imf-zinc-price-forecast-dataset
7924	7924	lol_match_stats_and_team_abbreviation _matrix	21,000 League of Legends matches, 2018-2021 	['video games']	"League of Legends matches between 2018-2021 with a team abbreviation matrix. Includes 21,000 match stats and 374 unique tournaments/series. 
If you found this dataset useful check out https://gol.gg/esports/home/ for more match results and in depth analysis. 
Note: I have no affiliation with this site."	75	834	15	calenmcnickles	lol-match-stats-and-team-abbreviation-matrix
7925	7925	PAW-beit-large-patch16-512		[]		1	12	0	a24998667	paw-beit-large-patch16-512
7926	7926	Aquaculture-Dataset	This Dataset Contains the details about the Aquaspecies	['earth and nature']	"Context
This Datasets contains the Details of the aquaculture species Exported and Imported and about the cat fish species.
Content
It has the data of the fish varieties that are Imported and Exported in the recent years.
Acknowledgements
These data collected with the help of the @data.world Thanks a lot!
Inspiration
Try to find the amount , quality and quantity of the aqua species were imported and exported in the recent years using Machine Learning Algorithms."	31	359	11	balavashan	aquaculturedataset
7927	7927	INEV Econ Olympiad		['earth and nature']		1	14	0	frantiekmaek	inev-econ-olympiad
7928	7928	nsk_image_search3_man3		[]		1	5	0	motono0223	nsk-image-search3-man3
7929	7929	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_10		[]		0	18	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-10
7930	7930	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_09		[]		0	1	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-09
7931	7931	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_07		[]		0	13	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-07
7932	7932	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_06		[]		0	9	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-06
7933	7933	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_04		[]		0	9	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-04
7934	7934	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_03		[]		0	9	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-03
7935	7935	Hodge data		[]		1	38	0	tutkudorukbayramoglu	hodge-data
7936	7936	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_02		[]		0	8	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-02
7937	7937	2021_DATA_GLD_TFRECORD_EXT_DOWNLOAD_01		[]		0	21	0	fdfyaytkt	2021-data-gld-tfrecord-ext-download-01
7938	7938	Cat, dog classification (animal dataset)		[]		0	21	0	crackmind	cat-dog-classification-animal-dataset
7939	7939	Georeferenced Forest-Fires 2017 Chile🔥🔥 	Georeferenced forest-fires and Characteristics between 01-07-2016 and 30-06-2017	['biology', 'natural disasters', 'government', 'geospatial analysis', 'tabular data', 'online communities']	"Source: http://catalogo.geoportal.cl/geoportal/catalog/search/resource/resumen.page?uuid=%7BC63BC15F-D431-4598-A871-2285C7DDEE27%7D
Columns:
Temporada: Corresponds to the statistical period that starts a season being from July 1 of year x and ends on June 30 of year x+1; if the season is 2014-2015, it is identified as season 2015
Codreg: Codes of the Political Administrative Division associated with regions of Chile. (As of 2020 there are 15 regions)
Codprov: Codes of the Political Administrative Division associated with the provinces of Chile
Codcom: Codes of the Political Administrative Division associated to the communes of Chile
Ámbito: Scope of the territory where it was affected (CONAF or Forestry Company)
Numero: Corresponds to the correlative number of the fire in a region
Nombre_inc: Corresponds to the name assigned to the fire
UTM_este: Corresponds to the value of the East coordinate in UTM projection
UTM_norte: Corresponds to the value of the North coordinate in UTM projection
Inicio_C: Corresponds to the roads or others, near the place where the fire started, to establish the causal relationship of the origin of the fire
Combus_i: Refers to the type of material with which the fire started
Causa_Gene: Corresponds to the estimated general cause of the wildfire 
Causa_Espe: Corresponds to the estimated specific cause of the wildfire
Pino_0010: Corresponds to the burned area of pine trees between 0 and 10 years old
Pino_11_17: Corresponds to the burned area of pine trees between 11 and 17 years old
Pino_18: Corresponds to the burned area of pine trees over 18 years old
Eucalipto: Corresponds to the burned area of Eucalyptus
Otras_plan: Corresponds to the area affected by the fire of artificial plantations other than those mentioned above, which may be exotic or native, in addition to timber and/or forage species, excluding fruit tree species
Total_plan: Corresponds to the sum of the columns: Pino_0010, Pino_11_17, Pino_18, Eucalipto and Otras_plan
Arbolado: Corresponds to the area affected by native tree vegetation
Matorral: Corresponds to the affected area of native scrub vegetation
Pastizal: Corresponds to the affected area of native grassland vegetation
Total_veg: Corresponds to the total affected area of natural vegetation, which includes the sum of tree, shrub, and grassland
Agricola: Superficies afectadas por incendios que no son consideradas superficies forestales y corresponden a superficie agrícola
Desechos: Area covered with agricultural or forestry waste or dead wood (waste)
Total_otra: Total other areas are all areas affected by the fire that are not considered forest areas
Sup_t_a: Total area affected, corresponds to the sum of the total area affected in plantations, the total area affected in natural vegetation, and the total area affected in other areas
Long: Longitude
Lat: Latitude"	71	625	10	sandorabad	georeferenced-forestfires-2017-chile
7940	7940	Indian Startups funding(in 2021)	Dataset is all about the funding of Indian startups in 2021	['business', 'finance', 'economics', 'data analytics', 'investing']	"<img src=""https://www.ngosify.com/wp-content/uploads/2021/12/new-year-gifs-2022-1.gif"">
HAPPY NEW YEAR TO ALL THE KAGGLERS
The term startup refers to a company in the first stages of operations. Startups are founded by one or more entrepreneurs who want to develop a product or service for which they believe there is demand. These companies generally start with high costs and limited revenue, which is why they look for capital from a variety of sources such as venture capitalists.
KEY TAKEAWAYS
A startup is a company that's in the initial stages of business.
Founders normally finance their startups and may attempt to attract outside investment before they get off the ground.
Funding sources include family and friends, venture capitalists, crowdfunding, and loans.
Startups must also consider where they'll do business and their legal structure.
Startups come with high risk as failure is very possible but they can also be very unique places to work with great benefits, a focus on innovation, and great opportunities to learn."	111	561	8	ramjasmaurya	indian-startupsin-2021
7941	7941	Features_2_Indicators_parquet		[]		0	11	0	shathur14	features-2-indicators-parquet
7942	7942	SolanaPriceHistorical		[]		0	15	0	daduky123	solanapricehistorical
7943	7943	Social network dataset 		['social networks']		0	35	1	crackmind	social-network-dataset
7944	7944	Anime Recommendation	10000 anime and their correlated anime based on user descriptions	['anime and manga']	"Context
This dataset contains information about anime and recommendation of a similar anime based on their perceived similarity by users. There are similar datasets on Kaggle but this is more comprehensive. It serves to show a basic correlation in terms of plot, character designs, etc among the animes. 
The database is expected to be updated every 120 days. Upcoming additions :
10,000 more anime entries. 
Content
The dataset contains 10015x3 data fields. Column names are self-explanatory.
Acknowledgements
Thanks to:
MyAnimeList for providing manga data.
Jikan API for providing user's preference."	3	94	1	astronautelvis	anime-recommendation
7945	7945	popular names the year when I was Born	names for both mail and female	[]		0	18	1	adehghani	popular-names-the-year-when-i-was-born
7946	7946	jigsaw_xlnet_train_256		[]		3	132	0	vaby667	jigsaw-xlnet-train-256
7947	7947	Smart Device Data	Cleaned data from Fitbit Fitness Tracker Data	['exercise', 'business']	"Context
This data was collected to complete the Google data analytics certification capstone project.
Content
This data from approximately 33 FitBit users was collected. The data includes users' activity levels, calories burned, sleep data, and more. Each dataset contains users' Id's and a timestamp.
Inspiration
This is the first step in my data analytics journey!"	12	130	0	jcolbert	smart-device-data
7948	7948	DragonBall Z		['anime and manga']		3	30	0	pablitopiova	dragonball-z
7949	7949	tensorflow-great-barrier-reef		['earth and nature']		339	71	3	yondraco	tensorflowgreatbarrierreef
7950	7950	crypto_models		[]		0	114	0	dwoodlock	crypto-models
7951	7951	COCO VGG-19 Features	Features for COCO Images	[]	"Last convolutional features of VGG-19 for all images in the COCO dataset. Developed for use in VQA modelling. 
Features are of size 14x14x512 extracted from images of size 448x448. Reshaped to be size 196x512.
Features are stored as .npy files with image_id as the filename.
Currently only contains features for train and val images in COCO 2014 dataset."	0	40	1	itsmariodias	coco-vgg19-features
7952	7952	Cricket_Matches_Analysis	Cricket matches analysis of IPL all seasons	['cricket', 'intermediate', 'exploratory data analysis', 'data visualization', 'data analytics']	This is a dataset of all matches played in all seasons of the Indian version of 20-20 cricket, widely known as Indian Premier League(IPL)	154	599	12	ishaanthareja007	cricket-matches-analysis
7953	7953	choix 0.3.4	https://github.com/lucasmaystre/choix	[]		0	9	0	coldfir3	choix-034
7954	7954	CSVDatsset		[]		1	37	0	mohammedaaltaha	csvdatsset
7955	7955	multi_image		[]		0	16	0	inzamulhoque	multi-image
7956	7956	starfish-data		['earth and nature']		0	19	0	killuazoldyck6066	starfish-data
7957	7957	Real Estate 		[]		0	21	1	rhitazajana	real-estate
7958	7958	petfinder_ensumble		[]		3	39	0	ktakita	petfinder-ensumble
7959	7959	tracks		['music']		0	20	0	noelko	tracks
7960	7960	video-data		[]		0	5	0	mahendragaddam	videodata
7961	7961	Massey Ferguson Tractors Nigeria	Massey Ferguson Tractors For Sale in Nigeria	[]		0	16	0	qamerabbas	massey-ferguson-tractors-nigeria
7962	7962	bucket	Amazon S3: Object storage built to retrieve any amount of data from anywhere	['business']		1	17	1	rishirajacharya	bucket
7963	7963	TEST2_FOR_TEST2		[]		26	1432	1	maxmar	test2-for-test2
7964	7964	ab_testing	ab testing, ab, testing	[]		0	8	1	blirinddanjolli	ab-testing
7965	7965	efficientnetb0		[]		0	20	0	e94076039	efficientnetb0
7966	7966	vit_l_p32_384_classification_ext_0_1		[]		0	1	0	xyzdivergence	vit-l-p32-384-classification-ext-0-1
7967	7967	Annotated Rickshaw Images from Bangladesh	Annotated Rickshaw images for rickshaw detection and tracking	['arts and entertainment']		0	47	2	mansibmursalin	annotated-rickshaw-images-from-bangladesh
7968	7968	yunbert		[]		2	12	0	weisihao	yunbert
7969	7969	VanillaSTFTTransformerTest3		[]		1	25	0	gernotzcklein	vanillastfttransformertest3
7970	7970	Youtube Videos - 5-Minute Crafts	Massive dataset of Youtube Channel - 5-Minute Videos containing metadata & stats	['arts and entertainment', 'beginner', 'intermediate', 'tabular data', 'text data']	"Context
5-Minute Crafts is the Top 10 Most Viewed and Subscribed channel and this is what amazed me. I want to find the insights which lead the success of the channel.
Content
The data represents the Video Meta data, description, tags and most important statistics of the video.
Acknowledgements
Youtube and 5-Minute Crafts Channel
Inspiration
Most liked topic in the channel.
View, Like and Comment count based on the video tags?
What does the description say about the video?
What are the most used tags?"	40	494	6	mikitkanakia	youtube-videos-5minute-videos
7971	7971	Titanic		['statistical analysis', 'classification', 'logistic regression']		1	21	1	rhitazajana	titanic
7972	7972	GPTiny		[]		5	42	0	scirpus	gptiny
7973	7973	Car Purchase		[]		16	25	0	sgunited	car-purchase
7974	7974	New-13		[]		0	5	0	gmlwlskim	new13
7975	7975	model140		[]		0	15	0	malekbadreddine	model140
7976	7976	cell-images-for-detecting-malaria	Infected and non-infected cells splitted	[]		0	14	0	antoniofe	cellimagesfordetectingmalaria
7977	7977	2017 usa rates		['finance']		0	3	0	ssybcccbbbyyy	2017-usa-rates
7978	7978	cit-HepTh Graph (SNAP)		[]		0	13	0	wolfram77	graph-snap-cit-hepth
7979	7979	BankChurn		[]		3	7	0	abrahamanderson	bankchurn
7980	7980	pf-142-train-1-data		[]		0	10	0	makotoikeda	pf-142-train-1-data
7981	7981	cit-HepPh Graph (SNAP)	Temporal Paper citation network of Arxiv High Energy Physics category	['research', 'earth and nature', 'physics', 'education']	"Arxiv HEP-PH (high energy physics phenomenology ) citation graph is from the e-print arXiv and covers all the citations within a dataset of 34,546 papers with 421,578 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this.
The data covers papers in the period from January 1993 to April 2003 (124 months). It begins within a few months of the inception of the arXiv, and thus represents essentially the complete history of its HEP-PH section.
The data was originally released as a part of 2003 KDD Cup.
Added an additional temporal-edges file cit-HepPh-temporal.txt, which follows the same formatting as that of other temporal graphs in the Stanford Large Network Dataset Collection."	1	33	2	wolfram77	graph-snap-cit-hepph
7982	7982	Creating Folds		[]		1	6	0	ni7san	creating-folds
7983	7983	f2oof1754		[]		1	14	1	nischaydnk	f2oof1754
7984	7984	f1oof1747		[]		1	15	1	nischaydnk	f1oof1747
7985	7985	y_test		[]		0	30	0	yijiangfan	y-test
7986	7986	y_train		[]		0	17	0	yijiangfan	y-train
7987	7987	X_test		[]		0	23	0	yijiangfan	x-test
7988	7988	X_train		[]		0	2	0	yijiangfan	x-train
7989	7989	Crocodile|Alligator|Gharial Classification fastai	This is a quick way to download image data from DuckDuckGo using fastai.	['biology', 'animals', 'beginner', 'cnn', 'image data', 'multiclass classification']	"Context
This dataset is inspired from fastai's quick approach to scrape images from DuckDuckGo. In their implementation, they used it to extract different types of bears. 
Content
The images are downloaded from DuckDuckGo using fastai's search_images_ddg  function. I've created a repo to replicate this scraping process.
In addition to arranging the images in three separate category folders, there is a pandas dataframe called data.csv containing the following columns:
Image URL
Image local path
Image format: jpg 
Crocodilia group: crocodile, alligator, and gharial
In my code, I have already filtered out the jpg format files for the ease of analysis.
Acknowledgements
Kudos to the fastai team for creating a seamless method to extract image data for quick analysis. The notebook from which part of my work is derived can be found here.
Inspiration
On the surface, the problem is a little tricky as differentiating the alligators and crocodiles would be difficult given their similar characteristics. Major visible difference between the two is that the alligator has a wider mouth. Gharial's have a narrow snout so we can observe a distinct feature.
I'm looking for and also will be working on two things primarily: <br>
1. What is the accuracy with which we can classify the images?
2. Is it possible to get three classes from unsupervised learning? Or do we get more classes due to even different species of crocodiles such as Nile crocodile and Saltwater crocodile."	122	1421	21	rrrohit	crocodile-gharial-classification-fastai
7990	7990	Motorbike Sales in South Africa	Checking the dynamics of how Motorbikes are sold in South Africa	[]		3	37	0	conradmogane	motorbike-sales-in-south-africa
7991	7991	nlpaug Package for Augmentation of NLP Datasets	Version 1.10.0 from GitHub to allow usage in offline notebooks	['education', 'software', 'nlp']	"This library provides augmentation methods for NLP work. It is provided here as a dataset, downloaded from GitHub, to allow usage in offline notebooks. For the original, go to:
https://github.com/makcedward/nlpaug"	7	73	1	andypenrose	nlpaug-from-github
7992	7992	petfinder2-detect-info	train.csv with object detect info by rcnn_R_50_FPN_3x	[]		97	100	0	masaishi	petfinder2-detected-info
7993	7993	Google Trends		['internet']		1	34	1	infinator	google-trends
7994	7994	cup_tr		[]		0	1	0	yijiangfan	cup-tr
7995	7995	Badminton Game Data for Loh Kean Yew (2012-2020)	Tournaments and Matches Data for Loh Kean Yew from 2012 to 2020	['games', 'sports', 'exploratory data analysis', 'data visualization']	"Context
The dataset contains the match data of Loh Kean Yew (newly crowned Badminton World Champion 2021).
Content
The dataset contains matches from tournaments listed on Loh Kean Yew's page on BWF website.
- Year: Year of the tournament/match
- Tournament: Name of the tournament
- Round: The round in which the match belongs to
- Match Time: The match time played
- Player 1: Name of Player 1
- Player 2: Name of Player 2
- Player 1 Set 1: Score of Player 1 in Set 1
- Player 2 Set 1: Score of Player 2 in Set 1
- Player 1 Set 2: Score of Player 1 in Set 2
- Player 2 Set 2: Score of Player 2 in Set 2
- Player 1 Set 3: Score of Player 1 in Set 1
- Player 2 Set 3: Score of Player 2 in Set 2
Acknowledgements
The dataset was collected from BWF official website (https://bwfbadminton.com/). I wrote codes to web scrape the information.
Inspiration
Inspired by Singapore's first newly crowned Badminton World Champion, I was curious to find out Loh Kean Yew's performance over the past years."	3	76	0	andyphua	badminton-game-data-for-loh-kean-yew-20122020
7996	7996	Reviews	Classify using user reviews	['ratings and reviews']		0	10	0	varun23	reviews
7997	7997	Janna Matrices		[]		0	4	0	wolfram77	matrices-janna
7998	7998	sample_pic		[]		0	34	0	harishkvel	sample-pic
7999	7999	AnimeGetchu 64x64	Anime face from Brian Chao but resized to 64x64. All credits goes to Mckinsey666	['anime and manga']		0	31	0	sebastiendelprat	animegetchu-64x64
8000	8000	Test Data for Segmentation - Land and Water plots		['earth and nature']		0	10	0	animeshsinha1309	test-data-for-segmentation-land-and-water-plots
8001	8001	Rotten Tomatoes		['movies and tv shows']		10	270	10	mvanshika	rotten-tomatoes
8002	8002	extend_mnist	mnist+the four operations of arithmetic and parentheses	['computer science', 'programming', 'beginner', 'tabular data', 'image data', 'multiclass classification']	"Description
The data were produced by Zhou Xiong, Zhong Yuhong, Yang Fan, and Wu Peigang from Harbin Institute of technology. The data was published in their blog:
https://blog.csdn.net/qq_34919953/article/details/81112313.  I reverse the color and transform them into a numpy format. If you want the origin data, you can download it from https://pan.baidu.com/s/13INBDskrO5vWnhMAr0xJdQ with code  jods
Data format
TL;DR
use img,label to get data
0-9 is digit;
10 is +
11 is -
12 is ×
13 is ÷
14 is (
15 is )
Each file data has two attributes, img and label. Where img represents the image and label represents the label. label is an integer of 0-15 (although its type is float, you may need to convert the type). The meaning of label is as follows:
| label | meaning |
| --- | --- |
| 0-9 | digit |
|10  | + |
| 11 | - |
| 12 | × |
| 13 | ÷ |
| 14 | ( |
| 15 | ) |
example of reading data in python
python
import numpy as np
dir = ""path/to/your/data""
with np.load(dir) as data:
    img = data['img']
    label = data['label']"	0	21	0	ruiqurm	extend-mnist
8003	8003	VLSI Matrices		['education']		1	38	0	wolfram77	matrices-vlsi
8004	8004	Negre Matrices		[]		0	10	0	wolfram77	matrices-negre
8005	8005	Newman Graphs		[]		6	20	0	wolfram77	graphs-newman
8006	8006	Precima Matrices		[]		0	2	0	wolfram77	matrices-precima
8007	8007	Schulthess Graphs		[]		0	2	0	wolfram77	graphs-schulthess
8008	8008	Loan Data Final	Final data for Loan Eligibility application	['finance', 'banking']	"Context
Final dataset for loan eligibility"	7	54	1	mukeshmanral	loan-data-final
8009	8009	Sorensen Graphs		[]		0	9	0	wolfram77	graphs-sorensen
8010	8010	YOLOX-install		[]		1	66	2	tensorchoko	yoloxinstall
8011	8011	jigsaw2021-wat054-data		[]		0	7	0	wataoka	jigsaw2021-wat054-data
8012	8012	Freescale Matrices		[]		0	1	0	wolfram77	matrices-freescale
8013	8013	Melanoma 2020 Test Data	Melanoma Test  Images from 2020	['cancer']		7	77	0	saschamet	melanoma-test
8014	8014	albert-xxlarge-v2-cached-lm-510-train		[]		0	24	0	a24998667	albert-xxlarge-v2-cached-lm-510-train
8015	8015	petfinder_224model		[]		0	78	0	ktakita	petfinder-224model
8016	8016	trained_model		[]		1	28	0	cngvng	trained-model
8017	8017	Light data synboost		[]		3	13	0	shashwatnaidu07	light-data-synboost
8018	8018	Musical Instruments JSON File		['music']		0	23	1	mvanshika	musical-instruments-json-file
8019	8019	Multi Prophet		['religion and belief systems']	"Content
Multi Prophet is a procedure for forecasting time series data for multipe dependent variables based on Facebook Prophet package.
Source
https://github.com/vonum/multi-prophet
Aim
To use Multi Prophet when internet access is not permitted according to the competition rule.
How to use
import sys
sys.path.append('../input/multi-prophet/multi-prophet-master')
from multi_prophet import MultiProphet"	1	25	2	stpeteishii	multi-prophet
8020	8020	pf-142-train-2-data		[]		0	6	0	makotoikeda	pf-142-train-2-data
8021	8021	PetFinder Cait m36 384 11 Folds		[]		0	33	6	mathurinache	petfinder-cait-m36-384-11-folds
8022	8022	Water Pump RUL - Predictive Maintenance	Sensor data from a pump with the remaining useful life in hours given.	['manufacturing', 'time series analysis', 'lstm', 'keras', 'pytorch']	"Context
Here is the link to the original dataset Dataset. The difference in this data is that it contains the remaining useful life up to the next failure.
Content
The data are from all available sensors, all of them are raw values. The total sensor a are 52.
Acknowledgements
Thanks to UnknownClass for creating the original dataset.
Inspiration
This data can be used for a wide variety of tasks but I feel this data will be very useful in tasks such as Predictive Maintainance and Time-series forecasting."	9	118	1	anseldsouza	water-pump-rul-predictive-maintenance
8023	8023	build_knn_model_function_py	buil knn model function (quick )	['beginner', 'classification', 'ensembling', 'tabular data', 'binary classification']	"usage 💯 
ID = ""PassengerId""
TARGET = ""Survived""
SEED = 2021
N_SPLITS = 5
GRID_SEARCH_CV_NUM = 5
SCORING = ""roc_auc""
knnModel = buildKNNModel(train,ID,TARGET,N_SPLITS,SEED,GRID_SEARCH_CV_NUM,SCORING)"	11	296	3	rhythmcam	build-knn-model-function-py
8024	8024	starry_night		[]		5	42	0	just4jcgeorge	starry-night
8025	8025	SampleSuperstore	Dataset of a Sample Superstore	['business', 'intermediate', 'exploratory data analysis', 'tabular data', 'python']	As a business manager, try to find out the weak areas where you can work to make more profit.	120	585	14	ishaanthareja007	samplesuperstore
8026	8026	datasetfort5v5		[]		0	17	0	yygqwjl	datasetfort5v5
8027	8027	Sanbercode Final Project		[]	"Pada final project ini teman-teman diminta untuk memprediksi gaji seseorang apakah kurang dari sama dengan 7jt atau lebih dari 7jt berdasarkan beberapa keterangan pada kolom-kolom dataset. Buatlah model klasifikasi yang dapat menentukan nilai pada kolom 'Gaji' dengan mengubah nilainya berdasarkan ketentuan sebagai berikut:
0 : Gaji &lt;= 7jt
1 : Gaji &gt; 7jt
Silahkan download file train.csv sebagai data training, dan gunakan file test.csv sebagai data test yang akan di jadikan penilaian di project ini.
Terdapat tiga file yang digunakan pada final project ini, file train.csv berisi dataset yang memiliki kolom label digunakan untuk traning model, sedangkan file test.csv berisi dataset tanpa kolom label, yang nantinya teman-teman dapat menambahkan kolom 'Gaji' berisi prediksi teman-teman sesuai ketentuan pada tab Overview. Kemudian teman-teman diminta untuk membuat dataset baru berisi kolom 'id' dan kolom 'Gaji' yang diambil dari dataset test yang telah teman-teman tambahkan prediksinya, dataset baru ini yang nantinya akan dijadikan file submission. Contoh isi dari submission dapat teman-teman lihat pada file sample_submission.csv
File descriptions
train.csv - file training set
test.csv - file test set
sample_submission.csv - contoh file submission dalam format yang benar
 Data fields
id - id unique yang dimiliki setiap sample
Umur - umur yang dimiliki setiap sample
Kelas Pekerja - kelompok kelas pekerjaan masing-masing sample
Berat Akhir - berisi nilai akumulasi berdasarkan populasi, ras, dan gender dengan umur 16+ suatu wilayah, sample yang diambil dari wilayah yang memiliki karakteristik demografis yang sama akan memiliki nilai berat akhir yang sama
Pendidikan - tingkat pendidikan terakhir masing-masing sample
Jmlh Tahun Pendidikan - berisi jumlah tahun masing-masing sample mengenyam pendidikan
Status Perkawinan - status perkawinan masing-masing sample
Pekerjaan - pekerjaan saat ini masing-masing sample
Jenis Kelamin - jenis kelamin masing-masing sample
Keuntungan Kapital - keuntungan yang didapat jika sample menjual semua aset miliknya
Kerugian Kapital - kerugian yang didapat jika sample menjual semua aset miliknya
Jam per Minggu - jam kerja masing-masing sample setiap minggunya
Gaji - nilai gaji masing-masing sample apakah kurang dari sama dengan 7jt atau lebih dari 7jt"	1	46	0	muhtaufiqfirmansyah	sanbercode-final-project
8028	8028	ICC T20 World Cup 2021 Tweets 🏏	Explore the tweets made by fans with the #T20Worldcup hashtag	['cricket', 'sports', 'exploratory data analysis', 'nlp', 'data visualization', 'text data']	"Content
The 2021 ICC Men's T20 World Cup is the seventh ICC Men's T20 World Cup tournament, with the matches taking place in the United Arab Emirates and Oman from 17 October to 14 November 2021. The West Indies are the defending champions.
There was due to be a preceding 2020 T20 World Cup held in Australia from 18 October to 15 November 2020, but in July 2020, the International Cricket Council (ICC) confirmed that this tournament had been postponed, due to the COVID-19 pandemic. In August 2020, the ICC confirmed that India would host the 2021 tournament as planned, with Australia being named as the host for the succeeding 2022 tournament. However, in June 2021, the ICC announced that the tournament had been moved to the United Arab Emirates due to growing concerns over the COVID-19 pandemic situation in India, and a possible third wave of the pandemic in the country. The tournament began on 17 October 2021, with the tournament's final scheduled to be played on 14 November 2021. The preliminary rounds of the tournament were played in the UAE and Oman.
Acknowledgements
Content
The tweets have the #T20WorldCup hashtag. The collection started on 20/10/2021 and will be updated on a daily basis.
Information regarding the data
The data totally consists of 11,000+ records with 13 columns. The description of the features is given below
| No |Columns | Descriptions |
| -- | -- | -- |
| 1 | user_name | The name of the user, as they’ve defined it.  |
| 2 | user_location | The user-defined location for this account’s profile. |
| 3 | user_description | The user-defined UTF-8 string describing their account. |
| 4 | user_created | Time and date, when the account was created. |
| 5 | user_followers | The number of followers an account currently has. |
| 6 | user_friends | The number of friends an account currently has. |
| 7 | user_favourites | The number of favorites an account currently has |
| 8 | user_verified | When true, indicates that the user has a verified account |
| 9 | date | UTC time and date when the Tweet was created |
| 10 | text | The actual UTF-8 text of the Tweet |
| 11 | hashtags | All the other hashtags posted in the tweet along with #T20WorldCup |
| 12 | source | Utility used to post the Tweet, Tweets from the Twitter website have a source value - web  |
| 13 | is_retweet | Indicates whether this Tweet has been Retweeted by the authenticating user. |
Starter Notebook:
ICC T20 Worldcup 2021 Tweets EDA 
Inspiration
You can use this data to dive into the subjects that use this hashtag, look to the geographical distribution, evaluate sentiments, looks to trends."	138	1487	10	kaushiksuresh147	icc-t20-world-cup-2021-tweets
8029	8029	exp033-swin-l-224-fastai		[]		0	2	0	kurokurob	exp033-swin-l-224-fastai
8030	8030	Smart Korean beef auction price 스마트 한우 경매	Korean beef auction data from 한우경매.kr	['categorical data', 'animals', 'tabular data', 'text data']	"Context
The datasets are from a website for the Korean beef Online Auctions, by 축산플랫폼 주식회사.
I made a structured dataset by collecting and adjusting '혈통우'(pedigree cow) and '큰소'(big cow) auction results related datasets.
k-beef_raw.csv : whole data 
beef_train.csv : datasets for train
beef_test.csv : datasets for test without '상태', '낙찰가' colums.  
beef_answer.csv : answers(price) for test 
Content
일자 : the time in days that the bid was placed, from the start of the auction
번호 : unique identifier of an auction
출하주 : seller's name
개체번호 : unique identifier of an each cow
성별: gender of an each cow - 암(Female) , 수(male)
kpn : Korean Proven Bull No
계대 : generation of each kpn cow
중량 : weight
최저가 : reservation price
낙찰가 : winning bid
상태 : status
비고 : characteristics of each cow (ex. scars, pregnant, kpn...) 
종류 : type of cow - '혈통우'(pedigree cow) and '큰소'(big cow)
지역 : region
Acknowledgements
The original dataset can be found here.
Inspiration
For each cow, what is the relationship between '종류', kpn, and the '최저가'? 
Why don't you tokenize the '비고'. 
Read csv as pandas with encoding='cp949'.
refer to 한우경매 예측"	4	65	0	jskim1738	smart-korean-beef-auction
8031	8031	tfrec_pet_224		[]		0	18	0	bachaboos	tfrec-pet-224
8032	8032	iris-numeric-dataset.csv	All categorical values , outliers - empty and nan values are removed . 	['plants', 'internet']	The iris dataset , all outliers , empty values and nan values are removed and edited . And converted into numeric form , can be easily deployed in Ml model as dataset , without doing any data maupluation , except scaling . You can dowload the orignal iris dataset , the categorical one for reference .	8	57	0	niranjandasmm	irisnumericdatasetcsv
8033	8033	CVE and CWE mapping Dataset(2021)	Dataset consist of CVE's from 2002-21 and all three views of CWE	['software', 'data analytics', 'classification', 'text data', 'public safety']	"Please give an Upvote if you feel that the data is Useful and Amazing. Your Upvotes will Motivate me to contribute more to this platform😊
Context
&gt; A software vulnerability is a defect in software that could allow an attacker to gain control of a system. These defects can be because of the way the software is designed, or because of a flaw in the way that it’s coded.
An attacker can use a software flaw to steal or alter sensitive data, join a botnet, install a backdoor, or plant other types of malware by exploiting a software flaw. In addition, once an attacker has gained access to one network host, they can utilize that host to gain access to other hosts on the same network.
The National Vulnerability Archive (NVD) is the world's largest and most comprehensive database of publicly disclosed vulnerabilities in commercial and open source software. When an attacker discovers that your software is vulnerable to a known flaw, he or she will have a better understanding of what types of attacks to execute against it. If the attack is successful, the attacker will be able to execute malicious commands on the target system. Although the National Vulnerability Database and the Common Vulnerabilities and Exposures (CVE) list are commonly used interchangeably, there are notable discrepancies between the two databases despite their close association. The CVE dictionary was created by the non-profit MITRE Corporation in 1999, five years before the NVD.
CVE stands for Common Vulnerabilities and Exposures, and it's a standard reporting format for publicly known security flaws. CVE's primary goal is to standardize how a security vulnerability or risk is identified - with a number, a description, and at least one public reference. CVE is open to the public and is free to use. CVE-2020-16891 is an example of a CVE ID, which contains the CVE prefix, the year the CVE ID was assigned or the year the vulnerability was made public, and the sequence number digits. The CVE description comprises information such as the affected product's and vendor's names, a list of impacted versions, the vulnerability type, the impact, the access required for an attacker to exploit the issue, and the critical code components. The CVE reference includes the vulnerability reports, advisories, or sources that detail the vulnerability and the exploitation that could occur.
The distinction between CVE and CWE is that one addresses symptoms while the other addresses the root of the problem. The CVE is just a list of currently known flaws with specific systems and products, whereas the CWE categorizes categories of software vulnerabilities. The CWE is well-suited to identifying the most dangerous security flaws. The categories can assist you figure out what's causing your systems to fail and how to solve it. When it comes to which conditions make you the most vulnerable, it's usually the most common.
The Software Development representation groups flaws around ideas used or encountered frequently in software development, whereas the Hardware Design representation groups flaws around concepts used or encountered frequently in hardware design. Using several layers of abstraction, the Research Concept representation promotes research into different sorts of weaknesses and arranges items by actions. Each hierarchical representation is utilized to navigate the complete list based on your particular point of view.
Content
The dataset was collected from MITRE and NVD  organizations webpage. 
Global_Dataset.xlsx  : This dataset comprises all of the vulnerabilities reported in the NVD database between 2002 and 2021. The CVE ID, vulnerability description, CVSS scores, severity of the vulnerability, and corresponding CWE IDs are all included in the dataset.
synonym_mapping.json : It contains synonyms for some of the cyber security terms that were used in vulnerability descriptions reported by NVD, which can be found in the CWE Glossary. The words in the JSON file were the stemmed word from its original form. (It can be used as a additional preprocessing step to reduce the words domain)
Each CWE List View has corresponding CWE data, such as ID, Name, Description, Extended Description, and so on. It also has a hierarchical structure, with cwe_paths containing all of the different pathways from the root to the hierarchy's nodes. Vulnerability Dataset is a dataset of all vulnerabilities that corresponds to a CWE in the view.
What can be done with this?
The CVE-to-CWE classification is an active research area various research papers are published. The CVE-to-CWE mapping is an multi label node classification and Non-mandatory leaf node prediction problem were the CWE's in each view were aligned in a hierarchical directed acyclic graph. The Global_Dataset can be further used for various applications such as Data Analyzis, Data Visualisation, EDA, NLP projects, Clustering , etc.
Some Research Works on CVE Classification
ThreatZoom: Hierarchical Neural Network for CVEs to CWEs Classification
V2W-BERT: A Framework for Effective Hierarchical Multiclass
Classification of Software Vulnerabilities
A Study on the Classification of Common Vulnerabilities and Exposures using Naïve Bayes"	55	516	18	krooz0	cve-and-cwe-mapping-dataset
8034	8034	Retail_questions		[]		1	14	0	aravinthbosem	retail-questions
8035	8035	Retail_questions		[]		1	14	0	aravinthbosem	retail-questions
8036	8036	Cloudtnine	PHEONIXENT PROUDLY PRESENTS CLOUDTNINE AND HIS DEBUT ALBUM THE NINTH CHAPTER	['websites', 'celebrities', 'music', 'business', 'social networks']		2	27	1	evanmichaelwilson	cloudtnine
8037	8037	biodesign_4		[]		1	4	0	zmjjiang	biodesign-4
8038	8038	biodesign_4_64		[]		4	2	0	zmjjiang	biodesign-4-64
8039	8039	petfinder_2022		[]		0	38	0	bachaboos	petfinder-2022
8040	8040	YOLOv5-DS		[]		0	20	0	cowfrica	yolov5ds
8041	8041	OWM_model		[]		0	14	0	sufeng314	owm-model
8042	8042	phonemized-text-corpus-for-mlm		[]		5	56	0	a24998667	phonemized-text-corpus-for-mlm
8043	8043	SAM_MuBenchDataset	The dataset used the SAM paper	[]		0	28	0	tamnguyencs	sam-mubenchdataset
8044	8044	P_Pr_Xcep+Res50+Res152		[]		0	3	0	durgampranay	p-pr-xcepres50res152
8045	8045	Basketball Betting Dataset		['basketball', 'gambling']		29	539	0	visualize25	basketball-betting-dataset
8046	8046	BADUTA-BATITA_KEMRANJEN		[]	"Penelitian ini menggunakan data sekunder dari data Perencanaan Program 
Gizi (PPG) yang merupakan data dasar di Kecamatan Kemranjen, Kabupaten
Banyumas, Jawa tengah tahun 2019 dengan jumlah data sebesar 550 baduta"	3	29	0	datapublic	badutabatita-kemranjen
8047	8047	IBM_Debater_(R)_CE-ACL-2014.v0	Argument Detection datasets from IBM Project Debater	['health', 'nlp']		1	38	2	kaggleqrdl	ibm-debater-acl-2014
8048	8048	KDEF2021		[]		3	17	0	ahtcmstp	kdef2021
8049	8049	vitl_p16_224_in21k_ext_0_1		[]		0	2	0	xyzdivergence	vitl-p16-224-in21k-ext-0-1
8050	8050	survey lung cancer		['cancer']		14	104	0	ajisofyan	survey-lung-cancer
8051	8051	pet2-tfrecords2-384-012		[]		0	7	0	bamps53	pet2-tfrecords2-384-012
8052	8052	iris_Dataset		[]		1	21	0	ajisofyan	iris-dataset
8053	8053	IBM_Debater_(R)_claim_sentences_search	Argument Detection datasets from IBM Project Debater	['nlp']		4	81	3	kaggleqrdl	ibm-debater-claim-sentences-search
8054	8054	IBM_Debater_(R)_ArgsInASR_Findings-2020.v1	Argument Detection datasets from IBM Project Debater	['nlp']		2	20	3	kaggleqrdl	ibmdebater-argsinasrfindings-2020
8055	8055	pet2-tfrecords2-384-011		[]		0	10	0	bamps53	pet2-tfrecords2-384-011
8056	8056	IBMDebaterEvidenceSentences	Argument Detection datasets from IBM Project Debater	['nlp']		3	27	2	kaggleqrdl	ibmdebater-evidencesentences
8057	8057	wikipedia_evidence_dataset_29429	Argument Detection datasets from IBM Project Debater	[]		2	32	1	kaggleqrdl	wikipedia-evidence-dataset-29429
8058	8058	Rainfall in Saudi Arabia	Historical Data of 30 Years in Major Cities.	['weather and climate', 'tabular data', 'middle east']	"Context
The dataset contains the total rainfall (in mm) observed by PME MET Station General Authority for Statistics from 2009-2019.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Time Period: 2009 - 2019
Granularity: Monthly
Regions: 30 in Saudi Arabia
Rainfall Unit: mm
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	14	108	1	m0hannad	rainfall-in-saudi-arabia
8059	8059	timm-master		[]		1	12	0	a24998667	timm-master
8060	8060	Data_Gender		[]		0	13	0	ajisofyan	data-gender
8061	8061	pet2-tfrecords2-384-010		[]		0	2	0	bamps53	pet2-tfrecords2-384-010
8062	8062	seasonaltrendresidual		[]		0	27	0	ibrahimabah	seasonaltrendresidual
8063	8063	pet2-tfrecords2-384-009		[]		0	1	0	bamps53	pet2-tfrecords2-384-009
8064	8064	landmark-448-train-3		[]		0	21	0	kittenraidrua	landmark-448-train-3
8065	8065	NFL 2022 Processed Data		[]		1	46	0	mauromauro	nfl-2022-processed-data
8066	8066	 petfinder_with_is_cat		[]		2	36	0	shawndong98	petfinder-with-is-cat
8067	8067	MOVIEDATA		[]		0	26	0	anugak	moviedata
8068	8068	MovieData		[]		0	23	0	subbupala	moviedata
8069	8069	jigsaw2021-wat049-data		[]		0	6	0	wataoka	jigsaw2021-wat049-data
8070	8070	nfs-data-npy		[]		0	14	0	revanthraj	nfsdatanpy
8071	8071	Physionet database for Arrhythmia 		['heart conditions']		1	57	3	ladontist7777	physionet-database-for-arrhythmia
8072	8072	Goldx.csv		['internet']		2	13	0	sesiliaina	goldxcsv
8073	8073	Feedback Prize LB 2021-12-28	Feedback Prize Public LB 2021-12-28	['exploratory data analysis', 'data visualization', 'data analytics', 'matplotlib', 'python']		1	11	1	krist0phersmith	feedback-prize-lb-20211228
8074	8074	COVID19 Tweets (Jan24 to May25, 2020)	Tweets sent from US users containing COVID-19 key words	['united states', 'nlp', 'data analytics', 'text data', 'covid19']		2	54	0	lingshuhu	covid19-tweets-jan24-to-may25-2020
8075	8075	UFC Results - Fights up to Dec 2021	All results from UFC 1 to the last fight night of Dec 2021	['sports', 'tabular data', 'gambling', 'martial arts']	"Context
UFC dataset scraped from ufcstats.com
Content
Fights up to Dec 2021. Draws have been excluded.
Acknowledgements
Data owned by the UFC. To be used for entertaining purposes."	8	56	0	andres6garzon	ufc-results-fights-up-to-dec-2021
8076	8076	LOGG3-DAta		[]		0	9	0	pombali5	logg3data
8077	8077	longformer-large-4096		[]		216	413	6	hengzheng	longformerlarge4096
8078	8078	finaldataset		[]		0	17	0	skloveyyp	finaldataset
8079	8079	petf_swin_large_508_model_mixup10_metricfixed		[]		0	6	0	nhac43	petf-swin-large-508-model-mixup10-metricfixed
8080	8080	Travel in Guatemala with Hostelworld Data	A backpacker's guide: costs, ratings, and more.	['hotels and accommodations']	"Context
Guatemala is a beautiful country to visit and can accommodate a variety of travelers. From traveling on a budget to none at all. This datasets give an overview of the hostels available for booking through hostelworld.com. Travelers should note that this dataset is for budget backpackers! 
Content
This data was acquired from a popular hostel booking site, hostelworld.com. Data is organized by hostel name including the hostel popular backpacking location, starting cost per night, breakfast and wifi included, the average rating out of 10 (10 being the best), and the number of ratings. Data acquired for only hostelworld.com listings categorized as ""hostel."" 
Acknowledgements
Big thanks to hostelworld.com for giving travelers a different way to book their travel! Whether it for the social environment, cost effective, or experience itself. Hostelworld.com provides solid booking advice for hostel goers. 
Inspiration
I was inspired to pull some Guatemalan hostel data because it is a place close to my heart! Plus backpacking had such a profound impact on my own life. I thought it would be cool to play out scenarios of a backpacker's travel itinerary options. Have you backpacked before?"	40	309	5	lexipetzold	hostels-in-guatemala-hostelworld-data
8081	8081	dataset_cell		[]		0	70	0	yuyuki11235	dataset-cell
8082	8082	masked_rcnn_weights		[]		0	27	0	doroteo	masked-rcnn-weights
8083	8083	Sartorius: COCO Dataset		[]		1	19	0	awsaf49	sartorius-coco-dataset
8084	8084	Dataset of worldwide GDP during Covid Pandemic2020	Dataset of GDP during Corona(2020) to predict state of GDP in such pandemic	['business', 'social science', 'economics', 'covid19']	"This file contains  worldwide GDP information during corona pandemic . The dataset shows the information of GDP of 2020 .
~First column is the name of the countries .
~Second column contains the Nominal GDP per capita
~Third column of the dataset contains the PPP GDP per capita
~Fourth column contains GDP growth rate per capita (in percentage) . Negative indicates 
 that GDP has fallen .
~Last column contains the decision if GDP has fallen or arise.
What can be done by this Dataset?
~Prediction condition GDP during any pandemic like Covid-19 based on this dataset.
Currency used - US Dollar($)
Banner Credit - Mainly taken from y-axis.com"	268	1780	17	mdjafrilalamshihab	dataset-of-worldwide-gdp-during-covid-pandemic2020
8085	8085	Dataset Herbal Leaves		['biology']		51	37	0	rizkikecek	dataset-herbal-leaves
8086	8086	ccpvc0712v21v3		[]		0	29	0	javadkhorramdel	ccpvc0712v21v3
8087	8087	vehicle detection	car, bus, truck, ambulance dataset	['automobiles and vehicles']	"This dataset was used in project for detecting vehicles in an image and also in an video. This dataset contain 556 images for each class of vehicle. For each image the annotations are saved as TXT files in YOLO format. The annotations where created using LabelImg application.
The classes in this dataset is:
- Ambulance
- Bus
- Car
- Truck
To check the work on the project using this Dataset go to link: https://github.com/rohan300557/Vehicle-Detection-Yolo"	25	367	6	rohan300557	vehicle-detection
8088	8088	VAERSData_2012_2021		[]		0	48	0	jeffatennis	vaersdata-2017-2020
8089	8089	PetFinder-transformers		[]		5	53	0	ludovick	petfindertransformers
8090	8090	new_datasets		[]		1	23	0	doroteo	new-datasets
8091	8091	road anomaly synthesis		['earth and nature']		2	9	0	shashil	road-anomaly-synthesis
8092	8092	vinbigdata		[]		0	17	0	pisnai	vinbigdata
8093	8093	MBTI Personality Types 500 Dataset	~100K preprocessed records of posts and personality types	['social science', 'psychology', 'classification', 'text data', 'social networks']	"Context
MBTI (Myers-Briggs Type Indicator) is an introspective self-report questionnaire indicating differing psychological preferences (cognitive functions) in how people perceive the world and make decisions
Content
~106K records of preprocessed posts and their authors' personality types.
Posts are equal-sized: 500 words per sample.
Note: If you want to use the original dataset (without preprocessing), you can find it below as 2 separate datasets
Acknowledgements
Big Thanks to :
- Dylan Storey, who provided a DATASET of ~1.7M records of posts collected from Reddit using Google big query.
- Mitchell Jolly (datasnaek), who provided a DATASET of ~9K records of posts collected from PersonalityCafe forum, where each record has the last 50 posts written by the corresponding user.
Inspiration
Use your knowledge, use your iNtuition"	462	5727	27	zeyadkhalid	mbti-personality-types-500-dataset
8094	8094	cots_pretrain		[]		9	8	1	kenakai16	cots-pretrain
8095	8095	cpvcpm6		[]		0	2	0	mehdialiyari	cpvcpm6
8096	8096	HeadHunter Employer Review Competition	Данные для классификации запросов на русском. 	[]		8	80	2	aleron751	headhunter-employer-review-competition
8097	8097	cpvcpm7		[]		0	20	0	mehdialiyari	cpvcpm7
8098	8098	Sartorius Dataset		['earth and nature']		3	128	0	ihtishamahmad	sartorius-dataset
8099	8099	road anomaly icnet segmentation		[]		2	34	0	shashil	road-anomaly-icnet-segmentation
8100	8100	cpvcp7		[]		0	3	0	mehdialiyari	cpvcp7
8101	8101	cpvcp6		[]		0	19	0	mehdialiyari	cpvcp6
8102	8102	glove_data		[]		0	5	0	enkrish259	glove-data
8103	8103	CSWin-Transformer	CSWin transformer imagenet pretrained models	['earth and nature', 'computer vision', 'deep learning', 'image data', 'transformers']	"CSWin Transformer (the name CSWin stands for Cross-Shaped Window) is introduced in arxiv, which is a new general-purpose backbone for computer vision. It is a hierarchical Transformer and replaces the traditional full attention with our newly proposed cross-shaped window self-attention. The cross-shaped window self-attention mechanism computes self-attention in the horizontal and vertical stripes in parallel that from a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. With CSWin, we could realize global attention with a limited computation cost.
CSWin Transformer achieves strong performance on ImageNet classification (87.5 on val with only 97G flops) and ADE20K semantic segmentation (55.7 mIoU on val), surpassing previous models by a large margin.
Main Results on ImageNet
| model | pretrain | resolution | acc@1 | #params | FLOPs |
| --- | --- | --- | --- | --- | --- |
CSWin-T | ImageNet-1K | 224x224 | 82.8 | 23M | 4.3G |
CSWin-S | ImageNet-1k | 224x224 | 83.6 | 35M | 6.9G |
CSWin-B | ImageNet-1k | 224x224 | 84.2 | 78M | 15.0G |
CSWin-B | ImageNet-1k | 384x384 | 85.5 | 78M | 47.0G |
CSWin-L | ImageNet-22k | 224x224 | 86.5 | 173M | 31.5G |
CSWin-L | ImageNet-22k | 384x384 | 87.5 | 173M | 96.8G |"	28	150	4	byfone	cswintransformer
8104	8104	Delhi Road accident data 		[]		4	51	2	shomick1509	delhi-road-accident-data
8105	8105	Dataset for spam classification		[]		3	32	0	ayasyabatta	dataset-for-spam-classification
8106	8106	Predictions		['religion and belief systems']		0	28	0	ravinajadhav	predictions
8107	8107	Twitter_sentiment_analysis		[]		1	39	0	siddharthapal1999	twitter-sentiment-analysis
8108	8108	Shops Buy Sell Info		['retail and shopping']		1	42	0	sirajulislamtapas	shops-buy-sell-info
8109	8109	Covid19 in France	Covid data in France	['software', 'regression', 'covid19', 'matplotlib', 'pandas', 'sklearn']	"Context
The motivation of this study is to forecast future cases of coronavirus in France
Content
Details of datasets:
File format: CSV
Code
You can find a notebook in the code tab.
Sources
Extracted from : https://data.europa.eu/data/datasets/5f69ecb155c43420918410b8?locale=en"	437	2182	17	hraouak	covid19-in-france
8110	8110	Sartorius Stage2 Models		['clothing and accessories']		7	39	0	namgalielei	sartorius-stage2-models
8111	8111	Disease 		['health conditions']		1	33	0	ravinajadhav	disease
8112	8112	Real estate Paris	Real estate ads for the Parisian market	['real estate', 'exploratory data analysis', 'clustering', 'regression']	"Context
Web scrape of a website in order to collect data on the parisian real estate market.
Content
You will find 4 files in this dataset:
1. ads.csv : general info of the ad such as price, description and fee
2. equipements.csv: equipements of the property
3. neighborhood_facilities.csv:  restaurants, bars, hospitals ..
4. transports.csv : public transport nearby
Have fun !"	52	515	5	spicemix	real-estate-paris
8113	8113	whitespace		['computer science']		9	24	1	chasembowers	whitespace
8114	8114	SEED trasformato con TCA soggetto target completo		[]		0	31	0	daviderusso7	seed-trasformato-con-tca-soggetto-target-completo
8115	8115	matplotlib		['computer science', 'programming']		2	14	1	abrahamanderson	matplotlib
8116	8116	SVR_head		[]		0	12	0	suyinchen1024	svr-head
8117	8117	line_examples		[]		0	0	0	abrahamanderson	line-examples
8118	8118	Nigerian E-commerce Sales Dataset	Nigerian E-commerce Sales Dataset	['e-commerce services']		12	71	3	babajidedairo	nigerian-ecommerce-sales-dataset
8119	8119	How Urban Or Rural Is Your State?	FiveThirtyEight Urbanization Index	['cities and urban areas', 'social science', 'agriculture', 'beginner', 'urban planning']	"About this dataset
&gt; <p>This folder contains the data behind the story <a href=""https://fivethirtyeight.com/features/how-urban-or-rural-is-your-state-and-what-does-that-mean-for-the-2020-election/"" target=""_blank"" rel=""nofollow"">How Urban Or Rural Is Your State? And What Does That Mean For The 2020 Election?</a></p>
<p><a href=""https://data.world/fivethirtyeight/urbanization-index/workspace/file?filename=urbanization-state.csv"">urbanization-state.csv</a> contains FiveThirtyEight's urbanization index for every state. This number is calculated as the natural logarithm of the average number of people living within a five-mile radius of a given resident in each census tract in that state.</p>
<p><a href=""https://data.world/fivethirtyeight/urbanization-index/workspace/file?filename=urbanization-census-tract.csv"">urbanization-census-tract.csv</a> contains FiveThirtyEight's urbanization index for every census tract.</p>
<p>See the <a href=""https://data.world/fivethirtyeight/urbanization-index/workspace/file?filename=data_dictionary_urbanization.csv"">data dictionary</a> for column descriptions.</p>
<p>If you find this information useful, please <a href=""mailto:data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p>License: <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a><br>
Source: <a href=""https://github.com/fivethirtyeight/data/tree/master/urbanization-index"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/urbanization-index</a></p>
This dataset was created by data.world's Admin and contains around 70000 samples along with Lat Tract, Population, technical information and other features such as:
- Urbanindex
- Adj Radiuspop 5
- and more.
How to use this dataset
&gt; - Analyze Statefips in relation to State
- Study the influence of Long Tract on Lat Tract
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	51	737	10	yamqwe	urbanization-indexe
8120	8120	factos dataset		['earth and nature']		0	2	0	tirtharajsinha	factos-dataset
8121	8121	factorial dataset		[]		0	18	0	tirtharajsinha	factorial-dataset
8122	8122	pet2-tfrecords2-448		[]		0	1	0	bamps53	pet2-tfrecords2-448
8123	8123	landmark-448-val-2		[]		2	12	0	kittenraidrua	landmark-448-val-2
8124	8124	G88_AST_F2_Weights		[]		0	10	0	groupof88rc	g88-ast-f2-weights
8125	8125	P_Pr_k=3-ResNets152		[]		0	12	0	durgammohanpranay	p-pr-k3resnets152
8126	8126	Kickstarter_sample_2018	Crowdfunding projects	['finance']		1	24	1	jackydeng	kickstarter-sample-2018
8127	8127	The Complete History Of MLB	Every franchise’s relative strength after every game	['baseball', 'sports', 'history', 'beginner', 'statistical analysis']	"About this dataset
&gt; <p>This file contains links to the data behind <a href=""https://projects.fivethirtyeight.com/complete-history-of-mlb/"" target=""_blank"" rel=""nofollow"">The Complete History Of MLB</a> and our <a href=""https://projects.fivethirtyeight.com/2018-mlb-predictions/"" target=""_blank"" rel=""nofollow"">MLB Predictions</a>.</p>
<p><code>mlb_elo.csv</code> contains game-by-game Elo ratings and forecasts back to 1871.</p>
<p><code>mlb_elo_latest.csv</code> contains game-by-game Elo ratings and forecasts for only the latest season.</p>
<p>The data contains two separate systems for rating teams; the simpler Elo ratings, used for <a href=""https://projects.fivethirtyeight.com/complete-history-of-mlb/"" target=""_blank"" rel=""nofollow"">The Complete History Of MLB</a>, and the more involved — and confusingly named — ""ratings"" that are used in our <a href=""https://projects.fivethirtyeight.com/2018-mlb-predictions/"" target=""_blank"" rel=""nofollow"">MLB Predictions</a>. The main difference is that Elo ratings are reverted to the mean between seasons, while the more involved ratings use preseason team projections from several projection systems and account for starting pitchers. More information can be found in <a href=""https://fivethirtyeight.com/features/how-our-mlb-predictions-work/"" target=""_blank"" rel=""nofollow"">this article</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data/tree/master/mlb-elo"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/mlb-elo</a></p>
<p><em><strong>License:</strong></em> The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a>. If you find it useful, please <a href=""data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><em><strong>Updated:</strong></em> synced from source weekly</p>
This dataset was created by FiveThirtyEight and contains around 2000 samples along with Team1, Pitcher1, technical information and other features such as:
- Pitcher2 Rgs
- Score2
- and more.
How to use this dataset
&gt; - Analyze Neutral in relation to Season
- Study the influence of Pitcher2 on Team2
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	22	153	2	yamqwe	mlb-eloe
8128	8128	pet2-tfrecords2-512		[]		0	2	0	bamps53	pet2-tfrecords2-512
8129	8129	ViT_L16_224		[]		0	7	0	aftabhussaincui	vit-l16-224
8130	8130	authentication		[]		0	17	0	vickyap	authentication
8131	8131	sensorssci		[]		0	5	0	gg8910	sensorssci
8132	8132	baoat_durerized 		[]		0	15	0	scalpah	baoat-durerized
8133	8133	MVA Vehicle Sales Counts by Month 	The number of new and used vehicles and the sales dollars sold by month.  	[]		3	24	5	sardorabdirayimov	mva-vehicle-sales-counts-by-month
8134	8134	Superbowl Ads Over The Years:	According To Super Bowl Ads, Americans Love America, Animals And Sex.	['sports', 'economics', 'beginner', 'recommender systems']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>This dataset contains the data behind the story <a href=""http://projects.fivethirtyeight.com/super-bowl-ads"" target=""_blank"">According To Super Bowl Ads, Americans Love America, Animals And Sex</a>.</p>
<p>superbowl-ads.csv contains a list of ads from the 10 brands that had the most advertisements in Super Bowls from 2000 to 2020, according to data from <a href=""https://superbowl-ads.com/"" target=""_blank"">superbowl-ads.com</a>, with matching videos found on YouTube. FiveThirtyEight staffers then came up with seven defining characteristics of a Super Bowl ad, watched every video and evaluated each according to the taxonomy in the table below.</p>
<h2><strong>License</strong></h2>
<p>Data released under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 License</a></p>
<h2><strong>Source</strong></h2>
<p><a href=""https://github.com/fivethirtyeight/superbowl-ads"" target=""_blank"">GitHub</a></p>
This dataset was created by FiveThirtyEight and contains around 200 samples along with Use Sex, Celebrity, technical information and other features such as:
- Patriotic
- Danger
- and more.
How to use this dataset
&gt; - Analyze Youtube Url in relation to Year
- Study the influence of Brand on Animals
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	42	350	1	yamqwe	superbowl-adse
8135	8135	Face_image		[]		3	37	0	ravinajadhav	face-image
8136	8136	sartorius_models2		[]		14	206	0	anonamename	sartorius-models2
8137	8137	house_price		[]		1	32	0	nikitkashyap	house-price
8138	8138	E-commerce Dataset 		['business', 'e-commerce services']	"Introduction
Analyzing the purchases of our customers for 1 year in America E-commerce dataset. How are their customer's online buying habits?
Columns name and meanings:
Order_Date: The date the product was ordered.
Aging: The time from the day the product is ordered to the day it is delivered.
Customer_id: Unique id created for each customer.
Gender: Gender of customer.
Device_Type: The device the customer uses to actualize the transaction (Web/Mobile).
Customer_Login_Type: The type the customer logged in. Such as Member, Guest etc.
Product_Category: Product category
Product: Product
Sales: Total sales amount
Quantity: Unit amount of product
Discount: Percent discount rate
Profit: Profit
Shipping_cost: Shipping cost
Order_Priority: Order priority. Such as critical, high etc.
Payment_method: Payment method
Here is the some question that you can start with;
-What devices do my customers use to reach me?
-Who is the customer base?
-What product categories am I selling?
-Which product categories do I sell to whom? (Gender Distribution by Category or Product?)
-Which login type do my customers prefer when shopping?
-How does the date and time affect my sales? (Total sales by month, the days of week or time arrival)
-From which product do I earn the most profit per unit?
-How is my delivery speed and order priority?(Delivery Time distribution of order priority by months)"	48	275	3	mervemenekse	ecommerce-dataset
8139	8139	efficientdet weights tf od api		[]		0	3	0	lazcoder	efficientdet-weights-tf-od-api
8140	8140	binance btcusdt trade data 338321018-479279912		['business']		0	18	0	hejianhua198711	binance-btcusdt-trade-data-338321018479279912
8141	8141	Prediction 		['religion and belief systems']		1	9	1	ravinajadhav	prediction
8142	8142	landmark-448-val-1		[]		0	28	0	kittenraidrua	landmark-448-val-1
8143	8143	Segmented nature	This dataset contains a images and their segmentation map.	['earth and nature', 'gan', 'cnn', 'image data']	"Context
This dataset was created to train a GauGan2 model.
Find how in How GauGan 2 works.
Content
This dataset is compossed by two folders and a CSV file.
IMPORTANT:
This dataset already is build. Not recommended to use to train.
Inspiration
This dataset is inspired in the GauGan dataset."	21	388	6	lprdosmil	segmented-nature
8144	8144	Historical Stock Price of  (FAANG + 5) companies	"""I will tell you how to become rich"""	['business', 'exploratory data analysis', 'time series analysis', 'tabular data', 'regression', 'investing']	"Context
The subject matter of this dataset contains the stock prices of the 10 popular companies ( Apple, Amazon, Netflix, Microsoft, Google, Facebook, Tesla, Walmart, Uber and Zoom)
Content
Within the dataset one will encounter the following:
The date - ""Date""
The opening price of the stock - ""Open""
The high price of that day - ""High""
The low price of that day - ""Low""
The closed price of that day - ""Close""
The amount of stocks traded during that day - ""Volume""
The stock's closing price that has been amended to include any distributions/corporate actions that occurs before next days open - ""Adj[usted] Close""
Time period - 2015 to 2021 (day level)
Tasks
- Exploratory Data Analysis
- Tell a visualization story
- Compare stock price growth between companies
- Stock price prediction
- Time series analysis"	91	862	16	suddharshan	historical-stock-price-of-10-popular-companies
8145	8145	Twitter Non-Fungible Tokens Data	98,267 verified tweets data for the query 'NFTs' from Oct 15 to Dec 20, 2021	['social networks']		7	50	2	adimathur1	twitter-nonfungible-tokens-data
8146	8146	multTask		[]		0	20	0	suyinchen1024	multtask
8147	8147	pet2-tfrecords2-384-008		[]		0	12	0	bamps53	pet2-tfrecords2-384-008
8148	8148	adslxpt		[]		0	4	0	phanitata	adslxpt
8149	8149	company sales	sales of a company based on advertising from 3 forms: TV, Newspaper and Radio	['business', 'beginner', 'linear regression', 'tabular data', 'pandas']		198	1515	20	ishaanthareja007	company-sales
8150	8150	crop-dataset		[]		0	7	0	shivanimukunde	cropdataset
8151	8151	Birds, Altruism, Selfishness and Natural Selection	”Selfish Gene” by Richard Dawkins	['biology', 'animals']	"Context
""The Selfish Gene is a 1976 book on evolution by the ethologist Richard Dawkins, in which the author builds upon the principal theory of George C. Williams's Adaptation and Natural Selection (1966). Dawkins uses the term ""selfish gene"" as a way of expressing the gene-centred view of evolution (as opposed to the views focused on the organism and the group), popularising ideas developed during the 1960s by W. D. Hamilton and others. From the gene-centred view, it follows that the more two individuals are genetically related, the more sense (at the level of the genes) it makes for them to behave cooperatively with each other.""
http://www.math.utah.edu/~cherk/teach/3150/3150-12/selfish_gene.pdf
https://en.wikipedia.org/wiki/The_Selfish_Gene
Content
""In July 2017, a poll to celebrate the 30th anniversary of the Royal Society science book prize listed The Selfish Gene as the most influential science book of all time.""
https://en.wikipedia.org/wiki/The_Selfish_Gene
Acknowledgements
""Richard Dawkins a British evolutionary biologist and author. He is an emeritus fellow of New College, Oxford and was Professor for Public Understanding of Science in the University of Oxford from 1995 to 2008.""
http://www.math.utah.edu/~cherk/teach/3150/3150-12/selfish_gene.pdf
https://en.wikipedia.org/wiki/Richard_Dawkins 
Photo by Gary Bendig on Unsplash
Inspiration
Darwin and his contributions to the Science of Evolution. Natural Selection, which is result of the struggle for existence."	8	282	13	mpwolke	cusersmarildownloadsselfishpdf
8152	8152	Employee Salaries Datatset	Regression Problem to predict the Employee Salaries	['people', 'business', 'beginner', 'linear regression', 'regression']	"Description:
The data consists of Salaries of Employees, including their gender, age & phD degree. The dataset is downloaded from UCI Machine Learning Repository.
Properties of the Dataset: \
Number of Instances: 100\
Number of Attributes: 4 including the class attribute
The project is simple yet challenging as it has very limited features & samples. Can you build a regression model to capture all the patterns in the dataset, also maintaining the generalisability of the model?
Acknowledgements
The dataset is referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the employee salaries w.r.t multiple feature.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	253	1788	8	yasserh	employee-salaries-datatset
8153	8153	PAW-swin-large-patch4-window12-384-in22k		['animals']		1	31	0	a24998667	paw-swin-large-patch4-window12-384-in22k
8154	8154	Project_data		[]		1	13	0	mohamedgamaltw92	project-data
8155	8155	pet_ age_estimator		[]		1	20	0	nakayodo	pet-age-estimator
8156	8156	sefaaaagftgfgf		[]		0	14	0	snanakt	sefaaaagftgfgf
8157	8157	Bishkek pollution Kyrgyzhydromet data		['pollution']	"Context
The data of air pollution fetched from Kyrgyzhydromet web site.
Source
Data: http://gov.meteo.kg/?map=5
Code for fetching data: https://github.com/pavelis/meteo-kg-pollution"	18	978	0	pavelisayenko	bishkek-pollution-kyrgyzhydromet-data
8158	8158	Amazon_reviews		[]		0	11	0	siddharthapal1999	amazon-reviews
8159	8159	nonadherencedataset	A dataset containing details of patients for non adherence.	['health']		0	40	0	medsdata	nonadherencedataset
8160	8160	roberta-base-validfit		['arts and entertainment']		0	23	0	qianghanshao	robertabasevalidfit
8161	8161	resfaaaa		[]		0	32	0	snanakt	resfaaaa
8162	8162	Titanic_Prediction_Class	Titanic Preidciton class (80.6)	['beginner', 'classification', 'binary classification', 'python']	"usage : 
titanic = TitanicPrediction(2021,5,10,10)
titanic.buildModel()
titanic.makeSubmitCSV()"	38	900	23	rhythmcam	titanic-prediction-class
8163	8163	bert-base-pretrained-rudditfit		[]		0	18	0	qhshao	bertbasepretrainedrudditfit
8164	8164	ıjjlkjlkjklj		[]		0	17	0	snanakt	jjlkjlkjklj
8165	8165	bert-base_pretrained_validfit		[]		1	22	0	sqianghan	bertbase-pretrained-validfit
8166	8166	bert-pretrained-jigclsfit		[]		1	4	0	sqianghan	bertpretrainedjigclsfit
8167	8167	R101_test_as_train_pretrained__livecell_best_e17		[]		0	7	0	itaynivtau	r101-test-as-train-pretrained-livecell-best-e17
8168	8168	Fluorem Matrices		[]		0	21	0	wolfram77	matrices-fluorem
8169	8169	spambase2		[]		0	20	0	younsovski	spambase2
8170	8170	Martin Matrices	Chemical oceanography; a marine nitrogen cycle inverse model.	['earth science', 'chemistry', 'weather and climate', 'cycling', 'oceania']	"Martin/marine1: chemical oceanography; a marine nitrogen cycle inverse model
A matrix submitted by Taylor Martin, Stanford, discussed the following paper:
Title: Modeling oceanic nitrite concentrations and isotopes using a 3D
inverse N cycle model
Authors: Taylor S. Martin(1), Francois Primeau(2), and Karen L. Casciotti(1)
(1) Stanford University, Department of Earth System Science
(2) University of California, Irvine, Department of Earth System Science
Received: 05 Sep 2018
Abstract. Nitrite (NO2-) is a key intermediate in the marine nitrogen (N) cycle
and a substrate in nitrification, which produces nitrate (NO3-), as well as
water column N loss processes, denitrification and anammox. In models of the
marine N cycle, NO2- is often not considered as a separate state variable,
since NO3- occurs in much higher concentrations in the ocean. In oxygen
deficient zones (ODZs), however, NO2- represents a substantial fraction of the
bioavailable N, and modeling its production and consumption is important to
understanding the N cycle processes occurring there, especially those where
bioavailable N is lost from or retained within the water column. Here we
present the expansion of a global 3D inverse N cycle model to include NO2- as a
reactive intermediate as well as the processes that produce and consume NO2- in
marine ODZs. NO2- accumulation in ODZs is accurately represented by the model
involving NO3- reduction, NO2- reduction, NO2- oxidation, and anammox. We model
both 14N and 15N and use a compilation of oceanographic measurements of NO3-
and NO2- concentrations and isotopes to place a better constraint on the N
cycle processes occurring. The model is optimized using a range of isotope
effects for denitrification and NO2- oxidation, and we find that the larger
(more negative) inverse isotope effects for NO2- oxidation along with
relatively high rates of NO2- oxidation give a better simulation of NO3- and
NO2- concentrations and isotopes in marine ODZs.
How to cite: Martin, T. S., Primeau, F., and Casciotti, K. L.: Modeling
oceanic nitrite concentrations and isotopes using a 3D inverse N cycle model,
Biogeosciences Discuss., https://doi.org/10.5194/bg-2018-397, in review, 2018."	1	43	1	wolfram77	matrices-martin
8171	8171	LeGresley Matrices	Power flow analyses of electrical grids by Patrick LeGresley.	['energy', 'electronics', 'electricity']	"Power flow analyses of electrical grids.
Submitted by Patrick LeGresley, Stanford, July 2013.
Patrick LeGresley is a researcher in the NLP group within Applied Deep Learning Research at NVIDIA, where he focuses on system aspects of training large language models. He received his PhD in Aeronautics and Astronautics from Stanford University. In the past, he also worked at the Baidu Silicon Valley AI Lab, working on systems research and deep learning for speech recognition."	0	18	1	wolfram77	matrices-legresley
8172	8172	handudu		[]		1	13	0	handudu	handudu
8173	8173	IPSO Matrices		[]		0	11	0	wolfram77	matrices-ipso
8174	8174	Dziekonski Matrices		[]		0	9	0	wolfram77	matrices-dziekonski
8175	8175	Davis Matrices		[]		0	2	0	wolfram77	matrices-davis
8176	8176	Dattorro Matrices		[]		0	5	0	wolfram77	matrices-dattorro
8177	8177	pretrain		[]		0	34	0	randomtreesg	pretrain
8178	8178	CPM Matrices		[]		0	11	0	wolfram77	matrices-cpm
8179	8179	Chevron Matrices		[]		0	12	0	wolfram77	matrices-chevron
8180	8180	Brogan Matrices	Specular surface reconstruction problem by Joel Brogan, Notre Dame.	['computer vision', 'data visualization', 'survey analysis', 'eyes and vision']	"Specular surface reconstruction problem,
Submitted by Joel Brogan, Notre Dame, July 2014
Requires a sparse QR factorization, x=A\b
specular
Name                            specular
Group                           Brogan
Matrix ID                       2658
Num Rows                        477,976
Num Cols                        1,600
Nonzeros                        7,647,040
Pattern Entries                 7,647,616
Kind                            Computer Vision Problem
Symmetric                     No
Date                            2014
Author                        J. Brogan
Editor                        T. Davis
Structural Rank               1,442
Structural Rank Full            false
Num Dmperm Blocks             4
Strongly Connect Components     159
Num Explicit Zeros            576
Pattern Symmetry                0%
Numeric Symmetry                0%
Cholesky Candidate            no
Positive Definite             no
Type                            real"	0	19	0	wolfram77	matrices-brogan
8181	8181	Bodendiek Matrices	Curl-Curl operator of 2nd order Maxwell's equations by A. Bodendiek.	['earth and nature', 'mobile and wireless', 'electronics', 'electricity']	"Curl-Curl operator of 2nd order Maxwell's equations, A. Bodendiek
From Andre' Bodendiek, Institut Computational Mathematics,
TU Braunschweig
The following matrix collection consists of the curl-curl-operator
of a second-order Maxwell's equations with PEC boundary conditions,
i.e. E x n = 0, where E and n denote the electric field strength
and the unit outer normal of the computational domain. The
curl-curl-operator has been discretized using the Finite Element
Method with first-order Nedelec elements resulting in the weak
formulation
1/mu0 ( curl E, curl v ),
where v resembles a test function of H(curl) and
mu0 = 1.25 1e-9 H / mm denotes the magnetic permeability of vacuum,
see [Hipt02]. 
In general, the underlying model problem of Maxwell's equations
results from a Coplanar Waveguide, which will be considered for
the analysis of parasitic effects in the development of new
semiconductors. Since the corresponding dynamical systems are often
high-dimensional, model order reduction techniques have become an
appealing approach for the efficient simulation and accurate analysis
of the parasitic effects. However, different kinds of model order
techniques require the repeated solution of high-dimensional linear
systems of the original model problem, see [Bai02,An09]. Therefore,
the development of efficient solvers resembles an important task
in model order reduction.
Each matrix CurlCurl_"	0	4	0	wolfram77	matrices-bodendiek
8182	8182	Arenas Graphs	Networks from Alex Arenas, Univeristy Rovira i Virgili, Tarragona, Spain.	['music', 'earth and nature', 'biology', 'email and messaging']	"Networks from Alex Arenas, Univeristy Rovira i Virgili, Tarragona, Spain
http://deim.urv.cat/~aarenas/data/welcome.htm
PGPgiantcompo:
Graph of the largest component of the network of users of the
Pretty-Good-Privacy algorithm for secure information interchange.
Reference: M. Boguna, R. Pastor-Satorras, A.  Diaz-Guilera and
A. Arenas, Physical Review E, vol. 70, 056122 (2004).
jazz:
Network of Jazz musicians.
Reference: P.Gleiser and L. Danon , Adv. Complex Syst.6, 565 (2003).
celegans_metabolic:
Graph of the metabolic network of C.elegans.
Reference: Community identification using Extremal Optimization,
J. Duch and A. Arenas, Physical Review E , vol. 72, 027104, (2005).
email:
Network of e-mail interchanges between members of
the Univeristy Rovira i Virgili (Tarragona).
Reference: R. Guimera, L. Danon, A. Diaz-Guilera, F. Giralt and A. Arenas
Physical Review E , vol. 68, 065103(R), (2003).
All data compiled by members of Alex Arenas' group."	0	18	0	wolfram77	graphs-arenas
8183	8183	Lab 01		['earth and nature']		1	15	1	prernachauhan	lab-01
8184	8184	ANSYS Matrices	Underdetermined systems by Emmannuel Delor, ANSYS.	['science and technology', 'manufacturing', 'engineering', 'feature engineering']	"Underdetermined systems needing well-conditioned bases to be found.
The goal is to find a permutation or factorization that places A in upper
trapezoidal form, [R1 R2] where R1 is well-conditioned, square, and upper
triangular, and where R1\R2 is as sparse as possible.  Submitted to the
UF collection by Emmannuel Delor, ANSYS.
opts.tol = 0.01 ;
[m n] = size(A) ;
x = ones (n,1) ;
y = Ax ;
[c R P info] = spqr (A, y, opts) ;
info
Rs = R (:, 1:m) ;    % must be well conditioned
fprintf ('condest(Rs) %g\n', condest (Rs)) ;
xs = x (1:m) ;
xm = x (m+1:n) ;
A2 = -Rs \ R (:, m+1:n) ;
y2 =  Rs \ c ;
norm(A2xm + y2 - xs)   % should be very small
nnz (A2)                % should also be as small as possible"	1	11	0	wolfram77	matrices-ansys
8185	8185	AG-Monien Graphs	AG-Monien Graph Collection by Ralf Diekmann and Robert Preis.	['earth science', 'science and technology', 'automobiles and vehicles', 'advanced']	"AG-Monien Graph Collection, Ralf Diekmann and Robert Preis
http://www2.cs.uni-paderborn.de/fachbereich/AG/monien/RESEARCH/PART/graphs.html
A collection of test graphs from various sources.  Many of the graphs
include XY or XYZ coordinates.  This set also includes some graphs from
the Harwell-Boeing collection, the NASA matrices, and some random matrices
which are not included here in the AG-Monien/ group of the UF Collection.
In addition, two graphs already appear in other groups:
AG-Monien/big : same as Nasa/barth5, Pothen/barth5 (not included here)
   AG-Monien/cage_3_11 : same as Pajek/GD98_c (included here)
The AG-Monien/GRID subset is not included.  It contains square grids that
are already well-represented in the UF Collection.
These graphs appear in this set, as individual graphs, all with XY or XYZ
coordinates:
AG-Monien/3elt
AG-Monien/3elt_dual
AG-Monien/airfoil1
AG-Monien/airfoil1_dual
AG-Monien/big_dual
AG-Monien/crack
AG-Monien/crack_dual
AG-Monien/grid1
AG-Monien/grid1_dual
AG-Monien/grid2
AG-Monien/grid2_dual
AG-Monien/netz4504
AG-Monien/netz4504_dual
AG-Monien/ukerbe1
AG-Monien/ukerbe1_dual
AG-Monien/whitaker3
AG-Monien/whitaker3_dual
AG-Monien/brack2
AG-Monien/wave
AG-Monien/diag
AG-Monien/L
AG-Monien/L-9
AG-Monien/stufe
AG-Monien/stufe-10
AG-Monien/biplane-9
AG-Monien/shock-9
Note that L-9, stufe-10, biplane-9 and shock-9 were L.9, stufe.10,
etc, in the AG-Monien set.  The UF Collection does not permit ""."" in
the matrix name.
Six more problem sets are included as sequences, each sequence being
a single problem instance in the UF Collection:
AG-Monien/bfly:  10 butterfly graphs 3..12
   AG-Monien/cage:  45 cage graphs 3..12
   AG-Monien/cca:   10 cube-connected cycle graphs, no wrap
   AG-Monien/ccc:   10 cube-connected cycle graphs, with wrap
   AG-Monien/debr:  18 De Bruijn graphs
   AG-Monien/se:    13 shuffle-exchange graphs
The primary graph (Problem.A) in each sequence is the last graph
in the sequence.  In the Matrix Market and Rutherford-Boeing
formats, the filenames will differ from the names given below,
because in the UF Collection, the file name gives the place of
a graph in its sequence.  The correspondence with the original
graph names is given below.
Graphs in the bfly sequence:
 1 : BFLY3        :      24 nodes      48 edges      96 nonzeros
 2 : BFLY4        :      64 nodes     128 edges     256 nonzeros
 3 : BFLY5        :     160 nodes     320 edges     640 nonzeros
 4 : BFLY6        :     384 nodes     768 edges    1536 nonzeros
 5 : BFLY7        :     896 nodes    1792 edges    3584 nonzeros
 6 : BFLY8        :    2048 nodes    4096 edges    8192 nonzeros
 7 : BFLY9        :    4608 nodes    9216 edges   18432 nonzeros
 8 : BFLY10       :   10240 nodes   20480 edges   40960 nonzeros
 9 : BFLY11       :   22528 nodes   45056 edges   90112 nonzeros
10 : BFLY12       :   49152 nodes   98304 edges  196608 nonzeros
Graphs in the cage sequence:
 1 : cage_3_5     :      10 nodes      15 edges      30 nonzeros
 2 : cage_3_6     :      14 nodes      21 edges      42 nonzeros
 3 : cage_3_7     :      24 nodes      36 edges      72 nonzeros
 4 : cage_3_8     :      30 nodes      45 edges      90 nonzeros
 5 : cage_3_9.1   :      58 nodes      87 edges     174 nonzeros
 6 : cage_3_9.2   :      58 nodes      87 edges     174 nonzeros
 7 : cage_3_9.3   :      58 nodes      87 edges     174 nonzeros
 8 : cage_3_9.4   :      58 nodes      87 edges     174 nonzeros
 9 : cage_3_9.5   :      58 nodes      87 edges     174 nonzeros
10 : cage_3_9.6   :      58 nodes      87 edges     174 nonzeros
11 : cage_3_9.7   :      58 nodes      87 edges     174 nonzeros
12 : cage_3_9.8   :      58 nodes      87 edges     174 nonzeros
13 : cage_3_9.9   :      58 nodes      87 edges     174 nonzeros
14 : cage_3_9.10  :      58 nodes      87 edges     174 nonzeros
15 : cage_3_9.11  :      58 nodes      87 edges     174 nonzeros
16 : cage_3_9.12  :      58 nodes      87 edges     174 nonzeros
17 : cage_3_9.13  :      58 nodes      87 edges     174 nonzeros
18 : cage_3_9.14  :      58 nodes      87 edges     174 nonzeros
19 : cage_3_9.15  :      58 nodes      87 edges     174 nonzeros
20 : cage_3_9.16  :      58 nodes      87 edges     174 nonzeros
21 : cage_3_9.17  :      58 nodes      87 edges     174 nonzeros
22 : cage_3_9.18  :      58 nodes      87 edges     174 nonzeros
23 : cage_3_10.1  :      70 nodes     105 edges     210 nonzeros
24 : cage_3_10.2  :      70 nodes     105 edges     210 nonzeros
25 : cage_3_10.3  :      70 nodes     105 edges     210 nonzeros
26 : cage_3_11    :     112 nodes     168 edges     336 nonzeros
27 : cage_3_12    :     126 nodes     189 edges     378 nonzeros
28 : cage_3_13    :     272 nodes     408 edges     816 nonzeros
29 : cage_3_14    :     406 nodes     609 edges    1218 nonzeros
30 : cage_3_15    :     620 nodes     930 edges    1860 nonzeros
31 : cage_4_5     :      19 nodes      38 edges      76 nonzeros
32 : cage_4_6     :      26 nodes      52 edges     104 nonzeros
33 : cage_4_7     :      76 nodes     152 edges     304 nonzeros
34 : cage_4_8     :      80 nodes     160 edges     320 nonzeros
35 : cage_5_5     :      30 nodes      75 edges     150 nonzeros
36 : cage_5_6     :      42 nodes     105 edges     210 nonzeros
37 : cage_6_6     :      62 nodes     186 edges     372 nonzeros
38 : cage_7_5     :      50 nodes     175 edges     350 nonzeros
39 : cage_8_5     :      94 nodes     376 edges     752 nonzeros
40 : cage_8_6     :     114 nodes     456 edges     912 nonzeros
41 : cage_9_5     :     118 nodes     531 edges    1062 nonzeros
42 : cage_9_6     :     146 nodes     657 edges    1314 nonzeros
43 : cage_10_6    :     182 nodes     910 edges    1820 nonzeros
44 : cage_12_6    :     266 nodes    1596 edges    3192 nonzeros
45 : cage_14_6    :     366 nodes    2562 edges    5124 nonzeros
Graphs in the cca sequence:
 1 : CCA3         :      24 nodes      28 edges      56 nonzeros
 2 : CCA4         :      64 nodes      80 edges     160 nonzeros
 3 : CCA5         :     160 nodes     208 edges     416 nonzeros
 4 : CCA6         :     384 nodes     512 edges    1024 nonzeros
 5 : CCA7         :     896 nodes    1216 edges    2432 nonzeros
 6 : CCA8         :    2048 nodes    2816 edges    5632 nonzeros
 7 : CCA9         :    4608 nodes    6400 edges   12800 nonzeros
 8 : CCA10        :   10240 nodes   14336 edges   28672 nonzeros
 9 : CCA11        :   22528 nodes   31744 edges   63488 nonzeros
10 : CCA12        :   49152 nodes   69632 edges  139264 nonzeros
Graphs in the ccc sequence:
 1 : CCC3         :      24 nodes      36 edges      72 nonzeros
 2 : CCC4         :      64 nodes      96 edges     192 nonzeros
 3 : CCC5         :     160 nodes     240 edges     480 nonzeros
 4 : CCC6         :     384 nodes     576 edges    1152 nonzeros
 5 : CCC7         :     896 nodes    1344 edges    2688 nonzeros
 6 : CCC8         :    2048 nodes    3072 edges    6144 nonzeros
 7 : CCC9         :    4608 nodes    6912 edges   13824 nonzeros
 8 : CCC10        :   10240 nodes   15360 edges   30720 nonzeros
 9 : CCC11        :   22528 nodes   33792 edges   67584 nonzeros
10 : CCC12        :   49152 nodes   73728 edges  147456 nonzeros
Graphs in the debr sequence:
 1 : DEBR3        :       8 nodes      13 edges      26 nonzeros
 2 : DEBR4        :      16 nodes      29 edges      58 nonzeros
 3 : DEBR5        :      32 nodes      61 edges     122 nonzeros
 4 : DEBR6        :      64 nodes     125 edges     250 nonzeros
 5 : DEBR7        :     128 nodes     253 edges     506 nonzeros
 6 : DEBR8        :     256 nodes     509 edges    1018 nonzeros
 7 : DEBR9        :     512 nodes    1021 edges    2042 nonzeros
 8 : DEBR10       :    1024 nodes    2045 edges    4090 nonzeros
 9 : DEBR11       :    2048 nodes    4093 edges    8186 nonzeros
10 : DEBR12       :    4096 nodes    8189 edges   16378 nonzeros
11 : DEBR13       :    8192 nodes   16381 edges   32762 nonzeros
12 : DEBR14       :   16384 nodes   32765 edges   65530 nonzeros
13 : DEBR15       :   32768 nodes   65533 edges  131066 nonzeros
14 : DEBR16       :   65536 nodes  131069 edges  262138 nonzeros
15 : DEBR17       :  131072 nodes  262141 edges  524282 nonzeros
16 : DEBR18       :  262144 nodes  524285 edges 1048570 nonzeros
17 : DEBR19       :  524288 nodes 1048573 edges 2097146 nonzeros
18 : DEBR20       : 1048576 nodes 2097149 edges 4194298 nonzeros
Graphs in the se sequence:
 1 : SE3          :       8 nodes      10 edges      20 nonzeros
 2 : SE4          :      16 nodes      21 edges      42 nonzeros
 3 : SE5          :      32 nodes      46 edges      92 nonzeros
 4 : SE6          :      64 nodes      93 edges     186 nonzeros
 5 : SE7          :     128 nodes     190 edges     380 nonzeros
 6 : SE8          :     256 nodes     381 edges     762 nonzeros
 7 : SE9          :     512 nodes     766 edges    1532 nonzeros
 8 : SE10         :    1024 nodes    1533 edges    3066 nonzeros
 9 : SE11         :    2048 nodes    3070 edges    6140 nonzeros
10 : SE12         :    4096 nodes    6141 edges   12282 nonzeros
11 : SE13         :    8192 nodes   12286 edges   24572 nonzeros
12 : SE14         :   16384 nodes   24573 edges   49146 nonzeros
13 : SE15         :   32768 nodes   49150 edges   98300 nonzeros"	0	15	0	wolfram77	graphs-ag-monien
8186	8186	ADNI_Extracted_Axial	Alzheimer's Disease Multiclass imaging data	['research', 'neuroscience', 'classification', 'image data', 'multiclass classification']	"Context
ADNI provides Alzheimer's data in Nifti or DICOM format which is 3D volumetric data. It becomes slightly difficult to work directly on the 3D data, hence the given dataset was created for easy implementation of the image processing algorithms.
Content
This dataset consists of 2D axial images extracted from the ADNI baseline dataset which consisted of Nifti images. It consists of 3 classes, i.e. AD (Alzheimer's Disease), CI (Mild Cognitive Impaired) and CN (Common Normal) subjects
Acknowledgements
The images have been extracted from the ADNI Baseline dataset (NIFTI format) which consisted of 199 instances. The original images can be downloaded from https://ida.loni.usc.edu/login.jsp?project=ADNI.
Inspiration
Alzheimer's is one of the most prevailing neurodegenerative disorder these days. Data availability becomes a hurdle for researchers. Thus this dataset would be of great help for those researchers who are working in the field of AD diagnosis."	5	112	1	katalniraj	adni-extracted-axial
8187	8187	Harry Potter Reviews	Top reviews from Goodreads	['movies and tv shows', 'global', 'intermediate', 'nlp', 'data visualization', 'ratings and reviews']	"From Wiki
Harry Potter is a series of seven fantasy novels written by British author J. K. Rowling. The novels chronicle the lives of a young wizard, Harry Potter, and his friends Hermione Granger and Ron Weasley, all of whom are students at Hogwarts School of Witchcraft and Wizardry. The main story arc concerns Harry's struggle against Lord Voldemort, a dark wizard who intends to become immortal, overthrow the wizard governing body known as the Ministry of Magic and subjugate all wizards and Muggles (non-magical people).
Since the release of the first novel, Harry Potter and the Philosopher's Stone, on 26 June 1997, the books have found immense popularity, positive reviews, and commercial success worldwide. They have attracted a wide adult audience as well as younger readers and are often considered cornerstones of modern young adult literature.2 As of February 2018, the books have sold more than 500 million copies worldwide, making them the best-selling book series in history, and have been translated into eighty languages.3 The last four books consecutively set records as the fastest-selling books in history, with the final instalment selling roughly 2.7 million copies in the United Kingdom and 8.3 million copies in the United States within twenty-four hours of its release.
The series was originally published in English by two major publishers, Bloomsbury in the United Kingdom and Scholastic Press in the United States. All versions around the world are printed by Grafica Veneta in Italy.4
A play, Harry Potter and the Cursed Child, based on a story co-written by Rowling, premiered in London on 30 July 2016 at the Palace Theatre, and its script was published by Little, Brown. The original seven books were adapted into an eight-part namesake film series by Warner Bros. Pictures, which is the third-highest-grossing film series of all time as of February 2020. In 2016, the total value of the Harry Potter franchise was estimated at $25 billion,5 making Harry Potter one of the highest-grossing media franchises of all time.
Play with it as you like.
Source:Goodreads"	56	530	2	notkrishna	harry-potter-reviews
8188	8188	landmark-448-train-4		[]		0	14	0	kittenraidrua	landmark-448-train-4
8189	8189	SartoriusNew Mask RCNN V21		[]		3	89	0	namgalielei	sartoriusnew-mask-rcnn-v21
8190	8190	JIO_Telecom_Churn_Prediction	Prediction according Good & Active phase	['business']	"Business Problem Overview
Let us say that Reliance Jio Infocomm Limited approached us with a
problem. There is a general tendency in the telecom industry that
customers actively switch from one operator to another. As the
telecom is highly competitive, the telecommunications industry
experiences an average of 18-27% annual churn rate. Since, it costs
7-12 times more to acquire a new customer as compared to retaining
an existing one, customer retention is an important aspect when
compared with customer acquisition which is why our clients, Jio,
wants to retain their high profitable customers and thus, wish to
predict those customers which have a high risk of churning.
Also, since a postpaid customer usually informs the operator prior to
shifting their business to a competitor’s platform, our client is more
concerned regarding its prepaid customers that usually churn or shift
their business to a different operator without informing them which
results in loss of business because Jio couldn’t offer any promotional
scheme in time, to prevent churning.
As per Jio, there are two kinds of churning - revenue based and usage
based. Those customers who have not utilized any revenue-generating
facilities such as mobile data usage, outgoing calls, caller tunes, SMS
etc. over a given period of time. To determine such a customer, Jio
usually uses an aggregate metrics like ‘customers who have generated
less than ₹ 7 per month in total revenue’. However, the disadvantage
of using such a metric would be that many of Jio customers who use
their services only for incoming calls will also be counted/treated as
churn since they do not generate direct revenue. In such scenarios,
revenue is generated by their relatives who also uses Jio network to
call them. For example, many users in rural areas only receive calls
from their wage-earning siblings in urban areas.
The other type of Churn, as per our client, is usage based which
consists of customers who do not use any of their services i.e., no
calls (either incoming or outgoing), no internet usage, no SMS, etc.
The problem with this segment is that by the time one realizes that a
customer is not utilizing any of the services, it may be too late to take
any corrective measure since the said customer might already
switched to another operator. Currently, our client, Reliance Jio
Infocomm Limited, have approached us to help them in predicting
customers who will churn based on the usage-based definition
Another aspect that we have to bear in mind is that as per Jio, 80% of
their revenue is generated from 20% of their top customers. They call
this group High-valued customers. Thus, if we can help reduce churn
of the high-value customers, we will be able to reduce significant
revenue leakage and for this they want us to define high-value
customers based on a certain metric based on usage-based churn
and predict only on high-value customers for prepaid segment.
Understanding the Data-set
The data-set contains customer-level information for a span of four
consecutive months - June, July, August and September. The months
are encoded as 6, 7, 8 and 9, respectively. The business objective is to
predict the churn in the last (i.e. the ninth) month using the data
(features) from the first three months. To do this task well,
understanding the typical customer behavior during churn will be
helpful.
Understanding Customer Behavior During Churn
Customers usually do not decide to switch to another competitor
instantly, but rather over a period of time (this is especially applicable
to high-value customers). In churn prediction, we assume that there
are three phases of customer lifecycle:
1) The ‘good’ phase: In this phase, the customer is happy with the
service and behaves as usual.
2) The ‘action’ phase: The customer experience starts to sore in this
phase, for e.g. he/she gets a compelling offer from a competitor,
faces unjust charges, becomes unhappy with service quality etc. In
this phase, the customer usually shows different behavior than
the ‘good’ months. Also, it is crucial to identify high-churn-risk
customers in this phase, since some corrective actions can be
taken at this point (such as matching the competitor’s
offer/improving the service quality etc.)
3) The ‘churn’ phase: In this phase, the customer is said to have
churned. You define churn based on this phase. Also, it is
important to note that at the time of prediction (i.e. the action
months), this data is not available to you for prediction. Thus,
after tagging churn as 1/0 based on this phase, you discard all
data corresponding to this phase.
In this case, since you are working over a four-month window, the
first two months are the ‘good’ phase, the third month is the ‘action’
phase, while the fourth month is the ‘churn’ phase.
Data Dictionary
 The data-set is available in a csv file named as “Company
Data.csv” and the data dictionary has been provided in a separate
file named as “Data Dictionary.csv” The data dictionary contains
meanings of abbreviations as well. Some frequent ones are loc
(local), IC (incoming), OG (outgoing), T2T (telecom operator to
telecom operator), etc.
 The attributes containing 6, 7, 8, 9 as suffixes imply that those
correspond to the months 6, 7, 8, 9 respectively.
The following data preparation steps are crucial for this problem:
1. Derive new features
Use your business understanding to derive features you think could
be important indicators of churn. This is one of the most important
parts of data preparation since good features are often the
differentiators between good and bad models.
2. Filter high-value customers
Define high-value customers as follows: Those who have recharged
with an amount more than or equal to X, where X is the 70th
percentile of the average recharge amount in the first two months (the
good phase).
3. Tag churners and remove attributes of the churn phase
Now tag the churned customers (churn=1, else 0) based on the fourth
month as follows: Those who have not made any calls (either incoming
or outgoing) AND have not used mobile internet even once in the
churn phase. The attributes you need to use to tag churners are:
 total_ic_mou_9
 total_og_mou_9
 vol_2g_mb_9
 vol_3g_mb_9
After tagging churners, remove all the attributes corresponding to the
churn phase (all attributes having ‘9’, etc. in their names).
Assignment:
i. Clean the Data-set and prepare it for model building
ii. Derive atleast 2 new features based on available variables in the
data-set. Justify your reasons on how the derived features will
add-on to your model prediction
iii. There might be class imbalance present when you define your
target variable. Use any technique necessary to handle it.
iv. Explore the data-set and visualize it using matplotlib and seaborn
(atleast 5 different graphs from each library)
v. Clearly define which metric or metrics will you use to evaluate
your model. Justify your answer
vi. Create 2 logistic models - one with PCA for dimensionality
reduction and another without PCA using all features and
compare their results in terms of accuracy, precision, recall, etc.
vii. In part v above, using PCA will improve computational time but
will have an impact on interpret-ability of the model. Hence, in
your opinion, should we use PCA or not. Justify your answer
viii. Now, train a model using SVM and compare it with Logistic
Regression model built with PCA from the above step and
summarize your findings
ix. Train a Decision Tree, a Random Forest on the dataset. Tune the hyperparameters using GridSearchCV(), if
required, and evaluate the models on your chosen metrics as per V
above
x. Provide a list of atleast 4 most important and least important
features that contribute in predicting churn
xi. Finally, compare all the models (logistic with PCA, logistic without
PCA, SVM, Decision Tree, Random Forest) and recommend
one final model for deployment. Base your recommendation
on chosen metric, computational resources
required, interpret-ability, ease of usage, feature importance, etc.
NOTE: A good code is readable, well commented and properly
structured with all your assumptions defined properly. We
encourage good coding practices and thus it carries 20% of
total weight-age."	2	23	0	arzooparihar	jio-telecom-churn-prediction
8191	8191	sartorius-resnet-50-classifier-finetuned		['biology']		0	23	0	uchiborikoki	sartoriusresnet50classifierfinetuned
8192	8192	satorius_model_best		[]		0	29	1	kakarroto	satorius-model-best
8193	8193	linear regression and multi regression.py		[]		9	82	2	deepakvats052	linear-regression-and-multi-regressionpy
8194	8194	Plant Disease Prediction Dataset		['research', 'earth and nature', 'image data', 'python', 'ml ethics']		5	100	1	shuvranshu	plant-disease-prediction-dataset
8195	8195	conmmdet		[]		2	23	1	dantelockhart	conmmdet
8196	8196	cupy_whl_python37		[]		0	28	1	kakarroto	cupy-whl-python37
8197	8197	binance btcusdt trade data 479279913-594360905		['business']		0	2	0	hejianhua198711	binance-btcusdt-trade-data-479279913594360905
8198	8198	titanic1		[]		0	16	0	wsyanjiaxi	titanic1
8199	8199	PAW-swin-base-patch4-window12-384-in22k		[]		2	36	0	a24998667	paw-swin-base-patch4-window12-384-in22k
8200	8200	speech123		[]		0	0	0	jinagamjyotsnaa	speech123
8201	8201	swin384newptexp1		[]		2	11	1	nischaydnk	swin384newptexp1
8202	8202	speech (1) 		['social science']		0	1	0	jinagamjyotsnaa	speech-1
8203	8203	FuelConsumptionCo2		[]		0	6	0	unnagirilavanya	fuelconsumptionco2
8204	8204	checkpoint		['law']		0	20	0	pororsenokporoshenko	checkpoint
8205	8205	landmark-448-train-1		[]		1	22	0	kittenraidrua	landmark-448-train-1
8206	8206	Sartorius-stage2-Z500-20211230081742		[]		0	45	0	hideyukizushi	sartorius-stage2-z500-20211230081742
8207	8207	kagglejson		[]		2	3	0	stdcout42	kagglejson
8208	8208	food_wastage		[]		1	8	0	sanamps	food-wastage
8209	8209	 Pedal Me Bicycle Deliveries Data Set	This Dataset is for Beginners	['cycling', 'beginner', 'intermediate', 'time series analysis', 'text data']	"Data Set Information:
A dataset about the number of weekly bicycle package deliveries by Pedal Me in London during 2020 and 2021. Nodes in the graph represent geographical units and edges are proximity based mutual adjacency relationships.
Attribute Information:
Attributes are the weekly deliveries done by Pedal Me in certain regions of London.
Relevant Papers:
PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models
Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Nicolas Collignon, Rik Sarkar
Source:
Benedek Rozemberczki, The University of Edinburgh, benedek.rozemberczki '@' gmail.com"	36	352	9	dhinaharp	pedal-me-bicycle-deliveries-data-set
8210	8210	shouzi		[]		0	36	0	shouzi	shouzi
8211	8211	exp24 cascade X152 		[]		1	58	0	imeintanis	exp24-cascade-x152
8212	8212	Political Partisanship Tweets	Tweets sent from the US politicians and non-politician users	['politics']		1	38	0	lingshuhu	political-partisanship-tweets
8213	8213	Accelerometer Data Set	This dataset was generated for use on 'Prediction of Motor Failure Time	['electronics', 'beginner', 'intermediate', 'tabular data', 'regression']	"Data Set Information:
This dataset was generated for use on 'Prediction of Motor Failure Time Using An Artificial Neural Network' project (DOI: 10.3390/s19194342). A cooler fan with weights on its blades was used to generate vibrations. To this fan cooler was attached an accelerometer to collect the vibration data. With this data, motor failure time predictions were made, using an artificial neural networks. To generate three distinct vibration scenarios, the weights were distributed in three different ways: 1) 'red' - normal configuration: two weight pieces positioned on neighboring blades; 2) 'blue' - perpendicular configuration: two weight pieces positioned on blades forming a 90Â° angle; 3) 'green' - opposite configuration: two weight pieces positioned on opposite blades. A schematic diagram can be seen in figure 3 of the paper.
Devices used:
Akasa AK-FN059 12cm Viper cooling fan (Generate the vibrations)
MMA8452Q accelerometer (Measure vibration)
Data collection method:
17 rotation speeds were set up, ranging from 20% to 100% of the cooler maximum speed at 5% intervals; for the three weight distribution configurations in the cooler blades. Note that the Akasa AK-FN059 cooler has 1900 rpm of max rotation speed.
The vibration measurements were collected at a frequency of 20 ms for 1 min for each percentage, generating 3000 records per speed. Thus, in total, 153,000 vibration records were collected from the simulation model.
Source:
Gustavo Scalabrini Sampaio
gustavo.sampaio '@' mackenzista.com.br
Postgraduate Program in Electrical Engineering and Computing, Mackenzie Presbyterian University, SÃ£o Paulo, Brazil.
Arnaldo Rabello de Aguiar Vallim Filho
arnaldo.aguiar '@' mackenzie.br
Computer Science Dept., Mackenzie Presbyterian University, SÃ£o Paulo, Brazil.
Leilton Santos da Silva
leilton '@' emae.com.br
EMAEâ€”Metropolitan Company of Water & Energy, SÃ£o Paulo, Brazil.
Leandro Augusto da Silva
leandroaugusto.silva '@' mackenzie.br
Postgraduate Program in Electrical Engineering and Computing, Mackenzie Presbyterian University, SÃ£o Paulo, Brazil.
Donator:
Gustavo Scalabrini Sampaio"	76	930	12	dhinaharp	accelerometer-data-set
8214	8214	config1		[]		0	67	0	houchao1999	config1
8215	8215	G8_CELL_Weights		[]		0	99	0	groupof88rc	g8-cell-weights
8216	8216	Top 100 Korean Drama (MyDramaList)	Korean Drama from MyDramaList website (as of Thursday December 30th, 2021)	['arts and entertainment', 'movies and tv shows', 'data visualization', 'recommender systems', 'korea']	"Top 100 Korean Drama on MyDramaList
According to mydramalist.com, ""MyDramaList.com is a community-based project which provides Asian drama & movie fans"".  On the website, fans can ""create their very own drama watchlists, rate dramas and films, write reviews"" and many more engaging activities. This dataset ranks the Top 100 Korean Drama rating given by the users on the website. 
Content
Name: Korean Drama name
Year of release: Release year of the drama
Aired Date: Aired Date (start) - (end)
Aired On: Aired on what day(s) of the week
Number of Episode: How many episodes are there
Network: What Network is the drama aired on
Duration: How long is one episode approximately
Content Rating: Content rate for appropriate audience
Synopsis: Short story of the drama
Cast: Actors and Actresses in the drama
Genre: Genre that the drama is listed in 
Tags: Tags that the drama is listed in 
Rank: Ranking on the website
Rating: Rating by the users on the website out of ten
Acknowledgements
This data is taken from the website https://mydramalist.com/shows/top_korean_dramas?page=1. This is my first time doing web scrapping. I wouldn't be able to do it without the help of StackOverflow and YouTube.
Inspiration
I have been a huge fan of Korean Drama and K pop since high-school.  It is fun to integrate what I love with my interest toward data science. Some of the interesting questions (possibly tasks) here would be:
What is/are the popular Genres and Theme (Tags column) and does it leads to higher rating? 
Does the day at which is the drama is aired affects its rating?
Create a recommendation system for the Drama (and check with the recommendation by the website)
The possibility is endless...keep striving!"	1137	8191	38	chanoncharuchinda	top-100-korean-drama-mydramalist
8217	8217	plantpathology2020fgvc777		[]		0	15	0	bigstar1999	plantpathology2020fgvc777
8218	8218	unicorn companies around the world in 2021		[]		5	69	6	chaudharypriyanshu	unicorn-companies-around-the-world-in-2021
8219	8219	face mask train		[]		11	55	0	rahilmehtaucoe2784	face-mask-train
8220	8220	GooglePlayStore- Case Study1		[]		1	11	0	ashishbainsla	googleplaystore-case-study1
8221	8221	googleplaystore		[]		12	31	0	thisisbhupendrasingh	googleplaystore
8222	8222	STUDENTS PERFORMANCE DATASET	This Dataset contains information about the Students performance .	['education']	"Context
This Dataset contains the information about the students and their Performance  try to predict using the simple machine Learning Algorithm.
Content
It contains the Specific information about the Schooling,Family issues,personal Relationship of the students,Internet facility and so on.
Acknowledgements
This data are gathered from numerous sources thanks a lot @uci_repository.
Inspiration
Try to find out the issues with the Students as they are future Personalities ,to save their Schooling and Youth life."	616	3918	26	balavashan	students-performance-dataset
8223	8223	jigsaw-pretrainedvalid-finetune-fgm-910finished		[]		18	29	0	shaoqh	jigsawpretrainedvalidfinetunefgm910finished
8224	8224	Online Education System - Review		['education', 'statistical analysis', 'classification', 'online communities', 'covid19']	"Pandemic has influenced all spheres of the humanity.  COVID-19 impacted the education vertical in larger manner. Traditional classroom environment plays a very vital role in molding the life of an individual. Bond nurtured in the early ages of the life acts as the great moral support in the latter stages of the journey. As the pandemic has forced us into online education, this data collection aims to analyze the impact of online education. To check out the satisfactory level of the learners, review was conducted.
Gender – Male, Female
Home Location – Rural, Urban
Level of Education – Post Graduate, School, Under Graduate
Age – Years
Number of Subjects – 1- 20 
Device type used to attend classes – Desktop, Laptop, Mobile
Economic status – Middle Class, Poor, Rich
Family size – 1 -10
Internet facility in your locality – Number scale (Very Bad to Very Good)
Are you involved in any sports? – Yes, No
Do elderly people monitor you? – Yes, No
Study time – Hours
Sleep time – Hours
Time spent on social media – Hours
Interested in Gaming? – Yes, No
Have separate room for studying? – Yes, No
Engaged in group studies? – Yes, No
Average marks scored before pandemic in traditional classroom – range
Your interaction in online mode - Number scale (Very Bad to Very Good)
Clearing doubts with faculties in online mode - Number scale (Very Bad to Very Good)
Interested in? – Practical, Theory, Both
Performance in online - Number scale (Very Bad to Very Good)
Your level of satisfaction in Online Education – Average, Bad, Good
radhakrishnan, sujatha (2021), “Online Education System - Review”, Mendeley Data, V1, doi: 10.17632/bzk9zbyvv7.1"	91	474	4	sujaradha	online-education-system-review
8225	8225	swin_model_wgts		[]		0	9	3	kakarroto	swin-model-wgts
8226	8226	advertising.csv		[]		4	14	0	unnagirilavanya	advertisingcsv
8227	8227	muntazar_dataset		[]		0	6	0	faisalalfarhan	muntazar-dataset
8228	8228	trained_models		[]		3	234	0	awaptk	trained-models
8229	8229	longformer-large-4096		[]		1	25	0	goldenlock	longformer-large-4096
8230	8230	face mask Dataset		[]		8	78	0	alfaromeo676	face-mask-dataset
8231	8231	slstme		[]		1	15	0	fyp2022	slstme
8232	8232	satrotius_CBNet_swin_small_Adam_fold_2		[]		0	21	0	hwigeon	satrotius-cbnet-swin-small-adam-fold-2
8233	8233	myimdb		[]		1	25	0	hahaha5689	myimdb
8234	8234	nsk_image_search3_man2		[]		0	9	0	motono0223	nsk-image-search3-man2
8235	8235	bert-base-validafit		[]		1	30	0	qhshao	bertbasevalidafit
8236	8236	namldata		[]		0	30	0	anhthemduocngu	namldata
8237	8237	movie ratings	For building a movie recommender system	['movies and tv shows', 'nlp', 'recommender systems']	This dataset contains movie titles, movie ratings given by users to various movies	6	117	2	vrajm1209	movie-ratings
8238	8238	厦门国际银行数创金融杯金融营销建模大赛		[]		7	64	1	liuzhuangzhuang	finance-marketing
8239	8239	Predict weight based on food and step count		['exercise', 'nutrition', 'beginner', 'food']	"Context
Weight prediction  based  on daily food and daily step count. I want to see my weighht change overall the period with basic physical activity and diet .I am 176 cms ideal weight is 70-72 kgs data listed until i acheive that slab.steps count has been taken from my Iphone Health app and food measurement from an online webapp.My food mostly consists of rice its a staplefood.
Note: I have deleted old Dataset as they are not my personal.
Content
This dataset consists my overall data collection.Weight has been taken everyday morning."	4	129	0	nageshn	predict-weight-based-on-food-and-step-count
8240	8240	bert-base-pretrained		[]		0	23	0	shaoqh	bertbasepretrained
8241	8241	mouse-writh-classifer		[]		0	13	0	pipihuan	mousewrithclassifer
8242	8242	13_fold		[]		0	18	0	gmlwlskim	13-fold
8243	8243	50 startups profit 		[]		2	16	0	garesothmen	50-startups-profit
8244	8244	include_left		[]		0	21	0	mohitsawwalakhe	include-left
8245	8245	mmdetection-master		[]		0	9	1	dantelockhart	mmdetectionmaster
8246	8246	oubgdata9		[]		0	7	0	missinglove	oubgdata9
8247	8247	diamonds.csv		[]		0	24	0	asmaalthobaity	diamondscsv
8248	8248	ten-kfold		[]		1	4	0	aliyumuazu	tenkfold
8249	8249	BostonHousing		[]		0	24	0	jaynewquist	bostonhousing
8250	8250	Maskdetection2111		[]		0	15	0	antonyjoseph21	maskdetection2111
8251	8251	jigsaw_multi		[]		0	6	0	qinyukun	jigsaw-multi
8252	8252	HR Dataset v101		[]		2	35	0	vicako	hr-dataset-v101
8253	8253	total fashion mnist cnn class py	load and preprocess data + build and evaluate model	['earth and nature', 'deep learning', 'cnn', 'keras', 'tensorflow']	"Context
code example 💯 
fmCNN = FashionMnistCNN()
fmCNN.initCNN(32,3,True)
fmCNN.addCNN(64,3,True)
fmCNN.flatten()
fmCNN.addDense(100)
fmCNN.dropOut(0.4)
fmCNN.addSoftmaxDense(10)
fmCNN.trainModel(20)
model = fmCNN.getModel()
history = fmCNN.getModelHistory()"	6	103	5	rhythmcam	total-fashion-mnist-cnn-class-py
8254	8254	crypto_competish_1234	For crypto competition	['time series analysis', 'model comparison', 'model explainability']		0	106	1	mdominguez2010	crypto-competish-1234
8255	8255	Sartorius CPs last		[]		1	10	0	theoviel	sartorius-cps-last
8256	8256	up from server		[]		1	6	0	gernotzcklein	passtpot20
8257	8257	Okun's Law	Macroeconomic Aggregates of both Unemployment and Gross Domestic Products	['law']	"Context
The importance of Okun's Law at the economic level especially when we talk about the problem of Unemployment, this model helps us to know how to remove or reduce the problem of unemployment. further, it can give us more information about the type of unemployment. 
However, for this version of Data, I based on the several countries to know the special elements with them when we apply the same model. that's why I choose annual frequentists because we do have not the same volume of data for all countries that I had to choose it.
Content
First of all, this data is available for everyone, just install wbdata package in your notebook. 
So this data has 4 features that are really important to build Okun's Law model.
GDP growth: Gross Domestic Products growth (annual %)
Unemployment_TLF: Unemployment, total (% of total labor force) (modeled ILO estimate)
Unemployment_AEF ::  Unemployment with advanced education, female (% of female labor force with advanced education)
Unemployment_AEM: Unemployment with advanced education, male (% of male labor force with advanced education)
Unemployment_AET :  Unemployment with advanced education (% of total labor force with advanced education)
Acknowledgements
Okun's Law model is like this : 
Unemployment = beta * GDP Growth + alpha
Inspiration
As you know the data is not enough, so we need to use the Bayesian approach to estimate these coefficients."	3	45	4	youneseloiarm	okuns-law
8258	8258	dataset_delivery		[]		1	8	0	samighazouani	dataset-delivery
8259	8259	USA Police violence		['crime']		3	20	0	hamidosharaf	usa-police-violence
8260	8260	World Population	Dataset of the world's population	['global', 'geography', 'people', 'social science', 'tabular data']	"Content
The dataset has 6 columns described as following:
Rank: Country rank by population
Country: Country name
Region: Country region
Population: Country population
Percentage: Percentage of population worldwide
Date: Date when population was measured
Questions to be answered
What is the population of each region ?
Which country has the most population in each region ?
What is the percentage of the first 10 countries ?"	812	4715	20	khaiid	world-population
8261	8261	Yemekhane		[]		0	28	0	caglarhekimci	yemekhane
8262	8262	MegaVirada21		[]		0	32	0	walterdaraujo	megavirada21
8263	8263	titanic		[]		0	6	0	selmayaman	titanic
8264	8264	Yemekhane		[]		1	33	0	harunmertstn	yemekhane
8265	8265	Cause of death dataset 2001-2010 (EU region)		['europe', 'healthcare', 'health conditions']		0	35	0	ranimeiyammai	cause-of-death-dataset-20012010-eu-region
8266	8266	Water Body Segmentation From Satellite Images	Dataset created from Sentinel-2 images	['water bodies', 'earth science', 'computer science', 'programming', 'classification', 'image data', 'python']	"Dataset info
The dataset is still in progress, so the number of photos will increase.
Dataset contains 10 images created from photos taken by Sentinel-2, and QGIS3 project files, which allow you to edit images.
Each image consists of:
- Segmentation mask of water bodies (manually created),
- Image with RGB and NIR bands
- Image with Red Edge 1-4 and SWIR 1-2 bands
Background
The dataset was created for testing a new method of water body segmentation from satellite images.
License
All Sentinel-2 data products are provided under the terms and conditions prescribed by the European Commission’s Copernicus Programme.  For detailed information on data policy, appropriate usage, and citation of Sentinel data, click on this link: https://www.usgs.gov/media/files/sentinel-2-terms-and-conditions.
Load dataset script
Python script that loads data into numpy arrays:
https://github.com/MateuszStarczyk/water-segmentation-dataset-load-script"	9	113	1	mateuszst	water-body-segmentation-from-satellite-images
8267	8267	model_final_f96b26.pkl		['sports']		0	14	0	kareemmosharkawy1	model-final-f96b26pkl
8268	8268	YemekhaneRezervasyonw/Null		[]		0	3	0	harunmertstn	yemekhanerezervasyonwnull
8269	8269	Hojapapacriolla		[]		0	11	1	deiverfabian	hojapapacriolla
8270	8270	mapping_qy_ldr_train		[]		1	16	0	yaohuiliu	mapping-qy-ldr-train
8271	8271	petfinder-breeds-tfr		[]		0	6	0	ks2019	petfinder-breeds-tfr
8272	8272	Breast Cancer Dataset	Binary Classification Prediction for type of Breast Cancer	['healthcare', 'classification', 'tabular data', 'binary classification', 'cancer']	"Description:
Breast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.
The key challenges against it’s detection is how to classify tumors into malignant (cancerous) or benign(non cancerous). We ask you to complete the analysis of classifying these tumors using machine learning (with SVMs) and the Breast Cancer Wisconsin (Diagnostic) Dataset.
Acknowledgements:
This dataset has been referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict whether the cancer type is Malignant or Benign.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	1953	11729	43	yasserh	breast-cancer-dataset
8273	8273	Housing		['social issues and advocacy']		0	19	0	faisalalfarhan	housing
8274	8274	mapaas		[]		0	31	0	rachelalbrecht	mapaas
8275	8275	SCS CellPose Weights		[]		0	9	0	aishikai	scs-cellpose-weights
8276	8276	Covid-19 Dataset		[]		6	47	5	durgeshrao9993	covid19-dataset
8277	8277	TCC_Stenico		[]		1	66	1	jessstenico	tcc-stenico
8278	8278	reef-subsequence-dataset		[]		0	10	0	artgro	reefsubsequencedataset
8279	8279	reef-dataset		['earth science']		0	19	0	artgro	reefdataset
8280	8280	face mask		[]		0	35	0	aykutmurkit	face-mask
8281	8281	SwinT_detectron2		[]	https://github.com/xiaohu2015/SwinT_detectron2.git	0	5	0	anonamename	swint-detectron2
8282	8282	feedback-prize: bio ner train data		['computer science', 'programming']	"There are now 4 different files to choose from 😳 
Folder original uses the data given by the hosts.
Folder corrected uses the data from my corrected notebook (https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize)
split_at_whitespace.json first splits the text at whitespace and then assigns a label to each token. This does not pass through a Tokenizer object.
include_whitespace.json does not split at whitespace. This output comes out of a Tokenizer. This is tokenizer-specific so make sure you use the right tokenizer :) This file is currently for BigBird
split_at_whitespace.json has three columns
1. id = file id
2. words = text from file split at whitespace
3. bio = label assigned to each word
include_whitespace.json has   columns
1. id = file id
2. input_ids = input_id for each token produced by Tokenizer
3. attention_mask = mask value for attention in model - produced by Tokenizer
4. offset_mapping = way to map tokens back to character positions in original text
5. labels = label assigned to each token (these are the actual label values and not numerical ids)
Used for training NER models. 
Notebook used to make it here: https://www.kaggle.com/nbroad/feedback-prize-bio-format-for-ner
Example training notebook here: https://www.kaggle.com/nbroad/bigbird-ner-training-pt-gpu-feedback-prize
Labels:
B-Claim. 
B-Concluding Statement
B-Counterclaim
B-Evidence 
B-Lead
B-Position
B-Rebuttal
I-Claim
I-Concluding Statement
I-Counterclaim
I-Evidence
I-Lead
I-Position
I-Rebuttal
O"	15	145	5	nbroad	feedbackprize-bio-ner-train-data
8283	8283	Best Team in Champions League History? 	Champions League final match Team data from 1956 - 2017	['football']	"Key variables and their explanation for understanding:
Winner: Team Wins Title
Finalist: Runner up team
Winner score: winning team scored goals
Finalist score: Runner up team scored goals
Stadium: Venue where game was played
Attendees: people who watched the match live on stadium
Capacity: Actual capacity of stadium
People attendant match in excess: (Attendees – Capacity)
Note: Because of some stadium are not in use and demolished, I assumed capacity of stadium equals to attendees
Note:  People attendant match in excess shows you the popularity of team and fans visited the stadium to watch live."	3	35	1	soccer988	best-team-in-champions-league-history
8284	8284	Web_Google		[]		0	2	0	mehrankazeminia	web-google
8285	8285	TomAndJerry_Train	Train test for short film of tom and jerry used to train RUnet and SRGAN	['arts and entertainment']		1	37	0	adamformnek	tomandjerry-train
8286	8286	bbc_news		[]		2	3	0	vishwajit285	bbc-news
8287	8287	NBfenlei	sssssssssssssssssssssssssssssssss	[]		0	50	1	sg2333333	nbfenlei
8288	8288	Credit-scoring data	The data is collect from real sources. 	['banking', 'intermediate', 'data cleaning', 'classification', 'text data']	The data is collected from the real source and converted to numeric values for better training and classifying the people who are able or not able to pay their credits.	52	313	1	islombekdavronov	creditscoring-data
8289	8289	sartorius-cellpose-models	class-specific cellpose models - three different 5-fold CV sets 	[]		1	35	0	qitvision	sartoriuscellposemodels
8290	8290	Green-Deal		[]		0	29	0	annaschmer	greendeal
8291	8291	nishika_narou_HiF_models		[]		0	14	0	hiroakifukuse	nishika-narou-hif-models
8292	8292	Bikeshare Analysis - Divvy Data	Google Certificate Capstone Project	['education']		0	49	0	atipping	google-certificate-capstone-project
8293	8293	Ocorrências Criminais RJ	Registros Criminais do Estado do Rio de Janeiro	['cities and urban areas', 'brazil', 'crime', 'text data']	"Context
O Rio de Janeiro é uma das mais belas cidades do Brasil, porém, infelizmente,  também é uma das mais perigosas. Anos de políticas públicas predatórias acentuaram a marginalização e desorganização urbana da cidade, abrindo cenário para vários crimes, cometidos diariamente por todo o estado.
Desde 2006, o Instituto de Segurança Pública do Rio de Janeiro  realiza a compilação de atendimentos de todas as delegacias do Rio.
Content
Disponibilizamos 3 datasets, descritos a seguir:
BaseDPEvolucaoMensalCisp - Evolução Mensal das Estatísticas por Delegacia;
PopulacaoEvolucaoMensalCisp - Evolução Mensal da População coberta por cada Delegacia;
delegacias - Informações sobre as Delegacias do Estado
Acknowledgements
Esse conjunto de dados foi disponibilizado através do Instituto de Segurança Pública do Rio de Janeiro.
Inspiration
Uso para pesquisas referenciais e projetos educacionais."	7	71	1	rodrigpn	ocorrncias-criminais-rj
8294	8294	asgfdhgd		[]		0	0	0	rvravi	asgfdhgd
8295	8295	SLP_valid_test		[]		1	9	0	badgenius	slp-valid-test
8296	8296	petfinder-solv07		[]		0	2	0	aerdem4	petfinder-solv7
8297	8297	How Can Sellers Leverage Amazon A+ content 		['retail and shopping']	Amazon is a leading online platform offering ample opportunities to sellers to sell and expand their base on the global front through enhanced content. Here’s everything you need to know about Amazon A+ content and how you can take advantage of it to bring in more sales and conversions to your kitty.	0	17	0	sophiehayes	how-can-sellers-leverage-amazon-a-content
8298	8298	pytorch_pretrained_3		[]		2	10	1	titericz	pytorch-pretrained-3
8299	8299	Sartrius cellpose train dataset		['transportation']		25	84	0	kawano	sartrius-cellpose-train-dataset
8300	8300	Data scientist salary	Data science jobs dataset cleaned for data analysis and modelling	['united states', 'business', 'computer science', 'data cleaning', 'data visualization', 'data analytics', 'jobs and career']	"Context
This dataset was made by scrapping the job postings related to the position of 'Data Scientist' from www.glassdoor.com in USA, I used selenium to scrap the data. After scrapping the raw data, I removed the duplicated rows from it which reduced the records from 1000 to 742. After this, several simplifications were performed to make the data user friendly for further data analysis and modelling.
Content
With each job, I got the following columns: Job title, Salary Estimate, Job Description, Rating, Company, Location, Company , Headquarters, , , any Size, Company Founded Date, Type of Ownership, Industry, Sector, Revenue, Competitors.
Note: Columns with value -1 means either the data scraping was unsuccessful for that or the data was not present.
Acknowledgements
Thanks to Ken Jee for the inspiration behind the dataset and special thanks to Ezequiel Starecinch for the scrapper.
Inspiration
I was inspired by Ken Jee to scrap this dataset and to clean it.
https://github.com/PlayingNumbers/ds_salary_proj
While scrapping and cleaning, I took ideas from Ezequiel Starecinch.
https://github.com/echestare"	2360	13836	63	nikhilbhathi	data-scientist-salary-us-glassdoor
8301	8301	75000+ used cars dataset with specifications	Make, Engine Type, Engine Capacity, Price and lots of features of 75000+ cars	['cities and urban areas', 'business', 'beginner', 'intermediate', 'linear regression']	Pakistan is the world's 5th largest country by population. As the population increases, the demand for goods and the necessities of life also increases, and the car is one of them. PakWheels is the largest online marketplace for car shoppers and sellers in Pakistan. The data in this dataset is web scrapped from the PakWheels website using the Python library BeautifulSoup.	150	960	13	ebrahimhaquebhatti	75000-used-cars-dataset-with-specifications
8302	8302	swinpytexp3oof1778		[]		0	5	1	nischaydnk	swinpytexp3oof1778
8303	8303	petfinder_extracted_pretrained_1		[]		2	12	1	titericz	petfinder-extracted-pretrained-1
8304	8304	The Human Freedom Index	A global measurement of personal, civil, and economic freedom	['global', 'politics', 'economics']	"Context
A central purpose of The Human Freedom Index is to paint a broad but reasonably accurate picture of the extent of overall freedom in the world. A larger purpose is to more carefully explore what we mean by freedom and to better understand its relationship to any number of other social and economic phenomena. 
Content
The Human Freedom Index measures economic freedoms such as the freedom to trade or to use sound money, and it captures the degree to which people are free to enjoy the major freedoms often referred to as civil liberties—freedom of speech, religion, association, and assembly— in the countries in the survey. In addition, it includes indicators on rule of law, crime and violence, freedom of movement, and legal discrimination against same-sex relationships. We also include nine variables pertaining to women-specific freedoms that are found in various categories of the index.
Citation
Ian Vásquez, Fred McMahon, Ryan Murphy, and Guillermina Sutter Schneider,&nbsp;The Human Freedom Index 2021: A Global Measurement of Personal, Civil, and Economic Freedom&nbsp;(Washington: Cato Institute and the Fraser Institute, 2021)."	10088	77395	342	gsutters	the-human-freedom-index
8305	8305	satrotius_CBNet_swin_small_Adam_fold_1		[]		0	16	0	hwigeon	satrotius-cbnet-swin-small-adam-fold-1
8306	8306	sahiyolo		[]		1	43	0	sahilchachra	sahiyolo
8307	8307	Heart Disease Dataset	A simple records of Heart Patients monitored - Binary Classification Problem.	['healthcare', 'classification', 'tabular data', 'binary classification', 'heart conditions']	"Description:
This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to
this date. The ""goal"" field refers to the presence of heart disease in the patient. It is integer-valued from 0 (no presence) to 4.
Acknowledgements:
This dataset has been referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict whether or not the patients have Heart Disease.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	615	3737	21	yasserh	heart-disease-dataset
8308	8308	Lovely Doggo with Bonky (fastai & timm) 17.86		[]		1	40	1	hiteshkumars	lovely-doggo-with-bonky-fastai-timm-1786
8309	8309	Bitcoin time series with different time intervals	BTC-USD, Time Series OHLCV	['finance', 'investing', 'currencies and foreign exchange']	"Context
BTC-USD Dataset, with different time intervals.
Right now 1 day and 15 min candles.
Because traders use different time frame charts in trading platforms for their technical analysis.
Here I am collecting the most used time intervals.
Note
Prices in this dataset make no guarantee or warranty on the accuracy or completeness of the data provided.
Inspirations
Time Series Analysis, Forecast, Technical Analysis, Build Trading Strategy with short interval data."	192	1613	21	nisargchodavadiya	bitcoin-time-series-with-different-time-intervals
8310	8310	yolov5s		[]		0	29	0	sahilchachra	yolov5s
8311	8311	price-paid-data		['business']		1	11	0	cemsina	pricepaiddata
8312	8312	Data_excel		[]		3	13	0	nhthunhhong	data-excel
8313	8313	Daily Gold Price (2015-2021) Time Series	Historical Daily Gold Price 	['time series analysis']	"Content
Daily gold prices (2014-01-01 to 2021-12-29)
Acknowledgements
Raw Data Source: https://in.investing.com/commodities/gold-mini
This data frame is preprocessed to time series analysis and forecasting
Inspiration
Forecast, Predict Prices, Time Series Forecasting
Note
Gold Prices in this dataset makes no guarantee or warranty on the accuracy or completeness of the data provided."	433	2382	15	nisargchodavadiya	daily-gold-price-20152021-time-series
8314	8314	all jigsaw dataset		[]		15	63	0	themadrambito	all-jigsaw-dataset
8315	8315	ecvivalent		[]		0	15	0	shaldymovkonstantin	ecvivalent
8316	8316	Movies_Test		[]		0	17	0	mehrankazeminia	movies-test
8317	8317	mmdet pseudo label model		['earth and nature']		37	33	0	duykhanh99	mmdet-pseudo-label-model
8318	8318	SentEval	SentEval from https://github.com/facebookresearch/SentEval	['computer science', 'programming', 'nlp', 'text data']	"Our modification to SentEval:
Add the all setting to all STS tasks.
Change STS-B and SICK-R to not use an additional regressor.
SentEval: evaluation toolkit for sentence embeddings
SentEval is a library for evaluating the quality of sentence embeddings. We assess their generalization power by using them as features on a broad and diverse set of ""transfer"" tasks. SentEval currently includes 17 downstream tasks. We also include a suite of 10 probing tasks which evaluate what linguistic properties are encoded in sentence embeddings. Our goal is to ease the study and the development of general-purpose fixed-size sentence representations.
(04/22) SentEval new tasks: Added probing tasks for evaluating what linguistic properties are encoded in sentence embeddings
(10/04) SentEval example scripts for three sentence encoders: SkipThought-LN/GenSen/Google-USE
Dependencies
This code is written in python. The dependencies are:
Python 2/3 with NumPy/SciPy
Pytorch&gt;=0.4
scikit-learn&gt;=0.18.0
Transfer tasks
Downstream tasks
SentEval allows you to evaluate your sentence embeddings as features for the following downstream tasks:
| Task      | Type                          | #train    | #test     | needs_train   | set_classifier |
|---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:|
| MR         | movie review                  | 11k       | 11k       | 1 | 1 |
| CR         | product review                | 4k        | 4k        | 1 | 1 |
| SUBJ       | subjectivity status           | 10k       | 10k       | 1 | 1 |
| MPQA       | opinion-polarity  | 11k       | 11k       | 1 | 1 |
| SST          | binary sentiment analysis     | 67k       | 1.8k      | 1 | 1 |
| SST          | fine-grained sentiment analysis   | 8.5k      | 2.2k      | 1 | 1 |
| TREC        | question-type classification  | 6k        | 0.5k      | 1 | 1 |
| SICK-E       | natural language inference    | 4.5k      | 4.9k      | 1 | 1 |
| SNLI       | natural language inference    | 550k      | 9.8k      | 1 | 1 |
| MRPC | paraphrase detection  | 4.1k | 1.7k | 1 | 1 |
| STS 2012     | semantic textual similarity   | N/A       | 3.1k      | 0  | 0 |
| STS 2013    | semantic textual similarity   | N/A       | 1.5k      | 0  | 0 |
| STS 2014   | semantic textual similarity   | N/A       | 3.7k      | 0  | 0 |
| STS 2015    | semantic textual similarity   | N/A       | 8.5k      | 0  | 0 |
| STS 2016    | semantic textual similarity   | N/A       | 9.2k      | 0  | 0 |
| STS B     | semantic textual similarity   | 5.7k      | 1.4k      | 1 | 0 |
| SICK-R       | semantic textual similarity | 4.5k        | 4.9k      | 1 | 0 |
| COCO        | image-caption retrieval       | 567k      | 5*1k      | 1 | 0 |
where needs_train means a model with parameters is learned on top of the sentence embeddings, and set_classifier means you can define the parameters of the classifier in the case of a classification task (see below).
Note: COCO comes with ResNet-101 2048d image embeddings. More details on the tasks.
Probing tasks
SentEval also includes a series of probing tasks to evaluate what linguistic properties are encoded in your sentence embeddings:
| Task      | Type                          | #train    | #test     | needs_train   | set_classifier |
|---------- |------------------------------ |-----------:|----------:|:-----------:|:----------:|
| SentLen  | Length prediction | 100k      | 10k       | 1 | 1 |
| WC   | Word Content analysis | 100k      | 10k       | 1 | 1 |
| TreeDepth    | Tree depth prediction | 100k      | 10k       | 1 | 1 |
| TopConst | Top Constituents prediction   | 100k      | 10k       | 1 | 1 |
| BShift   | Word order analysis   | 100k      | 10k       | 1 | 1 |
| Tense    | Verb tense prediction | 100k      | 10k       | 1 | 1 |
| SubjNum  | Subject number prediction | 100k      | 10k       | 1 | 1 |
| ObjNum   | Object number prediction  | 100k      | 10k       | 1 | 1 |
| SOMO | Semantic odd man out  | 100k      | 10k       | 1 | 1 |
| CoordInv | Coordination Inversion | 100k         | 10k       | 1 | 1 |
Download datasets
To get all the transfer tasks datasets, run (in data/downstream/):
bash
./get_transfer_data.bash
This will automatically download and preprocess the downstream datasets, and store them in data/downstream (warning: for MacOS users, you may have to use p7zip instead of unzip). The probing tasks are already in data/probing by default.
How to use SentEval: examples
examples/bow.py
In examples/bow.py, we evaluate the quality of the average of word embeddings.
To download state-of-the-art fastText embeddings:
bash
curl -Lo glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip
curl -Lo crawl-300d-2M.vec.zip https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip
To reproduce the results for bag-of-vectors, run (in examples/):
bash
python bow.py
As required by SentEval, this script implements two functions: prepare (optional) and batcher (required) that turn text sentences into sentence embeddings. Then SentEval takes care of the evaluation on the transfer tasks using the embeddings as features.
examples/infersent.py
To get the InferSent model and reproduce our results, download our best models and run infersent.py (in examples/):
bash
curl -Lo examples/infersent1.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent1.pkl
curl -Lo examples/infersent2.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent2.pkl
examples/skipthought.py - examples/gensen.py - examples/googleuse.py
We also provide example scripts for three other encoders:
SkipThought with Layer-Normalization in Theano
GenSen encoder in Pytorch
Google encoder in TensorFlow
Note that for SkipThought and GenSen, following the steps of the associated githubs is necessary.
The Google encoder script should work as-is.
How to use SentEval
To evaluate your sentence embeddings, SentEval requires that you implement two functions:
prepare (sees the whole dataset of each task and can thus construct the word vocabulary, the dictionary of word vectors etc)
batcher (transforms a batch of text sentences into sentence embeddings)
1.) prepare(params, samples) (optional)
batcher only sees one batch at a time while the samples argument of prepare contains all the sentences of a task.
prepare(params, samples)
* params: senteval parameters.
* samples: list of all sentences from the tranfer task.
* output: No output. Arguments stored in ""params"" can further be used by batcher.
Example: in bow.py, prepare is is used to build the vocabulary of words and construct the ""params.word_vect* dictionary of word vectors.
2.) batcher(params, batch)
batcher(params, batch)
* params: senteval parameters.
* batch: numpy array of text sentences (of size params.batch_size)
* output: numpy array of sentence embeddings (of size params.batch_size)
Example: in bow.py, batcher is used to compute the mean of the word vectors for each sentence in the batch using params.word_vec. Use your own encoder in that function to encode sentences.
3.) evaluation on transfer tasks
After having implemented the batch and prepare function for your own sentence encoder,
1) to perform the actual evaluation, first import senteval and set its parameters:
python
import senteval
params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}
2) (optional) set the parameters of the classifier (when applicable):
python
params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,
                                 'tenacity': 5, 'epoch_size': 4}
You can choose nhid=0 (Logistic Regression) or nhid&gt;0 (MLP) and define the parameters for training.
3) Create an instance of the class SE:
python
se = senteval.engine.SE(params, batcher, prepare)
4) define the set of transfer tasks and run the evaluation:
python
transfer_tasks = ['MR', 'SICKEntailment', 'STS14', 'STSBenchmark']
results = se.eval(transfer_tasks)
The current list of available tasks is:
python
['CR', 'MR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC', 'SNLI',
'SICKEntailment', 'SICKRelatedness', 'STSBenchmark', 'ImageCaptionRetrieval',
'STS12', 'STS13', 'STS14', 'STS15', 'STS16',
'Length', 'WordContent', 'Depth', 'TopConstituents','BigramShift', 'Tense',
'SubjNumber', 'ObjNumber', 'OddManOut', 'CoordinationInversion']
SentEval parameters
Global parameters of SentEval:
bash
senteval parameters
task_path                   # path to SentEval datasets (required)
seed                        # seed
usepytorch                  # use cuda-pytorch (else scikit-learn) where possible
kfold                       # k-fold validation for MR/CR/SUB/MPQA.
Parameters of the classifier:
bash
nhid:                       # number of hidden units (0: Logistic Regression, &amp;gt;0: MLP); Default nonlinearity: Tanh
optim:                      # optimizer (""sgd,lr=0.1"", ""adam"", ""rmsprop"" ..)
tenacity:                   # how many times dev acc does not increase before training stops
epoch_size:                 # each epoch corresponds to epoch_size pass on the train set
max_epoch:                  # max number of epoches
dropout:                    # dropout for MLP
Note that to get a proxy of the results while dramatically reducing computation time,
we suggest the prototyping config:
python
params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}
params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,
                                 'tenacity': 3, 'epoch_size': 2}
which will results in a 5 times speedup for classification tasks.
To produce results that are comparable to the literature, use the default config:
python
params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}
params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,
                                 'tenacity': 5, 'epoch_size': 4}
which takes longer but will produce better and comparable results.
For probing tasks, we used an MLP with a Sigmoid nonlinearity and and tuned the nhid (in [50, 100, 200]) and dropout (in [0.0, 0.1, 0.2]) on the dev set.
References
Please considering citing 1 if using this code for evaluating sentence embedding methods.
SentEval: An Evaluation Toolkit for Universal Sentence Representations
1 A. Conneau, D. Kiela, SentEval: An Evaluation Toolkit for Universal Sentence Representations
@article{conneau2018senteval,
  title={SentEval: An Evaluation Toolkit for Universal Sentence Representations},
  author={Conneau, Alexis and Kiela, Douwe},
  journal={arXiv preprint arXiv:1803.05449},
  year={2018}
}
Contact: aconneau@fb.com, dkiela@fb.com
Related work
J. R Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, S. Fidler - SkipThought Vectors, NIPS 2015
S. Arora, Y. Liang, T. Ma - A Simple but Tough-to-Beat Baseline for Sentence Embeddings, ICLR 2017
Y. Adi, E. Kermany, Y. Belinkov, O. Lavi, Y. Goldberg - Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, ICLR 2017
A. Conneau, D. Kiela, L. Barrault, H. Schwenk, A. Bordes - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, EMNLP 2017
S. Subramanian, A. Trischler, Y. Bengio, C. J Pal - Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning, ICLR 2018
A. Nie, E. D. Bennett, N. D. Goodman - DisSent: Sentence Representation Learning from Explicit Discourse Relations, 2018
D. Cer, Y. Yang, S. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, Y. Sung, B. Strope, R. Kurzweil - Universal Sentence Encoder, 2018
A. Conneau, G. Kruszewski, G. Lample, L. Barrault, M. Baroni - What you can cram into a single vector: Probing sentence embeddings for linguistic properties, ACL 2018"	5	51	0	williamhua	senteval
8319	8319	top10_1228		[]		0	7	0	tonyjagger	top10-1228
8320	8320	diseasDataFrame		[]		1	13	0	mrtacr2534	diseasdataframe
8321	8321	diabetic smalldataset	A few samples of DR dataset about 4000 samples	['diabetes']		9	13	0	abdhendi	diabetic-smalldataset
8322	8322	Mario-PPO-6-2		[]		0	23	0	phamquangvinh2k1	marioppo62
8323	8323	exp030-swin-t-plme-novlaf-lrdown-addpet1		[]		0	2	0	kurokurob	exp030-swin-t-plme-novlaf-lrdown-addpet1
8324	8324	bankingw		[]		1	23	0	waleedmasta	bankingw
8325	8325	WildFire-Smoke-Dataset-Yolo	🔥🔥🔥 Fireup against the Wild Fire	['beginner', 'intermediate', 'computer vision', 'deep learning', 'image data']	"This is the dataset for training a YOLO object detection model. This dataset contains all the train, test, valid splits for training a yolo model for detecting wildfire smoke.
Please upvote if this dataset is helpful."	20	261	4	ahemateja19bec1025	wildfiresmokedatasetyolo
8326	8326	Mario-PPO-6-1		[]		0	23	0	phamquangvinh2k1	marioppo61
8327	8327	swin_large_234		[]		0	54	0	oscarrangel	swin-large-234
8328	8328	marriage		[]		0	36	0	madbros	marriage
8329	8329	NormalRBC	RBC cell images for image processing 	[]		1	25	0	tanveer98	normalrbc
8330	8330	pf-138-train-5-data		[]		1	16	0	makotoikeda	pf-138-train-5-data
8331	8331	WildFire-Smoke-Dataset-Tensorflow	🔥🔥🔥 Fireup against the Wild Fire	['environment', 'computer science', 'intermediate', 'computer vision', 'dnn', 'image data']	"This is the dataset for training a TensorFlow object detection model. This dataset contains all the train, test, valid splits for training a model for detecting wildfire smoke.
Please upvote if this dataset is helpful."	39	336	6	ahemateja19bec1025	wildfiresmokedataset
8332	8332	PDNET_train_test		[]		0	20	0	jisnava	pdnet-train-test
8333	8333	paper-detection-books		['literature']		3	24	0	aleksandartokarev	paperdetectionbooks
8334	8334	VOX_DEV		[]		6	14	0	marinamaher	vox-dev
8335	8335	bigdata		['business']		0	1	0	shuyitan18	bigdata
8336	8336	trained model		[]		0	26	0	ekaterinasedykh	trained-model
8337	8337	bd_lab2		[]		0	10	0	avcolgate	bd-lab2
8338	8338	datng-bot		[]		1	23	0	danny48	datngbot
8339	8339	PDNET_train_test		[]		0	20	0	jisnava	pdnet-train-test
8340	8340	paper-detection-books		['literature']		3	24	0	aleksandartokarev	paperdetectionbooks
8341	8341	VOX_DEV		[]		6	14	0	marinamaher	vox-dev
8342	8342	bigdata		['business']		0	1	0	shuyitan18	bigdata
8343	8343	trained model		[]		0	26	0	ekaterinasedykh	trained-model
8344	8344	bd_lab2		[]		0	10	0	avcolgate	bd-lab2
8345	8345	datng-bot		[]		1	23	0	danny48	datngbot
8346	8346	Temperature in Reddelich, MV, Germany	Temperature data from my weather station, 5min intervals, May 2021 - Dec 2021	['europe', 'weather and climate', 'time series analysis', 'statistical analysis', 'tabular data']	"Temperature data in Reddelich, MV, Germany, in 5min intervals from 21 May 2020 - 29 December 2021. I plan to update the data regularly (monthly? quarterly?). The data are collected from one of the sensors connected to a ""TFA Dostmann Nexus Funk Wetterstation, 35.1075"" weather station. I bought this station specfically because it has been mentioned somewhere on the web that you can access the weather station from a Raspberry Pi via a USB cable and the open source weewx software. If you want to do the same, you may have to modify the required driver (TE923). But maybe the modifications I had to make are already implemented in the source code now (I have not updated it. Never change a running system.)"	3	192	0	rolandrau	temperature-in-reddelich
8347	8347	fer2013		[]		0	7	0	tranngocdu	fer2013
8348	8348	Tmv_2train		[]		0	9	0	balasubramaniyanmani	tmv-2train
8349	8349	HTS Checkpoints		['law']		0	17	0	hariwh0	hts-checkpoints
8350	8350	sartorius cellpose final		['arts and entertainment']		0	46	0	damtrongtuyen	sartorius-cellpose-final
8351	8351	Software Development Agency		['computer science', 'programming']		1	33	0	kimang2303	software-development-agency
8352	8352	Earthquakes 20-21 Dataset	Earthquake Dataset from Dec 2020 to Dec 2021	['earth science']	"Context
Earthquakes are one of the most impactful disasters on Earth. An earthquake is what happens when two blocks of the earth suddenly slip past one another.
This leads to crack in continental plates causing the impacts we have seen and some of us have experienced.
Content
This is the Earthquake dataset which consists of all the information about its origin, time, date and other factors. All-in-all, it describes everything about the origin of the earthquakes at a particular place and a particular time.
Acknowledgements
Acknowledgement: USGS Earthquake archives- https://earthquake.usgs.gov/earthquakes/
Inspiration
Can data science and machine learning together make the coming of an earthquake predictable? Well, that's what we have to find out."	4	41	0	arnavr10880	earthquakes-2021-dataset
8353	8353	Preprocessed Scans - OSIC PFP	OSIC Pulmonary Fibrosis Progression preprocessed CT scans	['medicine', 'computer vision', 'cnn', 'image data', 'health conditions']		0	24	0	gfugante	preprocessed-scans-osic-pfp
8354	8354	Covid-19		[]		0	10	0	hebamohamed	covid19
8355	8355	bank loan1		['banking']		0	11	0	jackliangjie	bank-loan1
8356	8356	indus-sample		[]		19	17	0	rabiya1212	indussample
8357	8357	Arabic summarization (BBC News)		[]		2	25	0	fadyelkbeer	arabic-summarization-bbc-news
8358	8358	datajantung		[]		0	24	0	maulanamajid	datajantung
8359	8359	image dataset		[]		0	11	0	chenlanglang	image-dataset
8360	8360	Upcoming Movies Dataset by using TMDB Api		['arts and entertainment']		2	8	0	shubhamsingh01122000	upcoming-movies-dataset-by-using-tmdb-api
8361	8361	Titanic		[]		0	18	0	littlehhh	titanic
8362	8362	Bert-base		[]		0	32	0	qianghanshao	bertbase
8363	8363	BW_DATASET_BIKEHOLE		[]		2	19	0	zmjjiang	bw-dataset-bikehole
8364	8364	model132		[]		0	23	0	malekbadreddine	model132
8365	8365	Jaffe2021		[]		3	38	0	ahtcmstp	jaffe2021
8366	8366	Titanic		[]		0	10	0	parthoece	titanic
8367	8367	1_50_finishdata		[]		0	32	1	bigcat111	1-50-finishdata
8368	8368	Blog-1		['online communities']		0	3	0	dexterking	blog1
8369	8369	swin_large_384_tl		[]		3	11	0	watanabetakahiro	swin-large-384-tl
8370	8370	dataset		[]		2	41	0	muhammadali4488	dataset
8371	8371	Air Passengers Forecast Dataset	Can you Forecast the future Air Passengers?	['people', 'business', 'beginner', 'time series analysis', 'tabular data', 'news']	"Description:
The ""spam"" concept is diverse: advertisements for products/websites, make money fast schemes, chain letters, pornography...
Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'George' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general-purpose spam filter.
The dataset, taken from the UCI ML repository, contains about 4600 emails labelle as spam or ham. 
Acknowledgements:
This dataset has been referred from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict whether or not the email is spam.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	403	3579	27	yasserh	air-passengers-forecast-dataset
8372	8372	Water Pump Sensor Data	Sensor data from a pump with the remaining useful life in hours given.	['business', 'manufacturing', 'time series analysis', 'lstm', 'keras', 'pytorch']	"Context
Here is the link to the original dataset Dataset. The difference in this data is that it contains the remaining useful life up to the next failure.
Content
The data are from all available sensors, all of them are raw values. The total sensor a are 52.
Acknowledgements
Thanks to UnknownClass for creating the original dataset.
Inspiration
This data can be used for a wide variety of tasks but I feel this data will be very useful in tasks such as Predictive Maintainance and Time-series forecasting."	6	52	0	anseldsouza	water-pump-sensor-data
8373	8373	Academic Blogs		['education']		0	17	0	jakericksons	academic-blogs
8374	8374	1106land		[]		0	27	0	chenwensheng	1106land
8375	8375	410714225物理四 陳駒	000000000000000000000000000000000000	[]		6	22	0	chenchu123	410714225final
8376	8376	410710245 109全學年度技專校院課程清單		[]		2	24	0	chenwangcheng	410710245-109
8377	8377	20211229		[]		0	21	0	amanda410714218	20211229
8378	8378	out marry 		['arts and entertainment']		0	6	0	s410711217	out-marry
8379	8379	tourists		[]		1	29	0	ben410923021	tourists
8380	8380	HU Chan Han		['arts and entertainment']		0	14	0	www6qwww	hu-chan-han
8381	8381	UFO-Sightings-Dataset	🛸🛸🛸🧿🧿🧿🛸🛸🛸🌌🌌🌌	['history', 'beginner', 'intermediate', 'statistical analysis', 'tabular data']	"This is a dataset containing all the ufo sightings that had occured in the history all over the world.
This dataset contains the city/state, date of occurance and also the description of the event. This is the dataset scraped from the wikipedia and also remodeled using CSV using python
Please upvote the dataset if you like this and find this useful."	13	166	4	ahemateja19bec1025	ufosightingsdataset
8382	8382	Book Dictionary From GoodReads(6000+ books)	Books data from goodreads for 6000+ books with all the necessary columns.	['text mining', 'recommender systems', 'k-means', 'text data', 'nltk']		7	86	0	subhradiphalder	book-dictionary-from-goodreads5000-books
8383	8383	slstmg-model		['clothing and accessories']		1	45	0	fyp2022	slstmgmodel
8384	8384	img-data		[]		0	29	0	yangman0817	imgdata
8385	8385	Fundamentals of Academic Writing You Should Know		[]		0	80	0	robertnicholson	fundamentals-of-academic-writing-you-should-know
8386	8386	Study Art of Distributing Word Count in Essays		['education']		0	68	0	robertnicholson	study-art-of-distributing-word-count-in-essays
8387	8387	Use of Colons, Semi-Colons, and Dashes in Essays		[]		0	70	0	robertnicholson	use-of-colons-semicolons-and-dashes-in-essays
8388	8388	Risky Common Citation Mistakes Made by Students		[]		0	64	0	robertnicholson	risky-common-citation-mistakes-made-by-students
8389	8389	chinese_dataset		[]		1	39	0	crazydad	chinese-dataset
8390	8390	A Definitive Guide to Prepositions – Guide 2022		[]		0	70	0	robertnicholson	a-definitive-guide-to-prepositions-guide-2022
8391	8391	svr exp024		[]		0	2	0	kurokurob	svr-exp024
8392	8392	car_videos		[]		0	1	0	amogh0810	car-videos
8393	8393	BBBC021 Compound Profiling Dataset	A dataset for benchmarking bioimage features	['earth and nature', 'biology', 'artificial intelligence', 'computer vision', 'classification', 'image data']		3	57	0	stanleyhua	bbbc021
8394	8394	landmark-448-train-2		[]		0	30	0	kittenraidrua	landmark-448-train-2
8395	8395	Mushroom Dataset	This is a Challenge for ML learners	['biology', 'beginner', 'intermediate', 'tabular data']		266	2880	23	dhinaharp	mushroom-dataset
8396	8396	test data starbucks survey		['business', 'xgboost', 'binary classification']	Test data for	6	77	0	rrrino39	test-data-starbucks-survey
8397	8397	yuvalBert		[]		0	19	0	homoalways	yuvalbert
8398	8398	s0backk		[]		0	10	0	praveendor	s0backk
8399	8399	longformer-base-4096		[]		10	14	0	goldenlock	longformer-base-4096
8400	8400	Credi Card Approval		[]		0	46	0	vrdapanas	credi-card-approval
8401	8401	Covid Xray Dataset		[]		0	31	0	jarvisai7	covid-xray-dataset
8402	8402	MathorCup		[]		1	27	0	psycat	mathorcup
8403	8403	FOREST FIRE DATASET	Find the area (in ha) got affected by the forest fire using ML algorithm 	['business']	"Context
This Dataset contains the details about the forest fire that happens a othen due to several conditions try to predict the Areas got affected so the futher forest fire can be cotrolled due to the Machine learning .
Content
It contains the Factors that induce the Fire also the Factors that makes useful like Wind and Rain respectively.
Acknowledgements
This contents are specially taken from the uci repository and edit by us.thanks a lot for the dataset @uci_repository.
Inspiration
Try to predict the area which is going to be get affected so that the Area can be saved in prior before the fauna and flora species got to ash by the fire."	152	1147	12	balavashan	forest-fire-dataset
8404	8404	cat_pic		[]		8	40	0	just4jcgeorge	cat-pic
8405	8405	jigsaw2021-wat041-data		[]		0	1	0	wataoka	jigsaw2021-wat041-data
8406	8406	NIFTY50 text data	Textual data about companies in the NIFTY 50	['business', 'finance']	"Content
This dataset contains the text contents of the wikipedia article corresponding to each company in the NIFTY 50.
Acknowledgements
This data has been webscrapped from wikipedia."	0	24	0	aakashsaroop	nifty50-text-data
8407	8407	S&P500 text data	Textual data about companies in the S&P 500	['business', 'finance']	"Content
This dataset contains the text contents of the wikipedia article corresponding to each company in the S&P 500. 
Acknowledgements
This data has been webscrapped from wikipedia."	0	15	0	aakashsaroop	sp500-text-data
8408	8408	lending club		[]		3	26	1	xiaohexiao	lending-club
8409	8409	checkpoints_vien_100		[]		0	56	0	vuhoangphuc	checkpoints-vien-100
8410	8410	embedd_layer_100d_		[]		0	18	0	fyp2022	embedd-layer-100d
8411	8411	Mario-PPO-5-4		[]		0	15	0	vinhphmquang	marioppo54
8412	8412	Mario-PPO-5-3		[]		0	13	0	vinhphmquang	marioppo53
8413	8413	Mario-PPO-5-2		[]		0	51	0	vinhphambg	marioppo52
8414	8414	Mario-PPO-5-1		[]		0	33	0	vinhphambg	marioppo51
8415	8415	Mario-PPO-6-4		[]		0	14	0	quangvinhphm	marioppo64
8416	8416	Insider Trading (SEC Form 4) I	Data from SEC form 4, USA equities, Insider Trades 	['law', 'banking', 'economics', 'tabular data', 'investing']	"The data was collected via 'pandas.read_html()' and, it refers to all transactions between 13-12-2021 and 22-12-2021 (DD-MM-YY).
This data set is the first version.
Source: http://openinsider.com/"	142	1732	30	sandorabad	insider-trading-sec-form-4-i
8417	8417	Mario-PPO-6-3		[]		0	24	0	quangvinhphm	marioppo63
8418	8418	intensify		[]		0	0	0	zxykobe	intensify
8419	8419	ebay_train		[]		0	5	0	lovleenkaur	ebay-train
8420	8420	Annotation Correction | V2 duplicate		['software']		0	27	0	light367	annotation-correction-v2-duplicate
8421	8421	Turkey Agricultural Production 2020-2021*	Fruits, Beverages and Spices Crops, Vegetables, Cereals and Other Crops	['agriculture', 'text data']	"The dataset contains agricultural production amount and production year according to TUIK.
Columns:
Product Name
Amount of Production
Production Year"	9	81	0	sezginvural	turkey-agricultural-production-20192020
8422	8422	large_384_lazy_4e5		[]		0	7	0	kingkong153	large-384-lazy-4e5
8423	8423	jigsaw2021-wat046-data		[]		0	3	0	wataoka	jigsaw2021-wat046-data
8424	8424	Seattle Airbnb Data	Seattle Airbnb data taken from insideairbnb Oct 23, 2021	['hotels and accommodations']		5	51	1	kjcodes	seattle-airbnb-data
8425	8425	wqqqqq		[]		0	87	0	quellaq	wqqqqq
8426	8426	code_paai		[]		16	11	0	raphaelbourgade	code-paai
8427	8427	shar2awy-corrected_annotations		[]		0	27	0	eslamomar	shar2awycorrected-annotations
8428	8428	cost-of-wind-2		[]		1	2	0	johnnyt001	costofwind2
8429	8429	pf-138-train-4-data		[]		0	15	0	makotoikeda	pf-138-train-4-data
8430	8430	pf-138-train-3-data		[]		0	4	0	makotoikeda	pf-138-train-3-data
8431	8431	withfollowers		[]		0	11	0	maideyildiz	withfollowers
8432	8432	feedback prize: corrected train csv		[]	"This is my ""corrected"" version of train.csv for the Feedback Prize competition. There were numerous labeling errors, so I came up with a way to improve it.
See this notebook for more detail: https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize
New columns are:
- text_by_index (can ignore)
- new_start (replaces discourse_start)
- new_end (replaces discourse_end)
- text_by_new_index (replaces discourse_text)
- new_predictionstring (replaces predictionstring)"	67	141	4	nbroad	feedback-prize-corrected-train-csv
8433	8433	sat_annotation_correction train.csv		[]		0	12	0	light367	sat-annotation-correction-traincsv
8434	8434	CORONA VIRUS DATA IN BRAZIL (SINCE 2020/02/26)	Data collected on 2021/03/13	['geospatial analysis', 'data visualization', 'covid19']	"Context
The world is currently experiencing a pandemic and scientists from around the world have collaborated in the search for solutions to combat COVID-19. We, as a data scientist, can contribute to the scientific community by providing inputs and insights through data, which are generated at all times.
Content
This dataset has cases of COVID-19 registered in Brazil since February 26"	96	2490	21	gomes555	covid19br
8435	8435	itay_livecell_R101_test_as_train_best_e6		[]		1	12	0	itaynivtau	itay-livecell-r101-test-as-train-best-e6
8436	8436	helpper		[]		1	6	0	chakirachahbounlock	helpper
8437	8437	Spor_Salonu_Rezervasyon		[]		2	26	0	emresanli	spor-salonu-rezervasyon
8438	8438	computed_mfcc		[]		1	35	1	giuseppemagazz	computed-mfcc
8439	8439	Amazon Job Reviews from Indeed.com	Used octoparse to scrape data	['beginner', 'exploratory data analysis', 'data visualization', 'data analytics', 'tabular data']		3	92	2	lgalbavt	amazon-job-reviews-from-indeedcom
8440	8440	NER-images		['arts and entertainment']		0	11	2	thedrcat	nerimages
8441	8441	New Mexico By-County COVID-19 Data Set		[]		3	178	0	benkesselring	new-mexico-bycounty-covid19-data-set
8442	8442	seed_color_nets		[]		0	10	0	youssefelkilany	seed-color-nets
8443	8443	Electronic Music Features — 202101 BeatportTop100	Electronic Music audio features from Beatport’s Top 100 charts from January 2021	['music', 'classification']	"Context
Electronic dance music features a very high number of sometimes overlapping subgenres. Music tech companies attempt to cut through the turmoil of genre labels by providing (and improving) relatively detailed taxonomies that help both humans and algorithms to find and connect to music. Beatport, for example, separated its electronic dance music catalogue into 30+ subgenres as of Jan 2021.
Content
This project is a continuation of the previous efforts of Caparrini, who released two datasets encompassing the audio features of Beatport’s Top 100 tracks from 23 subgenres in 2016 and 29 subgenres in 2018. The present data is retrieved from Beatport's Jan 2021 Top-100 lists, which covered 33 categories. These included DJ Tools, which was not an actual subgenre but a collection of sound samples destined for DJs and producers — therefore, it was excluded from the project. The Electronica category, a loose collection of tracks related to various subgenres, was also omitted. Instead, the dataset incorporates 100 tracks out of the 439 releases labelled as Ambient; these were sampled from the full timespan of the Ambient catalogue (duplicate artists were excluded). Ambient is more defined in terms of intrinsic musical qualities than Electronica, while being listed as a subgenre of the latter on Beatport. Many electronic dance music releases feature ambient tracks or influences, which warranted its inclusion into the dataset. (Note: ambient tracks are usually beatless, and their BPM prediction is often erroneous, while BPM values are the most significant features of the model. This may confuse the prediction; to reduce the importance of this class, while training the model a class weight of 0.5 was applied to Ambient.)
The first 92 columns contain audio features extracted using pyAudioAnalysis and Essentia from two minutes samples of each track provided by Beatport. The resulting 3200-tracks dataset included 17 duplicates. These were replaced with tracks/features extracted from Beatport's Dec 2020 Top-100 lists (top-of-the-chart releases were selected). The last three columns contain Beatport's subgenre category, as well as the artist and track name.
Acknowledgements
Special thanks to Antonio Caparrini, Javier Arroyo, Laura Pérez-Molina and Jaime Sánchez-Hernández, who used the original datasets in their article “Automatic subgenre classiﬁcation in an electronic dance music taxonomy”. Their publication provided inspiration for the article “Dance Librarian: Sonic Explorations of the Bandcamp Underground”."	7	110	1	bvitos	electronic-music-features-202101-beatporttop100
8444	8444	2022 Startups Dataset	250 Highest valued startups	['business']	"Context
Startups are an interesting field to explore and taking a look at the field can expand your knowledge and let you think outside the box
Content
The dataset has 5 columns (Company, Valuation, Valuation_date, Industry, Country)
Company: Describes company name
Valuation: Describes the valuation of the company
Valuation_date: Describes the date of valuation
Industry: Describes the industry of the company
Country: Describes the country of the company
Inspiration
What industry has the most startups ?
What is the total value of these startups ?
How many AI companies are in the top 250 startup companies ?"	360	2723	25	khaiid	startups-by-valuation
8445	8445	pytorch image models master		['art']		0	16	0	hanszhou615	pytorch-image-models-master
8446	8446	timmmaster		[]		0	19	0	hanszhou615	timmmaster
8447	8447	tez lib		[]		0	15	0	hanszhou615	tez-lib
8448	8448	bdd100k_effdet		[]		6	28	0	ngobao	bdd100k-effdet
8449	8449	swear_words_dataset		['video games']	"Context
Two curse word csvs, each containing one curse word per row.
Acknowledgements
Words derived from https://www.noswearing.com/dictionary/ and http://www.bannedwordlist.com/, as suggested by Cursing in English on Twitter."	2	44	0	johnzhangy	swear-words-dataset
8450	8450	MMDet lib ds v2		[]		5	54	0	awsaf49	mmdet-lib-ds-v2
8451	8451	swinexp1oof177		[]		0	12	1	nischaydnk	swinexp1oof177
8452	8452	US Unemployment Data (1948-2021)	US unemployment rate based on age ranges and gender	['government', 'economics', 'beginner', 'intermediate', 'time series analysis']	Contains the monthly unemployment rate from 1948-2021. None of the data is seasonally adjusted. This file also contains information on subsets of the population, including based on age ranges from 16-55 and over, and unemployment rates for men and women. This data is collected by the US Bureau of Labor Statistics.	352	1971	9	axeltorbenson	unemployment-data-19482021
8453	8453	MMDet repo ds		[]		4	43	1	awsaf49	mmdet-repo-ds
8454	8454	IBM_DATA		[]		0	7	0	natsreyes	ibm-data
8455	8455	titanic		[]		2	4	0	huseyinlaco	titanic
8456	8456	TPS-Dec2021-data-reduced	This is the train and test set after preprocessing the data from TPS-Dec2021	['business', 'transportation']		1	32	0	tqrahman	tpsdec2021datareduced
8457	8457	PENERAPAN METODE C4.5		[]		0	36	0	uzee15	penerapan-metode-c45
8458	8458	Historical series of fuel prices in Brazil	Historical Series of Fuel Prices, based on the price survey	['business']		7	68	0	nelsonlara	historical-series-of-fuel-prices-in-brazil
8459	8459	📊 Financial market screener	A start to help making an investment decision 	['business', 'finance', 'investing']	"Context
In this dataset you will find several characteristics on global companies listed on the stock exchange. These characteristics are analyzed by millions of investors before they invest their money. 
Analyze the stock market performance of thousands of companies ! This is the objective of this dataset !
Content
Among thse charateristics you will find : 
The symbol : The stock symbol is a unique series of letters assigned to a security for trading purposes.
The shortname : The name of the company
The sector : The sector of the company (Technology, Financial services, consumer cyclical...)
The country : The location of the head office.
The market capitalisation : Market capitalization refers to the total dollar market value of a company's outstanding shares of stock. It is calculated by multiplying the total number of a company's outstanding shares by the current market price of one share.
The current ratio : The current ratio is a liquidity ratio that measures a company’s ability to pay short-term obligations. A current ratio that is in line with the industry average or slightly higher is generally considered acceptable. A current ratio that is lower than the industry average may indicate a higher risk of distress or default.
The beta : Beta is a measure of a stock's volatility in relation to the overall market. A beta greater than 1.0 suggests that the stock is more volatile than the broader market, and a beta less than 1.0 indicates a stock with lower volatility.
The dividend rate : Represents the ratio of a company's annual dividend compared to its share price. (%)
All this data is public data, obtained from the annual financial reports of these companies. They have been retrieved from the Yahoo Finance API and have been checked beforehand. 
Inspiration
This dataset has been designed so that it is possible to build a recommendation engine. For example, from an existing position in a portfolio, recommend an alternative with similar characteristics (sector, market capitalization, current ratio,...) but more in line with an investor's expectations (may be with less risk or with more dividends etc...)
If you have question about this dataset you can contact me"	41	360	9	pierrelouisdanieau	financial-market-screener
8460	8460	Diabetic Retinopathy		['diabetes']		2	12	0	analaura000	diabetic-retinopathy
8461	8461	million dataset		['earth and nature']		0	14	0	padmini25	million-dataset
8462	8462	Pizza Restaurants	Pizzerias and their Menus	['cooking and recipes', 'food', 'restaurants']	"Content
In this dataset you will find information about pizza restaurants around the world and their menus.
Acknowledgements
This dataset comes form https://data.world/datafiniti/pizza-restaurants-and-pizzas-on-their-menus."	191	1435	12	rishidamarla	pizza-restaurants
8463	8463	GitHub Repo Sample Data	Text code content from BigQuery's GitHub public repo data	['computer science', 'nlp', 'tabular data', 'text data', 'bigquery']	"About
This dataset consists of samples of non binary files, their contents and extensions from BigQuery's GitHub public sample repo data.
File info
This dataset consists of two CSV files:
- filenames_with_ext.csv
  - This CSV lists all filenames with extensions from BigQuery's GitHub public sample repo data. Files with no extensions have been excluded.
- filecontent_with_top_ext.csv
  - This CSV has samples of non binary files, their contents and extensions from BigQuery's GitHub public sample repo data with subject to some constraints.
Data extraction
To understand how this data was extracted and what constraints were used, refer to the following notebook:
GitHub Repo Data - mayur7garg"	8	220	2	mayur7garg	github-repo-sample-data
8464	8464	Credit Cards Approval Dataset		[]		2	38	3	cembkmaz	credit-cards-approval-dataset
8465	8465	Formula 1 World Championship (1950 - 2021)	F1 race data from 1950 to 2021	['auto racing', 'sports']	"Context
Formula 1 (a.k.a. F1 or Formula One) is the highest class of single-seater auto racing sanctioned by the Fédération Internationale de l'Automobile (FIA) and owned by the Formula One Group. The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950. The word ""formula"" in the name refers to the set of rules to which all participants' cars must conform. A Formula One season consists of a series of races, known as Grands Prix, which take place worldwide on purpose-built circuits and on public roads.
Content
The dataset consists of all information on the Formula 1 races, drivers, constructors, qualifying, circuits, lap times, pit stops, championships from 1950 till the latest 2021 season.
Acknowledgements
The data is compiled from http://ergast.com/mrd/
Inspiration
""Races are won at the track. Championships are won at the factory."" - Mercedes (2019)
With the amount of data being captured, analyzed and used to design, build and drive the Formula 1 cars is astounding. It is a global sport being followed by millions of people worldwide and it is very fascinating to see drivers pushing their limit in these vehicles to become the fastest racers in the world!"	19323	103404	702	rohanrao	formula-1-world-championship-1950-2020
8466	8466	alzheimer_point_cloud_voxel		[]		2	45	0	parthoghosh	alzheimer-point-cloud-voxel
8467	8467	pointrend_rcnn_X_101_32x8d_FPN_3x	pretrained model for PointRend's Detectron implementation	['deep learning']		0	27	0	ekaterinasedykh	pointrend-rcnn-x-101-32x8d-fpn-3x
8468	8468	fs lost and found trainids		[]		1	3	0	synboost	fs-lost-and-found-trainids
8469	8469	Movie Lens Dataset		['arts and entertainment']		1	34	3	cembkmaz	movie-lens-dataset
8470	8470	Netflix Plans & Prices 	Netflix Prices Country Wise in US Dollar as well as in local Currency	['beginner', 'tabular data']	"Content
The dataset contains the prices of Netflix's different Plans in different Countries in their Local Currency. For Comparison I have standardized the prices in US Dollar with conversion rate as of Dec 2021.
Acknowledgements
I have scrapped the data from Netflix
Inspiration
I have been keen to understand how companies fluctuate their pricing depending on different countries purchasing power."	14	110	1	apoorvgupta25	netflix-prices
8471	8471	allenai-longformer-large-4086	https://huggingface.co/allenai/longformer-large-4096	[]	"Context
This is a copy of the LongFormer model from allenai in HuggingFace
https://huggingface.co/allenai/longformer-large-4096
Content
Contains the model files for the longformer-large-4096 version of the LongFormer.
For more information about the LongFormer, see the paper:
https://arxiv.org/abs/2004.05150
How to use the model?
https://github.com/allenai/longformer
Acknowledgements
Thanks to AllenAI and HuggingFace for creating and hosting this model publicly."	6	89	3	diegoalejogm	allenailongformerlarge4086
8472	8472	Movie Dataset Rating	Latest Most Rated Movie Dataset- Play with data and have fun 	['arts and entertainment', 'movies and tv shows', 'intermediate', 'data analytics', 'tabular data', 'text data']	"Context
I have collected dataset from the website The Movie database through API and converted into csv file for further use of data.
Content
The dataset consist of movie data in order of top rated movies consisting of columns id, title , release date, overview , popularity, vote_average , vote_count , video columns.  You can analyze this data and can produce valuable outcome out of it.
Acknowledgements
Seeing this community the way people help each other in solving doubts, future knowledge , career path and many thing boosted in me to contribute to the community.
Inspiration
What can be different ways you can use this dataset for. Think and try something new to have fun with data."	1150	7055	30	ayushjain001	movie-dataset-rating
8473	8473	COVID-19 PAKISTAN		[]		0	12	0	itzhaseeb	covid19-pakistan
8474	8474	imdb movie data set		['movies and tv shows']		5	52	1	adisrw	imdb-movie-data-set
8475	8475	first dataset IBM		['earth and nature']		0	10	0	sonujakhar	first-dataset-ibm
8476	8476	vasitani68		[]		0	19	0	vasitani68	vasitani68
8477	8477	shandong_mat		[]		0	2	0	wjfwjf	shandong-mat
8478	8478	images		[]		0	158	0	yukanas	images
8479	8479	Exploratory data analysis||EDA Car dataset || 		[]		0	37	1	sulimanhaqqyarjan	exploratory-data-analysiseda-car-dataset
8480	8480	EEGMODEL		[]		0	19	0	matisfeller	eegmodel
8481	8481	Ad Tracking Fraud Detection	"Copy of toy ""Ad Tracking Fraud Detection"" dataset"	[]		4	55	0	ambisinistra	ad-tracking-fraud-detection
8482	8482	yolo_weights		[]		0	5	0	prometheus123	yolo-weights
8483	8483	Training for MobileNet v2 1.0		[]		0	4	0	lienchibaob1812254	training-for-mobilenet-v2-10
8484	8484	Soccer detailed players match data	Detailed statistics by players for soccer matchs	['football', 'exploratory data analysis', 'clustering', 'feature engineering', 'multiclass classification']	"Context
Big fan of football, I wanted to approach this sport with the data to be able to predict the results and to do data mining. I share some of the data I collected in my quest.
Content
This dataset contains two files with the same structure.
The first contains the details of the match statistics for each player of the home team.
The second contains the same information for away teams
Each row  of these files represent the detailed statitics for one player for the given match
Inspiration
matches outcome 
football scouting"	41	293	1	spicemix	soccer-detailed-players-match-data
8485	8485	RapidKL Bus Data - H2 2021	RapidKL historical bus movements data from 1 July 2021 - 30 November 2021.	['cities and urban areas', 'transportation']	"Content
In general, the data is separated via folders specifying months, and files specifying the days of the months. Within each file are line-delimited JSON entries describing each data point. 
| Attribute | Description |
| --- | --- |
| dt_received | When is the data received by the RapidKL backend system. |
| dt_gps | The datetime of the GPS transceiver on the bus itself. |
| latitude | Latitude position of bus. |
| longitude | Longitude position of bus. |
| dir | [UNSURE] Which direction of the route the bus is currently driving on. | 
| speed | Speed of the bus. |
| angle | Which angle (compass angle) the bus is facing on. |
| route | Route the bus is on. |
| bus_no | Bus number plate. |
| trip_no | Unique identifier of the particular bus trip. |
| busstop_id | Identifier of the bus stop the bus is most closest to. |
| captain_id | Id of the bus captain that is driving the trip. |
| trip_rev_kind | An identifier on what type of trip the bus is on. |
| engine_status | Bus engine status. |
| isOKU | If bus contains facilities for disabled people. |
If you would like to download the compressed data, or files that are already grouped by months, you may do so here (compression rate: 4%):
https://drive.google.com/drive/folders/1A1s55BxwLoOCZPqXZ9_jdJiU-oT96J7G?usp=sharing"	2	22	0	kwongtn	rapidkl-bus-data-h2-2021
8486	8486	kaerururu-petfinder2-085		[]		0	7	0	kaerunantoka	kaerururu-petfinder2-085
8487	8487	Hangikredi Hiring Challange		['jobs and career']		2	45	3	cembkmaz	hangikredi-hiring-challange
8488	8488	model2		[]		0	3	0	rahulharod	model2
8489	8489	TMDB dataset for Text_preprocessing in NLP		[]		0	21	0	sugunapriya	tmdb-dataset-for-text-preprocessing-in-nlp
8490	8490	model1		[]		0	19	0	rahulharod	model1
8491	8491	patient name		[]		4	29	0	momankhan	patient-name
8492	8492	1920_2nd_pth		[]		1	24	0	dragonzhang	1920-2nd-pth
8493	8493	Cars Dataset		['automobiles and vehicles']		12	154	2	furqanamjad	cars-dataset
8494	8494	weights		['exercise']		0	21	0	rahulharod	weights
8495	8495	Rice_Stone	Rice stone Dataset with various Format 	[]		1	59	13	balasubramaniamv	rice-stone
8496	8496	Pretrained_model		[]		0	12	0	rahulharod	pretrained-model
8497	8497	gjr_vol		[]		3	48	0	konmue	gjr-vol
8498	8498	Chittagong Stock Exchange dataset		[]		0	13	0	tausiffardin	chittagong-stock-exchange-dataset
8499	8499	RapidKL Bus Data - H1 2021 (March - June)	RapidKL historical bus movements data from 27 March 2021 - 30 June 2021.	['cities and urban areas', 'transportation']	"Content
In general, the data is separated via folders specifying months, and files specifying the days of the months. Within each file are line-delimited JSON entries describing each data point. 
| Attribute | Description |
| --- | --- |
| dt_received | When is the data received by the RapidKL backend system. |
| dt_gps | The datetime of the GPS transceiver on the bus itself. |
| latitude | Latitude position of bus. |
| longitude | Longitude position of bus. |
| dir | [UNSURE] Which direction of the route the bus is currently driving on. | 
| speed | Speed of the bus. |
| angle | Which angle (compass angle) the bus is facing on. |
| route | Route the bus is on. |
| bus_no | Bus number plate. |
| trip_no | Unique identifier of the particular bus trip. |
| busstop_id | Identifier of the bus stop the bus is most closest to. |
| captain_id | Id of the bus captain that is driving the trip. |
| trip_rev_kind | An identifier on what type of trip the bus is on. |
| engine_status | Bus engine status. |
| isOKU | If bus contains facilities for disabled people. |
If you would like to download the compressed data, or files that are already grouped by months, you may do so here (compression rate: 4%):
https://drive.google.com/drive/folders/1A1s55BxwLoOCZPqXZ9_jdJiU-oT96J7G?usp=sharing"	1	39	0	kwongtn	rapidkl-bus-data-h1-2021-march-june
8500	8500	Dhaka stock exchange dataset		['investing']		0	4	0	tausiffardin	dhaka-stock-exchange-dataset
8501	8501	PlayGolf_NaiveBayes		[]		0	18	3	mmellinger66	playgolf-naivebayes
8502	8502	jigsaw2021-wat045-data		[]		0	2	0	wataoka	jigsaw2021-wat045-data
8503	8503	Fileas		[]		0	6	0	fastbrick	fileas
8504	8504	vgg_yeni		[]		8	25	0	ranabedir	vgg-yeni
8505	8505	xxxx12131		[]		0	12	0	helloggfss	xxxx12131
8506	8506	No wilds		[]		0	54	0	ianianh	no-wilds
8507	8507	Phishing and Benign Websites	An annotated dataset of 38,800 phishing and benign websites.	['categorical data', 'computer science', 'classification']	"Context
Phishing is a cybercrime in which deceitful websites lure naive users and trick them into disclosing confidential information, such as social media passwords or financial data. This phishing dataset can be used for training supervised or semi-supervised phishing detection models.
Content
The dataset contains 38,800 URLs that have been classified as either phishing or benign.
Citation
&gt; Mowar, Peya, & Jain, Mini. (2021, December 28). Phishing and Benign Websites Dataset. 2021 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA) (CyberSA), Dublin, Ireland. https://doi.org/10.5281/zenodo.5807622"	25	295	1	peyamowar	phishing-and-benign-websites
8508	8508	NEU-DET_coco		[]		6	79	0	nolaurence	neudet-coco
8509	8509	pet2-tfrecords2-384-007		[]		0	5	0	bamps53	pet2-tfrecords2-384-007
8510	8510	Product Listing Dataset Ebay	This dataset includes product listing data from Ebay	['retail and shopping']	"Context
This dataset was created by our in-house Web Scraping and Data Mining teams at PromptCloud and DataStock. You can download the full dataset here. This sample contains 30K records. You can download the full dataset here
Content
Total Records Count : 284157  Domain Name : ebay.com  Date Range : 01st Apr 2021 - 30th Apr 2021   File Extension : csv
Available Fields : Uniq Id, Crawl Timestamp, Pageurl, Website, Title, Num Of Reviews, Average Rating, Number Of Ratings, Model Num, Sku, Upc, Manufacturer, Model Name, Price, Monthly Price, Stock, Carrier, Color Category, Internal Memory, Screen Size, Specifications, Five Star, Four Star, Three Star, Two Star, One Star, Discontinued, Broken Link, Seller Rating, Seller Num Of Reviews, Used   
Acknowledgements
We wouldn't be here without the help of our in house web scraping and data mining teams at PromptCloud and DataStock.
Inspiration
This dataset was created keeping in mind our data scientists and researchers across the world."	17	183	2	promptcloud	product-listing-dataset-ebay
8511	8511	biodesign_wing_layout		[]		1	11	0	zmjjiang	biodesign-wing-layout
8512	8512	dataset uji hasi ujian		['earth and nature']		6	48	0	mochdwifebrianto	dataset-uji-hasi-ujian
8513	8513	pretrained models		['clothing and accessories']		1	33	0	mohammedredaomramn	pretrained-models
8514	8514	biodesign_3		[]		1	11	0	zmjjiang	biodesign-3
8515	8515	more_data		[]		4	36	0	aronbryant	more-data
8516	8516	trained-dm-hw2		[]		2	10	0	silviayen	traineddmhw2
8517	8517	handpose		[]		0	7	0	evren07	handpose
8518	8518	dataset hasil ujian		[]		0	36	0	mochdwifebrianto	dataset-hasil-ujian
8519	8519	test_data		[]		1	24	0	yunqidu	test-data
8520	8520	biodesign_2		[]		3	34	0	zmjjiang	biodesign-2
8521	8521	American Airlines Group Stock Data	American Airlines Group Inc. (AAL) | NasdaqGS | Currency in USD	['business', 'finance', 'time series analysis', 'investing', 'datetime']	"What is American Airlines Group?
American Airlines Group Inc. is an American publicly traded airline holding company headquartered in Fort Worth, Texas. It was formed on December 9, 2013, by the merger of AMR Corporation, the parent company of American Airlines, and US Airways Group, the parent company of US Airways. Integration was completed when the Federal Aviation Administration granted a single operating certificate for both carriers on April 8, 2015, and all flights now operate under the American Airlines brand. The group operates the largest airline in the world, as measured by number of passengers carried, by fleet size and by scheduled passenger-kilometers flown. The company ranked No. 70 in the Fortune 500 list of the largest United States corporations based on its 2019 revenue, but, impacted by the COVID-19 pandemic, it lost $2.2 billion in the first quarter of 2020 alone and accepted government aid. American Airlines is reported to be shrinking its passenger fleet.
Information about this dataset
This dataset provides historical data of American Airlines Group Inc. (AAL). The data is available at a daily level. Currency is USD."	516	3421	30	varpit94	american-airlines-group-stock-data
8522	8522	long_v14		[]		7	69	3	sytuannguyen	long-v14
8523	8523	denoise dataset		['earth and nature']		0	13	0	chenlanglang	denoise-dataset
8524	8524	ShortSqueeze Stock Data	A mini stock dataset of popular tickers in subreddit ShortSqueeze.	['business', 'finance', 'time series analysis']	"Mini ShortSqueeze Stock Data in 5 Minute Intervals
Most 8 mentioned tickers in subreddit /r ShortSqueeze in past week (22nd Nov - 29th Nov 2021) Open, High, Low, Close, Volume data for a 1-month timeframe.
Acknowledgements
Data collected with AlphaVantage."	83	983	13	calven22	shortsqueeze-stock-data
8525	8525	LAB4 assignment		[]		0	23	0	carolynenw	lab4-assignment
8526	8526	LEC Regular Season 2021 / LOL	European League of Legends Competition	['video games', 'exploratory data analysis', 'data visualization']	"Context
League of Legends is one of the world's most famous video games. It's played by over 100 million active users every month. League of Legends is perhaps the most prominent Esport game.
Nowadays, their competitions are becoming more and more professional and little by little new information about the players is appearing.
Content
This dataset includes the statistics of all LEC (League of Legends European Championship) players for the 2021 regular season.
The same variables are found in all 3 datasets:
Role: The position of the player
Name: Name of the player
Games: Number of games played
Wins: Games won
Loses: Matches lost
WR: Win rate (Wins/Games)
K: Average kills per game
D: Average death per game
A: Average assists per game
KDA: Kills+Assists/Deaths
CS: Average minions killed per game
CS/M: Average minions killed per minute
G: Average gold per game (per thousand)
G/M: Gold per minute
KPAR: Kill Participation (Kills+Assist / Team Kills)
KS: Kills Share(Kills / Team Kills)
GS: Team Gold %
CP: Champs Played
If you need any further information,  you can contact me in the Discussion section or on Twitter (https://twitter.com/jordipg05)."	75	532	5	jordipompas	lec-regular-season-2021
8527	8527	Insurance Claims Fraud Data 	Fraud Detection on Insurance	['insurance']	"Context
This data set contains Insurance Claim data for Fraud Detection.
Content
It consist of 3 dataset
1. Employee Data  - this the master data of the employee ( a.k.a agents or adjusters ) working on the insurance claims
2. Vendor Data - this is the master data of the vendor who assist insurance company in investigating the claims
3. Claims Data - this is the claim level transaction details submitted by customer to the insurance company for re-imbursement
Inspiration
We can use this data to solve following use case:
- Claim Level Fraud Detection. 
- Employee Fraud Detection
- Employee Vendor Collusion"	223	967	5	mastmustu	insurance-claims-fraud-data
8528	8528	aesrfwsdfsdf		[]		0	19	0	csepython	aesrfwsdfsdf
8529	8529	ceshimsra		[]		0	3	0	gjy123123	ceshimsra
8530	8530	unknown for known icnet		['arts and entertainment']		2	51	0	shashwatnaidu	unknown-for-known-icnet
8531	8531	Health-Response-Score Datasets	Health Data set with lots 4 csv files. Sponsor, Responses, Score, and Users.	['healthcare', 'public health', 'health', 'health conditions', 'sql']	"Health - Care
Health is very first base of every creature. It doesn't matter if it is human, Animal, Trees or crops. But in todays hectic life we have forgotten about this basic thing. 
Here is the dataset of various employees and their response + score and question that we asked.
This Dataset is regarding the Data gathered from various employees and there daily routine + their body checkups + other related information.
Dataset -
hra_qna_scores.csv : This csv file contains Question Id and related Score. 
hra_responses.csv : This csv file contains response of the user, when response is stored and how related question Id and user Id. 
users_data.csv : Here This csv file contains user data and when the user account is created.
sponsor_data.csv : This csv file is contains the sponsors.
Acknowledgements
A Very Big Thank to all the users that took time to fill all the details. 
Inspiration
EDA.
Prediction of different disease for users.
Creativity of the data scientist is welcomed.
Al the best. 
Happy learning!"	75	679	2	kartikgo	healthresponsescore-datasets
8532	8532	jigsaw2021-wat043-data		[]		0	3	0	wataoka	jigsaw2021-wat043-data
8533	8533	NFL GAN Data		[]		0	24	0	lukegeorge	nfl-gan-data
8534	8534	lost and found icnet seg output		[]		3	8	0	synboost	lost-and-found-icnet-seg-output
8535	8535	ChurnData.csv		['internet']		6	17	0	dinaouahbi	churndatacsv
8536	8536	Bangladesh Covid-19 dataset		[]		2	42	0	nniisshhaann	bangladesh-covid19-dataset
8537	8537	Cricket Scores Analysis Using K-Means Algorithms		['cricket']		6	50	0	greencounter31	cricket-scores-analysis-using-kmeans-algorithms
8538	8538	lendingclub		[]		1	56	0	hanidaaadzkia	lendingclub
8539	8539	Automobile Mileage Prediction	Mileage prediction for ML beginners. Follows a standard ML pipeline approach.	['automobiles and vehicles', 'beginner', 'exploratory data analysis', 'data visualization', 'linear regression']	"Auto MPG Data Set
Note: (This data set is available in UCI at https://archive.ics.uci.edu/ml/datasets/auto+mpg An excerpt of the problem statement is reproduced here for convenience.)
Abstract: Revised from CMU StatLib library, data concerns city-cycle fuel consumption
Data Set Information:
This dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute ""mpg"", 8 of the original instances were removed because they had unknown values for the ""mpg"" attribute. The original dataset is available in the file ""auto-mpg.data-original"".
""The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes."" (Quinlan, 1993)
Attribute Information:
mpg: continuous (target variable)
cylinders: multi-valued discrete
displacement: continuous
horsepower: continuous
weight: continuous
acceleration: continuous
model year: multi-valued discrete
origin: multi-valued discrete
car name: string (unique for each instance)
Data explorer has a brief description of each field."	86	592	2	saigeethac	autompg
8540	8540	Reddit Comment	Analyzing Comment on Reddit about the Vaccine	['healthcare', 'data analytics', 'text data']		0	41	0	nmd1011	reddit-comment-about-vaccine
8541	8541	Sartorius-stage2-Z423-20211228093446		[]		0	25	0	hideyukizushi	sartorius-stage2-z423-20211228093446
8542	8542	IS THAT SANTA? (Image Classification)	Santa Claus Classification	['religion and belief systems', 'classification', 'deep learning', 'image data', 'binary classification', 'holidays and cultural events']	"Context
Ho, Ho, Ho It's Christmas time so why not have a Christmas themed dataset to practice image classification.
Content
This is a binary classification image dataset where you can have to train a deep learning model which can predict if an image contains Santa Claus or not.
Inspiration
This was completely my idea, but upon googling it was found that pyimagesearch has a similar private dataset. However this data set is completely built and compiled by me :)"	102	2234	23	deepcontractor	is-that-santa-image-classification
8543	8543	DSE and CSE datasets	DSE and CSE Dataset for Stock Prediction	['computer science', 'tabular data']		3	60	1	rafiaaaa	dse-and-cse-datasets
8544	8544	Bangalore house rent details (1BHK, 2BHK, 3BHK)	Rental prices for various categories of house in Bangalore (1000 rows)	['india', 'housing', 'beginner', 'classification', 'text data']		2	54	0	csunnikrishnan	bangalore-house-rent-details-1bhk-2bhk-3bhk
8545	8545	coughmodel		[]		0	15	0	v1olet	coughmodel
8546	8546	sartorius-unet		[]		1	81	0	puthipongworasaran	sartoriusunet
8547	8547	effv2m		[]		0	3	0	v1olet	effv2m
8548	8548	model-VGG		['clothing and accessories']		2	11	0	lienchibaob1812254	modelvgg
8549	8549	project		[]		2	32	0	hongthi	project
8550	8550	coughfolder		[]		0	5	0	v1olet	coughfolder
8551	8551	Video Neural Style Transfer		[]		1	11	0	inclusivebusinesslab	video-neural-style-transfer
8552	8552	Car Details		[]		10	89	7	sana306	car-details
8553	8553	Optical Character Recognition Dataset (OCR)	Handwriting dataset in Russian	[]		2	14	0	tamirpuzanov	nto-task2-dataset
8554	8554	California Tobacco Retail Surveillance		['retail and shopping']		2	49	9	sana306	california-tobacco-retail-surveillance
8555	8555	Segmented-Data-Retrieval-1		['earth and nature']		0	208	0	ayanvishwakarma12	segmenteddataretrieval1
8556	8556	kaerururu-petfinder2-084		[]		0	5	0	kaerunantoka	kaerururu-petfinder2-084
8557	8557	Blog 10		['online communities']		0	4	0	benalexx	blog-10
8558	8558	Blog 9		['online communities']		0	16	0	benalexx	blog-9
8559	8559	Blog 8		['online communities']		0	3	0	benalexx	blog-8
8560	8560	Blog 7		['online communities']		0	18	0	benalexx	blog-7
8561	8561	SwinT_Base_384		[]		0	26	0	peixiangong	swint-base-384
8562	8562	Blog 6		[]		0	25	0	benalexx	blog-6
8563	8563	Handwritten Image 	Handwritten Image with size 28*28	[]		0	70	8	sana306	handwritten-image
8564	8564	neuralStyleTransfer		[]		0	73	0	devanshusingh	neuralstyletransfer
8565	8565	tfidf_submission		[]		2	8	0	panser	tfidf-submission
8566	8566	World Countries (Locations)	List of Countries with respective Geolocations	[]		5	34	7	methoomirza	world-countries-locations
8567	8567	exp_i1_colab_R101_maxinst350_best_e12		[]		0	9	0	itaynivtau	exp-i1-colab-r101-maxinst350-best-e12
8568	8568	my_model3		[]		0	11	0	senhaoan	my-model3
8569	8569	tadpole data		['business']		0	9	0	pserddl	tadpole-data
8570	8570	Sartorius-stage1-Z426-20211228072004		[]		0	21	0	hideyukizushi	sartorius-stage1-z426-20211228072004
8571	8571	Blog 5		['online communities']		0	22	0	benalexx	blog-5
8572	8572	Blog 4		['online communities']		0	17	0	benalexx	blog-4
8573	8573	Blog 3		['online communities']		0	4	0	benalexx	blog-3
8574	8574	data/news		['news']		2	25	0	soarfeng	datanews
8575	8575	Blog 2		['online communities']		0	4	0	benalexx	blog-2
8576	8576	Blog 1		['online communities']		0	7	0	benalexx	blog-1
8577	8577	Melanoma_Dataset		[]		0	16	0	udaykrgupta	melanoma-dataset
8578	8578	swin-T-384-22k		[]		0	16	0	peixiangong	swint38422k
8579	8579	bbc-text		[]		0	15	0	tuozhenliu	bbctext
8580	8580	deteco neuro only1		[]		0	17	0	cellsegmentation	deteco-neuro-only1
8581	8581	timtim		[]		0	7	0	kugaichen	timtim
8582	8582	TripletLoss		['arts and entertainment']	"Content
The dataset contains the RGB face images (128x128x3) for performing triplet loss.
train_triplet.csv - [ face1: content,   face2: positive,   face3: negative ]
test.csv - [ if face1 and face2 are similar then label is 1, otherwise 0 ]"	0	34	1	devanshusingh	tripletloss
8583	8583	mmdetection_new		[]		1	9	0	kugaichen	mmdetection-new
8584	8584	Titanic_newds		[]		1	25	0	phani3344	titanic-newds
8585	8585	pf-138-train-2-data		[]		0	4	0	makotoikeda	pf-138-train-2-data
8586	8586	How Does a Bike-Share Navigate Speedy Success?		[]	"Quarterly Cyclistic trip data
In 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that
are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and
returned to any other station in the system anytime.
Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments.
One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes,
and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers
who purchase annual memberships are Cyclistic members.
Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the
pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will
be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a
very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic
program and have chosen Cyclistic for their mobility needs
the data collected from Google Analytics Professional Certificate course for the case study. this data is collected quarterly in one year. data focuses for year 2019
Thanks to the Google Analytics Professional Certificate course that allows me to do the analysis of this data and find the data-driven decisions and identify the trends
to analyze this data, i will follow the data analysis process which are: ASK, PREPARE, PROCESS, ANALYZE, SHARE and ACT."	1	38	0	victorsalim	how-does-a-bikeshare-navigate-speedy-success
8587	8587	include		[]		0	4	0	mohitsawwalakhe	include
8588	8588	Student Learning Preferences 		['education', 'beginner', 'intermediate', 'advanced', 'data analytics']	"The dataset represent the student learning style bad on the VARK Model .
The dataset has been collect as a sample  of 12011 student to determine their learning style.
There are 5 questions for each learning category in the dataset, starts with the third column.  The questions are grouped together based on V or A or K which can be seen in the last column.
The scoring for the questions' based o the Likert scale, Strongly agree=5 until Strongly disagree=1."	32	254	1	ebouearmand59	student-learning-preferences
8589	8589	cots_yolov4_final	final weight for cots trained on darknet yolov4	[]		0	27	1	iamprateek	cots-yolov4-final
8590	8590	Biden Approval Polling	Publicly available polling on Joe Biden.	['politics']	"Context
There is a contract on Predictit.org tracking Biden's approval rating that I've followed out of fun (no profit).   I often used this data to help predict where it will go next.  For example, Rasmussen is a very fresh pollster (daily), while the others are somewhat lagging.   You can even subscribe to Rasmussen's service and get updates before most people, though this particular exploit is well known.
Content
The data is fairly straightforward and I include a brief description of each of the columns. 
Something to be aware: a few pollsters, such as Ipsos, will report wildly different results within days because they are sometimes polling for specific organizations, such as the Economist and sometimes just for themselves.  In these different surveys, there are different questions used, and thus different 'house effects' (ie: political bias).   For example, some surveys start with the question ""Do you approve of the direction of the country?"", while others will start with ""Do you approve of Joe Biden?""
Acknowledgements
I would like to acknowledge Nate Silver and the whole 538 crew for aggregating this data.  Very interesting and informative - https://projects.fivethirtyeight.com/biden-approval-rating/
Please note I have removed all 538 model specific information such as weights, grades, etc.
Inspiration
I think it'd be very cool to see how far ahead we could predict changes in Biden's approval ratings, possibly using other sources such as twitter and news organizations, plus maybe other datasets on Kaggle itself."	9	135	2	kaggleqrdl	biden-approval-polling
8591	8591	E-commerce brand score prediction		['finance']	"Context
I use this data to do the prediction, as you can see, there is a ""品牌库3.0等级"" which means the rank of this brand. And I used other features to predict their ranks."	4	100	0	hanshunfan	ecommerce-brand-score-prediction
8592	8592	penglong3		[]		0	18	0	tonychen52	penglong3
8593	8593	petfinder-final-lr-170275		[]		0	5	0	tonyxu	petfinder-final-lr-170275
8594	8594	Content Image For Neural Style Transfer		[]		0	11	0	inclusivebusinesslab	content-image-for-neural-style-transfer
8595	8595	raw_street_view		[]		0	4	0	andreimuresanu	raw-street-view
8596	8596	diabetes dataset		[]		3	30	0	mochdwifebrianto	diabetes-dataset
8597	8597	Fine tune with uncond trained GPT model		['business']		0	14	0	jiaminggogogo	fine-tune-with-uncond-trained-gpt-model
8598	8598	wxm-200		[]		30	395	0	yigexineren	wxm-200-full
8599	8599	kaggler_za_user_e003_r1_0		[]		0	53	0	resistance0108	kaggler-za-user-e003-r1-0
8600	8600	dataset test ujian		[]		5	22	0	mochdwifebrianto	dataset-test-ujian
8601	8601	autocorrect		['email and messaging']		4	24	2	kaggleqrdl	autocorrect
8602	8602	data set testing ujian		[]		0	21	0	mochdwifebrianto	data-set-testing-ujian
8603	8603	BaseTortugas		[]		0	36	0	miguelespinozac	basetortugas
8604	8604	biodesign		[]		2	4	0	zmjjiang	biodesign
8605	8605	petfinder-baseline-swt-176180-svr-all		[]		0	4	0	tonyxu	petfinder-baseline-swt-176180-svr-all
8606	8606	pth_mot20		[]		0	19	0	tonyjagger	pth-mot20
8607	8607	GEN_TEST_SADDLE_3		[]		1	6	0	zmjjiang	gen-test-saddle-3
8608	8608	Quantium retail data		[]		2	18	0	mukeshkumar95	quantium-retail-data
8609	8609	Wind Energy in Germany	Temperature and wind energy production 2017-2019	['energy', 'renewable energy', 'time series analysis']	"Context
The goal of this dataset is to forecast wind power generation at a daily rate using different time series and traditional machine learning models.
Content
This dataset is pretty simple, It's a time series dataset containing measurements of daily temperature, wind production and capacity from 2017 to 2019.
The columns in the dataset are : 
* utc_timestamp : Time in UTC 
* wind_generation : Daily wind production in MW
* wind_capacity : Electrical capacity of wind in MW
* temperature : Daily Temperature in degrees C
Acknowledgements
I would like to thank OPSD for making this dataset available publicly.
https://open-power-system-data.org/"	67	684	7	aymanlafaz	wind-energy-germany
8610	8610	Cognitive modeling of complex systems	Examples of weights and descriptions of cognitive maps	['earth and nature', 'exploratory data analysis', 'time series analysis', 'data analytics', 'feature engineering']	"Context
The dataset contains examples of weights and descriptions of cognitive maps of complex systems for cognitive modeling in various fields
Content
Each cognitive map is described by two files - one contains the weights of its edges, and the other (with a similar name) - the names of vertices and their type (input, target, or state variable) (default, all can be input or target in turn).
Acknowledgements
Examples of cognitive modeling techniques are described in my articles (with co-authors): 
- https://visnyk.vntu.edu.ua/index.php/visnyk/article/view/2238/2195 (from there, a picture was used for the dataset logo)
- https://ieeexplore.ieee.org/document/8100371
Thanks to the articles I refer to in my articles.
Thanks for the article, where there is a simple and clear example - I use it in my baseline notebook.
Inspiration
I invite you to develop notebooks that allow cognitive modeling of complex systems (playing scenarios for their development, optimization of modes, etc.) using graph theory."	21	422	9	vbmokin	cognitive-modeling-of-complex-systems
8611	8611	211229_feedback_train		[]		0	19	0	sskkaz	211229-feedback-train
8612	8612	Road Cycling Performance Testing	Power output, blood lactate, and heart rate data from maximal performance tests	['sports', 'data visualization', 'data analytics']	"Context
According to the research authors, this data was collected from triathletes and cylists who were recruited in Canberra, Australia from December 2015 to March 2016.
Content
This dataset contains cycling performance data from Woods et al., ""The effects of intensified training on resting metabolic rate (RMR), body composition and performance in trained cyclists"" (2018). All power output data was recorded from a calibrated stationary cycling ergometer (Wattbike Pro).
Acknowledgements
Credit for this database is entirely attributed to the authors of the research:
Amy L. Woods, Anthony J. Rice, Laura A. Garvican-Lewis, Alice M. Wallett, Bronwen Lundy, Margot A. Rogers, Marijke Welvaert, Shona Halson, Andrew McKune, Kevin G. Thompson
Inspiration
Consider the following:
1. What is the reliability of the performance measures? 
2. What is the relationship between performance measures?
3. What is the best way to report and visualise the data to an athlete or coach?"	18	147	0	redlineracer	roady-cycling-performance-testing
8613	8613	life_expectacy.csv		['internet']		0	12	0	ibangfik	life-expectacycsv
8614	8614	Harapan kehidupan		[]		0	19	0	ibangfik	harapan-kehidupan
8615	8615	life expect		[]		1	30	0	ibangfik	life-expect
8616	8616	Chicago Bike Sharing Dec20 Nov21	Bike-Sharing Ridership in Chicago from December 2020 to November 2021	['united states', 'weather and climate', 'transportation', 'beginner', 'data analytics']	"Context
The City of Chicago has bike sharing available for its residents and tourist via a service provider called Divvy. Divvy tracks data for all their trips, including geolocation, user types, bike types, dates, and times. These data is available to the public on Divvy's website under their license agreement. Similarly, I obtained Chicago's weather data from the U.S. National Weather Service's online database. Using these datasets, I analyzed ridership trends and consumer behavior in segments of the population, and looked for correlations between ridership and temperature.
Content
There are two datasets: ""Bike Share Data"" and ""Chicago Temperature"". The Bike Share Data dataset contains all bike rides from December 2020 to November 2021.  It has a &gt;5M rows and 13 columns: ride_id (char), rideable type (factor), start and end time (datetime), start and end station name (char), start and end station id (int), start and end lat and long (geo), and usertype (factor). The Chicago Temperature dataset contains the average daily temperature for the same time period. It is in wide form, with days of the month as the rows and months as the column.
References
Divvy. (2021). Divvy System Data [Dec 2020 - Nov 2021]. Retrieved from https://ride.divvybikes.com/system-data
United States. National Weather Service. (2021). NOAA Online Weather Data: Calendar Day Summaries [Chicago Area, 2020-2021]. Retrieved from https://www.weather.gov/wrh/Climate?wfo=lot
Llorente, D. (2017). Bicycle Tires on La Vuelta [Photograph]. Retrieved from https://unsplash.com/photos/K7yTmKG7kac"	7	56	1	ejpascualj	chicago-bike-sharing-dec20-nov21
8617	8617	O2Ojiqixuexi		[]		0	36	0	sukasu577	o2ojiqixuexi
8618	8618	Final-Model		[]		0	17	0	ehabibrahim758	finalmodel
8619	8619	Kazakhstan IT vacancies 2020-2021		['employment', 'income', 'jobs and career']	"Context
Dataset has above 200 vacancies in Kazakhstan and these vacancies only related to IT.
Content
Position: type of position
Company: company name
Salary: how much is salary
City: where is company located
Experience: experience year
Employment: type of employment
Key skills: main skills are required for vacancy
Date of publication: when data was publicated
Inspiration
With this dataset, I wanted to get answers for questions?
How much big salary of IT workers in KZ?
How experience influences on salary?
In which cities besides Almaty and Nur-Sultan there are IT jobs?"	5	56	2	olzhassamatov	kazakhstan-it-vacancies-20202021
8620	8620	stockdi		[]		1	22	0	danielstuartlavi	stockdi
8621	8621	Churn dataset with intl call		[]		4	39	0	davidsonsteele	churn-dataset-with-intl-call
8622	8622	Paris Saint Germain (PSG) YouTube Channel Videos	video_ids, views , likes, comments and descriptions of PSG YouTube Channel Video	['arts and entertainment', 'football', 'tabular data']	"Youtube Data - PSG FC
PSG - Paris Saint-Germain Football Club is currently one of the biggest club in Europea. In 2021, the club acquired big players (Messi, Ramos, Donnarumma, Wijnaldum and Hakimi) to join their already in-depth team. The presence of these players has had an influence on their social media account. 
This data shows the current number of videos in their YouTube channel and various statistics attributed to the video using  YouTube's official API. The data was last updated on 27th, December 2021"	1	38	0	warrienelly	paris-saint-germain-psg-youtube-channel-videos
8623	8623	Africa Energy Resource Database		['energy']		6	58	1	oreowolabi	africa-energy-resource-database
8624	8624	titanic		[]		0	3	0	muzafferhasimgezgin	titanic
8625	8625	generation-of-solar		['renewable energy']		8	16	0	johnnyt001	generationofsolar
8626	8626	Crowd-sourced Fitbit datasets	Crowd-sourced Fitbit datasets 12/03/2016 to 11/04/2016 only	['health and fitness', 'exercise']	"These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016.  This dataset covers the period from 12/03/2016 to 11/04/2016 only, the second part of this dataset is uploaded by Möbius 
Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. Individual reports can be parsed by export session ID (column A) or timestamp (column B).  Variation between output represents the use of different types of Fitbit trackers and individual tracking behaviours/preferences.
This dataset was originally uploaded by Furberg, Robert; Brinton, Julia; Keating, Michael ; Ortiz, Alexa, on 31/05/2016  and it contains two folders:
mturkfitbit_export_4.12.16-5.12.16.zip
mturkfitbit_export_3.12.16-4.11.16.zip
This dataset is the uploaded contents of the second folder (mturkfitbit_export_3.12.16-4.11.16.zip)"	4	55	1	salihobaid	fitbit-datasets-0312201605122016
8627	8627	cost-of-solar		['renewable energy']		10	15	0	johnnyt001	costofsolar
8628	8628	cost-of-wind		['energy']		8	12	0	johnnyt001	costofwind
8629	8629	generation-of-wind		[]		16	10	0	johnnyt001	generationofwind
8630	8630	[CIC-AndMal-2020] Static-Dynamic Malware analysis	400K android apps with 14 prominent malware categories and 191 malware families.	['arts and entertainment', 'data analytics', 'classification', 'tabular data', 'multiclass classification']	"Introduction
This dataset contains 200K android malware apps which are labeled and characterized into corresponding family. Benign android apps (200K) are collected from Androzoo dataset to balance the huge dataset. We collected 14 malware categories including adware, backdoor, file infector, no category, Potentially Unwanted Apps (PUA), ransomware, riskware, scareware, trojan, trojan-banker, trojan-dropper, trojan-sms,trojan-spy and zero-day.
A complete taxonomy of all the malware families of captured malware apps is created by dividing them into 8 categories such as sensitive data collection, media, hardware, actions/activities, internet connection, C&C, antivirus and storage & settings. 
Dataset details
<table>
<tbody>
<tr>
<th>Category</th>
<th>Number of families</th>
<th>Number of samples</th>
</tr>
<tr>
<td>Adware</td>
<td>48</td>
<td>47,210</td>
</tr>
<tr>
<td>Backdoor</td>
<td>11</td>
<td>1,538</td>
</tr>
<tr>
<td>File Infector</td>
<td>5</td>
<td>669</td>
</tr>
<tr>
<td>No Category</td>
<td>-</td>
<td>2,296</td>
</tr>
<tr>
<td>PUA</td>
<td>8</td>
<td>2,051</td>
</tr>
<tr>
<td>Ransomware</td>
<td>8</td>
<td>6,202</td>
</tr>
<tr>
<td>Riskware</td>
<td>21</td>
<td>97,349</td>
</tr>
<tr>
<td>Scareware</td>
<td>3</td>
<td>1,556</td>
</tr>
<tr>
<td>Trojan</td>
<td>45</td>
<td>13,559</td>
</tr>
<tr>
<td>Trojan-Banker</td>
<td>11</td>
<td>887</td>
</tr>
<tr>
<td>Trojan-Dropper</td>
<td>9</td>
<td>2,302</td>
</tr>
<tr>
<td>Trojan-SMS</td>
<td>11</td>
<td>3,125</td>
</tr>
<tr>
<td>Trojan-Spy</td>
<td>11</td>
<td>3,540</td>
</tr>
<tr>
<td>Zero-day</td>
<td>-</td>
<td>13,340</td>
</tr>
</tbody>
</table>

Static analysis
AndroidManifest.xml contains a lot of features that can be used for static analysis. The main extracted features include:
Activities: An android activity is one screen of the android app's user interface
Broadcast receivers and providers
Metadata: It is basically an additional option to store information that can be accessed through the entire project
The permissions requested by application: It protects the privacy of the user and is needed to access sensitive user data (such as contacts and SMS)
System features (such as camera and internet)
Static Features
<table>
<tbody>
<tr>
<th>Feature</th>
<th>Values</th>
</tr>
<tr>
<td>Package Name</td>
<td>""com.fb.iwidget""</td>
</tr>
<tr>
<td>Activities</td>
<td>""com.fb.iwidget.OverlayActivity""<br> ""org.acra.CrashReportDialog""<br> ""com.batch.android.BatchActionActivity""<br> ""com.fb.iwidget.MainActivity""<br> ""com.fb.iwidget.PreferencesActivity""<br> ""com.fb.iwidget.PickerActivity""<br> ""com.fb.iwidget.IntroActivity""</td>
</tr>
<tr>
<td>Services</td>
<td>""com.batch.android.BatchActionService""<br> ""com.fb.iwidget.MainService""<br> ""com.fb.iwidget.SnapAccessService""</td>
</tr>
<tr>
<td>Receivers/Providers</td>
<td>""com.fb.iwidget.ExpandWidgetProvider""<br> ""com.fb.iwidget.ActionReceiver""</td>
</tr>
<tr>
<td>Intents Actions</td>
<td>""android.accessibilityservice.AccessibilityService""<br> ""android.appwidget.action.APPWIDGET_UPDATE""<br> ""android.intent.action.BOOT_COMPLETED""<br> ""android.intent.action.CREATE_SHORTCUT""<br> ""android.intent.action.MAIN""<br> ""android.intent.action.MY_PACKAGE_REPLACED""<br> ""android.intent.action.USER_PRESENT""<br> ""android.intent.action.VIEW""<br> ""com.fb.iwidget.action.SHOULD_REVIVE""</td>
</tr>
<tr>
<td>Intents Categories</td>
<td>""android.intent.category.BROWSABLE""<br> ""android.intent.category.DEFAULT""<br> ""android.intent.category.LAUNCHER""</td>
</tr>
<tr>
<td>Permissions</td>
<td>""android.permission.ACCESS_NETWORK_STATE""<br> ""android.permission.CALL_PHONE""<br> ""android.permission.INTERNET""<br> ""android.permission.RECEIVE_BOOT_COMPLETED""<br> ""android.permission.SYSTEM_ALERT_WINDOW""<br> ""com.android.vending.BILLING""<br> ""android.permission.BIND_ACCESSIBILITY_SERVICE""</td>
</tr>
<tr>
<td>Meta-Data</td>
<td>""android.accessibilityservice""<br> ""android.appwidget.provider""</td>
</tr>
<tr>
<td>#Icons</td>
<td>331</td>
</tr>
<tr>
<td>#Pictures</td>
<td>0</td>
</tr>
<tr>
<td>#Videos</td>
<td>0</td>
</tr>
<tr>
<td>Audio files</td>
<td>0</td>
</tr>
<tr>
<td>Videos</td>
<td>0</td>
</tr>
<tr>
<td>Size of the App</td>
<td>4.2M</td>
</tr>
</tbody>
</table>

Dynamic analysis
For understanding the behavioral changes of these malware categories and families, six categories of features are extracted after executing the malware in an emulated environment. The main extracted features include:
Memory: Memory features define activities performed by malware by utilizing memory.
API: Application Programming Interface (API) features delineate the communication between two applications.
Network: Network features describe the data transmitted and received between other devices in the network. It indicates foreground and background * network usage.
Battery: Battery features describe the acces to battery wakelock and services by malware.
Logcat: Logcat features write log messages corresponding to a function performed by malware.
Process: Process features count the interaction of malware with toal number of process.
0<table>
<tbody>
<tr>
<th>Feature</th>
<th>Values</th>
</tr>
<tr>
<td>Package Name</td>
<td>""com.fb.iwidget""</td>
</tr>
<tr>
<td>Activities</td>
<td>""com.fb.iwidget.OverlayActivity""<br> ""org.acra.CrashReportDialog""<br> ""com.batch.android.BatchActionActivity""<br> ""com.fb.iwidget.MainActivity""<br> ""com.fb.iwidget.PreferencesActivity""<br> ""com.fb.iwidget.PickerActivity""<br> ""com.fb.iwidget.IntroActivity""</td>
</tr>
<tr>
<td>Services</td>
<td>""com.batch.android.BatchActionService""<br> ""com.fb.iwidget.MainService""<br> ""com.fb.iwidget.SnapAccessService""</td>
</tr>
<tr>
<td>Receivers/Providers</td>
<td>""com.fb.iwidget.ExpandWidgetProvider""<br> ""com.fb.iwidget.ActionReceiver""</td>
</tr>
<tr>
<td>Intents Actions</td>
<td>""android.accessibilityservice.AccessibilityService""<br> ""android.appwidget.action.APPWIDGET_UPDATE""<br> ""android.intent.action.BOOT_COMPLETED""<br> ""android.intent.action.CREATE_SHORTCUT""<br> ""android.intent.action.MAIN""<br> ""android.intent.action.MY_PACKAGE_REPLACED""<br> ""android.intent.action.USER_PRESENT""<br> ""android.intent.action.VIEW""<br> ""com.fb.iwidget.action.SHOULD_REVIVE""</td>
</tr>
<tr>
<td>Intents Categories</td>
<td>""android.intent.category.BROWSABLE""<br> ""android.intent.category.DEFAULT""<br> ""android.intent.category.LAUNCHER""</td>
</tr>
<tr>
<td>Permissions</td>
<td>""android.permission.ACCESS_NETWORK_STATE""<br> ""android.permission.CALL_PHONE""<br> ""android.permission.INTERNET""<br> ""android.permission.RECEIVE_BOOT_COMPLETED""<br> ""android.permission.SYSTEM_ALERT_WINDOW""<br> ""com.android.vending.BILLING""<br> ""android.permission.BIND_ACCESSIBILITY_SERVICE""</td>
</tr>
<tr>
<td>Meta-Data</td>
<td>""android.accessibilityservice""<br> ""android.appwidget.provider""</td>
</tr>
<tr>
<td>#Icons</td>
<td>331</td>
</tr>
<tr>
<td>#Pictures</td>
<td>0</td>
</tr>
<tr>
<td>#Videos</td>
<td>0</td>
</tr>
<tr>
<td>Audio files</td>
<td>0</td>
</tr>
<tr>
<td>Videos</td>
</tr>
<tr>
<td>Size of the App</td>
<td>4.2M</td>
</tr>
</tbody>
</table>
Acknowledgements
Mitacs Globalink Program for providing the Research Internship (GRI) o.Harrison McCain Young Scholar Foundation funds from University of New Brunswick (UNB) ,CCCS."	62	600	11	albertozorzetto	cic-andmal-2020-dynamic-static-analysis
8631	8631	spam/not spam		[]		0	33	0	sajalshovon	spamnot-spam
8632	8632	nsk_image_search3_man1		[]		0	4	0	motono0223	nsk-image-search3-man1
8633	8633	synthesis_spade		[]		0	4	0	shashil	synthesis-spade
8634	8634	Snoot Scoop - NACSW	Results from NACSW's scent detection dog sport trials	['united states', 'sports', 'animals', 'beginner']	"Context
National Association of Canine Scent Work, LLC (NACSW) is an organization in the US that arranges well-structured and engaging canine scent detection competitions. Think a for-fun competition version of the kind of work a sniffer dog does when they're screening luggage at an airport.
Dogs competing in NACSW are trained to detect 3 target odors - Birch, Anise and Clove. At the trials, cotton swabs or similar items scented with one or more of these odors are hidden out of view. The dogs must source the locations of the hides and indicate to their handler who reports the find to the judge. If the team correctly sources a hide, they earn points for it. 
Teams work in diverse environments (referred to as elements) searching for hides in indoor locations, outdoor locations, on vehicles and in containers. The number of hides, search area sizes and complexity of the scent puzzles the teams encounter increases with increasing levels of competition. At higher levels of competition teams also encounter unknown number of hides including unknown no hides (blank areas).
Content
The dataset that was analyzed consists of the results of NACSW NW1, NW2, NW3, Elite, Summit, L1, L2 & L3 trials from 2009 through 2021. It was obtained by scraping the results posted at NACSW's trial results page.
NACSW's full trial rule book is available here and can be used as a reference in understanding how the scores, times, faults, errors in the results data translate into pass/fail results, titles, high-in-trial and other placements. The rule book also provides more information on the different elements.
Acknowledgements
Thank you, NACSW for making this trove of historical scent work data available publicly to the trial competitors and fans of this dog sport and venue.
Inspiration
For Exploration
How did the popularity of this dog sport change over time and space since its introduction?
What are the top breeds in this sport (and by what definition of ""top"")? How has this changed over the years?
On average, how many attempts does it take for a team to earn their NW1 title?
Is one search element (amongst Interior, Exterior, Vehicle, Container) the most difficult for all dogs on average? 
Does success in any one element correlate with success in another element?
My own attempts at exploring  and analyzing this data for some of these questions can be found at this project site and corresponding repository
For Prediction
What states will next year's NW1 overall high placing dogs earn their titles from? 
Which new breeds will appear in the Summit trials in the next 5 years?
What breed mix would make a good scent work dog?"	0	113	0	saylibenadikar	nacsw-trial-results
8635	8635	Unshuffled COTS TFRecords		[]		0	22	0	shreyasagarwal	unshuffled-cots-tfrecords
8636	8636	Shuffled (500 groups) COTS TFRecords		[]		0	34	0	shreyasagarwal	shuffled-500-groups-cots-tfrecords
8637	8637	Fully Shuffled COTS TFRecords		[]		0	14	0	shreyasagarwal	fully-shuffled-cots-tfrecords
8638	8638	Historical Testkits Demand		['history']		2	98	0	svetlanastephanova	historical-testkits-demand
8639	8639	sartorius-cell-instance-segmentation_test		[]		0	9	0	kareemmosharkawy	sartoriuscellinstancesegmentation-test
8640	8640	salePrice		['arts and entertainment']		0	13	0	mahnoorrana	saleprice
8641	8641	housing_data		[]		0	33	0	mahnoorrana	housing-data
8642	8642	Sartorius_detectron2_model		[]		3	47	2	virajkadam	sartorius-detectron2-model
8643	8643	titanictraini.csv		[]		0	8	0	jaysudhirpatil	titanictrainicsv
8644	8644	yolov5		[]		0	8	0	inpyohong	yolov5
8645	8645	sample_submittion		[]		0	35	0	trokhymovych	sample-submittion
8646	8646	tps_dec21	Dataset Resulting from pre-processing	['north america', 'geography', 'business', 'feature engineering', 'tabular data']		2	85	0	jcaliz	tps-dec21
8647	8647	Results 		['standardized testing']		0	8	0	katyashebeko	results
8648	8648	The Our World in Data COVID vaccination data	Coronavirus (COVID-19) Vaccinations	[]		1	39	0	imrobintomar	covid19-vaccinations-data
8649	8649	res_coco_caffe		[]		0	39	0	mradul2	res-coco-caffe
8650	8650	PetFinder-Model41		[]		0	31	0	lftuwujie	petfindermodel41
8651	8651	Interest over time - Dogecoin (2021)		['currencies and foreign exchange']		1	37	0	kreshan23	interest-over-time-dogecoin-2021
8652	8652	P_Pr_Resnet152		[]		0	27	0	durgammohanpranay	p-pr-resnet152
8653	8653	Switzerland Covid19 Cases Time Series		[]		2	48	0	ollibolli	switzerland-covid19-cases-time-series
8654	8654	TF-LongFormer-v14		['exercise']	These model weights were trained by user @kaggleqrdl in his notebook version 20 here	118	289	7	cdeotte	tflongformerv14
8655	8655	nti_2_2		[]		0	40	0	timofeev25	nti-2-2
8656	8656	newobject		[]		1	49	1	zahraahashim	newobject
8657	8657	NSE ALL STOCK DATA FROM 2-1-2017 TO UPDATED_WEEKLY	DATA OF INDIAN STOCKS LISTED IN NSE	['business']	This file will be updated every week for updated data of NSE stocks.	2	63	0	bapanpathak	nse-all-stock-data-from-02jan2017-to-24dec2021
8658	8658	Climate Change Global Temperature Data 	Analyze and Forecast Global Climatic trends and patterns	['atmospheric science']	"This datasets contains 3 csv files. This are – 
1)  GlobalTemperatures.csv
2)  GlobalLandTemperaturesByCountry.csv
3)  GlobalLandTemperaturesByState.csv
The GlobalTemperatures.csv contains :
•   Date: starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures
•   LandAverageTemperature: global average land temperature in celsius
•   LandAverageTemperatureUncertainty: the 95% confidence interval around the average
•   LandMaxTemperature: global average maximum land temperature in celsius
•   LandMaxTemperatureUncertainty: the 95% confidence interval around the maximum land temperature
•   LandMinTemperature: global average minimum land temperature in celsius
•   LandMinTemperatureUncertainty: the 95% confidence interval around the minimum land temperature
•   LandAndOceanAverageTemperature: global average land and ocean temperature in celsius
•   LandAndOceanAverageTemperatureUncertainty: the 95% confidence interval around the global average land and ocean temperature
We just take Date and LandAverageTemperature for our analysis.
The GlobalTemperaturesByCountry.csv contains :
•   Date : Same as Date column of GlobalTemperatures.csv.
•   AverageTemperature : Same as  LandAverageTemperature  column of GlobalTemperatures.csv.
•   Country : Name of countries from which the data contains.
The GlobalTemperaturesByCountry.csv contains :
•   Date : Same as Date column of GlobalTemperatures.csv.
•   AverageTemperature : Same as  LandAverageTemperature column of GlobalTemperatures.csv.
•   Country : Name of countries from which the data contains.
•   State: Name of state from which the data contains."	26	140	0	sachinsarkar	climate-change-global-temperature-data
8659	8659	RepoRec		[]		1	11	0	sciannameaandrea	reporec
8660	8660	RecommenderData		[]		1	6	0	sciannameaandrea	recommenderdata
8661	8661	INTEL Stock Data	Intel Corporation (INTC) | NasdaqGS | Currency in USD	['finance', 'time series analysis', 'currencies and foreign exchange', 'datetime']	"What is INTEL?
Intel Corporation is an American multinational corporation and technology company headquartered in Santa Clara, California. It is the world's largest semiconductor chip manufacturer by revenue, and is the developer of the x86 series of microprocessors, the processors found in most personal computers. Intel ranked No. 45 in the 2020 Fortune 500 list of the largest United States corporations by total revenue for nearly a decade, from 2007 to 2016 fiscal years.
Information about this dataset
This dataset provides historical data of Intel Corporation (INTC). The data is available at a daily level. Currency is USD."	170	1711	13	varpit94	intel-stock-data
8662	8662	the rock		[]		2	40	0	baotruongg	the-rock
8663	8663	insurancecb		[]		1	40	0	amitsavaliya	insurancecb
8664	8664	Titanic Dataset	Titanic Logistic Regression	[]		3	32	0	abhishektripathy	titanic-abhishek
8665	8665	preprocessing		[]		0	28	0	jiacheng1216	preprocessing
8666	8666	pet2-tfrecords2-384-006		[]		0	21	0	bamps53	pet2-tfrecords2-384-006
8667	8667	bollywooddataset		[]		0	37	0	saranshasati	bollywooddataset
8668	8668	catimage		[]		0	11	0	sheikhabujubayer	catimage
8669	8669	training_data		[]		1	34	0	raafatalaa	training-data
8670	8670	unifieddeifinu		[]		0	28	0	crained	unifieddeifinu
8671	8671	Tourism Demand Statistics 2015-2020 Saudi Arabia	Main Indicators of Tourism in Saudi Arabia	['exploratory data analysis', 'time series analysis', 'travel', 'middle east']		9	109	0	aasimuddin	tourism-demand-statistics-20152020-saudi-arabia
8672	8672	CMO Historical Annual Data		['business']		0	38	0	islombekdavronov	cmo-historical-annual-data
8673	8673	house_price		[]		1	35	0	mahnoorrana	house-price
8674	8674	large_4_7_384		[]		1	9	0	kingkong153	large-4-7-384
8675	8675	hosue_price		[]		0	29	0	mahnoorrana	hosue-price
8676	8676	MyProject_Dataset		[]		2	59	0	rajureddybaddam	myproject-dataset
8677	8677	Carvana Image Masking Challenge (Binary PNG)		[]		48	195	3	ipythonx	carvana-image-masking-challenge-binary-png
8678	8678	cots_cocoapi_base		[]		1	45	0	kenakai16	cots-cocoapi-base
8679	8679	pytorch_toolbelt		[]		0	49	0	wuyhbb	pytorch-toolbelt
8680	8680	cots_yolo_base		[]		1	27	0	kenakai16	cots-yolo-base
8681	8681	cots_env_base		[]		1	24	0	kenakai16	cots-env-base
8682	8682	Population dynamics of Helicoverpa armigera	Smart insect traps monitor insects' population in plantations	['biology', 'animals', 'agriculture', 'data analytics', 'python']	"Title: Smart insect traps report the population dynamics of Helicoverpa armigera in cotton plantations
Context
The electronic funnel trap (e-funnel) is an optical counter attached to typical, plastic, funnel traps to automatically count all captured Lepidoptera species with known pheromone as they enter in the bucket. The pests follow the chemical signals of the pheromone and land on the pheromone dispenser. With time they get exhausted and fall in the trap. Once an insect falls in, it interrupts a flow of infrared light and thus it is counted. The device senses the entrance of the insect and registers a capture event, the data-stamp of the event, GPS coordinates and the environmental variables at that time. Data are delivered through the mobile network from the field to a server without human intervention. At the server data are streamlined and visualized and alerts are issued automatically.
Why are e-traps important?
1. They sample insect fauna in order to make informative assumptions on the actual pest load of large areas to reach decisions on treatment. 
2. They timely predict the occurrence of the onset of an infestation and the location of its concentration can define the decisions on an insecticide 
application program.
3. They reduce considerable monitoring cost and, therefore can expand to hundreds of nodes that monitor large spatial scales. Man power constraints and cost can make manual monitoring problematic.
4. The knowledge of where and when there is an infestation load can mitigate uniform, unnecessary sprayings that are usually applied by farmers in fear of crop loss.
5. One can use historical data to predict the evolution of the infestation load.
Content
In the Summer of 2021, 5 e-traps have been deployed in different parts of northern Greece, to monitor Helicoverpa armigera (the cotton bollworm) in large cotton plantations for approximately three months. The dataset consists of 5 different excel files corresponding to approximately 3 months of monitoring time (June-September).
The code snippets analyzing the data in the 'Code' tab investigate several aspects and statistical characteristics of this data as well as predicting future infestation load based on past data.
Acknowledgments
The data have been recorded using automatic insect traps (optical counters) published in:
Iraklis I. Rigakis, Kiki N. Varikou, Antonis E. Nikolakakis, Zacharias D. Skarakis, Nikolaos A. Tatlas, Ilyas G. Potamitis, The e-funnel trap: Automatic monitoring of lepidoptera; a case study of tomato leaf miner, Computers and Electronics in Agriculture, Volume 185, 2021, 106154, ISSN 0168-1699, https://doi.org/10.1016/j.compag.2021.106154.
Inspiration
E-traps are common traps enhanced with the capability of reporting wirelessly a key piece of information about their captured content (typically counts of captured Lepidoptera, time of capture, environmental parameters during capture, GPS coordinates). E-traps based on optical counters can provide consistent estimates of insects’ presence (detection/infestation onset), population dynamics (monitoring) but they are also useful for evaluating insecticide treatment efficacy (post-treatment analysis) and control purposes (population reduction). These traps offer services that are impractical to deliver manually. They monitor and report insect populations 24 h a day and thus can determine the precise onset and evolution of an infestation. If the user configures the report update on a per hour basis, then the network of traps can infer the dynamics of pest populations. Dense reporting can evaluate the impact of a control treatment (e.g., chemical/biological spraying, release of beneficial entomophagous insects etc.). E-traps timestamp all captured events, so one can track insects’ response to pheromones, their activity related to the circadian rhythm and the optimal time for treatment application.
E-traps based on optical counters, count captured insects upon their entrance and therefore they do not face the problem of camera-based traps with insects piling up, disintegrating and illumination variations. Optical counters are totally immune to audio noise as they do not make use of microphones. Our implementation discerns the direction of the moving insect and therefore does not register double counts. Insects that differ significantly in size compared to the targeted pest are rejected as they produce light fluctuations whose amplitudes are out of bound. E-traps using optical counters rely on the specificity of the pheromone to attract only the targeted pest."	3	515	3	potamitis	population-dynamics-of-helicoverpa-armigera
8683	8683	Hotel Reviews from TripAdvisor	Mussoorie Reviews                     . 	['nlp', 'ratings and reviews', 'hotels and accommodations']	"Once there was a businessman having a  good hotel in the city of Mussoorie near Mall Road. Well, what struck your mind first when your heard about Mussoorie? Let me guess, and i am certain that it was beautiful mountains, greenery, cold breeze, and scenic beauty. So to enjoy all this you must have a  great place to stay and for this comfortability we are talking about that businessmen who owns a hotel in this beautiful hill station. 
But this business man was old and his hotel had a very gentle and nice vintage touch , a place where you can sense the glory of past time along with the essence of woods from which it was made. It had soft carpets , beautiful wooden crafted bed, velvet stitched cushions and a fire place in every room to make you feel comfortable and cozy in your stay. Overall I can say that it was such a place where you can get the British times architecture along with Indian hospitality.
As we have seen in all these years of rapid advancement in urban sector and tourism that sometimes in the race of this modern world the  vintage and old crafts lag behind. its not because that the owners of such businesses does not want to go ahead its just they don't know the right course of action and also the right idea that in what field they are lagging and exactly what their customers are expecting from them.
The owner of one such business asked you to somehow figure out what are the positives and negatives about his hotel according to the customer. He gave you a csv file which contains reviews, date of stay and rating given by reviewer. He also asked you to somehow help in predicting the sentiment of some reviews."	23	252	5	ruchibhadauria	hotel-reviews-from-tripadvisor
8684	8684	detect_outlier_py	help to detect all outlier data 	['beginner', 'exploratory data analysis', 'data cleaning', 'outlier analysis', 'python']	"USAGE: getAllOutlerData(DATAFRAME , STD_DISTANCE).  
STD_DISTANCE  EXAMPLE : # 1.64:over 90%  1.96:over 95%   2.58:over 99%   3.3:over 99.9%
[CODE]
outlierIndex = getAllOutlerData(df,2.58)
print(""Toal outlierIndex = "" , outlierIndex)
print(""Toal outlier lenght = "" , len(outlierIndex))
print(""Toal outlier percent = "" , round((100 * len(outlierIndex)/len(df)),2) ,""%"")
[RESULT]
[SepalLengthCm's outlier]
maxValue= 7.97  minValue= 3.71
SepalLengthCm's outlier Index: No outlier
[SepalWidthCm's outlier]
maxValue= 4.17  minValue= 1.94
SepalWidthCm's outlier Data:' [4.4, 4.2]
SepalWidthCm's outlier Index:' [15, 33]
[PetalLengthCm's outlier]
maxValue= 8.3  minValue= -0.78
PetalLengthCm's outlier Index: No outlier
[PetalWidthCm's outlier]
maxValue= 3.16  minValue= -0.76
PetalWidthCm's outlier Index: No outlier
Toal outlierIndex =  [15, 33]
Toal outlier lenght =  2
Toal outlier percent =  1.33 %"	22	342	6	rhythmcam	detect-outlier-py
8685	8685	Municipality Bus Utilization		[]		1	26	0	berkantaslan	municipality-bus-utilization
8686	8686	Indian Actor Images Dataset	A Dataset consist of 6750 Indian Actor Images in 135 different Classes	['arts and entertainment', 'movies and tv shows', 'computer vision', 'image data', 'gpu']	"Context
The cinema of India consists of films produced in the nation of India. Cinema is immensely popular in India. Every year more than 1800 films get produced in various languages in India. Mumbai, Chennai, Kolkata, Hyderabad, Kochi, Bangalore, Bhubaneshwar-Cuttack, and Guwahati are the major centers of film production in India. As of 2013, India ranked first in terms of annual film output, followed by Nigeria, Hollywood a and China. In 2012, India produced 1,602 feature films. The Indian film industry reached overall revenue of $1.86 billion (₹93 billion) in 2011. In 2015, India had a total box office gross of US$2.1 billion, the third-largestt in the world. In 2011, Indian cinema sold over 3.5 billion tickets worldwide, 900,000 more than Hollywood.
The overall revenue of Indian cinema reached US$1.3 billion in 2000. The industry is segmented by language. The Hindi language film industry is known as Bollywood, the largest sector, representing 43% of box office revenue. The combined revenue of the Tamil and Telugu film industries represents 3 36%. Prominent movie industries include Tamil, Telugu, Malayalam, Kannada a and Tulu cinemas. Another prominent film culture is Bengali cinema, which was largely associated with the parallel cinema movement, in contrast to the masala films more prominent in Bollywood and Southern films at the time.
Indian cinema is a global enterprise. Its films have a following throughout Southern Asia and across Europe, North America, Asia, the Greater Middle East, Eastern Africa, China a and elsewhere, reaching over 90 countries. Biopics including Dangal became transnational blockbusters grossing over $300 million worldwide. Millions of Indians overseas watch Indian films, accounting for some 12% of revenues. Music rights alone account for 4–5% of net revenues.
Content
In this Dataset, we have 6750 Indian Actor (Male and Female) Images in 135 different categories or classes."	136	1865	56	iamsouravbanerjee	indian-actor-images-dataset
8687	8687	Top 395 Cryptocurrency Dataset	This Dataset is a Collection of 395 Different Cryptocurrencies	['history', 'finance', 'tabular data', 'investing', 'currencies and foreign exchange']	"Context
A cryptocurrency, crypto-currency, or crypto is a collection of binary data which is designed to work as a medium of exchange. Individual coin ownership records are stored in a ledger, which is a computerized database using strong cryptography to secure transaction records, to control the creation of additional coins, and to verify the transfer of coin ownership. Cryptocurrencies are generally fiat currencies, as they are not backed by or convertible into a commodity. Some crypto schemes use validators to maintain the cryptocurrency. In a proof-of-stake model, owners put up their tokens as collateral. In return, they get authority over the token in proportion to the amount they stake. Generally, these token stakes get additional ownership in the token overtime via network fees, newly minted tokens, or other such reward mechanisms.
Cryptocurrency does not exist in physical form (like paper money) and is typically not issued by a central authority. Cryptocurrencies typically use decentralized control as opposed to a central bank digital currency (CBDC). When a cryptocurrency is minted or created prior to issuance or issued by a single issuer, it is generally considered centralized. When implemented with decentralized control, each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database
A cryptocurrency is a tradable digital asset or digital form of money, built on blockchain technology that only exists online. Cryptocurrencies use encryption to authenticate and protect transactions, hence their name. There are currently over a thousand different cryptocurrencies in the world, and many see them as the key to a fairer future economy.
Bitcoin, first released as open-source software in 2009, is the first decentralized cryptocurrency. Since the release of bitcoin, many other cryptocurrencies have been created.
Content
This Dataset is a collection of records of 395 Different Cryptocurrencies.
Acknowledgements
https://finance.yahoo.com/"	325	2445	37	iamsouravbanerjee	cryptocurrency-dataset-2021-395-types-of-crypto
8688	8688	PPC model 2021		[]		1	54	0	khotijahs1	ppc-model-2021
8689	8689	Loan data set		[]		0	26	0	rameshrau	loan-data-set
8690	8690	Iris of eye dataset	this dataset can be used in iris recognition system to recognize a person 	['deep learning', 'svm', 'cnn', 'image data', 'multiclass classification']		10	173	1	mohmedmokhtar	iris-of-eye-dataset
8691	8691	dog breed iden- segintofold		[]		0	24	0	sunnygeorge	dog-breed-iden-segintofold
8692	8692	Covid19_morocco		[]		4	12	0	mohamedshappan	covid19-morocco
8693	8693	French second hand cars	scraped data fom french website referencing private or professional adds	['automobiles and vehicles', 'data cleaning', 'classification', 'clustering', 'regression']	"Context
French second hand market scraped data for personal side project. The dataset represents announcements made by individuals or professionals.
The fixed price cannot be representative of the real value of the vehicle because it is freely fixed"	291	1922	12	spicemix	french-second-hand-car
8694	8694	Retail data from Quantium Analytics		[]		1	51	0	mukeshkumar95	retail-data-from-quantium-analytics
8695	8695	highvaldata		[]		2	25	0	aronbryant	highvaldata
8696	8696	Panel Dataset / Cost Data of U.S. Airlines	Cost Data for U.S. Airlines, 90 Oservations On 6 Firms For 15 Years, 1970-1984	['statistical analysis', 'tabular data', 'regression']	"Dataset Details:
Data Set contains Cost Data for U.S. Airlines, 90 Observations  On 6 Firms For 15 Years, 1970-1984
Predictors:
I = Airline,
T = Year,
Q = Output, in revenue passenger miles, index number,
PF = Fuel price,
LF = Load factor, the average capacity utilization of the fleet.
Response:
C = Total cost, in $1000,
Acknowledgements and Credit
These data are a subset of a larger data set provided to the author by Professor Moshe Kim.
They were originally constructed by Christensen Associates of Madison, Wisconsin.
Inspiration
Perform various econometric analyses to check which model suits best for the given dataset. To start with can check this notebook which is programmed in R."	56	1209	24	sandhyakrishnan02	paneldata
8697	8697	sale_price_dataset		[]		0	33	0	mahnoorrana	sale-price-dataset
8698	8698	house_dataset		[]		2	21	0	mahnoorrana	house-dataset
8699	8699	Images_Indus_Hospital		[]		12	5	0	arsalanmubeen123	images-indus-hospital
8700	8700	Movie Genre+Plot+Poster	Dataset for Movie Genre Classification	['movies and tv shows']	"Context
This dataset contains poster and plot summary corresponding to a movie title along with its genre. This dataset can be used to predict a movie's genre based on its plot summary and/or poster image.
Content
The data has been originally taken from: https://www.kaggle.com/jrobischon/wikipedia-movie-plots .
From this dataset, movies having similar genres have been grouped together. ex: For ex: Genres labelled as action & adventure, action & children, live action, heist, martial arts, war, superhero, actioneer etc. have all been included as action. 
Movie posters corresponding to the movie names, genres and plots have been webscrapped from iMDb. 
Acknowledgements
This movie name, genre and plot summary was originally posted by JustinR(@jrobischon).
The movie posters have been webscrapped from iMDb."	9	53	0	aakashsaroop	movie-genreplotposter
8701	8701	GEN_TEST_SADDLE_2		[]		2	27	0	zmjjiang	gen-test-saddle-2
8702	8702	Quickbooks connection diagnostic tool		[]		0	23	0	williamacker	quickbooks-connection-diagnostic-tool
8703	8703	12_27_17		[]		0	14	1	terencenlp	12-27-17
8704	8704	marketing_data	Revised data from a marketing campaign	[]	"Context
A response model can provide a significant boost to the efficiency of a marketing campaign by increasing responses or reducing expenses. The objective is to predict who will respond to an offer for a product or service
Content
Age - customer's age at the date of the campaign
Education - customer’s level of education
Marital - customer’s marital status
Income - customer’s yearly household income
Kidhome - number of small children in customer’s household
Teenhome - number of teenagers in customer’s household
DtCustomer - date of customer’s enrolment with the company
Recency - number of days since the last purchase
MntWines - amount spent on wine products in the last 2 years
MntFruits - amount spent on fruits products in the last 2 years
MntMeatProducts - amount spent on meat products in the last 2 years
MntFishProducts - amount spent on fish products in the last 2 years
MntSweetProducts - amount spent on sweet products in the last 2 years
MntGoldProds - amount spent on gold products in the last 2 years
NumDealsPurchases - number of purchases made with discount
NumWebPurchases - number of purchases made through company’s web site
NumCatalogPurchases - number of purchases made using catalogue
NumStorePurchases - number of purchases made directly in stores
NumWebVisitsMonth - number of visits to company’s web site in the last month
AcceptedCmp1 - 1 if customer accepted the offer in the 1st campaign, 0 otherwise
AcceptedCmp2 - 1 if customer accepted the offer in the 2nd campaign, 0 otherwise
AcceptedCmp3 - 1 if customer accepted the offer in the 3rd campaign, 0 otherwise
AcceptedCmp4 - 1 if customer accepted the offer in the 4th campaign, 0 otherwise
AcceptedCmp5 - 1 if customer accepted the offer in the 5th campaign, 0 otherwise
Response (target) - 1 if customer accepted the offer in the last campaign, 0 otherwise
Complain - 1 if customer complained in the last 2 years
Country - customer's country
Inspiration
The main objective is to train a predictive model which allows the company to maximize the profit of the next marketing campaign."	19	186	2	emmetbrown	marketing-data
8705	8705	mmdet mask rcnn swin transformer v1		['retail and shopping']		15	121	0	duykhanh99	mmdet-mask-rcnn-swin-transformer-v1
8706	8706	ESA 05		[]		0	38	0	edwardbarton	esa-05
8707	8707	ESA 04		[]		0	30	0	edwardbarton	esa-04
8708	8708	ESA 03		[]		0	31	0	edwardbarton	esa-03
8709	8709	ESA 02		[]		0	4	0	edwardbarton	esa-02
8710	8710	ESA 01		[]		0	17	0	edwardbarton	esa-01
8711	8711	atmmachine		[]		1	36	0	nileshsharma2004	atmmachine
8712	8712	tablet		[]		0	28	0	dilaranuraltparmak	tablet
8713	8713	Huggingface Models	Pytorch and Tensoflow Huggingface Models	['clothing and accessories', 'tensorflow', 'pytorch', 'transformers']		0	164	0	harip98	huggingface-models
8714	8714	Feedback_proprocessed_csv		[]	"This is preprocessed csv dataset for Feedback Prize problem.
Preprocessing takes some time,  I made this so that we can just load it and get right to work!
First, about train-text-df
Description of the preprocess
In a nutshell, I converted all text words into NER labels and save this in a dataframe.
Why this process was applied?
There were some discrepancies with number of words and labels, so I balanced these elements and make labels for NER problem.
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	3	34	1	fightingmuscle	feedback-proprocessed-csv
8715	8715	forest-fire		[]		0	9	0	weerinphas	forestfire
8716	8716	wildfire		[]		0	33	0	weerinphas	wildfire
8717	8717	epoch_26		[]		2	21	1	liao12345	epoch-26
8718	8718	epoch_28		[]		0	26	0	liao12345	epoch-28
8719	8719	nebula_graph_shareholding_dataset	A dataset(generator) for Stock shareholding analysis, ready for Nebula Graph	['business', 'economics', 'beginner', 'investing']	"Context
This is a data(generator) for dummy Stock Corp-Person Knowledge Graph, you could generate it in any scale, check out more on: https://siwei.io/nebula-holdshare-dataset/ .
Content
More on https://siwei.io/nebula-holdshare-dataset/ & https://github.com/wey-gu/nebula-shareholding-example
Acknowledgements
Thanks to Upstream Projects ❤️
Python Faker https://github.com/joke2k/faker/
pydbgen https://github.com/tirthajyoti/pydbgen
Nebula Graph https://github.com/vesoft-inc/nebula"	12	303	1	littlewey	nebula-graph-shareolding-dataset
8720	8720	tourism		['travel']		5	29	0	dihwhydsa	tourism
8721	8721	epoch_22		[]		0	42	0	liao12345	epoch-22
8722	8722	New Year's Resolutions	5011 tweets containing new year's resolutions.	['culture and humanities', 'people and society', 'tabular data', 'text data', 'holidays and cultural events']	"About this dataset
&gt; Another year comes to a close and with that an opportunity for new beginings - and New Year's Resolutions is an opportunity to do just that.
&gt; At the same time, in a 2014 report, 35% of participants who failed their New Year's Resolutions admitted they had unrealistic goals, 33% of participants did not keep track of their progress, and 23% forgot about them; about one in 10 respondents claimed they made too many resolutions. 1
&gt; A 2007 study from the University of Bristol involving 3,000 people showed that 88% of those who set New Year resolutions fail, despite the fact that 52% of the study's participants were confident of success at the beginning. 2
&gt; With this dataset, containing 5011 tweets of new year's resolutions, you can use the collective knowledge to improve your odds of success in your own resolutions!
How to use this dataset
&gt; - Apply Topic Modeling or Clustering to Identify Common Goals;
- Explore New Year's Resolutions and use this knowledge to make your own!
&gt; Note that this dataset uses ; as delimiter, due to free text fields containing variable amount of commas.
Highlighted Notebooks
&gt; - Your kernel can be featured here!
- More datasets
Acknowledgements
If you use this dataset in your research, please credit the authors.
&gt; ### Citation
&gt; CrowdFlower.com [Internet]. Data for Everyone. Available from: https://www.crowdflower.com/data-for-everyone/.
&gt; ### Sources used in the description
&gt; 1 Hutchison, Michelle (29 December 2014). ""Bunch of failures or just optimistic? finder.com.au New Year's Resolution Study shows New Year novelty fizzles fast - finder.com.au"". finder.com.au. Retrieved 19 April 2018.
&gt; 2 Lehrer, Jonah (December 26, 2009). ""Blame It on the Brain"". The Wall Street Journal. ISSN 0099-9660.
&gt; ### License
License was not specified at source, yet data is public and free.
&gt; ### Splash banner
Icon by Freepik."	534	4133	30	andrewmvd	new-years-resolutions
8723	8723	nebula_ownthink_property_graph	Chinese Open Source Knowledge Graph by Ownthink for Nebula Graph import ready	['earth and nature']		50	143	1	littlewey	nebula-ownthink-property-graph
8724	8724	QAT with uncond training GPT ep1		['games']		0	52	0	jiaminggogogo	qat-with-uncond-training-gpt-ep1
8725	8725	petfinder-baseline-swt-174402-svr-all		[]		0	4	0	tonyxu	petfinder-baseline-swt-174402-svr-all
8726	8726	pf-137-train-fold1-data		[]		5	15	0	makotoikeda	pf-137-train-fold1-data
8727	8727	Ethical hacking course online	Cyber security training | Ethical hacking course online	['education', 'programming']		16	337	1	cysecon	ethical-hacking-course-online
8728	8728	pytorch-gradcam		[]		0	35	0	guohansheng	pytorchgradcam
8729	8729	AbstractiveAODS		[]		1	14	0	stbultebelay	abstractiveaods
8730	8730	Silent Speech Raw EMG Dataset		[]		0	14	0	rabinnepal	silent-speech-raw-emg-dataset
8731	8731	vit_large		[]		0	47	0	watanabetakahiro	vit-large
8732	8732	Time Serie Store Sales		['retail and shopping']		18	93	4	ammarnassanalhajali	time-serie-store-sales
8733	8733	Employee Performance Analysis INX Future Inc.	Employee Performance 	[]		7	118	0	eshwarganta	employee-performance-analysis-inx-future-inc
8734	8734	Room Occupancy detection data (IoT sensor)	Temperature, Humidity, Light, CO2 , HumidityRatio, time	[]	"binary classification (room occupancy) from Temperature,Humidity,Light and CO2. 
occupancy was obtained from time stamped pictures that were taken every minute.
source
Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Luis M. Candanedo, Véronique Feldheim. Energy and Buildings. Volume 112, 15 January 2016, Pages 28-39."	83	740	9	kukuroo3	room-occupancy-detection-data-iot-sensor
8735	8735	Gesture_Recognition		[]		0	37	0	nilayparikh12	gesture-recognition
8736	8736	UAVid2020		[]		0	39	0	l0new0lf	uavid2020
8737	8737	Data Jagung Test		[]		0	19	0	rahmalisaaulia	data-jagung-test
8738	8738	petfinder-baseline-swt-173328-svr-all		[]		0	4	0	tonyxu	petfinder-baseline-swt-173328-svr-all
8739	8739	smallargfix		[]		0	4	0	yutotakaki	smallargfix
8740	8740	manipulation		[]		0	22	0	amogh0810	manipulation
8741	8741	ximen-fubiao		[]		1	27	0	hongxiangyu	ximenfubiao
8742	8742	xiamen		[]		1	63	2	hongxiangyu	xiamen
8743	8743	CheckPointscnet_x101_64		[]		0	50	0	light367	checkpointscnet-x101-64
8744	8744	petfinder-baseline-swt224-175702-svr-all		[]		0	4	0	tonyxu	petfinder-baseline-swt224-175702-svr-all
8745	8745	petfinder-baseline-swt-174358-svr-all		[]		0	4	0	tonyxu	petfinder-baseline-swt-174358-svr-all
8746	8746	clash royale clustering output		[]		5	60	0	penyultk	clash-royale-clustering-output
8747	8747	Southbell -Churn w/intl calls		['business']		1	35	0	kittyfergusonblevins	southbell-churn-wintl-calls
8748	8748	COTS_YoloXs_test		[]		0	29	0	junwoonlee	cots-yoloxs-test
8749	8749	MaskDetection		[]		4	24	0	antonyjoseph21	maskdetection
8750	8750	Chinese boundary and coordinate for province&city	中国边界、省与省内各市经纬度点数据（省市经纬度点数据不包含台湾、香港、澳门）	['geography', 'computer science', 'text data', 'json']	"Please upvote if useful for u.
Way to use this data set,see here.
Data Source
AMap API
Data Combine
txt
Chinese boundry data.
json
Latitude and longitude for every province and city in China."	9	286	6	holoong9291	china-province-city-latitude-longitude
8751	8751	autoalbument	Albument policy for cifar 10 images	['arts and entertainment']		0	36	0	pavybez	autoalbument
8752	8752	kaerururu-petfinder2-080		[]		0	2	0	kaerunantoka	kaerururu-petfinder2-080
8753	8753	jigsaw2021-wat042-data		[]		0	6	0	wataoka	jigsaw2021-wat042-data
8754	8754	Stereo image		['arts and entertainment']		2	86	0	achintyasri	stereo-image
8755	8755	xinlidataset		[]		0	31	0	dingdong18k	xinlidataset
8756	8756	vit_large_384		[]		1	12	0	watanabetakahiro	vit-large-384
8757	8757	yansi_trainval_4000		[]		1	5	0	iwrange	yansi-trainval-4000
8758	8758	MovieLens 100K Dataset/ml-100k		[]		0	40	0	anushasingampalli	movielens-100k-datasetml100k
8759	8759	daw_cell_instance_segmentation_1		[]		0	35	0	dougwa	daw-cell-instance-segmentation-1
8760	8760	images as digital data (handwrite detection)		[]		2	96	6	hisoka2020	images-as-digital-data-handwrite-detection
8761	8761	Baseball events from retrosheet.org		['baseball']		41	690	0	jraddick	baseball-events-from-retrosheetorg
8762	8762	inputt		[]		0	33	0	yehiahossam	inputt
8763	8763	aaaaaa	abcefgaaaaaaaaaaaaaaaaaa	[]		1	35	0	calneco	aaaaaa
8764	8764	energy-dataset		['energy']		63	49	0	strafetotheheaven	energydataset
8765	8765	CheckpointScnet		[]		0	30	0	cellsegmentation	checkpointscnet
8766	8766	climatsAUS_v2		[]		4	67	0	lionelbottan	climatsaus-v2
8767	8767	detectron2 offline	Install detecton2 offline	['data visualization', 'deep learning']		0	24	0	ekaterinasedykh	detectron2-offline
8768	8768	flightdata.csv		[]		4	39	1	irina464520	flightdatacsv
8769	8769	traindata		[]		0	14	0	timmy106340103	traindata
8770	8770	YOLOv5		[]		0	8	0	cowfrica	yolov5
8771	8771	swin timm		[]		3	32	0	ks2019	swin-timm
8772	8772	PetFinder-Model48-2		[]		0	31	0	lftuwujie	petfindermodel482
8773	8773	images_flatten		[]		0	22	0	slimbu	images-flatten
8774	8774	icnet_synboost_output		[]		2	10	0	synboost	icnet-synboost-output
8775	8775	ai_dataset	suuuuuuuuuuuuuuuuuuuuuuuuuuuuuu	[]		0	15	0	albertorota	ai-dataset
8776	8776	Pretrained Models		['clothing and accessories']		0	37	0	djin31	pretrained-models
8777	8777	Sartorius COCO Folds		['arts and entertainment']		8	114	0	ichimarugin	sartorius-coco-folds
8778	8778	ImagesData		[]		4	48	0	khaledanaqwa	imagesdata
8779	8779	Georeferenced Car Accidents (Santiago de Chile) I	Georeferenced Pedestrian Car Collisions 2015, Santiago de Chile	['cities and urban areas', 'geography', 'government', 'automobiles and vehicles', 'tabular data']	"Georeferenced Pedestrian Car Collisions 2015, Santiago de Chile
This is a public Data Set from the government of Chile
Source: IDE Chile (https://www.ide.cl/index.php/salud/item/1600-atropellos-ano-2015)"	108	1080	14	sandorabad	georeferenced-car-accidents-santiago-de-chile
8780	8780	GRallRounds		[]		1	39	1	pavfedotov	grallrounds
8781	8781	digits images (handwrite detection)		[]		3	18	1	hisoka2020	digits-images-handwrite-detection
8782	8782	Case Study How Does a Bike-Share Navigate		[]		0	19	0	manolomatiax	case-study-how-does-a-bikeshare-navigate
8783	8783	Cr Loan data		[]		4	24	0	ibtehajali	cr-loan-data
8784	8784	Patient Survival Prediction	Classification Problem	['categorical data', 'health', 'exploratory data analysis', 'data cleaning', 'dimensionality reduction', 'binary classification']	The predictors of in-hospital mortality for admitted patients remain poorly characterized. We aimed to develop and validate a prediction model for all-cause in-hospital mortality among admitted patients.	670	4772	20	mitishaagarwal	patient
8785	8785	FaceMask Dataset	Beware and Wear💊💊💊💉💉💉🛡🛡🛡	['computer vision', 'cnn', 'image data', 'binary classification', 'covid19']	"PLEASE UPVOTE 😁😁😁
This is the dataset that I gathered from different sources for FaceMask Detection.
There are around 1350 images in those two files and there is a handling code in the Notebook named ""Official"" at the end.
Please use the dataset wisely and only for good purposes."	376	3866	26	ahemateja19bec1025	facemask-dataset
8786	8786	Covid-19 US	The New York Times Data 	['news', 'covid19']		14	740	0	salikhussaini49	covid19-us
8787	8787	maskrcnn_final		[]		1	55	0	ccvipchenbin	maskrcnn-final
8788	8788	hatebert-regular		[]		0	6	0	panser	hatebertregular
8789	8789	SachinBatting		['exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics', 'tabular data']		0	61	0	sasidharsubramaniyam	sachin-odi-batting
8790	8790	House_prices-rfr, lasso, ridge, xgboost, catboost		[]		1	13	0	toerto	house-pricesrfr-lasso-ridge-xgboost-catboost
8791	8791	T-SQL Queries On Covid-19 Data Set		['business', 'covid19', 'sql']	"This CSV file shows Covid-19 Records From 2020-01-01 to 2021-12-16 For each country,
I provided sql files to show my work."	19	118	0	mohammadaltiti	tsql-queries-on-covid19-data-set
8792	8792	Magnus Carlsen Lichess Games Dataset	All online Bullet and Blitz games from 2017 to 2021	['categorical data', 'tabular data']	"Context
This repository contains all of World Champion Magnus Carlsen' online games played on the Lichess chess server between 2017 and 2021. The files were extracted using the Lichess API, and then processed using a python library called pgn2data. This is a library I wrote, that converts PGN (Portable Game Notation) formatted files into CSV tabulated datasets. The library also adds a number of additional descriptive features to each output, to give more insight into what happened in each game.
Magnus has used a number of different user names over the past few years, so this data provides a consolidated view of all of his performance during the time he has been on the platform.
Content
There are two files:
The file ""carlsen games"" contains all of the top level information for each game (e.g result, score,location, date, time etc..).
The file ""carlsen game moves"" contains all the moves for each game (e.g move number, piece, location etc..). 
The two files are mapped together using the unique identifer ""game_id"".
The original PGN files from Lichess are also included in this data repository. There is also an additional file (eco codes) to identify the openings listed in the games file.
Acknowledgements
Lichess is a free and open-source Internet chess server run by a non-profit organization, and they make all of their data available to be extracted from their platform."	38	1080	5	zq1200	magnus-carlsen-lichess-games-dataset
8793	8793	churn image		[]		1	11	0	ahedjneed	churn-image
8794	8794	kaerururu-petfinder2-079		[]		0	3	0	kaerunantoka	kaerururu-petfinder2-079
8795	8795	Magnus Carlsen Lichess Games Dataset	All online Bullet and Blitz games from 2017 to 2021	['categorical data', 'tabular data']	"Context
This repository contains all of World Champion Magnus Carlsen' online games played on the Lichess chess server between 2017 and 2021. The files were extracted using the Lichess API, and then processed using a python library called pgn2data. This is a library I wrote, that converts PGN (Portable Game Notation) formatted files into CSV tabulated datasets. The library also adds a number of additional descriptive features to each output, to give more insight into what happened in each game.
Magnus has used a number of different user names over the past few years, so this data provides a consolidated view of all of his performance during the time he has been on the platform.
Content
There are two files:
The file ""carlsen games"" contains all of the top level information for each game (e.g result, score,location, date, time etc..).
The file ""carlsen game moves"" contains all the moves for each game (e.g move number, piece, location etc..). 
The two files are mapped together using the unique identifer ""game_id"".
The original PGN files from Lichess are also included in this data repository. There is also an additional file (eco codes) to identify the openings listed in the games file.
Acknowledgements
Lichess is a free and open-source Internet chess server run by a non-profit organization, and they make all of their data available to be extracted from their platform."	38	1080	5	zq1200	magnus-carlsen-lichess-games-dataset
8796	8796	churn image		[]		1	11	0	ahedjneed	churn-image
8797	8797	kaerururu-petfinder2-079		[]		0	3	0	kaerunantoka	kaerururu-petfinder2-079
8798	8798	Gufhtugu 		[]		0	13	0	ehteshamalikhan	gufhtugu
8799	8799	dogpicture		[]		0	50	0	alminaecemeskicindil	dogpicture
8800	8800	Gughtugu dataset		[]		0	16	0	ehteshamalikhan	gughtugu-dataset
8801	8801	PetFinder-Model48		[]		0	7	0	lftuwujie	petfindermodel48
8802	8802	edu_numpy_data		[]		0	32	2	muhammadammarjamshed	edu-numpy-data
8803	8803	student_data_2		[]		2	27	2	muhammadammarjamshed	student-data-2
8804	8804	Covid-Xray		[]		0	59	0	jarvisai7	covidxray
8805	8805	EconDataBase	Aggregate of United Stated Economy 	['business', 'economics', 'tabular data', 'investing']	"We know this data isn't clean yet, but that's a problem in the economic realm.
So this data has two total categories, one is monthly recurring, and the other is quarterly recurring.
We can remove null values according to the type of frequentist that we need.
This data contains 56 symbols, each symbol has a special meaning for us.
You can read the manual Guide to find out the original name of each symbol and also to find out the periods of each symbol present in our data.
If you want the detailed elements of this Data, please go to this site EconDB"	4	150	1	youneseloiarm	econdatabase
8806	8806	New_alzheimer_MRI		[]		4	38	1	marcopinamonti	new-alzheimer-mri
8807	8807	pth_9h		[]		1	20	0	dragonzhang	pth-9h
8808	8808	Sarcasm Dataset		[]		1	22	0	tyb001	sarcasm-dataset
8809	8809	 data to accept the person		[]		0	37	0	disable	data-to-accept-the-person
8810	8810	2013-2018 Seat Changes in NA.csv	National Assembly Seat Changes in 2018 Pakistan Election	['government', 'politics']		0	18	0	ehteshamalikhan	20132018-seat-changes-in-nacsv
8811	8811	pretrain_1_ft		[]		0	3	0	hangy132	pretrain-1-ft
8812	8812	Codes_data		[]		0	29	0	yashjaipurialucknow	codes-data
8813	8813	Spacy-2.2.4-whl		[]		0	75	0	delaram	spacy224whl
8814	8814	Vegetables and commodity Prices of INDIAN Mandis 		[]		4	52	0	vardhaman111	vegetables-and-commodity-prices-of-indian-mandis
8815	8815	sartorius models		['clothing and accessories']		0	4	0	khihunh	sartorius-models
8816	8816	Price prediction(RFR, Ridge, Lasso, XGB, CatBoost)		[]		1	39	0	toerto	price-predictionrfr-ridge-lasso-xgb-catboost
8817	8817	RAFds-F		[]		6	59	0	mohammedaaltaha	rafdsf
8818	8818	pet2-tfrecords2-384-005		[]		0	2	0	bamps53	pet2-tfrecords2-384-005
8819	8819	DateTranslationModel	Seq2Seq model using encoder decoder architecture for translating dates	[]		0	39	1	bkanupam	datetranslationmodel
8820	8820	Video Games List	This is a list of Video Games made for windows	['video games', 'categorical data', 'recommender systems', 'tabular data']	This is a list of games made for windows from 1983 to 2021. This dataset can be used for recommendation systems, The dataset has the title of the Game, year of release, developers/game studio,  publishers, and genres.	263	2221	13	amoghrrao2	video-games-list
8821	8821	hatebert_offenseval		[]		0	7	0	panser	hatebert-offenseval
8822	8822	hatebert_abuseval		[]		1	11	0	panser	hatebert-abuseval
8823	8823	List of kaggle Grandmasters	(including individual sub-tiers and country data)	['computer science']	"Description
This is a list of kaggle Grandmasters and their individual tiers and countries.
Note: The dataset does not include Grandmasters who have gone on to become kaggle staff."	32	558	17	carlmcbrideellis	list-of-kaggle-grandmasters
8824	8824	Consumer Complaint - Finance	Consumer Complaint Database is collection of consumer financial products 	['finance', 'classification', 'text data', 'multiclass classification']	"Context
The Consumer Complaint Database is a collection of complaints about consumer financial products and services that we sent to companies for response. Complaints are published after the company responds, confirming a commercial relationship with the consumer, or after 15 days, whichever comes first. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database. The database generally updates daily.
Additional Metadata
Resource Type - Dataset(csv format)
Metadata Created Date - November 10, 2020
Metadata Updated Date -  November 10, 2020
Publisher - Consumer Financial Protection Bureau
Unique Identifier -     Unknown
Maintainer - devops@cfpb.gov
Format of Dataset
CSV Format
Jason Format
Web Resource
Source
https://catalog.data.gov/dataset/consumer-complaint-database"	111	1223	15	meetnagadia	consumer-complaint-finance
8825	8825	SOIL_features		[]		0	2	1	prateekiet	soil-features
8826	8826	lands for sale in Tunisia		[]		0	74	1	samermakni	lands-for-sale-in-tunisia
8827	8827	houses and apartments for sale in Tunisia.		['cities and urban areas', 'africa', 'real estate', 'beginner', 'text data']		9	85	1	samermakni	houses-and-apartments-for-sale-in-tunisia
8828	8828	aus_town_gps		[]		0	2	0	lionelbottan	aus-town-gps
8829	8829	climatsAUS		[]		0	2	0	lionelbottan	climatsaus
8830	8830	AirFoil Self Noise		['business']		0	20	0	kakapari	airfoil-self-noise
8831	8831	swin_large_384		[]		1	39	0	watanabetakahiro	swin-large-384
8832	8832	mof2_estimator_r3		[]		5	18	0	honihitak	mof2-estimator-r3
8833	8833	Taxi Trajectory Predition(1)		[]		7	44	1	teddy880511	taxi-trajectory-predition1
8834	8834	Coursera Interaction Design Course 7		['education']		1	38	0	nicowilson666	coursera-interaction-design-course-7
8835	8835	Diabetes2019		[]		2	16	0	saicharanmthyapwar	diabetes2019
8836	8836	HateBERT		[]		0	8	0	ryo1993	hatebert
8837	8837	private_dataset		[]		3	9	0	khalidoublal	private-dataset
8838	8838	Destiny 2 weapon info		[]		1	49	0	winstons101	destiny-2-weapon-info
8839	8839	02460model		[]		0	37	0	zihozoeng	02460model
8840	8840	It's my first project.		['video games']		0	5	0	mominmujahid	its-my-first-project
8841	8841	fastaisvr		[]		0	3	0	kunihikofurugori	fastaisvr
8842	8842	timminstall		[]		1	16	2	tensorchoko	timminstall
8843	8843	svr_large_224_7_lazy		[]		0	11	0	kingkong153	svr-large-224-7-lazy
8844	8844	checkpoint(epochs42)		[]		0	43	0	aminulpalash	checkpointepochs42
8845	8845	Jordan_Autoscore_cars		[]		1	28	0	emranbashabsheh	jordan-autoscore-cars
8846	8846	BABE - Media Bias Annotations By Experts	A high-quality dataset of sentence and word level textual bias	[]		0	51	0	timospinde	babe-media-bias-annotations-by-experts
8847	8847	abcdef		[]		0	0	0	surprisek	abcdef
8848	8848	World Leaders and their Win in Elections	Dataset gives you the glimpse of reknowned world leaders's journey.	['geography', 'people', 'people and society', 'politics', 'economics']	"there is only one leader being both head of state and head of government. In other cases, mainly in semi-presidential and parliamentary systems, the head of state and the head of government are different people. In semi-presidential and parliamentary systems, the head of government role (i.e. executive branch) is fulfilled by both the listed head of government and the head of state. In single-party systems, ruling party's leader (i.e. General Secretary) is usually the de facto top leader of the state, though sometimes this leader also holds presidency or premiership.
States where head of state differs from head of government are mainly parliamentary systems. Often a leader holds both positions in presidential systems or dictatorships. Some states have semi-presidential systems where the head of government role is fulfilled by both the listed head of government and the head of state.
a regime (also ""régime"") is the form of government or the set of rules, cultural or social norms, etc. that regulate the operation of a government or institution and its interactions with society."	138	1042	10	ramjasmaurya	world-leaders-and-their-election-wins
8849	8849	QoL_COPING_COVID_Positive		[]		0	33	0	rubayetshafin	qol-coping-covid-positive
8850	8850	titanic_preprocess	titanice preprocess train, test data (feature engineering)	['beginner', 'exploratory data analysis', 'data analytics', 'feature engineering', 'pandas']	"how to use library 
load data and preprocess data at the same time 
use below code:
train,test = loadAndPreprocess(train_path,test_path)"	4	74	2	rhythmcam	titanic-preprocess
8851	8851	Adult_for_Class_Imbalance_Data		[]		4	40	3	nagendraavadanam	adult-for-class-imbalance-data
8852	8852	baseline_20211225_models		[]		0	32	0	qinyukun	baseline-20211225-models
8853	8853	nni-2.5		[]		0	16	0	guohansheng	nni25
8854	8854	grad_cam		[]		0	20	0	zephyruszx	grad-cam
8855	8855	pytorch_lightning		[]		0	29	0	zephyruszx	pytorch-lightning
8856	8856	hihihi		[]		0	12	0	abhi6333	hihihi
8857	8857	SundeepDataset		[]		0	4	0	sundeepcheruku	sundeepdataset
8858	8858	RankSRGAN		[]		0	23	0	jn6213113115wt1	ranksrgan
8859	8859	okokokok		[]		0	26	0	abhi6333	okokokok
8860	8860	Enterpret_absa		[]		2	9	1	enkrish259	enterpret-absa
8861	8861	Unpredictability_What_Where_When		[]		0	59	0	vtsogli	unpredictability-what-where-when
8862	8862	deberta_large		[]		0	37	0	masatakaaoki	deberta-large
8863	8863	Walmart Dataset	Walmart Store Sales Prediction - Regression Problem	['beginner', 'linear regression', 'tabular data', 'regression', 'retail and shopping']	"Description:
One of the leading retail stores in the US, Walmart, would like to predict the sales and demand accurately. There are certain events and holidays which impact sales on each day. There are sales data available for 45 stores of Walmart. The business is facing a challenge due to unforeseen demands and runs out of stock some times, due to the inappropriate machine learning algorithm. An ideal ML algorithm will predict demand accurately and ingest factors like economic conditions including CPI, Unemployment Index, etc.
Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data. Historical sales data for 45 Walmart stores located in different regions are available.
Acknowledgements
The dataset is taken from Kaggle.
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the sales w.r.t single & multiple features.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	1727	11160	39	yasserh	walmart-dataset
8864	8864	Retinal OCT		['eyes and vision']		12	21	0	jeevankjk	retinal-oct
8865	8865	vehiclev1		[]		16	46	0	v1olet3	vehiclev1
8866	8866	Alzheimer Dataset with FCIE	Brain MRI dataset preprocessed using Fuzzy Color Image Enhancement Technique	['diseases', 'classification', 'cnn', 'image data', 'tensorflow']	"The original brain MRI dataset was re-created using the Fuzzy Color Image Enhancement (FCIE) algorithm.
The dataset contains four classes of images in training, validation as well as test set:
1. Mild Demented
2. Moderate Demented
3. Non Demented
4. Very Mild Demented
There are a total of 6400 MRI images with a train-validation-test split ratio of 75-15-10 percent."	12	134	0	gautamgc75	dataset-alzheimer-with-fcie
8867	8867	checkpoint100		[]		0	26	0	aditisahastrabudhe	checkpoint100
8868	8868	unknown_dataset		[]		2	37	0	shashwatnaidu	unknown-dataset
8869	8869	large_pth		[]		0	32	0	dragonzhang	large-pth
8870	8870	auto_preprocess_pyfile	auto preprocess py file 	['beginner', 'exploratory data analysis', 'numpy', 'pandas']		0	59	3	rhythmcam	auto-preprocess-pyfile
8871	8871	jigsaw2021-wat044-data		[]		0	11	0	wataoka	jigsaw2021-wat044-data
8872	8872	weiboData		[]		3	98	0	tianbaojie	weibodata
8873	8873	Apollo_1		[]		0	25	0	amogh0810	apollo-1
8874	8874	objectdetection2112121		[]		9	35	0	antonyjoseph21	objectdetection2112121
8875	8875	Police Department Incident Reports: 2018-2021	police incident reports filed by officers and by individuals 	['tabular data', 'text data']		10	84	3	rushikeshdarge	police-department-incident-reports-20182021
8876	8876	VOC-Super-Res		[]		0	19	0	koushik0901	vocsuperres
8877	8877	Petfinder-fastai-kf-11-single-pretrain-svr-meta		['animals']		2	9	0	bobber	petfinderfastaikf11singlepretrainsvrmeta
8878	8878	Miniddsm and Mias Cropped Images		[]		10	60	0	fereshtej	miniddsm-and-mias-cropped-images
8879	8879	Ravdess		['education']	"Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)
Speech audio-only files (16bit, 48kHz .wav) from the RAVDESS. Full dataset of speech and song, audio and video (24.8 GB) available from Zenodo. Construction and perceptual validation of the RAVDESS is described in our Open Access paper in PLoS ONE.
Check out our Kaggle Song emotion dataset.
Files
This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.
File naming convention
Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:
Filename identifiers
Modality (01 = full-AV, 02 = video-only, 03 = audio-only).
Vocal channel (01 = speech, 02 = song).
Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).
Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.
Statement (01 = ""Kids are talking by the door"", 02 = ""Dogs are sitting by the door"").
Repetition (01 = 1st repetition, 02 = 2nd repetition).
Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).
Filename example: 03-01-06-01-02-01-12.wav
Audio-only (03)
Speech (01)
Fearful (06)
Normal intensity (01)
Statement ""dogs"" (02)
1st Repetition (01)
12th Actor (12)
Female, as the actor ID number is even.
How to cite the RAVDESS
Academic citation
If you use the RAVDESS in an academic publication, please use the following citation: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.
All other attributions
If you use the RAVDESS in a form other than an academic publication, such as in a blog post, school project, or non-commercial product, please use the following attribution: ""The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)"" by Livingstone & Russo is licensed under CC BY-NA-SC 4.0."	13	18	0	dejolilandry	ravdess
8880	8880	Foundational Learning	% of children achieving minimum proficiency in reading and numeracy	['literature', 'education', 'tabular data', 'primary and secondary schools']	"Content
&gt; This dataset is based on foundational learning skills and contains the percentage of children achieving minimum proficiency in 
- reading: 
The child is considered to have foundational reading skills if s/he succeeds in: 
  1. word recognition, 
  2. literal questions, and 
  3. inferential questions
&gt;- numeracy. 
The child is considered to have foundational numeracy skills if s/he succeeds in: 
  1. number reading, 
  2. number discrimination, 
  3. addition, and 
  4. pattern recognition, 
| Region, Sub-region | UNICEF regions and UNICEF Sub-regions  |
|--------------------|----------------------------------------|
| EAP                | East Asia and the Pacific              |
| ECA                | Europe and Central Asia                |
| EECA               | Eastern Europe and Central Asia        |
| ESA                | Eastern and Southern Africa            |
| LAC                | Latin America and the Caribbean        |
| MENA               | Middle East and North Africa           |
| NA                 | North America                          |
| SA                 | South Asia                             |
| SSA                | Sub-Saharan Africa                     |
| WCA                | West and Central Africa                |
Acknowledgements
&gt; I'd like to thank UNICEF for aggregating this data!"	97	857	13	ruchi798	foundational-learning
8881	8881	BerlinEmoDB		['social science']	"Berlin Database of Emotional Speech
General information
As a part of the DFG funded research project SE462/3-1 in 1997 and 1999 we recorded a database of emotional utterances spoken by actors. The recordings took place in the anechoic chamber of the Technical University Berlin, department of Technical Acoustics. Director of the project was Prof. Dr. W. Sendlmeier, Technical University of Berlin, Institute of Speech and Communication, department of communication science. Members of the project were mainly Felix Burkhardt, Miriam Kienast, Astrid Paeschke and Benjamin Weiss.
More information about the Berlin Emotional Speech Database and the analysis results you will find in the several publications mentioned here.
For details about the usage of this database read section Emo-DB.
Elements of the navigation frame
Home
Emo-DB
Download
Documentation
Results of Analyses and Perception Test
Contact
Home
With a click on this link you come back to the very beginning. At this site you can choose between normal (1024x768 pixel) and high resolution (1280x1024 pixel) in order to get the best viewing results for your screen size.
Emo-DB
This section explains how to use this website to see what is in the database and how to configure the layout of the presentation of the various kinds of information included in this speech database.
In short there are the sound files itself, the label files (syllable label files and phone label files), information about the results of different perception tests (including the recognition of emotions, the evaluation of naturalness, the syllable stress and the strength of the displayed emotions) as well as some results of the measurements of fundamental frequency, energy, loudness, duration, stress and rhythm.
How to start:
After a click on Emo-DB (in the navigation frame on top) you will see a new frame here. There you have the possibility to choose speaker, text and emotion. The database will show you all the utterances available fulfilling your request. Since the database contains more than 500 utterances be careful not to choose all speakers, all texts and all emotions if you do not want to wait too long for a response.
The result of your request will be shown after clicking on the button ""...show!"".
You will see a table containing the following information:
column: a number (continuous)
2.-6. column: the results from the perception tests:
percentage of correct recognition of emotion
percentage of people who thought that the displayed emotion is performed reasonably convincing
the emotion recognition of every single participant in the perception experiment (especially interesting if you want to know with which other emotions the utterance has been confused)
emotional strength (1= very weak emotion, 7= very strong emotion)
standard deviation of the afore mentioned value
column: a pho-File containing the original F0- and duration values for synthesizing this specific file with MBROLA
column: the letters P - L - A - Y: you can click on each of them to hear ...
(P) the original sound
(L) the MBROLA resynthesized version (with the original F0 contour)
(A) a version with stylized F0 contour resynthesized with MBROLA according to the stylization algorithm by D'Alessandro & Mertens
(Y) another resynthesized audio file with a stylized F0 contour according to an algorithm written by Sascha Fagel
column: a button to click to see the graphic of the utterance (according to the display configuration shown on the right side)
In the right frame you can configure the display of the chosen utterance according to your needs.
Note: After changing an option to see the effect you will have to reload the configuration AND click again on the button with the name of the utterance.
Options for configuring the graphic display:
Scaling:
real = time scale is the same for all utterances
perc = timeline is scaled so that the whole utterance fits in the window size
Label:
none = no label files are shown
|||| = only borders between syllables are shown
|a|b|c| = borders and syllable labels are shown
a,b,c = borders are only shown between syllable labels, not over the complete window height (try and look - you will see)
Stress:
you can only choose if you want to see it or not
the yellow bars indicate the stress level of each syllable (average value from the perception test);
units are from 0 to 3 (according to unstressed, normal stressed, strongly stressed and emphatically stressed)
thin dark-red lines constitute the reticule with lines at the values 1 (lowest one), 2 (middle one) and 3 (highest one)
F0:
normal = F0 values are shown as measured (no values at voiceless parts)
interpolated = F0 values are shown with intermediate values (calculated by linear interpolation)
Stylization:
none = no F0 stylization will be displayed
spline = F0 stylization with spline functions will be displayed (programmed by Sascha Fagel)
linear = linear F0 stylization will be displayed (stylization method developed by D'Alessandro & Mertens)
gliss. threshold:
differential glissando threshold is a parameter used for the stylization algorithm by D'Alessandro & Mertens), a higher value results in less chunks, a lower value produces a more detailed stylization and therefore more chunks
diff. gliss. threshold:
differential glissando threshold (another parameter used for the stylization algorithm by D'Alessandro & Mertens), lower value = more chunks
Trend:
linear global trend (=regression line) and the slope of it will be displayed
Histograms:
F0 histogram of the utterance will be displayed
if you are especially interested in the histograms you can get a better view (with units marked) under ""results - histograms""
Energy:
a dark blue curve representing the measured energy values is shown
Loudness:
a blue curve representing the calculated loudness values is shown, the algorithm used for loudness calculation has been developed by Zwicker (see Zwicker, Fastl (1990): ""Psychoacoustics"")
Rhythm Events:
if you choose one of the letters A-H red dots (mostly on maxima of the loudness curve) representing the calculated rhythm events are shown
the algorithm used for calculation of the rhythm events is also based on a method developed by Zwicker (see the aformentioned book, chapter ""Rhythm"", page 245f.)
letters A to H represent different reference values for the loudness maximum which is needed for calculation
A - reference value equals the loudness maximum of every single utterance
B - reference value equals the loudness maximum of a specific text and a specific emotion (all speakers)
C - reference value equals the loudness maximum of the respective text (all speaker, all emotions)
D - reference value equals the loudness maximum of the respective emotion (all texts, all speakers)
E - reference value equals the loudness maximum of all utterances (max. of whole database)
F - reference value equals the loudness maximum of a specific text and a specific speaker (all emotions)
G - reference value equals the loudness maximum of the respective speaker (all texts, all emotions)
H - reference value equals the loudness maximum of a specific emotion and a specific speaker (all texts)
Download
There you will find the possibility (not now but soon!) to download the audio and label files of this database. You can use it for your own analyses as long as you point out the origin of the data correctly. However you will not be able to download and install this graphical web interface for local installation.
Documentation
Its the page you are reading now. If you want to know more let me know by email.
Results of Analyses and Perception Test
There you will find a few of our analysis results regarding measurements of fundamental frequency, duration and stress. You can see histograms of the fundamental frequency of one or more utterances at the same time in different scales (linear scale in Hz and a logarithmic scale in semi tones). Furthermore you can see the results of one of the perception tests which included the recognition of emotions and the naturalness of the utterances.
I hope this is self-explaining - if not, ask me for explanation.
Contact
On this site you will find a contact address. If you would like to know more, if you have questions or want to make comments do not hesitate to ring or mail us.
Additional Information
Every utterance is named according to the same scheme:
Positions 1-2: number of speaker
Positions 3-5: code for text
Position 6: emotion (sorry, letter stands for german emotion word)
Position 7: if there are more than two versions these are numbered a, b, c ....
Example: 03a01Fa.wav is the audio file from Speaker 03 speaking text a01 with the emotion ""Freude"" (Happiness).
Information about the speakers
03 - male, 31 years old
08 - female, 34 years
09 - female, 21 years
10 - male, 32 years
11 - male, 26 years
12 - male, 30 years
13 - female, 32 years
14 - female, 35 years
15 - male, 25 years
16 - female, 31 years
Code of texts
code    text (german)   try of an english translation
a01 Der Lappen liegt auf dem Eisschrank.    The tablecloth is lying on the frigde.
a02 Das will sie am Mittwoch abgeben.   She will hand it in on Wednesday.
a04 Heute abend könnte ich es ihm sagen.    Tonight I could tell him.
a05 Das schwarze Stück Papier befindet sich da oben neben dem Holzstück.    The black sheet of paper is located up there besides the piece of timber.
a07 In sieben Stunden wird es soweit sein.  In seven hours it will be.
b01 Was sind denn das für Tüten, die da unter dem Tisch stehen? What about the bags standing there under the table?
b02 Sie haben es gerade hochgetragen und jetzt gehen sie wieder runter. They just carried it upstairs and now they are going down again.
b03 An den Wochenenden bin ich jetzt immer nach Hause gefahren und habe Agnes besucht.  Currently at the weekends I always went home and saw Agnes.
b09 Ich will das eben wegbringen und dann mit Karl was trinken gehen.   I will just discard this and then go for a drink with Karl.
b10 Die wird auf dem Platz sein, wo wir sie immer hinlegen. It will be in the place where we always store it.
Code of emotions:
letter  emotion (english)   letter  emotion (german)
A   anger   W   Ärger (Wut)
B   boredom L   Langeweile
D   disgust E   Ekel
F   anxiety/fear    A   Angst
H   happiness   F   Freude
S   sadness T   Trauer
N = neutral version"	3	60	0	dejolilandry	berlinemodb
8882	8882	w7s3v2final		[]		0	19	0	linhtrngxun	w7s3v2final
8883	8883	w7s4v2final		[]		2	6	0	linhtrngxun	w7s4v2final
8884	8884	w7s2v2		[]		0	4	0	kindyatrng	w7s2v2
8885	8885	w7s1v2		[]		0	43	0	kindyatrng	w7s1v2
8886	8886	dog_classification0.0		[]		0	37	0	yuezongjie	dog-classification00
8887	8887	Kickstarter Projects - KASDD		['finance']		1	41	0	ekojsalim	kickstarter-projects-kasdd
8888	8888	styledd		[]		0	47	0	yangxingyue	styledd
8889	8889	utkface-tfrec		[]		1	41	0	darshgandhi	utkfacetfrec
8890	8890	pawpularityscorefront_tfrecord_fold_5-9		[]		0	7	0	nizhen	pawpularityscorefront-tfrecord-fold-59
8891	8891	pawpularityscorefront_tfrecord_fold_0-4		[]		0	14	0	nizhen	pawpularityscorefront-tfrecord-fold-04
8892	8892	Bert_uty		[]		0	26	0	tensorchoko	bert-uty
8893	8893	Jigsaw_finetune_data		[]		0	36	0	qianghanshao	jigsaw-finetune-data
8894	8894	Bert_base		[]		0	18	0	tensorchoko	bert-base
8895	8895	test data csv	just made up data in excel	[]		1	62	0	pbatch21	test-data-csv
8896	8896	keras_models		[]		1	26	0	ranabedir	keras-models
8897	8897	vgg16_data		[]		2	18	0	ranabedir	vgg16-data
8898	8898	TestData		['business']		0	3	0	pbatch21	testdata
8899	8899	Crypto Fear and Greed Index	The Fear & Greed Index for Bitcoin and other cryptocurrencies (alternative.me).	['investing', 'currencies and foreign exchange']	"Crypto Fear and Greed Index
Each day, the website https://alternative.me/crypto/fear-and-greed-index/ publishes this index based on analysis of emotions and sentiments from different sources crunched into one simple number: The Fear & Greed Index for Bitcoin and other large cryptocurrencies.
Why Measure Fear and Greed?
The crypto market behaviour is very emotional. People tend to get greedy when the market is rising which results in FOMO (Fear of missing out). Also, people often sell their coins in irrational reaction of seeing red numbers. With our Fear and Greed Index, we try to save you from your own emotional overreactions. There are two simple assumptions:
Extreme fear can be a sign that investors are too worried. That could be a buying opportunity.
When Investors are getting too greedy, that means the market is due for a correction.
Therefore, we analyze the current sentiment of the Bitcoin market and crunch the numbers into a simple meter from 0 to 100. Zero means ""Extreme Fear"", while 100 means ""Extreme Greed"". See below for further information on our data sources.
Data Sources
We are gathering data from the five following sources. Each data point is valued the same as the day before in order to visualize a meaningful progress in sentiment change of the crypto market.
First of all, the current index is for bitcoin only (we offer separate indices for large alt coins soon), because a big part of it is the volatility of the coin price.
But let’s list all the different factors we’re including in the current index:
Volatility (25 %)
We’re measuring the current volatility and max. drawdowns of bitcoin and compare it with the corresponding average values of the last 30 days and 90 days. We argue that an unusual rise in volatility is a sign of a fearful market.
Market Momentum/Volume (25%)
Also, we’re measuring the current volume and market momentum (again in comparison with the last 30/90 day average values) and put those two values together. Generally, when we see high buying volumes in a positive market on a daily basis, we conclude that the market acts overly greedy / too bullish.
Social Media (15%)
While our reddit sentiment analysis is still not in the live index (we’re still experimenting some market-related key words in the text processing algorithm), our twitter analysis is running. There, we gather and count posts on various hashtags for each coin (publicly, we show only those for Bitcoin) and check how fast and how many interactions they receive in certain time frames). A unusual high interaction rate results in a grown public interest in the coin and in our eyes, corresponds to a greedy market behaviour.
Surveys (15%) currently paused
Together with strawpoll.com (disclaimer: we own this site, too), quite a large public polling platform, we’re conducting weekly crypto polls and ask people how they see the market. Usually, we’re seeing 2,000 - 3,000 votes on each poll, so we do get a picture of the sentiment of a group of crypto investors. We don’t give those results too much attention, but it was quite useful in the beginning of our studies. You can see some recent results here.
Dominance (10%)
The dominance of a coin resembles the market cap share of the whole crypto market. Especially for Bitcoin, we think that a rise in Bitcoin dominance is caused by a fear of (and thus a reduction of) too speculative alt-coin investments, since Bitcoin is becoming more and more the safe haven of crypto. On the other side, when Bitcoin dominance shrinks, people are getting more greedy by investing in more risky alt-coins, dreaming of their chance in next big bull run. Anyhow, analyzing the dominance for a coin other than Bitcoin, you could argue the other way round, since more interest in an alt-coin may conclude a bullish/greedy behaviour for that specific coin.
Trends (10%)
We pull Google Trends data for various Bitcoin related search queries and crunch those numbers, especially the change of search volumes as well as recommended other currently popular searches. For example, if you check Google Trends for ""Bitcoin"", you can’t get much information from the search volume. But currently, you can see that there is currently a +1,550% rise of the query „bitcoin price manipulation“ in the box of related search queries (as of 05/29/2018). This is clearly a sign of fear in the market, and we use that for our index.
There's a story behind every dataset and here's your opportunity to share yours.
Copyright disclaimer
This dataset is produced and maintained by the administrators of https://alternative.me/crypto/fear-and-greed-index/.
This published version is an unofficial copy of their data, which can be also collected using their API (e.g., GET https://api.alternative.me/fng/?limit=10&format=csv&date_format=us)."	25	153	3	adelsondias	crypto-fear-and-greed-index
8900	8900	lda-data		[]		0	30	0	roxanneylin	ldadata
8901	8901	Listed company info: e.g. Symbol, Company name..		['finance', 'banking', 'investing']		9	131	1	coronatianmao	listed-company-info-eg-symbol-company-name
8902	8902	Central bank interest rate for each country		['finance', 'banking', 'investing']		1	65	1	coronatianmao	central-bank-interest-rate-for-each-country
8903	8903	mof2_classifier		[]		4	19	0	honihitak	mof2-classifier
8904	8904	my_pf_dataset5		[]		0	41	0	jukijuki	my-pf-dataset5
8905	8905	PetFinder-Model46		[]		2	37	0	lftuwujie	petfindermodel46
8906	8906	KSLC: Dataset	Kenyan Sign Language Classification Dataset	[]		2	18	0	yonaschanie	kslc-dataset
8907	8907	NFLdictionaries		[]		0	24	0	kaggledoer	nfldictionaries
8908	8908	bellabeat		[]		0	14	0	kumrnish	bellabeat
8909	8909	Euroleague All-time Players	Personal information about all-time Euroleague players	['basketball', 'sports']	"Context
The idea of scraping these data comes from a personal interest in players date of birth. However it may be expanded in future with other players' statistics available on the Euroleague website."	1	52	1	rusiano	euroleague-alltime-players
8910	8910	Cyclistic bike-share datasets 11/20 - 10/21	Google Capstone Case Study 	['data cleaning', 'data analytics', 'text data', 'dplyr', 'ggplot2']	"cyclisticbikeshare
About the company 
•   In 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime. 
•   Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments. One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers who purchase annual memberships are Cyclistic members. 
•   Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs. 
•   Moreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends. 
Content
The data has been made available by ‘Motivate International Inc’ under Data License Agreement. For more details click here 
Data sets has been downloaded from:  https://divvy-tripdata.s3.amazonaws.com/index.html 
Provided data type – internal, first party data, original, current, trustworthy 
Cyclistic’s historical trip data of last 12 months (01.11.2020 -10.30.2021) was downloaded for analysis (12 csv files)
Saved CSV files and XLS files in different folders
Data was stored locally, and copies of every dataset was stored in google drive, in case I need to access original data quickly from any device
Data sets contain the same number of columns, same names and same data for easy merging
Limitations: Financial information and identity information is not available, so I won’t be able to analyze casual or member customers’ financial data, to create financially vining offer for casual customers to convert them into members
cyclisticbikeshare
About the company 
•   In 2016, Cyclistic launched a successful bike-share offering. Since then, the program has grown to a fleet of 5,824 bicycles that are geotracked and locked into a network of 692 stations across Chicago. The bikes can be unlocked from one station and returned to any other station in the system anytime. 
•   Until now, Cyclistic’s marketing strategy relied on building general awareness and appealing to broad consumer segments. One approach that helped make these things possible was the flexibility of its pricing plans: single-ride passes, full-day passes, and annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Customers who purchase annual memberships are Cyclistic members. 
•   Cyclistic’s finance analysts have concluded that annual members are much more profitable than casual riders. Although the pricing flexibility helps Cyclistic attract more customers, Moreno believes that maximizing the number of annual members will be key to future growth. Rather than creating a marketing campaign that targets all-new customers, Moreno believes there is a very good chance to convert casual riders into members. She notes that casual riders are already aware of the Cyclistic program and have chosen Cyclistic for their mobility needs. 
•   Moreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members. In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ, why casual riders would buy a membership, and how digital media could affect their marketing tactics. Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends. 
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Analyzing how company’s annual members and casual riders use Cyclistic bikes differently to identify trends and convert casual riders into annual members."	6	75	1	marikakuprava	cyclistic-bikeshare-datasets-1120-1021
8911	8911	steel_mask		[]		0	21	0	vigneshirtt	steel-mask
8912	8912	shs_train_df		[]		0	40	0	light367	shs-train-df
8913	8913	moscow_sports_grounds_2021		[]		1	33	0	asdorzhiev2021	moscow-sports-grounds-2021
8914	8914	datasetImport		[]		0	9	0	anthonydupre0411	datasetimport
8915	8915	VisonData PLN banknotes 2021 	VisonData PLN banknotes 2021 VisonData PLN banknotes 2021 	['currencies and foreign exchange']		0	67	0	theatm	visondata-pln-banknotes-2021
8916	8916	butterflies		['biology']		0	30	0	jesuz19	butterflies
8917	8917	pymatting		[]		0	35	0	sedatgolgyaz	pymatting
8918	8918	Ambulance_Car_Data		[]		2	26	0	chaosblackdragon	ambulance-car-data
8919	8919	detectron2-0.6-cu111-cp37m-wheel		['automobiles and vehicles']		0	39	0	ddmitry	detectron206cu111cp37mwheel
8920	8920	Tokenized DS (FEEDBACK INFER)	Tokenized with roberta FAST tokenizer	['arts and entertainment', 'nlp']	"Context
BIO (or IOB) for NER inference, tokenized with [(fast) RoBERTa ] (https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizerFast)
Competition = Feedback Prize
Content
Huggingface Dataset format, BIO labeled
load with
Dataset.load_from_disk('./working-dir-path/bio_DS_tokenized')
Acknowledgements
Based on N. Broad's bio format for NER notebook"	0	58	0	roeilevy	bio-ds-robertatokenizerfast-feedback-infer
8921	8921	PetFinder_large_224_lazy		[]		6	44	0	kingkong153	petfinder-large-224-lazy
8922	8922	spotify_dataset	spotify dataset with songs from various years including multiple features	[]		15	145	0	christinobarbosa	spotify-dataset
8923	8923	5 Celebrities Faces Dataset	Dataset for 5 Celebrities Faces	['celebrities', 'news']		1	34	0	seifreda	5-celebrities-dataset
8924	8924	Waymo-Open Training Dataset 13		[]		0	17	0	mohammedosama	waymo-open-training-dataset-13
8925	8925	Nfnets Keras	The code of the package nfnets-keras: https://github.com/ypeleg/nfnets-keras	['computer science', 'programming', 'computer vision', 'deep learning', 'image data', 'keras', 'python']	"Keras implementation of Normalizer-Free Networks and SGD - Adaptive Gradient Clipping
Paper: https://arxiv.org/abs/2102.06171.pdf
Original code: https://github.com/deepmind/deepmind-research/tree/master/nfnets
Do upvote this dataset if it helps your work!
Note: Huge Credit to this comment for the pytorch implementation this repository is based on.
Note: See this comment for a generic implementation for any optimizer as a temporary reference for anyone who needs it.
Installation
Install from PyPi:
pip3 install nfnets-keras
or install the latest code using:
pip3 install git+https://github.com/ypeleg/nfnets-keras
Usage
NFNetF Model
Use any of the NFNetF models like any other keras Model!
python
from nfnets_keras import NFNetF3
model = NFNetF3(include_top = True, num_classes = 10)
model.compile('adam', 'categorical_crossentropy')
model.fit(X, y)
WSConv2D
Use WSConv2D like any other keras.layer.
python
from nfnets_keras import WSConv2D
conv = Conv2D(16, 3)(l)
w_conv = WSConv2D(16, 3)(conv)
SGD_AGC - Adaptive Gradient Clipping
Similarly, use SGD_AGC like keras.optimizer.SGD
python
from nfnets_keras import SGD_AGC
model.compile( SGD_AGC(lr=1e-3), loss='categorical_crossentropy' )
TODO
[x] WSConv2D
[x] SGD - Adaptive Gradient Clipping
[x] Function to automatically replace Convolutions in any module with WSConv2d
[x] Documentation
[x] NFNets
[ ] NF-ResNets
Credit for the original pytroch implementation
https://github.com/vballoli/nfnets-pytorch
Cite Original Work
To cite the original paper, use:
@article{brock2021high,
  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},
  title={High-Performance Large-Scale Image Recognition Without Normalization},
  journal={arXiv preprint arXiv:},
  year={2021}
}"	15	489	6	yamqwe	nfnets-keras
8926	8926	Medium data science articles dataset	Data about 100k+ data science articles published on medium in 2020.	['arts and entertainment', 'data analytics', 'text data', 'online communities', 'social networks']	"Context
As a beginner data science writer, I found guides about how to optimize my Text,  but none of those
was specific about data science. So let's build it!
Content
The data was collected using this notebook
The dataset contains information about all Medium articles published in 2020 that contains one of the following tags:
Data Science
Machine Learning
Artificial Inteligence
Deep Learning
Data
Big Data
Analytics"	14	322	5	viniciuslambert	medium-data-science-articles-dataset
8927	8927	COTS-coco		[]		1	9	0	phoenix9032	cotscoco
8928	8928	Solana Data	Solana USD (SOL1-USD) | Currency in USD | 2020 - 2021	['business', 'finance', 'time series analysis', 'investing', 'currencies and foreign exchange', 'datetime']	"What is Solana?
Solana is a public blockchain platform. It is open-source and decentralized, with consensus achieved using proof of stake and proof of history. Its internal cryptocurrency is SOL. Bloomberg considers Solana to be ""a potential long-term rival for Ethereum"". Like Ethereum, Solana can interact with smart contracts. On 14 September 2021, the Solana blockchain went offline after a surge of transactions caused the network to fork, and different validators had different views on the state of the network. The network was successfully brought back online early on 15 September. The Solana platform has become increasingly popular as of October 2021 because of the perceived mining efficiency and scalability, i.e., the perception that the blockchain will be relatively easy to use when lots of computers are running it. Particularly, the perceived scalability leads many to think it will be an effective DeFi platform. Solana's scaling capability, comfortably handling roughly 50,000 transactions per second, is also exponentially faster than any major competitor.
Data Description
This dataset provides the history of daily prices of Solana. All the column descriptions are provided. Currency is USD."	102	1290	9	varpit94	solana-data
8929	8929	Waymo-Open Training Dataset 12		[]		0	58	0	mohammedosama	waymo-open-training-dataset-12
8930	8930	Cardio		['exercise']		0	32	0	shannonmcettrick	cardio
8931	8931	Best Data Science Courses - Udemy	“Torture the data, and it will confess to anything.”	['education', 'computer science', 'beginner', 'exploratory data analysis', 'data cleaning', 'data visualization', 'data analytics']	"This data contains top 160 best data science courses from udemy
A beginner friendly dataset to practice exploratory data analysis
Can be used as a reference link to courses
Tasks
- Clean the data
- Create meaningful visualizations
- Exploratory data analysis
- Identify the best data science course - justify why
- Try to create word clouds"	336	2469	21	suddharshan	best-data-science-courses-udemy
8932	8932	pt_longformer_base_4096	Pytorch - Huggingface Longformer Base 4096	[]		2	36	0	nasheqlbrm	pt-longformer-base-4096
8933	8933	llvmlite		[]		0	20	0	benihime91	llvmlite
8934	8934	Weighted-Boxes-Fusion		[]		0	29	0	benihime91	weightedboxesfusion
8935	8935	cell_segmentation		[]		3	7	0	henini	cell-segmentation
8936	8936	DatasetBinary		[]		0	29	0	sunnyyadav1309	datasetbinary
8937	8937	ArabicT5Tokenizer		[]		0	5	0	fadyelkbeer	arabict5tokenizer
8938	8938	checkpoint95		[]		0	56	0	aditisahastrabudhe	checkpoint95
8939	8939	US Restaurants Post Covid Uncleaned Dataset		['restaurants']		5	44	0	fahadsyed97	us-restaurants-post-covid-uncleaned-dataset
8940	8940	World Happiness Report Data Analytics 2021	World Happiness Report 2021	['religion and belief systems']		3	75	0	yequantan	whr2021
8941	8941	Auto MPG Dataset	Prediction of Auto MPG - ML Regression Dataset	['automobiles and vehicles', 'beginner', 'linear regression', 'tabular data', 'regression']	"Description:
The data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository
""The data concerns city-cycle fuel consumption in miles per gallon,
to be predicted in terms of 3 multivalued discrete and 5 continuous
attributes."" (Quinlan, 1993)
Number of Instances: 398
Number of Attributes: 9 including the class attribute
Acknowledgements
Dataset: UCI Machine Learning Repository<br>
Data link : https://archive.ics.uci.edu/ml/datasets/auto+mpg
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the sales w.r.t a single & multiple features.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	79	538	7	yasserh	auto-mpg-dataset
8942	8942	sonar.csv		['software']		0	27	0	mdrahatzaman	sonarcsv
8943	8943	Sartorius-733-20211225154052		[]		0	16	0	hideyukizushi	sartorius-733-20211225154052
8944	8944	Sociolla product		['data visualization', 'data analytics', 'e-commerce services', 'pandas', 'plotly']	"I made the description in ""Overview"" notebook"	1	124	1	jalall	sociolla-product
8945	8945	Data Surat Pembaca Pelanggan iBox	Apakah layanan after sales service iBox bagus?	['people', 'people and society', 'business', 'text mining', 'text data']	"Context
This file contains public complaints from 3 different websites. I hope to find insights regarding the Apple's iBox after-sales services In Indonesia.
Content
This data only contains 5 Columns
- Nama
- Websites (this indicates about where the data is coming from)
- Judul (Contains the articles title)
- Tanggal (Dates)
- Isi (The articles content)
Acknowledgements
This project is only for learning purposes, I crawled all the data by myself using Scrapy
Inspiration
This data is open for everyone, so feel free to add some insights!"	6	60	0	wimaputra	data-surat-pembaca-pelanggan-ibox
8946	8946	online_retail_ll		[]		0	6	0	omerparlak	online-retail-ll
8947	8947	online_retail_ll		[]		0	6	0	omerparlak	online-retail-ll
8948	8948	gg;kfjdklh		[]		0	32	0	ziadelsoudy	ggkfjdklh
8949	8949	my_super_pkl		[]		0	33	0	kotashimomura	my-super-pkl
8950	8950	US Covid-19 Dataset		[]		0	11	0	fahadsyed97	us-covid19-dataset
8951	8951	checkpoint90		[]		0	16	0	aditisahastrabudhe	checkpoint90
8952	8952	dataset_diabetic_01		[]		0	18	0	prathyusha158m	dataset-diabetic-01
8953	8953	diabetic_dataset		[]		5	45	1	prathyusha158m	diabetic-dataset
8954	8954	Advertising Sales Dataset	Advertising Budget & Sales Prediction using Rregression	['business', 'marketing', 'beginner', 'linear regression', 'tabular data']	"Description:
The advertising dataset captures the sales revenue generated with respect to advertisement costs across multiple channels like radio, tv, and newspapers. 
It is required to understand the impact of ad budgets on the overall sales.
Acknowledgement:
The dataset is taken from Kaggle
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the sales w.r.t a single & multiple features.
Also evaluate the models & compare their respective scores like R2, RMSE, etc."	330	2137	12	yasserh	advertising-sales-dataset
8955	8955	covidtweetsdatanew		[]		0	5	0	mlcovidresearch	covidtweetsdatanew
8956	8956	maskrcnn20		[]		2	53	0	ccvipchenbin	maskrcnn20
8957	8957	Insurance Claims - Fraud Detection		['insurance']	Source: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4954928053318020/1058911316420443/167703932442645/latest.html	61	475	9	mykeysid10	insurance-claims-fraud-detection
8958	8958	final merge		[]		1	41	0	maideyildiz	final-merge
8959	8959	Drug200		[]		0	54	0	dinaouahbi	drug200
8960	8960	alzheimer_dataset		[]		5	78	1	parthoghosh	alzheimer-dataset
8961	8961	snapshots		[]		0	23	0	sifatshikdar	snapshots
8962	8962	hugeface_base_train		[]		0	20	0	dragonzhang	hugeface-base-train
8963	8963	keras_resnet		[]		0	2	0	sifatshikdar	keras-resnet
8964	8964	Penguins Data	Beginner Practice Set	['hockey']		2	23	0	ashishsk7	penguins-data
8965	8965	Prediction of House prices		[]		6	58	0	ravichandra498	prediction-of-house-prices
8966	8966	keras_retinanet		[]		0	32	0	sifatshikdar	keras-retinanet
8967	8967	inception_weights		[]		0	2	0	guizengyou	inception-weights
8968	8968	Orders	ecommerce dataset for transactions	['retail and shopping', 'e-commerce services']	"Context
e-commerce data
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	10	113	2	rahulaggarwal	orders
8969	8969	BikeShare datasets		[]		0	57	0	mohammedhassan20	bikeshare-datasets
8970	8970	Sartorius-Pre-732-20211225120612		[]		0	21	0	hideyukizushi	sartorius-pre-732-20211225120612
8971	8971	IMDV_6500+ movie information		['movies and tv shows']		3	42	0	mdkowsaralamshuvo	imdv-6500-movie-information
8972	8972	gan_dataset1		[]		0	30	0	antonioxv	gan-dataset1
8973	8973	VideoLikesDatasetClassification		[]		0	17	0	ziadelsoudy	videolikesdatasetclassification
8974	8974	trashcar		[]		0	5	1	tonysmallfish	trashcar
8975	8975	Yunos_dataset	This dataset is used to train & create Index of Yuno	['search engines', 'arts and entertainment', 'nlp', 'anime and manga']		5	113	1	iamparadox	yunos-dataset
8976	8976	Extensive Movie Dataset (reupload)	Movies with more than 100 votes taken from IMDb dataset	['movies and tv shows', 'tabular data']		11	101	1	arensonz	imdb-extensive-dataset-reupload
8977	8977	train_dataset		[]		0	11	0	jahnveenarang	train-dataset
8978	8978	PhysioNet-EEGMI		[]		1	30	0	fes2255	physioneteegmi
8979	8979	spotifyxyx		[]		0	44	0	arcute	spotifyxyx
8980	8980	QS World University Rankings 2017-2022	Discover the world’s top universities from 2017-2022.	['universities and colleges', 'education', 'exploratory data analysis']	"Acknowledgements
data source: https://www.topuniversities.com/university-rankings/world-university-rankings/2022
image credit: https://www.pexels.com/photo/newly-graduated-people-wearing-black-academy-gowns-throwing-hats-up-in-the-air-267885/"	996	5000	27	prasertk	qs-world-university-rankings-2021
8981	8981	petf_swin_large_508_model_384_metricfixed		[]		4	5	0	nhac43	petf-swin-large-508-model-384-metricfixed
8982	8982	best2440		[]		4	31	1	w3579628328	best2440
8983	8983	pf-yolov5-breed-data		[]		8	17	0	makotoikeda	pf-yolov5-breed-data
8984	8984	model120		[]		0	30	0	malekbadreddine	model120
8985	8985	Video-dataset-person	Person Counting_Person Attendance Realtime	[]		12	63	0	mirfana	videodatasetperson
8986	8986	PySpark Datasets		[]		17	126	1	rashid60	pyspark-datasets
8987	8987	Arabic App Reviews W2V		[]		0	31	0	aiaarisaac	arabic-app-reviews-w2v
8988	8988	CBNetV2 Swin Large HTC		[]		0	44	0	ubamba98	cbnetswinlargehtc
8989	8989	Resnet50 ( Weights and JSON)		[]		1	37	0	aanisha07	resnet50-weights-and-json
8990	8990	NST images		[]		1	52	0	derekcai	nst-images
8991	8991	What Do Men Think It Means To Be A Man?	Masculinity Survey: 200 Answers, Demographics, Technical information	['gender', 'social science', 'beginner', 'social issues and advocacy', 'social networks']	"About this dataset
&gt; <p>This directory contains data behind the story <a href=""https://fivethirtyeight.com/features/what-do-men-think-it-means-to-be-a-man"" target=""_blank"" rel=""nofollow"">What Do Men Think It Means To Be A Man?</a>.</p>
<p><code>masculinity-survey.csv</code> contains the results of a survey of 1,615 adult men conducted by SurveyMonkey in partnership with FiveThirtyEight from May 10-22, 2018. The modeled error estimate for this survey is plus or minus 2.5 percentage points. The percentages have been weighted for age, race, education, and geography using the Census Bureau’s American Community Survey to reflect the demographic composition of the United States age 18 and over. Crosstabs with less than 100 respondents have been left blank because responses would not be statistically significant.</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 200 samples along with Adult Men, No Children, technical information and other features such as:
- Age 35 64
- Race White
- and more.
How to use this dataset
&gt; - Analyze Sexual Orientation Gay/ Bisexual in relation to Has Children
- Study the influence of Race Non White on Age 18 34
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	108	1331	6	yamqwe	masculinity-surveye
8992	8992	Political Elasticity Scores	The House Districts That Swing The Most (And Least) With The National Mood	['politics', 'social science', 'beginner', 'news', 'social networks']	"About this dataset
&gt; <p>This folder contains the data behind the story <a href=""https://fivethirtyeight.com/features/election-update-the-house-districts-that-swing-the-most-and-least-with-the-national-mood/"" target=""_blank"">Election Update: The House Districts That Swing The Most (And Least) With The National Mood</a>.</p>
<p>An elasticity score measures how sensitive a state or district it is to changes in the national political environment.</p>
<p><code>elasticity-by-state.csv</code> contains the elasticity scores for each state and the District of Columbia.</p>
<p><code>elasticity-by-district.csv</code> contains the elasticity scores for all 435 congressional districts.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by data.world's Admin and contains around 100 samples along with Elasticity, Elasticity, technical information and other features such as:
- Elasticity
- Elasticity
- and more.
How to use this dataset
&gt; - Analyze Elasticity in relation to Elasticity
- Study the influence of Elasticity on Elasticity
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	26	330	3	yamqwe	political-elasticity-scorese
8993	8993	File1.csv		['programming']		0	28	0	aniketprashantgiri	file1csv
8994	8994	TPS-Dec public notebooks		['business']		14	29	2	yamqwe	tpsdec-public-notebooks
8995	8995	Frequency		[]		1	39	0	abhishekkumar010	frequency
8996	8996	Star Trek Lines	All Star Trek lines from The Original Series to Enterprise	['popular culture', 'literature', 'movies and tv shows', 'people', 'naive bayes', 'text data']	"Star Trek Lines
All Star Trek quotes from The Original Series to Enterprise, in various grouping methods. View the GitHub repo here to see how it was done!
lines_by_character.json is a JSON object with the key being the character's name and the value an array of lines they have spoken in any series of Star Trek.
lines_by_episode.json is a JSON object with the key being the short name of the series (e.g. TOS, TNG) and the value being details of the series, and lines grouped by episode number.
raw_lines.json is just all lines combined into one JSON array.
This data was scraped from chakoteya.net."	7	233	2	danielohanessian	star-trek-lines
8997	8997	Species		['biology']		1	34	0	mjazzy	species
8998	8998	mof2_estimator_r2		[]		2	9	0	honihitak	mof2-estimator-r2
8999	8999	image_ma		[]		0	3	0	amogh0810	image-ma
9000	9000	loan_dataset		[]		0	31	1	irvanmangolo	loan-dataset
9001	9001	TESLA Stock Data	TESLA Inc. (TSLA) | NasdaqGS Real Time Price | Currency in USD	['finance', 'time series analysis', 'investing']	"What is TESLA?
Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California. Tesla's current products include electric cars, battery energy storage from home to grid-scale, solar panels and solar roof tiles, as well as other related products and services. 
Information about this dataset
This dataset provides historical data of TESLA INC. stock (TSLA). The data is available at a daily level. Currency is USD."	837	4222	25	varpit94	tesla-stock-data-updated-till-28jun2021
9002	9002	IISE_2022_Fleet FDD	Synthetic_Data_Experiment	[]		0	52	1	nupirl	iise-2022-fleet-fdd
9003	9003	output		[]		0	20	0	maideyildiz	output
9004	9004	scaai_eval_savedmodels		[]		0	48	0	adityakane	scaai-eval-savedmodels
9005	9005	cyclistic case study - from Apr 2020 to Mar 2021	Google Analytic Capstone	['business']		0	10	0	hoinhi	cyclistic-case-study-from-apr-2020-to-mar-2021
9006	9006	Boston-Housing-Dataset	The boston housing dataset with column names.	['education', 'computer science', 'regression', 'social issues and advocacy']	"Context
This is a copy of the original Boston Housing Dataset.  As of December 2021, the original link doesn't contain the dataset so I'm uploading it if anyone wants to use it.
I'll implement a linear regression model to predict the output 'MEDV' variable using PyTorch (check the companion notebook).
I took the data given in this link and processed it to include the column names as well.
Acknowledgements
https://www.kaggle.com/prasadperera/the-boston-housing-dataset/data
Inspiration
Good luck on your data science career :)"	66	501	2	simpleparadox	bostonhousingdataset
9007	9007	Text_PreProccessing_Part001_Part012_Part023		[]		2	29	0	hiiammrcat	text-preproccessing-part001-part012-part023
9008	9008	50 Years Of World Cup Doppelgangers	How every athlete played in every men’s World Cup from 1966 to 2018 	['football', 'sports', 'history', 'social science', 'beginner']	"About this dataset
&gt; <p>This file contains links to the data behind <a href=""https://projects.fivethirtyeight.com/world-cup-comparisons/"" target=""_blank"">50 Years Of World Cup Doppelgangers</a>.</p>
<p><code>world_cup_comparisons.csv</code> contains all historical players and their associated z-score for each of the 16 metrics.</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 6000 samples along with Fouls Z, Crosses Z, technical information and other features such as:
- Clearances Z
- Blocks Z
- and more.
How to use this dataset
&gt; - Analyze Boxtouches Z in relation to Fouled Z
- Study the influence of Nsxg Z on Team
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	48	397	3	yamqwe	world-cup-comparisonse
9009	9009	merged		[]		0	32	0	maideyildiz	merged
9010	9010	Harry_Potter_movie_BO_Collection		[]		1	10	0	sandhiyakumar	harry-potter-movie-bo-collection
9011	9011	The LeBron James Decision-Making Machine	The LeBron James Decision-Making Machine	['basketball', 'sports', 'history', 'social science', 'beginner']	"About this dataset
&gt; <p>This folder contains the data behind the story <a href=""https://fivethirtyeight.com/features/the-lebron-james-decision-making-machine/"" target=""_blank"" rel=""nofollow"">The LeBron James Decision-Making Machine</a>.</p>
<p><code>lebron.xslx</code> contains the data used to create the hypothetical depth charts for what every NBA team might look like with LeBron James. Each player carries a rating on offense and on defense, based on our CARMELO projection system. For team projections, a player is also allocated a certain number of minutes per game at each position, guided by CARMELO's playing-time projections. Players with 0 allocated minutes are no longer on the team in question, and are indicated with orange cells. The spreadsheet indicates a team's projected payroll after adding LeBron, compared with the NBA salary cap and luxury-tax thresholds. Players with salaries listed under ""$$$ Shed"" have left the team, for a reason indicated to the right. (Players departing for special reasons, such as the stretch provision, are color coded when applicable.) A team's projected W-L record is generated from its players' ratings and playing-time projections, and that is used (along with the team's average age and the wins added by its best player) to calculate the team's odds of winning a championship over the next four seasons.""</p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 1000 samples along with Unnamed: 5, Unnamed: 3, technical information and other features such as:
- Https://www.basketball Reference.com/contracts/hou.html
- The Cavs Will Be Able To Offer Le Bron James A Projected Max Contract Of 5 Years, $205m This Summer; All Other Teams Can Offer 4 Years, $152m.
- and more.
How to use this dataset
&gt; - Analyze Unnamed: 10 in relation to Unnamed: 18
- Study the influence of Unnamed: 16 on Unnamed: 12
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	15	256	3	yamqwe	lebrone
9012	9012	Home Alone Twitter Dataset	Home Alone Dataset from Twitter	['asia', 'beginner', 'classification', 'text data', 'social networks']	"Description
Dataset merupakan data yang diambil dari twitter dengan pencarian ""Home Alone"" yang saat trending di twitter pada tanggal 25/12/2021.
Dataset yang dipaparkan merupakan data mentah tanpa pemrosesan sedikitpun."	19	217	4	fajriyan	home-alone-twitter-dataset
9013	9013	COVID-19 high risk individuals per ICU bed	How One High-Risk Community In Rural South Carolina Is Bracing For COVID-19	['healthcare', 'public health', 'health', 'beginner', 'health conditions']	"About this dataset
&gt; <p>This dataset contains the data behind the story <a href=""https://fivethirtyeight.com/features/how-one-high-risk-community-in-rural-south-carolina-is-bracing-for-covid-19/"" target=""_blank"" rel=""nofollow"">How One High-Risk Community In Rural South Carolina Is Bracing For COVID-19</a>.</p>
<p><a href=""https://data.world/fivethirtyeight/covid-19-high-risk-individuals-per-icu-bed/workspace/file?filename=mmsa-icu-beds.csv"">mmsa-icu-beds.csv</a> combines data from the Centers for Disease Control and Prevention’s <a href=""https://www.cdc.gov/brfss/smart/smart_2017.html"" target=""_blank"" rel=""nofollow"">Behavioral Risk Factor Surveillance System (BRFSS)</a>, a collection of health-related surveys conducted each year of more than 400,000 Americans, and the <a href=""https://khn.org/news/as-coronavirus-spreads-widely-millions-of-older-americans-live-in-counties-with-no-icu-beds/#lookup"" target=""_blank"" rel=""nofollow"">Kaiser Family Foundation</a> to show the number of people who are at high risk of becoming seriously ill from COVID-19 per ICU bed in each metropolitan area, micropolitan area or metropolitan division for which we have data.</p>
<p>Being high risk is defined by a number of health conditions and behaviors. Based on the CDC’s <a href=""https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-at-higher-risk.html?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fspecific-groups%2Fpeople-at-higher-risk.html"" target=""_blank"" rel=""nofollow"">list of the relevant underlying conditions</a> that put people at higher risk of serious illness from COVID-19, plus the advice of experts from the Cleveland Clinic, the American Lung Association and the American Heart Association, we counted people as at risk if they’re 65 or older; if they have ever been told they have hypertension, coronary heart disease, a myocardial infarction, angina, a stroke, chronic kidney disease, chronic obstructive pulmonary disease, emphysema, chronic bronchitis or diabetes; if they currently have asthma or a BMI over 40; if they smoke cigarettes every day or some days or use e-cigarettes or vaping products every day or some days; or if they’re currently pregnant. We included every individual who meets at least one of these conditions but counted them only once each, so anyone with multiple conditions doesn’t get counted multiple times. We were not able to include a number of conditions for which we did not have location-based data from the BRFSS, such as liver disease, having smoked, vaped or dabbed marijuana in the last 30 days, and getting cancer treatment or being on immunosuppression medications.</p>
<p>See the data dictionary for column descriptions.</p>
<p>If you find this information useful, please <a href=""mailto:data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><em><strong>License:</strong></em> <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a><br>
<em><strong>Source:</strong></em> <a href=""https://github.com/fivethirtyeight/data/tree/master/covid-geography"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/covid-geography</a></p>
This dataset was created by data.world's Admin and contains around 100 samples along with High Risk Per Icu Bed, Icu Beds, technical information and other features such as:
- Hospitals
- High Risk Per Hospital
- and more.
How to use this dataset
&gt; - Analyze Total Percent At Risk in relation to High Risk Per Icu Bed
- Study the influence of Icu Beds on Hospitals
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	93	702	4	yamqwe	covid-19-high-risk-individuals-per-icu-bede
9014	9014	US non-voters poll data	6000 collected samples, technical information	['united states', 'people', 'politics', 'beginner', 'social networks']	"About this dataset
&gt; <p>This dataset contains the data behind <a href=""https://projects.fivethirtyeight.com/non-voters-poll-2020-election/"" target=""_blank"" rel=""nofollow"">Why Many Americans Don't Vote</a>.</p>
<p>Data presented here comes from polling done by Ipsos for FiveThirtyEight, using Ipsos’s KnowledgePanel, a probability-based online panel that is recruited to be representative of the U.S. population. The poll was conducted from Sept. 15 to Sept. 25 among a sample of U.S. citizens that oversampled young, Black and Hispanic respondents, with 8,327 respondents, and was weighted according to general population benchmarks for U.S. citizens from the U.S. Census Bureau’s Current Population Survey <a href=""https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar19.pdf"" target=""_blank"" rel=""nofollow"">March 2019 Supplement</a>. The voter file company Aristotle then matched respondents to a voter file to more accurately understand their voting history using the panelist’s first name, last name, zip code, and eight characters of their address, using the <a href=""https://www.uspsoig.gov/document/national-change-address-program"" target=""_blank"" rel=""nofollow"">National Change of Address program</a> if applicable. Sixty-four percent of the sample (5,355 respondents) matched, although we also included respondents who did not match the voter file but described themselves as voting “rarely” or “never” in our survey, so as to avoid underrepresenting nonvoters, who are less likely to be included in the voter file to begin with. We dropped respondents who were only eligible to vote in three elections or fewer. We defined those who almost always vote as those who voted in all (or all but one) of the national elections (presidential and midterm) they were eligible to vote in since 2000; those who vote sometimes as those who voted in at least two elections, but fewer than all the elections they were eligible to vote in (or all but one); and those who rarely or never vote as those who voted in no elections, or just one.</p>
<p>The data included here is the final sample we used: 5,239 respondents who matched to the voter file and whose verified vote history we have, and 597 respondents who did not match to the voter file and described themselves as voting ""rarely"" or ""never,"" all of whom have been eligible for at least 4 elections.</p>
<p>If you find this information useful, please <a href=""mailto:data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>License:</em></strong> <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a></p>
<p><em><strong>Source:</strong></em> <a href=""https://github.com/fivethirtyeight/data/tree/master/non-voters"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/non-voters</a></p>
This dataset was created by data.world's Admin and contains around 6000 samples along with Race, Q27 6, technical information and other features such as:
- Q4 6
- Q8 3
- and more.
How to use this dataset
&gt; - Analyze Q10 3 in relation to Q8 6
- Study the influence of Q6 on Q10 4
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	39	339	4	yamqwe	us-non-voters-poll-datae
9015	9015	Omicron_rise		[]		4	75	1	sssain	omicron-rise
9016	9016	test111		[]		0	31	0	yehiahossam	test111
9017	9017	test_2		[]		1	48	0	yehiahossam	test-2
9018	9018	Titanic_compitition_datasets		[]		0	5	1	virajoak	titanic-compitition-datasets
9019	9019	PetFinder-Model42		[]		0	20	0	lftuwujie	petfindermodel42
9020	9020	cat_human_faces		[]	"Content
This dataset is roughly 11.6k images. 6000 of these images are (512, 512, 3) human faces resized (LANCZOS) from the (1024, 1024, 3) FFHQ human faces dataset located here. The remaining 5.6k images are cats and are also in the shape (512, 512, 3). They were not resized. They were taken from the AFHQ dataset located here.
This is a combined dataset between 2 previous datasets I've created: larxel_cat_faces and mini_ffhq_512."	8	40	2	vincenttu	cat-human-faces
9021	9021	Titanic1		[]		0	1	1	alexandramoraru1	titanic1
9022	9022	Titanic		[]		0	11	0	alexandramoraru1	titanic
9023	9023	Lethosos World Bank		['banking']		1	28	0	denestalk	lethosos-world-bank
9024	9024	ccpvc0712v21v2		[]		0	18	0	javadkhorramdel	ccpvc0712v21v2
9025	9025	Monthly Orders for snacks-Munchies Application	These are my monthly Orders of snacks from an application called Munchies	['beginner', 'intermediate', 'classification', 'tabular data', 'food']	"Context
I made these records from an App i used to order snacks
Content
These contain relevant information to my order along with some adjustment.
Acknowledgements
I thank the help of an important collegue.
Inspiration
I look forward to the information you may provide me in terms of spending or health etc. I look forward to all types of insights."	20	246	4	muhammadammarjamshed	monthly-orders-for-snacksmunchies-application
9026	9026	Jigsaw Toxic Comments all dataset		[]		1	50	1	ks2019	jigsaw-toxic-comments-all-dataset
9027	9027	Wikipedia talk labels		['business']		0	39	1	ks2019	wikipedia-talk-labels
9028	9028	BDL's Sayrafa exchange rates (USD to LBP)	Bank du Liban (BDL)'s Sayrafa exchange rates (USD to LBP)	['finance']		2	253	2	najielkotob	bdls-sayrafa-exchange-rates-usd-to-lbp
9029	9029	ccpvc0712v1v2		[]		0	0	0	javadkhorramdel	ccpvc0712v1v2
9030	9030	Waymo-Open Training Dataset 11		[]		0	27	0	mohammedosama	waymo-open-training-dataset-11
9031	9031	twitter dataset(Nigeria electricity		['africa', 'data visualization', 'electricity', 'pandas', 'seaborn']		8	31	1	kareemrasheed89	twitter-datasetnigeria-electricity
9032	9032	Sartorius CPs ens11		[]		23	24	0	theoviel	sartorius-cps-ens11
9033	9033	Cells Out of Sample (COOS-7) Dataset	A microscopy dataset for testing the generalization of classifiers	['earth and nature', 'biology', 'artificial intelligence', 'computer vision', 'classification', 'image data']		5	79	0	stanleyhua	coos-7
9034	9034	Chess Dataset from UCI Repo		['computer science']		0	24	3	bhavinmoriya	chess-dataset-from-uci-repo
9035	9035	ChangeToString		[]		0	28	0	ivanechen	changetostring
9036	9036	data_titanic		[]		0	8	0	orlovvlad	data-titanic
9037	9037	ccpvc0712m_v2		[]		0	0	0	javadkhorramdel	ccpvc0712m-v2
9038	9038	NSE-Cash.Market-Bhavcopies-2017	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	4	34	0	mayookhlad	nsecashmarketbhavcopies2017
9039	9039	NSE-Cash.Market-Bhavcopies-2015	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	36	0	mayookhlad	nsecashmarketbhavcopies2015
9040	9040	NSE-Cash.Market-Bhavcopies-2016	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	2	22	0	mayookhlad	nsecashmarketbhavcopies2016
9041	9041	NSE-Cash.Market-Bhavcopies-2014	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	4	38	0	mayookhlad	nsecashmarketbhavcopies2014
9042	9042	NSE-Cash.Market-Bhavcopies-2012	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	30	0	mayookhlad	nsecashmarketbhavcopies2012
9043	9043	NSE-Cash.Market-Bhavcopies-2011	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	33	0	mayookhlad	nsecashmarketbhavcopies2011
9044	9044	NSE-Cash.Market-Bhavcopies-2010	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	45	0	mayookhlad	nsecashmarketbhavcopies2010
9045	9045	NSE-Cash.Market-Bhavcopies-2009	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	40	0	mayookhlad	nsecashmarketbhavcopies2009
9046	9046	NSE-Cash.Market-Bhavcopies-2008	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	31	0	mayookhlad	nsecashmarketbhavcopies2008
9047	9047	NSE-Cash.Market-Bhavcopies-2007	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	1	6	0	mayookhlad	nsecashmarketbhavcopies2007
9048	9048	NSE-Cash.Market-Bhavcopies-2006	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	4	32	0	mayookhlad	nsecashmarketbhavcopies2006
9049	9049	NSE-Cash.Market-Bhavcopies-2005	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	39	0	mayookhlad	nsecashmarketbhavcopies2005
9050	9050	NSE-Cash.Market-Bhavcopies-2004	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	26	0	mayookhlad	nsecashmarketbhavcopies2004
9051	9051	NSE-Cash.Market-Bhavcopies-2003	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	40	0	mayookhlad	nsecashmarketbhavcopies2003
9052	9052	NSE-Cash.Market-Bhavcopies-2002	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	3	39	0	mayookhlad	nsecashmarketbhavcopies2002
9053	9053	PY-BigBird-v26		['computer science']		363	573	9	cdeotte	py-bigbird-v26
9054	9054	diabetes-health-dataset-classfaction-model		[]		8	123	0	nadiahajrasi	diabeteshealthdatasetclassfactionmodel
9055	9055	Rajesh Hamal and Other Nepali Actors	Image Dataset for Classification	['movies and tv shows', 'image data', 'news']	"Context
Rajesh Hamal to Nepal and Nepalese is what Chuck Norris is to Hollywood and Rajanikath is to India / Bollywood. There are countless jokes - some original and most translated/adapted his ""powers"". 
Content
This dataset includes 100 images of Rajesh Hamal and 100 other images of other random Nepali actor, each on separate folders.
Acknowledgements
Ritu helped me collect these images
Inspiration
Inspired by Jian Yang's Hot Dog detector app as seen in the TV series Silicon Valley"	2	54	0	aashishghimire	rajesh-hamal-and-other-nepali-actors
9056	9056	wine.data		[]		3	2	0	safarbekjon	winedata
9057	9057	NSE-Cash.Market-Bhavcopies-2001	NSE India's Bhavcopy dataset for Cash market	['india', 'finance', 'beginner', 'tabular data', 'investing']	"Context
Cash market data for analysis
Content
OHLC and Volume data for Equities
Acknowledgements
NSE India
https://www.nseindia.com/
Inspiration
Covid"	7	98	0	mayookhlad	nsecashmarketbhavcopies2001
9058	9058	sb_p4w7_224_in22k_classification_external		[]		0	1	0	xyzdivergence	sb-p4w7-224-in22k-classification-external
9059	9059	HCM_dataset		[]		0	4	0	saraomary	hcm-dataset
9060	9060	CYCLoPs Protein Localization Dataset	A yeast wt2 protein localization dataset	['biology', 'artificial intelligence', 'classification', 'image data']		0	64	0	stanleyhua	cyclops-protein-loc
9061	9061	checkpoint85		[]		0	35	0	aditisahastrabudhe	checkpoint85
9062	9062	Dataset For Measuring Cognitive Load		[]		3	43	0	mainulislammahi	dataset-for-measuring-cognitive-load
9063	9063	Arabic Motivational Quotes		[]		6	26	0	ninamaamary	arabic-motivational-quotes
9064	9064	Log4Shell Tweets	Tweets that mention the Log4Shell Java vulnerability	['computer science', 'internet', 'programming', 'social networks']	"Context
This data set contains tweets that mention Log4J and the Log4Shell vulnerability. Log4Shell (CVE-2021-44228) is a zero-day vulnerability in Log4j, a popular Java logging framework, involving arbitrary code execution.
Content
Data gathered through the Twitter API on December 24, 2021. The data set contains tweets from December 9, 2021 through December 24, 2021.
Inspiration
I wanted to see if interest in the vulnerability was subsiding yet, so I collected all of the tweets so far to find out."	72	681	19	bcruise	log4shell-tweets
9065	9065	NYC Taxi Trip Duration	Predict the NYC Taxi Trip Duration - Intermediate ML Project	['transportation', 'intermediate', 'linear regression', 'tabular data', 'regression']	"Description:
The competition dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this playground competition. Based on individual trip attributes, participants should predict the duration of each trip in the test set.
The datset contains the following fields:
id - a unique identifier for each trip
vendor_id - a code indicating the provider associated with the trip record
pickup_datetime - date and time when the meter was engaged
dropoff_datetime - date and time when the meter was disengaged
passenger_count - the number of passengers in the vehicle (driver entered value)
pickup_longitude - the longitude where the meter was engaged
pickup_latitude - the latitude where the meter was engaged
dropoff_longitude - the longitude where the meter was disengaged
dropoff_latitude - the latitude where the meter was disengaged
store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip
trip_duration - duration of the trip in seconds
Acknowledgement:
The dataset is taken from Kaggle:\
https://www.kaggle.com/c/nyc-taxi-trip-duration/data
Objective:
Understand the Dataset & cleanup (if required).
Build Regression models to predict the duration of taxi trip.
Also evaluate the models & compare thier respective scores like R2, RMSE, etc.!"	106	993	7	yasserh	nyc-taxi-trip-duration
9066	9066	More_complete_Pokemon_ALL_GEN_dataframe_2.0	All gen Pokedex (Semi-Complete)	['intermediate', 'data analytics', 'tabular data', 'anime and manga', 'pandas']		5	56	0	thomasbuddemberg	more-complete-pokemon-dataframe-20
9067	9067	FER2013-F		[]		3	50	0	mohammedaaltaha	fer2013f
9068	9068	Diabetes		['diabetes']		0	14	0	hendamia01	diabetes
9069	9069	sl_p4w7_224_in22k_classification_external		[]		0	1	0	xyzdivergence	sl-p4w7-224-in22k-classification-external
9070	9070	santa-solutions		['religion and belief systems']		10	70	2	chaudharypriyanshu	santasolutions
9071	9071	RAF_DB		[]		4	93	0	mohammedaaltaha	raf-db
9072	9072	Complete IPL Dataset	Team and Player wise Data of all top IPL teams	['games', 'cricket', 'india', 'sports', 'tabular data']	"Indian Premier League
The Indian Premier League (IPL) is a professional men's Twenty20 cricket league, contested by ten teams based out of ten Indian cities. The league was founded by the Board of Control for Cricket in India (BCCI) in 2007. It is usually held between March and May of every year and has an exclusive window in the ICC Future Tours Programme.
What's in here?
This dataset provides teamwise player prices and positions in the team. Along with that, it has players' data of batsmen and bowlers showing their performances. This can be used to analyze the cause of winning and losing, and various factors affecting that, and much more!
IIT Bombay WIDs
My college's analytics club team motivated me for this so big thanks to them! ✌️"	9	130	1	prena0808	ipl-dataset
9073	9073	Salesxlsx		[]		2	29	0	amirrezaei1997	salesxlsx
9074	9074	Smartphones Ranking Dataset 		[]		2	60	0	omkargode	smartphones-ranking-dataset
9075	9075	House Price Prediction		[]		3	70	0	kevintsq	house-price-prediction
9076	9076	pet2-tfrecords2-384-004		[]		0	33	0	bamps53	pet2-tfrecords2-384-004
9077	9077	data_1.csv		['internet']		1	22	0	shankarprasad	data-1csv
9078	9078	Two datasets for vision-based real-time detection	Images Datasets {Two sets}	['computer science', 'image data']	"Context
We have created two challenging datasets on different unstructured scenarios. The first dataset consists of 4000 images
 and the second one consists of 5000 images. 
They are used in our published work: http://www.ijmerr.com/uploadfile/2021/0923/20210923034438547.pdf
Content
The first dataset consists of 4000 images and the second one consists of 5000 images. 
The are available at They were taken under various environmental conditions and various 
road classifications such as different day times, indoor/outdoor scenes, shapes (normal or curved), 
and various surface structures (soil, asphalt).
IMPORTANT NOTE
in order to use the Datasets please contact the author
aljarrah5@yahoo.com"	1	92	0	ramiahmad	two-datasets-for-visionbased-realtime-detection
9079	9079	Cyclistic bike-share (april 2020 to sept 2021)	Google Analytics Capstone	['cycling', 'beginner', 'tabular data', 'dplyr', 'ggplot2']	"Context
This dataset comes from the Google Data Analytics capstone project
Acknowledgements
The data was collected by Motivate International Inc. and available under this license agreement."	2	54	0	jibinsajeev	cyclistic-bikeshare-april-2020-to-sept-2021
9080	9080	UHack Sentiments 2.0		[]		0	16	0	biswajitroy7890	uhack-sentiments-20
9081	9081	Titanic Dataset	Titanic Survival Prediction Dataset	['natural disasters', 'beginner', 'classification', 'tabular data', 'binary classification']	"Description:
The sinking of the Titanic is one of the most infamous shipwrecks in history.
On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone on board, resulting in the death of 1502 out of 2224 passengers and crew.
While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.
In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).
Acknowledgements:
This dataset has been referred from Kaggle: https://www.kaggle.com/c/titanic/data.
Objective:
Understand the Dataset & cleanup (if required).
Build a strong classification model to predict whether the passenger survives or not.
Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms."	594	2963	18	yasserh	titanic-dataset
9082	9082	Petfinder-fastai-kf-v16-Norm-Dense-Dec-23		['animals']		8	65	1	bobber	petfinderfastaikfv16normdensedec23
9083	9083	Protocol		[]		0	32	0	daviduzan	protocol
9084	9084	MS COCO		[]		3	76	0	mnassrib	ms-coco
9085	9085	xlnet-3folds-new		[]		0	43	0	yuzhoudiyishuai	xlnet3foldsnew
9086	9086	TMDB 9450 Movies Dataset	Top Rated Movies (Latest)	['arts and entertainment', 'movies and tv shows']		6	38	2	sayanroy729	top-rated-movies
9087	9087	G-research recalc trandata		[]		2	31	0	matrixneo	gresearch-recalc-trandata
9088	9088	Waymo-Open Training Dataset 10		[]		0	33	0	mohammedosama	waymo-open-training-dataset-10
9089	9089	pet2-tfrecords2-384-002		[]		0	9	0	bamps53	pet2-tfrecords2-384-002
9090	9090	Data T		['business']		0	0	0	tushartangricn	data-t
9091	9091	toxicity_analysis_basic_model_predictions		[]		0	24	0	trokhymovych	toxicity-analysis-basic-model-predictions
9092	9092	Standard Data science bowl 2018		['education']		2	39	0	tinhlai	standard-data-science-bowl-2018
9093	9093	Vegetable Image Dataset	Vegetable classification and recognition	['computer vision', 'cnn', 'image data', 'multiclass classification', 'food']	"Context
The initial experiment is done with 15 types of common vegetables that are found throughout the world. The vegetables that are chosen for the experimentation are- bean, bitter gourd, bottle gourd, brinjal, broccoli, cabbage, capsicum, carrot, cauliflower, cucumber, papaya, potato, pumpkin, radish and tomato. A total of 21000 images from 15 classes are used where each class contains 1400 images of size 224×224 and in *.jpg format. The dataset split 70% for training, 15% for validation, and 15% for testing purpose.
Content
This dataset contains three folders:
train (15000 images)
test (3000 images)
validation (3000 images)
each of the above folders contains subfolders for different vegetables wherein the images for respective vegetables are present.
Data Collection
The images in this dataset were collected by us from  vegetable farm and market for a project.
Acknowledgements
We would like to give thanks to the people who helped us regarding data collection.
Inspiration
From vegetable production to delivery, several common steps are operated manually. Like picking, and sorting vegetables. Therefore, we decided to solve this problem using deep neural architecture, by developing a model that can detect and classify vegetables. That model can be implemented in different types of devices and can also solve other problems related to the identification of vegetables, like labeling the vegetables automatically without any need for human work."	503	3411	26	misrakahmed	vegetable-image-dataset
9094	9094	little_model		[]		1	52	0	jaypume	little-model
9095	9095	climate dataset of Kashmir		['atmospheric science']	One of the objectives is to analyse the climate trends over the past years observed in the valley of Kashmir, for parameters such as mean temperature, precipitation and cloud cover, to visualise and understand the changes that have occurred.The major aim of the study is to forecast different climate variables such as mean temperature, average minimum temperature, average maximum temperature, cloud cover and precipitation, till the end of the current century, using the ARIMA model, to allow for better understanding of the projected changes and thus, in turn, structuring of better strategies to tackle climate change in the region	1	26	0	salimmir	climate-dataset-of-kashmir
9096	9096	Bella Beat case study		['arts and entertainment']		10	26	1	way2studytable	bella-beat-case-study
9097	9097	Sartorius-Pre-804-20211224130329		[]		0	16	0	hideyukizushi	sartorius-pre-804-20211224130329
9098	9098	checkpoint-80		['law']		0	42	0	sameerp30	checkpoint80
9099	9099	detectron_2_model		[]		2	84	1	hawkeat	detectron-2-model
9100	9100	mof2_estimator_r1		[]		2	8	0	honihitak	mof2-estimator-r1
9101	9101	Titanic N Fold Dataset	Titanic N Fold Dataset (5,10,20)	['beginner', 'numpy', 'pandas']		3	50	2	rhythmcam	titanic-training-n-fold-dataset
9102	9102	CyclistTrip	Divvy Bike Ride Data May 2021	[]		0	36	0	hilwani	cyclisttrip
9103	9103	TestDev		[]		19	126	0	freeforuse	testdev
9104	9104	Chicago Crime with Climate Data, 2021	Multiclass classification using ML and DL	['weather and climate', 'crime', 'multiclass classification', 'sklearn', 'pytorch']	"In this project I used machine learning and deep learning multiclass classification algorithms to predict types of crime commited in the city of Chicago in 2021. Moreover, I added weather data as features to the models with hope that the last will enrich the models and improve predictions.
project page on GitHub:
https://github.com/Mark-Rozenberg/Crime-And-Climate"	23	341	0	markrozenberg	chicago-crime-with-climate-data-2021
9105	9105	MotoGP Standing 2020	Standing of 2020 MotoGP championship + transposed versione	['auto racing', 'tabular data']	"Context
MotoGP 2020 championship standing
Content
Two versions of the cumulative standing, one with pilots on columns and races on rows, the other one with pilots on rows and races on columns."	48	310	9	fabriziocominetti	motogp-standing-2020
9106	9106	BMI Dataset	Predicting the BMI with help of various attributes.	['healthcare', 'health', 'computer science', 'beginner', 'classification', 'logistic regression', 'multiclass classification']	"Description:
A simple yet challenging project, to estimate the BMI based on the Gender, Height & Weight.
The complexity arises due the fact that dataset has less samples, & is highly imbalanced.
Can you overcome these obstacles & build a good predictive model to classify them?
This data frame contains the following columns:
Gender : Male / Female
Height : Number (cm)
Weight : Number (Kg)
Index :
0 - Extremely Weak
1 - Weak
2 - Normal
3 - Overweight
4 - Obesity
5 - Extreme Obesity
Source:
Kaggle - 
https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict the various categories of BMI.
Compare the evaluation metrics of vaious classification algorithms."	1507	10316	40	yasserh	bmidataset
9107	9107	Sartorius-Pre-902-20211224111005		[]		0	18	0	hideyukizushi	sartorius-pre-902-20211224111005
9108	9108	Spam Emails Dataset	Spam Emails Dataset - ML with NLP	['internet', 'beginner', 'classification', 'binary classification']	"Description:
The ""spam"" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...
Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.
The dataset, taken from the UCI ML repository, contains about 4600 emails labelled as spam or ham. 
The dataset can be downloaded here: https://archive.ics.uci.edu/ml/datasets/spambase
Objective:
Understand the Dataset & cleanup (if required).
Build classification models to predict whether or not the email spam.
Compare the evaluation metrics of vaious classification algorithms."	198	1607	7	yasserh	spamemailsdataset
9109	9109	PetFinder-Model43		[]		0	37	0	lftuwujie	petfindermodel43
9110	9110	sportdata		[]		0	2	0	mahdihemmat	sportdata
9111	9111	Benefits of buying a condominium for sale		[]	"A condominium for sale is an investment that provides a lot of benefits. It is a good place to live and has its own amenities. There are many things to consider before purchasing The Atelier condo. The length of stay, the number of people who would be occupying the apartment, and its architectural and interior design are some of the factors to consider. It is also important to consider the proximity of major establishments, such as schools, shopping malls, and restaurants.
Condos for sale are a great way to invest in a home that is affordable and offers some perks. Although the cost of ownership is lower than a single-family home's, the benefits of living in a condo are unbeatable. A condo for rent is typically more affordable than a single-family home. For those who want to live in a convenient place, a high-rise building with an elevator or laundry facility is a great choice.
There are several pros and cons to purchasing a condo. It is generally cheaper than buying a single-family house. It also requires less maintenance and upkeep. Some condos can be approved by an HOA, which means they are not governed directly by a homeowner's association. A condo is a great option for anyone looking for an easy-care, affordable home. A condominium is the best option for first-time buyers.
Despite the cost of living, a condo for sale offers many benefits. A high-rise residential building has 24-hour security, a recreational area, and more. The building also must pass strict safety standards and have no vacancy problems. If you don't want to deal with the upkeep, a condo purchase is a great option. The HOA takes care of all maintenance costs, so you don't have to worry about them.
Choosing a condo for sale is an excellent option if you're looking for a small home but can't afford the big expenses. They are typically less expensive than single-family homes and can be an ideal choice for first-time buyers, downsizers, and those who want to avoid the hassle of upkeep. A condo for sale can be an excellent choice for first-time buyers. Consider the cost of upkeep when you are looking for a condo to make your new home.
A condominium is a great investment. It can be a great place to live, and it's ideal for first-time buyers and people who want to downsize. It's a great way for you to live your life without worrying about maintenance. Your new investment will be a success if you are happy with your home. A condo for sale will allow you to enjoy all the amenities of a high-rise building without having to worry about upkeep."	0	40	0	sashagreg	benefits-of-buying-a-condominium-for-sale
9112	9112	kdd cup data		[]		0	68	0	reddde	kdd-cup-data
9113	9113	tgbr-train		[]		0	36	0	prateekagnihotri	tgbr-train
9114	9114	swin_bs8_f5_meta_noaug		[]		1	50	0	watanabetakahiro	swin-bs8-f5-meta-noaug
9115	9115	mini_ffhq_512		[]	"Content
This dataset is from the FFHQ dataset by NVLabs located here.
I took the first 6 folders and preprocessed them to shape 512 with LANCZOS resampling."	0	24	1	vincenttu	mini-ffhq-512
9116	9116	larxel_cat_faces		[]	"Content
This is a zip file containing the train and val cat images from this kaggle dataset here: https://www.kaggle.com/andrewmvd/animal-faces."	1	27	1	vincenttu	larxel-cat-faces
9117	9117	recorder		[]		0	38	0	mayukawaiiyo	recorder
9118	9118	cell instance segmentation models		['internet']		0	24	0	leolu1998	cell-instance-segmentation-models
9119	9119	mskrcnn_epoch_45		[]		1	45	0	xuhua0907	mskrcnn-epoch-45
9120	9120	PetFinder kfold		[]		0	7	0	kingkong153	petfinder-kfold
9121	9121	Spam_Classifier		[]		1	13	0	dharanireddy	spam-classifier
9122	9122	mmcvtools		[]		0	39	0	randomtreesg	mmcvtools
9123	9123	petdfinder_pawlist_images		[]		1	14	0	ktakita	petdfinder-pawlist-images
9124	9124	loan_data	loan data for machine learning with python	['computer science']		2	36	0	shengkaii	loan-data
9125	9125	Google Stock Data	Alphabet Inc. (GOOG) | NasdaqGS - NasdaqGS Real Time Price | Currency in USD	['business', 'finance', 'internet', 'time series analysis', 'investing', 'news', 'datetime']	"What is Google?
Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five companies in the American information technology industry, along with Amazon, Facebook, Apple, and Microsoft. Google was founded on September 4, 1998, by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14% of its publicly-listed shares and control 56% of the stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly-owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's Internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet.
Information about this dataset
This dataset provides historical data of Alphabet Inc. (GOOG). The data is available at a daily level. Currency is USD."	756	5537	31	varpit94	google-stock-data
9126	9126	toxicity		['public health']		0	14	0	weihengluo	toxicity
9127	9127	Checkpoint-75		['law']		0	6	0	sameerp30	checkpoint75
9128	9128	COVID-19 Impact On Airport Traffic	COVID-19's Impact on Airport Traffic in (USA, Chile, Canada, Australia)	['north america', 'australia', 'covid19', 'canada']	"Description 
COVID-19 is something we all are aware of by now, it has affected almost each and everything humans have had connection with . With the rise in COVID cases worldwide in early 2020, a lot of movement was seen throughout the world, people were sent back to their native countries or places , as a result places such as AIrports, Ports, Stations became highly populated during that period . As a result , the density of traffic in such places (Airports etc.) increased a lot , this dataset gives us the density of traffic in four international airports during the march-2020 period ."	7	103	1	neelakashchatterjee	covid19-impact-on-airport-traffic
9129	9129	Global_wheels		[]		1	31	3	soumya9977	global-wheels
9130	9130	bad-words	cuss words to use for jigsaw competition	[]		4	68	1	sahib12	badwords
9131	9131	train_NER		[]	"train_NER columns:
1. id: such as E1FA876D6E6C
2. text: Dear Senator, I am writting this letter to give my opinion on whether or...
3. entities: ['O', 'O', 'B-Lead', 'I-Lead', 'I-Lead', 'I-Lead', 'I-Lead', 'I-Lead', 'I-Lead', 'I-Lead'....]"	3	36	0	acanoe	train-ner
9132	9132	coco_train_BgMaskfromBoxes		[]		1	37	0	monishnatarajan	coco-train-bgmaskfromboxes
9133	9133	JJH1224		[]		0	35	0	ningkang218	jjh1224
9134	9134	Fer2013JH		[]		0	24	0	gengxiaochao	fer2013jh
9135	9135	Sentiment Analysis		['business']		0	29	0	kevintsq	sentiment-analysis
9136	9136	COVIDNL	COVID Cases in Nuevo León	['beginner', 'text data', 'covid19', 'mexico']		0	40	0	oone97	covidnl
9137	9137	Monthly Average City Temperatures Turkey		[]		2	15	0	idegkta	monthly-average-city-temperatures-turkey
9138	9138	tfgbr-yolov5-lovo-fold0		[]		0	29	0	ks2019	tfgbr-yolov5-lovo-fold0
9139	9139	AfricanFashion		[]		1	31	0	soalade	africanfashion
9140	9140	LSTM_Training_One_by_one		[]		0	11	0	dipanjankaranjai	lstm-training-one-by-one
9141	9141	Social_net		[]		0	67	0	kamranabbasov13	social-net
9142	9142	my-roberta-base	roberta-base from huggingface	['arts and entertainment', 'deep learning']	"Content
Huggingface roberta-base"	0	32	0	acanoe	myrobertabase
9143	9143	config		[]		0	7	0	keywhere	config
9144	9144	pretrain		[]		0	43	2	keywhere	pretrain
9145	9145	malaria_segmentation		[]		9	105	0	zeeshanshaik75	malaria-segmentation
9146	9146	Name Entity Recognition with Transformer v2 output		['software']		0	31	0	lonnieqin	name-entity-recognition-with-transformer-v2-output
9147	9147	r/Bitcoin 2010-2021	Title and Text from subreddit r/bitcoin from 2010 to 2021 December	['currencies and foreign exchange']		4	60	0	neoyipeng2018	rbitcoin-20102021
9148	9148	InputData		[]		0	23	0	ivanechen	inputdata
9149	9149	simclr-gu		[]		0	43	0	gusicun	simclrgu
9150	9150	simclr-gpu		[]		0	26	0	gusicun	simclrgpu
9151	9151	simclr-public		[]		0	44	0	gusicun	simclrpublic
9152	9152	circa-data		[]		1	36	0	mostofa	circadata
9153	9153	original-simclr-public		[]		0	2	0	gusicun	originalsimclrpublic
9154	9154	bert-pytorch		['arts and entertainment']		0	22	0	lulumu	bertpytorch
9155	9155	IBM data science python project		['sampling', 'computer science', 'intermediate', 'text data', 'regression']		0	40	1	prajwal111299	ibm-data-science-python-project
9156	9156	cifar-fs		[]		0	6	0	keywhere	cifarfs
9157	9157	Valorant vlr.gg Results and Stats 	Match results and player stats scraped from vlr.gg	['games', 'video games', 'sports']	"Context
Valorant is a tactical first-person shooter game developed by Riot Games that was officially release on June 2, 2020. A month after their release, Riot Games also released a Valorant API, which was given out to only a number of product developers, and most data is not publicly accessible. However, we do still have a way to obtain data from official matches, and that is from vlr.gg, a website that logs official matches and players stats, by scraping data off their pages. Please be aware that this is data scraped from the vlr.gg website using an Unofficial API created by the community and not using Riot Games' API.
Content
The dataset consists of data from the match results and player stats from vlr.gg scraped on December 24, 2021. The oldest match data that was able to be scraped successfully was on June 14, 2020. The original API created can be found here, while my edited version can be found here. Make sure to check the rows for faulty data and missing values -- I am 100% sure that you should be able to find some.
Match results and player stats in this data set are as such:
Match Results (results.csv):
I feel the need to mention that if a match is a bo1, the score will be the number of rounds won (eg 13-11), however, if it the match is not a bo1 but a series of maps (eg bo2, bo3, bo5) then it shows as the number of maps won (eg 2 - 1).
team1 = name of team 1
team2 = name of team 2
score1 = score of team 1
score2 = score of team 2
time_completed = interval time between match time and date and scraping time and date
round_info = round of the tournament ie group stage of which group of tournament_name
tournament_name = name of tournament in which the match was played
match_page = subset of link to the match page
tournament_icon = link to the tournament icon
Player Stats (stats.csv):
Please be aware that the player stats are obtained per region, per map, then per agent, in that specific order. That means all of the stats below are per map per agent, ie kill_deaths means ratio of kills per map per agent. In the future, the stats could also be scraped per event series, per event name, and per event stage, but they're listed as IDs so it would take quite some time to label them accordingly.
player = player name
org = current organization
rds = total rounds played
average_combat_score = overall average combat score (combat score can be calculated as such)
kill_deaths = ratio of kill deaths
average_damage_per_round = average damage per round
kills_per_round = kills per round
assists_per_round = assists per round
first_kills_per_round = first kills per round
first_deaths_per_round = first deaths per round
headshot_percentage = overall headshot percentage
clutch_success_percentage = ratio of clutch wins over total clutch situations played
clutch (won/played) = number of clutches won over number of clutch situations played
total_kills = total number of kills
total_deaths = total number of deaths
total_assists = total number of assists
total_first_kills = total number of first kills
total_first_deaths = total number of first deaths
map_id = ID of the map played ('1': bind, '2': haven, '3': split, '5': ascent, '6': icebox, '8': breeze, '9': fracture)
agent = name of the agent played
region = region in which the player plays
Acknowledgements
Credits to the original creators of the API here. I only managed to add in a few things to get some more data and fix a little bit here and there as the vlr.gg pages seemed to have been updated since the API was created. Give all the props to them 👏 
Inspiration
Show us creative analytics such as, which agent gets the most first kills in a particular map, which map has the highest average total kills, which agent produces the highest average assists (and if decreases or increases across all maps), etc. As for predictive analysis, perhaps you can segment players based on their stats into clusters such as player roles or tiers, or predict which agents or which class list (sentinels, duelist, smokers, etc) do these players play based on stats. Have fun being creative 💯"	30	397	5	hidious	valorant-vlrgg-results-and-stats
9158	9158	fer2013gd		[]		0	14	0	gengxiaochao	fer2013gd
9159	9159	ali_recmd		[]		0	25	0	leesin666	ali-recmd
9160	9160	Base de datos de vulnerabilidad ante el FEN	Vulnerabilidad de IE ante la ocurrencia de Deslizamientos y/o Inundaciones	[]		8	161	0	johnlopezvega	base-de-datos-de-vulnerabilidad-ante-el-fen
9161	9161	f30k_precomp_SCAN		[]		0	19	0	larry2000	f30k-precomp-scan
9162	9162	kaerururu-petfinder2-077		[]		0	4	0	kaerunantoka	kaerururu-petfinder2-077
9163	9163	mof2_estimetor		[]		0	8	0	honihitak	mof2-estimetor
9164	9164	vehicle_details		[]		0	7	0	amirman6	vehicle-details
9165	9165	Consumer Reports Naughty or Nice List	Businesses that were naughty or nice from 2012-2015	['business', 'holidays and cultural events', 'ratings and reviews']	"Content
Policies, like products, sometimes fall short of expectations. From the years 2012-2015, Consumer Reports released an annual Naughty & Nice list, a campaign to shed light on company policies or practices that help or hinder the public.
The list includes retailers, airlines, telecom companies, and others that they dinged for fees, fine print, or punitive practices; others were lauded for generous and outstanding customer service.
Several Consumer Reports staff members who cover retailing, finance, electronics, and other beats contributed to the list, as did social media input. In each case, they verified the policy and/or practice either by direct contact or reading through the details on the company’s website. Although they cited companies by name, other businesses may engage in similar practices—for better or worse. And praise or blame for a specific policy doesn’t mean Consumer Reports gives a thumbs-down or thumbs-up or for everything else that company does or the way it treats customers.
Acknowledgements
Consumer Reports
Inspiration
Which companies were most naughty or nice?
What were the most common reasons for being placed on either list?"	38	565	7	csafrit2	consumer-reports-naughty-or-nice-list
9166	9166	INT303 Big Data Analysis		[]		0	3	0	yishihong18	int303-big-data-analysis
9167	9167	Shoot around the coffee table at 360 degrees	Output RGB, Normal, and Semantic / Instance	['artificial intelligence', 'computer science', 'image data']	"Content
Shoot around the coffee table at 360 degrees
Output RGB, Normal, and Semantic / Instance"	0	45	0	luznoc	shoot-around-the-coffee-table-at-360-degrees
9168	9168	stockpredictionmodels		[]		0	6	0	jutaratwongmek	stockpredictionmodels
9169	9169	Great-Barrier-Reef-YOLO-Model_Weights	This dataset contains the models I trained outside Kaggle for GBR competition	['computer science']		0	42	0	samuelfung206	greatbarrierreefyolomodel-weights
9170	9170	cat_dogs_pr		[]		0	32	0	durgampranay	cat-dogs-pr
9171	9171	Binance BTCUSDT USDS-Margined Range Bar v1.43		[]		0	34	0	missterryli	binance-btcusdt-usdsmargined-range-bar-v143
9172	9172	Christmas Tree Sales 2004-2016	Number of Christmas trees sold	['religion and belief systems', 'holidays and cultural events']	"Content
Christmas trees sold in the US from 2004 - 2016
Acknowledgements
Data World
Inspiration
Are real trees or fake trees experiencing a greater increase in popularity?"	129	824	4	csafrit2	christmas-tree-sales-20042016
9173	9173	80-20 train test Data science bowls 2018		[]		0	39	0	tinhlai	8020-train-test-data-science-bowls-2018
9174	9174	SIIM Pneumothorax Segmentation Checkpoints		[]		1	22	0	vaillant	siim-ptx-checkpoints
9175	9175	Training_fold_5_d		[]		0	16	0	sidhantthole123	training-fold-5-d
9176	9176	PY-BigBird-v14		['computer science']		18	42	1	cdeotte	py-bigbird-v14
9177	9177	League of Legends Diamond Matches (Preseason 12)	Preseason 12 Diamond match data with early game snapshot data and summary data	['games', 'video games', 'tabular data']		18	191	2	sabrinasummers	league-of-legends-diamond-matches-preseason-12
9178	9178	What and Where Are the World's Oldest Businesses_		[]		1	31	0	manuelandresespitia	what-and-where-are-the-worlds-oldest-businesses
9179	9179	tfgbr-yolov5-lovo-fold1		[]		0	24	0	ks2019	tfgbr-yolov5-lovo-fold1
9180	9180	CENSORED WEB-SITES BY ALL COUNTRIES	Sites that were or are currently censored	['websites', 'arts and entertainment', 'science and technology', 'computer science', 'data visualization', 'data analytics']	"CENSORED WEB-SITES BY ALL COUNTRIES
Sites that were or are currently banned.
This data was created by each country's own users.
Some of the sites you have seen may have been active again."	48	516	4	brsdincer	censored-websites-by-all-countries
9181	9181	NEW YOLOV5 Verison		[]		0	21	1	hossamfakher	new-yolov5-verison
9182	9182	cell-clean-astro-mask		[]		1	42	1	jieqingyang	cellcleanastromask
9183	9183	tfgbr-yolov5-lovo-fold2		[]		0	3	0	ks2019	tfgbr-yolov5-lovo-fold2
9184	9184	FC Bayern Face Recognation	Bayern Munich Players Dataset for Face recognation Project	['football']	"A Bayern Munich Players dataset Collected from Google<br>
The data contains 5 Bayern Munich players and each player has about 100 random raw images collected from Google after cleaning the data each player got around from 30 to 60 images <br><br>
so our data have 5 classes:<br>
Kingsley Coman -&gt; class 0
Joshua Kimmich  -&gt; class 1
Robert Lewandowski -&gt; class 2
Manuel Neuer  -&gt; class 3
Leory Sane  -&gt; class 4 <br><br>
The Data contains 230 rows and 4097 Columns 
4096 Features are the Pixels of the Images and the Last Column is the target column including the class for each row
Data is already Cleaned, Preprocessed and Scaled
Apply machine and deep learning algorithms to the data and build a face recognition system that Recognizes any image of these five players"	21	229	5	eyadgk	fc-bayern-face-recognation
9185	9185	mean_mfcc		[]		0	1	0	ambulygin	mean-mfcc
9186	9186	RSNA Pneumonia Detection Checkpoints		['automobiles and vehicles']		1	215	13	vaillant	rsna18-pna-checkpoints
9187	9187	yolov5-lovo-fold2		[]		0	0	0	ks2019	yolov5-lovo-fold2
9188	9188	Sartorious Cascade RCNN		[]		0	58	0	aishikai	sartorious-cascade-rcnn
9189	9189	Trabalho DM		[]		0	45	0	mathiasdutranichele	trabalho-dm
9190	9190	Punt Effectiveness Rating Chart	Chart for new statistic developed for NFL Big Data Bowl 2022	['football', 'statistical analysis', 'data analytics', 'r']	The chart contains lookup values for a new Punt statistic developed for the  NFL Big DataBowl 2022 Competition and used by https://www.kaggle.com/charliezimmerman/punt-effectiveness-rating-a-new-punt-statistic. The statistic evaluates punts based on Net Punt Yards and Punt Line of Scrimmage.	1	38	1	charliezimmerman	punteffectivenesschart
9191	9191	Cricket on Reddit	News, banter and occasional serious discussion on the great game	['cricket']	"Context
Cricket (r/Cricket), is a subreddit where people divagate, blander and sometime discuss seriously about this great game.
The data is not filtered.
Collection
Reddit posts from subreddit r/Cricket, downloaded from https://www.reddit.com/r/Cricket using praw (The Python Reddit API Wrapper).
Script used for collection can be found here: Reddit extract content
Content
Data contains both posts and comments.
Both posts and comments contains the following fields:
* title - relevant for posts
 score - relevant for posts - based on impact, number of comments
 id - unique id for posts/comments
 url - relevant for posts - url of post thread
 commns_num - relevant for post - number of comments to this post
 created - date of creation
 body - relevant for posts/comments - text of the post or comment
* timestamp - timestamp
Acknowledgements
All merit goes to the contributors to the posts of subreddit r/Cricket. I only collects them daily.
Inspiration
You can use the data to:
* Perform sentiment analysis;
* Identify discussion topics;"	75	2300	19	gpreda	cricket-on-reddit
9192	9192	Clothing Dataset	2779 clothing images classified into 6 types and 5 designs.	['clothing and accessories', 'classification']	"Context
There's a story behind every dataset and here's your opportunity to share yours.
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	6	180	1	gabrielalbertin	clothing-dataset
9193	9193	Animals 		[]		1	29	0	aahmed1234	animals
9194	9194	Waymo-Open Training Dataset 9		[]		2	26	0	mohammedosama	waymo-open-training-dataset-9
9195	9195	arima_data		[]		0	6	0	younesradi	arima-data
9196	9196	robust scanner base		[]		2	80	0	riadhossainapsis	robust-scanner-base
9197	9197	Naughty or Nice List 2021	The official Naughty and Nice List for 2021	['popular culture', 'music', 'earth and nature', 'religion and belief systems', 'holidays and cultural events', 'news']	"The Department of Christmas Affairs in The North Pole Government maintains an official naughty or nice list. This is the most recent version.
With over 255 births globally per minute, the Naughty and Nice list is being continually reviewed and updated. If your name is missing, use the Name submission form to submit your name and we will add it to our processing queue: https://www.christmasaffairs.com/list/index.html 
If your name is on the naughty list, you can make a request for review.
Acknowledgements
The Department of Christmas Affairs. The North Pole Government
Inspiration
This data will be more useful for data science purposes when it contains data from multiple years. In the future, it will list the current year's status and the status for previous years. 
Which names are most likely to be nice or naughty?"	36	564	3	csafrit2	naught-or-nice-list-2021
9198	9198	compete		[]		0	26	0	abhishekprajapat	compete
9199	9199	PetFinder-Model40		[]		0	24	0	lftuwujie	petfindermodel40
9200	9200	Immunization campaign tracker 		['public health']		0	24	0	ashishsk7	immunization-campaign-tracker
9201	9201	09_09_2020		[]		2	10	0	lemzar	09-09-2020
9202	9202	Sartorius CPs ens10		[]		1	18	0	theoviel	sartorius-cps-ens10
9203	9203	Moscow_car_prices		[]		2	13	0	lemzar	moscow-car-prices
9204	9204	face disease		['health conditions']		9	66	0	hilmiher	face-disease
9205	9205	Census Dataset	Census datasets contains randomly generate data using Faker package in Python	['social science']		5	118	10	cankatsrc	census-dataset
9206	9206	check_point		[]	Checkpoint for glas dataset - MedT	0	52	0	phclihng	check-point
9207	9207	CoAP-DDoS	DDoS Dataset for CoAP Protocol in IoT	[]		2	91	0	jaredalanmathews	coapddos
9208	9208	inputdata		[]		0	34	0	maheshmadhvi	inputdata
9209	9209	DataGaji		[]		6	38	0	mahesaanugrah	datagaji
9210	9210	my_pf_dataset4		[]		0	22	0	jukijuki	my-pf-dataset4
9211	9211	petfinder_swin_libs		[]		0	7	0	itsuki9180	petfinder-swin-libs
9212	9212	NLP AV Sentiment Identification		[]		0	46	1	mohamedziauddin	nlp-av-sentiment-identification
9213	9213	TradeMap2009-2020Dataset		[]		0	30	0	fatihgulsen	trademap20092020dataset
9214	9214	uHack Sentiments 2.0 Decode Code Words		[]		0	4	1	sravanneeli	uhack-sentiments-20-decode-code-words
9215	9215	int303_1		[]		1	24	0	jiadongguo1822760	int303-1
9216	9216	my_pf_dataset3		[]		0	79	0	jukijuki	my-pf-dataset3
9217	9217	For Course Project		['education']		1	87	0	zhangbinliang	for-course-project
9218	9218	Dummy customer data		['business']		0	26	0	mjjabarian	dummy-customer-data
9219	9219	cbnetv2cascadeswinsmalllargeimgres		[]		4	41	0	ubamba98	cbnetv2cascadeswinsmalllargeimgres
9220	9220	Imagenet_compression		[]		0	20	0	hideinshadow	imagenet-compression
9221	9221	xlnet_base_cased		[]		0	10	0	bokgaaicheng	xlnet-base-cased
9222	9222	19970628		[]		0	54	0	liping11	19970628
9223	9223	Base_longformer		[]		0	33	0	icarus96	base-longformer
9224	9224	Amazon Stock Data	AMZN | Nasdaq | Currency in USD	['business', 'finance', 'beginner', 'intermediate', 'advanced', 'investing']	"What is Amazon?
Amazon.com, Inc. is an American multinational technology company which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Microsoft, and Facebook. The company has been referred to as ""one of the most influential economic and cultural forces in the world"", as well as the world's most valuable brand.
Data Description
This dataset provides the history of daily prices of Amazon stock (AMZN). All the column descriptions are provided. Currency is USD."	481	1992	12	varpit94	amazon-stock-data
9225	9225	ResNet50_Pretrained		[]		0	26	0	yonaschanie	resnet50-pretrained
9226	9226	Titanic -  For Machine Learning		[]		0	29	0	abid25	titanic-for-machine-learning
9227	9227	BBC NEWS NLP		['news']		3	23	0	weslatimarwen	bbc-news-nlp
9228	9228	Light_data		[]		1	25	0	shashwatnaidu07	light-data
9229	9229	resource.csv		['software']		0	12	0	nethrac	resourcecsv
9230	9230	models		['clothing and accessories']		0	33	0	wuhuairlines	models
9231	9231	Cruise Ship Data		[]		7	111	0	shauryajain	cruise-ship-data
9232	9232	CATBOOST_MODEL_DECEMBER_NEW		[]		0	30	0	aditya01233	catboost-model-december-new
9233	9233	Checkpoint-70		['law']		0	23	0	sameerp30	checkpoint70
9234	9234	Moscow_springs		[]		0	6	0	ivanbovsunovskiy	moscow-springs
9235	9235	English Grammar Error.	For grammar error detection training.	[]		6	70	0	s105022206	grammer-error
9236	9236	bart-base		['arts and entertainment']		0	3	0	bokgaaicheng	bartbase
9237	9237	roberta-ner-700		['arts and entertainment']		0	48	1	icarus96	robertaner700
9238	9238	Titanic - Machine Learning from Disaster		['business']		2	24	0	kazihasib00	titanic-machine-learning-from-disaster
9239	9239	Bone Detection		['health']		5	52	0	naimishareddy17	bone-detection
9240	9240	How Amazon Competitor Research Can Help You 		[]	Amazon competitor research helps businesses identify market gaps, modify products and create effective business strategies. You can outsource Amazon competitor analysis services to add more value to your business. You will get necessary information presented to you in the form of detailed documents, charts, graphs, and reports.	0	38	0	janemiller	how-amazon-competitor-research-can-help-you
9241	9241	DIPdataset		[]		0	17	0	wuhuairlines	dipdataset
9242	9242	The Pen Addict Podcast	Episode number, title, guests, date and length for The Pen Addict Podcast	['arts and entertainment']	"Context
Episode number, title, guests, date and length for The Pen Addict Podcast
More info at https://bleistift.blog/2018/03/pen-addict-podcast-299/ and https://bleistift.blog/?p=9535
Content
What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents, too.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	8	162	3	matthiasmeckel	the-pen-addict-podcast
9243	9243	INT303333		[]		0	4	0	zhuohangli1823221	int303333
9244	9244	delfog		[]		1	49	0	yuanshup	delfog
9245	9245	Sartorius-801-20211223133157		[]		0	31	0	hideyukizushi	sartorius-801-20211223133157
9246	9246	subaru ds train 1223 half		['automobiles and vehicles']		0	14	1	wuliaokaola	subaru-ds-train-1223-half
9247	9247	TataMotorsSharesData		[]		2	45	0	samachakole	tatamotorssharesdata
9248	9248	oxptest_dataset		[]		0	22	0	javadkhorramdel	oxptest-dataset
9249	9249	test_DB		[]		1	0	0	ruizhekan1825143	test-db
9250	9250	Gini Coefficient of Countries	The Gini coefficient is the most commonly used measure of income distribution	[]		1	65	3	therockk	gini-coefficient-of-countries
9251	9251	glove6b50		[]		0	16	0	jslijb	glove6b50
9252	9252	TPS12 DEC blender	TPS12 DEC blender 0.95685	[]		2	62	2	teckmengwong	blender
9253	9253	w7s4ppo	w7s4v3sssssssssssssssssssssssssss	[]		0	33	0	linhtrngxun	w7s3v3
9254	9254	pretrained_word_embedding		[]		0	26	0	yumintseng	pretrained-word-embedding
9255	9255	mathercup		[]		0	37	0	cliuch	mathercup
9256	9256	Flickr Faces 70k Thumbnails 128x128	https://github.com/NVlabs/ffhq-dataset	[]		24	125	1	imcr00z	flickr-faces-70k-thumbnails-128x128
9257	9257	Waymo-Open Training Dataset 8		[]		0	35	0	mohammedosama	waymo-open-training-dataset-8
9258	9258	Housing prices France 35	Housing prices in France Ille-et-Vilaine	['housing', 'business', 'real estate', 'geospatial analysis', 'social issues and advocacy']	"This is the french house sold from 2014 to present in the departement of Ille-et-Vilaine The data are updatable every 6 month.
Build on the top of DVF+ data  from Cerema.
Licence: Open V2"	29	237	0	cheneblanc	housing-prices-35-fr
9259	9259	conc_aaa		[]		0	34	0	aigotoaimasakimm	conc-aaa
9260	9260	student details		['universities and colleges']		5	61	0	ahmadabdullah13	student-details
9261	9261	cascadeR50		[]		0	20	0	zhihualiu	cascader50
9262	9262	Kaggle-checkpoint-65		['computer science']		0	25	0	sameerp30	kagglecheckpoint65
9263	9263	VGG1016		[]		0	42	1	bajojov	vgg1016
9264	9264	coco_lp_preview	Small subset of the moroccan-vehicle-registration-plates dataset in COCO format	[]		0	8	1	nouamane	coco-lp-preview
9265	9265	FliteData		[]		0	26	0	tatyanaboudanova	flitedata
9266	9266	fold10_365_renew		[]		0	37	0	fly22131	fold10-365-renew
9267	9267	HCV_data		[]		2	26	6	aminizahra	hcv-data
9268	9268	dcn_v2		[]		1	70	0	liqitong	deepctr
9269	9269	xgboostPreprocessedDecember_NEW	DECEMBER TPS TEMPORARAY FILES	[]		0	23	0	aditya01233	xgboostpreprocesseddecember-new
9270	9270	dcn111		[]		0	0	0	liqitong	dcn1111
9271	9271	lgbmPreprocessedDecemberTpsNew		[]		0	23	0	aditya01233	lgbmpreprocesseddecembertpsnew
9272	9272	Data Analysis on Covid-19 state-wise India Vaccine 	Data Visualization on latest Indian state_wise vaccine as on 20 December2021	['india', 'healthcare', 'public health', 'exploratory data analysis', 'data visualization', 'python']	"A COVID‑19 vaccine is a vaccine intended to provide acquired immunity against severe acute respiratory syndrome coronavirus 2 (SARS‑CoV‑2), the virus that causes coronavirus disease 2019 (COVID‑19). 
This Dataset contains India's state-wise vaccination data on 16 October 2021."	86	620	3	swatikhedekar	state-wise-india-covid19vaccination
9273	9273	swin_transform		[]		0	19	0	godliang2	swin-transform
9274	9274	St Nicholas III	The third Christmas pilgrimage	['religion and belief systems']		0	33	1	jbomitchell	st-nicholas-iii
9275	9275	Manually annotated images for Basics of Autonomous	Basics of Autonomous Vehicles Development	[]		0	52	0	veersingh230799	manually-annotated-images-for-basics-of-autonomous
9276	9276	Trump Lawsuits	What Trump's Legal Battles Tell Us About Presidential Power	['united states', 'government', 'law', 'politics', 'beginner']	"About this dataset
&gt; <p>This dataset contains the data behind the stories:</p>
<ul>
<li><a href=""https://fivethirtyeight.com/features/what-trumps-legal-battles-tell-us-about-presidential-power/"" target=""_blank"" rel=""nofollow"">What Trump's Legal Battles Tell Us About Presidential Power</a></li>
<li><a href=""https://fivethirtyeight.com/features/why-it-might-be-impossible-to-overturn-a-presidential-pardon/"" target=""_blank"" rel=""nofollow"">Why It Might Be Impossible To Overturn A Presidential Pardon</a></li>
<li><a href=""https://fivethirtyeight.com/features/will-the-supreme-court-fast-track-cases-involving-trump/"" target=""_blank"" rel=""nofollow"">Will The Supreme Court Fast-Track Cases Involving Trump?</a></li>
<li><a href=""https://fivethirtyeight.com/features/why-one-of-trumps-biggest-legal-threats-is-new-yorks-attorney-general/"" target=""_blank"" rel=""nofollow"">Why One of Trump’s Biggest Legal Threats Is New York’s Attorney General</a></li>
<li><a href=""https://fivethirtyeight.com/features/should-judges-pay-attention-to-trumps-tweets/"" target=""_blank"" rel=""nofollow"">Should Judges Pay Attention To Trump’s Tweets?</a></li>
<li><a href=""https://fivethirtyeight.com/features/trump-is-losing-the-legal-fight-against-sanctuary-cities-but-it-may-still-pay-off-politically/"" target=""_blank"" rel=""nofollow"">Trump Is Losing The Legal Fight Against Sanctuary Cities, But It May Still Pay Off Politically</a></li>
<li><a href=""https://fivethirtyeight.com/features/will-trumps-latest-lawsuits-keep-congress-from-investigating-future-presidents/"" target=""_blank"" rel=""nofollow"">Will Trump’s Latest Lawsuits Keep Congress From Investigating Future Presidents?</a></li>
</ul>
<p>See the data dictionary for column descriptions.</p>
<p>If you find this information useful, please <a href=""mailto:data@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p>License: <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a><br>
Source<a href=""https://github.com/fivethirtyeight/data/tree/master/trump-lawsuits"" target=""_blank"" rel=""nofollow"">: </a><a href=""https://github.com/fivethirtyeight/data/tree/master/trump-lawsuits"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data/tree/master/trump-lawsui</a>ts</p>
This dataset was created by data.world's Admin and contains around 100 samples along with Docket Orig, Trump Category, technical information and other features such as:
- Current Location
- Issue
- and more.
How to use this dataset
&gt; - Analyze Jurisdiction in relation to Nature
- Study the influence of Judge on Plaintiff
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	34	467	9	yamqwe	trump-lawsuitse
9277	9277	srm-kernel		['computer science', 'programming']		0	3	0	niujianmin	srmkernel
9278	9278	data-infer		[]		0	18	0	antrngphannh	datainfer
9279	9279	mpbcsx	brest cancer semantic	['cancer']		3	54	0	karthikpatilhub	mpbcsx
9280	9280	joncare1223		[]		0	38	0	lxinha	joncare1223
9281	9281	microscope tfrec		['earth and nature']		1	136	0	nickthenick23	microscope-tfrec
9282	9282	int303		[]		1	28	0	xiaoranyu	int303
9283	9283	ImagesForBozekLabWiki		[]		0	81	0	lucassancere	imagesforbozeklabwiki
9284	9284	Predicting Women's NBA (WNBA)	2021 WNBA ELO, Predictions, technical information	['basketball', 'sports', 'government', 'beginner', 'time series analysis']	"About this dataset
&gt; <h2><strong>About</strong></h2>
<p>This file contains links to the data behind our <a href=""https://projects.fivethirtyeight.com/2021-wnba-predictions/"" target=""_blank"">WNBA Predictions</a>. More information on how our WNBA Elo model works can be found in <a href=""https://fivethirtyeight.com/methodology/how-our-wnba-predictions-work/"" target=""_blank"">this article</a>.</p>
<p><code>wnba_elo.csv</code> contains game-by-game Elo ratings and forecasts since 1997.</p>
<p><code>wnba_elo_latest.csv</code> contains game-by-game Elo ratings and forecasts for only the latest season.</p>
<h2><strong>License</strong></h2>
<p>Data released under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"">Creative Commons Attribution 4.0 License</a></p>
<h2><strong>Source</strong></h2>
<p><a href=""https://github.com/fivethirtyeight/data/blob/master/wnba-forecasts/README.md"" target=""_blank"">GitHub</a></p>
This dataset was created by data.world's Admin and contains around 6000 samples along with Home Team Postgame Rating, Home Team, technical information and other features such as:
- Date
- Away Team
- and more.
How to use this dataset
&gt; - Analyze Neutral in relation to Home Team Pregame Rating
- Study the influence of Away Team Postgame Rating on Season
- More datasets
Acknowledgements
If you use this dataset in your research, please credit data.world's Admin 
Start A New Notebook!"	37	464	5	yamqwe	wnba-forecastse
9285	9285	Waymo-Open Training Dataset 7		[]		0	12	0	mohammedosama	waymo-open-training-dataset-7
9286	9286	Rare Pepes	Can The Blockchain Turn Pepe The Frog Into Modern Art?	['beginner', 'retail and shopping']	"About this dataset
&gt; <p>Data behind the story <a href=""https://fivethirtyeight.com/features/pepe-the-frog-symbolism-cryptoart-blockchain/"" target=""_blank"" rel=""nofollow"">Can The Blockchain Turn Pepe The Frog Into Modern Art?</a><br>
There are four data files, described below. You can also find further information about individual Rare Pepe assets at <a href=""https://rarepepewallet.com/feed"" target=""_blank"" rel=""nofollow"">Rare Pepe Wallet</a>.</p>
<p><code>ordermatches_all.csv</code> contains all Rare Pepe order matches from the beginning of the project, in late 2016, until Feb. 3. All order matches include a pair of assets (a “forward asset” and a “backward asset”) one of which is a Rare Pepe and the other of which is either XCP, the native <a href=""https://counterparty.io/"" target=""_blank"" rel=""nofollow"">Counterparty</a> token, or Pepe Cash. The time of the order match can be determined by the block.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Block</code></td>
<td>The block number</td>
</tr>
<tr>
<td><code>ForwardAsset</code></td>
<td>The type of forward asset</td>
</tr>
<tr>
<td><code>ForwardQuantity</code></td>
<td>The quantity of forward asset</td>
</tr>
<tr>
<td><code>BackwardAsset</code></td>
<td>The type of backward asset</td>
</tr>
<tr>
<td><code>BackwardQuantity</code></td>
<td>The quantity of backward asset</td>
</tr>
</tbody>
</table></div>
<p><code>blocks_timestamps.csv</code> is a pairing of block and timestamp. This can be used to determine the actual time an order match occurred, which can then be used to determine the dollar value of Pepe Cash or XCP at the time of the trade.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Block</code></td>
<td>The block number</td>
</tr>
<tr>
<td><code>Timestamp</code></td>
<td>A Unix timestamp</td>
</tr>
</tbody>
</table></div>
<p><code>pepecash_prices.csv</code> contains the dollar price of Pepe Cash over time.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Timestamp</code></td>
<td>A Unix timestamp</td>
</tr>
<tr>
<td><code>Price</code></td>
<td>The price of Pepe Cash in dollars</td>
</tr>
</tbody>
</table></div>
<p><code>xcp_prices.csv</code> contains the dollar price of XCP over time.</p>
<div style=""overflow-x:auto;""><table><thead>
<tr>
<th>Header</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Timestamp</code></td>
<td>A Unix timestamp</td>
</tr>
<tr>
<td><code>Price</code></td>
<td>The price of XCP in dollars</td>
</tr>
</tbody>
</table></div>
<p><strong><em>Source:</em></strong> <a href=""http://rarepepefoundation.com/"" target=""_blank"" rel=""nofollow"">Rare Pepe Foundation</a></p>
<p>The data is available under the <a href=""http://creativecommons.org/licenses/by/4.0/"" target=""_blank"" rel=""nofollow"">Creative Commons Attribution 4.0 International License</a> and the code is available under the <a href=""http://opensource.org/licenses/MIT"" target=""_blank"" rel=""nofollow"">MIT License</a>. If you do find it useful, please <a href=""andrei.scheinkman@fivethirtyeight.com"" target=""_blank"" rel=""nofollow"">let us know</a>.</p>
<p><strong><em>Source:</em></strong> <a href=""https://github.com/fivethirtyeight/data"" target=""_blank"" rel=""nofollow"">https://github.com/fivethirtyeight/data</a></p>
This dataset was created by FiveThirtyEight and contains around 30000 samples along with Backward Quantity, Block, technical information and other features such as:
- Forward Quantity
- Backward Asset
- and more.
How to use this dataset
&gt; - Analyze Forward Asset in relation to Backward Quantity
- Study the influence of Block on Forward Quantity
- More datasets
Acknowledgements
If you use this dataset in your research, please credit FiveThirtyEight 
Start A New Notebook!"	10	332	2	yamqwe	rare-pepese
9287	9287	data test 1		[]		0	21	0	fannysalsabila	data-test-1
9288	9288	cascade mask rcnn cell model		[]		0	78	0	duykhanh99	cascade-mask-rcnn-cell-model
9289	9289	datasetsh		[]		0	26	0	shiminhsu	datasetsh
9290	9290	Arrays		[]		0	19	0	aamohite	arrays
9291	9291	MINDSummary		[]		2	58	0	melbahlun23	mindsummary
9292	9292	pca_feature		[]		0	27	0	daynaenderson	pca-feature
9293	9293	red wine quality		['alcohol']		0	49	0	procomp	red-wine-quality
9294	9294	processed-data		[]		0	32	0	silviayen	processeddata
9295	9295	INT_DATA		[]		0	33	0	ichenlei	int-data
9296	9296	Palmer Penguins 	Data Set using R program	['data cleaning', 'data visualization', 'ggplot2', 'tidyverse', 'r']	"The Palmer Penguins data contains size measurements, clutch observations, and blood isotope ratios for three penguin species observed on three islands in the Palmer Archipelago, Antarctica over a study period of three years.
These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network."	21	189	3	way2studytable	palmer-penguins-using-r
9297	9297	preprocessedTrainingAndTestingData_NEW		[]		0	22	0	aditya01233	preprocessedtrainingandtestingdata-new
9298	9298	Train_age		[]		0	26	0	artemrogulin	train-age
9299	9299	Question1		[]		0	32	0	goudake	question1
9300	9300	mmdetection		[]		0	30	0	henini	mmdetection
9301	9301	MatchingModel23/12		[]		2	31	0	fakerhust	matchingmodel2312
9302	9302	drshtcr101livecell		[]		2	11	0	phoenix9032	drshtcr101livecell
9303	9303	2021-專題PHM08		[]		2	39	0	chinihc01	2021phm08
9304	9304	Waymo-Open Training Dataset 6		[]		0	29	0	mohammedosama	waymo-open-training-dataset-6
9305	9305	checkpoint_textsum		[]		0	42	0	antrngphannh	checkpoint-textsum
9306	9306	Transformers for RecSys		['movies and tv shows']		0	24	0	hariwh0	transformers-for-recsys
9307	9307	All XBox One Games	All Xbox One games data	['games', 'board games', 'card games', 'video games', 'online communities']	"All Xbox One Games Dataset
The Xbox One is a line of home video game consoles developed by Microsoft. Announced in May 2013, it is the successor to Xbox 360 and the third base console in the Xbox series of video game consoles. It was first released in North America, parts of Europe, Australia, and South America in November 2013 and in Japan, China, and other European countries in September 2014. It is the first Xbox game console to be released in China, specifically in the Shanghai Free-Trade Zone. Microsoft marketed the device as an ""all-in-one entertainment system"", hence the name ""Xbox One"". An eighth-generation console, it mainly competed against Sony's PlayStation 4 and Nintendo's Wii U a and later the Switch. This dataset contains details of all Xbox One Games published so far."	152	1178	14	shivamb	all-xbox-one-games
9308	9308	Pixar studio movies data	Pixar Studio movies data about budget and profit (Wikipedia data)	['movies and tv shows', 'beginner', 'intermediate', 'text data', 'comics and animation']	"Context
This Data Shows about Pixar Studio Movies.
Content
its simple and usable data and for Beginners.
Acknowledgements
this data is for learning .
Inspiration
and thanks to Wikipedia ."	154	1017	7	sandipdevre	pixar-studio12
9309	9309	kaerururu-global-wheat-detection-late-004		[]		0	17	0	kaerunantoka	kaerururu-global-wheat-detection-late-004
9310	9310	uHack Sentiments 2.0: Decode Code Words		['intermediate', 'nlp', 'clustering', 'multiclass classification', 'multilabel classification']	"The challenge here is to analyze and deep dive into the natural language text (reviews) and bucket them based on their topics of discussion. Furthermore, analyzing the overall sentiment will also help the business to make tangible decisions.
The data set provided to you has a mix of customer reviews for products across categories and retailers. We would like you to model on the data 
to bucket the future reviews in their respective topics (Note: A review can talk about multiple topics)
Overall polarity (positive/negative sentiment)
Train: 6136 rows x 14 columns
Test: 2631 rows x 14 columns
Topics (Components, Delivery and Customer Support, Design and Aesthetics, Dimensions, Features, Functionality, Installation, Material, Price, Quality and Usability)
Polarity (Positive/Negative)
Note: The target variables are all encoded in the train dataset for convenience. Please submit the test results in the similar encoded fashion for us to evaluate your results.
| | Field Name  Data Type   Purpose Variable type
Id  Integer Unique identifier for each review   Input
Review  String  Review written by customers on a retail website Input
Components  String  1: aspects related to components&nbsp;  Target
        0: None 
Delivery and Customer Support   String  1: some aspects related to delivery, return, exchange and customer support&nbsp;    Target
        0: None 
Design and Aesthetics   String  1: some aspects related to components   Target
        0: None 
Dimensions  String  1: related to product dimension and size    Target
        0: None 
Features    String  1: related to product features  Target
        0 : None
Functionality   String  1: related to working of a product  Target
        0: None 
Installation    String  1: related to installation of the product   Target
        0: None 
Material    String  1: related to material of the product   Target
        0: None 
Price   String  1: related to pricing details of a product  Target
        0: None 
Quality String  1: related to quality aspects of a product  Target
        0: None 
Usability   String  1: related to usability of a product    Target
        0: None 
Polarity    Integer 1: Positive sentiment;&nbsp;    Target
        0: Negative Sentiment    |  |
| --- | --- |
|  |  | |  |
| --- | --- |
|  |  |
Skills:
Text Pre-processing – Lemmatization , Tokenization, N-Grams and other relevant methods
Multi-Class Classification, Multi-label Classification
Optimizing Log Loss
Overview
Ugam, a Merkle company, is a leading analytics and technology services company. Our customer-centric approach delivers impactful business results for large corporations by leveraging data, technology, and expertise. 
We consistently deliver superior, impactful results through the right blend of human intelligence and AI. With 3300+ people spread across locations worldwide, we successfully deploy our services to create success stories across industries like Retail & Consumer Brands, High Tech, BFSI, Distribution, and Market Research & Consulting. Over the past 21 years, Ugam has been recognized by several firms including Forrester and Gartner, named the No.1 data science company in India by Analytics Insight, and certified as a Great Place to Work®.
Problem Statement:
The last two decades have witnessed a significant change in how consumers purchase products and express their experience/opinions in reviews, posts, and content across platforms. These online reviews are not only useful to reflect customers’ sentiment towards a product but also help businesses fix gaps and find potential opportunities which could further influence future purchases. 
Participants need develop a machine learning model that can analyse customers’ sentiments based on their reviews and feedback.
NOTE: 
The prize money will be for the interested candidates who are willing to get interviewed or hired by Ugam.
Winner are requested to come to the Machine Leaning Developers Summit2022, happening at Bangalore, for receiving the prize money.
dataset link: https://machinehack.com/hackathon/uhack_sentiments_20_decode_code_words/overview"	16	230	1	manishtripathi86	uhack-sentiments-20-decode-code-words
9311	9311	netflix_titles		[]		5	52	0	manjunathgb	netflix-titles
9312	9312	data_enhance_Angle1		[]		6	83	0	shyxmql	data-enhance-angle1
9313	9313	Helmet detection dataset	Helmet images including people wearing it	['arts and entertainment', 'automobiles and vehicles', 'intermediate', 'image data']	"Context
This dataset is part of the helmet detection project. 
Content
It consists of image data. Helmet kept on the table, people wearing a helmet.
Acknowledgements
Thanks to Google, downloaded the data from the internet."	33	337	10	zwartfreak	helmet-images
9314	9314	britleness-index.csv		[]		0	18	0	ayberktigli	britlenessindexcsv
9315	9315	Energy		['energy']		0	10	1	mahdizeynali4228	energy
9316	9316	tensfwwhl		[]		2	23	0	pratik1297	tensfwwhl
9317	9317	Indian Food Images Dataset	This Dataset consist of 4000 Indian Food Images in 80 different Classes	['computer vision', 'classification', 'deep learning', 'cooking and recipes', 'food']	"Context
Indian cuisine consists of a variety of regional and traditional cuisines native to the Indian subcontinent. Given the diversity in soil, climate, culture, ethnic groups, and occupations, these cuisines vary substantially and use locally available spices, herbs, vegetables, and fruits. Indian food is also heavily influenced by religion, in particular Hinduism, cultural choices, and traditions. Centuries of Islamic rule, particularly by the Mughals, also introduced dishes like samosa and pilaf.
Historical events such as invasions, trade relations, and colonialism have played a role in introducing certain foods to this country. The Columbian discovery of the New World brought a number of new vegetables and fruit to India. A number of these such as the potato, tomatoes, chilies, peanuts, and Guava have become staples in many regions of India. Indian cuisine has shaped the history of international relations; the spice trade between India and Europe was the primary catalyst for Europe's Age of Discovery. Spices were bought from India and traded around Europe and Asia. Indian cuisine has influenced other cuisines across the world, especially those from Europe (especially Britain), the Middle East, Southern African, East Africa, Southeast Asia, North America, Mauritius, Fiji, Oceania, and the Caribbean.
Content
In this Dataset, we have 4000 Indian Food Images in 80 different categories or classes."	385	4876	83	iamsouravbanerjee	indian-food-images-dataset
9318	9318	Energy Astar		['energy']		0	8	0	mahdizeynali4228	energy-astar
9319	9319	Energy Star		['energy']		0	24	1	parvizghorbanzadeh	energy-star
9320	9320	Jiomart Products dataset	Jiomart Products data: name, image, price, discount, type, subtype	['india', 'tabular data', 'e-commerce services']	"Context
This is a dataset containing products details from the e-commerce website: Jiomart.
Content
This dataset has the following fields:
- title: name of product
- discountedPrice: discounted price of product captured on 23/12/21 11:50AM
- price: price of product captured on 23/12/21 11:50AM
- filename: product image link
- type: category of product
- subType: sub-category of product
Inspiration
Analyses of the pricing, category and brand can be performed."	97	555	9	satyamsundaram	jiomart-products-dataset
9321	9321	HR_Dataset		[]		1	40	0	swapnamadhu	hr-dataset
9322	9322	Best model Great barrier reef		[]		0	36	0	bavesh123	best-model-great-barrier-reef
9323	9323	IceVision for CUDA11	Use with GPU notebooks running CUDA 11	['computer vision', 'pytorch']		0	136	1	abee82	icevision
9324	9324	webexample1.html		[]		0	19	0	rishabhsoni01	webexample1html
9325	9325	NIFTY-50 Stocks Dataset	It contains Stock Data for 50 of the Largest Indian Companies Listed on the NSE	['business', 'beginner', 'exploratory data analysis', 'data visualization', 'tabular data', 'text data', 'investing']	"Context
The NIFTY 50 is a benchmark Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange. It is one of the two main stock indices used in India, the other being the BSE SENSEX.
Nifty 50 is owned and managed by NSE Indices (previously known as India Index Services & Products Limited), which is a wholly-owned subsidiary of the NSE Strategic Investment Corporation Limited. NSE Indices had a marketing and licensing agreement with Standard & Poor's for co-branding equity indices until 2013. The Nifty 50 index was launched on 22 April 1996, and is one of the many stock indices of Nifty.
The NIFTY 50 index has shaped up to be the largest single financial product in India, with an ecosystem consisting of exchange-traded funds (onshore and offshore), exchange-traded options at NSE, and futures and options abroad at the SGX. NIFTY 50 is the world's most actively traded contract. WFE, IOM, and FIA surveys endorse NSE's leadership position.
The NIFTY 50 index covers 13 sectors (as of 30 April 2021) of the Indian economy and offers investment managers exposure to the Indian market in one portfolio. Between 2008 & 2012, the NIFTY 50 index's share of NSE's market capitalization fell from 65% to 29% due to the rise of sectoral indices like NIFTY Bank, NIFTY IT, NIFTY Pharma, NIFTY SERV SECTOR, NIFTY Next 50, etc. The NIFTY 50 Index gives a weightage of 39.47% to financial services, 15.31% to Energy, 13.01% to IT, 12.38% to consumer goods, 6.11% to Automobiles a and 0% to the agricultural sector.
The NIFTY 50 index is a free-float market capitalization weighted index. The index was initially calculated on a full market capitalization methodology. On 26 June 2009, the computation was changed to a free-float methodology. The base period for the NIFTY 50 index is 3 November 1995, which marked the completion of one year of operations of the National Stock Exchange Equity Market Segment. The base value of the index has been set at 1000 and a base capital of ₹ 2.06 trillion.
Content
In this Dataset, we have records of all the NIFTY-50 stocks along with various parameters.
Important Note
% change is calculated with respect to adjusted price on ex-date for Dividend, Bonus, Rights & Face Value Split.
52 weeks high & 52 week low prices are adjusted for Bonus, Split & Rights Corporate actions.
365 days % Change and 30 days % Change values are adjusted With respect to corporate actions.
Acknowledgements
For more, you can visit the website of the National Stock Exchange of India Limited (NSE): https://www1.nseindia.com/"	4517	30685	227	iamsouravbanerjee	nifty50-stocks-dataset
9326	9326	Songs 2		['music']		0	29	0	danielparkes	songs-2
9327	9327	zadanie		[]		0	17	0	ilyabaranchikov	zadanie
9328	9328	11.2K_LSTM		[]		0	8	0	dipanjankaranjai	112k-lstm
9329	9329	roberta-ner2		['arts and entertainment']		0	3	0	icarus96	robertaner2
9330	9330	fruitoranges		[]		15	26	0	syifanuraini	fruitoranges
9331	9331	my_json		[]		2	36	0	eslamomar	my-json
9332	9332	penglong_data2		[]		1	29	0	tonychen52	penglong-data2
9333	9333	Video games and consoles	List of video games and consoles extracted from Launchbox-app.com	['games', 'video games', 'data cleaning', 'tabular data', 'pandas']	"Context
While working on a new data science project, I was always fascinated with video games and wondered if I could ever get a list of video games and consoles ever produced.
i came across https://launchbox-app.com and by browsing through their game and console section, I knew this site would be where I get my data.
Content
I contacted launchbox-app.com support rather than web scraping through their site and they gave me the metadata.zip file of what I needed.
Using python and the xml library, I was able to get the data and convert it to .csv format.
You can find out more on my work on this at https://github.com/thatdatascienceguy/VideoGameData and on my blog: https://thatdatascienceguy.com
Acknowledgements
Big thanks to the staff at Launchbox-app for providing me the metadata and allowing me to work on getting this information.
Do note that this data was extracted around November 2020. Some consoles and video games created afterwards may not show on these datasets."	39	248	1	thatdatasciguy	video-games-and-consoles
9334	9334	Waymo-Open Training Dataset 5		[]		0	36	0	mohammedosama	waymo-open-training-dataset-5
9335	9335	NFL Combine - Performance Data (2009 - 2019)	10 years of NFL Combine results including the draft outcome	['sports', 'beginner', 'intermediate', 'data analytics', 'classification']	"Context
This dataset contains information from the NFL Combine (2009 to 2019), including the results from sports performance tests and draft outcomes.
Content
As sports statistics are in the public domain, this database was freely downloaded from https://www.pro-football-reference.com/
Acknowledgements
I appreciate the efforts of https://www.pro-football-reference.com/ in collating and hosting sports related data, and Kaggle for providing a platform for sharing datasets and knowledge.
Inspiration
This dataset is useful for beginners and intermediate users, where they can practice visualisations, analytics, imputation, data cleaning/wrangling, and classification modelling. For example: 
What are the variables of importance in predicing round pick or draft status? 
Which school has the highest number of players being drafted into NFL?
What position type or player type is most represented at the NFL Combine?
Do drafted and undrafted players perform differently on performance tests?"	56	334	0	redlineracer	nfl-combine-performance-data-2009-2019
9336	9336	IMDb extensive		['movies and tv shows']		27	148	1	ngochieunguyen	imdb-extensive
9337	9337	CityScape_Gp		[]		1	13	0	mohamedabdelbasset	csgp2021tekomoro-iti-semanticsegmentation
9338	9338	python_module		[]		0	31	0	appleorange714	python-module
9339	9339	petfinder_wieght		[]		1	128	0	bachaboos	petfinder-wieght
9340	9340	TFRecord-GDSC		[]		0	39	0	tongkhangte	tfrecordgdsc
9341	9341	data-enhance		[]		1	41	0	dddduyunqi	dataenhance
9342	9342	HD-OCT of MH (preoperative and postoperative)	OCT scans and clinical data prior and up to 1 year after macular hole surgery.	['healthcare', 'biology', 'medicine', 'image data', 'eyes and vision']	"Prediction of visual acuity improvement after macular hole surgery
The data represent successful macular hole surgeries from 2014 to 2018 at CHU de Québec.
In total, the dataset contains 2658 OCT scans (images) from 493 patients along with their clinical data.
It is difficult for a clinician to predict what the long-term impact of this operation will be on a patient's visual acuity.
This is where the operation is interesting to analyze from a machine learning perspective.
We are interested in knowing if we can, from examples of surgery, predict to what extent the operation will have a positive impact on the patient's visual acuity.
For each patient, we have pre-operative clinical data.
Also, at different pre- and post-operative times, we have a visual acuity measurement as well as two images (horizontal and vertical OCT scans) of the eye with a macular hole.
Brief description of the data set
The data set is separated to match the reference article from Godbout et al., (2021).
It consists of 4 folders:
train: Training set in the article
validation: Validation set in the article
test: Test set in the article
others: Data set where the measured vision at 6 months is not available (not in the article)
For each of the folders, a clinical_data.csv file contains information about each patient in the record.
The csvfile has some missing data, which can be labelled as -9, -7 or simply be absent.
An octs file is also present. It contains all the OCTs owned for the patients in the dataset, stored in the format {patient_id}{time}{orientation}.tiff, where orientation is either ""H"" or ""V"", respectively for horizontal and vertical scans.
IMPORTANT: Some OCTs are missing. 
Most of the time, horizontal and vertical OCTs of a patient are available for each timestep for which a visual acuity measurement is available, but this is not always the case.
Therefore, the possible absence of OCTs must be taken into account when processing the dataset.
Features description
id: Unique identifier of the patient.
age: Age of the patient.
pseudophakic: Characteristic describing if a patient has received a cataract surgery in the past. 1 if yes, 0 otherwise.
mh_duration: Duration (in weeks) of the patient's macular hole.
elevated_edge: Presence or not of elevated edge in the patient. 1 if yes, 0 otherwise.
mh_size: Size (in micrometers) of the macular hole before the operation.
VA_baseline: Visual acuity (measured in ETDRS letters) of the patient before surgery.
VA_2weeks: Visual acuity (measured in ETDRS letters) of the patient 2 weeks after surgery.
VA_3months: Visual acuity (measured in ETDRS letters) of the patient 3 months after surgery.
VA_6months: Visual acuity (measured in ETDRS letters) of the patient 6 months after surgery.
VA_12months: Visual acuity (measured in ETDRS letters) of the patient 12 months after surgery.
References
Godbout, M., et al. ""Predicting Visual Improvement after Macular Hole Surgery: a Cautionary Tale on Deep Learning with Very Limited Data."" arXiv preprint arXiv:2109.09463 (2021)."	13	130	0	mathieugodbout	oct-postsurgery-visual-improvement
9343	9343	[CARLA] Densely Annotated Driving Dataset	CARLA Driving Simulator data, Semantically Segmented in 12 Classes	['science and technology', 'transportation', 'automobiles and vehicles', 'computer vision', 'simulations']	"Context
The DAVID data set consists of 28 video sequences of urban driving recorded
in the CARLA simulator.For each frame, the ground truth pixel-wise semantic class labels
were recorded as well. 
This dataset consists of 28 video sequences that consist of a total of 10 767 frames. An
equal number of 10 767 pixel-wise semantic labels is provided. The videos were recorded at a
frame rate of 10 Hz. The average sequence duration is 38.4 s. Half the sequences were recorded
in sunny weather. Nine sequences were recorded in rain and the remaining five were recorded
in cloudy conditions. The recorded driving scenarios include regular driving, traffic jams as
well as stopping and starting at traffic lights
Data
The images and labels folders are in .tar format because Kaggle won't let me upload more than 10000 files,so yea, they have to be extracted , like so:
!mkdir images &amp;&amp; mkdir labels 
!tar -xvf ../input/carla-densely-annotated-driving-dataset/images.tar -C ./images/
!tar -xvf ../input/carla-densely-annotated-driving-dataset/labels.tar -C ./labels/
Once extracted , the structure of the Dataset will be the follow:
images/                        Folder containing input Videos
├─ Video_XXX/                  Folder Containing each frame of the   XXX video (XXX goes from 000 to 027)
│  ├─ vXXX_0000                First frame of the XXX video
│  ├─ vXXX_0001                Second frame of the XXX video
│  ├─ ...                      etc..
labels/                        Folder containing semantically labeled videos
├─ Video_XX/                   Folder Containing each semantically labeled frame of XXX video (XXX goes from 000 to 027)
│  ├─ vXXX_0000                First semantically labeled frame of the XXX video
│  ├─ vXXX_0001                Second semantically labeled frame of the XXX video
│  ├─ ...                      etc..
classes_rgb_values.csv         CSV Contining the RGB values of each semantic class in the labeled frames and their frequency
video_info.csv                 Provides additional info on each video like the Duration ,the wether, and others"	54	1044	15	albertozorzetto	carla-densely-annotated-driving-dataset
9344	9344	GeForceNOW Games List	A List of Games available on the GeForceNOW cloud platform	['games', 'video games']	"Context
I like gaming and I like coding, so from the first days when NVidia GeForceNOW has been launched I've started to interest in the list of the games available so I have started to share the list on GFNGames.COM in a manner that it lets anyone search it
When I've started to code the steam offers the idea was clear, let's play all the Steam Games available, joking 😆 
Content
I grab the data hardly by using the Steam API and as it stops queries every hundred games so I have to wait a little bit like when I was waiting hours to load a game from a floppy disk in the early 80s
Acknowledgements
I want to thank the GFN community on REDDIT and Discord that motivates me to keep waiting the Steam API 😭 
Inspiration
I hope if we can see if there are old games that we can still play on GeForceNOW with Steam and I hope that Ubisoft and Epic will have an API too to grab their games
The code is available on https://github.com/nazimboudeffa/gfnlist"	4	112	0	nazimboudeffa	geforcenow-steam-games-list
9345	9345	CNU CIC 2021 service count	Chungnam National University, CNU, CIC, 충남대, 8282, 민원	[]		2	31	0	youngseok	cnu-cic-2021-service-count
9346	9346	Waymo-Open Training Dataset 4		[]		0	24	0	mohammedosama	waymo-open-training-dataset-4
9347	9347	paai_dataset	paai_article_dataset	[]		12	4	0	raphaelbourgade	paai-dataset
9348	9348	bert_test		[]		0	6	0	learnitanyway	bert-test
9349	9349	yolov5-lib	Just the yoloV5 from ultralytics to use in Great Barrier Reef Comp	[]		0	14	0	pedrocouto39	yolov5lib
9350	9350	mydata		['business']		0	26	0	acelinscak	mydata
9351	9351	Pretrained_weights		[]		0	15	0	angie0angie	pretrained-weights
9352	9352	Waymo-Open Training Dataset 3		[]		0	21	0	mohammedosama	waymo-open-training-dataset-3
9353	9353	standard		[]		0	15	0	kaggledoer	standard
9354	9354	tendency examples		['social science']		1	3	0	kaggledoer	tendency-examples
9355	9355	persian sms dataset	Persian real SMS Dataset	[]	persian sms (text messages) dataset	20	267	8	amirshnll	persian-sms-dataset
9356	9356	persian news dataset	Persian News Dataset	['news']	Persian News Dataset	6	152	5	amirshnll	persian-news-dataset
9357	9357	Irisflower		[]		0	26	0	getexcelsior	irisflower
9358	9358	Covid patient datasets	The Covid patient datasets dataset is collected by randomly sampling	['asia', 'health', 'classification', 'json']	The Covid patient datasets dataset is collected by randomly sampling	353	2344	3	amirshnll	covid-patient-datasets
9359	9359	nfl-2022		[]		0	35	0	vavan137	nfl2022
9360	9360	Tesla Used Car Snapshot		[]		1	69	0	robottums	tesla-used-car-snapshot
9361	9361	Divvy-Trips-year	Divvy_Exercise_Full_Year_Analysis	['travel']		0	15	1	soutanbasak	divvytripsyear
9362	9362	train_processed_annotations_coco		[]		1	20	0	youssefelkilany	train-processed-annotations-coco
9363	9363	50_startup dataset		['earth and nature']		3	28	0	ayushsarraf0731	50-startup-dataset
9364	9364	Sign Language Digits Dataset		['computer vision', 'deep learning', 'keras', 'tensorflow', 'python']	"Details of datasets:
Image size: 100 x 100 pixels
Color space: RGB
Number of classes: 10 (Digits: 0-9)
Number of participant students: 218
Number of samples per student: 10
This data set contains images of sign language digits. There are ten classes, labeled as 0 through 9, and each class is made up of images of hands showing the sign for that particular digit.
Each class has between 204 and 208 samples. The total data set contains 2062 samples
By Turkey Ankara Ayrancı Anadolu High School Students
Turkey Ankara Ayrancı Anadolu High School's Sign Language Digits Dataset
This dataset is prepared by our school students.
https://github.com/ardamavi/Sign-Language-Digits-Dataset"	23	165	4	javaidahmadwani	sign-language-digits-dataset
9365	9365	COTTS processed CSV		[]		39	48	1	yovinyahathugoda	cotts-processed-csv
9366	9366	chomeo		[]		2	6	0	vnglvn	chomeo
9367	9367	longggg		[]		0	31	0	dngthnhlong0379	longggg
9368	9368	PetFinder-Model38-1		[]		0	16	0	lftuwujie	petfindermodel381
9369	9369	PetFinder-Model38-2		[]		0	4	0	lftuwujie	petfindermodel382
9370	9370	famdata		[]		3	77	0	sacphung	famdata
9371	9371	Data Science Job Postings and Salaries	Job postings scraped from SimplyHired.com	['business', 'exploratory data analysis', 'data cleaning', 'numpy', 'pandas', 'sklearn']	"Context
This dataset contains California (US) job postings for data science-related jobs taken from SimplyHired.
Content
The dataset was acquired by scraping on 17 December 2021. There are four CSV files contained in this dataset. One of the CSV files (raw_data.csv) contains the 1342 rows of data from scraping. The other three CSV files were created from the raw_data.csv file by cleaning (not including imputation). These other three CSV files are:
title_location_company_salary.csv which contains 1288 cleaned job titles, companies, locations, and salaries,
qualifications.csv which contains the cleaned qualifications given by a binary matrix and is related to the title_location_company_salary.csv file by the index, and
benefits.csv which contains the cleaned benefits given by a binary matrix and is related to the title_location_company_salary.csv file by the index.
The scraper and cleaning script can be found at this GitHub repo.
Acknowledgements
The job postings can be found here.
Inspiration
This dataset can be used for practicing data cleaning, exploratory data analyses, and predictive modeling."	238	1469	15	michaelbryantds	california-salaries-in-data-science
9372	9372	Maker autoencoder		['computer science']		0	16	0	yaajnusubramanian	maker-autoencoder
9373	9373	102 Category Oxford Models		['earth and nature', 'clothing and accessories', 'multiclass classification', 'pytorch']	"Context
https://github.com/av192/Flower-Classifier
Content
Contains model trained on 102 Category oxford flower dataset
Acknowledgements
Trained on https://www.kaggle.com/c/oxford-102-flower-pytorch/ data. Link to original data set website -  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/"	1	62	1	apoorvbhardwaj	102-category-oxford-models
9374	9374	TRUNET_Dataset		[]		4	22	0	hemantkhairnar	trunet-dataset
9375	9375	2141241212		[]		0	20	0	pudge228	2141241212
9376	9376	ImagesDataSet		[]		3	18	0	khaledanaqwa	imagesdataset
9377	9377	scvp dataset	Learning One-shot View Planning via Set Covering	['earth and nature', 'deep learning']	"Context
This is the dataset and pre-trained model for https://github.com/psc0628/SCVP-Simulation.
Content
Each object has the pair of grid.txt and ids.txt for each rotation.
last.pth.tar is the pre-trained SCVP network model."	0	118	0	sicongpan	scvp-dataset
9378	9378	tfgbr yolox clahe fold 0		[]		0	22	0	ks2019	tfgbr-yolox-clahe-fold-0
9379	9379	MatchingModel22/12		[]		0	31	0	dunglduy	matchingmodel2212
9380	9380	Underwater_image_enhancement		[]		1	25	1	yovinyahathugoda	underwater-image-enhancement
9381	9381	Wiki-raw		[]		1	19	0	a24998667	wiki-raw
9382	9382	santa-brand-new-bag		['religion and belief systems']		0	36	2	crained	santabrandnewbag
9383	9383	tf-longformer		[]		0	64	0	ryokusnadi	tflongformer
9384	9384	Airline Customer Service		[]		12	86	0	josema95	airline-customer-service
9385	9385	COVID-19 Worldwide	A dataset that describes and shows COVID-19 status of 224 countries	['covid19']	"Data was collected from Jan/24/2019 till Dec/20/2021
Content
The dataset has 8 columns which are: (Id, Country, Cases, Deaths, Recovered, Active, Critical, Population)
Id: Identifier of a country
Country: Name of the country
Cases: Number of cases
Deaths: Number of deaths
Recovered: Number of recovered cases
Active: Number of active cases
Critical: Number of critical cases
Population: The population of the country"	60	332	7	khaiid	covid19-jan2019-dec2021
9386	9386	norfair-0.3.1-py3		[]		162	798	19	parapapapam	norfair031py3
9387	9387	Dataset		[]		0	46	0	sandeepanmukherjee	dataset
9388	9388	UPI_DATA		[]		0	16	0	dnyaneshpainjane	upi-data
9389	9389	North East Monsoon - Weather Data	The rain in Tamil Nadu at this time is primarily caused by the NEM	['weather and climate']	"Context
Weather forecasts are made by collecting as much data as possible about the current state of the atmosphere (particularly the temperature, humidity and wind) and using understanding of atmospheric processes (through meteorology) to determine how the atmosphere evolves in the future.
Content
This data collected from IOT devices as well as local weather station, this experiment done to understand how atmospheric pressure changes over the period of time.
Acknowledgements
We wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.
Inspiration
Your data will be in front of the world's largest data science community. What questions do you want to see answered?"	28	263	7	santhoshkumarv	north-east-monsoon-weather-data-tamilnadu
9390	9390	malignant_test		[]		0	23	1	chinwaichun	malignant-test
9391	9391	belign_test		[]		0	15	1	chinwaichun	belign-test
9392	9392	Concept Drifted Data	Instagram Concept Drifted Datasets	['data analytics', 'classification', 'adversarial learning']	"Content
There is 1 training dataset, and 54 concept drifted testing datasets.
Acknowledgements
Please cite Gabriella Colletti for use of these datasets."	7	129	4	gabriellacolletti	concept-drifted-data
9393	9393	akhmatova		[]		0	35	0	kharitonovanatalya	akhmatova
9394	9394	Corn-Diseases		['food']		5	49	0	rahmalisaaulia	corndiseases
9395	9395	IAM datset of words		[]		9	35	0	nikhilvinodkhodake	iam-datset-of-words
9396	9396	Roberta_ner		[]		0	28	0	icarus96	roberta-ner
9397	9397	Commission Model examples	Excel, easy to understand	['finance', 'data analytics', 'investing']	"Context
I am showcasing the financial commissions model on Kaggle. On Excel we can utilize IF statements to chart rates that reward workers based on quotas. By compiling sales on a large or small scale we can easily derive the necessary compensation for workers. 
Content
The first sheet uses simple IF statements to derive a commission payment for different rates. The Sales company exceeded their quota of  $95,000.00, 
 and reached $99,343.00, which is a 104.6% return on investment. 
On sheet 2 there is a detailed breakdown of individual employee rates and their deserved commission. The difference in sheet 2 is the use of nested IF statements, which can get much more complex if not catalogued properly.
Acknowledgements
There are two guides on YouTube which I credit heavily for these models here are the links:
https://www.youtube.com/watch?v=bkrSVS7-CYo&list=PLQnuraB9JKXdUlDVZtcfG2_sO_uL_XyMm&index=4
https://www.youtube.com/watch?v=0Ahqr6Xdkos&list=PLQnuraB9JKXdUlDVZtcfG2_sO_uL_XyMm&index=12
Inspiration
Thanks for reading, and enjoy!"	4	39	2	lamarmcmillan	commission-model-examples
9398	9398	t74_Database	From National Chin-Yi University of Technology	['universities and colleges']		0	63	0	vivia0525	t74-database
9399	9399	data test		[]		0	27	0	fannysalsabila	data-test
9400	9400	russian_alcohol_consumption		[]		8	35	0	hrshilptel	russian-alcohol-consumption
9401	9401	video frame restoration		[]		2	77	0	cookiemonsteryum	video-frame-restoration
9402	9402	Sartorius-Pre-801-20211222140639		[]		0	35	0	hideyukizushi	sartorius-pre-801-20211222140639
9403	9403	video frame restoration		[]		2	77	0	cookiemonsteryum	video-frame-restoration
9404	9404	Sartorius-Pre-801-20211222140639		[]		0	35	0	hideyukizushi	sartorius-pre-801-20211222140639
9405	9405	eurosat_numpy		[]		1	21	0	adithyansukumar	eurosat-numpy
9406	9406	statusData		[]		0	14	0	jayantsingh	statusdata
9407	9407	The Matrix	 Movie Transcripts of Matrix Trilogy	['movies and tv shows', 'art', 'categorical data', 'text data']	"The Matrix
The Matrix is a 1999 science fiction action film written and directed by the Wachowskis. It is the first installment in The Matrix film series, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano. It depicts a dystopian future in which humanity is unknowingly trapped inside a simulated reality, the Matrix, which intelligent machines have created to distract humans while using their bodies as an energy source. When computer programmer Thomas Anderson, under the hacker alias ""Neo"", uncovers the truth, he ""is drawn into a rebellion against the machines"" along with other people who have been freed from the Matrix.
Learn More.
This dataset was scrapped Online.
File System
Movie-Name.csv  - Movie Name in  the Matrix Trilogy 
Column Name
1. Character_Name - the movie character name who said the dialogue.
2. Character_dialogue - dialogue uttered by the actor/character"	6	67	0	nixongeno	the-matrix-movie-transcripts
9408	9408	Sartorius-Pre-800-20211222134413		[]		0	4	0	hideyukizushi	sartorius-pre-800-20211222134413
9409	9409	ClimatsAUS		[]		3	23	0	samuelguerin	climatsaus
9410	9410	petfinder_334Model		[]		0	43	0	ktakita	petfinder-334model
9411	9411	New Plant Grape Diseases		['food']		6	90	2	fannysalsabila	new-plant-grape-diseases
9412	9412	assignmentdatascience		[]		0	43	0	mhasheem	assignmentdatascience
9413	9413	LSTM_11K_SPACED		[]		0	36	0	dipanjankaranjai	lstm-11k-spaced
9414	9414	LSTM_corrected_Spaced_11.2k		[]		0	28	0	dipanjankaranjai	lstm-corrected-spaced-112k
9415	9415	SwinTransfomers-ImageNet	Pretrained on ImageNet	[]		3	96	1	ziqhua	swintransfomerlargeimagenet
9416	9416	LSTM_11.2k		[]		0	31	0	dipanjankaranjai	lstm-112k
9417	9417	disease and their symptoms		['health conditions']		91	499	1	hagari	disease-and-their-symptoms
9418	9418	Indian Vehicle Dataset	Indian Vehicle dataset for Detection,Classification problems	['india', 'transportation', 'automobiles and vehicles', 'computer vision', 'deep learning', 'image data']	"This dataset consists of images of niche Indian Vehicle such as Autorikshaw, Tempo, trucks, etc.
Introduction
The dataset consists of niche Indian vehicle images for classification and object detection. It is observed that there is very little or no dataset available on these niche vehicles like autorickshaw, tempo, trucks, etc. The images have been taken in varied weather conditions in daylight, evening, and night. The dataset has a wide variety of variations of illumination, distances, viewpoints, etc. This dataset represents a very challenging set of images of vehicles of niche classes. This dataset can be used for imge recognition and object detection for driver assistance systems, autonomous driving, etc.
Vehicle Classes
Indian Auto
Indian Truck
Bus
Truck
Tempo Traveller
Tractor
Car
Two Wheelers
Dataset Features
Approx. 10,000+ unique images for each class
Captured by 3000+ unique users
Captured with 50+ cities across India
Captured using mobile phones
Highly diverse
Various lighting conditions like day, night
Outdoor scene with a variety of viewpoints
Dataset Format
Classification and detection annotations available
Multiple category annotations possible
COCO, PASCAL VOC and YOLO formats
To get more images, Visit: https://github.com/datacluster-labs/Datacluster-Datasets/tree/gh-pages/Indian%20Vehicle%20Image%20Dataset
To download full datasets or to submit a request for your dataset needs, please contact on sales@datacluster.ai
Visit www.datacluster.in to know more.
Note:
All the images are manually verified and are contributed by the large contributor base on DataCluster platform."	308	3584	9	dataclusterlabs	indian-vehicle-dataset
9419	9419	augmented osteoporosis without osteophenia		['health conditions']		6	92	0	smruthisathyamoorthy	augmented-osteoporosis-without-osteophenia
9420	9420	Weekly Mortality, Republic of Korea		[]		10	261	1	unifey	weekly-mortality-republic-of-korea
9421	9421	binance btcusdt trade data 594360906-713393846		['business']		0	14	0	hejianhua198711	binance-btcusdt-trade-data-594360906713393846
9422	9422	dcn_liqitong		[]		1	30	0	liqitong	dcn-liqitong
9423	9423	xlnet-base-cased-toxic		[]		7	113	1	vaby667	xlnetbasecasedtoxic
9424	9424	sturges-oof		['art']		2	51	0	ferlockx	sturgesoof
9425	9425	rice_oof		[]		1	32	0	ferlockx	rice-oof
9426	9426	test111		[]		0	43	0	ilyabaranchikov	test111
9427	9427	talib11		[]		0	33	0	fansichen	talib11
9428	9428	Student online learning data		[]		19	242	0	chdedu	student-online-learning-data
9429	9429	sartorius coco train and val annotations		['arts and entertainment']		0	30	3	soumya9977	sartorius-coco-train-and-val-annotations
9430	9430	CyclisticBikeShareAnalysis		[]		0	44	0	karthiknn14	cyclisticbikeshareanalysis
9431	9431	MatchingModel22/12		[]		0	32	0	fakerhust	matchingmodel2212
9432	9432	Timm CoAt pretrained weights		[]		0	20	0	andralex	timm-coat-pretrained-weights
9433	9433	Great-Barrier-Reef-in-YOLO-Format		[]		5	71	0	samuelfung206	greatbarrierreefinyoloformat
9434	9434	Stock data		[]		0	46	0	starblack	stock-data
9435	9435	Solana Historical Data	Solana Historical data from 11-1-2019 to 22-12-2021	['business', 'beginner', 'data analytics', 'investing', 'currencies and foreign exchange']	"Solana
Solana is a public blockchain platform. It achieves consensus using the proof of stake mechanism. Its internal cryptocurrency is SOL. In 2021, Bloomberg journalist Joanna Ossinger described Solana as ""a potential long-term rival for Ethereum"", citing superior transaction speeds and lower associated costs.
Data
The dataset contains a total of 5 features. The details for them are as follows:
Close Price — It is the market close price for currency for that particular day.
High Price — It is the highest price of currency for the day.
Low Price — It is the lowest price for currency for that day.
Open Price — It is the market open price for currency for that day.
Volume from / to — The volume of currency that is being in the trade for that day
Goal
Prediction of the price"	1	22	2	mohamedabidi97	solana-historical-data
9436	9436	Elon Musk's tweets	Elon Musk's tweets from 2010-06-04 to 2017-04-05.	['internet']		16	119	2	mohamedabidi97	elon-musks-tweets
9437	9437	Fer2013PD		[]		0	30	0	gengxiaochao	fer2013pd
9438	9438	Cleaned_Cyclistic_Data	2021 data cleaned for lat, lng, station id, negative duration	['business']	"This is a bike sharing data for a fictitious Cyclistic company. Actual data is based on Divvy, a Chicago bike sharing.
The original data was cleaned using Postgres and Google Sheet
This data has been cleaned to exclude data that's missing the station IDs and trips that has duration over 24 hours. 
Several columns was created to calculate trip durations and day of the week.
--- Finding duplicate , assumed duplicated ID means duplicated data. Result = no duplicated data found---
SELECT ride_id, COUNT(ride_id) AS ride_id_count
FROM ""Cyclistic""
GROUP BY ride_id
HAVING COUNT(ride_id)&gt;1
--- Extract station table for data cleaning ----
SELECT DISTINCT start_station_name, start_station_id, end_station_id, end_station_name
FROM ""Cyclistic""
ORDER BY start_station_name ;
Using Google Sheet Clean start_station_id code, clean missing station name, clean station id with extra .0, Assign id to NULL station data 
---- Update main table with cleaned station name and id ----
UPDATE ""Cyclistic""
SET end_lng = lng
FROM ""cleaned_station_info""
WHERE start_station_id = id;
---- Original data has latitude and longitude data that varies by small amount of decimal points. To make the data more uniform, the latitude and longitude were averaged out based on the station ID and use 8 decimal points for location accuracy. Data was then checked using Google Maps to make sure data is accurate to the nearest Divvy location in Chicago. ---
SELECT DISTINCT start_station_id, start_station_name, ROUND(AVG(start_lat)::DECIMAL,8) lat, ROUND(AVG(start_lng)::DECIMAL,8) lng
FROM ""Cyclistic""
GROUP BY start_station_id, start_station_name
ORDER BY start_station_id
--- Create a cleaned table for export excluding data that are less than 2 minutes and more than 24 hours. Based on data where duration less than 2 minutes, the ride always ends up at the same station. It is assumed that the rider canceled the ride or had trouble using the bike, therefore this data is excluded. For data more than 24 hours it's assumed that there's an error in docking in the bicycle or other problem with logging out of the ride. 
New table also exclude NULL data where start_station_name and end_station_name is missing ---
SELECT \
* \
FROM ( \
SELECT \
ride_id, member_casual, rideable_type,\
start_station_id, start_station_name,\
end_station_id, end_station_name,\
started_at, ended_at,\
ended_at - started_at as duration,\
start_lat, start_lng, end_lat, end_lng\
FROM ""Cyclistic""\
WHERE start_station_name IS NOT NULL AND end_station_name IS NOT NULL
) AS duration_tbl\
WHERE duration &gt;= '00:02' and duration &lt;= '24:00' \"	2	38	0	stanleyprawiradjaja	cleaned-cyclistic-data
9439	9439	fer2013N		[]		0	28	0	ningkang218	fer2013n
9440	9440	startup		[]		0	41	0	shijiecao1718119	startup
9441	9441	9aprelya_xlsx		[]		0	25	0	ilyabaranchikov	9aprelya-xlsx
9442	9442	RealToxicityPrompts		[]		1	29	1	wataoka	realtoxicityprompts
9443	9443	MildDemented2		[]		0	25	0	kaggleliuyuan	milddemented2
9444	9444	PTB-XL - AF detection	Detecting atrial fibrilation from ecg signals	['heart conditions']		2	64	0	giovannibaj	ptbxl-af-detection
9445	9445	subject_dataset		[]		0	19	0	kseniadrokov	subject-dataset
9446	9446	Astolfodtaset		[]		2	18	0	stibeta	astolfodtaset
9447	9447	ML Regression Study Dataset	Machine Learning Regression Study Dataset	['computer science', 'beginner', 'regression']		5	74	4	rhythmcam	ml-regression-study-dataset
9448	9448	agv bana voc		[]		2	78	1	mradul2	agv-bana-voc
9449	9449	CSV SHOp		[]		1	52	0	abdullahaz1z	csv-shop
9450	9450	feedback_data_torch_ner		[]		0	30	0	majormz	feedback-data-torch-ner
9451	9451	submits		[]		0	24	0	petarou	submits
9452	9452	自练习用的练习数据		[]		0	7	0	hise1031	lianxishuju
9453	9453	clearwei		[]		1	32	0	llixxin	clearwei
9454	9454	clearwei		[]		3	43	0	marylli	clearwei
9455	9455	swin_transformer_384_1k_pth	swin_transformer_384_1k_pth	[]		44	102	2	czs1311	swin-transformer-384-1k-pth
9456	9456	Data_path		[]		1	38	0	sanasubhedar	data-path
9457	9457	Find the Best Mobile App Development Company in	Mobile App Development Company in banglore	['categorical data', 'computer science', 'programming']	Aalpha is one of the top mobile app development company in Bangalore. They have a team of experienced professionals that are close to the product.	1	44	0	anushapeter	find-the-best-mobile-app-development-company-in
9458	9458	YOLOV5-new		[]		0	26	0	dengyuchen	yolov5new
9459	9459	abisheks-folds		['arts and entertainment']		1	38	0	omomo17	abisheksfolds
9460	9460	Reddit Art Collection (512x512) (NSFW included)		['online communities']		4	67	0	dmitrishevchenko	reddit-art-collection-512x512-nsfw-included
9461	9461	all_data_vnm	VNM testing demo prediction	[]		3	75	0	manhnd1105	all-data-vnm
9462	9462	fegtjh		[]		0	13	0	xxxjjjqqq	fegtjh
9463	9463	cascade_mask_rcnn_x101_train_12epoch		[]		1	38	0	daicongxmu	cascade-mask-rcnn-x101-train-12epoch
9464	9464	Divvy BikeSharing Data | Impact of COVID (2015-21)	Chicago Divvy dataset to analyze the impact of riding preference due to COVID.	['united states', 'transportation', 'cycling', 'tabular data', 'covid19']	"Context
Year wise Divvy Bike Sharing data from 2015 to September 2021, to analyze change in movement pattern and people preferences in Bike sharing.
Content
Dataset contains starting time, ending time, start station id, end station id along with trip duration for each rides.
Acknowledgements
Source: Divvy Bikesharing dataset is publicly available at https://divvy-tripdata.s3.amazonaws.com/index.html.
Smaller chunks of data has been downloaded, transformed, cleaned and aggregated yearwise. This dataset has aggregated data from 2015 to September 2021.
Inspiration
This dataset was created with an intention to analyze people's change in preference in Ride-Sharing services due to Covid-19 (Corona Virus pandemic). How various government policies impacted the ridership, and explore change in people's movement before and after pandemic.
To know further about how the dataset was created, please refer this article here: https://laxmena.com/posts/divvy-covid-part1"	106	1712	11	laxmena	divvy-ridesharing-dataset
9465	9465	bert_embeddings		[]		5	56	0	azzamradman	bert-embeddings
9466	9466	kaerururu-petfinder2-075		[]		0	5	0	kaerunantoka	kaerururu-petfinder2-075
9467	9467	Company IPOs (2019 - 2021)	List of all company IPOs from 2019 to 2021	['business', 'finance', 'law', 'investing']	"Company IPO : 2019 - 2021
An initial public offering (IPO) refers to the process of offering shares of a private corporation to the public in a new stock issuance. An IPO allows a company to raise capital from public investors. The transition from a private to a public company can be an important time for private investors to fully realize gains from their investment as it typically includes a share premium for current private investors. Meanwhile, it also allows public investors to participate in the offering.
Global initial public offering (IPO) issuances and proceeds set a new record in 2021, rising 64% and 67% respectively compared with 2020 The fourth quarter of the year was particularly active, with deal numbers reaching their highest since the same period in 2007 However, growth in IPO activity in Asia Pacific was relatively weak
Source
https://stockanalysis.com/"	535	3243	27	shivamb	company-ipos-2019-2021
9468	9468	Dude Perfect Youtube Stats and Video Thumbnails		[]		1	72	4	robikscube	dude-perfect-youtube-stats-and-video-thumbnails
9469	9469	Cocomelon Youtube Video Stats and Thumbnails	Stats for one of the top youtube channels with kids videos!	[]		7	50	6	robikscube	cocomelon-youtube-video-stats-and-thumbnails
9470	9470	Mr Beast Youtube Video Statistics	View, Like and Comment stats for all 250 MrBeast Youtube Videos	['arts and entertainment', 'social science', 'time series analysis']	"MrBeast Youtube Video Stats
MrBeast is one of the biggest youtubers ever. His videos are some of the most viewed of all time and he has perfected the art of gaining views.
This dataset was created using youtube's official api and shows the date created, view count, comments, and upvote counts for all of MrBeast's videos as of December 20, 2021."	87	840	22	robikscube	mr-beast-youtube-video-statistics
9471	9471	Bike detection dataset	Motorbike on road, single and multiple motorbikes	['automobiles and vehicles', 'beginner', 'intermediate', 'image data', 'video data']	"Context
This dataset is part of my final year project where we had to detect moving bikes.
Content
This dataset consists of images of bikes on road. You can train your model to detect bikes using this dataset.
Acknowledgements
Thanks to Google, I collected all images from the internet."	43	332	9	zwartfreak	bike-images
9472	9472	Helmet_Detection_Dataset	Safty_Helmet_Detection 	[]		0	56	0	yiyanghao	ntt-east
9473	9473	Predicting Divorce	Predict if a couple will divorce using ML	['culture and humanities', 'public health', 'social science', 'random forest']	"Context
Answers to certain questions can provide key information regarding if a couple is likely to get divorced in the future.
Content
Attribute Information:
Questions are ranked on a scale of 0-4 with 0 being the lowest and 4 being the highest. The last category states if the couple has divorced. 
If one of us apologizes when our discussion deteriorates, the discussion ends.
I know we can ignore our differences, even if things get hard sometimes.
When we need it, we can take our discussions with my spouse from the beginning and correct it.
When I discuss with my spouse, to contact him will eventually work.
The time I spent with my wife is special for us.
We don't have time at home as partners.
We are like two strangers who share the same environment at home rather than family.
I enjoy our holidays with my wife.
I enjoy traveling with my wife.
Most of our goals are common to my spouse.
I think that one day in the future, when I look back, I see that my spouse and I have been in harmony with each other.
My spouse and I have similar values in terms of personal freedom.
My spouse and I have similar sense of entertainment.
Most of our goals for people (children, friends, etc.) are the same.
Our dreams with my spouse are similar and harmonious.
We're compatible with my spouse about what love should be.
We share the same views about being happy in our life with my spouse
My spouse and I have similar ideas about how marriage should be
My spouse and I have similar ideas about how roles should be in marriage
My spouse and I have similar values in trust.
I know exactly what my wife likes.
I know how my spouse wants to be taken care of when she/he sick.
I know my spouse's favorite food.
I can tell you what kind of stress my spouse is facing in her/his life.
I have knowledge of my spouse's inner world.
I know my spouse's basic anxieties.
I know what my spouse's current sources of stress are.
I know my spouse's hopes and wishes.
I know my spouse very well.
I know my spouse's friends and their social relationships.
I feel aggressive when I argue with my spouse.
When discussing with my spouse, I usually use expressions such as ‘you always’ or ‘you never’ .
I can use negative statements about my spouse's personality during our discussions.
I can use offensive expressions during our discussions.
I can insult my spouse during our discussions.
I can be humiliating when we discussions.
My discussion with my spouse is not calm.
I hate my spouse's way of open a subject.
Our discussions often occur suddenly.
We're just starting a discussion before I know what's going on.
When I talk to my spouse about something, my calm suddenly breaks.
When I argue with my spouse, ı only go out and I don't say a word.
I mostly stay silent to calm the environment a little bit.
Sometimes I think it's good for me to leave home for a while.
I'd rather stay silent than discuss with my spouse.
Even if I'm right in the discussion, I stay silent to hurt my spouse.
When I discuss with my spouse, I stay silent because I am afraid of not being able to control my anger.
I feel right in our discussions.
I have nothing to do with what I've been accused of.
I'm not actually the one who's guilty about what I'm accused of.
I'm not the one who's wrong about problems at home.
I wouldn't hesitate to tell my spouse about her/his inadequacy.
When I discuss, I remind my spouse of her/his inadequacy.
I'm not afraid to tell my spouse about her/his incompetence.
**
Acknowledgements
Relevant Papers:
Yöntem, M , Adem, K , İlhan, T , Kılıçarslan, S. (2019). DIVORCE PREDICTION USING CORRELATION BASED FEATURE SELECTION AND ARTIFICIAL NEURAL NETWORKS. Nevşehir Hacı Bektaş Veli University SBE Dergisi, 9 (1), 259-273. Retrieved from [Web Link]
Citation Request:
Yöntem, M , Adem, K , İlhan, T , Kılıçarslan, S. (2019). DIVORCE PREDICTION USING CORRELATION BASED FEATURE SELECTION AND ARTIFICIAL NEURAL NETWORKS. Nevşehir Hacı Bektaş Veli University SBE Dergisi, 9 (1), 259-273. Retrieved from [Web Link]
Inspiration
What are the key indicators for divorce?
Which questions/factors are most significant when predicting divorce?"	938	7726	40	csafrit2	predicting-divorce
9474	9474	stl_unmt_ende_dataset		[]		0	23	0	hitmiss	stl-unmt-ende-dataset
9475	9475	supermarket analysis		[]		2	43	0	ankitjayara	supermarket-analysis
9476	9476	Loans Full schema		[]		1	12	0	mayurspawar	loans-full-schema
9477	9477	Loan Dataset		[]		0	17	0	mayurspawar	loan-dataset
9478	9478	Adult Income		[]		5	57	0	angadk268	adult-income
9479	9479	py_code10_22		[]		0	26	1	jongkilee	py-code10-22
9480	9480	0812 961 3804 Dijual rumah jakarta Cibubur Depok	Dijual rumah jakarta Cibubur Depok	['marketing', 'internet', 'beginner', 'image data']		0	49	0	dijualrumahcibubur	0812-961-3804-dijual-rumah-jakarta-cibubur-depok
9481	9481	jay music midi		[]		1	10	0	fkmouse	jay-music-midi
9482	9482	nlpDemo		[]		0	1	0	qcchcp	nlpdemo
9483	9483	Christmas Quotes	Quotes tagged christmas	['movies and tv shows', 'religion and belief systems', 'text mining', 'text data', 'holidays and cultural events']	"Context
Christmas is Coming.
Content
The dataset consist of all 1291 quotes tagged christmas on goodreads (https://www.goodreads.com/quotes/tag/christmas). The authors, tags and amount of likes on each quote makes up the attributes of the dataset 
Acknowledgements
We wouldn't be here without the help of: https://www.goodreads.com/
Inspiration
Spirit of Christmas"	37	361	9	ikpeleambrose	christmas-quotes
9484	9484	Continue training GPT2 1 Ep		['arts and entertainment']		0	5	0	jiaminggogogo	continue-training-gpt2-1-ep
9485	9485	TD Gifs		[]		0	36	0	bkumagai	td-gifs
9486	9486	train_weights		[]		1	70	0	tonychen52	weights
9487	9487	nico_dataset		[]		0	46	0	derekqu	nico-dataset
9488	9488	basearg		[]		0	7	0	toshikiharaguchi	basearg
9489	9489	models		['clothing and accessories']		16	202	0	kazrico	models
9490	9490	Combined Dataset for Emotion Recognition Mels		['online communities']		1	89	0	phantasm34	emo-data-mels
9491	9491	largearg_dp02		[]		0	4	0	yutotakaki	largearg-dp02
9492	9492	bdb_predsV1	bdb_preds_conditionalrf	[]		5	46	0	sportsstatseli	bdb-predsv1
9493	9493	Stanford Earthquake Dataset (STEAD)	A Global Data Set of Seismic Signals for AI	['earth science', 'geology', 'physics', 'signal processing']	"Properties of the dataset
STEAD includes two main classes of earthquake and non-earthquake signals recorded by seismic instruments. The earthquake class contains only one category of local-earthquakes with about 1,050,000 three component seismograms (each 1 minute long) associated with ~ 450,000 earthquakes that occurred between January 1984 and August 2018. The earthquakes in the data set were recorded by 2,613 receivers (seismometers) worldwide located at local distances (within 350 km of the earthquakes). The non-earthquake class currently contains only one category of seismic noise including ~100,000 samples. Most of the seismograms have been recorded since 2000 in the United States and Europe where denser station coverage is available.
The seismic data is provided as individual NumPy arrays containing three waveforms (each waveform has 6000 samples associated with 60 seconds of ground motion recorded in east-west, north-south, and vertical directions respectively). 35 attributes (labels) for each earthquake and 8 attributes for each noise seismogram are associated with each NumPy array. Noise attributes are mainly limited to the information about the recording instrument (e.g. network code, code, type, and location of the receiver). For the earthquake data, attributes include the station information, the earthquake information (e.g. origin time, epicentral location, depth, magnitude, magnitude type, focal mechanism, arrival times of P and S phases, estimated errors, etc), and recorded signal (e.g. measurement of the signal-to-noise ratio for each component, the end of signal’s dominant energy (coda-end), and epicentral distance).
The dataset is divided into 2 files:
- merge.csv - Contains relevant metadata
- merge.hdf5 - Contains the seismic waveforms
Original paper:
Mousavi, S. M., Sheng, Y., Zhu, W., Beroza G.C., (2019). STanford EArthquake Dataset (STEAD): A Global Data Set of Seismic Signals for AI, IEEE Access, doi:10.1109/ACCESS.2019.2947848"	31	505	5	isevilla	stanford-earthquake-dataset-stead
9494	9494	Asphalt Cracks		[]		2	46	0	abrahamanderson	asphalt-cracks
9495	9495	Maternal Health Risk Data	Predicting health risks for pregnant patients	['healthcare', 'public health', 'health', 'regression', 'health conditions']	"Context
Data has been collected from different hospitals, community clinics, maternal health cares through the IoT based risk monitoring system.
Age: Age in years when a woman is pregnant.
SystolicBP: Upper value of Blood Pressure in mmHg, another significant attribute during pregnancy.
DiastolicBP: Lower value of Blood Pressure in mmHg, another significant attribute during pregnancy.
BS: Blood glucose levels is in terms of a molar concentration, mmol/L.
HeartRate: A normal resting heart rate in beats per minute.
Risk Level: Predicted Risk Intensity Level during pregnancy considering the previous attribute.
Acknowledgements
Relevant Papers:
Ahmed M., Kashem M.A., Rahman M., Khatun S. (2020) Review and Analysis of Risk Factor of Maternal Health in Remote Area Using the Internet of Things (IoT). In: Kasruddin Nasir A. et al. (eds) InECCE2019. Lecture Notes in Electrical Engineering, vol 632. Springer, Singapore. [Web Link]
IoT based Risk Level Prediction Model for Maternal Health Care in the Context of Bangladesh, STI-2020, [under publication in IEEE]
Inspiration
Which health conditions are the strongest indications for health risks during pregnancy?"	519	4103	19	csafrit2	maternal-health-risk-data
9496	9496	Medical Personal Protective Equipment Images		['medicine', 'image data', 'covid19']	"Context
CPPE - 5 (Medical Personal Protective Equipment) is a new challenging dataset with the goal to allow the study of subordinate categorization of medical personal protective equipment, which is not possible with other popular data sets that focus on broad level categories.
Some features of this dataset are:
high quality images and annotations (~4.6 bounding boxes per image)
real-life images unlike any current such dataset
Labels
Labels
1 -Coverall
2 - Face_Shield
3 -Gloves
4-Goggles
5-Mask
Acknowledgements
Dagli, Rishit, and Ali Mustufa Shaikh. ‘CPPE-5: Medical Personal Protective Equipment Dataset’. ArXiv:2112.09569 [Cs], Dec. 2021. arXiv.org, http://arxiv.org/abs/2112.09569."	3	95	3	skaarface	medical-personal-protective-equipment-images
9497	9497	 Higher Education Students Performance Evaluation	Machine Learning Prediction Analysis	['research', 'education', 'multiclass classification']	"Abstract
The data was collected from the Faculty of Engineering and Faculty of Educational Sciences students in 2019. The purpose is to predict students' end-of-term performances using ML techniques.
Attribute Information:
Student ID
1- Student Age (1: 18-21, 2: 22-25, 3: above 26)
2- Sex (1: female, 2: male)
3- Graduated high-school type: (1: private, 2: state, 3: other)
4- Scholarship type: (1: None, 2: 25%, 3: 50%, 4: 75%, 5: Full)
5- Additional work: (1: Yes, 2: No)
6- Regular artistic or sports activity: (1: Yes, 2: No)
7- Do you have a partner: (1: Yes, 2: No)
8- Total salary if available (1: USD 135-200, 2: USD 201-270, 3: USD 271-340, 4: USD 341-410, 5: above 410)
9- Transportation to the university: (1: Bus, 2: Private car/taxi, 3: bicycle, 4: Other)
10- Accommodation type in Cyprus: (1: rental, 2: dormitory, 3: with family, 4: Other)
11- Mother's education: (1: primary school, 2: secondary school, 3: high school, 4: university, 5: MSc., 6: Ph.D.)
12- Father's education: (1: primary school, 2: secondary school, 3: high school, 4: university, 5: MSc., 6: Ph.D.)
13- Number of sisters/brothers (if available): (1: 1, 2:, 2, 3: 3, 4: 4, 5: 5 or above)
14- Parental status: (1: married, 2: divorced, 3: died - one of them or both) ***Listed as ""Kids""...woops
15- Mother's occupation: (1: retired, 2: housewife, 3: government officer, 4: private sector employee, 5: self-employment, 6: other)
16- Father's occupation: (1: retired, 2: government officer, 3: private sector employee, 4: self-employment, 5: other)
17- Weekly study hours: (1: None, 2: &lt;5 hours, 3: 6-10 hours, 4: 11-20 hours, 5: more than 20 hours)
18- Reading frequency (non-scientific books/journals): (1: None, 2: Sometimes, 3: Often)
19- Reading frequency (scientific books/journals): (1: None, 2: Sometimes, 3: Often)
20- Attendance to the seminars/conferences related to the department: (1: Yes, 2: No)
21- Impact of your projects/activities on your success: (1: positive, 2: negative, 3: neutral)
22- Attendance to classes (1: always, 2: sometimes, 3: never)
23- Preparation to midterm exams 1: (1: alone, 2: with friends, 3: not applicable)
24- Preparation to midterm exams 2: (1: closest date to the exam, 2: regularly during the semester, 3: never)
25- Taking notes in classes: (1: never, 2: sometimes, 3: always)
26- Listening in classes: (1: never, 2: sometimes, 3: always)
27- Discussion improves my interest and success in the course: (1: never, 2: sometimes, 3: always)
28- Flip-classroom: (1: not useful, 2: useful, 3: not applicable)
29- Cumulative grade point average in the last semester (/4.00): (1: &lt;2.00, 2: 2.00-2.49, 3: 2.50-2.99, 4: 3.00-3.49, 5: above 3.49)
30- Expected Cumulative grade point average in the graduation (/4.00): (1: &lt;2.00, 2: 2.00-2.49, 3: 2.50-2.99, 4: 3.00-3.49, 5: above 3.49)
31- Course ID
32- OUTPUT Grade (0: Fail, 1: DD, 2: DC, 3: CC, 4: CB, 5: BB, 6: BA, 7: AA)
Acknowledgements
Relevant Papers:
YÄ±lmaz N., Sekeroglu B. (2020) Student Performance Classification Using Artificial Intelligence Techniques. In: Aliev R., Kacprzyk J., Pedrycz W., Jamshidi M., Babanli M., Sadikoglu F. (eds) 10th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions - ICSCCW-2019. ICSCCW 2019. Advances in Intelligent Systems and Computing, vol 1095. Springer, Cham.
Citation Request:
YÄ±lmaz N., Sekeroglu B. (2020) Student Performance Classification Using Artificial Intelligence Techniques. In: Aliev R., Kacprzyk J., Pedrycz W., Jamshidi M., Babanli M., Sadikoglu F. (eds) 10th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions - ICSCCW-2019. ICSCCW 2019. Advances in Intelligent Systems and Computing, vol 1095. Springer, Cham.
Inspiration
Which students are most likely to succeed?"	1357	8231	31	csafrit2	higher-education-students-performance-evaluation
